<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.936097">
New Issues and Solutions in Computer-aided Design of
MCTI and Distractors Selection for Bulgarian
</title>
<author confidence="0.825502">
Ivelina Nikolova
</author>
<affiliation confidence="0.834723">
Institute for Parallel Processing, Bulgarian Academy of Sciences
</affiliation>
<address confidence="0.8836935">
25A Acad. G. Bonchev Str.
1113 Sofia
</address>
<email confidence="0.997098">
iva@lml.bas.bg
</email>
<sectionHeader confidence="0.987543" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906066666667">
We describe a methodology for improving the
generation of multiple-choice test items through
the usage of language technologies. We apply
common natural language processing techniques,
like constituency parsing and automatic term
extraction together with additional morpho-
syntactic rules on raw instructional material in
order to determine its key terms. These key
terms are then used for the creation of fill-in-
the blank test items and the selection of distrac-
tors. Our work aims at proving the availabil-
ity and compatibility of language resources and
technologies for Bulgarian, as well as at assessing
the readiness for implementation of these tech-
niques in real-world applications.
</bodyText>
<sectionHeader confidence="0.971083" genericHeader="keywords">
Keywords
</sectionHeader>
<bodyText confidence="0.6361885">
information extraction, natural language processing application
in e-learning
</bodyText>
<sectionHeader confidence="0.997182" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969117647059">
Multiple-choice tests (MCT) are a common tool to as-
sess learners achievements. They are widely proven
to be efficient. During the last years MCT gained
even more popularity due to the growth of the e-
learning programmes. In these programmes, which
are offered by universities and other educational in-
stitutions, multiple-choice questions appear to be the
most frequently used evaluation tool. Multiple-choice
is a form of assessment in which respondents are asked
to select the best possible answer(s) out of a list of
choices. We refer to the questions as stems, the best
option as correct answer and the rest of the given
choices as distractors. The demand for great quanti-
ties of such tests and the availability of already ad-
vanced learning technologies gave rise to a new re-
search area dealing with the generation of multiple-
choice test items (MCTI) and the suggestion of dis-
tractors from raw text.
The manual preparation of MCT is a time and ef-
fort consuming task. Teaching experts who prepare
the tests have much broader knowledge in their field
in general, compared to the specific content which is
explicitly included in the particular instructional ma-
terial. They have to tune the tests carefully to the
knowledge of the test takers. Hence one of the most
difficult subtasks during the creation of test items is to
decide whether a question does really have its answer
in the taught material. With an automatic extraction
of test items from the instructional material, this prob-
lem is easily solved and the time for test designing is
significantly reduced. An automatic extraction allows
the test designers to oversee large instructional mate-
rials in a new manner, giving them a content overview
and helping them to take faster decisions about the
topics to be included in a test and concrete questions
which could be given to the learners.
The generation of multiple-choice questions with the
help of natural language processing (NLP) technolo-
gies is an active research area in which different tools
for text processing are used in order to transform the
facts from the instructional materials to questions for
students assessment. The items produced in this way
are often used in Computer Assisted Language Learn-
ing (CALL), for vocabulary [2], grammar [3, 4, 1] or
language proficiency testing [11, 5], as well as in com-
prehension testing in specific subject areas in the na-
tive language [6]. Our aim is to produce multiple-
choice test items for testing learners achievements es-
pecially in the second area - learners comprehension of
specified instructional material.
We present the design of a workbench for test de-
signers employing language technologies for generation
of MCTI (stem, correct answer and distractors), which
are to be wrapped as learning objects (LO) and can
be loaded in an e-learning environment. The task is
divided into three subtasks: automatic keyterm extrac-
tion; sentence extraction and stem transformation and
distractors selection. In particular, we discuss our con-
tributions to an improved methodology for keyterm
and distractors selection and stem transformation.
The remainder of the article is organised as follows:
Section 2 describes the state-of-the-art; Section 3 re-
veals the motivation of the author; Section 4 outlines
the overall architecture of the workbench; Section 5
presents a detailed view of the text processing phases;
Section 6 presents a discussion on tests done with the
system and Section 7 gives a conclusion and issues for
future work.
</bodyText>
<sectionHeader confidence="0.999017" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999705666666667">
One of the first works on our topic was presented by
[3]. Fairon implemented a corpus search for finding
sentences or short parts of text that match initially
</bodyText>
<page confidence="0.7197645">
40
Multilingual Resources, Technologies and Evaluation for Central and Eastern European Languages 2009, Bulgaria, pages 40–46
</page>
<bodyText confidence="0.999955931034483">
preselected linguistic patterns. Later, [5] proposed a
word sense disambiguation method for locating sen-
tences in which designated words carry specific senses,
and applied a collocation-based method for selecting
distractors that are necessary for multiple-choice cloze
items. Our work differs from these approaches as far
as we detect relevant terms automatically, henceforth
called keyterms. Furthermore, for distractors selection
we employ morpho-syntactic information.
Authors working on vocabulary testing [2] use defi-
nitions or examples given for the focal term in Word-
Net in order to produce a non-interrogative stem. [6]
also employ WordNet, but only as a tool for distractors
selection. Their approach is domain independent; fur-
thermore, the authors report a 6-10 times speed-up in
comparison with a manual test elicitation. Similarly,
[11] uses a thesaurus in order to find distractors for
stem, generated by replacing the verb of the chosen
sentence with a blank. [4] apply standard classifica-
tion methods in order to decide the position in the gap
in the generation of fill-in-the-blank (FIB) test items.
Other researchers who are actively working in the area
include [1], who are focusing on the different types of
question models with application mainly in the lan-
guage learning. In our approach we extract sentences
which contain the central terms for the given material
in Bulgarian and produce FIB type of questions out
of them. Along with that, we also suggest the correct
answer and distractors.
</bodyText>
<sectionHeader confidence="0.995586" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.999931181818182">
The fact that we are not familiar with any related work
for learning materials in Bulgarian (except for previ-
ous work of the author [8]), together with the presence
of sophisticated language technologies for Bulgarian,
which allow for complex text analysis strongly inspired
us to work out the practical potential of our ideas.
Moreover, the growing interest in the field, which is
due to its significant practical importance, was a mo-
tivating factor to concretise our aims and more pre-
cisely to apply the developed technology for e-learning
purposes.
</bodyText>
<sectionHeader confidence="0.983787" genericHeader="method">
4 Workbench Outline
</sectionHeader>
<bodyText confidence="0.999950588235294">
The system is designed in a way that it accepts in-
structional material from the test designer in form of
raw text and produces draft learning objects - MCTI
of FIB type with their correct answer and possible dis-
tractors.
Our approach is based on the assumption that the
learner knowledge is tested over the terms, central to
the learning materials. As shown in Fig. 1, once the
text is submitted, a list of generated FIB questions,
concerning keyterms from the instructional material,
is presented to the test designer. At this moment, she
can modify all MCTI components and then save or
export them as a learning object or as a plain text
document.
The list of FIB stems serves as a cross-reference to
the whole text and facilitates for the test-designer in
summarising the learning topics.
</bodyText>
<sectionHeader confidence="0.986179" genericHeader="method">
5 Data Processing
</sectionHeader>
<bodyText confidence="0.999981181818182">
This section describes the processing of the data from
the user input to the output of the draft learning
objects. Well-established language technologies, like
parsing and automatic term extraction are employed.
Additionally, linguistic assumptions are taken into
consideration. An overview of the data processing
chain is shown in Fig. 2.
The input of the test designer is plain text instruc-
tional material, which has to be parsed in order to
extract lexico-syntactic features from the text. Due to
the importance of parsing as a basic source of infor-
mation used later on for the test items generation, we
have picked a statistical parser which reports state-of-
the-art results and has been tuned to work with Bul-
garian - the Berkeley parser [9, 10]. The parser was
trained on BulTreeBank1. Parsing texts from the same
domain as the training corpus gave highly satisfactory
results.
After reformatting the parsed text, we extract from
it all nouns and noun phrase structures as well as
names. From the tools offering fast structure querying
for our purposes the most appropriate turned out to
be the CLaRK system2. As it is based on Xpath ex-
pression querying it is fully configurable. In contrast
with the NP extractor Morena we used earlier, CLaRK
allows for the manual specification of sequences of con-
stituents.
In order to overcome the language inflection, the
extracted morpho-syntactic structures are stemmed
using BulStem [7] and organised in an internal
representation format, where each stem3 maps to all
NPs having the same stem. Here is an example of a
partial record for the stem saxoH (law):
</bodyText>
<construct confidence="0.9911504">
[stem value=”3axoH” occurences=51
type=”N” isKeyterm=false]
[instance value=”3axoH”]
[instance value=”3axoHHTe”]
[instance value=”3axoHa”]
</construct>
<bodyText confidence="0.9999706875">
In this case, the stem is saxoH (law). It has a total
of 51 occurences in the document. Some of them are
zaxon (law), zaxonume (the laws), saxoHa (the law)
and it is type noun. Other NP types are np-A-N- NP
composed of adjective and noun, np-N-PP - NP com-
posed of noun and prepositional phrase, NE-loc - name
of the type location, NE-org - name of the type organ-
isation, NE-Pers - name of the type person, NE-other
- name of the type other). For each wordform (phrase)
corresponding to the stem, only one instance is gen-
erated. If the wordform appears at least twice, only
the counter occurences is incremented, but no new in-
stance is created. The attribute isKeyterm is initially
set to false for all stems. After the keyterm threshold
is set, it is turned to true for the terms which belong to
the keyterms list. This representation is the starting
</bodyText>
<footnote confidence="0.99541975">
1 A HPSG-based Syntactic Treebank of Bulgarian (BulTree-
Bank), http://bultreebank.org/.
2 CLaRK - an XML Based System for Corpora Development,
http://bultreebank.org/clark/index.html.
</footnote>
<page confidence="0.9062125">
3 A stem is the common prefix of several wordforms.
41
</page>
<figureCaption confidence="0.99995">
Fig. 1: Workbench supporting the development of multiple-choice test items.
Fig. 2: Data processing.
</figureCaption>
<page confidence="0.99129">
42
</page>
<bodyText confidence="0.999920727272727">
point for any further processing in order to generate
the test item stem and distractors.
The occurences attribute of the stem field is used
later on for calculating the threshold for important
terms. The instances are used in order to expand the
stem and query the text for sentences containing the
exact keyterms. In the extracted sentences, we replace
the keyterm with a blank and offer it as an FIB item
together with the keyterm as its correct answer. Then,
applying some linguistic rules on the keyterm yields a
set of distractor suggestions.
</bodyText>
<subsectionHeader confidence="0.979992">
5.1 Keyterms Extraction
</subsectionHeader>
<bodyText confidence="0.98910465625">
In order to extract the terms which are central for the
instructional material and to filter out the less impor-
tant ones we establish some requirements a keyterm
should adhere to:
– keyterms are nouns and noun phrases from the
text, which frequency is higher than a set threshold;
– keyterms are the noun phrases, which contain a
keyterm with frequency higher than the set threshold;
– all names are keyterms.
The first step in this respect is to extract all po-
tential keyterms. In previous research, we have con-
centrated on extracting nouns, noun phrases of the
type np − A − N and names, but now, in order to ex-
tend the list of valuable keyterms, we have inserted an
additional type of noun phrases: np − N − PP. In
domains like Law, where the specific terms tend to be
longer, exactly this structure greatly helps in detecting
keyterms. After all potential keyterms are extracted
they are stemmed and the two lists of terms – the
stemmed and the original one – are arranged in the
internal representation shown above.
As we have determined in previous research and
is reported also by other authors [6], in instructional
materials the keyterms are often repeated in order to
make the learner remember them. That is why sim-
ple term frequency is a better measure than TF-IDF,
which tends to lower the score of the most often used
words. We store the frequencies of our terms (fti) in
the occurences attributes for each stem. We sort these
frequencies and calculate the number of words having
equal frequency (rftp). Then we set the threshold as
follows:
</bodyText>
<equation confidence="0.972225">
threshold = maxfti − 2, {rftp &gt; fti} (1)
</equation>
<bodyText confidence="0.999984846153846">
We have established this procedure for threshold set-
ting empirically, by observing manually prepared test
items and analysing the keyterms used in questions
and answers.
Once the threshold is set, we initialise the list of
keyterms by adding to it all nouns or noun phrases
that have frequency higher than the threshold. In
the second step, we add to the keyterm list all noun
phrases that contain a keyterm, without regarding
their occurrence frequency. In a third step, we add
all names as keyterms. For example: Along with the
stem of the noun HpaBo (Right/Law) all these noun
phrase stems will be added to the list of keyterms:
</bodyText>
<figure confidence="0.887723">
4 All string comparisons are done in stemmed fashion.
ppavo na жajtbu (right of complaint)
ppavo na жuvot (right of living)
ppavo na zaconodatejt unuzyuatuv (right of legislation
initiative)
meжdunapod ppav (International Law) etc.
</figure>
<bodyText confidence="0.993134">
All NPs containing stop words are removed. In our
case stop words most often appear to be personal and
possessive pronouns.
The belonging of each stem to the keyterm list is
implemented by turning the value of the attribute
isKeyterm to true in the internal representation shown
above.
</bodyText>
<subsectionHeader confidence="0.996433">
5.2 Stem Extraction
</subsectionHeader>
<bodyText confidence="0.982693307692308">
We aim to produce a MCTI, which is of FIB type, and
along with it, to suggest a correct answer and possi-
ble distractors. Seen from this perspective, our task
may be thought of as vocabulary testing where op-
tional answers are available. Taking into account the
constraints we have put on the extraction of keyterms,
we decided to relax the syntactic restrictions about the
position of the keyterm, resp. the blank. As only re-
quirement in this respect, we set the extraction of well-
formed sentences. In terms of the grammar we use,
these are sentences wrapped in a VPS constituents.
We extract all sentences from the text which contain
at least one of the keyterms. For each of the keyterms
in a sentence, we check whether it is a part of a longer
keyterm:
– if it is not, we replace it with blank and save the
so-produced stem;
– if it is contained in a longer keyterm, then we
replace with blank the longest keyterm it is a part of
and then save the stem.
Consider the following example. A sentence con-
taining a keyterm ppavo (law) is:
Vъnxnata politika na Republika BъlgariH se
osъwestvHva v sъotvet-stvie s principite i normite
na meжdunarodnoto ppavo.
(The foreign policy of Republic of Bulgaria is realised
in conformity with the International Law.)
The longest sequence of words containing the keyterm
ppavo (right/Law) and being a keyterm is mew-
dynapodnoto ppavo (International Law). That is
why the system catches mewdynapodnoto ppavo
and replaces it with a blank and produces the stem:
Vъnxnata politika na Republika BъlgariH se
osъwestvHva v sъotvet-stvie s principite i normite
na .
Respectively in the following sentence:
Quжdencite i quжdestrannite Eoridiqeski lica
ne mogat da pridobivat pravo na sobstvenost vъrhu
zemH osven pri nasledHvane po zakon.
</bodyText>
<footnote confidence="0.99688375">
5 VPS -head-subject verb phrase for full definitions -
HPSG-based Syntactic Treebank of Bulgarian (BulTree-
Bank), BulTreeBank Project Technical Report 05. 2004,
http://bultreebank.org/TechRep/BTB-TR05.pdf
</footnote>
<page confidence="0.999658">
43
</page>
<bodyText confidence="0.993017333333333">
(The foreigners and foreign legal bodies cannot acquire
land property rights except for the case of inheritance by
law.)
the longest sequence containing the term npaeo
and being a keyterm is the phrase ”npaeo na co6-
cmeenocm eapxy aemw” (land property right). Hence
the system will replace this keyterm with blank
and will produce the following FIB stem (instead of
considering for a keyterm npaeo only).
</bodyText>
<subsectionHeader confidence="0.540913">
Quжdencite i quжdestrannite Ioridiqeski lica ne
</subsectionHeader>
<bodyText confidence="0.825863">
mogat da pridobivat osven pri nasledHvane
po zakon.
</bodyText>
<subsectionHeader confidence="0.998916">
5.3 Suggestion of Distractors
</subsectionHeader>
<bodyText confidence="0.989672393939394">
In well designed multiple-choice questions, the distrac-
tors are semantically close to the correct answer, as
well as to each other in a sense. On the basis of our
previous work and observations over manually pre-
pared tests we suppose that distractors are close to
each other if they look alike, too. Very often in tests
for beginners, the distractors are noun phrases which
contain the same noun as the one in the correct answer
but with a different modifier or the other way round -
the same modifier, but different noun. Our approach
was based mainly on this assumption so far and now
we want to extend the idea to the following:
– distractors of NPs of the type np − N − PP are
only NPs of the same type and the same head noun;
– distractors of NPs of the type np−A−N are only
NPs of the same type, which contain the same noun
and different modifier or contain the same modifier
and different noun;
– distractors of nouns are all NPs containing the
given noun;
– distractors of names are names of the same type
(for ex. for a keyterm which is a name of the type
Org all names of the type Org are distractors).
The distractors are matched with the keyterms in
a stemmed fashion too. Later on they are expanded
to their full form and they are offered to the test-
designer. Given the previously shown examples we
may produce the following distractors:
Stem: Vъnxnata politika na Republika Bъl-
gariH se osъwestvHva v sъotvet-stvie s principite
i normite na meжdunarodnoto pravo. (The foreign
policy of Republic of Bulgaria is realised in conformity
with the International Law.)
</bodyText>
<figure confidence="0.4881296">
Keyterm/correct answer: meжdunarodnoto pravo
(the International Law)
Type: np-A-N
Possible distractors: vъtrexnoto pravo (the
Domestic Law); izbiratelno pravo (franchise)
</figure>
<footnote confidence="0.4275095">
Consider the case when the keyword is a part of
np-N-PP phrase:
Stem: Quжdencite i quжdestrannite Ioridiqeski
lica ne mogat da pridobivat pravo na sobstvenost
</footnote>
<bodyText confidence="0.893887285714286">
vъrhu zemH osven pri nasledRvane po zakon. (The
foreigners and foreign legal bodies cannot acquire land
property rights except in case of inheritance by law.)
Keyterm/correct answer: pravo na sobstvenost
vъrhu zemff (land property right)
Type: np-N-PP
Possible distractors: pravo na advokat-ska zax-
qita (right of advocate defense), pravo na жalbi (right
of complaint), pravo na жivot (right of life), pravo
na zakonodatelna iniciativa (right of legislation
initiative), pravo na liqna svoboda (right of personal
freedom), pravo na polzvane (right to use), pravo na
stroeж (right of construction), pravo na trud (right to
work).
When the keyterms are names, all names of the
same type are distractors to each other. For example
when processing the Bulgarian Constitution the
names of the type NE-Loc are only Coouw (Sofia)
and Eaazapuw (Bulgaria) and they are treated always
as options to each other. As Sofia is a city and
Bulgaria a country they hardly assimilate each other
and unfortunately we can not cope with this issue yet
as we do not rely on any external resources which
could deliver us this additional information. Consider
the following problematic example where a question
complying with this rule would look like.
Stem: Stolicata na Republika Bъlgaria e grad
Sofi�. (The capital of Republic of Bulgaria is Sofia.)
</bodyText>
<equation confidence="0.291071">
Keyterm/correct answer: Sofiv (Sofia)
Type: NE-Loc
Possible distractors: Bъlgariar (Bulgaria)
</equation>
<bodyText confidence="0.9999783">
We want to make clear that this is not the gen-
eral figure but only a specific case. Our observation is
that with the additional rule for distractors selection
of names, the performance of the system significantly
increases. Of course the recognition of the names is
a matter of good parsing, but as the parser model is
trained on BulTreeBank, which contains annotation
for named entities, we rely on comparatively high rate
of recognition at least in the domains in which the
parser was trained.
</bodyText>
<sectionHeader confidence="0.980823" genericHeader="evaluation">
6 Testing and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.990844">
6.1 Assessment of Results Obtained
from the Bulgarian Constitution
</subsectionHeader>
<bodyText confidence="0.999660777777778">
As a first try of evaluation of our system, we run it
over an extracted part of the Bulgarian Constitution
and asked several experts to evaluate the quality of the
resulting MCTI and the features of the system. The
Law domain is characterised with its relatively long
terms and also the high frequency of the terms and
importance of most of the sentences. The linguistic
patterns we chose for extracting keyterms and distrac-
tors turned out to suit very well the keyterms in the
</bodyText>
<page confidence="0.998036">
44
</page>
<bodyText confidence="0.99909752">
Law domain. This is apparent from the results shown
in Fig 3. 75% of the input sentences were extracted as
MCTI. The high number of selected FIB stems means
that a high percentage of sentences in the text contain
keyterms. This is explicable with the nature of the
laws where the redundant information is reduced to a
minimum.
The average number of suggested distractors is be-
tween 2 and 3. The number of distractors for the ex-
amined texts varies between 0 and 7 and often all of
the distractors are good suggestions. For about one
third of the resulting MCTI the system could not offer
distractors, which is due to the fact that the distrac-
tors are selected only from the submitted instructional
materials and no external sources are used.
The criteria for evaluating the results were the fol-
lowing:
Quality of the question (1-3):
1 - the question is not proper for testing learners on
this material; 2 - the question is unclear; 3 - the ques-
tion is a well formed sentence, concerning terms which
are central for the instructional material.
Quality of the answer (1-3):
1 - the answer is not central for the instructional ma-
terial; 2 - the answer is central for the instructional
material but more specific or general than the desired
answer; 3 - the answer is a central for the instructional
material and concrete enough.
Fig. 3 shows the trends in average scores given by
the experts when assessing the quality of the stems
and correct answers (keyterms) of the selected ques-
tions (shown on the X-axis), according to the criteria
given above (shown on the Y-axis). Both the stems
and answers received most often the highest mark (3).
Half of the evaluators have given the highest score (3)
to all of the questions and the other half have given
different marks maximum to the half of the sentences.
This means that keyterms are correctly chosen by the
algorithm and they have high importance for the ma-
terial. Given this fact, we can explain the high score
given to the stems by the fact that in Law the sentences
structure is very compact. Almost each sentence rep-
resents a separate rule and they are often independent
from each other. In this respect the legal texts differ
significantly from texts in humanities like Geography
and History where references are often used and terms
are not that strictly defined. The high scores given for
question quality could be also explained with the fact
that the questions are directly extracted from the text,
and thus their grammatical well-formness is preserved.
</bodyText>
<subsectionHeader confidence="0.994621">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999970175675676">
Our aim with this work was to explore the field of auto-
matic generation of MCTI and to prove the availability
and compatibility of the language resources and tech-
nologies for Bulgarian as well as to assess the readiness
for the implementation of these techniques in real-
world applications. We were attracted by the high
level of the work done in this area for English and we
wanted to check whether it is possible to build a work-
ing prototype using some existing tools and to make
inferences about the directions in which language tech-
nologies (LT) development for Bulgarian should take.
Given the fact that the state-of-the-art in LT for En-
glish and for Bulgarian is incomparable, we wanted to
point out concrete steps which must be taken in con-
sideration to help the development of the next gener-
ation LT for Bulgarian.
From this experiment we can clearly point out sev-
eral decisive factors, whose improvement will lead to
more satisfactory results and overall progress in the
area. First of all, as a fundamental basis of all fur-
ther processing, improvements in parsing will result
in more correct extraction of target morpho-syntactic
structures. In the experiment we noticed that for doc-
uments from the same domains as the ones in the
training corpus the parsing performs with very high
precision which is comparable to the state-of-the-art
results declared by the parser-developers, but for other
documents, however, the precision drops dramatically.
This is due to the fact that the parser is fully statisti-
cal and does not accept any additional POS input with
the parsing string as some other parsers do. Improved
syntactic analysis would mean more correct keyterm
extraction and better distractors selection.
Our work so far, although employing several differ-
ent language processing techniques is strongly depen-
dent on the parsing results and limited to the lexemes
available in the instructional material. A complemen-
tary resource like a Thesaurus of any kind would give
us the options to go beyond the limits of the pro-
cessed text and will extend the capabilities of our sys-
tem. Dictionary of synonyms/antonyms, dictionaries
of names in Bulgarian will also be of great help for
defining better possible distractors and go one step
further and form a question-like stem instead of a FIB
one. For this purpose in future we intend to integrate
BalkaNet 6 as a component in the described system.
The lack of additional resources for conceptual
processing in Bulgarian is tangible. A terminolog-
ical dictionary would set a common terminological
frame for the analysed materials and would facilitate
the keyterm and distractors selection; dictionaries of
names would be of great help in defining better pos-
sible distractors as well, variety of annotated corpora
in different domains would improve the parser perfor-
mance.
When talking about resources we must mention as
well the quality of the input resources. From the pro-
cessing algorithm it is clear that some kinds of texts are
hard to analyse. For example, tabular data just trans-
formed to plain text format will would not constitute
good sentences. Mathematical or chemical formulae
will hardly fit in any of the patterns adapted for other
domains. The input used for similar systems should
be carefully adjusted for the specific needs.
Stemming seems to be satisfactory enough. There
is no need for applying lemmatisation on extracted
terms. In the observed samples, we have not found
examples of overstemming or understemming which
would be better solved by lemmatisation. We explain
this with the fact that after stemming we work mostly
with phrases and then inflexional ambiguity is much
lower which makes the technique for transforming the
wordforms to a single one (stem/lemma) less signifi-
cant.
</bodyText>
<footnote confidence="0.4393925">
6 Multilingual lexical database comprising of individual Word-
Nets for the Balkan languages
</footnote>
<page confidence="0.998661">
45
</page>
<figureCaption confidence="0.98804">
Fig. 3: Average scores given to stems and answers.
</figureCaption>
<bodyText confidence="0.999990366666667">
The chosen morpho-syntactic categories prove to be
efficient and catch most of the terminology available in
the instructional materials. We are especially satisfied
with the addition of the noun phrases of the type np-N-
PP which tend to match keyterm phrases in domains
with comparatively longer terms like Law. We no-
tice that even more categories coul be added (like the
ones satisfying the regular expression A+N). In com-
parison with previously reported work we noted that
the new approach in distractor suggestion gains sig-
nificant improvement from filtering useless distractors.
Our expectation is that in a large-scale evaluation, the
distractors, which are names, would contribute signif-
icantly to the overall efficiency of the system.
Under a direct comparison, the results we obtain
for Bulgarian are not as good as those obtained for
English, but this discrepancy can be explained by the
presence of much more sophisticated language tech-
nologies for English. The presence of such tools and
resources for Bulgarian will help us to gain concep-
tual knowledge about the target terms, to build more
semantically-grounded distractors and to better filter
significant from insignificant terms and sentences. Due
to the limited resources available for Bulgarian, the
capabilities of our system are also limited. However,
we have implemented the main idea of the automatic
MCTI generation and have shown what can be done
with some of the existing language resources for Bul-
garian as well as we have also scatched the gaps that
need to be addressed in the future.
</bodyText>
<sectionHeader confidence="0.990869" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999986266666667">
Our aim with this work was to explore the field of au-
tomatic MCTI generation and to prove the availability
and compatibility of the language resources and tech-
nologies for Bulgarian as well as to assess the readi-
ness for the implementation of these techniques in real-
world applications.
Our ideas for future development are related to ex-
periments with a larger variety of question types and
better distractors selection by involving dependency
parsing and more external resources. We are working
on improvement of the user interface as it is a main
issue concerning the test designers’ efficiency and will
allow a real-time evaluation. Deeper evaluation, in-
cluding classical test theory and error analysis in order
to improve the output is also one of our future goals.
</bodyText>
<sectionHeader confidence="0.999242" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999973226415095">
[1] I. Aldabe, M. L. De Lacalle, and M. Maritxalar. Automatic
acquisition of didactic resources: generating test-based ques-
tions. In I. F. de Castro, editor, Proceeding of SINTICE 07,
pages 105–111, 2007.
[2] J. C. Brown, G. A. Frishkoff, and M. Eskenazi. Automatic
question generation for vocabulary assessment. In HLT ’05:
Proceedings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Processing,
pages 819–826, Morristown, NJ, USA, 2005. Association for
Computational Linguistics.
[3] C. Fairon. A web-based system for automatic language skill
assessment: Evaling. In Proceedings of Computer Mediated
Language Assessment and Evaluation in Natural Language
Processing Workshop, pages 62–67, 1999.
[4] A. Hoshino and N. Hiroshi. A real-time multiple-choice ques-
tion generation for language testing: A preliminary study. In
Proceedings of the Second Workshop on Building Educational
Applications Using NLP, pages 17–20, Ann Arbor, Michigan,
June 2005. Association for Computational Linguistics.
[5] C.-L. Liu, C.-H. Wang, Z.-M. Gao, and S.-M. Huang. Ap-
plications of lexical information for algorithmically compos-
ing multiple-choice cloze items. In Proceedings of the Second
Workshop on Building Educational Applications Using NLP,
pages 1–8, Ann Arbor, Michigan, June 2005. Association for
Computational Linguistics.
[6] R. Mitkov, L. A. Ha, and N. Karamis. A computer-aided en-
vironment for generating multiple-choice test items. Natural
Language Engineering, 12.:177–194, 2006.
[7] P. Nakov. Bulstem: Design and evaluation of inflectional stem-
mer for bulgarian. In Proceedings of Workshop on Balkan
Language Resources and Tools (1st Balkan Conference in In-
formatics, Thessaloniki, Greece, 2003.
[8] I. Nikolova. Language technologies for instructional resources
in bulgarian. In K. Balogh, editor, Proceedings of 13th Student
Session at ESSLLI 2008, pages 135–142, 2008.
[9] S. Petrov, L. Barrett, R. Thibaux, and D. Klein. Learning accu-
rate, compact, and interpretable tree annotation. In Proceed-
ings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433–440, Sydney, Australia,
July 2006. Association for Computational Linguistics.
[10] S. Petrov and D. Klein. Improved inference for unlexicalized
parsing. In Human Language Technologies 2007: The Con-
ference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April 2007. Asso-
ciation for Computational Linguistics.
[11] E. Sumita, F. Sugaya, and S. Yamamoto. Measuring non-
native speakers’ proficiency of English by using a test with
automatically-generated fill-in-the-blank questions. In Pro-
ceedings of the Second Workshop on Building Educational
Applications Using NLP, pages 61–68, Ann Arbor, Michigan,
June 2005. Association for Computational Linguistics.
</reference>
<page confidence="0.999612">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.172826">
<title confidence="0.981702">New Issues and Solutions in Computer-aided Design of MCTI and Distractors Selection for Bulgarian</title>
<author confidence="0.451792">Ivelina</author>
<affiliation confidence="0.536701">Institute for Parallel Processing, Bulgarian Academy of</affiliation>
<address confidence="0.398718">25A Acad. G. Bonchev 1113</address>
<abstract confidence="0.999823375">We describe a methodology for improving the generation of multiple-choice test items through the usage of language technologies. We apply common natural language processing techniques, like constituency parsing and automatic term extraction together with additional morphosyntactic rules on raw instructional material in order to determine its key terms. These key terms are then used for the creation of fill-inthe blank test items and the selection of distractors. Our work aims at proving the availability and compatibility of language resources and technologies for Bulgarian, as well as at assessing the readiness for implementation of these techniques in real-world applications.</abstract>
<keyword confidence="0.9793385">Keywords information extraction, natural language processing application</keyword>
<intro confidence="0.83976">in e-learning</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Aldabe</author>
<author>M L De Lacalle</author>
<author>M Maritxalar</author>
</authors>
<title>Automatic acquisition of didactic resources: generating test-based questions.</title>
<date>2007</date>
<booktitle>Proceeding of SINTICE 07,</booktitle>
<pages>105--111</pages>
<editor>In I. F. de Castro, editor,</editor>
<contexts>
<context position="3300" citStr="[3, 4, 1]" startWordPosition="521" endWordPosition="523">ew manner, giving them a content overview and helping them to take faster decisions about the topics to be included in a test and concrete questions which could be given to the learners. The generation of multiple-choice questions with the help of natural language processing (NLP) technologies is an active research area in which different tools for text processing are used in order to transform the facts from the instructional materials to questions for students assessment. The items produced in this way are often used in Computer Assisted Language Learning (CALL), for vocabulary [2], grammar [3, 4, 1] or language proficiency testing [11, 5], as well as in comprehension testing in specific subject areas in the native language [6]. Our aim is to produce multiplechoice test items for testing learners achievements especially in the second area - learners comprehension of specified instructional material. We present the design of a workbench for test designers employing language technologies for generation of MCTI (stem, correct answer and distractors), which are to be wrapped as learning objects (LO) and can be loaded in an e-learning environment. The task is divided into three subtasks: autom</context>
<context position="6008" citStr="[1]" startWordPosition="941" endWordPosition="941">n order to produce a non-interrogative stem. [6] also employ WordNet, but only as a tool for distractors selection. Their approach is domain independent; furthermore, the authors report a 6-10 times speed-up in comparison with a manual test elicitation. Similarly, [11] uses a thesaurus in order to find distractors for stem, generated by replacing the verb of the chosen sentence with a blank. [4] apply standard classification methods in order to decide the position in the gap in the generation of fill-in-the-blank (FIB) test items. Other researchers who are actively working in the area include [1], who are focusing on the different types of question models with application mainly in the language learning. In our approach we extract sentences which contain the central terms for the given material in Bulgarian and produce FIB type of questions out of them. Along with that, we also suggest the correct answer and distractors. 3 Motivation The fact that we are not familiar with any related work for learning materials in Bulgarian (except for previous work of the author [8]), together with the presence of sophisticated language technologies for Bulgarian, which allow for complex text analysi</context>
</contexts>
<marker>[1]</marker>
<rawString>I. Aldabe, M. L. De Lacalle, and M. Maritxalar. Automatic acquisition of didactic resources: generating test-based questions. In I. F. de Castro, editor, Proceeding of SINTICE 07, pages 105–111, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Brown</author>
<author>G A Frishkoff</author>
<author>M Eskenazi</author>
</authors>
<title>Automatic question generation for vocabulary assessment.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>819--826</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="3281" citStr="[2]" startWordPosition="519" endWordPosition="519">erials in a new manner, giving them a content overview and helping them to take faster decisions about the topics to be included in a test and concrete questions which could be given to the learners. The generation of multiple-choice questions with the help of natural language processing (NLP) technologies is an active research area in which different tools for text processing are used in order to transform the facts from the instructional materials to questions for students assessment. The items produced in this way are often used in Computer Assisted Language Learning (CALL), for vocabulary [2], grammar [3, 4, 1] or language proficiency testing [11, 5], as well as in comprehension testing in specific subject areas in the native language [6]. Our aim is to produce multiplechoice test items for testing learners achievements especially in the second area - learners comprehension of specified instructional material. We present the design of a workbench for test designers employing language technologies for generation of MCTI (stem, correct answer and distractors), which are to be wrapped as learning objects (LO) and can be loaded in an e-learning environment. The task is divided into th</context>
<context position="5339" citStr="[2]" startWordPosition="830" endWordPosition="830">es and Evaluation for Central and Eastern European Languages 2009, Bulgaria, pages 40–46 preselected linguistic patterns. Later, [5] proposed a word sense disambiguation method for locating sentences in which designated words carry specific senses, and applied a collocation-based method for selecting distractors that are necessary for multiple-choice cloze items. Our work differs from these approaches as far as we detect relevant terms automatically, henceforth called keyterms. Furthermore, for distractors selection we employ morpho-syntactic information. Authors working on vocabulary testing [2] use definitions or examples given for the focal term in WordNet in order to produce a non-interrogative stem. [6] also employ WordNet, but only as a tool for distractors selection. Their approach is domain independent; furthermore, the authors report a 6-10 times speed-up in comparison with a manual test elicitation. Similarly, [11] uses a thesaurus in order to find distractors for stem, generated by replacing the verb of the chosen sentence with a blank. [4] apply standard classification methods in order to decide the position in the gap in the generation of fill-in-the-blank (FIB) test item</context>
</contexts>
<marker>[2]</marker>
<rawString>J. C. Brown, G. A. Frishkoff, and M. Eskenazi. Automatic question generation for vocabulary assessment. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 819–826, Morristown, NJ, USA, 2005. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fairon</author>
</authors>
<title>A web-based system for automatic language skill assessment: Evaling.</title>
<date>1999</date>
<booktitle>In Proceedings of Computer Mediated Language Assessment and Evaluation in Natural Language Processing Workshop,</booktitle>
<pages>62--67</pages>
<contexts>
<context position="3300" citStr="[3, 4, 1]" startWordPosition="521" endWordPosition="523">ew manner, giving them a content overview and helping them to take faster decisions about the topics to be included in a test and concrete questions which could be given to the learners. The generation of multiple-choice questions with the help of natural language processing (NLP) technologies is an active research area in which different tools for text processing are used in order to transform the facts from the instructional materials to questions for students assessment. The items produced in this way are often used in Computer Assisted Language Learning (CALL), for vocabulary [2], grammar [3, 4, 1] or language proficiency testing [11, 5], as well as in comprehension testing in specific subject areas in the native language [6]. Our aim is to produce multiplechoice test items for testing learners achievements especially in the second area - learners comprehension of specified instructional material. We present the design of a workbench for test designers employing language technologies for generation of MCTI (stem, correct answer and distractors), which are to be wrapped as learning objects (LO) and can be loaded in an e-learning environment. The task is divided into three subtasks: autom</context>
<context position="4596" citStr="[3]" startWordPosition="730" endWordPosition="730">n. In particular, we discuss our contributions to an improved methodology for keyterm and distractors selection and stem transformation. The remainder of the article is organised as follows: Section 2 describes the state-of-the-art; Section 3 reveals the motivation of the author; Section 4 outlines the overall architecture of the workbench; Section 5 presents a detailed view of the text processing phases; Section 6 presents a discussion on tests done with the system and Section 7 gives a conclusion and issues for future work. 2 Related Work One of the first works on our topic was presented by [3]. Fairon implemented a corpus search for finding sentences or short parts of text that match initially 40 Multilingual Resources, Technologies and Evaluation for Central and Eastern European Languages 2009, Bulgaria, pages 40–46 preselected linguistic patterns. Later, [5] proposed a word sense disambiguation method for locating sentences in which designated words carry specific senses, and applied a collocation-based method for selecting distractors that are necessary for multiple-choice cloze items. Our work differs from these approaches as far as we detect relevant terms automatically, hence</context>
</contexts>
<marker>[3]</marker>
<rawString>C. Fairon. A web-based system for automatic language skill assessment: Evaling. In Proceedings of Computer Mediated Language Assessment and Evaluation in Natural Language Processing Workshop, pages 62–67, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hoshino</author>
<author>N Hiroshi</author>
</authors>
<title>A real-time multiple-choice question generation for language testing: A preliminary study.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>17--20</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3300" citStr="[3, 4, 1]" startWordPosition="521" endWordPosition="523">ew manner, giving them a content overview and helping them to take faster decisions about the topics to be included in a test and concrete questions which could be given to the learners. The generation of multiple-choice questions with the help of natural language processing (NLP) technologies is an active research area in which different tools for text processing are used in order to transform the facts from the instructional materials to questions for students assessment. The items produced in this way are often used in Computer Assisted Language Learning (CALL), for vocabulary [2], grammar [3, 4, 1] or language proficiency testing [11, 5], as well as in comprehension testing in specific subject areas in the native language [6]. Our aim is to produce multiplechoice test items for testing learners achievements especially in the second area - learners comprehension of specified instructional material. We present the design of a workbench for test designers employing language technologies for generation of MCTI (stem, correct answer and distractors), which are to be wrapped as learning objects (LO) and can be loaded in an e-learning environment. The task is divided into three subtasks: autom</context>
<context position="5803" citStr="[4]" startWordPosition="908" endWordPosition="908">h called keyterms. Furthermore, for distractors selection we employ morpho-syntactic information. Authors working on vocabulary testing [2] use definitions or examples given for the focal term in WordNet in order to produce a non-interrogative stem. [6] also employ WordNet, but only as a tool for distractors selection. Their approach is domain independent; furthermore, the authors report a 6-10 times speed-up in comparison with a manual test elicitation. Similarly, [11] uses a thesaurus in order to find distractors for stem, generated by replacing the verb of the chosen sentence with a blank. [4] apply standard classification methods in order to decide the position in the gap in the generation of fill-in-the-blank (FIB) test items. Other researchers who are actively working in the area include [1], who are focusing on the different types of question models with application mainly in the language learning. In our approach we extract sentences which contain the central terms for the given material in Bulgarian and produce FIB type of questions out of them. Along with that, we also suggest the correct answer and distractors. 3 Motivation The fact that we are not familiar with any related</context>
</contexts>
<marker>[4]</marker>
<rawString>A. Hoshino and N. Hiroshi. A real-time multiple-choice question generation for language testing: A preliminary study. In Proceedings of the Second Workshop on Building Educational Applications Using NLP, pages 17–20, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-L Liu</author>
<author>C-H Wang</author>
<author>Z-M Gao</author>
<author>S-M Huang</author>
</authors>
<title>Applications of lexical information for algorithmically composing multiple-choice cloze items.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>1--8</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3340" citStr="[11, 5]" startWordPosition="528" endWordPosition="529">and helping them to take faster decisions about the topics to be included in a test and concrete questions which could be given to the learners. The generation of multiple-choice questions with the help of natural language processing (NLP) technologies is an active research area in which different tools for text processing are used in order to transform the facts from the instructional materials to questions for students assessment. The items produced in this way are often used in Computer Assisted Language Learning (CALL), for vocabulary [2], grammar [3, 4, 1] or language proficiency testing [11, 5], as well as in comprehension testing in specific subject areas in the native language [6]. Our aim is to produce multiplechoice test items for testing learners achievements especially in the second area - learners comprehension of specified instructional material. We present the design of a workbench for test designers employing language technologies for generation of MCTI (stem, correct answer and distractors), which are to be wrapped as learning objects (LO) and can be loaded in an e-learning environment. The task is divided into three subtasks: automatic keyterm extraction; sentence extrac</context>
<context position="4868" citStr="[5]" startWordPosition="767" endWordPosition="767"> author; Section 4 outlines the overall architecture of the workbench; Section 5 presents a detailed view of the text processing phases; Section 6 presents a discussion on tests done with the system and Section 7 gives a conclusion and issues for future work. 2 Related Work One of the first works on our topic was presented by [3]. Fairon implemented a corpus search for finding sentences or short parts of text that match initially 40 Multilingual Resources, Technologies and Evaluation for Central and Eastern European Languages 2009, Bulgaria, pages 40–46 preselected linguistic patterns. Later, [5] proposed a word sense disambiguation method for locating sentences in which designated words carry specific senses, and applied a collocation-based method for selecting distractors that are necessary for multiple-choice cloze items. Our work differs from these approaches as far as we detect relevant terms automatically, henceforth called keyterms. Furthermore, for distractors selection we employ morpho-syntactic information. Authors working on vocabulary testing [2] use definitions or examples given for the focal term in WordNet in order to produce a non-interrogative stem. [6] also employ Wo</context>
</contexts>
<marker>[5]</marker>
<rawString>C.-L. Liu, C.-H. Wang, Z.-M. Gao, and S.-M. Huang. Applications of lexical information for algorithmically composing multiple-choice cloze items. In Proceedings of the Second Workshop on Building Educational Applications Using NLP, pages 1–8, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
<author>L A Ha</author>
<author>N Karamis</author>
</authors>
<title>A computer-aided environment for generating multiple-choice test items.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<contexts>
<context position="3430" citStr="[6]" startWordPosition="546" endWordPosition="546">te questions which could be given to the learners. The generation of multiple-choice questions with the help of natural language processing (NLP) technologies is an active research area in which different tools for text processing are used in order to transform the facts from the instructional materials to questions for students assessment. The items produced in this way are often used in Computer Assisted Language Learning (CALL), for vocabulary [2], grammar [3, 4, 1] or language proficiency testing [11, 5], as well as in comprehension testing in specific subject areas in the native language [6]. Our aim is to produce multiplechoice test items for testing learners achievements especially in the second area - learners comprehension of specified instructional material. We present the design of a workbench for test designers employing language technologies for generation of MCTI (stem, correct answer and distractors), which are to be wrapped as learning objects (LO) and can be loaded in an e-learning environment. The task is divided into three subtasks: automatic keyterm extraction; sentence extraction and stem transformation and distractors selection. In particular, we discuss our cont</context>
<context position="5453" citStr="[6]" startWordPosition="851" endWordPosition="851">tterns. Later, [5] proposed a word sense disambiguation method for locating sentences in which designated words carry specific senses, and applied a collocation-based method for selecting distractors that are necessary for multiple-choice cloze items. Our work differs from these approaches as far as we detect relevant terms automatically, henceforth called keyterms. Furthermore, for distractors selection we employ morpho-syntactic information. Authors working on vocabulary testing [2] use definitions or examples given for the focal term in WordNet in order to produce a non-interrogative stem. [6] also employ WordNet, but only as a tool for distractors selection. Their approach is domain independent; furthermore, the authors report a 6-10 times speed-up in comparison with a manual test elicitation. Similarly, [11] uses a thesaurus in order to find distractors for stem, generated by replacing the verb of the chosen sentence with a blank. [4] apply standard classification methods in order to decide the position in the gap in the generation of fill-in-the-blank (FIB) test items. Other researchers who are actively working in the area include [1], who are focusing on the different types of </context>
<context position="12350" citStr="[6]" startWordPosition="2000" endWordPosition="2000">e have concentrated on extracting nouns, noun phrases of the type np − A − N and names, but now, in order to extend the list of valuable keyterms, we have inserted an additional type of noun phrases: np − N − PP. In domains like Law, where the specific terms tend to be longer, exactly this structure greatly helps in detecting keyterms. After all potential keyterms are extracted they are stemmed and the two lists of terms – the stemmed and the original one – are arranged in the internal representation shown above. As we have determined in previous research and is reported also by other authors [6], in instructional materials the keyterms are often repeated in order to make the learner remember them. That is why simple term frequency is a better measure than TF-IDF, which tends to lower the score of the most often used words. We store the frequencies of our terms (fti) in the occurences attributes for each stem. We sort these frequencies and calculate the number of words having equal frequency (rftp). Then we set the threshold as follows: threshold = maxfti − 2, {rftp &gt; fti} (1) We have established this procedure for threshold setting empirically, by observing manually prepared test ite</context>
</contexts>
<marker>[6]</marker>
<rawString>R. Mitkov, L. A. Ha, and N. Karamis. A computer-aided environment for generating multiple-choice test items. Natural Language Engineering, 12.:177–194, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
</authors>
<title>Bulstem: Design and evaluation of inflectional stemmer for bulgarian.</title>
<date>2003</date>
<booktitle>In Proceedings of Workshop on Balkan Language Resources and Tools (1st Balkan Conference in Informatics,</booktitle>
<location>Thessaloniki, Greece,</location>
<contexts>
<context position="9167" citStr="[7]" startWordPosition="1462" endWordPosition="1462">the training corpus gave highly satisfactory results. After reformatting the parsed text, we extract from it all nouns and noun phrase structures as well as names. From the tools offering fast structure querying for our purposes the most appropriate turned out to be the CLaRK system2. As it is based on Xpath expression querying it is fully configurable. In contrast with the NP extractor Morena we used earlier, CLaRK allows for the manual specification of sequences of constituents. In order to overcome the language inflection, the extracted morpho-syntactic structures are stemmed using BulStem [7] and organised in an internal representation format, where each stem3 maps to all NPs having the same stem. Here is an example of a partial record for the stem saxoH (law): [stem value=”3axoH” occurences=51 type=”N” isKeyterm=false] [instance value=”3axoH”] [instance value=”3axoHHTe”] [instance value=”3axoHa”] In this case, the stem is saxoH (law). It has a total of 51 occurences in the document. Some of them are zaxon (law), zaxonume (the laws), saxoHa (the law) and it is type noun. Other NP types are np-A-N- NP composed of adjective and noun, np-N-PP - NP composed of noun and prepositional p</context>
</contexts>
<marker>[7]</marker>
<rawString>P. Nakov. Bulstem: Design and evaluation of inflectional stemmer for bulgarian. In Proceedings of Workshop on Balkan Language Resources and Tools (1st Balkan Conference in Informatics, Thessaloniki, Greece, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Nikolova</author>
</authors>
<title>Language technologies for instructional resources in bulgarian. In</title>
<date>2008</date>
<booktitle>Proceedings of 13th Student Session at ESSLLI 2008,</booktitle>
<pages>135--142</pages>
<editor>K. Balogh, editor,</editor>
<contexts>
<context position="6488" citStr="[8]" startWordPosition="1023" endWordPosition="1023">in the generation of fill-in-the-blank (FIB) test items. Other researchers who are actively working in the area include [1], who are focusing on the different types of question models with application mainly in the language learning. In our approach we extract sentences which contain the central terms for the given material in Bulgarian and produce FIB type of questions out of them. Along with that, we also suggest the correct answer and distractors. 3 Motivation The fact that we are not familiar with any related work for learning materials in Bulgarian (except for previous work of the author [8]), together with the presence of sophisticated language technologies for Bulgarian, which allow for complex text analysis strongly inspired us to work out the practical potential of our ideas. Moreover, the growing interest in the field, which is due to its significant practical importance, was a motivating factor to concretise our aims and more precisely to apply the developed technology for e-learning purposes. 4 Workbench Outline The system is designed in a way that it accepts instructional material from the test designer in form of raw text and produces draft learning objects - MCTI of FIB</context>
</contexts>
<marker>[8]</marker>
<rawString>I. Nikolova. Language technologies for instructional resources in bulgarian. In K. Balogh, editor, Proceedings of 13th Student Session at ESSLLI 2008, pages 135–142, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="8484" citStr="[9, 10]" startWordPosition="1352" endWordPosition="1353">echnologies, like parsing and automatic term extraction are employed. Additionally, linguistic assumptions are taken into consideration. An overview of the data processing chain is shown in Fig. 2. The input of the test designer is plain text instructional material, which has to be parsed in order to extract lexico-syntactic features from the text. Due to the importance of parsing as a basic source of information used later on for the test items generation, we have picked a statistical parser which reports state-ofthe-art results and has been tuned to work with Bulgarian - the Berkeley parser [9, 10]. The parser was trained on BulTreeBank1. Parsing texts from the same domain as the training corpus gave highly satisfactory results. After reformatting the parsed text, we extract from it all nouns and noun phrase structures as well as names. From the tools offering fast structure querying for our purposes the most appropriate turned out to be the CLaRK system2. As it is based on Xpath expression querying it is fully configurable. In contrast with the NP extractor Morena we used earlier, CLaRK allows for the manual specification of sequences of constituents. In order to overcome the language </context>
</contexts>
<marker>[9]</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="8484" citStr="[9, 10]" startWordPosition="1352" endWordPosition="1353">echnologies, like parsing and automatic term extraction are employed. Additionally, linguistic assumptions are taken into consideration. An overview of the data processing chain is shown in Fig. 2. The input of the test designer is plain text instructional material, which has to be parsed in order to extract lexico-syntactic features from the text. Due to the importance of parsing as a basic source of information used later on for the test items generation, we have picked a statistical parser which reports state-ofthe-art results and has been tuned to work with Bulgarian - the Berkeley parser [9, 10]. The parser was trained on BulTreeBank1. Parsing texts from the same domain as the training corpus gave highly satisfactory results. After reformatting the parsed text, we extract from it all nouns and noun phrase structures as well as names. From the tools offering fast structure querying for our purposes the most appropriate turned out to be the CLaRK system2. As it is based on Xpath expression querying it is fully configurable. In contrast with the NP extractor Morena we used earlier, CLaRK allows for the manual specification of sequences of constituents. In order to overcome the language </context>
</contexts>
<marker>[10]</marker>
<rawString>S. Petrov and D. Klein. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April 2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>F Sugaya</author>
<author>S Yamamoto</author>
</authors>
<title>Measuring nonnative speakers’ proficiency of English by using a test with automatically-generated fill-in-the-blank questions.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>61--68</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3340" citStr="[11, 5]" startWordPosition="528" endWordPosition="529">and helping them to take faster decisions about the topics to be included in a test and concrete questions which could be given to the learners. The generation of multiple-choice questions with the help of natural language processing (NLP) technologies is an active research area in which different tools for text processing are used in order to transform the facts from the instructional materials to questions for students assessment. The items produced in this way are often used in Computer Assisted Language Learning (CALL), for vocabulary [2], grammar [3, 4, 1] or language proficiency testing [11, 5], as well as in comprehension testing in specific subject areas in the native language [6]. Our aim is to produce multiplechoice test items for testing learners achievements especially in the second area - learners comprehension of specified instructional material. We present the design of a workbench for test designers employing language technologies for generation of MCTI (stem, correct answer and distractors), which are to be wrapped as learning objects (LO) and can be loaded in an e-learning environment. The task is divided into three subtasks: automatic keyterm extraction; sentence extrac</context>
<context position="5674" citStr="[11]" startWordPosition="885" endWordPosition="885">or multiple-choice cloze items. Our work differs from these approaches as far as we detect relevant terms automatically, henceforth called keyterms. Furthermore, for distractors selection we employ morpho-syntactic information. Authors working on vocabulary testing [2] use definitions or examples given for the focal term in WordNet in order to produce a non-interrogative stem. [6] also employ WordNet, but only as a tool for distractors selection. Their approach is domain independent; furthermore, the authors report a 6-10 times speed-up in comparison with a manual test elicitation. Similarly, [11] uses a thesaurus in order to find distractors for stem, generated by replacing the verb of the chosen sentence with a blank. [4] apply standard classification methods in order to decide the position in the gap in the generation of fill-in-the-blank (FIB) test items. Other researchers who are actively working in the area include [1], who are focusing on the different types of question models with application mainly in the language learning. In our approach we extract sentences which contain the central terms for the given material in Bulgarian and produce FIB type of questions out of them. Alo</context>
</contexts>
<marker>[11]</marker>
<rawString>E. Sumita, F. Sugaya, and S. Yamamoto. Measuring nonnative speakers’ proficiency of English by using a test with automatically-generated fill-in-the-blank questions. In Proceedings of the Second Workshop on Building Educational Applications Using NLP, pages 61–68, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>