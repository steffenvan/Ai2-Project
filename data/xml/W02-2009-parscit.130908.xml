<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99437">
Cross-dataset Clustering: Revealing Corresponding
Themes Across Multiple Corpora
</title>
<author confidence="0.942199">
Ido DAGAN
</author>
<affiliation confidence="0.763507">
Department of Computer Science
Bar-Ilan University
Ramat-Gan, Israel, 52900
and LingoMotors Inc.
</affiliation>
<email confidence="0.944158">
dagan@lingomotors.com
</email>
<author confidence="0.948716">
Zvika MARX
</author>
<affiliation confidence="0.9852455">
Center for Neural Computation
The Hebrew University
</affiliation>
<address confidence="0.656702">
and CS Dept., Bar-Ilan University
Ramat-Gan, Israel, 52900
</address>
<email confidence="0.98285">
zvim@cs.huji.ac.il
</email>
<author confidence="0.90789">
Eli SHAMIR
</author>
<affiliation confidence="0.935637333333333">
School of Computer Science
and Engineering
The Hebrew University
</affiliation>
<address confidence="0.689018">
Jerusalem, Israel, 91904
</address>
<email confidence="0.954085">
shamir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.984411" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926714285714">
We present a method for identifying
corresponding themes across several corpora
that are focused on related, but distinct,
domains. This task is approached through
simultaneous clustering of keyword sets
extracted from the analyzed corpora. Our
algorithm extends the information-
bottleneck soft clustering method for a
suitable setting consisting of several
datasets. Experimentation with topical
corpora reveals similar aspects of three
distinct religions. The evaluation is by way
of comparison to clusters constructed
manually by an expert.
</bodyText>
<sectionHeader confidence="0.995539" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891879518073">
This paper addresses the problem of detecting
corresponding subtopics, or themes, within
related bodies of text. Such task is typical to
comparative research, whether commercial or
scientific: a conceivable application would aim
at detecting corresponding characteristics
regarding, e.g., companies, markets, legal
systems or political organizations.
Clustering has often been perceived as a mean
for extracting meaningful components from data
(Tishby, Pereira and Bialek, 1999). Regarding
textual data, clusters of words (Pereira, Tishby
and Lee, 1993) or documents (Lee and Seung,
1999; Dhillon and Modha, 2001) are often
interpreted as capturing topics or themes that
play prominent role in the analyzed texts.
Our work extends the &amp;quot;standard&amp;quot; clustering
paradigm, which pertains to a single dataset.
We address a setting in which several datasets,
corresponding to related domains, are given.
We focus on the comparative task of detecting
those themes that are expressed across several
datasets, rather than discovering internal themes
within each individual dataset.
More specifically, we address the task of
clustering simultaneously multiple datasets such
that each cluster includes elements from several
datasets, capturing a common theme, which is
shared across the sets. We term this task cross-
dataset (CD) clustering.
In this article we demonstrate CD clustering
through detecting corresponding themes across
three different religions. That is: we apply our
approach to three sets of religion-related
keywords, extracted from three corpora, which
include encyclopedic entries and introductory
articles regarding Buddhism, Christianity and
Islam. Each one of three representative
keyword-sets, which were extracted from the
above corpora, presumably encapsulates topics
and themes discussed within its source corpus.
Our algorithm succeeds to reveal common
themes such as scriptures, rituals and schools
through respective keyword clusters consisting
of terms such as Sutra, Bible and Quran;
Full ffloon, Easter and Id al [Mtr; Theravada,
Protestant and Shiite (see Table 1 below for a
detailed depiction of our results).
The CD clustering algorithm, presented in
Section 2.2 below, extends the information
bottleneck (IB) soft clustering method. Our
modifications to the IB formulation enable the
clustering algorithm to capture characteristic
patterns that run across different datasets, rather
then being &amp;quot;trapped&amp;quot; by unique characteristics of
individual datasets.
Like other topic discovery tasks that are
approached by clustering, the goal of CD
clustering is not defined in precise terms. Yet, it
is clear that its focus on detecting themes in a
comparative manner, within multiple datasets,
distinguishes CD clustering substantially from
the standard single-dataset clustering paradigm.
A related problem, of detecting analogies
between different information systems has been
addressed in the past within cognitive research
(e.g. Gentner, 1983; Hofstadter et al., 1995).
Recently, a related computational method for
detecting corresponding themes has been
introduced (coupled clustering, Marx et al.,
2002). The coupled clustering setting, however,
being focused on detecting analogies, is limited
to two data sets. Further, it requires similarity
values between pairs of data elements as input:
this setting does not seem straightforwardly
applicable to the multiple dataset setting. Our
method, in distinction, uses a more direct source
of information, namely word co-occurrence
statistics within the analyzed corpora. Another
difference is that we take the &amp;quot;soft&amp;quot; approach to
clustering, producing probabilities of
assignments into clusters rather than a
deterministic 0/1 assignment values.
</bodyText>
<sectionHeader confidence="0.984526" genericHeader="method">
2 Algorithmic Framework
</sectionHeader>
<subsectionHeader confidence="0.999725">
2.1 Review of the IB Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.9918587">
The information bottleneck (IB) iterative
clustering method is a recent approach to soft
(probabilistic) clustering for a single set, denoted
by X, consisting of elements to be clustered
(Tishby, Pereira &amp; Bialek, 1999). Each element
x∈X is identified by a probabilistic feature
vector, with an entry, p(y�x), for every feature y
from a pre-determined set of features Y. The
p(yIx) values are estimated from given co-
occurrence data:
</bodyText>
<equation confidence="0.9403835">
p(y*)
= count(x, y) ∑ y&apos;∈Y count(x, y&apos; )
</equation>
<bodyText confidence="0.910031545454545">
(hence ∑ y∈Y p(y#) = 1 for every x in X).
The IB algorithm is derived from information
theoretic considerations that we do not address
here. It computes, through an iterative EM-like
process, probabilistic assignments p(cIx) for
each element x into each cluster c. Starting with
random (or heuristically chosen) p(c�x) values at
time t = 0, the IB algorithm iterates the
following steps until convergence:
IB1: Calculate for each cluster c its marginal
probability:
</bodyText>
<equation confidence="0.987763">
ptc = ∑ x∈ X p x pt − c x
( ) ( ) 1 ( � ) .
</equation>
<bodyText confidence="0.70727">
IB2: Calculate for each feature y and cluster c
a conditional probability p(yIc):
</bodyText>
<equation confidence="0.986041333333333">
p y c
t ( � ) = ∑x∈X X y I x)ptm (x 10
(Bayes&apos; rule is used to compute p(xlc)).
</equation>
<bodyText confidence="0.997371333333333">
IBI Calculate for each element x and each
cluster c a value p(c�x), indicating the
&amp;quot;probability of assignment&amp;quot; of x into c:
</bodyText>
<equation confidence="0.9991206">
pt#cIx)=p#)simt,β(x,c)
( &apos; ) ( ,
c sim x c
Y , β
t
</equation>
<bodyText confidence="0.997529863636364">
with simt Y,β(x,c) = exp{−βDKL[ p(y�x)�� pt(y�c) ]�
(DKL is the Kullback-Leibler divergence, see
Cover &amp; Thomas, 1991).
The parameter β controls the sensitivity of the
clustering procedure to differences between the
p(y|c) values. The higher β is, the more
&amp;quot;determined&amp;quot; the algorithm becomes in
assigning each element into the closest cluster.
As β is increased, more clusters that are
separable from each other are obtained upon
convergence (the target number of clusters is
fixed). We want to ensure, however, that
assignments do not follow more than necessary
minute details of the given data, as a result of
too high β (similarly to over generalization in
supervised settings). The IB algorithm is
therefore applied repeatedly in a cooling-like
process: it starts with a low β value,
corresponding to low temperature, which is
increased every repetition of the whole iterative
converging cycle, until the desired number of
separate clusters is obtained.
</bodyText>
<subsectionHeader confidence="0.999132">
2.2 The Cross-dataset (CD) Clustering Method
</subsectionHeader>
<bodyText confidence="0.999820571428572">
The (soft) CD clustering algorithm receives as
input multiple datasets along with their feature
vectors. In the current application, we have
three sets extracted from the corresponding
corpora — XBuddhism, XChristianity, and XIslam — each of
~150 keywords to be clustered. A particular
keyword might appear in two or more of the
</bodyText>
<figure confidence="0.750209">
∑
&apos;
pt
c
&apos; ) ,
</figure>
<bodyText confidence="0.986580213114754">
datasets, but the CD setting considers it as a
distinct element within each dataset, thus
keeping the sets of clustered elements disjoint.
Like the IB clustering algorithm, the CD
algorithm produces probabilistic assignments of
the data elements.
The feature set Y consists, in the current work, of
about 7000 content words, each occurs in at least
two of the examined corpora. The set of
features is used commonly for all datasets, thus
it underlies a common representation, which
enables the clustering process to compare
elements of different sets.
Naively approached, the original IB algorithm
could be utilized unaltered to the multiple-
dataset setting, simply by applying it to the
unified set X, consisting of the union of the
disjoint Xi&apos;s. The problem of this simplistic
approach is that each dataset has its own
characteristic features and feature combinations,
which correspond to prominent topics discussed
uniquely in that corpus. A standard clustering
method, such as the IB algorithm, would have a
tendency to cluster together elements that
originate in the same dataset, producing clusters
populated mostly by elements from a single
dataset (cf. Marx et al, 2002). The goal of CD
clustering is to neutralize this tendency and to
create clusters containing elements that share
common features across different datasets.
To accomplish this goal, we change the criterion
by which elements are assigned into clusters.
Recall that the assignment of an element x to a
cluster c is determined by the similarity of their
characterizing feature distributions, p(y|x) and
p(y|c) (step IB3). The problem lies in using the
p(y|c) distribution, which is determined by
summing p(y|x) values over all cluster elements,
to characterize a cluster without taking into
account dataset boundaries. Thus, for a certain
y, p(y|c) might be high despite of being
characteristic only for cluster elements
originating in a single dataset. This results in
the tendency discussed above to favor clusters
consisting of elements of a single dataset.
Therefore, we define a biased probability
distribution, p�c(y),to be used by the CD
clustering algorithm for characterizing a cluster
c. It is designed to call attention to y&apos;s that are
typical for cluster members in all, or most,
different datasets. Consequently, an element x
would be assigned to a cluster c (as in step IB3)
in accordance to the degree of similarity
between its own characteristic features and those
characterizing other cluster members from all
datasets. The resulting clusters would thus
contain representatives of all datasets.
The definition of p�c(y) is based on the joint
probability p(y,c,Xi). First, compute the
geometric mean of p(y,c,Xi) over all Xi, weighted
by p(Xi):
</bodyText>
<equation confidence="0.97975">
ρ(y,c) = ∏ i (p(y,c,Xi)) p(Xi)
</equation>
<bodyText confidence="0.9702732">
(see Appendix below for the details of how
p(Xi) and p(y,c,Xi) are calculated).
ρ is not a probability measure, but just a
function of y and c into [0,1].However, since a
geometric mean reflects &amp;quot;uniformity&amp;quot; of the
averaged values, ρ captures the degree to which
p(y,c,Xi) values are high across all datasets.
We found empirically that at this stage, it is
advantageous to normalize ρ across all clusters
and then to rescale the resulting probabilities
(over the c&apos;s, for each y) by the original p(y):
ρ&apos;(y,c)= ( ρ(y,c)m c&apos; ρ(y,c&apos;) ) X p(y) .
Finally, to obtain a probability distribution over
y for each cluster c, normalize the obtained
ρ&apos;(y,c) over all y&apos;s:
</bodyText>
<equation confidence="0.869485">
p�c(y) = ρ&apos;(y,c)M y&apos; ρ&apos;(y&apos;,c) .
</equation>
<bodyText confidence="0.977449555555556">
As explained, p�c(y) characterizes c (analogously
to p(y|c) in IB), while ensuring that the feature-
based similarity of c to any element x reflects
feature distribution across all data sets.
The CD clustering algorithm, starting at t = 0,
iterates, in correspondence to the IB algorithm,
the following steps:
CD1: Calculate for each cluster c its marginal
probability (same as IB1):
</bodyText>
<equation confidence="0.984763">
pt (c) = ∑x∈U Xi p x p c x
( ) 1 (  |) .
t −
</equation>
<listItem confidence="0.9777">
CD2: Compute p�c(y) as described above.
CD3: Compute p(c|x) (with p�c(y) playing the
role played by p(y|c) in IB3):
</listItem>
<equation confidence="0.98951">
p c SIM x c
Y ,β( , )
t( ) t
pt(c  |x)= Y&apos; )SIMt ,β(x,c
with SIMtY,β(x,c) = exp �−βDKL[ p(y|x) ||c
pt � (y)]�.
</equation>
<table confidence="0.5105455">
∑
&apos;
pt
c
(
c
&apos;
,
)
3 CD Clustering for Religion Comparison
</table>
<bodyText confidence="0.985750090909091">
The three corpora that are focused on the
compared religions — Buddhism, Christianity
and Islam — have been downloaded from the
Internet. Each corpus contains 20,000—40,000
word tokens (5—10 Megabyte). We have used a
text mining tool to extract most religion
keywords that form the three sets to which we
applied the CD algorithm. The software we
have used — TextAnalyst 2.0 — identifies within
the corpora key-phrases, from which we have
excluded items that appear in fewer than three
</bodyText>
<tableCaption confidence="0.991719">
Table 1: Results of religion keyword CD clustering. The authors have set the cluster titles. For each
cluster c and each religion, the 15 keywords x with the highest probability of assignment within the
cluster are displayed (assignment probabilities, i.e. p(c|x❑ values are indicated in brackets). Terms
that were used by the expert (see Table 2) are underlined.
</tableCaption>
<table confidence="0.955715538461539">
Buddhism Christianity Islam
C1 (Cherished Qualities)
god (.68), amida (.58), bodhisattva (.50), god (.69), good works (.65), love of god god (.86), one god (.84), allah (.76),
salvation (.45), enlightenment (.43), deva (.62), salvation (.60), gift (.58), intercession bless (.76), worship (.75), submission
(.43), attain (.41), sacrifice (.39), awaken (.56), repentance (.55), righteousness (.53), (.73), peace (.73), command (.72),
peace (.52), love (.51), obey god (.49), guide (.71), divinity (.70), messanger
(.70), believe (.62), mankind (.61),
commandment (.58), witness (.57)
(.25), spirit (.25), nirvana (.24),
buddha nature (.24), humanity (.22), saviour (.48), atonement (.46), holy ghost
(.45), jesus christ (.45)
speech (.18), teach (.18)
C2 (Customs and Festivals)
</table>
<bodyText confidence="0.5818536">
full moon (.99), stupa (.98), mantra (.96), easter (.99), sunday (.99), christmas (.99), id al fitr (.99), friday (.99), ramadan
pilgrim (.96), monastery (.89), temple (.86), service (.98), city (.98), eucharist (.96), (.99), eid (.99), pilgrim (.99), mosque
statue (.73), worship (.61), monk (.54), pilgrim (.95), pentecost (.93), jerusalem (.99), mecca (.99), kaaba (.99), salat
mandala (.32), trained (.23), bhikkhu (.15), (.91), pray (.89), worship (.82), minister (.73), (.99), fasting (.99), medina (.98), city
disciple (.12), meditation (.11), nun (.11) ministry (.70), read bible (.50), mass (.24) (.98), pray (.98), hijra (.97), charity (.96)
</bodyText>
<subsectionHeader confidence="0.687345">
C3 (Spiritual States)
</subsectionHeader>
<bodyText confidence="0.8812705">
phenomena (.94), problem (.93), moral (.96), problem (.94), argue (.91), moral (.93), spirit (.79), question (.75),
mindfulness (.92), awareness (.92), question (.87), argument (.74), experience life (.71), freedom (.67), existence
consciousness (.91), law (.88), emptiness (.73), incarnation (.72), relationship (.71), (.56), humanity (.53), find (.52), faith
(.88), samadhi (.87), sense (.87), idolatry (.58), find (.45), law (.41), learn (.38), (.52), code (.51), law (.41), universe
experience (.86), wisdom (.83), moral (.83), confession (.34), foundation (.32), faith (.31) (.39), being (.36), teach (.35),
karma (.82), find (.81), exist (.80) commandment (.29)
</bodyText>
<note confidence="0.526249">
C4 (Sorrow, Sin, Punishment and Reward)
</note>
<bodyText confidence="0.982238714285714">
lamentation (.99), grief (.99), animal (.89), punish (.94), hell (.93), violence (.86), fish hell (.97), earth (.88), heaven (.87),
pain (.87), death (.86), kill (.84),
(.86), sin (.83), earth (.81), soul (.78), death death (.85), sin (.85), alcohol (.69),
reincarnation (.81), realm (.76), samsara (.77), sinner (.76), sinful (.74), heaven (.73), satan (.60), face (.59),
(.69), rebirth (.61), dukkha (.56), anger (.53), satan (.72), suffer (.71), flesh (.71), day of judgment (.52), deed (.48),
soul (.43), nirvana (.43), birth (.33) judgment (.67) angel (.25), being (.24), universe (.16),
existence (.13), bearing (.12)
</bodyText>
<subsectionHeader confidence="0.712448">
C5 (Schools, Traditions and their Originating Places)
</subsectionHeader>
<bodyText confidence="0.9994426">
korea (.99), china (.99), tibet (.99), cardinal (.99), orthodox (.99), protestant africa (.99), shiite (.99), sunni (.99),
theravada (.99), school (.99), asia (.99), (.99), university (.99), vatican (.99), catholic shia (.99), west (.99), christianity (.99),
founded (.99), west (.99), sri lanka (.99), (.99), bishop (.99), rome (.99), pope (.99), arab (.99), founded (.98), arabia (.97),
mahayana (.99), india (.99), history (.99), monk (.99), tradition (.99), theology (.99), sufi (.96), history (.96), fiqh (.95),
hindu (.99), japan (.99), study (.99) baptist (.98), church (.98), divinity (.93) scholar (.91), imam (.90), jew (.89)
</bodyText>
<subsectionHeader confidence="0.260022">
C6 (Names, Places, Characters, Narratives)
</subsectionHeader>
<bodyText confidence="0.9995352">
gautama (.96), king (.95), friend (.68), bethlehem (.98), jordan (.97), mary (.95), husband (.99), ismail (.98), father
disciple (.60), birth (.48), hear (.43), ascetic lamb (.90), king (.90), second coming (.81), (.97), son (.95), mother (.94), born
(.41), amida (.40), deva (.33), teach (.19), born (.76), israel (.74), child (.73), elijah (.72), (.92), wife (.92), child (.89), ali (.88),
sacrifice (.15), statue (.14), buddha (.12), baptize (.70), john the baptist (.68), priest musa (.71), isa (.70), ibrahim (.67),
bodhisattva (.12), dharma (.09) (.68), adultery (.65), zion (.61) caliph (.43), tribe (.35), saint (.30)
</bodyText>
<sectionHeader confidence="0.462659" genericHeader="method">
C7 (Scripture)
</sectionHeader>
<bodyText confidence="0.9987386">
tripitaka (.98), sanskrit (.94), translate (.93), hebrew (.99), translate (.99), gospels (.99), translatee (.99), bible (.99), write (.98),
sutra (.85), discourse (.79), pali canon greek (.99), book (.98), new testament book (.97), hadith (.96), sunna (.96),
(.74), story (.66), book (.64), word (.61), write (.98), old testament (.96), passage (.96), quran (.94), word (.93), story (.93),
(.45), buddha (.39), lama (.32), text (.23),
matthew (.95), write (.94), luke (.93), revelation (.88), companion (.80),
muhammad (.80), prophet (.73),
dharma (.21), teacher (.17) apostle (.93), bible (.91), paul (.90),
john (.90) writing (.71), read quran (.46)
corpus documents1. Thus, composite and rare
terms as well as phrases that the software has
inappropriately segmented were filtered out. We
have added to the automatically extracted terms
additional items contributed by a comparative
religion expert (about 15% of the sets were thus
not extracted automatically, but those terms
occur frequently enough to underlie informative
co-occurrence vectors).
The common set of features consists of all
corpus words that occur in at least three different
documents within two or three of the corpora,
excluding a list of common function words. Co-
occurrences were counted within a bi-directional
five-word window, truncated by sentence ends.
The number of clusters produced — seven — was
empirically determined as the maximal number
with relatively large proportion (p(c) &gt; .01) for
all clusters. Trying eight clusters or more, we
obtain clusters of minute size, which apparently
do not reveal additional themes or topics. Table
1 presents, for each cluster c and each religion,
the 15 keywords x with the highest p(cIx) values.
The number 15 has no special meaning other
than providing rich, balanced and displayable
notion of all clusters. The displayed 3×15
keyword subsets are denoted ĉ1...ĉ7.
We gave each cluster a title, reflecting our
(naive) impression of its content. As we
interpret the clusters, they indeed reveal
prominent aspects of religion: rituals (ĉ2),
schools (ĉ5), narratives (ĉ6) and scriptures (ĉ7).
More delicate issues, such as cherished qualities
(ĉ1), spiritual states (ĉ3), suffering and sin (ĉ4)
are reflected as well, in spite of the very
different position taken by the distinct religions
with regard to these issues.
</bodyText>
<subsectionHeader confidence="0.999965">
3.1 Comparison to Expert Data
</subsectionHeader>
<bodyText confidence="0.999409">
We have asked an expert of comparative religion
studies to simulate roughly the CD clustering
task: assigning (freely-chosen) keywords into
corresponding subsets, reflecting prominent
resembling aspects that cut across the three
examined religions. The expert was not asked to
indicate a probability of assignment, but he was
allowed to use the same keyword in more than
</bodyText>
<footnote confidence="0.991769">
1 An evaluation copy of TextAnalyst, by
MicroSystems Ltd., can be downloaded from
http://www.megaputer.com/php/eval.php3
</footnote>
<bodyText confidence="0.9998915">
one cluster. The expert clusters, with the
exclusion of terms that we were not able to
locate in our corpora, are displayed in Table 2.
In addition to our tags — e1...e8 — the expert gave
a title to each cluster.
Although the keyword-clustering task is highly
subjective, there are notable overlapped regions
shared by the expert clusters and ours. Two
expert topics — `Books&apos; (e1) and `Ritual&apos; (e3) —
are clearly captured (by ĉ7 and ĉ2 respectively).
`Society and Politics&apos; (e4) and `Establishments&apos;
(e5) — are both in some correspondence with our
`Schools and Traditions&apos; cluster (ĉ5). On the
other hand, our output fails to capture the
`Mysticism&apos; expert cluster (e6). Further, our
output suggests the `spiritual states&apos; theme (ĉ3)
and distinguishes cherished qualities (ĉ1) from
sin and suffering (ĉ4). Such observations might
make sense but are not covered by the expert.
To quantify the overall level of overlap between
our output and the expert&apos;s, we introduce
suitable versions of recall and precision
measures.
We want the recall measure to reflect the
proportion of expert terms that are captured by
our configuration, provided that an optimal
correspondence between our clusters to the
expert is considered. Hence, for each expert
cluster, ej, we find a particular ĉk with maximal
number of overlapping terms (note that two or
more expert clusters are allowed to be covered
by a single ĉk, to reflect cases where several
related sub-topics are merged within our results).
Denote this maximal number by M(ej):
</bodyText>
<equation confidence="0.783203">
M(ej) = maxk =17 I{x∈ej: x∈ĉk) }I .
</equation>
<bodyText confidence="0.99742625">
Consequently, the recall measure R is defined to
be the sum of the above maximal overlap counts
over all expert clusters, divided by all 131 expert
terms (repetitions in distinct clusters counted):
</bodyText>
<equation confidence="0.987909">
R = ∑j=1...8 M(ej) � 131.
</equation>
<bodyText confidence="0.999956666666667">
To estimate how precise our results are, we are
interested in the relative part of our clusters,
reduced to the expert terms, which has been
assigned to the &amp;quot;right&amp;quot; expert cluster by the
same optimal correspondence. Note that in this
case we do not want to sum up several M values
that are associated with a single ĉk: a single
cluster covering several expert clusters should
be considered as an indication of poor precision.
Furthermore, if we do this, we might recount
some of ĉk�s terms (specifically, keywords that
the expert has included in several clusters; this
might result in precision &gt; 100%). We need
therefore to consider at most one M value per ĉk,
namely the largest one. Define
</bodyText>
<equation confidence="0.842352">
M*(ĉk) = maxj =17 {M(ej): |ĉk ∩ ej |= M(ej)�
(M*(ĉk) = 0 if the set on the right-hand side is
</equation>
<bodyText confidence="0.999622333333333">
empty, i.e. there is no ej that share M(ej)
elements with ĉk). The global precision measure
P is the sum of all M* values, divided by the
number of expert terms appearing among the ĉk�s
(repetitions counted), which are, in the current
case, the 94 underlined terms in Table 1:
</bodyText>
<equation confidence="0.899873">
P = ∑k =17 M*(ĉk) / 94.
</equation>
<bodyText confidence="0.999108947368421">
Our algorithm has achieved the following
values: R = 67/131 = 0.51, P = 58/94 = 0.62.
This is a notable improvement relatively to the
1B algorithm results: R = 42/131 = 0.32and
P = 32/82 = 0.39(random assignment of the
keywords into seven clusters yield average
values R = 0.36, P = 0.33). As we have
expected, three of the clusters produced by the
1B algorithms are populated, with very high
probability, by most keywords of a single
religion. Within these specific religion clusters
as well as the other sparsely populated clusters,
the ranking inducted by the p(c|x) values is not
very informative regarding particular sub-topics.
Thus, the 1B performs the CD clustering task
poorly, even in comparison to random results.
We note that, similarly to our algorithm, the 1B
algorithm produces at most 7 clusters of non-
negligible size. This somewhat supports our
</bodyText>
<tableCaption confidence="0.9416256">
Table 2: The expert cross-dataset clusters. Cluster titles were assigned by the expert. For each expert
cluster, the best fitting automated cross-dataset cluster is indicated on the right-hand side, as well as
the number of relevant expert words it includes. The terms of this best-fit cluster are underlined.
Superscripts indicate indices of the cross-dataset cluster(s), among ĉ1...ĉ7, to which each term
belongs.
</tableCaption>
<table confidence="0.959818821428572">
Buddhism Christianity Islam
e1: Scriptures ĉ7 4 14 (of 19)
sutra7, mantra2, mandala2, koan, new testament7, old testament7, quran7, hadith7, sunna7, sharia,
pali canon7 bible7, apostle7, revelation, john7, muhammad7
paul7, luke7, matthew7
e2: Beliefs and Ideas ĉ4 4 10 (of 25)
nirvana14, four noble truths, resurrection, heaven4, hell4, trinity, prophet7, allah1, one god1,
dharma6,7, dukkha4, buddha second coming6, jesus christ1, five pillars, heaven4, hell4
love of god1, god1, satan4, cross,
nature1, tantra, emptiness3,
reincarnation4
dove4, fish4
e3: Ritual, Prayer, Holydays ĉ2 4 16 (of 20)
meditation2, statue2, sacrifice1,6, sunday2, pray2, confession3, pilgrim2, charity2, ramadan2,
gift, stupa2 eucharist2, christmas2, baptism fasting2, id al fitr2, pray2,
friday2, kaaba2, mecca2
e4: Society and Politics ĉ5 4 9 (of 19)
dalai lama, monk2, bodhisattva1,6, rome5, vatican5, church5, minister2, sharia, caliph6, imam5, shia5,
lama7 sunna7, ali6, sufi5
priest6, cardinal5, pope5, bishop5
e5: Establishments ĉ5 4 6 (of 10)
monastery2, temple2, sangha, church5, cardinal5, pope5, bishop5 mosque2, imam5
school5
e6: Mysticism ĉ2 4 2 (of 11)
meditation2, nirvana14, samadhi3, eucharist2, miracle, crucifixion, sufi5
tantra suffer4, love1, saint
e7: Learning and Education ĉ5 4 4 (of 8)
monastery2, monk2, sutra7, monk5, university5, theology5,
</table>
<page confidence="0.465270333333333">
meditation2
divinity5
e8: Names and Places ĉ6 4 7 (of 20)
</page>
<bodyText confidence="0.975634545454546">
gautama6, buddha6,7 jesus christ1, john the baptist6, muhammad7, ali6, mecca2,
jordan6, jerusalem2, bethlehem6, medina2
mary6, rome5, john7, paul7, luke7,
matthew7, zion6
impression that the limit on number of
&amp;quot;interesting&amp;quot; clusters reflects intrinsic
exhaustion of the information embodied within
the given data. It is yet to be carefully examined
whether this observation provides any hint
regarding the general issue of the &amp;quot;right&amp;quot; number
of clusters.
</bodyText>
<sectionHeader confidence="0.995133" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999973387096774">
This paper addressed the relatively unexplored
problem of detecting corresponding themes
across multiple corpora. We have developed an
extended clustering algorithm that is based on
the appealing and highly general Information
Bottleneck method. Substantial effort has been
devoted to adopting this method for the Cross-
Dataset clustering task.
Our approach was demonstrated empirically on
the challenging task of finding corresponding
themes across different religions. Subjective
examination of the system&apos;s output, as well as its
comparison to the output of a human expert,
demonstrate the potential benefits of applying
this approach in the framework of comparative
research, and possibly in additional text mining
applications.
Given the early stage of this line of research,
there is plenty of room for future work. In
particular, further research is needed to provide
theoretic grounding for the CD clustering
formulations and to specify their properties.
Empirical work is needed to explore the
potential of the proposed paradigm for other
textual domains as well as for related
applications. Particularly, we have recently
presented a similar framework for template
induction in information extraction (cross-
component clustering, Marx, Dagan, &amp; Shamir,
2002), which should be studied in relation to the
CD algorithm presented here.
</bodyText>
<sectionHeader confidence="0.989097" genericHeader="conclusions">
Appendix
</sectionHeader>
<bodyText confidence="0.999881">
The value of p(Xi), which is required for the
calculations in Section 3.2, is given directly
from the input co-occurrence data as follows:
The values pt(c|Xi), pt(y|c,Xi) are calculated from
values that are available at time step t−1:
</bodyText>
<equation confidence="0.8671424">
p c X
t (  |) t −
i = ∑ x∈Xi ( ) 1 (  |),
p x p c
p y c X
(  |, ) = ∑ x∈Xi p y x p x c X
t i t−
(  |) (  |, )
1
i
(pt−1(x|c,Xi) is due to Bayes&apos; rule conditioned on
Xi: pt−1(x|c,Xi) = pt−1(c|x) � p(x) / pt−1(c|Xi)� note
that pt−1(c|x) = pt−1(c|x,Xi) ).
Finally we have:
pt(y,c,Xi) = pt(y|c,Xi) � pt(c|Xi) � p(Xi) .
</equation>
<sectionHeader confidence="0.99131" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999891833333333">
We thank Eitan Reich for providing the expert
data, as well as for illuminating discussions.
This work has been partially supported by
ISRAEL SCIENCE FOUNDATION founded by
The Academy of Sciences and Humanities
(grants 574/98-1 and 489/00).
</bodyText>
<sectionHeader confidence="0.996702" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999511272727273">
Cover, T. M. and Thomas, J. A. (1991). Elements of
Information Theory. New York: John Wiley &amp;
Sons, Inc.
Dhillon I. S. and Modha D. S. (2001). Concept
decompositions for large sparse text data using
clustering. Machine Learning, 42/1, pp. 143175.
Gentner, D. (1983). Structure-mapping: a theoretical
framework for analogy. Cognitive Science, 7, pp.
155170.
Hofstadter, D. R. and the Fluid Analogies Research
Group (1995). Fluid Concepts and Creative
Analogies. New-York: Basic Books, 518 p.
Lee D. D. and Seung H. S. (1999). Learning the
parts of objects by non-negative matrix
factorization. Nature, 401/6755, pp. 788791.
Marx, Z., Dagan, I. and Shamir, E. (2002). Cross-
component clustering for template induction.
Workshop on Text Learning (TextML-2002),
Sydney, Australia.
Marx, Z., Dagan, I., Buhmann, J. M. and Shamir, E.
(2002). Coupled clustering: a method for detecting
structural correspondence. Journal of Machine
Learning Research, accepted for publication.
Pereira, F. C. N., Tishby N. and Lee L. J. (1993).
Distributional clustering of English words. In:
Proceedings of the 31st Annual Meeting of the
Association for Computational Linguistics ACL&apos;
93, Columbus, OH, pp. 183190.
Tishby, N., Pereira, F. C. and Bialek, W. (1999).
The information bottleneck method. In: The 37th
Annual Allerton Conference on Communication,
Control, and Computing, Urbana-Champaign, IL,
pp. 368379.
</reference>
<equation confidence="0.995600125">
p(Xi) = ∑
count x y
( , )
x&apos;∈X y∈Y count(x&apos; , y)
y Y
∈
∑ ∈ i
x X
</equation>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.127754">
<title confidence="0.969719">Cross-dataset Clustering: Revealing Themes Across Multiple Corpora</title>
<author confidence="0.872764">Ido</author>
<affiliation confidence="0.928209">Department of Computer Bar-Ilan</affiliation>
<address confidence="0.960685">Ramat-Gan, Israel,</address>
<email confidence="0.630536">andLingoMotorsdagan@lingomotors.com</email>
<author confidence="0.987414">Zvika MARX</author>
<affiliation confidence="0.999655333333333">Center for Neural Computation The Hebrew University and CS Dept., Bar-Ilan University</affiliation>
<address confidence="0.999883">Ramat-Gan, Israel, 52900</address>
<email confidence="0.992195">zvim@cs.huji.ac.il</email>
<author confidence="0.985048">Eli</author>
<affiliation confidence="0.885867666666667">School of Computer and The Hebrew</affiliation>
<address confidence="0.995835">Jerusalem, Israel,</address>
<email confidence="0.999656">shamir@cs.huji.ac.il</email>
<abstract confidence="0.998805066666667">We present a method for identifying corresponding themes across several corpora that are focused on related, but distinct, domains. This task is approached through simultaneous clustering of keyword sets extracted from the analyzed corpora. Our algorithm extends the informationbottleneck soft clustering method for a suitable setting consisting of several datasets. Experimentation with corpora reveals similar aspects of three distinct religions. The evaluation is by way of comparison to clusters constructed manually by an expert.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<location>New York:</location>
<contexts>
<context position="6256" citStr="Cover &amp; Thomas, 1991" startWordPosition="928" endWordPosition="931"> IB algorithm iterates the following steps until convergence: IB1: Calculate for each cluster c its marginal probability: ptc = ∑ x∈ X p x pt − c x ( ) ( ) 1 ( � ) . IB2: Calculate for each feature y and cluster c a conditional probability p(yIc): p y c t ( � ) = ∑x∈X X y I x)ptm (x 10 (Bayes&apos; rule is used to compute p(xlc)). IBI Calculate for each element x and each cluster c a value p(c�x), indicating the &amp;quot;probability of assignment&amp;quot; of x into c: pt#cIx)=p#)simt,β(x,c) ( &apos; ) ( , c sim x c Y , β t with simt Y,β(x,c) = exp{−βDKL[ p(y�x)�� pt(y�c) ]� (DKL is the Kullback-Leibler divergence, see Cover &amp; Thomas, 1991). The parameter β controls the sensitivity of the clustering procedure to differences between the p(y|c) values. The higher β is, the more &amp;quot;determined&amp;quot; the algorithm becomes in assigning each element into the closest cluster. As β is increased, more clusters that are separable from each other are obtained upon convergence (the target number of clusters is fixed). We want to ensure, however, that assignments do not follow more than necessary minute details of the given data, as a result of too high β (similarly to over generalization in supervised settings). The IB algorithm is therefore applie</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. M. and Thomas, J. A. (1991). Elements of Information Theory. New York: John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
<author>D S Modha</author>
</authors>
<title>Concept decompositions for large sparse text data using clustering.</title>
<date>2001</date>
<booktitle>Machine Learning, 42/1,</booktitle>
<pages>143--175</pages>
<contexts>
<context position="1653" citStr="Dhillon and Modha, 2001" startWordPosition="216" endWordPosition="219">troduction This paper addresses the problem of detecting corresponding subtopics, or themes, within related bodies of text. Such task is typical to comparative research, whether commercial or scientific: a conceivable application would aim at detecting corresponding characteristics regarding, e.g., companies, markets, legal systems or political organizations. Clustering has often been perceived as a mean for extracting meaningful components from data (Tishby, Pereira and Bialek, 1999). Regarding textual data, clusters of words (Pereira, Tishby and Lee, 1993) or documents (Lee and Seung, 1999; Dhillon and Modha, 2001) are often interpreted as capturing topics or themes that play prominent role in the analyzed texts. Our work extends the &amp;quot;standard&amp;quot; clustering paradigm, which pertains to a single dataset. We address a setting in which several datasets, corresponding to related domains, are given. We focus on the comparative task of detecting those themes that are expressed across several datasets, rather than discovering internal themes within each individual dataset. More specifically, we address the task of clustering simultaneously multiple datasets such that each cluster includes elements from several da</context>
</contexts>
<marker>Dhillon, Modha, 2001</marker>
<rawString>Dhillon I. S. and Modha D. S. (2001). Concept decompositions for large sparse text data using clustering. Machine Learning, 42/1, pp. 143175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gentner</author>
</authors>
<title>Structure-mapping: a theoretical framework for analogy.</title>
<date>1983</date>
<journal>Cognitive Science,</journal>
<volume>7</volume>
<pages>155--170</pages>
<contexts>
<context position="3999" citStr="Gentner, 1983" startWordPosition="554" endWordPosition="555">eristic patterns that run across different datasets, rather then being &amp;quot;trapped&amp;quot; by unique characteristics of individual datasets. Like other topic discovery tasks that are approached by clustering, the goal of CD clustering is not defined in precise terms. Yet, it is clear that its focus on detecting themes in a comparative manner, within multiple datasets, distinguishes CD clustering substantially from the standard single-dataset clustering paradigm. A related problem, of detecting analogies between different information systems has been addressed in the past within cognitive research (e.g. Gentner, 1983; Hofstadter et al., 1995). Recently, a related computational method for detecting corresponding themes has been introduced (coupled clustering, Marx et al., 2002). The coupled clustering setting, however, being focused on detecting analogies, is limited to two data sets. Further, it requires similarity values between pairs of data elements as input: this setting does not seem straightforwardly applicable to the multiple dataset setting. Our method, in distinction, uses a more direct source of information, namely word co-occurrence statistics within the analyzed corpora. Another difference is </context>
</contexts>
<marker>Gentner, 1983</marker>
<rawString>Gentner, D. (1983). Structure-mapping: a theoretical framework for analogy. Cognitive Science, 7, pp. 155170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Hofstadter</author>
</authors>
<title>and the Fluid Analogies Research Group</title>
<date>1995</date>
<volume>518</volume>
<pages>p.</pages>
<marker>Hofstadter, 1995</marker>
<rawString>Hofstadter, D. R. and the Fluid Analogies Research Group (1995). Fluid Concepts and Creative Analogies. New-York: Basic Books, 518 p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lee</author>
<author>H S Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<volume>401</volume>
<pages>788--791</pages>
<contexts>
<context position="1627" citStr="Lee and Seung, 1999" startWordPosition="212" endWordPosition="215">ly by an expert. 1 Introduction This paper addresses the problem of detecting corresponding subtopics, or themes, within related bodies of text. Such task is typical to comparative research, whether commercial or scientific: a conceivable application would aim at detecting corresponding characteristics regarding, e.g., companies, markets, legal systems or political organizations. Clustering has often been perceived as a mean for extracting meaningful components from data (Tishby, Pereira and Bialek, 1999). Regarding textual data, clusters of words (Pereira, Tishby and Lee, 1993) or documents (Lee and Seung, 1999; Dhillon and Modha, 2001) are often interpreted as capturing topics or themes that play prominent role in the analyzed texts. Our work extends the &amp;quot;standard&amp;quot; clustering paradigm, which pertains to a single dataset. We address a setting in which several datasets, corresponding to related domains, are given. We focus on the comparative task of detecting those themes that are expressed across several datasets, rather than discovering internal themes within each individual dataset. More specifically, we address the task of clustering simultaneously multiple datasets such that each cluster include</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Lee D. D. and Seung H. S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401/6755, pp. 788791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Marx</author>
<author>I Dagan</author>
<author>E Shamir</author>
</authors>
<title>Crosscomponent clustering for template induction.</title>
<date>2002</date>
<booktitle>Workshop on Text Learning (TextML-2002),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="26809" citStr="Marx, Dagan, &amp; Shamir, 2002" startWordPosition="4123" endWordPosition="4127">n the framework of comparative research, and possibly in additional text mining applications. Given the early stage of this line of research, there is plenty of room for future work. In particular, further research is needed to provide theoretic grounding for the CD clustering formulations and to specify their properties. Empirical work is needed to explore the potential of the proposed paradigm for other textual domains as well as for related applications. Particularly, we have recently presented a similar framework for template induction in information extraction (crosscomponent clustering, Marx, Dagan, &amp; Shamir, 2002), which should be studied in relation to the CD algorithm presented here. Appendix The value of p(Xi), which is required for the calculations in Section 3.2, is given directly from the input co-occurrence data as follows: The values pt(c|Xi), pt(y|c,Xi) are calculated from values that are available at time step t−1: p c X t ( |) t − i = ∑ x∈Xi ( ) 1 ( |), p x p c p y c X ( |, ) = ∑ x∈Xi p y x p x c X t i t− ( |) ( |, ) 1 i (pt−1(x|c,Xi) is due to Bayes&apos; rule conditioned on Xi: pt−1(x|c,Xi) = pt−1(c|x) � p(x) / pt−1(c|Xi)� note that pt−1(c|x) = pt−1(c|x,Xi) ). Finally we have: pt(y,c,Xi) = pt(</context>
<context position="4162" citStr="Marx et al., 2002" startWordPosition="574" endWordPosition="577">tasks that are approached by clustering, the goal of CD clustering is not defined in precise terms. Yet, it is clear that its focus on detecting themes in a comparative manner, within multiple datasets, distinguishes CD clustering substantially from the standard single-dataset clustering paradigm. A related problem, of detecting analogies between different information systems has been addressed in the past within cognitive research (e.g. Gentner, 1983; Hofstadter et al., 1995). Recently, a related computational method for detecting corresponding themes has been introduced (coupled clustering, Marx et al., 2002). The coupled clustering setting, however, being focused on detecting analogies, is limited to two data sets. Further, it requires similarity values between pairs of data elements as input: this setting does not seem straightforwardly applicable to the multiple dataset setting. Our method, in distinction, uses a more direct source of information, namely word co-occurrence statistics within the analyzed corpora. Another difference is that we take the &amp;quot;soft&amp;quot; approach to clustering, producing probabilities of assignments into clusters rather than a deterministic 0/1 assignment values. 2 Algorithm</context>
<context position="8663" citStr="Marx et al, 2002" startWordPosition="1311" endWordPosition="1314">pproached, the original IB algorithm could be utilized unaltered to the multipledataset setting, simply by applying it to the unified set X, consisting of the union of the disjoint Xi&apos;s. The problem of this simplistic approach is that each dataset has its own characteristic features and feature combinations, which correspond to prominent topics discussed uniquely in that corpus. A standard clustering method, such as the IB algorithm, would have a tendency to cluster together elements that originate in the same dataset, producing clusters populated mostly by elements from a single dataset (cf. Marx et al, 2002). The goal of CD clustering is to neutralize this tendency and to create clusters containing elements that share common features across different datasets. To accomplish this goal, we change the criterion by which elements are assigned into clusters. Recall that the assignment of an element x to a cluster c is determined by the similarity of their characterizing feature distributions, p(y|x) and p(y|c) (step IB3). The problem lies in using the p(y|c) distribution, which is determined by summing p(y|x) values over all cluster elements, to characterize a cluster without taking into account datas</context>
</contexts>
<marker>Marx, Dagan, Shamir, 2002</marker>
<rawString>Marx, Z., Dagan, I. and Shamir, E. (2002). Crosscomponent clustering for template induction. Workshop on Text Learning (TextML-2002), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Marx</author>
<author>I Dagan</author>
<author>J M Buhmann</author>
<author>E Shamir</author>
</authors>
<title>Coupled clustering: a method for detecting structural correspondence.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<note>accepted for publication.</note>
<contexts>
<context position="4162" citStr="Marx et al., 2002" startWordPosition="574" endWordPosition="577">tasks that are approached by clustering, the goal of CD clustering is not defined in precise terms. Yet, it is clear that its focus on detecting themes in a comparative manner, within multiple datasets, distinguishes CD clustering substantially from the standard single-dataset clustering paradigm. A related problem, of detecting analogies between different information systems has been addressed in the past within cognitive research (e.g. Gentner, 1983; Hofstadter et al., 1995). Recently, a related computational method for detecting corresponding themes has been introduced (coupled clustering, Marx et al., 2002). The coupled clustering setting, however, being focused on detecting analogies, is limited to two data sets. Further, it requires similarity values between pairs of data elements as input: this setting does not seem straightforwardly applicable to the multiple dataset setting. Our method, in distinction, uses a more direct source of information, namely word co-occurrence statistics within the analyzed corpora. Another difference is that we take the &amp;quot;soft&amp;quot; approach to clustering, producing probabilities of assignments into clusters rather than a deterministic 0/1 assignment values. 2 Algorithm</context>
<context position="8663" citStr="Marx et al, 2002" startWordPosition="1311" endWordPosition="1314">pproached, the original IB algorithm could be utilized unaltered to the multipledataset setting, simply by applying it to the unified set X, consisting of the union of the disjoint Xi&apos;s. The problem of this simplistic approach is that each dataset has its own characteristic features and feature combinations, which correspond to prominent topics discussed uniquely in that corpus. A standard clustering method, such as the IB algorithm, would have a tendency to cluster together elements that originate in the same dataset, producing clusters populated mostly by elements from a single dataset (cf. Marx et al, 2002). The goal of CD clustering is to neutralize this tendency and to create clusters containing elements that share common features across different datasets. To accomplish this goal, we change the criterion by which elements are assigned into clusters. Recall that the assignment of an element x to a cluster c is determined by the similarity of their characterizing feature distributions, p(y|x) and p(y|c) (step IB3). The problem lies in using the p(y|c) distribution, which is determined by summing p(y|x) values over all cluster elements, to characterize a cluster without taking into account datas</context>
</contexts>
<marker>Marx, Dagan, Buhmann, Shamir, 2002</marker>
<rawString>Marx, Z., Dagan, I., Buhmann, J. M. and Shamir, E. (2002). Coupled clustering: a method for detecting structural correspondence. Journal of Machine Learning Research, accepted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>N Tishby</author>
<author>L J Lee</author>
</authors>
<title>Distributional clustering of English words. In:</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics ACL&apos; 93,</booktitle>
<pages>183--190</pages>
<location>Columbus, OH,</location>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, F. C. N., Tishby N. and Lee L. J. (1993). Distributional clustering of English words. In: Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics ACL&apos; 93, Columbus, OH, pp. 183190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tishby</author>
<author>F C Pereira</author>
<author>W Bialek</author>
</authors>
<title>The information bottleneck method. In:</title>
<date>1999</date>
<booktitle>The 37th Annual Allerton Conference on Communication, Control, and Computing, Urbana-Champaign, IL,</booktitle>
<pages>368--379</pages>
<marker>Tishby, Pereira, Bialek, 1999</marker>
<rawString>Tishby, N., Pereira, F. C. and Bialek, W. (1999). The information bottleneck method. In: The 37th Annual Allerton Conference on Communication, Control, and Computing, Urbana-Champaign, IL, pp. 368379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>