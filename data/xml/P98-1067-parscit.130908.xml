<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001466">
<title confidence="0.993847">
Toward General-Purpose Learning for Information Extraction
</title>
<author confidence="0.9978">
Dayne Freitag
</author>
<affiliation confidence="0.939855">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.80868">
dayneOcs.cmu.edu
</email>
<sectionHeader confidence="0.979064" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981">
Two trends are evident in the recent evolution of
the field of information extraction: a preference
for simple, often corpus-driven techniques over
linguistically sophisticated ones; and a broaden-
ing of the central problem definition to include
many non-traditional text domains. This devel-
opment calls for information extraction systems
which are as retargetable and general as possi-
ble. Here, we describe SRV, a learning archi-
tecture for information extraction which is de-
signed for maximum generality and flexibility.
SRV can exploit domain-specific information,
including linguistic syntax and lexical informa-
tion, in the form of features provided to the sys-
tem explicitly as input for training. This pro-
cess is illustrated using a domain created from
Reuters corporate acquisitions articles. Fea-
tures are derived from two general-purpose NLP
systems, Sleator and Temperly&apos;s link grammar
parser and Wordnet. Experiments compare the
learner&apos;s performance with and without such
linguistic information. Surprisingly, in many
cases, the system performs as well without this
information as with it.
</bodyText>
<sectionHeader confidence="0.996397" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999477490196079">
The field of information extraction (IE) is con-
cerned with using natural language processing
(NLP) to extract essential details from text doc-
uments automatically. While the problems of
retrieval, routing, and filtering have received
considerable attention through the years, IE is
only now coming into its own as an information
management sub-discipline.
Progress in the field of IE has been away from
general NLP systems, that must be tuned to
work in a particular domain, toward faster sys-
tems that perform less linguistic processing of
documents and can be more readily targeted at
novel domains (e.g., (Appelt et al., 1993)). A
natural part of this development has been the
introduction of machine learning techniques to
facilitate the domain engineering effort (Riloff,
1996; Soderland and Lehnert, 1994).
Several researchers have reported IE systems
which use machine learning at their core (Soder-
land, 1996; Calif and Mooney, 1997). Rather
than spend human effort tuning a system for an
IE domain, it becomes possible to conceive of
training it on a document sample. Aside from
the obvious savings in human development ef-
fort, this has significant implications for infor-
mation extraction as a discipline:
Retargetability Moving to a novel domain
should no longer be a question of code mod-
ification; at most some feature engineering
should be required.
Generality It should be possible to handle a
much wider range of domains than previ-
ously. In addition to domains characterized
by grammatical prose, we should be able to
perform information extraction in domains
involving less traditional structure, such as
netnews articles and Web pages.
In this paper we describe a learning algorithm
similar in spirit to FOIL (Quinlan, 1990), which
takes as input a set of tagged documents, and a
set of features that control generalization, and
produces rules that describe how to extract in-
formation from novel documents. For this sys-
tem, introducing linguistic or any other infor-
mation particular to a domain is an exercise in
feature definition, separate from the central al-
gorithm, which is constant. We describe a set of
experiments, involving a document collection of
newswire articles, in which this learner is com-
pared with simpler learning algorithms.
</bodyText>
<page confidence="0.997904">
404
</page>
<sectionHeader confidence="0.928902" genericHeader="introduction">
2 SRV
</sectionHeader>
<bodyText confidence="0.986771714285714">
In order to be suitable for the widest possible
variety of textual domains, including collections
made up of informal E-mail messages, World
Wide Web pages, or netnews posts, a learner
must avoid any assumptions about the struc-
ture of documents that might be invalidated by
new domains. It is not safe to assume, for ex-
ample, that text will be grammatical, or that all
tokens encountered will have entries in a lexicon
available to the system. Fundamentally, a doc-
ument is simply a sequence of terms. Beyond
this, it becomes difficult to make assumptions
that are not violated by some common and im-
portant domain of interest.
At the same time, however, when structural
assumptions are justified, they may be criti-
cal to the success of the system. It should be
possible, therefore, to make structural informa-
tion available to the learner as input for train-
ing. The machine learning method with which
we experiment here. SRV, was designed with
these considerations in mind. In experiments re-
ported elsewhere, we have applied SRV to collec-
tions of electronic seminar announcements and
World Wide Web pages (Freitag, 1998). Read-
ers interested in a more thorough description of
SRV are referred to (Freitag, 1998). Here, we
list its most salient characteristics:
</bodyText>
<listItem confidence="0.9989750625">
• Lack of structural assumptions. SRV
assumes nothing about the structure of a
field instance&apos; or the text in which it is
embedded—only that an instance is an un-
broken fragment of text. During learning
and prediction, SRV inspects every frag-
ment of appropriate size.
• Token-oriented features. Learning is
guided by a feature set which is separate
from the core algorithm. Features de-
scribe aspects of individual tokens, such as
capitalized, numeric, noun. Rules can posit
feature values for individual tokens, or for
all tokens in a fragment, and can constrain
the ordering and positioning of tokens.
• Relational features. SRV also includes
</listItem>
<bodyText confidence="0.9992789">
&apos;We use the terms field and field instance for the
rather generic IE concepts of slot and slot filler. For a
newswire article about a corporate acquisition, for exam-
ple, a field instance might be the text fragment listing
the amount paid as part of the deal.
a notion of relational features, such as
next-token, which map a given token to an-
other token in its environment. SRV uses
such features to explore the context of frag-
ments under investigation.
</bodyText>
<listItem confidence="0.975213533333333">
• Top-down greedy rule search. SRV
constructs rules from general to specific,
as in FOIL (Quinlan, 1990). Top-down
search is more sensitive to patterns in the
data, and less dependent on heuristics,
than the bottom-up search used by sim-
ilar systems (Soderland, 1996; Calif and
Mooney, 1997).
• Rule validation. Training is followed by
validation, in which individual rules are
tested on a reserved portion of the train-
ing documents. Statistics collected in this
way are used to associate a confidence with
each prediction, which are used to manip-
ulate the accuracy-coverage trade-off.
</listItem>
<sectionHeader confidence="0.765724" genericHeader="method">
3 Case Study
</sectionHeader>
<bodyText confidence="0.999792142857143">
SRV&apos;s default feature set, designed for informal
domains where parsing is difficult, includes no
features more sophisticated than those immedi-
ately computable from a cursory inspection of
tokens. The experiments described here were
an exercise in the design of features to capture
syntactic and lexical information.
</bodyText>
<subsectionHeader confidence="0.978997">
3.1 Domain
</subsectionHeader>
<bodyText confidence="0.9999633125">
As part of these experiments we defined an in-
formation extraction problem using a publicly
available corpus. 600 articles were sampled
from the &amp;quot;acquisition&amp;quot; set in the Reuters corpus
(Lewis, 1992) and tagged to identify instances
of nine fields. Fields include those for the official
names of the parties to an acquisition (acquired,
purchaser, seller), as well as their short names
(acqabr, purchabr, sellerabr), the location of the
purchased company or resource (acqloc), the
price paid (dlramt), and any short phrases sum-
marizing the progress of negotiations (status).
The fields vary widely in length and frequency
of occurrence, both of which have a significant
impact on the difficulty they present for learn-
ers.
</bodyText>
<subsectionHeader confidence="0.997427">
3.2 Feature Set Design
</subsectionHeader>
<bodyText confidence="0.999941">
We augmented SRV&apos;s default feature set with
features derived using two publicly available
</bodyText>
<page confidence="0.994212">
405
</page>
<figure confidence="0.990051">
+---G---+---G---+--Ss-+-Ce-+Ss*b+
I I I I I I
First Wisconsin Corp said.v it plans.v
</figure>
<figureCaption confidence="0.991581">
Figure 1: An example of link grammar feature
derivation.
</figureCaption>
<bodyText confidence="0.989825138888889">
NLP tools, the link grammar parser and Word-
net.
The link grammar parser takes a sentence as
input and returns a complete parse in which
terms are connected in typed binary relations
(&amp;quot;links&amp;quot;) which represent syntactic relationships
(Sleator and Temperley, 1993). We mapped
these links to relational features: A token on
the right side of a link of type X has a cor-
responding relational feature called left_X that
maps to the token on the left side of the link. In
addition, several non-relational features, such as
part of speech, are derived from parser output.
Figure 1 shows part of a link grammar parse
and its translation into features.
Our object in using Wordnet (Miller, 1995)
is to enable SRV to recognize that the phrases,
&amp;quot;A bought B,&amp;quot; and, &amp;quot;X acquired Y,&amp;quot; are in-
stantiations of the same underlying pattern. Al-
though &amp;quot;bought&amp;quot; and &amp;quot;acquired&amp;quot; do not belong
to the same &amp;quot;synset&amp;quot; in Wordnet, they are nev-
ertheless closely related in Wordnet by means
of the &amp;quot;hypernym&amp;quot; (or &amp;quot;is-a&amp;quot;) relation. To ex-
ploit such semantic relationships we created a
single token feature, called wn_word. In con-
trast with features already outlined, which are
mostly boolean, this feature is set-valued. For
nouns and verbs, its value is a set of identifiers
representing all synsets in the hypernym path to
the root of the hypernym tree in which a word
occurs. For adjectives and adverbs, these synset
identifiers were drawn from the cluster of closely
related synsets. In the case of multiple Word-
net senses, we used the most common sense of
a word, according to Wordnet, to construct this
set.
</bodyText>
<subsectionHeader confidence="0.968142">
3.3 Competing Learners
</subsectionHeader>
<bodyText confidence="0.9998992">
We compare the performance of SRV with that
of two simple learning approaches, which make
predictions based on raw term statistics. Rote
(see (Freitag, 1998)), memorizes field instances
seen during training and only makes predic-
tions when the same fragments are encountered
in novel documents. Bayes is a statistical ap-
proach based on the &amp;quot;Naive Bayes&amp;quot; algorithm
(Mitchell, 1997). Our implementation is de-
scribed in (Freitag, 1997). Note that although
these learners are &amp;quot;simple,&amp;quot; they are not neces-
sarily ineffective. We have experimented with
them in several domains and have been sur-
prised by their level of performance in some
cases.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999945291666667">
The results presented here represent average
performances over several separate experiments.
In each experiment, the 600 documents in the
collection were randomly partitioned into two
sets of 300 documents each. One of the two
subsets was then used to train each of the learn-
ers, the other to measure the performance of the
learned extractors.
We compared four learners: each of the two
simple learners, Bayes and Rote, and SRV with
two different feature sets, its default feature set,
which contains no &amp;quot;sophisticated&amp;quot; features, and
the default set augmented with the features de-
rived from the link grammar parser and Word-
net. We will refer to the latter as SRV+Iing.
Results are reported in terms of two metrics
closely related to precision and recall, as seen in
information retrieval: Accuracy, the percentage
of documents for which a learner predicted cor-
rectly (extracted the field in question) over all
documents for which the learner predicted; and
coverage, the percentage of documents having
the field in question for which a learner made
some prediction.
</bodyText>
<subsectionHeader confidence="0.673746">
4.1 Performance
</subsectionHeader>
<bodyText confidence="0.994883375">
Table 1 shows the results of a ten-fold exper-
iment comparing all four learners on all nine
fields. Note that accuracy and coverage must
be considered together when comparing learn-
ers. For example, Rote often achieves reasonable
accuracy at very low coverage.
Table 2 shows the results of a three-fold ex-
periment, comparing all learners at fixed cover-
</bodyText>
<figure confidence="0.999204266666667">
token: Corp
lg_tag: nil
lg_pos: noun
left_G
right_S
token: said
lg_tag: &amp;quot;v&amp;quot;
lg_pos: verb
left_S
right_C
token: it
lg_tag: nil
lg_pos: noun
left _C
righ_S
</figure>
<page confidence="0.997071">
406
</page>
<table confidence="0.99847075">
Alg Acc Coy Acc Coy Acc Coy
acquired purchaser seller
Rote 59.6 18.5 43.2 23.2 38.5 15.2
Bayes 19.8 100 36.9 100 15.6 100
SRV 38.4 96.6 42.9 97.9 16.3 86.4
SRVIng 38.0 95.6 42.4 96.3 16.4 82.7
acqabr purchabr sellerabr
Rote 16.1 42.5 3.6 41.9 2.7 27.3
Bayes 23.2 100 39.6 100 16.0 100
SRV 31.8 99.8 41.4 99.6 14.3 95.1
SRVIng 35.5 99.2 43.2 99.3 14.7 91.8
acqloc status dlramt
Rote 6.4 63.1 42.0 94.5 63.2 48.5
Bayes 7.0 100 33.3 100 24.1 100
SRV 12.7 83.7 39.1 89.8 50.5 91.0
SRVIng 15.4 80.2 41.5 87.9 52.1 89.4
</table>
<tableCaption confidence="0.9199575">
Table 1: Accuracy and coverage for all four
learners on the acquisitions fields.
</tableCaption>
<bodyText confidence="0.999665625">
age levels, 20% and 80%, on four fields which
we considered representative of the wide range
of behavior we observed. In addition, in order to
assess the contribution of each kind of linguis-
tic information (syntactic and lexical) to SRV&apos;s
performance, we ran experiments in which its
basic feature set was augmented with only one
type or the other.
</bodyText>
<subsectionHeader confidence="0.995056">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.99997725">
Perhaps surprisingly, but consistent with results
we have obtained in other domains, there is no
one algorithm which outperforms the others on
all fields. Rather than the absolute difficulty of
a field, we speak of the suitability of a learner&apos;s
inductive bias for a field (Mitchell, 1997). Bayes
is clearly better than SRV on the seller and
sellerabr fields at all points on the accuracy-
coverage curve. We suspect this may be due, in
part, to the relative infrequency of these fields
in the data.
The one field for which the linguistic features
offer benefit at all points along the accuracy-
coverage curve is acqabr.2 We surmise that two
factors contribute to this success: a high fre-
quency of occurrence for this field (2.42 times
</bodyText>
<footnote confidence="0.5761365">
2The acqabr differences in Table 2 (a 3-split exper-
iment) are not significant at the 95% confidence level.
However, the full 10-split averages, with 95% error mar-
gins, are: at 20% coverage, 61.5±4.4 for SRV and
68.5±4.2 for SRV-Fling; at 80% coverage, 37.1±2.0 for
SRV and 42.4±2.1 for SRV-I-ling.
</footnote>
<table confidence="0.999598090909091">
Field 80% 20% 80% 20% 80% 20%
Rote Bayes SRV
purchaser - 50.3 40.6 55.9 45.3 55.7
acqabr - 24.4 29.3 50.6 40.0 63.4
dlramt - 69.5 45.9 71.4 57.1 66.7
status 46.7 65.3 39.4 62.1 43.8 72.5
SRV+Iing srv+Ig srv-Fwn
purchaser 48.5 56.3 46.3 63.5 46.7 58.1
acqabr 44.3 75.4 40.4 71.4 41.9 72.5
dlramt 57.1 61.9 55.4 67.3 52.6 67.4
status 43.3 72.6 38.8 74.8 42.2 74.1
</table>
<tableCaption confidence="0.9970245">
Table 2: Accuracy from a three-split experiment
at fixed coverage levels.
</tableCaption>
<equation confidence="0.625225">
A fragment is a acqabr, if:
it contains exactly one token;
the token (T) is capitalized;
T is followed by a lower-case token;
T is preceded by a lower-case token;
</equation>
<bodyText confidence="0.894336875">
T has a right AN-link to a token (U)
with wn_word value &amp;quot;possession&amp;quot;;
U is preceded by a token
with wn_word value &amp;quot;stock&amp;quot;;
and the token two tokens before T
is not a two-character token.
to purchase 4.5 mmn Trilogy common shares at
acquire another 2.4 mmn
</bodyText>
<figureCaption confidence="0.7860034">
Figure 2: A learned rule for acqabr using linguis-
tic features, along with two fragments of match-
ing text. The AN-link connects a noun modifier
to the noun it modifies (to &amp;quot;shares&amp;quot; in both ex-
amples).
</figureCaption>
<bodyText confidence="0.996791384615385">
per document on average), and consistent oc-
currence in a linguistically rich context.
Figure 2 shows a SRVA-ling rule that is able
to exploit both types of linguistic informa-
tion. The Wordnet synsets for &amp;quot;possession&amp;quot; and
&amp;quot;stock&amp;quot; come from the same branch in a hy-
pernym tree-&amp;quot;possession&amp;quot; is a generalization
of &amp;quot;stock&amp;quot;3-and both match the collocations
&amp;quot;common shares&amp;quot; and &amp;quot;treasury shares.&amp;quot; That
the paths [right_ANI] and [right_AN prev_tok]
both connect to the same synset indicates the
presence of a two-word Wordnet collocation.
It is natural to ask why SRV-i-ling does not
</bodyText>
<footnote confidence="0.8205035">
3SRV, with its general-to-specific search bias, often
employs Wordnet this way-first more general synsets,
followed by specializations of the same concept.
Roach treasury shares
</footnote>
<page confidence="0.987313">
407
</page>
<bodyText confidence="0.9790625">
outperform SRV more consistently. After all,
the features available to SRV+Iing are a superset
of those available to SRV. As we see it, there are
two basic explanations:
• Noise. Heuristic choices made in handling
syntactically intractable sentences and in
disambiguating Wordnet word senses in-
troduced noise into the linguistic features.
The combination of noisy features and a
very flexible learner may have led to over-
fitting that offset any advantages the lin-
guistic features provided.
</bodyText>
<listItem confidence="0.6574825">
• Cheap features equally effective. The
simple features may have provided most
</listItem>
<bodyText confidence="0.99625475">
of the necessary information. For exam-
ple, generalizing &amp;quot;acquired&amp;quot; and &amp;quot;bought&amp;quot;
is only useful in the absence of enough data
to form rules for each verb separately.
</bodyText>
<subsectionHeader confidence="0.956291">
4.3 Conclusion
</subsectionHeader>
<bodyText confidence="0.999997523809524">
More than similar systems, SRV satisfies the cri-
teria of generality and retargetability. The sep-
aration of domain-specific information from the
central algorithm, in the form of an extensible
feature set, allows quick porting to novel do-
mains.
Here, we have sketched this porting process.
Surprisingly, although there is preliminary evi-
dence that general-purpose linguistic informa-
tion can provide benefit in some cases, most
of the extraction performance can be achieved
with only the simplest of information.
Obviously, the learners described here are
not intended to solve the information extraction
problem outright, but to serve as a source of in-
formation for a post-processing component that
will reconcile all of the predictions for a docu-
ment, hopefully filling whole templates more ac-
curately than is possible with any single learner.
How this might be accomplished is one theme
of our future work in this area.
</bodyText>
<sectionHeader confidence="0.995315" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989105">
Part of this research was conducted as part of
a summer internship at Just Research. And it
was supported in part by the Darpa HPKB pro-
gram under contract F30602-97-1-0215.
</bodyText>
<sectionHeader confidence="0.993969" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998818244897959">
Douglas E. Appelt, Jerry R. Hobbs, John Bear,
David Israel, and Mabry Tyson. 1993. FAS-
TUS: a finite-state processor for information
extraction from real-world text. Proceedings
of IJCA I-93, pages 1172-1178.
M. E. Calif and R. J. Mooney. 1997. Relational
learning of pattern-match rules for informa-
tion extraction. In Working Papers of ACL-
97 Workshop on Natural Language Learning.
D. Freitag. 1997. Using grammatical in-
ference to improve precision in informa-
tion extraction. In Notes of the ICML-97
Workshop on Automata Induction, Gram-
matical Inference, and Language Acquisition.
http://www.cs.cmu.edu/fidupont/m197p/
m197_GI_wkshp.tar.
Dayne Freitag. 1998. Information extraction
from HTML: Application of a general ma-
chine learning approach. In Proceedings of
the Fifteenth National Conference on Artifi-
cial Intelligence (AAAI-98).
D. Lewis. 1992. Representation and Learning
in Information Retrieval. Ph.D. thesis, Univ.
of Massachusetts. CS Tech. Report 91-93.
G.A. Miller. 1995. WordNet: A lexical
database for English. Communications of the
ACM, pages 39-41, November.
Tom M. Mitchell. 1997. Machine Learning.
The McGraw-Hill Companies, Inc.
J. R. Quinlan. 1990. Learning logical def-
initions from relations. Machine Learning,
5(3):239-266.
E. Riloff. 1996. Automatically generating ex-
traction patterns from untagged text. In
Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence (AAAI-96),
pages 1044-1049.
Daniel Sleator and Davy Temperley. 1993.
Parsing English with a link grammar. Third
International Workshop on Parsing Tech-
nologies.
Stephen Soderland and Wendy Lehnert. 1994.
Wrap-Up: a trainable discourse module for
information extraction. Journal of Artificial
Intelligence Research, 2:131-158.
S. Soderland. 1996. Learning Text Analysis
Rules for Domain-specific Natural Language
Processing. Ph.D. thesis, University of Mas-
sachusetts. CS Tech. Report 96-087.
</reference>
<page confidence="0.997892">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984395">
<title confidence="0.999447">Toward General-Purpose Learning for Information Extraction</title>
<author confidence="0.998566">Dayne Freitag</author>
<affiliation confidence="0.9999415">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999516">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999877">dayneOcs.cmu.edu</email>
<abstract confidence="0.99946732">are evident in the recent evolution of the field of information extraction: a preference for simple, often corpus-driven techniques over linguistically sophisticated ones; and a broadening of the central problem definition to include many non-traditional text domains. This development calls for information extraction systems are as possi- Here, we describe learning architecture for information extraction which is designed for maximum generality and flexibility. exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training. This process is illustrated using a domain created from Reuters corporate acquisitions articles. Features are derived from two general-purpose NLP systems, Sleator and Temperly&apos;s link grammar parser and Wordnet. Experiments compare the learner&apos;s performance with and without such linguistic information. Surprisingly, in many cases, the system performs as well without this information as with it.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
<author>Jerry R Hobbs</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Mabry Tyson</author>
</authors>
<title>FASTUS: a finite-state processor for information extraction from real-world text.</title>
<date>1993</date>
<booktitle>Proceedings of IJCA I-93,</booktitle>
<pages>1172--1178</pages>
<contexts>
<context position="1919" citStr="Appelt et al., 1993" startWordPosition="280" endWordPosition="283">ld of information extraction (IE) is concerned with using natural language processing (NLP) to extract essential details from text documents automatically. While the problems of retrieval, routing, and filtering have received considerable attention through the years, IE is only now coming into its own as an information management sub-discipline. Progress in the field of IE has been away from general NLP systems, that must be tuned to work in a particular domain, toward faster systems that perform less linguistic processing of documents and can be more readily targeted at novel domains (e.g., (Appelt et al., 1993)). A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff, 1996; Soderland and Lehnert, 1994). Several researchers have reported IE systems which use machine learning at their core (Soderland, 1996; Calif and Mooney, 1997). Rather than spend human effort tuning a system for an IE domain, it becomes possible to conceive of training it on a document sample. Aside from the obvious savings in human development effort, this has significant implications for information extraction as a discipline: Retargetability</context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Tyson, 1993</marker>
<rawString>Douglas E. Appelt, Jerry R. Hobbs, John Bear, David Israel, and Mabry Tyson. 1993. FASTUS: a finite-state processor for information extraction from real-world text. Proceedings of IJCA I-93, pages 1172-1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Calif</author>
<author>R J Mooney</author>
</authors>
<title>Relational learning of pattern-match rules for information extraction.</title>
<date>1997</date>
<booktitle>In Working Papers of ACL97 Workshop on Natural Language Learning.</booktitle>
<contexts>
<context position="2230" citStr="Calif and Mooney, 1997" startWordPosition="326" endWordPosition="329"> information management sub-discipline. Progress in the field of IE has been away from general NLP systems, that must be tuned to work in a particular domain, toward faster systems that perform less linguistic processing of documents and can be more readily targeted at novel domains (e.g., (Appelt et al., 1993)). A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff, 1996; Soderland and Lehnert, 1994). Several researchers have reported IE systems which use machine learning at their core (Soderland, 1996; Calif and Mooney, 1997). Rather than spend human effort tuning a system for an IE domain, it becomes possible to conceive of training it on a document sample. Aside from the obvious savings in human development effort, this has significant implications for information extraction as a discipline: Retargetability Moving to a novel domain should no longer be a question of code modification; at most some feature engineering should be required. Generality It should be possible to handle a much wider range of domains than previously. In addition to domains characterized by grammatical prose, we should be able to perform i</context>
<context position="6204" citStr="Calif and Mooney, 1997" startWordPosition="982" endWordPosition="985">ticle about a corporate acquisition, for example, a field instance might be the text fragment listing the amount paid as part of the deal. a notion of relational features, such as next-token, which map a given token to another token in its environment. SRV uses such features to explore the context of fragments under investigation. • Top-down greedy rule search. SRV constructs rules from general to specific, as in FOIL (Quinlan, 1990). Top-down search is more sensitive to patterns in the data, and less dependent on heuristics, than the bottom-up search used by similar systems (Soderland, 1996; Calif and Mooney, 1997). • Rule validation. Training is followed by validation, in which individual rules are tested on a reserved portion of the training documents. Statistics collected in this way are used to associate a confidence with each prediction, which are used to manipulate the accuracy-coverage trade-off. 3 Case Study SRV&apos;s default feature set, designed for informal domains where parsing is difficult, includes no features more sophisticated than those immediately computable from a cursory inspection of tokens. The experiments described here were an exercise in the design of features to capture syntactic a</context>
</contexts>
<marker>Calif, Mooney, 1997</marker>
<rawString>M. E. Calif and R. J. Mooney. 1997. Relational learning of pattern-match rules for information extraction. In Working Papers of ACL97 Workshop on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
</authors>
<title>Using grammatical inference to improve precision in information extraction.</title>
<date>1997</date>
<booktitle>In Notes of the ICML-97 Workshop on Automata Induction, Grammatical Inference, and Language Acquisition. http://www.cs.cmu.edu/fidupont/m197p/ m197_GI_wkshp.tar.</booktitle>
<contexts>
<context position="9850" citStr="Freitag, 1997" startWordPosition="1573" endWordPosition="1574">uster of closely related synsets. In the case of multiple Wordnet senses, we used the most common sense of a word, according to Wordnet, to construct this set. 3.3 Competing Learners We compare the performance of SRV with that of two simple learning approaches, which make predictions based on raw term statistics. Rote (see (Freitag, 1998)), memorizes field instances seen during training and only makes predictions when the same fragments are encountered in novel documents. Bayes is a statistical approach based on the &amp;quot;Naive Bayes&amp;quot; algorithm (Mitchell, 1997). Our implementation is described in (Freitag, 1997). Note that although these learners are &amp;quot;simple,&amp;quot; they are not necessarily ineffective. We have experimented with them in several domains and have been surprised by their level of performance in some cases. 4 Results The results presented here represent average performances over several separate experiments. In each experiment, the 600 documents in the collection were randomly partitioned into two sets of 300 documents each. One of the two subsets was then used to train each of the learners, the other to measure the performance of the learned extractors. We compared four learners: each of the </context>
</contexts>
<marker>Freitag, 1997</marker>
<rawString>D. Freitag. 1997. Using grammatical inference to improve precision in information extraction. In Notes of the ICML-97 Workshop on Automata Induction, Grammatical Inference, and Language Acquisition. http://www.cs.cmu.edu/fidupont/m197p/ m197_GI_wkshp.tar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Information extraction from HTML: Application of a general machine learning approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98).</booktitle>
<contexts>
<context position="4678" citStr="Freitag, 1998" startWordPosition="731" endWordPosition="732">ond this, it becomes difficult to make assumptions that are not violated by some common and important domain of interest. At the same time, however, when structural assumptions are justified, they may be critical to the success of the system. It should be possible, therefore, to make structural information available to the learner as input for training. The machine learning method with which we experiment here. SRV, was designed with these considerations in mind. In experiments reported elsewhere, we have applied SRV to collections of electronic seminar announcements and World Wide Web pages (Freitag, 1998). Readers interested in a more thorough description of SRV are referred to (Freitag, 1998). Here, we list its most salient characteristics: • Lack of structural assumptions. SRV assumes nothing about the structure of a field instance&apos; or the text in which it is embedded—only that an instance is an unbroken fragment of text. During learning and prediction, SRV inspects every fragment of appropriate size. • Token-oriented features. Learning is guided by a feature set which is separate from the core algorithm. Features describe aspects of individual tokens, such as capitalized, numeric, noun. Rul</context>
<context position="9576" citStr="Freitag, 1998" startWordPosition="1531" endWordPosition="1532">tly boolean, this feature is set-valued. For nouns and verbs, its value is a set of identifiers representing all synsets in the hypernym path to the root of the hypernym tree in which a word occurs. For adjectives and adverbs, these synset identifiers were drawn from the cluster of closely related synsets. In the case of multiple Wordnet senses, we used the most common sense of a word, according to Wordnet, to construct this set. 3.3 Competing Learners We compare the performance of SRV with that of two simple learning approaches, which make predictions based on raw term statistics. Rote (see (Freitag, 1998)), memorizes field instances seen during training and only makes predictions when the same fragments are encountered in novel documents. Bayes is a statistical approach based on the &amp;quot;Naive Bayes&amp;quot; algorithm (Mitchell, 1997). Our implementation is described in (Freitag, 1997). Note that although these learners are &amp;quot;simple,&amp;quot; they are not necessarily ineffective. We have experimented with them in several domains and have been surprised by their level of performance in some cases. 4 Results The results presented here represent average performances over several separate experiments. In each experime</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Information extraction from HTML: Application of a general machine learning approach. In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Representation and Learning in Information Retrieval.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Massachusetts.</institution>
<contexts>
<context position="7036" citStr="Lewis, 1992" startWordPosition="1112" endWordPosition="1113"> prediction, which are used to manipulate the accuracy-coverage trade-off. 3 Case Study SRV&apos;s default feature set, designed for informal domains where parsing is difficult, includes no features more sophisticated than those immediately computable from a cursory inspection of tokens. The experiments described here were an exercise in the design of features to capture syntactic and lexical information. 3.1 Domain As part of these experiments we defined an information extraction problem using a publicly available corpus. 600 articles were sampled from the &amp;quot;acquisition&amp;quot; set in the Reuters corpus (Lewis, 1992) and tagged to identify instances of nine fields. Fields include those for the official names of the parties to an acquisition (acquired, purchaser, seller), as well as their short names (acqabr, purchabr, sellerabr), the location of the purchased company or resource (acqloc), the price paid (dlramt), and any short phrases summarizing the progress of negotiations (status). The fields vary widely in length and frequency of occurrence, both of which have a significant impact on the difficulty they present for learners. 3.2 Feature Set Design We augmented SRV&apos;s default feature set with features d</context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>D. Lewis. 1992. Representation and Learning in Information Retrieval. Ph.D. thesis, Univ. of Massachusetts. CS Tech. Report 91-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>39--41</pages>
<contexts>
<context position="8502" citStr="Miller, 1995" startWordPosition="1350" endWordPosition="1351"> takes a sentence as input and returns a complete parse in which terms are connected in typed binary relations (&amp;quot;links&amp;quot;) which represent syntactic relationships (Sleator and Temperley, 1993). We mapped these links to relational features: A token on the right side of a link of type X has a corresponding relational feature called left_X that maps to the token on the left side of the link. In addition, several non-relational features, such as part of speech, are derived from parser output. Figure 1 shows part of a link grammar parse and its translation into features. Our object in using Wordnet (Miller, 1995) is to enable SRV to recognize that the phrases, &amp;quot;A bought B,&amp;quot; and, &amp;quot;X acquired Y,&amp;quot; are instantiations of the same underlying pattern. Although &amp;quot;bought&amp;quot; and &amp;quot;acquired&amp;quot; do not belong to the same &amp;quot;synset&amp;quot; in Wordnet, they are nevertheless closely related in Wordnet by means of the &amp;quot;hypernym&amp;quot; (or &amp;quot;is-a&amp;quot;) relation. To exploit such semantic relationships we created a single token feature, called wn_word. In contrast with features already outlined, which are mostly boolean, this feature is set-valued. For nouns and verbs, its value is a set of identifiers representing all synsets in the hypernym pat</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, pages 39-41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning. The McGraw-Hill Companies, Inc.</booktitle>
<contexts>
<context position="9798" citStr="Mitchell, 1997" startWordPosition="1565" endWordPosition="1566">erbs, these synset identifiers were drawn from the cluster of closely related synsets. In the case of multiple Wordnet senses, we used the most common sense of a word, according to Wordnet, to construct this set. 3.3 Competing Learners We compare the performance of SRV with that of two simple learning approaches, which make predictions based on raw term statistics. Rote (see (Freitag, 1998)), memorizes field instances seen during training and only makes predictions when the same fragments are encountered in novel documents. Bayes is a statistical approach based on the &amp;quot;Naive Bayes&amp;quot; algorithm (Mitchell, 1997). Our implementation is described in (Freitag, 1997). Note that although these learners are &amp;quot;simple,&amp;quot; they are not necessarily ineffective. We have experimented with them in several domains and have been surprised by their level of performance in some cases. 4 Results The results presented here represent average performances over several separate experiments. In each experiment, the 600 documents in the collection were randomly partitioned into two sets of 300 documents each. One of the two subsets was then used to train each of the learners, the other to measure the performance of the learned</context>
<context position="12905" citStr="Mitchell, 1997" startWordPosition="2088" endWordPosition="2089">s which we considered representative of the wide range of behavior we observed. In addition, in order to assess the contribution of each kind of linguistic information (syntactic and lexical) to SRV&apos;s performance, we ran experiments in which its basic feature set was augmented with only one type or the other. 4.2 Discussion Perhaps surprisingly, but consistent with results we have obtained in other domains, there is no one algorithm which outperforms the others on all fields. Rather than the absolute difficulty of a field, we speak of the suitability of a learner&apos;s inductive bias for a field (Mitchell, 1997). Bayes is clearly better than SRV on the seller and sellerabr fields at all points on the accuracycoverage curve. We suspect this may be due, in part, to the relative infrequency of these fields in the data. The one field for which the linguistic features offer benefit at all points along the accuracycoverage curve is acqabr.2 We surmise that two factors contribute to this success: a high frequency of occurrence for this field (2.42 times 2The acqabr differences in Table 2 (a 3-split experiment) are not significant at the 95% confidence level. However, the full 10-split averages, with 95% err</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom M. Mitchell. 1997. Machine Learning. The McGraw-Hill Companies, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Learning logical definitions from relations.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<pages>5--3</pages>
<contexts>
<context position="3029" citStr="Quinlan, 1990" startWordPosition="457" endWordPosition="458">nt effort, this has significant implications for information extraction as a discipline: Retargetability Moving to a novel domain should no longer be a question of code modification; at most some feature engineering should be required. Generality It should be possible to handle a much wider range of domains than previously. In addition to domains characterized by grammatical prose, we should be able to perform information extraction in domains involving less traditional structure, such as netnews articles and Web pages. In this paper we describe a learning algorithm similar in spirit to FOIL (Quinlan, 1990), which takes as input a set of tagged documents, and a set of features that control generalization, and produces rules that describe how to extract information from novel documents. For this system, introducing linguistic or any other information particular to a domain is an exercise in feature definition, separate from the central algorithm, which is constant. We describe a set of experiments, involving a document collection of newswire articles, in which this learner is compared with simpler learning algorithms. 404 2 SRV In order to be suitable for the widest possible variety of textual do</context>
<context position="6018" citStr="Quinlan, 1990" startWordPosition="954" endWordPosition="955">ning of tokens. • Relational features. SRV also includes &apos;We use the terms field and field instance for the rather generic IE concepts of slot and slot filler. For a newswire article about a corporate acquisition, for example, a field instance might be the text fragment listing the amount paid as part of the deal. a notion of relational features, such as next-token, which map a given token to another token in its environment. SRV uses such features to explore the context of fragments under investigation. • Top-down greedy rule search. SRV constructs rules from general to specific, as in FOIL (Quinlan, 1990). Top-down search is more sensitive to patterns in the data, and less dependent on heuristics, than the bottom-up search used by similar systems (Soderland, 1996; Calif and Mooney, 1997). • Rule validation. Training is followed by validation, in which individual rules are tested on a reserved portion of the training documents. Statistics collected in this way are used to associate a confidence with each prediction, which are used to manipulate the accuracy-coverage trade-off. 3 Case Study SRV&apos;s default feature set, designed for informal domains where parsing is difficult, includes no features </context>
</contexts>
<marker>Quinlan, 1990</marker>
<rawString>J. R. Quinlan. 1990. Learning logical definitions from relations. Machine Learning, 5(3):239-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="2071" citStr="Riloff, 1996" startWordPosition="304" endWordPosition="305">e the problems of retrieval, routing, and filtering have received considerable attention through the years, IE is only now coming into its own as an information management sub-discipline. Progress in the field of IE has been away from general NLP systems, that must be tuned to work in a particular domain, toward faster systems that perform less linguistic processing of documents and can be more readily targeted at novel domains (e.g., (Appelt et al., 1993)). A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff, 1996; Soderland and Lehnert, 1994). Several researchers have reported IE systems which use machine learning at their core (Soderland, 1996; Calif and Mooney, 1997). Rather than spend human effort tuning a system for an IE domain, it becomes possible to conceive of training it on a document sample. Aside from the obvious savings in human development effort, this has significant implications for information extraction as a discipline: Retargetability Moving to a novel domain should no longer be a question of code modification; at most some feature engineering should be required. Generality It should</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044-1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a link grammar. Third International Workshop on Parsing Technologies.</title>
<date>1993</date>
<contexts>
<context position="8079" citStr="Sleator and Temperley, 1993" startWordPosition="1273" endWordPosition="1276">equency of occurrence, both of which have a significant impact on the difficulty they present for learners. 3.2 Feature Set Design We augmented SRV&apos;s default feature set with features derived using two publicly available 405 +---G---+---G---+--Ss-+-Ce-+Ss*b+ I I I I I I First Wisconsin Corp said.v it plans.v Figure 1: An example of link grammar feature derivation. NLP tools, the link grammar parser and Wordnet. The link grammar parser takes a sentence as input and returns a complete parse in which terms are connected in typed binary relations (&amp;quot;links&amp;quot;) which represent syntactic relationships (Sleator and Temperley, 1993). We mapped these links to relational features: A token on the right side of a link of type X has a corresponding relational feature called left_X that maps to the token on the left side of the link. In addition, several non-relational features, such as part of speech, are derived from parser output. Figure 1 shows part of a link grammar parse and its translation into features. Our object in using Wordnet (Miller, 1995) is to enable SRV to recognize that the phrases, &amp;quot;A bought B,&amp;quot; and, &amp;quot;X acquired Y,&amp;quot; are instantiations of the same underlying pattern. Although &amp;quot;bought&amp;quot; and &amp;quot;acquired&amp;quot; do not be</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Daniel Sleator and Davy Temperley. 1993. Parsing English with a link grammar. Third International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Wendy Lehnert</author>
</authors>
<title>Wrap-Up: a trainable discourse module for information extraction.</title>
<date>1994</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>2--131</pages>
<contexts>
<context position="2101" citStr="Soderland and Lehnert, 1994" startWordPosition="306" endWordPosition="309"> of retrieval, routing, and filtering have received considerable attention through the years, IE is only now coming into its own as an information management sub-discipline. Progress in the field of IE has been away from general NLP systems, that must be tuned to work in a particular domain, toward faster systems that perform less linguistic processing of documents and can be more readily targeted at novel domains (e.g., (Appelt et al., 1993)). A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff, 1996; Soderland and Lehnert, 1994). Several researchers have reported IE systems which use machine learning at their core (Soderland, 1996; Calif and Mooney, 1997). Rather than spend human effort tuning a system for an IE domain, it becomes possible to conceive of training it on a document sample. Aside from the obvious savings in human development effort, this has significant implications for information extraction as a discipline: Retargetability Moving to a novel domain should no longer be a question of code modification; at most some feature engineering should be required. Generality It should be possible to handle a much </context>
</contexts>
<marker>Soderland, Lehnert, 1994</marker>
<rawString>Stephen Soderland and Wendy Lehnert. 1994. Wrap-Up: a trainable discourse module for information extraction. Journal of Artificial Intelligence Research, 2:131-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
</authors>
<title>Learning Text Analysis Rules for Domain-specific Natural Language Processing.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="2205" citStr="Soderland, 1996" startWordPosition="323" endWordPosition="325">nto its own as an information management sub-discipline. Progress in the field of IE has been away from general NLP systems, that must be tuned to work in a particular domain, toward faster systems that perform less linguistic processing of documents and can be more readily targeted at novel domains (e.g., (Appelt et al., 1993)). A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff, 1996; Soderland and Lehnert, 1994). Several researchers have reported IE systems which use machine learning at their core (Soderland, 1996; Calif and Mooney, 1997). Rather than spend human effort tuning a system for an IE domain, it becomes possible to conceive of training it on a document sample. Aside from the obvious savings in human development effort, this has significant implications for information extraction as a discipline: Retargetability Moving to a novel domain should no longer be a question of code modification; at most some feature engineering should be required. Generality It should be possible to handle a much wider range of domains than previously. In addition to domains characterized by grammatical prose, we sh</context>
<context position="6179" citStr="Soderland, 1996" startWordPosition="980" endWordPosition="981">For a newswire article about a corporate acquisition, for example, a field instance might be the text fragment listing the amount paid as part of the deal. a notion of relational features, such as next-token, which map a given token to another token in its environment. SRV uses such features to explore the context of fragments under investigation. • Top-down greedy rule search. SRV constructs rules from general to specific, as in FOIL (Quinlan, 1990). Top-down search is more sensitive to patterns in the data, and less dependent on heuristics, than the bottom-up search used by similar systems (Soderland, 1996; Calif and Mooney, 1997). • Rule validation. Training is followed by validation, in which individual rules are tested on a reserved portion of the training documents. Statistics collected in this way are used to associate a confidence with each prediction, which are used to manipulate the accuracy-coverage trade-off. 3 Case Study SRV&apos;s default feature set, designed for informal domains where parsing is difficult, includes no features more sophisticated than those immediately computable from a cursory inspection of tokens. The experiments described here were an exercise in the design of featur</context>
</contexts>
<marker>Soderland, 1996</marker>
<rawString>S. Soderland. 1996. Learning Text Analysis Rules for Domain-specific Natural Language Processing. Ph.D. thesis, University of Massachusetts. CS Tech. Report 96-087.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>