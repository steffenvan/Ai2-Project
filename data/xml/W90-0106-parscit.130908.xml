<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.983933">
Natural Discourse Hypothesis Engine
</title>
<author confidence="0.974154">
Susanna Cumming
</author>
<affiliation confidence="0.825527">
Department of Linguistics
Campus Box 295
University of Colorado
Boulder, Colorado 80309
</affiliation>
<sectionHeader confidence="0.984547" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985411764706">
As text generation systems get more sophisticated
and capable of producing a wider syntactic and
lexical range, the issue of how to choose among
available grammatical options (preferably in a
human-like way) becomes more pressing. Thus,
devisers of text generation systems are frequently
called upon to provide their own analyses of the
discourse function of various kinds of alternations.
In this paper, I describe a proposed research tool
which is designed to help a researcher explore and
analyze a natural-language &amp;quot;target text&amp;quot; in order to
determine the contextual factors that predict the
choice of one or another lexical item or grammati-
cal feature. The project described in this paper is
still at the drawing-board stage; I welcome sugges-
tions about ways it could be changed or expanded
to fulfill particular analytic needs.
</bodyText>
<subsectionHeader confidence="0.810888">
Theoretical preliminaries
</subsectionHeader>
<bodyText confidence="0.999989672413793">
While some aspects of a natural-language text are deter-
mined by the nature of the information a speaker wishes to
convey to a hearer, there are many more aspects that seem
to be determined by certain cognitive needs that the hearer
has. Speakers tailor their output to the informational
resources and deficiencies of the hearer in several ways: by
adjusting the amount of information they make explicit, by
arranging new information relative to old information in a
maximally helpful way, and by giving special marking to
information that may be difficult for the hearer to access
for a variety of reasons. It is these strategies that give rise
to the wide variety of syntactic and lexical resources of
any natural language for saying the &amp;quot;same thing&amp;quot; in differ-
ent ways. We can call the relation between lexico-
grammatical features and the speaker&apos;s communicative
goals in choosing those features the &amp;quot;discourse functions&amp;quot;
of the features.
For any particular alternation, then, the best predictor
of the speaker&apos;s choice should be a model of the cognitive
state of the hearer. Unfortunately, neither human speakers
nor computer systems have direct access to the hearer&apos;s
mind. But linguists have long realized that we do have
access to a fair approximation of an important subset of the
information the hearer possesses at a given point in a dis-
course: namely the text which has been produced up to that
point. ([Chafe 1987] and [Givon 1983] are two con-
temporary expressions of that principle.) And in fact we
can make fairly good predictions of lexico-grammatical
choices based on inferences that come from the nature of
the preceding text. For instance, a referent that has been
referred to in the previous clause is likely to receive
minimal coding (a pronoun or zero, depending on syntactic
considerations). But this principle can be overridden by
the presence of other factors that interfere with the acces-
sibility of the referent — e.g. a paragraph break or another
competing referent — resulting in the use of a full noun
phrase. Or, to give another example, a speaker is likely to
use special syntax (such as the &amp;quot;presentative&amp;quot; or &amp;quot;there
is...&amp;quot; construction) to introduce a referent that will be
prominent in the following discourse; so a hearer is likely
to have easier access to a referent introduced in that way
than one that has been introduced &amp;quot;casually&amp;quot;, e.g. in a
prepositional phrase. Therefore, subsequent references to
a referent that has been introduced with a presentative are
more likely to be pronouns than noun phrases.
These are all factors that can be discerned in the
preceding text and are taken into account by speakers as
affecting the nature of the hearer&apos;s expectations. Therefore
under these perturbing circumstances the speaker can
decide to use a fuller reference, e.g. a proper name or noun
phrase. Figure 1 illustrates the relation between the dis-
course produced by the speaker, the hearer&apos;s mental state,
and the speaker&apos;s model of the hearer. In real face-to-face
interaction, the hearer can influence the speaker&apos;s model of
her or him in more direct ways — e.g. by verbal and non-
verbal indications of agreement, understanding, protest, or
confusion. But this two-way channel is available neither
to human writers nor to computer text generators.
</bodyText>
<page confidence="0.996891">
39
</page>
<figure confidence="0.997035333333333">
Speaker&apos;s communicative
goats
Hearer&apos;s mental state.,
including representation
of speech
Speaker&apos;s model
of hearer&apos;s state
X
9
</figure>
<figureCaption confidence="0.999928">
Figure 1: A model of communication.
</figureCaption>
<bodyText confidence="0.999970722222223">
The fact that we can draw inferences from preceding
text about the hearer&apos;s state justifies the methodology
common in functional linguistics whereby the linguist
investigates the function of a particular morpho-syntactic
alternation by attempting to discover correlations in natural
texts between that alternation and various features of the
context. This method is also likely to be the one which
will lead to the best results for text generation specialists,
since prior text is one information source any computa-
tional system has easy access to as a guide in making
future decisions.
There are, of course, currently available several large-
scale databases of English texts of various kinds, some of
which have been provided with various amounts and kinds
of coding (from word-class tagging to syntactic analysis).
However, for many kinds of discourse work these data-
bases have not been useful. On the one hand, such data-
bases are often not available in languages other than
English; on-The other, the coding that is provided assumes
an analysis which may not meet the needs of a particular
user. In fact it is often the case that aspects of the syntactic
analysis depend on a functional analysis having already
been done; so a pre-coded database may contain assump-
tions about the answers to the same qu-estions it is sup-
posed to be helping to solve. The tool described here is
designed for the user who is satisfied with a relatively
small amount of data, but wants to have total control over
the analysis of the corpus.
Currently, functional linguists often enter their text
data into commercial relational database tools, such as
DBase and Paradox. However, such tools have been
designed with other kinds of applications in mind, and
therefore there are many properties of natural language
texts which these tools can only capture by awkward or
indirect means. Specifically, natural language texts, unlike
e.g. client or sales lists, are simultaneously linear (order
matters) and recursively hierarchical. Thus, the query lan-
guages available with such database products are generally
good at extracting the kind of information from texts
which is least useful to linguists, but require considerable
ad hoc programming to extract the kind of information
most relevant to our needs. The tool proposed here, con-
versely, should answer most easily the questions a dis-
course analyst wants to ask most frequently.
These problems in representing certain crucial aspects
of natural language texts computationally using off-the-
shelf tools have sent many linguists back to marking up
xeroxed texts by hand with colored pencils. The approach
outlined here is intended to combine the advantages of the
colored-pencil technique (spontaneity, flexibility, and
direct involvement with the whole text) with the advan-
tages of computational techniques (quick and painless
identification, compilation, and comparison of various
features of text units).
</bodyText>
<subsectionHeader confidence="0.948862">
Description of the tool
</subsectionHeader>
<bodyText confidence="0.999294777777778">
The tool proposed in this paper will aid in the generation
of hypotheses concerning the discourse function of a
lexico-grammatical feature (or combination of features) by
allowing the researcher to isolate instances of that feature
and view them in relation to various aspects of its dis-
course context. When the researcher has arrived at and
stated a working hypothesis about which features of the
context are most relevant to predicting the targeted feature,
the system will be able to test the hypothesis against the
data and provide feedback by displaying both predicted
and non-predicted occurrences of the target feature.
Further refinements can then be made until the fit between
the researcher&apos;s hypothesis and the actual text is as good as
possible. The hypothesis can then be integrated into a text
generation system with some assurance that the lexico-
grammatical choices made by the system will approximate
those made by a human with a similar text-production task.
In order for the tool to be able to recognize and
compare various text features — including relatively
covert information such as the reference of noun phrases
and the mood of a sentence as well as relatively overt cues
such as lexical items and structure — the user must first
annotate the text, indicating a) its hierarchical structure,
and b) a set of features (attribute/value pairs) associated
with each constituent. The annotation will be largely
manual (though aspects of it can be automated, as will be
explained), because we want the tool itself to be neutral as
</bodyText>
<page confidence="0.990499">
40
</page>
<bodyText confidence="0.9883972">
to the theoretical assumptions inherent in any particular
analysis. Thus, it will be possible to use this tool to test
among other things the comparative predictive usefulness
of various possible analyses of a text.
Using the tool will have the following phases:
</bodyText>
<listItem confidence="0.950931375">
1. Annotation: mark constituents, and for each consti-
tuent indicate its category membership and any
other desired features. The amount and type of
annotation is entirely up to the user; it will never be
necessary to include any more detail than is rele-
vant for the current analysis. Constituents may be
anything from words to paragraphs or episodes.
2. Hypothesis formation: view the target feature and
context in various arrangements.
3. Hypothesis testing: state a hypothesis in terms of a
pattern in the context that predicts the occurrence of
the target feature.
4. Hypothesis refinement: make changes in hypothesis,
annotation, or both and retest against the same data;
extend analysis by testing against different data.
1. Annotation
</listItem>
<bodyText confidence="0.766882214285714">
An example text showing segmentation and sample feature
specification associated with three of the constituents is
given in figure 2.
[[We] [went] [from [Kobe]] [to [Shanghai],]
[[from [Shanghai]] [we] [went] [to [Singapore]].]
[[and] [there] [I] [still remember]
[[that] f[Pcople].[wilthe ship]]] [would toss]
[English pennies] [overboard]].] N
[[[and] [the [natives] [in [sampans]]]
[would see] [it] [[0] [coming].]]
[[0] [dive].) [[and] 01-. [come up]]
[[0] [having caught] [the p ny]
-
[[as] [it] [sank] [into [the sea
</bodyText>
<figureCaption confidence="0.999673">
Figure 2: Annotated text.
</figureCaption>
<bodyText confidence="0.9968504">
Annotation will have the following steps: segmenta-
tion, category specification, further feature specification,
and dependency specification. Furthermore, &amp;quot;pseudo-
constituents&amp;quot; may be added and specified at this stage, as
explained below.
</bodyText>
<listItem confidence="0.994028295454546">
1. Segmentation: Mark off a stretch of text to be con-
sidered a unit. Initially, allowable units must be
continuous and properly contained inside other
units; but eventually it would be nice to allow two
kinds of violations to this principle, namely discon-
tinuous constituents and overlapping constituents.
2. Category specification: Specify the category of the
constituent. The category will be stored as an
attribute-value pair (with the attribute &amp;quot;category&amp;quot;)
like other features. At the time of category specifi-
cation, the constituent will be assigned a unique ID,
also an attribute-value pair (with the attribute &amp;quot;id&amp;quot;).
3. Further feature specification: Specify further
attribute-value pairs. In order to ensure consistency
the user will be prompted with a list of a) attributes
that have previously been used with that category,
and b) values that have previously been associated
with that attribute. The user may select from these
lists or enter a new attribute or value.
4. Dependency relations: Some kinds of relations
between constituents are best treated by specifying
dependency relations between the two items, i.e. by
providing a labelled pointer linking the two. Syn-
tactic analyses differ from each other in what kinds
of relations are treated as constituency relations
(e.g. the relationship of sister nodes in a tree) and
what kinds are treated as pure dependency rela-
tions.
5. &amp;quot;Pseudo-constituent&amp;quot; insertion: For some purposes,
it may be desirable to allow the insertion of
&amp;quot;pseudoconstituents&amp;quot; in the text, i.e. phonologically
null &amp;quot;elements&amp;quot; that can be inferred from the
surface text, or boundaries which aren&apos;t marked by
any explicit linguistic element (such as &amp;quot;episode
boundaries&amp;quot; or possibly the beginning and end of
the text). Examples of null elements which can be
tracked in discourse as if they were overt include
&amp;quot;zero pronouns&amp;quot;, inferrable propositions, and ele-
ments of frames or schemas. Pseudo-constituents
can be inserted directly into the text like actual con-
stituents, but they should have distinctive marking
in their feature specification to indicate their
abstract status; in the examples I have distinguished
them by the feature &amp;quot;type = pseudo&amp;quot;.
</listItem>
<equation confidence="0.923813">
category = clause
Id = c111
mol. type = finite
relation = time
&apos;main = CLI 01
category= np
Id = np15
type = pseudo
number = plural
referent = NATIVES
status = generic
syn. role = subject
Iverb = WI
sem. role = actor
lverb = V71
class = human
category = np
Id = npt3
number = plural
referent = SHIP—FOU(
status = generic
syn. role = subject
(verb = V4)
sem. role = agent
(verb = V4)
class = human
</equation>
<page confidence="0.977838">
41
</page>
<bodyText confidence="0.976817846153846">
Figure 3 illustrates a possible implementation (using
pop-up menus) of the process of feature specification.
face which will allow the user to open a &amp;quot;query window&amp;quot;,
in which he or she can enter a partial specification of one
or more constituents to be matched against the text, using
the same entry techniques as are used to enter, edit, and
annotate actual text. This approach has the advantage of
letting the user make use of what he or she already knows
about the annotation process to specify the target pattern,
and furthermore it allows the identification of highly
complex patterns in an intuitive way.
A pattern specification will contain the following kinds
of elements:
</bodyText>
<figure confidence="0.987839066666667">
1. Actual structure (brackets), text, and feature
bundles.
&amp;quot;cat gory =
category values:
noun
verb
preposition
adjective
adverb
complementizer
noun phrase
verb group
prepositional phrase
clause
sentence
...paragraph
category = noun phrase
number =
number values:
singular
plural
mass
fregory = noun phrase
1
noun phrase attributes:
number
referent
status
sYn- role
sem. role
</figure>
<figureCaption confidence="0.999783">
Figure 3: Feature specification.
</figureCaption>
<subsectionHeader confidence="0.509023">
Queries
</subsectionHeader>
<bodyText confidence="0.999585">
Having annotated a text to the desired degree, it is then
possible to start fishing around for an analysis of the distri-
butional characteristics of the feature or combination of
features of interest to the analyst (henceforth &amp;quot;target
pattern&amp;quot;). Experience has shown that the most useful ways
to do this is to get an idea of the overall frequency of the
phenomenon, and then to &amp;quot;eyeball&amp;quot; the target pattern in
various kinds of context. Thus, the tool should have the
following capabilities:
</bodyText>
<listItem confidence="0.987196833333333">
1. Counting: Count the number of instances of the
target pattern.
2. Highlighting: Display the whole text, with constitu-
ents which match the target pattern highlighted by
color or a distinctive font.
3. Extraction: Extract constituents which match the
</listItem>
<bodyText confidence="0.975221666666667">
target pattern into a file of the same type as the text
file. It will then be possible to print out the output
file, or to subset it further by performing additional
highlighting or extraction queries on it.
Basic pattern specification. These operations require a
format for describing the target pattern. I propose an inter-
</bodyText>
<listItem confidence="0.98326975">
2. Variables which stand for text or features, used to
indicate relationships between parts of the pattern,
or to specify to parts of the pattern to be acted on
(counted, highlighted, extracted).
3. Various kinds of wildcard elements which will
match unspecified strings of structure or text.
4. Operators (and, or, not) which specify relations
between features, text, variables, or whole patterns.
</listItem>
<bodyText confidence="0.90433716">
It should be possible to save a query in a file for future
editing and re-use. This avoids repetitive typing of the
same query, while permitting refinement and adjustment of
queries that don&apos;t give the desired results the first time. It
also allows the creation of a series of related queries by
copying and slightly altering a prototype.
Variables (indicated here by strings of alphanumerics
starting with &amp;quot;#&amp;quot;) are used to specify text or elements of
feature bundles for reference elsewhere, either to be
matched or acted upon. A text variable is interpreted as
matching all the text in the constituent that contains it
(including text in embedded constituents). For instance,
the following two queries would match constituents con-
taining two elements of the same category conjoined by
&amp;quot;and&amp;quot;, as in &amp;quot;Jill and Liza&amp;quot; or &amp;quot;sang off-key and danced the
waltz&amp;quot;. The (a) version would highlight the whole con-
joined phrase; the (b) version would highlight each of the
conjoined phrases, but not &amp;quot;and&amp;quot;. (The dots are wildcards,
explained below. In these and the following examples,
feature specifications are indicated in small italics; literal
text is enclosed in quotes.)
(la) [ #txt [..]cat=#c &amp;quot;and&amp;quot; [..]cat=#c
highlight #txt
(2b) [ [#txt1 ..]cat=#c &amp;quot;and&amp;quot; [#txt2 ..]cat=#c
highlight #txt1 and #txt2
</bodyText>
<page confidence="0.996622">
42
</page>
<bodyText confidence="0.999955111111111">
An analysis of the types of queries I anticipate wanting
to make reveals that a fairly diverse set of wildcards is
necessary. Structure wildcards match elements of struc-
ture, i.e. combinations of brackets. Text and features are
ignored by these wildcards (see below). Two types of
structure wildcards are provided. The first, composed of
periods, stops at the first instance of the pattern in the
string; the second, composed of exclamation marks, finds
all instances of the pattern.
</bodyText>
<subsectionHeader confidence="0.853691">
1st All
match matches
</subsectionHeader>
<bodyText confidence="0.830316625">
Depth: a series of all right or all
left brackets
!! Constituents: a series of matching
left and right brackets (and their
contents)
!!! Any material
The following examples illustrate the use of structure wild-
cards:
</bodyText>
<equation confidence="0.410644">
(2) [ [ #X ]cat=np ]cat=ciause
</equation>
<bodyText confidence="0.9915582">
X matches a top-level NP which is the first constituent of a
clause. It matches &amp;quot;Jill&amp;quot; in &amp;quot;Jill ate the peach&amp;quot;, but nothing
in &amp;quot;On the next day Jill ate the peach&amp;quot; (because the first
top-level element is a prepositional phrase, not a noun
phrase).
</bodyText>
<equation confidence="0.68272">
(3a) [ . [ #X ]cat=np ]cat=c1ause
(3b) [ ! [ #X ]cat=np ]cat=c1ause
</equation>
<bodyText confidence="0.9917088">
X matches an NP which is part of the first constituent in a
clause, no matter how deeply it is embedded. In the sen-
tence &amp;quot;On the day after the fire Jill ate the peach&amp;quot;, both (a)
and (b) will match &amp;quot;the day after the fire&amp;quot;, and the (b)
version will also match &amp;quot;the fire&amp;quot;.
</bodyText>
<equation confidence="0.92696725">
(4a) [ [ #X 1
Jcat=np ]cat=clause
(4b) [ !! [ #X 1
,cat=np ]cat=c1ause
</equation>
<bodyText confidence="0.999519166666667">
X matches the first NP which is a top-level constituent of
the clause; in (b), it matches all NPs which are top-level
constituents of the clause. In the sentence &amp;quot;On the day
after the fire Jill ate the peach&amp;quot;, both versions will match
&amp;quot;Jill&amp;quot;; (b) could also match &amp;quot;the peach&amp;quot; if the sentence was
given a &amp;quot;flat&amp;quot; structural analysis (without a verb phrase).
</bodyText>
<equation confidence="0.949146">
(5a) [ [ #X ]cat=np ]cat=c1ause
(5b) [ !!! [ #X jcat=np ]cat.dause
</equation>
<bodyText confidence="0.980841666666667">
X matches the first noun phrase in a clause (no matter
where it is); (b) matches every noun phrase in a clause. In
&amp;quot;Much to the surprise of her neighbors, Jill ate the peach&amp;quot;,
(a) will match &amp;quot;the surprise of her neighbors&amp;quot;, while (b)
will additionally match &amp;quot;her neighbors&amp;quot;, &amp;quot;Jill&amp;quot;, and &amp;quot;the
peach&amp;quot;.
Text/feature wildcards are used both to match actual
text and to match elements of feature bundles (attributes
and values).
(nothing) Matches any text/feature or no
text/feature
Matches no text (the absence of text)
Matches any text, attribute, or value
The following examples show how these wildcards could
be used:
</bodyText>
<equation confidence="0.482055">
(6) [ #X @ [ lcat=clause ]cat=np
</equation>
<bodyText confidence="0.992712666666667">
X matches a noun phrase whose first element is a clause.
It would match &amp;quot;that he came&amp;quot; in &amp;quot;that he came surprised
me&amp;quot;, but not in &amp;quot;the fact that he came surprised me&amp;quot;.
</bodyText>
<equation confidence="0.880122">
(7) [ #X ]ref=*
X matches any constituent that has a referent specified.
</equation>
<bodyText confidence="0.994310142857143">
Figure 4 illustrates one way windows and menus could
be used to simplify the query process. The query window
is divided into two panes, a &amp;quot;pattern pane&amp;quot; and an &amp;quot;action
pane&amp;quot;. The various kinds of query elements can be either
typed in or selected from a set of pull-down menus. This
query is designed to extract noun phrases which have the
property of &amp;quot;referential ambiguity&amp;quot; in the sense of [Givon
1983]: referents which have been referred to in the
immediately previous clause, but where the previous
clause also contains another referent of the same semantic
class. (An example would be &amp;quot;Mary saw Jill, and she ran
off&apos;; she is referentially ambiguous, and in fact we would
expect a full noun phrase rather than a pronoun in this kind
of context.)
</bodyText>
<page confidence="0.999818">
43
</page>
<figureCaption confidence="0.999911">
Figure 4: A query.
</figureCaption>
<bodyText confidence="0.99963925">
Using queries to change annotation
Extensive experience with feature-coding of text data has
shown that there is a large number of features which are
predictable from the identity of lexical items and/or com-
binations of other features. In these cases it is very con-
venient to be able to partially code a text and to fill in the
redundant codes by using queries. Similarly, it is some-
times desirable to use redundant information in an
annotated text to change or delete features when you
change your mind about the best coding scheme. Further-
more, if it is possible to insert structure boundaries on the
basis of patterns found in the text, it is even possible to
perform some structure insertion automatically, giving the
user in effect the possibility of using queries to automati-
cally parse the text where this can be done reliably.
he most straightforward way to implement this is to allow
the user to specify two patterns, an &amp;quot;old&amp;quot; pattern and a
&amp;quot;new&amp;quot; pattern. The query illustrated in figure 5 creates a
verb phrase constituent wherever a verb is followed by a
noun phrase which is its object.
</bodyText>
<figureCaption confidence="0.997442">
Figure 5: A change query.
</figureCaption>
<bodyText confidence="0.999849724137931">
Distance queries. When text is viewed as a window to the
cognitive processes of the speaker, it becomes apparent
that the distance between e.g. two mentions of a referent or
two clauses referring to the same event can be important.
This is because the evidence suggests that the cognitive
accessibility (to the hearer) of a referent &amp;quot;decays&amp;quot; over
time, so that speakers need to do more work to accomplish
reference to something that has not been mentioned
recently (cf. [Givon 1983, Chafe 1987]; the explanation
presumably has to do with general properties of short term
memory storage). For this reason, it is desirable to have a
way to measure the text distance between two items.
There are various ideas &amp;quot;in the air&amp;quot; about what the
appropriate units for measuring distance are (e.g. clauses,
intonation units, conversational turns), and different
measures are clearly appropriate for different text types; so
it is desirable to define distance queries so that the user can
specify the unit to be counted as well as the beginning and
ending patterns (the two items whose separation is being
measured). The result of a simple distance query is natu-
rally an integer; but where the beginning and ending pat-
terns are general, the result should be a list of integers
which can then be averaged. Finally, in order to make it
easy to reference distance information in future queries, it
should be possible to save the distance result as a value of
a feature with a user-specified attribute.
Figure 6 illustrates a query which computes &amp;quot;lookback&amp;quot;
(i.e. the distance between two NPs with the same referent)
for pronouns, using the clause as a unit. It averages the
</bodyText>
<figure confidence="0.99849796875">
Query window
......................
Save query to file
and[...[]..
ref = not #ref 1
4.
Text Wildcards Variables Boolean
\
A0149.443.4.:
cot = clause
id = #X
cat = np
ref = #ref1
Count Highlight Extract Variables Boolean
extract itxt1 and #txt2
to (filename): ambig.dat
Change Query window Save query to file
Old:pattern•iirli
att,parie
. .
Text Wildcards Variables Boolean
(cat=clause)
[...fl
cat=verb ecat=np
id=jV syn role=obi
101,
Text Wildcards Variables Boolean
[—[[1..[]-1]
1 \.
cat=verb‘ &apos;cat=np
id=IV syn role=obi
cat=clause)
</figure>
<page confidence="0.983061">
44
</page>
<bodyText confidence="0.9523215">
lookback value, so that average pronoun lookback can be
compared with e.g. average full noun phrase lookback.
</bodyText>
<figure confidence="0.66677">
Distance query window
Save query to file
</figure>
<figureCaption confidence="0.936892">
Figure 7: A distance query for &amp;quot;mention&amp;quot;.
</figureCaption>
<figure confidence="0.996837516129032">
:fli&apos;iiining pattern pane
•..&amp;quot;:&amp;quot;Endingliattetiti&amp;quot;Paite&apos;
Text Wildcards Variables Boolean
[ 1
Text Wildcards Variables Boolean
[ , 1
lypeprn
I ref IR
-.0nita Dane
Text Wildcards Variables Boolean
[ [ catclause1
(ref=nof
•Action pane
List Sum Average Add—as—feature
Average
Distance query window Save query to file
Begirinitit:PAtfeidipane:
pane
Text Wildcards Variables Boolean
[ 1
Text Wildcards Variables Boolean
El
cat=np
ref =1
menlion=f M
\I ,
cal=text boundary
lype=pseudo
Text 1rddeards Variables Boolean
List Sum Average Add-as-feature
Add-as-feature pi
</figure>
<figureCaption confidence="0.999954">
Figure 6: A distance query for &amp;quot;lookback&amp;quot;.
</figureCaption>
<bodyText confidence="0.999960894736842">
Another significant use for the distance query is to Additional components
compute &amp;quot;mention number&amp;quot;, that is, whether a noun phrase In the future, it would be very desirable to implement the
contains the first, second, third etc. mention of a referent. possibility of adding additional components so that all lin-
This could be accomplished by measuring the distance guistic information need not be stored directly in the text.
between a noun phrase and the beginning of the text, using For instance, it would be nice to allow a lexicon com-
noun phrases referring to the same entity as the unit of dis- ponent, which would contain partial feature specifications
tance. Such a query is illustrated in figure 7 (which of lexical items (including e.g. category, number, tense,
assumes that text boundaries have been marked with semantic class). Queries could then search the lexicon as
pseudo-constituents). well as the text for the presnece of features, obviating the
need to manually perform highly predictable annotation. It
is easy to see how currently available large-scale lexical
databases could be adapted to this purpose, obviating a
large amount of hand annotation. A &amp;quot;structurecon&amp;quot; could
perform a similar function for larger units. These addi-
tions would be particularly useful for applications involv-
ing large amounts of rather similar text; they would be less
useful for small texts where open-class lexical items and
predictable structures are rarely repeated, or for lexically
and structurally diverse texts.
</bodyText>
<subsectionHeader confidence="0.922191">
Hypothesis refinement using queries
</subsectionHeader>
<bodyText confidence="0.9999865">
I will conclude with an example of the way this tool can be
used to generate and test hypotheses about the way linguis-
tic choices are made. Since the tool has not yet been
implemented, I have chosen an area where previous
research has given a good indication of what the answers
to queries would probably be: the introduction into dis-
</bodyText>
<page confidence="0.997148">
45
</page>
<bodyText confidence="0.9996932">
course of referents which are new to the hearer (in the
sense of &amp;quot;nonidentifiable&amp;quot;). The following discussion
draws largely on the results reported in [Du Bois 1980].
The category of nonidentifiable referent can be approxi-
mated by the feature &amp;quot;first mention&amp;quot;. Therefore, the first
step will be to use the method described above to automat-
ically add the &amp;quot;mention&amp;quot; feature to each noun phrase (this
procedure will assign the feature &amp;quot;mention = 0&amp;quot; to first
mentions). Then you can &amp;quot;eyeball&amp;quot; all first mentions in
context by using the following query:
</bodyText>
<figure confidence="0.403703">
(8) [ #X ]cat.np■mention=0
highlight #X
</figure>
<bodyText confidence="0.9992544">
Let&apos;s say you notice that many, but not all, first mentions
have an indefinite article. You wonder what proportion of
the data is accounted for by simply stating &amp;quot;first mentions
have an indefinite article&amp;quot;. You can determine this by
means of the following series of queries:
</bodyText>
<figure confidence="0.932511076923077">
(9) [#X &amp;quot;a&amp;quot; or &amp;quot;an&amp;quot; 1
,cat=np, mention=1
count #X
&apos;How many first mentions are marked with an indefinite
article?&apos;
(10) [#Y not &amp;quot;a&amp;quot; and not &amp;quot;an&amp;quot; 1
,cat=np, mention=0
count #Y
&apos;How many first mentions don&apos;t have an indefinite
article?&apos;
(11) [ta &amp;quot;a&amp;quot; or &amp;quot;an&amp;quot; 1
,cat=np, mention=not 0
count #Z
</figure>
<bodyText confidence="0.998005727272727">
&apos;How many things marked with indefinite articles aren&apos;t
first mentions?&apos;
Suppose you discover that, although all NPs with indefi-
nite articles are first mentions, and most first mentions
have an indefinite article, there is a large number of first
mentions which don&apos;t have an indefinite article. The next
step might be to highlight these so you can view them in
context and look for shared characteristics. For instance,
you might notice that plurals don&apos;t have indefinite articles.
You can then modify the original tests to check for number
as well, e.g.
</bodyText>
<equation confidence="0.6618755">
(12) [#Y not &amp;quot;a&amp;quot; and not &amp;quot;an&amp;quot; ..
cat=np, mention=0, num=sing
</equation>
<bodyText confidence="0.97772952631579">
highlight and count #Y
&apos;How many singular first mentions don&apos;t have an indefi-
nite article? Show them to me.&apos;
The response to this query might well reveal another
large class of first mentions which are actually marked
with a definite article. These are referents which can be
interpreted as &amp;quot;accessible&amp;quot; to the hearer by virtue of being
closely associated with (e.g. a part of) something that has
already been mentioned. One way to cope with these
instances is to insert &amp;quot;pseudo-constituents&amp;quot; into the text at
the point where the associated entity is mentioned, and re-
running the mention-number query. These references
would no longer count as first mentions, and a retry of
query 12 would reveal a much smaller number of excep-
tions.
This example could be extended, but it is hoped that it
will illustrate the way such a tool could be used for quick
and easy exploration of the functional attributes of linguis-
tic forms.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999642666666667">
This paper was improved by comments from Sandy
Thompson, Randy Sparks, and an anonymous reviewer.
They are not responsible for remaining faults.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.956572909090909">
[Chafe 1987] Chafe, Wallace L. Cognitive constraints on
information flow. In Russell S. Tomlin (editor),
Coherence and grounding in discourse. (Typological
Studies in Language 11). Amsterdam: Benjamins,
1987.
[Du Bois 1980] Du Bois, John W. Beyond definiteness:
the trace of identity in discourse. In Wallace Chafe
(editor), The Pear Stories, 203-275. Norwood: Ablex,
1980.
[Oivon 1983] Givon, Talmy, editor. Topic continuity in
discourse. Amsterdam: Benjamins, 1983.
</reference>
<page confidence="0.999601">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.022288">
<title confidence="0.996011">Natural Discourse Hypothesis Engine</title>
<author confidence="0.910431">Susanna</author>
<affiliation confidence="0.89786">Department of</affiliation>
<author confidence="0.515431">Campus Box</author>
<affiliation confidence="0.999365">University of</affiliation>
<address confidence="0.998955">Boulder, Colorado 80309</address>
<abstract confidence="0.995453978417266">As text generation systems get more sophisticated and capable of producing a wider syntactic and lexical range, the issue of how to choose among available grammatical options (preferably in a human-like way) becomes more pressing. Thus, devisers of text generation systems are frequently called upon to provide their own analyses of the discourse function of various kinds of alternations. In this paper, I describe a proposed research tool which is designed to help a researcher explore and analyze a natural-language &amp;quot;target text&amp;quot; in order to determine the contextual factors that predict the choice of one or another lexical item or grammatical feature. The project described in this paper is still at the drawing-board stage; I welcome suggestions about ways it could be changed or expanded to fulfill particular analytic needs. Theoretical preliminaries While some aspects of a natural-language text are determined by the nature of the information a speaker wishes to convey to a hearer, there are many more aspects that seem to be determined by certain cognitive needs that the hearer has. Speakers tailor their output to the informational resources and deficiencies of the hearer in several ways: by adjusting the amount of information they make explicit, by arranging new information relative to old information in a maximally helpful way, and by giving special marking to information that may be difficult for the hearer to access for a variety of reasons. It is these strategies that give rise to the wide variety of syntactic and lexical resources of any natural language for saying the &amp;quot;same thing&amp;quot; in different ways. We can call the relation between lexicogrammatical features and the speaker&apos;s communicative goals in choosing those features the &amp;quot;discourse functions&amp;quot; of the features. For any particular alternation, then, the best predictor of the speaker&apos;s choice should be a model of the cognitive state of the hearer. Unfortunately, neither human speakers nor computer systems have direct access to the hearer&apos;s mind. But linguists have long realized that we do have access to a fair approximation of an important subset of the information the hearer possesses at a given point in a discourse: namely the text which has been produced up to that point. ([Chafe 1987] and [Givon 1983] are two contemporary expressions of that principle.) And in fact we can make fairly good predictions of lexico-grammatical choices based on inferences that come from the nature of the preceding text. For instance, a referent that has been referred to in the previous clause is likely to receive minimal coding (a pronoun or zero, depending on syntactic considerations). But this principle can be overridden by the presence of other factors that interfere with the accessibility of the referent — e.g. a paragraph break or another competing referent — resulting in the use of a full noun phrase. Or, to give another example, a speaker is likely to use special syntax (such as the &amp;quot;presentative&amp;quot; or &amp;quot;there is...&amp;quot; construction) to introduce a referent that will be prominent in the following discourse; so a hearer is likely to have easier access to a referent introduced in that way than one that has been introduced &amp;quot;casually&amp;quot;, e.g. in a prepositional phrase. Therefore, subsequent references to a referent that has been introduced with a presentative are more likely to be pronouns than noun phrases. These are all factors that can be discerned in the preceding text and are taken into account by speakers as affecting the nature of the hearer&apos;s expectations. Therefore under these perturbing circumstances the speaker can decide to use a fuller reference, e.g. a proper name or noun phrase. Figure 1 illustrates the relation between the discourse produced by the speaker, the hearer&apos;s mental state, and the speaker&apos;s model of the hearer. In real face-to-face interaction, the hearer can influence the speaker&apos;s model of her or him in more direct ways — e.g. by verbal and nonverbal indications of agreement, understanding, protest, or confusion. But this two-way channel is available neither to human writers nor to computer text generators. 39 goats mental Speaker&apos;s model of hearer&apos;s state X 9 Figure 1: A model of communication. The fact that we can draw inferences from preceding text about the hearer&apos;s state justifies the methodology common in functional linguistics whereby the linguist investigates the function of a particular morpho-syntactic alternation by attempting to discover correlations in natural texts between that alternation and various features of the context. This method is also likely to be the one which will lead to the best results for text generation specialists, since prior text is one information source any computasystem has to as a guide in making future decisions. There are, of course, currently available several largescale databases of English texts of various kinds, some of which have been provided with various amounts and kinds of coding (from word-class tagging to syntactic analysis). However, for many kinds of discourse work these databases have not been useful. On the one hand, such databases are often not available in languages other than English; on-The other, the coding that is provided assumes an analysis which may not meet the needs of a particular user. In fact it is often the case that aspects of the syntactic analysis depend on a functional analysis having already been done; so a pre-coded database may contain assumpabout the answers to the same it is supposed to be helping to solve. The tool described here is designed for the user who is satisfied with a relatively small amount of data, but wants to have total control over the analysis of the corpus. Currently, functional linguists often enter their text data into commercial relational database tools, such as DBase and Paradox. However, such tools have been designed with other kinds of applications in mind, and therefore there are many properties of natural language texts which these tools can only capture by awkward or indirect means. Specifically, natural language texts, unlike client or sales lists, are simultaneously and hierarchical. the query languages available with such database products are generally good at extracting the kind of information from texts which is least useful to linguists, but require considerable ad hoc programming to extract the kind of information most relevant to our needs. The tool proposed here, conversely, should answer most easily the questions a discourse analyst wants to ask most frequently. These problems in representing certain crucial aspects of natural language texts computationally using off-theshelf tools have sent many linguists back to marking up xeroxed texts by hand with colored pencils. The approach outlined here is intended to combine the advantages of the colored-pencil technique (spontaneity, flexibility, and direct involvement with the whole text) with the advantages of computational techniques (quick and painless identification, compilation, and comparison of various features of text units). Description of the tool The tool proposed in this paper will aid in the generation hypotheses concerning discourse function of a lexico-grammatical feature (or combination of features) by researcher isolate instances of that feature and view them in relation to various aspects of its discourse context. When the researcher has arrived at and stated a working hypothesis about which features of the context are most relevant to predicting the targeted feature, the system will be able to test the hypothesis against the data and provide feedback by displaying both predicted and non-predicted occurrences of the target feature. Further refinements can then be made until the fit between the researcher&apos;s hypothesis and the actual text is as good as possible. The hypothesis can then be integrated into a text generation system with some assurance that the lexicogrammatical choices made by the system will approximate those made by a human with a similar text-production task. In order for the tool to be able to recognize and various features — including relatively covert information such as the reference of noun phrases and the mood of a sentence as well as relatively overt cues such as lexical items and structure — the user must first annotate the text, indicating a) its hierarchical structure, and b) a set of features (attribute/value pairs) associated with each constituent. The annotation will be largely manual (though aspects of it can be automated, as will be explained), because we want the tool itself to be neutral as 40 to the theoretical assumptions inherent in any particular analysis. Thus, it will be possible to use this tool to test among other things the comparative predictive usefulness of various possible analyses of a text. Using the tool will have the following phases: Annotation: constituents, and for each constituent indicate its category membership and any other desired features. The amount and type of annotation is entirely up to the user; it will never be necessary to include any more detail than is relevant for the current analysis. Constituents may be anything from words to paragraphs or episodes. Hypothesis formation: the target feature and context in various arrangements. Hypothesis testing: state a in terms of a pattern in the context that predicts the occurrence of the target feature. Hypothesis refinement: changes in hypothesis, annotation, or both and retest against the same data; extend analysis by testing against different data. 1. Annotation An example text showing segmentation and sample feature specification associated with three of the constituents is given in figure 2. [[We] [went] [from [Kobe]] [to [Shanghai],] [[from [Shanghai]] [we] [went] [to [Singapore]].] [[and] [there] [I] [still remember] [[that] f[Pcople].[wilthe ship]]] [would toss] [English pennies] [overboard]].] N [[[and] [the [natives] [in [sampans]]] [would see] [it] [[0] [coming].]] [dive].) [[and] up]] [[0] [having caught] [the p ny] - [[as] [it] [sank] [into [the sea Figure 2: Annotated text. Annotation will have the following steps: segmentation, category specification, further feature specification, and dependency specification. Furthermore, &amp;quot;pseudoconstituents&amp;quot; may be added and specified at this stage, as explained below. Segmentation: off a stretch of text to be considered a unit. Initially, allowable units must be continuous and properly contained inside other units; but eventually it would be nice to allow two kinds of violations to this principle, namely discontinuous constituents and overlapping constituents. Category specification: the category of the constituent. The category will be stored as an attribute-value pair (with the attribute &amp;quot;category&amp;quot;) like other features. At the time of category specification, the constituent will be assigned a unique ID, also an attribute-value pair (with the attribute &amp;quot;id&amp;quot;). Further feature specification: further attribute-value pairs. In order to ensure consistency the user will be prompted with a list of a) attributes that have previously been used with that category, and b) values that have previously been associated with that attribute. The user may select from these lists or enter a new attribute or value. Dependency relations: kinds of relations between constituents are best treated by specifying dependency relations between the two items, i.e. by providing a labelled pointer linking the two. Syntactic analyses differ from each other in what kinds of relations are treated as constituency relations (e.g. the relationship of sister nodes in a tree) and what kinds are treated as pure dependency relations. &amp;quot;Pseudo-constituent&amp;quot; insertion: some purposes, it may be desirable to allow the insertion of &amp;quot;pseudoconstituents&amp;quot; in the text, i.e. phonologically null &amp;quot;elements&amp;quot; that can be inferred from the surface text, or boundaries which aren&apos;t marked by any explicit linguistic element (such as &amp;quot;episode boundaries&amp;quot; or possibly the beginning and end of the text). Examples of null elements which can be tracked in discourse as if they were overt include &amp;quot;zero pronouns&amp;quot;, inferrable propositions, and elements of frames or schemas. Pseudo-constituents can be inserted directly into the text like actual constituents, but they should have distinctive marking in their feature specification to indicate their abstract status; in the examples I have distinguished them by the feature &amp;quot;type = pseudo&amp;quot;. category = clause Id = c111 mol. type = finite relation = time &apos;main = CLI 01 category= np Id = np15 type = pseudo number = plural referent = NATIVES status = generic syn. role = subject = = actor lverb = V71 class = human category = np Id = npt3 number = plural referent = SHIP—FOU( status = generic syn. role = subject (verb = V4) sem. role = agent (verb = V4) class = human 41 Figure 3 illustrates a possible implementation (using pop-up menus) of the process of feature specification. face which will allow the user to open a &amp;quot;query window&amp;quot;, in which he or she can enter a partial specification of one or more constituents to be matched against the text, using the same entry techniques as are used to enter, edit, and annotate actual text. This approach has the advantage of letting the user make use of what he or she already knows about the annotation process to specify the target pattern, and furthermore it allows the identification of highly complex patterns in an intuitive way. A pattern specification will contain the following kinds of elements: 1. Actual structure (brackets), text, and feature bundles. gory = category values: noun verb preposition adjective adverb complementizer noun phrase verb group prepositional phrase clause category = noun phrase number = number values: singular plural mass = noun phrase 1 noun phrase attributes: number referent status sYnrole sem. role Figure 3: Feature specification. Queries Having annotated a text to the desired degree, it is then possible to start fishing around for an analysis of the distributional characteristics of the feature or combination of features of interest to the analyst (henceforth &amp;quot;target pattern&amp;quot;). Experience has shown that the most useful ways to do this is to get an idea of the overall frequency of the phenomenon, and then to &amp;quot;eyeball&amp;quot; the target pattern in various kinds of context. Thus, the tool should have the following capabilities: Counting: the number of instances of the target pattern. Highlighting: the whole text, with constituents which match the target pattern highlighted by color or a distinctive font. Extraction: constituents which match the target pattern into a file of the same type as the text file. It will then be possible to print out the output file, or to subset it further by performing additional highlighting or extraction queries on it. Basic pattern specification. These operations require a for describing the target pattern. I propose an inter- 2. Variables which stand for text or features, used to indicate relationships between parts of the pattern, or to specify to parts of the pattern to be acted on (counted, highlighted, extracted). 3. Various kinds of wildcard elements which will match unspecified strings of structure or text. 4. Operators (and, or, not) which specify relations between features, text, variables, or whole patterns. It should be possible to save a query in a file for future editing and re-use. This avoids repetitive typing of the same query, while permitting refinement and adjustment of queries that don&apos;t give the desired results the first time. It also allows the creation of a series of related queries by copying and slightly altering a prototype. Variables (indicated here by strings of alphanumerics starting with &amp;quot;#&amp;quot;) are used to specify text or elements of feature bundles for reference elsewhere, either to be matched or acted upon. A text variable is interpreted as matching all the text in the constituent that contains it (including text in embedded constituents). For instance, the following two queries would match constituents containing two elements of the same category conjoined by &amp;quot;and&amp;quot;, as in &amp;quot;Jill and Liza&amp;quot; or &amp;quot;sang off-key and danced the waltz&amp;quot;. The (a) version would highlight the whole conjoined phrase; the (b) version would highlight each of the conjoined phrases, but not &amp;quot;and&amp;quot;. (The dots are wildcards, explained below. In these and the following examples, feature specifications are indicated in small italics; literal text is enclosed in quotes.) [ #txt highlight #txt [ [#txt1 [#txt2 ..]cat=#c highlight #txt1 and #txt2 42 An analysis of the types of queries I anticipate wanting to make reveals that a fairly diverse set of wildcards is necessary. Structure wildcards match elements of structure, i.e. combinations of brackets. Text and features are ignored by these wildcards (see below). Two types of structure wildcards are provided. The first, composed of periods, stops at the first instance of the pattern in the string; the second, composed of exclamation marks, finds all instances of the pattern. 1st All match matches Depth: a series of all right or all left brackets !! a series of matching left and right brackets (and their contents) !!! Any material The following examples illustrate the use of structure wildcards: [ [ #X ]cat=ciause X matches a top-level NP which is the first constituent of a clause. It matches &amp;quot;Jill&amp;quot; in &amp;quot;Jill ate the peach&amp;quot;, but nothing in &amp;quot;On the next day Jill ate the peach&amp;quot; (because the first top-level element is a prepositional phrase, not a noun phrase). [ . [ #X ]cat=c1ause [ ! [ #X ]cat=c1ause X matches an NP which is part of the first constituent in a clause, no matter how deeply it is embedded. In the sentence &amp;quot;On the day after the fire Jill ate the peach&amp;quot;, both (a) and (b) will match &amp;quot;the day after the fire&amp;quot;, and the (b) version will also match &amp;quot;the fire&amp;quot;. (4a) [ [ #X 1 ]cat=clause (4b) [ !! [ #X 1 X matches the first NP which is a top-level constituent of the clause; in (b), it matches all NPs which are top-level constituents of the clause. In the sentence &amp;quot;On the day after the fire Jill ate the peach&amp;quot;, both versions will match &amp;quot;Jill&amp;quot;; (b) could also match &amp;quot;the peach&amp;quot; if the sentence was given a &amp;quot;flat&amp;quot; structural analysis (without a verb phrase). [ [ #X ]cat=c1ause [ !!! [ #X ]cat.dause X matches the first noun phrase in a clause (no matter where it is); (b) matches every noun phrase in a clause. In &amp;quot;Much to the surprise of her neighbors, Jill ate the peach&amp;quot;, (a) will match &amp;quot;the surprise of her neighbors&amp;quot;, while (b) will additionally match &amp;quot;her neighbors&amp;quot;, &amp;quot;Jill&amp;quot;, and &amp;quot;the peach&amp;quot;. Text/feature wildcards are used both to match actual text and to match elements of feature bundles (attributes and values). (nothing) Matches any text/feature or no text/feature Matches no text (the absence of text) Matches any text, attribute, or value The following examples show how these wildcards could be used: [ #X @ [ ]cat=np X matches a noun phrase whose first element is a clause. It would match &amp;quot;that he came&amp;quot; in &amp;quot;that he came surprised me&amp;quot;, but not in &amp;quot;the fact that he came surprised me&amp;quot;. [ #X X matches any constituent that has a referent specified. Figure 4 illustrates one way windows and menus could be used to simplify the query process. The query window is divided into two panes, a &amp;quot;pattern pane&amp;quot; and an &amp;quot;action pane&amp;quot;. The various kinds of query elements can be either typed in or selected from a set of pull-down menus. This query is designed to extract noun phrases which have the property of &amp;quot;referential ambiguity&amp;quot; in the sense of [Givon 1983]: referents which have been referred to in the immediately previous clause, but where the previous clause also contains another referent of the same semantic (An example would be &amp;quot;Mary saw Jill, and referentially ambiguous, and in fact we would expect a full noun phrase rather than a pronoun in this kind of context.) 43 Figure 4: A query. Using queries to change annotation Extensive experience with feature-coding of text data has shown that there is a large number of features which are predictable from the identity of lexical items and/or combinations of other features. In these cases it is very convenient to be able to partially code a text and to fill in the redundant codes by using queries. Similarly, it is sometimes desirable to use redundant information in an annotated text to change or delete features when you change your mind about the best coding scheme. Furthermore, if it is possible to insert structure boundaries on the of patterns found in the text, it is to perform some structure insertion automatically, giving the user in effect the possibility of using queries to automatically parse the text where this can be done reliably. he most straightforward way to implement this is to allow the user to specify two patterns, an &amp;quot;old&amp;quot; pattern and a &amp;quot;new&amp;quot; pattern. The query illustrated in figure 5 creates a verb phrase constituent wherever a verb is followed by a noun phrase which is its object. Figure 5: A change query. Distance queries. When text is viewed as a window to the cognitive processes of the speaker, it becomes apparent that the distance between e.g. two mentions of a referent or two clauses referring to the same event can be important. This is because the evidence suggests that the cognitive accessibility (to the hearer) of a referent &amp;quot;decays&amp;quot; over time, so that speakers need to do more work to accomplish reference to something that has not been mentioned recently (cf. [Givon 1983, Chafe 1987]; the explanation presumably has to do with general properties of short term memory storage). For this reason, it is desirable to have a way to measure the text distance between two items. There are various ideas &amp;quot;in the air&amp;quot; about what the appropriate units for measuring distance are (e.g. clauses, intonation units, conversational turns), and different measures are clearly appropriate for different text types; so it is desirable to define distance queries so that the user can specify the unit to be counted as well as the beginning and ending patterns (the two items whose separation is being measured). The result of a simple distance query is naturally an integer; but where the beginning and ending patterns are general, the result should be a list of integers which can then be averaged. Finally, in order to make it easy to reference distance information in future queries, it should be possible to save the distance result as a value of a feature with a user-specified attribute. Figure 6 illustrates a query which computes &amp;quot;lookback&amp;quot; the distance between two the same referent) for pronouns, using the clause as a unit. It averages the Query window ...................... Save query to file and[...[].. ref = not #ref 1 4. Text Wildcards Variables Boolean \ cot = clause id = #X cat = np ref = #ref1 Count Highlight Extract Variables Boolean extract itxt1 and #txt2 to (filename): ambig.dat Change Query window Save query to file Old:pattern•iirli . . Text Wildcards Variables Boolean (cat=clause) [...fl id=jV syn role=obi 101, Text Wildcards Variables Boolean [—[[1..[]-1] \. id=IV syn role=obi cat=clause) 44 lookback value, so that average pronoun lookback can be compared with e.g. average full noun phrase lookback. Distance query window Save query to file Figure 7: A distance query for &amp;quot;mention&amp;quot;. • Text Wildcards Variables Boolean Text Wildcards Variables Boolean 1 lypeprn IR -.0nita Dane Text Wildcards Variables Boolean [ (ref=nof •Action pane List Sum Average Add—as—feature Average Distance query window Save query to file</abstract>
<title confidence="0.866294">Text Wildcards Variables Boolean Wildcards Variables</title>
<author confidence="0.851374">El</author>
<abstract confidence="0.9489054375">cat=np ref =1 \I , cal=text boundary lype=pseudo Variables Boolean List Sum Average Add-as-feature Figure 6: A distance query for &amp;quot;lookback&amp;quot;. Another significant use for the distance query is to compute &amp;quot;mention number&amp;quot;, that is, whether a noun phrase contains the first, second, third etc. mention of a referent. This could be accomplished by measuring the distance between a noun phrase and the beginning of the text, using noun phrases referring to the same entity as the unit of dis-tance. Such a query is illustrated in figure 7 (which assumes that text boundaries have been marked with pseudo-constituents). Additional components In the future, it would be very desirable to implement the possibility of adding additional components so that all lin-guistic information need not be stored directly in the text. For instance, it would be nice to allow a lexicon com-ponent, which would contain partial feature specifications of lexical items (including e.g. category, number, tense, semantic class). Queries could then search the lexicon as well as the text for the presnece of features, obviating the need to manually perform highly predictable annotation. It is easy to see how currently available large-scale lexical databases could be adapted to this purpose, obviating a large amount of hand annotation. A &amp;quot;structurecon&amp;quot; could perform a similar function for larger units. These addi-tions would be particularly useful for applications involv-ing large amounts of rather similar text; they would be less useful for small texts where open-class lexical items and predictable structures are rarely repeated, or for lexically and structurally diverse texts. Hypothesis refinement using queries conclude with an example of the way this tool can be used to generate and test hypotheses about the way linguistic choices are made. Since the tool has not yet been implemented, I have chosen an area where previous research has given a good indication of what the answers</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wallace L Chafe</author>
</authors>
<title>Cognitive constraints on information flow.</title>
<date>1987</date>
<booktitle>Coherence and grounding in discourse. (Typological Studies in Language 11).</booktitle>
<editor>In Russell S. Tomlin (editor),</editor>
<location>Amsterdam: Benjamins,</location>
<marker>[Chafe 1987]</marker>
<rawString>Chafe, Wallace L. Cognitive constraints on information flow. In Russell S. Tomlin (editor), Coherence and grounding in discourse. (Typological Studies in Language 11). Amsterdam: Benjamins, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Du Bois</author>
<author>W John</author>
</authors>
<title>Beyond definiteness: the trace of identity in discourse.</title>
<date>1980</date>
<booktitle>In Wallace Chafe (editor), The Pear Stories,</booktitle>
<pages>203--275</pages>
<location>Norwood: Ablex,</location>
<marker>[Du Bois 1980]</marker>
<rawString>Du Bois, John W. Beyond definiteness: the trace of identity in discourse. In Wallace Chafe (editor), The Pear Stories, 203-275. Norwood: Ablex, 1980.</rawString>
</citation>
<citation valid="true">
<date>1983</date>
<booktitle>Topic continuity in discourse.</booktitle>
<editor>Givon, Talmy, editor.</editor>
<location>Amsterdam: Benjamins,</location>
<marker>[Oivon 1983]</marker>
<rawString>Givon, Talmy, editor. Topic continuity in discourse. Amsterdam: Benjamins, 1983.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>