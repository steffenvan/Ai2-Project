<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.8071985">
Abstractive Multi-Document Summarization via Phrase Selection and
Merging∗
</title>
<author confidence="0.6830645">
Lidong Bing§ Piji Lib Yi Liaob Wai Lamb
Weiwei Guot Rebecca J. Passonneaut
</author>
<affiliation confidence="0.974907">
§Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA USA
bDepartment of Systems Engineering and Engineering Management,
The Chinese University of Hong Kong
tYahoo Labs, Sunnyvale, CA, USA
$Center for Computational Learning Systems, Columbia University, New York, NY, USA
</affiliation>
<email confidence="0.886644">
§lbing@cs.cmu.edu, b{pjli, yliao, wlam}@se.cuhk.edu.hk
twguo@yahoo-inc.com, tbecky@ccls.columbia.edu
</email>
<sectionHeader confidence="0.994755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854375">
We propose an abstraction-based multi-
document summarization framework that
can construct new sentences by exploring
more fine-grained syntactic units than sen-
tences, namely, noun/verb phrases. Dif-
ferent from existing abstraction-based ap-
proaches, our method first constructs a
pool of concepts and facts represented by
phrases from the input documents. Then
new sentences are generated by selecting
and merging informative phrases to max-
imize the salience of phrases and mean-
while satisfy the sentence construction
constraints. We employ integer linear op-
timization for conducting phrase selection
and merging simultaneously in order to
achieve the global optimal solution for a
summary. Experimental results on the
benchmark data set TAC 2011 show that
our framework outperforms the state-of-
the-art models under automated pyramid
evaluation metric, and achieves reasonably
well results on manual linguistic quality
evaluation.
</bodyText>
<sectionHeader confidence="0.998809" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994241142857143">
Existing multi-document summarization (MDS)
methods fall in three categories: extraction-based,
compression-based and abstraction-based. Most
∗ The work described in this paper is substan-
tially supported by grants from the Research and De-
velopment Grant of Huawei Technologies Co. Ltd
(YB2013090068/TH138232) and the Research Grant Coun-
cil of the Hong Kong Special Administrative Region, China
(Project Codes: 413510 and 14203414).
The work was done when Weiwei Guo was in Columbia Uni-
versity
summarization systems adopt the extraction-
based approach which selects some original sen-
tences from the source documents to create a short
summary (Erkan and Radev, 2004; Wan et al.,
2007). However, the restriction that the whole sen-
tence should be selected potentially yields some
overlapping information in the summary. To this
end, some researchers apply compression on the
selected sentences by deleting words or phrases
(Knight and Marcu, 2000; Lin, 2003; Zajic et
al., 2006; Harabagiu and Lacatusu, 2010; Li
et al., 2015), which is the compression-based
method. Yet, these compressive summarization
models cannot merge facts from different source
sentences, because all the words in a summary
sentence are solely from one source sentence.
In fact, previous investigations show that
human-written summaries are more abstractive,
which can be regarded as a result of sentence ag-
gregation and fusion (Cheung and Penn, 2013;
Jing and McKeown, 2000). Some works, albeit
less popular, have studied abstraction-based ap-
proach that can construct a sentence whose frag-
ments come from different source sentences. One
important work developed by Barzilay and McK-
eown (2005) employed sentence fusion, followed
by (Filippova and Strube, 2008; Filippova, 2010).
These works first conduct clustering on sentences
to compute the salience of topical themes. Then,
sentence fusion is applied within each cluster of
related sentences to generate a new sentence con-
taining common information units of the sen-
tences. The abstractive-based approaches gather
information across sentence boundary, and hence
have the potential to cover more content in a more
concise manner.
In this paper, we propose an abstractive MDS
framework that can construct new sentences by
</bodyText>
<page confidence="0.944365">
1587
</page>
<note confidence="0.988269">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1587–1597,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999954">
Figure 1: The constituency tree of a sentence from a news document.
</figureCaption>
<bodyText confidence="0.999835477272727">
exploring more fine-grained syntactic units than
sentences, namely, noun/verb phrases (NPs/VPs).
This idea is based on two observations. First, the
major constituent phrases loosely correspond to
the concepts and facts. After reading a set of doc-
uments describing the same topic or event, a per-
son digests these documents as key concepts and
facts in his/her mind, such as “an armed man”
and “walked into an Amish school” from Figure
1. Second, a summary writer re-organizes the key
concepts and facts to form new sentences for the
summary. Accordingly, our proposed framework
has two major components corresponding to the
above observations. The first component creates a
pool of concepts and facts represented by NPs and
VPs from the input documents. A salience score
is computed for each phrase by exploiting redun-
dancy of the document content in a global man-
ner. The second component constructs new sen-
tences by selecting and merging phrases based on
their salience scores, and ensures the validity of
new sentences using a integer linear optimization
model.
The contribution of this paper is two folds. (1)
We extract NPs/VPs from constituency trees to
represent key concepts/facts, and merge them to
construct new sentences, which allows more sum-
mary content units (SCUs) (Nenkova and Passon-
neau, 2004) to be included in a sentence by break-
ing the original sentence boundaries. (2) The de-
signed optimization framework for addressing the
problem is unique and effective. Our optimiza-
tion algorithm simultaneously selects and merges
a set of phrases that maximize the number of cov-
ered SCUs in a summary. Meanwhile, since the
basic unit is phrases, we design compatibility re-
lations among NPs and VPs, as well as other op-
timization constraints, to ensure that the gener-
ated sentences contain correct facts. Compared
with the sentence fusion approaches that compute
salience scores of sentence clusters, our proposed
framework explores a more fine-grained textual
unit (i.e., phrases), and maximizes the salience of
selected phrases in a global manner.
</bodyText>
<sectionHeader confidence="0.727823" genericHeader="method">
2 Description of Our Framework
</sectionHeader>
<bodyText confidence="0.999938375">
We first introduce how to extract NPs and VPs
from constituency trees, and subsequently calcu-
late salience scores for them. Then we formulate
the sentence generation task as an optimization
problem, and design constraints. In the end, we
perform several post-processing steps to improve
the order and the readability of the generated sen-
tences.
</bodyText>
<subsectionHeader confidence="0.997262">
2.1 Phrase Salience Calculation
</subsectionHeader>
<bodyText confidence="0.99996475">
The first component decomposes the sentences in
documents into a set of noun phrases (NPs) de-
rived from the subject parts of a constituency tree
and a set of verb-object phrases (VPs), represent-
ing potential key concepts and key facts, respec-
tively. These phrases will serve as the basic ele-
ments for sentence generation.
We employ Stanford parser (Klein and Man-
ning, 2003) to obtain a constituency tree for each
input sentence. After that, we extract NPs and VPs
from the tree as follows: (1) The NPs and VPs that
are the direct children of the sentence node (repre-
</bodyText>
<page confidence="0.987703">
1588
</page>
<bodyText confidence="0.976424480769231">
sented by the S node) are extracted. (2) VPs (NPs)
in a path on which all the nodes are VPs (NPs)
are also recursively extracted and regarded as hav-
ing the same parent node S. Recursive operation
in the second step will only be carried out in two
levels since the phrases in the lower levels may
not be able to convey a complete fact. Take the
tree in Figure 1 as an example, the corresponding
sentence is decomposed into phrases “An armed
man”, “walked into an Amish school, sent the boys
outside and tied up and shot the girls, killing three
of them”, “walked into an Amish school”, “sent
the boys outside”, and “tied up and shot the girls,
killing three of them”. 1 Because of the recursive
operation, the extracted phrases may have over-
laps. Later, we will show how to avoid such over-
lapping in phrase selection.
A salience score is calculated for each phrase to
indicate its importance. Different types of salience
can be incorporated in our framework, such as
position-based method (Yih et al., 2007), statis-
tical feature based method (Woodsend and Lap-
ata, 2012), concept-based method (Li et al., 2011),
etc. One key characteristic of our approach is
that the considered basic units are phrases instead
of sentences. Such finer granularity leaves more
room for better global salience score by poten-
tially covering more distinct facts. In our imple-
mentation, we adopt a concept-based weight in-
corporating the position information. The con-
cept set is designated to be the union set of un-
igrams, bigrams, and named entities in the docu-
ments. We remove stopwords and perform lemma-
tization before extracting unigrams and bigrams.
The position-based term frequency is used in the
concept weighting scheme. When counting the
frequency, each occurrence of a concept in an in-
put document is weighted with the paragraph po-
sition. The weight larger than 1 is given to the
concept occurrences in the first few paragraphs.
Specifically, the weight of the first paragraph is
B and the weight decreases as the position of the
paragraph increases from the beginning of the doc-
1We only consider the recursive operation for a VP with
more than one parallel sub-VPs, such as the highest VP in
Figure 1. The sub-VPs following modal, link or auxiliary
verbs are not extracted as individual VPs. In addition, we
also extract the clauses functioning as subjects of sentences
as NPs, such as “that clause”. Note that we also mention such
clauses as “noun phrase” although their syntactic labels could
be “SBAR” or “S”.
ument. The weighting function is:
</bodyText>
<equation confidence="0.993984666666667">
�H(p) = ρP ∗ B if p &lt; −(log B/ log ρ)
1 otherwise ,
(1)
</equation>
<bodyText confidence="0.9995672">
where p is the position of the paragraph starting
from 0, from beginning of the document, and ρ is
a positive constant and smaller than 1. Then, the
salience of a phrase is calculated as the summed
weights of its concepts.
</bodyText>
<subsectionHeader confidence="0.99954">
2.2 New Sentence Construction Model
</subsectionHeader>
<bodyText confidence="0.999263111111111">
The construction of new sentences is formulated
as an optimization problem which is able to si-
multaneously generate a group of sentences. Each
new sentence is composed of one NP and at least
one VP, where the NP and VPs may come from
different source sentences. In the process of new
sentence generation, the compatibility relation be-
tween NP and VP and a variety of summarization
requirements are jointly considered.
</bodyText>
<subsectionHeader confidence="0.679077">
2.2.1 Compatibility Relation
</subsectionHeader>
<bodyText confidence="0.999582866666667">
Compatibility relation is designed to indicate
whether an NP and a VP can be used to form a
new sentence. For example, the NP “Police” from
another sentence should not be the subject of the
VP “sent the boys outside” extracted from Figure
1. We use some heuristics to find compatibility,
and then expand the compatibility relation to more
phrases by extracting coreference.
To find coreference NPs (different mentions for
the same entity), we first conduct coreference res-
olution for each document with Stanford corefer-
ence resolution package (Lee et al., 2013). We
adopt those resolution rules that are able to achieve
high quality and address our need for summariza-
tion. In particular, Sieve 1, 2, 3, 4, 5, 9, and 10
in the package are used. A set of clusters are ob-
tained and each cluster contains the mentions that
refer to the same entity in a document. The clus-
ters from different documents in the same topic
are merged by matching the named entities. After
merging, the mentions that are not NPs extracted
in the phrase extraction step are removed in each
cluster. Two NPs in the same cluster are deter-
mined as alternative of each other.
To find alternative VPs, Jaccard Index is em-
ployed as the similarity measure. Specifically,
each VP is represented as a set of its concepts and
the index value is calculated for each pair of VPs.
If the value is larger than a threshold, the two VPs
are determined as alternative of each other.
</bodyText>
<page confidence="0.991949">
1589
</page>
<bodyText confidence="0.943024777777778">
We then define an indicator matrix F|N||V|, in
which F[i, j] = 1 if an NP Ni and a VP Vj come
from the same node S in the constituency tree, oth-
erwise, F[i, j] = 0. Let ˜Ni and ˜Vi represent the al-
ternative phrases of Ni and Vi as described above.
The compatibility matrix ˜F|N||V |is defined as fol-
lows:
r[i, j] = 1 if Ni and Vj are from the same sen-
tence
</bodyText>
<figure confidence="0.978732055555556">
Notation
Description
Noun phrase i and verb phrase i
Selection indicators of Ni and Vi
Co-occurrence indicators of pairs (Ni, Nj) and
(Vi, Vj)
Salience scores of Ni and Vi
Similarity of pair (Ni, Nj) and pair (Vi, Vj)
Ni, Vi
αi, βi
αij, βij
SNi , SV i
N V
Rij , Rij
r|N||V|
˜Ni, ˜Vi
˜r|N||V|
˜γij
</figure>
<figureCaption confidence="0.30942525">
⎧ 1 if Np ∈ ˜Ni ∧ F[i, q] = 1 (2)
⎨⎪⎪⎪⎪ 1 if Vq ∈˜Vj ∧ F[p,j] = 1
˜F[p, q] = 1 if F[p, q] = 1
⎪⎪⎪⎪⎩ 0 otherwise
</figureCaption>
<bodyText confidence="0.5825298">
The alternative phrases of Ni and Vi
˜r[i, j] = 1 means Ni and Vj are compatible for
being used to construct a new sentence
Sentence generation indicator for Ni and Vj if
˜r[i, j] = 1
</bodyText>
<tableCaption confidence="0.993927">
Table 1: Notations.
</tableCaption>
<bodyText confidence="0.9998405">
where ˜F[p, q] = 1 means Np and Vq are compat-
ible/permitted for constructing a new sentence. F˜
is the final compatibility matrix that we use in the
optimization. The first case of Equation 2 implies
that if Np and Ni are coreferent, Np can replace
Ni and serve as the subject of Ni’s VP (i.e., Vq).
The second case implies that if Vq is very similar
to Vj, Vq can be concatenated to Vj’s NP (i.e., Np).
</bodyText>
<subsubsectionHeader confidence="0.886708">
2.2.2 Phrase-based Content Optimization
</subsubsectionHeader>
<bodyText confidence="0.9997965">
The overall objective function of our optimization
formulation to select NPs and VPs is defined as:
</bodyText>
<equation confidence="0.98912775">
Xmax{ XαiSNi − αij(SNi + SNj )RNij
i i&lt;j
βij(SVi + SVj )RV ij},
(3)
</equation>
<bodyText confidence="0.986605225806451">
where αi and βi are selection indicators for the
NP Ni and the VP Vi, respectively. SNi and SVi
are the salience scores of Ni and Vi. αij and βij
are co-occurrence indicators of pairs (Ni, Nj) and
(Vi, Vj). RN and RV are the similarity of pairs
ij ij
(Ni, Nj) and (Vi, Vj). If Ni and Nj are coreferent,
RN ij = 1. Otherwise, the similarity is calculated
with the above Jaccard Index based method. The
notations are summarized in Table 1.
Specifically, we maximize the salience score of
the selected NPs and VPs as indicated by the first
and the third terms in Equation 3, and penalize the
selection of similar NP pairs and similar VP pairs
as indicated by the second and the fourth terms.
Meanwhile, the phrase selection is governed by a
set of constraints so that the selected phrases can
generate valid sentences. The constraints will be
explained in details in Section 2.2.3.
One characteristic of our objective function is
that NPs and VPs are treated differently, i.e., there
are different selection/penalty terms for NP and
VP. Such design enables us to avoid the false
penalty between an NP and a VP. For example, the
algorithm produces two sentences: the first sen-
tence is “the gunman shot ...” with an NP “the
gunman”, and the other sentence has a VP “con-
firmed the gunman died”. Obviously, we should
not penalize the redundancy between them, be-
cause mentioning the gunman is necessary in both
sentences.
</bodyText>
<subsectionHeader confidence="0.846089">
2.2.3 Sentence Generation Constraints
</subsectionHeader>
<bodyText confidence="0.999955291666667">
To summarize the related sentences in the docu-
ments, human writers usually merge the important
facts in different VPs about the same entity into a
single sentence, and omit the trivial facts. Also,
the same entity is likely to be described by coref-
erent NPs. Therefore, in our approach, only one
NP is selected and employed as the subject of the
newly generated sentence, which is then concate-
nated with the merged facts (i.e., VPs). If the com-
patibility entry ˜F[i, j] for Ni and Vj is 1, we de-
fine a sentence generation indicator ˜γij to indicate
whether both Ni and Vj are selected to construct a
new sentence in the summary.
We design the following groups of constraints
to realize our aim of phrase selection and new
sentence construction. The objective function and
constraints are linear, therefore the problem can
be solved by existing Integer Linear Programming
(ILP) solvers such as simplex algorithm (Dantzig
and Thapa, 1997).
NP validity. To maintain the consistency be-
tween the selection indicator α and the compati-
bility entry F˜ for NP Ni, we introduce two con-
straints as follows:
</bodyText>
<equation confidence="0.9439718">
∀i, j, αi ≥ ˜γiji ∀i, X ˜γij ≥ αi. (4)
j
X XβiSVi −
+ i&lt;j
i
</equation>
<page confidence="0.82539">
1590
</page>
<bodyText confidence="0.963324625">
These two constraints work together to ensure the
valid assignment of α according to the compatibil-
ity entry ˜I.
VP legality. Similarly, the following require-
ment guarantees the consistency between the se-
lection indicator β and the compatibility entry I˜
for selected VP Vi:
∀j,
</bodyText>
<equation confidence="0.974323">
� ˜γij = βj. (5)
i
</equation>
<bodyText confidence="0.996065285714286">
The above two constraints jointly ensure that the
selected NPs and VPs are able to form new sum-
mary sentences according to the values of sentence
generation indicators.
Not i-within-i. Two phrases in the same
path of a constituency tree cannot be chosen at the
same time:
</bodyText>
<construct confidence="0.6889145">
if ∃Vk Vj, then βk + βj ≤ 1, (6)
if ∃Nk Nj, then αk + αj ≤ 1.
</construct>
<bodyText confidence="0.993658791666666">
For example, “walked into an Amish school, sent
the boys outside and tied up and shot the girls,
killing three of them” and “walked into an Amish
school” cannot be both incorporated in the sum-
mary, because of the obvious redundancy.
Phrase co-occurrence. These constraints
control the co-occurrence relation of NPs or VPs.
For NPs, we introduce three constraints:
αij − αi ≤ 0,
αij − αj ≤ 0,
αi + αj − αij ≤ 1.
Constraints 7 to 9 ensure a valid solution of NP
selection. The first two constraints state that if the
units Ni and Nj co-occur in the summary (i.e.,
αij = 1), then we have to include them individ-
ually (i.e., αi = 1 and αj = 1). The third con-
straint is the inverse of the first two. Similarly, the
constraints for VPs are as follows:
βij − βi ≤ 0,
βij − βj ≤ 0,
βi + βj − βij ≤ 1.
Sentence number. In abstractive summariza-
tion, we do not prefer to generate many short sen-
tences. This is controlled by:
</bodyText>
<equation confidence="0.9457315">
� αi ≤ K, (13)
i
</equation>
<bodyText confidence="0.967361391304348">
where K is the maximum number of sentences.
Short sentence avoidance. We do not
select the VPs from very short sentences because a
short sentence normally cannot convey a complete
key fact (Woodsend and Lapata, 2012).
if l(S) &lt; M, Vi ∈ S, then βi = 0, (14)
where M is the threshold of the sentence length.
Pronoun avoidance. We exclude the NPs
that are pronouns from being selected as the sub-
ject of the new sentences. As previously observed
(Woodsend and Lapata, 2012), pronouns are nor-
mally not used by human summary writers. It is
because the summary is short and the narration
relation of sentences is relatively simple so that
pronouns are not needed. Moreover, in automatic
summary, pronouns will cause ambiguity in the
summary, especially when the sentence order is
automatically determined. Therefore, we model
the constraint as:
if Ni is pronoun, then αi = 0. (15)
Length constraint. The overall length of
the selected NPs and VPs is no larger than a limit
L:
</bodyText>
<equation confidence="0.9786465">
� {l(Ni) ∗ αi} + � {l(Vj) ∗ βj} ≤ L, (16)
i j
</equation>
<bodyText confidence="0.8964">
where l() is the word-based length of a phrase.
</bodyText>
<subsectionHeader confidence="0.99599">
2.3 Postprocessing
</subsectionHeader>
<bodyText confidence="0.999987058823529">
Recall that we require that one NP and at least
one VP compose a sentence. Thus, we form a
raw sentence with a selected NP as the subject
followed by the corresponding selected VPs that
are indicated by sentence generation indicator ˜γij
having the value 1. The VPs in a summary sen-
tence are ordered according to their natural order
if they come from the same document. Otherwise,
they are ordered according to the timestamps of
the corresponding documents. After that, if the to-
tal length is smaller than L, we add conjunctions
such as “and” and “then” to concatenate the VPs
for improving the readability of the newly gener-
ated sentences. The pseudo-timestamp of a sen-
tence is defined as the earliest timestamp of its
VPs and the sentences are ordered based on their
pseudo-timestamps.
</bodyText>
<subsectionHeader confidence="0.992743">
2.4 Relation to Existing MDS Approaches
</subsectionHeader>
<bodyText confidence="0.996742666666667">
Many existing extraction-based and compression-
based MDS approaches could be regarded as spe-
cial cases under our framework: (1) To simulate
</bodyText>
<page confidence="0.961349">
1591
</page>
<bodyText confidence="0.9998485">
extraction-based summarization, we just need to
constrain that the highest NP and the highest VP
from the same sentence are selected simultane-
ously. In addition, no NPs and VPs in lower lev-
els can be selected. Thus, the output only con-
tains the original sentences of the source docu-
ments. (2) To simulate compression-based sum-
marization, we can adapt our framework to con-
duct sentence selection and sentence compression
in a joint manner. Specifically, we only need to re-
strict that the NP and VPs of a summary sentence
must come from the same original sentence.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996624">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999935666666667">
The data set of traditional summarization task in
Text Analysis Conference (TAC) 2011 is used to
evaluate the performance of our approach. This
data set is the latest one and it contains 44 topics.
Each topic falls into one of 5 predefined event cat-
egories and contains 10 related news documents.
There are four writers to write model summaries
for each topic.
The data set of traditional summarization task in
TAC 2010 is employed as the development/tuning
data set. This data set contains 46 topics from the
same predefined categories. Each topic also has
10 documents and 4 model summaries.
Based on the tuning set, the key parameters of
our model are set as follows. The constants B and
ρ in the weighting function are set to 6 and 0.5
repectively. The similarity threshold in obtaining
the alternative VPs is 0.75. We did not observe sig-
nificant difference between cosine similarity and
Jaccard Index.
We mainly evaluate the system by pyramid eval-
uation. To gain a comprehensive understanding,
we also evaluate by ROUGE evaluation and man-
ual linguistic quality evaluation.
</bodyText>
<subsectionHeader confidence="0.99383">
3.2 Results with Pyramid Evaluation
</subsectionHeader>
<bodyText confidence="0.9996236">
The pyramid evaluation metric (Nenkova and Pas-
sonneau, 2004) involves semantic matching of
summary content units (SCUs) so as to recognize
alternate realizations of the same meaning. Differ-
ent weights are assigned to SCUs based on their
frequency in model summaries. A weighted inven-
tory of SCUs named a pyramid is created, which
constitutes a resource for investigating alternate
realizations of the same meaning. Such property
makes pyramid method more suitable to evalu-
</bodyText>
<table confidence="0.999864">
Auto-pyr Auto-pyr Rank in
System (Th: .6) (Th: .65) TAC 2011
Our 0.905 0.793 NA
22 0.878 0.775 1
43 0.875 0.756 2
17 0.860 0.741 3
</table>
<tableCaption confidence="0.9903845">
Table 2: Comparison with the top 3 systems in
TAC 2011.
</tableCaption>
<bodyText confidence="0.955558925">
ate summaries. Another widely used evaluation
metric is ROUGE (Lin and Hovy, 2003) and it
evaluates summaries from word overlapping per-
spective. Because of the strict string matching, it
ignores the semantic content units and performs
better when larger sets of model summaries are
available. In contrast to ROUGE, pyramid scor-
ing is robust with as few as four model summaries
(Nenkova and Passonneau, 2004). Therefore, in
recent summarization evaluation workshops such
as TAC, the pyramid is used as the major metric.
Since manual pyramid evaluation is time-
consuming, and the exact evaluation scores are
not reproducible especially when the assessors for
our results are different from those of TAC, we
employ the automated version of pyramid pro-
posed in (Passonneau et al., 2013). The automated
pyramid scoring procedure relies on distributional
semantics to assign SCUs to a target summary.
Specifically, all n-grams within sentence bounds
are extracted, and converted into 100 dimension
latent topical vectors via a weighted matrix fac-
torization model (Guo and Diab, 2012). Simi-
larly, the contributors and the label of an SCU
are transformed into 100 dimensional vector rep-
resentations. An SCU is assigned to a summary
if there exists an n-gram such that the similarity
score between the SCU low dimensional vector
and the n-gram low dimensional vector exceeds
a threshold. Passonneau et al. (2013) showed
that the distributional similarity based method pro-
duces automated scores that correlate well with
manual pyramid scores, yielding more accurate
pyramid scores than string matching based auto-
mated methods (Harnly et al., 2005). In this pa-
per, we adopt the same setting as in (Passonneau
et al., 2013): a 100 dimension matrix factorization
model is learned on a domain independent corpus,
which is drawn from sense definitions of WordNet
and Wiktionary2, and Brown corpus. We exper-
</bodyText>
<footnote confidence="0.971345">
2http://en.wiktionary.org/
</footnote>
<page confidence="0.962673">
1592
</page>
<table confidence="0.998963666666667">
ROUGE-2 ROUGE-SU4
System P R F1 P R F1
Our 0.117 0.117 0.117 0.148 0.147 0.148
22 0.112 0.114 0.113 0.147 0.150 0.148
43 0.132 0.135 0.134 0.162 0.166 0.164
17 0.128 0.131 0.129 0.157 0.160 0.159
</table>
<tableCaption confidence="0.999849">
Table 3: Performance under ROUGE metric.
</tableCaption>
<bodyText confidence="0.998691321428572">
iment with 2 threshold values, i.e., 0.6 and 0.65,
similar to those used in (Passonneau et al., 2013).
The top three systems in TAC 2011 evaluated
with manual pyramid score were System 22 (Li et
al., 2011), 43, and 17 (Ng et al., 2011). Table 2
shows the comparison with them under the auto-
mated pyramid evaluation. Our method achieves
the best results in both thresholds, which means
that our method is able to find more semantic con-
tent units (SCUs) than the state-of-the-art system
in TAC 2011. In addition, paired t-test (with p &lt;
0.01) comparing our model with the best system
in TAC 2011, i.e., System 22, shows that the per-
formance of our model is significantly better. It is
worth noting that the three systems used additional
external linguistic resources: System 22 used a
Wikipedia corpus for providing domain knowl-
edge, System 17 and 43 defined some category-
specific features. Without any domain adaption,
our framework can still achieve encouraging per-
formance.
We calculate Pearson’s correlation to measure
how well the automatic pyramid approximates the
manual pyramid scores for 50 system submissions
in TAC 2011. The values are 0.91 and 0.93 for
thresholds 0.6 and 0.65 respectively. It demon-
strates that the automated pyramid is reliable to
differentiate the performance of different methods.
</bodyText>
<subsectionHeader confidence="0.987118">
3.3 Results with ROUGE Evaluation
</subsectionHeader>
<bodyText confidence="0.999982181818182">
As mentioned above, we favor the pyramid evalua-
tion over the ROUGE score because it can measure
the summary quality beyond simply string match-
ing. Here, we also provide ROUGE score for our
reference. ROUGE-1.5.5 package3 is employed
with the same parameters as in TAC. The results
are summarized in Table 3. Our performance is
slightly better than System 22, and it is not as good
as System 43 and 17. The reason is that System 43
and 17 used category-specific features and trained
the feature weights with the category information
</bodyText>
<footnote confidence="0.649322">
3http://www.berouge.com/Pages/default.aspx
</footnote>
<bodyText confidence="0.999925083333333">
in TAC 2010 data. These features help them se-
lect better category-specific content for the sum-
mary. However, the usability of such features de-
pends on the availability of predefined categories
in the summarization task, as well as the avail-
ability of training data with the same predefined
categories for estimating feature weights. There-
fore, the adaptability of these methods is limited to
some extent. In contrast, our framework does not
define any category-specific feature and only uses
TAC 2010 data to tune the parameters for general
summarization purpose.
</bodyText>
<subsectionHeader confidence="0.991359">
3.4 Linguistic Quality Evaluation
</subsectionHeader>
<bodyText confidence="0.99999165625">
The linguistic quality of summaries is evaluated
using the five linguistic quality questions on gram-
maticality (Q1), non-redundancy (Q2), referential
clarity (Q3), focus (Q4), and coherence (Q5) in
Document Understanding Conferences (DUC). A
Likert scale with five levels is employed with 5 be-
ing very good with 1 being very poor. A summary
was blindly evaluated by three assessors on each
question. System 22 performed better than Sys-
tem 43 and 17 in TAC 2011 on the evaluation of
readability, which is an aggregation of the above
questions. Considering the intensive labor force of
manual assessment, we only conduct comparison
with System 22.
The results are given in Table 4. On average,
the two systems perform very closely. System 22
is an extraction-based method that picks the orig-
inal sentences, hence it achieves higher score in
Q1 grammaticality, while our approach has some
new sentences with grammar mistakes, which is a
common problem for abstractive methods and de-
serves more future research effort. For Q4 focus,
our score is higher than System 22, which reveals
that our summary sentences are relatively more co-
hesive. The score of Q3 referential clarity shows
that the referential relation is basically clear in our
summaries, even when new sentences are automat-
ically generated. In general, ignoring the gram-
maticality scores, our system still performs better
than System 22. Specifically, the average scores
of our system and System 22 on the last four ques-
tions are 3.37 and 3.33 respectively.
</bodyText>
<sectionHeader confidence="0.99668" genericHeader="method">
4 Qualitative Results
</sectionHeader>
<subsectionHeader confidence="0.999896">
4.1 Analysis of Summary Sentence Type
</subsectionHeader>
<bodyText confidence="0.9995285">
There are three types of sentences in the sum-
maries generated by our framework, namely, new
</bodyText>
<page confidence="0.918094">
1593
</page>
<table confidence="0.999599">
System Q1 Q2 Q3 Q4 Q5 AVG
Our 3.67 3.50 3.90 3.23 2.83 3.43
22 4.13 3.50 3.97 2.97 2.87 3.49
</table>
<tableCaption confidence="0.9999">
Table 4: Evaluation of linguistic quality.
</tableCaption>
<bodyText confidence="0.9999335">
sentences, compressed sentences, and original
sentences. A new sentence is constructed by merg-
ing the phrases from different original sentences.
A compressed sentence is generated by deleting
phrases from an original sentence. An original
sentence in the summary is directly extracted from
the input documents.
The percentage of different types of sentences
in our summaries is calculated. About 33% of the
summary sentences are newly constructed. This
demonstrates that our framework has good capa-
bility of merging phrases from the original sen-
tences so as to convey more information in com-
pacted summaries. In addition, about 44% of the
summary sentences are generated by compression.
It shows a unique characteristic of our framework:
sentence construction and sentence compression
are conducted in a unified model.
</bodyText>
<subsectionHeader confidence="0.999796">
4.2 Case Study
</subsectionHeader>
<bodyText confidence="0.9989955">
Table 5 shows the summary of the first topic,
i.e., “Amish Shooting”, by our framework.
The summary sentence ID and the sentence
type are given in the form of “[summary
sentence TD: sentence type]”. Each
selected phrase and the original sentence ID
where the phrase originated are given in the
form of “{selected phrase (original
sentence TD)}”. There are three compressed
sentences with IDs 1, 2, and 4, one new sentence
with ID 3, and two original sentences with IDs 5
and 6.
The new sentence is constructed from the fol-
lowing original sentences in which the extracted
NPs and VPs are indicated with colored parenthe-
ses:
</bodyText>
<listItem confidence="0.964357875">
(84): On Monday morning, (NP Charles Carl
Roberts IV) (VP (VP entered the West Nickel
Mines Amish School in Lancaster County) and
(VP shot 10 girls), (VP killing five)).
(85): (NP Roberts) (VP killed himself as police
stormed the building).
(150): (NP Roberts) (VP left what they de-
scribed as rambling notes for his family).
</listItem>
<bodyText confidence="0.986238529411765">
[1:C] {An armed man (25)} {walked into
an Amish school (25)} {tied up and shot the
girls, killing three of them. (25)} [2:C]
{A man who laid siege to a one-room Amish
schoolhouse (64)} {told his wife shortly be-
fore opening fire that he had molested two young
girls who were his relatives decades ago (64)}
{was tormented by dreams of molesting again.
(64)} [3:N] {Charles Carl Roberts IV (84)}
{killed himself as police stormed the building
(85)} {left what they described as rambling
notes for his family. (150)} [4:C] {The gun-
man (145)} {was not Amish (145)} {had not
attended the school. (145)} [5:O] {The shoot-
ings (148)} {occurred about 10:45 a.m.(148)}
[6:O] {Police (149)} {could offer no explana-
tion for the killings. (149)}
</bodyText>
<tableCaption confidence="0.884658">
Table 5: The summary of “Amish Shooting” topic.
</tableCaption>
<bodyText confidence="0.989368038461538">
The NPs of these sentences are coreferent so that
some of their VPs are merged and concatenated
with one NP, i.e., “Charles Carl Roberts IV”.
The summary sentences with IDs 1, 2, and 4
are compressions from the following original sen-
tences respectively:
(25): (NP An armed man) (VP(VP walked into
an Amish school), (VP sent the boys outside)
and (VP tied up and shot the girls, killing three
of them)), (NP authorities) (VP said).
(64): (NP(NP A man)who laid siege to a
one-room Amish schoolhouse),(VP killing five
girls),(VP(VP told his wife shortly before open-
ing fire that he had molested two young girls who
were his relatives decades ago)and(VP was tor-
mented by “dreams of molesting again”)),(NP
authorities)(VP said Tue).
(145): According to media reports, (NP the
gunman) (VP(VP was not Amish) and (VP had
not attended the school)).
Some uncritical information is excluded from
the summary sentences, such as “sent the boys
outside”, “authorities said”, etc. In addition, the
VP “killing five girls” of the original sentence
with ID 64 is also excluded since it has significant
redundancy with the summary sentence with ID 1.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.724271">
Existing multi-document summarization (MDS)
works can be classified into three categories:
</bodyText>
<page confidence="0.99059">
1594
</page>
<bodyText confidence="0.998908298701298">
extraction-based approaches, compression-based
approaches, and abstraction-based approaches.
Extraction-based approaches are the most stud-
ied of the three. Early studies mainly followed a
greedy strategy in sentence selection (C¸elikyilmaz
and Hakkani-T¨ur, 2011; Goldstein et al., 2000;
Wan et al., 2007). Each sentence in the docu-
ments is firstly assigned a salience score. Then,
sentence selection is performed by greedily select-
ing the sentence with the largest salience score
among the remaining ones. The redundancy is
controlled during the selection by penalizing the
remaining ones according to their similarity with
the selected sentences. An obvious drawback of
such greedy strategy is that it is easily trapped
in local optima. Later, unified models are pro-
posed to conduct sentence selection and redun-
dancy control simultaneously (McDonald, 2007;
Filatova and Hatzivassiloglou, 2004; Yih et al.,
2007; Gillick et al., 2007; Lin and Bilmes, 2010;
Lin and Bilmes, 2012; Sipos et al., 2012). How-
ever, extraction-based approaches are unable to
evaluate the salience and control the redundancy
on the granularity finer than sentences. Thus, the
selected sentences may still contain unimportant
or redundant phrases.
Compression-based approaches have been in-
vestigated to alleviate the above limitation. As
a natural extension of the extractive method, the
early works adopted a two-step approach (Lin,
2003; Zajic et al., 2006; Gillick and Favre, 2009).
The first step selects the sentences, and the second
step removes the unimportant or redundant units
from the sentences. Recently, integrated models
have been proposed that jointly conduct sentence
extraction and compression (Martins and Smith,
2009; Woodsend and Lapata, 2010; Almeida and
Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et
al., 2015). Note that our model also jointly con-
ducts phrase selection and phrase merging (new
sentence generation). Nonetheless, compressive
methods are unable to merge the related facts from
different sentences.
On the other hand, abstraction-based ap-
proaches can generate new sentences based on the
facts from different source sentences. In addition
to the previously mentioned sentence fusion work,
new directions have been explored. Researchers
developed an information extraction based ap-
proach that extracts information items (Genest and
Lapalme, 2011) or abstraction schemes (Genest
and Lapalme, 2012) as components for generat-
ing sentences. Summary revision was also inves-
tigated to improve the quality of automatic sum-
mary by rewriting the noun phrases or people ref-
erences in the summaries (Nenkova, 2008; Sid-
dharthan et al., 2011). Sentence generation with
word graph was applied for summarizing customer
opinions and chat conversations (Ganesan et al.,
2010; Mehdad et al., 2014).
Recently, the factors of information certainty
and timeline in MDS task were explored (Ng et
al., 2014; Wan and Zhang, 2014; Yan et al., 2011).
Researchers also explored some variants of the
typical MDS setting, such as query-chain focused
summarization that combines aspects of update
summarization and query-focused summarization
(Baumel et al., 2014), and hierarchical summa-
rization that scales up MDS to summarize a large
set of documents (Christensen et al., 2014). A
data-driven method for mining sentence structures
on large news archive was proposed and utilized
to summarize unseen news events (Pighin et al.,
2014). Moreover, some works (Liu et al., 2012;
K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao
et al., 2015) utilized deep learning techniques to
tackle some summarization tasks.
</bodyText>
<sectionHeader confidence="0.998264" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99988475">
We propose an abstractive MDS framework that
constructs new sentences by exploring more fine-
grained syntactic units, namely, noun phrases and
verb phrases. The designed optimization frame-
work operates on the summary level so that more
complementary semantic content units can be in-
corporated. The phrase selection and merging is
done simultaneously to achieve global optimal.
Meanwhile, the constructed sentences should sat-
isfy the constraints related to summarization re-
quirements such as NP/VP compatibility. Exper-
imental results on TAC 2011 summarization data
set show that our framework outperforms the top
systems in TAC 2011 under the pyramid metric.
For future work, one aspect is to enhance the
grammar quality of the generated new sentences
and compressed sentences. Another aspect is to
improve time efficiency of our framework, and its
major bottleneck is the time consuming ILP opti-
mzation.
</bodyText>
<page confidence="0.989193">
1595
</page>
<sectionHeader confidence="0.983064" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995547274509804">
Miguel Almeida and Andre Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In ACL, pages
196–206.
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Comput. Linguist., 31(3):297–328.
Tal Baumel, Raphael Cohen, and Michael Elhadad.
2014. Query-chain focused summarization. In ACL,
pages 913–922.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
HLT, pages 481–490.
Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming
Zhou. 2015. Ranking with recursive neural net-
works and its application to multi-document sum-
marization. In AAAI.
Asli C¸elikyilmaz and Dilek Hakkani-T¨ur. 2011.
Concept-based classification for multi-document
summarization. In ICASSP, pages 5540–5543.
Jackie Chi Kit Cheung and Gerald Penn. 2013. To-
wards robust abstractive multi-document summa-
rization: A caseframe analysis of centrality and do-
main. In ACL, pages 1233–1242.
Janara Christensen, Stephen Soderland, Gagan Bansal,
and Mausam. 2014. Hierarchical summarization:
Scaling up multi-document summarization. In ACL,
pages 902–912.
George B. Dantzig and Mukund N. Thapa. 1997. Lin-
ear Programming 1: Introduction. Springer-Verlag
New York, Inc.
Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil
Blunsom, and Nando de Freitas. 2014. Modelling,
visualising and summarising documents with a sin-
gle convolutional neural network. arXiv preprint
arXiv:1406.3830.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22(1):457–479.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In COLING.
Katja Filippova and Michael Strube. 2008. Sen-
tence fusion via dependency graph compression. In
EMNLP, pages 177–185.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In COLING,
pages 322–330.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
COLING, pages 340–348.
Pierre-Etienne Genest and Guy Lapalme. 2011.
Framework for abstractive summarization using
text-to-text generation. In MTTG, pages 64–73.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
ACL, pages 354–358.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Workshop on ILP for
NLP, pages 10–18.
Dan Gillick, Benoit Favre, and Dilek Hakkani-t¨ur.
2007. The icsi summarization system at tac 2008.
In Proc. of Text Understanding Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In NAACL-ANLP-
AutoSum, pages 40–48.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In ACL, pages 864–872.
Sanda Harabagiu and Finley Lacatusu. 2010. Us-
ing topic themes for multi-document summarization.
ACM Trans. Inf. Syst., 28(3):13:1–13:47.
Aaron Harnly, Ani Nenkova, Rebecca Passonneau, and
Owen Rambow. 2005. Automation of summary
evaluation by the pyramid method. In RANLP.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In NAACL,
pages 178–185.
Mikael K˚ageb¨ack, Olof Mogren, Nina Tahmasebi, and
Devdatt Dubhashi. 2014. Extractive summariza-
tion using continuous vector space models. In
CVSC@EACL, pages 31–39.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL, pages 423–430.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI-IAAI, pages 703–710.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Comput. Linguist., 39(4):885–916.
Huiying Li, Yue Hu, Zeyuan Li, Xiaojun Wan, and
Jianguo Xiao. 2011. Pkutm participation in
tac2011. In Proceedings of TAC.
Piji Li, Lidong Bing, Wai Lam, Hang Li, and Yi Liao.
2015. Reader-aware multi-document summariza-
tion via sparse coding. In IJCAI.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In HLT, pages 912–920.
</reference>
<page confidence="0.817583">
1596
</page>
<reference confidence="0.999622686746988">
Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In UAI, pages 479–490.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In NAACL, pages 71–78.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression: a pilot study. In
Proceedings of the sixth international workshop on
Information retrieval with Asian languages-Volume
11, pages 1–8. Association for Computational Lin-
guistics.
Yan Liu, Sheng-hua Zhong, and Wenjie Li. 2012.
Query-oriented multi-document summarization via
unsupervised deep learning. In AAAI.
Andr´e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extrac-
tion and compression. In Workshop on ILP for NLP,
pages 1–9.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
ECIR, pages 557–564.
Yashar Mehdad, Giuseppe Carenini, and Raymond T.
Ng. 2014. Abstractive summarization of spoken
and written conversations based on phrasal queries.
In ACL, pages 1220–1230.
Ani Nenkova and Rebecca J. Passonneau. 2004.
Evaluating content selection in summarization: The
pyramid method. In HLT-NAACL, pages 145–152.
Ani Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Third International
Joint Conference on Natural Language Processing,
IJCNLP, pages 118–125.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min yen
Kan, and Chew lim Tan. 2011. Swing: Exploit-
ing category-specific information for guided sum-
marization. In Proceedings of TAC.
Jun-Ping Ng, Yan Chen, Min-Yen Kan, and Zhoujun
Li. 2014. Exploiting timelines to enhance multi-
document summarization. In ACL, pages 923–933.
Rebecca J. Passonneau, Emily Chen, Weiwei Guo, and
Dolores Perin. 2013. Automated pyramid scoring
of summaries using distributional semantics. In ACL
(2), pages 143–147.
Daniele Pighin, Marco Cornolti, Enrique Alfonseca,
and Katja Filippova. 2014. Modelling events
through memory-based, open-ie patterns for abstrac-
tive summarization. In ACL, pages 892–901.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2011. Information status distinctions
and referring expressions: An empirical study of ref-
erences to people in news summaries. Comput. Lin-
guist., 37(4):811–842.
Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin learning of submod-
ular summarization models. In EACL, pages 224–
233.
Xiaojun Wan and Jianmin Zhang. 2014. Ctsum: Ex-
tracting more certain summaries for news articles.
In SIGIR, pages 787–796.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007. Manifold-ranking based topic-focused multi-
document summarization. In IJCAI, pages 2903–
2908.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In ACL, pages
565–574.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In EMNLP-CoNLL, pages 233–243.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011. Evolution-
ary timeline summarization: A balanced optimiza-
tion framework via iterative substitution. In SIGIR,
pages 745–754.
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In IJCAI, pages 1776–1782.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and
Richard Schwartz. 2006. Sentence compression
as a component of a multi-document summarization
system. In DUC at NLT/NAACL 2006.
</reference>
<page confidence="0.993507">
1597
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.103218">
<title confidence="0.921113">Abstractive Multi-Document Summarization via Phrase Selection and</title>
<author confidence="0.70442">J</author>
<affiliation confidence="0.726621">Learning Department, Carnegie Mellon University, Pittsburgh, PA bDepartment of Systems Engineering and Engineering The Chinese University of Hong Labs, Sunnyvale, CA,</affiliation>
<address confidence="0.445407">for Computational Learning Systems, Columbia University, New York, NY,</address>
<email confidence="0.964598">yliao,</email>
<abstract confidence="0.99482716">We propose an abstraction-based multidocument summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-ofthe-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Almeida</author>
<author>Andre Martins</author>
</authors>
<title>Fast and robust compressive summarization with dual decomposition and multi-task learning.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>196--206</pages>
<contexts>
<context position="34088" citStr="Almeida and Martins, 2013" startWordPosition="5659" endWordPosition="5662">s. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) o</context>
</contexts>
<marker>Almeida, Martins, 2013</marker>
<rawString>Miguel Almeida and Andre Martins. 2013. Fast and robust compressive summarization with dual decomposition and multi-task learning. In ACL, pages 196–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="3155" citStr="Barzilay and McKeown (2005)" startWordPosition="445" endWordPosition="449"> compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of </context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Comput. Linguist., 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tal Baumel</author>
<author>Raphael Cohen</author>
<author>Michael Elhadad</author>
</authors>
<title>Query-chain focused summarization.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>913--922</pages>
<contexts>
<context position="35474" citStr="Baumel et al., 2014" startWordPosition="5866" endWordPosition="5869">mmary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely,</context>
</contexts>
<marker>Baumel, Cohen, Elhadad, 2014</marker>
<rawString>Tal Baumel, Raphael Cohen, and Michael Elhadad. 2014. Query-chain focused summarization. In ACL, pages 913–922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In HLT,</booktitle>
<pages>481--490</pages>
<contexts>
<context position="34119" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="5663" endWordPosition="5666">nces may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest a</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In HLT, pages 481–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziqiang Cao</author>
<author>Furu Wei</author>
<author>Li Dong</author>
<author>Sujian Li</author>
<author>Ming Zhou</author>
</authors>
<title>Ranking with recursive neural networks and its application to multi-document summarization.</title>
<date>2015</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="35849" citStr="Cao et al., 2015" startWordPosition="5928" endWordPosition="5931"> Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more complementary semantic content units can be incorporated. The phrase selection and merging is done simultaneously to achieve global optimal. Meanwhile, the constructed sentences should satisfy the constraints related to summarization requirements such as NP/VP com</context>
</contexts>
<marker>Cao, Wei, Dong, Li, Zhou, 2015</marker>
<rawString>Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming Zhou. 2015. Ranking with recursive neural networks and its application to multi-document summarization. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli C¸elikyilmaz</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Concept-based classification for multi-document summarization. In</title>
<date>2011</date>
<booktitle>ICASSP,</booktitle>
<pages>5540--5543</pages>
<marker>C¸elikyilmaz, Hakkani-T¨ur, 2011</marker>
<rawString>Asli C¸elikyilmaz and Dilek Hakkani-T¨ur. 2011. Concept-based classification for multi-document summarization. In ICASSP, pages 5540–5543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Gerald Penn</author>
</authors>
<title>Towards robust abstractive multi-document summarization: A caseframe analysis of centrality and domain.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>1233--1242</pages>
<contexts>
<context position="2912" citStr="Cheung and Penn, 2013" startWordPosition="409" endWordPosition="412">n in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based a</context>
</contexts>
<marker>Cheung, Penn, 2013</marker>
<rawString>Jackie Chi Kit Cheung and Gerald Penn. 2013. Towards robust abstractive multi-document summarization: A caseframe analysis of centrality and domain. In ACL, pages 1233–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland</author>
<author>Gagan Bansal</author>
<author>Mausam</author>
</authors>
<title>Hierarchical summarization: Scaling up multi-document summarization.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>902--912</pages>
<contexts>
<context position="35590" citStr="Christensen et al., 2014" startWordPosition="5885" endWordPosition="5888">011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more compl</context>
</contexts>
<marker>Christensen, Soderland, Bansal, Mausam, 2014</marker>
<rawString>Janara Christensen, Stephen Soderland, Gagan Bansal, and Mausam. 2014. Hierarchical summarization: Scaling up multi-document summarization. In ACL, pages 902–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George B Dantzig</author>
<author>Mukund N Thapa</author>
</authors>
<title>Linear Programming 1: Introduction.</title>
<date>1997</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="15817" citStr="Dantzig and Thapa, 1997" startWordPosition="2626" endWordPosition="2629">nd employed as the subject of the newly generated sentence, which is then concatenated with the merged facts (i.e., VPs). If the compatibility entry ˜F[i, j] for Ni and Vj is 1, we define a sentence generation indicator ˜γij to indicate whether both Ni and Vj are selected to construct a new sentence in the summary. We design the following groups of constraints to realize our aim of phrase selection and new sentence construction. The objective function and constraints are linear, therefore the problem can be solved by existing Integer Linear Programming (ILP) solvers such as simplex algorithm (Dantzig and Thapa, 1997). NP validity. To maintain the consistency between the selection indicator α and the compatibility entry F˜ for NP Ni, we introduce two constraints as follows: ∀i, j, αi ≥ ˜γiji ∀i, X ˜γij ≥ αi. (4) j X XβiSVi − + i&lt;j i 1590 These two constraints work together to ensure the valid assignment of α according to the compatibility entry ˜I. VP legality. Similarly, the following requirement guarantees the consistency between the selection indicator β and the compatibility entry I˜ for selected VP Vi: ∀j, � ˜γij = βj. (5) i The above two constraints jointly ensure that the selected NPs and VPs are ab</context>
</contexts>
<marker>Dantzig, Thapa, 1997</marker>
<rawString>George B. Dantzig and Mukund N. Thapa. 1997. Linear Programming 1: Introduction. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Misha Denil</author>
<author>Alban Demiraj</author>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
<author>Nando de Freitas</author>
</authors>
<title>Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830.</title>
<date>2014</date>
<marker>Denil, Demiraj, Kalchbrenner, Blunsom, de Freitas, 2014</marker>
<rawString>Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, and Nando de Freitas. 2014. Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2156" citStr="Erkan and Radev, 2004" startWordPosition="293" endWordPosition="296">S) methods fall in three categories: extraction-based, compression-based and abstraction-based. Most ∗ The work described in this paper is substantially supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2013090068/TH138232) and the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous invest</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence text extraction.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="33218" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="5528" endWordPosition="5531">011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the senten</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence text extraction. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>177--185</pages>
<contexts>
<context position="3221" citStr="Filippova and Strube, 2008" startWordPosition="455" endWordPosition="458">els cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Lingu</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In EMNLP, pages 177–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>Multi-sentence compression: Finding shortest paths in word graphs.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>322--330</pages>
<contexts>
<context position="3239" citStr="Filippova, 2010" startWordPosition="459" endWordPosition="460">different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th</context>
</contexts>
<marker>Filippova, 2010</marker>
<rawString>Katja Filippova. 2010. Multi-sentence compression: Finding shortest paths in word graphs. In COLING, pages 322–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavita Ganesan</author>
<author>ChengXiang Zhai</author>
<author>Jiawei Han</author>
</authors>
<title>Opinosis: A graph-based approach to abstractive summarization of highly redundant opinions.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>340--348</pages>
<contexts>
<context position="35097" citStr="Ganesan et al., 2010" startWordPosition="5809" endWordPosition="5812">on to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to su</context>
</contexts>
<marker>Ganesan, Zhai, Han, 2010</marker>
<rawString>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: A graph-based approach to abstractive summarization of highly redundant opinions. In COLING, pages 340–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Etienne Genest</author>
<author>Guy Lapalme</author>
</authors>
<title>Framework for abstractive summarization using text-to-text generation.</title>
<date>2011</date>
<booktitle>In MTTG,</booktitle>
<pages>64--73</pages>
<contexts>
<context position="34686" citStr="Genest and Lapalme, 2011" startWordPosition="5745" endWordPosition="5748">; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also expl</context>
</contexts>
<marker>Genest, Lapalme, 2011</marker>
<rawString>Pierre-Etienne Genest and Guy Lapalme. 2011. Framework for abstractive summarization using text-to-text generation. In MTTG, pages 64–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Etienne Genest</author>
<author>Guy Lapalme</author>
</authors>
<title>Fully abstractive approach to guided summarization.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>354--358</pages>
<contexts>
<context position="34736" citStr="Genest and Lapalme, 2012" startWordPosition="5752" endWordPosition="5755">l., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, suc</context>
</contexts>
<marker>Genest, Lapalme, 2012</marker>
<rawString>Pierre-Etienne Genest and Guy Lapalme. 2012. Fully abstractive approach to guided summarization. In ACL, pages 354–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Workshop on ILP for NLP,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="33783" citStr="Gillick and Favre, 2009" startWordPosition="5615" endWordPosition="5618">eously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based appro</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Workshop on ILP for NLP, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-t¨ur</author>
</authors>
<title>The icsi summarization system at tac</title>
<date>2007</date>
<booktitle>In Proc. of Text Understanding Conference.</booktitle>
<marker>Gillick, Favre, Hakkani-t¨ur, 2007</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-t¨ur. 2007. The icsi summarization system at tac 2008. In Proc. of Text Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In NAACL-ANLPAutoSum,</booktitle>
<pages>40--48</pages>
<contexts>
<context position="32610" citStr="Goldstein et al., 2000" startWordPosition="5435" endWordPosition="5438">such as “sent the boys outside”, “authorities said”, etc. In addition, the VP “killing five girls” of the original sentence with ID 64 is also excluded since it has significant redundancy with the summary sentence with ID 1. 5 Related Work Existing multi-document summarization (MDS) works can be classified into three categories: 1594 extraction-based approaches, compression-based approaches, and abstraction-based approaches. Extraction-based approaches are the most studied of the three. Early studies mainly followed a greedy strategy in sentence selection (C¸elikyilmaz and Hakkani-T¨ur, 2011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassilogl</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In NAACL-ANLPAutoSum, pages 40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>864--872</pages>
<contexts>
<context position="23108" citStr="Guo and Diab, 2012" startWordPosition="3896" endWordPosition="3899">rkshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005). In this paper, we adopt the same setti</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In ACL, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
</authors>
<title>Using topic themes for multi-document summarization.</title>
<date>2010</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2496" citStr="Harabagiu and Lacatusu, 2010" startWordPosition="346" endWordPosition="349">ative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. </context>
</contexts>
<marker>Harabagiu, Lacatusu, 2010</marker>
<rawString>Sanda Harabagiu and Finley Lacatusu. 2010. Using topic themes for multi-document summarization. ACM Trans. Inf. Syst., 28(3):13:1–13:47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Harnly</author>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Owen Rambow</author>
</authors>
<title>Automation of summary evaluation by the pyramid method.</title>
<date>2005</date>
<booktitle>In RANLP.</booktitle>
<contexts>
<context position="23668" citStr="Harnly et al., 2005" startWordPosition="3984" endWordPosition="3987">a a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005). In this paper, we adopt the same setting as in (Passonneau et al., 2013): a 100 dimension matrix factorization model is learned on a domain independent corpus, which is drawn from sense definitions of WordNet and Wiktionary2, and Brown corpus. We exper2http://en.wiktionary.org/ 1592 ROUGE-2 ROUGE-SU4 System P R F1 P R F1 Our 0.117 0.117 0.117 0.148 0.147 0.148 22 0.112 0.114 0.113 0.147 0.150 0.148 43 0.132 0.135 0.134 0.162 0.166 0.164 17 0.128 0.131 0.129 0.157 0.160 0.159 Table 3: Performance under ROUGE metric. iment with 2 threshold values, i.e., 0.6 and 0.65, similar to those used in (</context>
</contexts>
<marker>Harnly, Nenkova, Passonneau, Rambow, 2005</marker>
<rawString>Aaron Harnly, Ani Nenkova, Rebecca Passonneau, and Owen Rambow. 2005. Automation of summary evaluation by the pyramid method. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Cut and paste based text summarization.</title>
<date>2000</date>
<booktitle>In NAACL,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="2937" citStr="Jing and McKeown, 2000" startWordPosition="413" endWordPosition="416">is end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather informat</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Hongyan Jing and Kathleen R. McKeown. 2000. Cut and paste based text summarization. In NAACL, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikael K˚ageb¨ack</author>
<author>Olof Mogren</author>
<author>Nina Tahmasebi</author>
<author>Devdatt Dubhashi</author>
</authors>
<title>Extractive summarization using continuous vector space models.</title>
<date>2014</date>
<booktitle>In CVSC@EACL,</booktitle>
<pages>31--39</pages>
<marker>K˚ageb¨ack, Mogren, Tahmasebi, Dubhashi, 2014</marker>
<rawString>Mikael K˚ageb¨ack, Olof Mogren, Nina Tahmasebi, and Devdatt Dubhashi. 2014. Extractive summarization using continuous vector space models. In CVSC@EACL, pages 31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="6908" citStr="Klein and Manning, 2003" startWordPosition="1033" endWordPosition="1037">en we formulate the sentence generation task as an optimization problem, and design constraints. In the end, we perform several post-processing steps to improve the order and the readability of the generated sentences. 2.1 Phrase Salience Calculation The first component decomposes the sentences in documents into a set of noun phrases (NPs) derived from the subject parts of a constituency tree and a set of verb-object phrases (VPs), representing potential key concepts and key facts, respectively. These phrases will serve as the basic elements for sentence generation. We employ Stanford parser (Klein and Manning, 2003) to obtain a constituency tree for each input sentence. After that, we extract NPs and VPs from the tree as follows: (1) The NPs and VPs that are the direct children of the sentence node (repre1588 sented by the S node) are extracted. (2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as having the same parent node S. Recursive operation in the second step will only be carried out in two levels since the phrases in the lower levels may not be able to convey a complete fact. Take the tree in Figure 1 as an example, the corresponding sentence</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In AAAI-IAAI,</booktitle>
<pages>703--710</pages>
<contexts>
<context position="2435" citStr="Knight and Marcu, 2000" startWordPosition="336" endWordPosition="339">search Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a se</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In AAAI-IAAI, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Comput. Linguist.,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="10964" citStr="Lee et al., 2013" startWordPosition="1731" endWordPosition="1734">ts are jointly considered. 2.2.1 Compatibility Relation Compatibility relation is designed to indicate whether an NP and a VP can be used to form a new sentence. For example, the NP “Police” from another sentence should not be the subject of the VP “sent the boys outside” extracted from Figure 1. We use some heuristics to find compatibility, and then expand the compatibility relation to more phrases by extracting coreference. To find coreference NPs (different mentions for the same entity), we first conduct coreference resolution for each document with Stanford coreference resolution package (Lee et al., 2013). We adopt those resolution rules that are able to achieve high quality and address our need for summarization. In particular, Sieve 1, 2, 3, 4, 5, 9, and 10 in the package are used. A set of clusters are obtained and each cluster contains the mentions that refer to the same entity in a document. The clusters from different documents in the same topic are merged by matching the named entities. After merging, the mentions that are not NPs extracted in the phrase extraction step are removed in each cluster. Two NPs in the same cluster are determined as alternative of each other. To find alternat</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Comput. Linguist., 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huiying Li</author>
<author>Yue Hu</author>
<author>Zeyuan Li</author>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Pkutm participation in tac2011.</title>
<date>2011</date>
<booktitle>In Proceedings of TAC.</booktitle>
<contexts>
<context position="8214" citStr="Li et al., 2011" startWordPosition="1267" endWordPosition="1270">tside and tied up and shot the girls, killing three of them”, “walked into an Amish school”, “sent the boys outside”, and “tied up and shot the girls, killing three of them”. 1 Because of the recursive operation, the extracted phrases may have overlaps. Later, we will show how to avoid such overlapping in phrase selection. A salience score is calculated for each phrase to indicate its importance. Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc. One key characteristic of our approach is that the considered basic units are phrases instead of sentences. Such finer granularity leaves more room for better global salience score by potentially covering more distinct facts. In our implementation, we adopt a concept-based weight incorporating the position information. The concept set is designated to be the union set of unigrams, bigrams, and named entities in the documents. We remove stopwords and perform lemmatization before extracting unigrams and bigrams. The position-based term frequency is used in the concept weighting scheme. Wh</context>
<context position="24396" citStr="Li et al., 2011" startWordPosition="4109" endWordPosition="4112"> model is learned on a domain independent corpus, which is drawn from sense definitions of WordNet and Wiktionary2, and Brown corpus. We exper2http://en.wiktionary.org/ 1592 ROUGE-2 ROUGE-SU4 System P R F1 P R F1 Our 0.117 0.117 0.117 0.148 0.147 0.148 22 0.112 0.114 0.113 0.147 0.150 0.148 43 0.132 0.135 0.134 0.162 0.166 0.164 17 0.128 0.131 0.129 0.157 0.160 0.159 Table 3: Performance under ROUGE metric. iment with 2 threshold values, i.e., 0.6 and 0.65, similar to those used in (Passonneau et al., 2013). The top three systems in TAC 2011 evaluated with manual pyramid score were System 22 (Li et al., 2011), 43, and 17 (Ng et al., 2011). Table 2 shows the comparison with them under the automated pyramid evaluation. Our method achieves the best results in both thresholds, which means that our method is able to find more semantic content units (SCUs) than the state-of-the-art system in TAC 2011. In addition, paired t-test (with p &lt; 0.01) comparing our model with the best system in TAC 2011, i.e., System 22, shows that the performance of our model is significantly better. It is worth noting that the three systems used additional external linguistic resources: System 22 used a Wikipedia corpus for p</context>
</contexts>
<marker>Li, Hu, Li, Wan, Xiao, 2011</marker>
<rawString>Huiying Li, Yue Hu, Zeyuan Li, Xiaojun Wan, and Jianguo Xiao. 2011. Pkutm participation in tac2011. In Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piji Li</author>
<author>Lidong Bing</author>
<author>Wai Lam</author>
<author>Hang Li</author>
<author>Yi Liao</author>
</authors>
<title>Reader-aware multi-document summarization via sparse coding.</title>
<date>2015</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="2514" citStr="Li et al., 2015" startWordPosition="350" endWordPosition="353">odes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work</context>
<context position="34137" citStr="Li et al., 2015" startWordPosition="5667" endWordPosition="5670">ant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) </context>
</contexts>
<marker>Li, Bing, Lam, Li, Liao, 2015</marker>
<rawString>Piji Li, Lidong Bing, Wai Lam, Hang Li, and Yi Liao. 2015. Reader-aware multi-document summarization via sparse coding. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In HLT,</booktitle>
<pages>912--920</pages>
<contexts>
<context position="33280" citStr="Lin and Bilmes, 2010" startWordPosition="5540" endWordPosition="5543">s is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant </context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In HLT, pages 912–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Learning mixtures of submodular shells with application to document summarization.</title>
<date>2012</date>
<booktitle>In UAI,</booktitle>
<pages>479--490</pages>
<contexts>
<context position="33302" citStr="Lin and Bilmes, 2012" startWordPosition="5544" endWordPosition="5547">a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentenc</context>
</contexts>
<marker>Lin, Bilmes, 2012</marker>
<rawString>Hui Lin and Jeff Bilmes. 2012. Learning mixtures of submodular shells with application to document summarization. In UAI, pages 479–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In NAACL,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="22114" citStr="Lin and Hovy, 2003" startWordPosition="3742" endWordPosition="3745"> as to recognize alternate realizations of the same meaning. Different weights are assigned to SCUs based on their frequency in model summaries. A weighted inventory of SCUs named a pyramid is created, which constitutes a resource for investigating alternate realizations of the same meaning. Such property makes pyramid method more suitable to evaluAuto-pyr Auto-pyr Rank in System (Th: .6) (Th: .65) TAC 2011 Our 0.905 0.793 NA 22 0.878 0.775 1 43 0.875 0.756 2 17 0.860 0.741 3 Table 2: Comparison with the top 3 systems in TAC 2011. ate summaries. Another widely used evaluation metric is ROUGE (Lin and Hovy, 2003) and it evaluates summaries from word overlapping perspective. Because of the strict string matching, it ignores the semantic content units and performs better when larger sets of model summaries are available. In contrast to ROUGE, pyramid scoring is robust with as few as four model summaries (Nenkova and Passonneau, 2004). Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In NAACL, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression: a pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume 11,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2446" citStr="Lin, 2003" startWordPosition="340" endWordPosition="341">the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whos</context>
<context position="33737" citStr="Lin, 2003" startWordPosition="5609" endWordPosition="5610">and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentenc</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Chin-Yew Lin. 2003. Improving summarization performance by sentence compression: a pilot study. In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume 11, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Liu</author>
<author>Sheng-hua Zhong</author>
<author>Wenjie Li</author>
</authors>
<title>Query-oriented multi-document summarization via unsupervised deep learning.</title>
<date>2012</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="35785" citStr="Liu et al., 2012" startWordPosition="5916" endWordPosition="5919">nd timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more complementary semantic content units can be incorporated. The phrase selection and merging is done simultaneously to achieve global optimal. Meanwhile, the constructed sentences should satisfy the con</context>
</contexts>
<marker>Liu, Zhong, Li, 2012</marker>
<rawString>Yan Liu, Sheng-hua Zhong, and Wenjie Li. 2012. Query-oriented multi-document summarization via unsupervised deep learning. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Workshop on ILP for NLP,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="34034" citStr="Martins and Smith, 2009" startWordPosition="5651" endWordPosition="5654">he redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that e</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Workshop on ILP for NLP, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In ECIR,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="33181" citStr="McDonald, 2007" startWordPosition="5526" endWordPosition="5527"> Hakkani-T¨ur, 2011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 200</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In ECIR, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>Abstractive summarization of spoken and written conversations based on phrasal queries.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>1220--1230</pages>
<contexts>
<context position="35119" citStr="Mehdad et al., 2014" startWordPosition="5813" endWordPosition="5816">entioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news ev</context>
</contexts>
<marker>Mehdad, Carenini, Ng, 2014</marker>
<rawString>Yashar Mehdad, Giuseppe Carenini, and Raymond T. Ng. 2014. Abstractive summarization of spoken and written conversations based on phrasal queries. In ACL, pages 1220–1230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="5379" citStr="Nenkova and Passonneau, 2004" startWordPosition="789" endWordPosition="793">f concepts and facts represented by NPs and VPs from the input documents. A salience score is computed for each phrase by exploiting redundancy of the document content in a global manner. The second component constructs new sentences by selecting and merging phrases based on their salience scores, and ensures the validity of new sentences using a integer linear optimization model. The contribution of this paper is two folds. (1) We extract NPs/VPs from constituency trees to represent key concepts/facts, and merge them to construct new sentences, which allows more summary content units (SCUs) (Nenkova and Passonneau, 2004) to be included in a sentence by breaking the original sentence boundaries. (2) The designed optimization framework for addressing the problem is unique and effective. Our optimization algorithm simultaneously selects and merges a set of phrases that maximize the number of covered SCUs in a summary. Meanwhile, since the basic unit is phrases, we design compatibility relations among NPs and VPs, as well as other optimization constraints, to ensure that the generated sentences contain correct facts. Compared with the sentence fusion approaches that compute salience scores of sentence clusters, o</context>
<context position="21433" citStr="Nenkova and Passonneau, 2004" startWordPosition="3626" endWordPosition="3630">pic also has 10 documents and 4 model summaries. Based on the tuning set, the key parameters of our model are set as follows. The constants B and ρ in the weighting function are set to 6 and 0.5 repectively. The similarity threshold in obtaining the alternative VPs is 0.75. We did not observe significant difference between cosine similarity and Jaccard Index. We mainly evaluate the system by pyramid evaluation. To gain a comprehensive understanding, we also evaluate by ROUGE evaluation and manual linguistic quality evaluation. 3.2 Results with Pyramid Evaluation The pyramid evaluation metric (Nenkova and Passonneau, 2004) involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning. Different weights are assigned to SCUs based on their frequency in model summaries. A weighted inventory of SCUs named a pyramid is created, which constitutes a resource for investigating alternate realizations of the same meaning. Such property makes pyramid method more suitable to evaluAuto-pyr Auto-pyr Rank in System (Th: .6) (Th: .65) TAC 2011 Our 0.905 0.793 NA 22 0.878 0.775 1 43 0.875 0.756 2 17 0.860 0.741 3 Table 2: Comparison with the top 3 systems in TAC 2011. a</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In HLT-NAACL, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
</authors>
<title>Entity-driven rewrite for multidocument summarization.</title>
<date>2008</date>
<booktitle>In Third International Joint Conference on Natural Language Processing, IJCNLP,</booktitle>
<pages>118--125</pages>
<contexts>
<context position="34942" citStr="Nenkova, 2008" startWordPosition="5788" endWordPosition="5789"> sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a l</context>
</contexts>
<marker>Nenkova, 2008</marker>
<rawString>Ani Nenkova. 2008. Entity-driven rewrite for multidocument summarization. In Third International Joint Conference on Natural Language Processing, IJCNLP, pages 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ping Ng</author>
<author>Praveen Bysani</author>
<author>Ziheng Lin</author>
<author>Min yen Kan</author>
<author>Chew lim Tan</author>
</authors>
<title>Swing: Exploiting category-specific information for guided summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of TAC.</booktitle>
<contexts>
<context position="24426" citStr="Ng et al., 2011" startWordPosition="4116" endWordPosition="4119">independent corpus, which is drawn from sense definitions of WordNet and Wiktionary2, and Brown corpus. We exper2http://en.wiktionary.org/ 1592 ROUGE-2 ROUGE-SU4 System P R F1 P R F1 Our 0.117 0.117 0.117 0.148 0.147 0.148 22 0.112 0.114 0.113 0.147 0.150 0.148 43 0.132 0.135 0.134 0.162 0.166 0.164 17 0.128 0.131 0.129 0.157 0.160 0.159 Table 3: Performance under ROUGE metric. iment with 2 threshold values, i.e., 0.6 and 0.65, similar to those used in (Passonneau et al., 2013). The top three systems in TAC 2011 evaluated with manual pyramid score were System 22 (Li et al., 2011), 43, and 17 (Ng et al., 2011). Table 2 shows the comparison with them under the automated pyramid evaluation. Our method achieves the best results in both thresholds, which means that our method is able to find more semantic content units (SCUs) than the state-of-the-art system in TAC 2011. In addition, paired t-test (with p &lt; 0.01) comparing our model with the best system in TAC 2011, i.e., System 22, shows that the performance of our model is significantly better. It is worth noting that the three systems used additional external linguistic resources: System 22 used a Wikipedia corpus for providing domain knowledge, Sys</context>
</contexts>
<marker>Ng, Bysani, Lin, Kan, Tan, 2011</marker>
<rawString>Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min yen Kan, and Chew lim Tan. 2011. Swing: Exploiting category-specific information for guided summarization. In Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ping Ng</author>
<author>Yan Chen</author>
<author>Min-Yen Kan</author>
<author>Zhoujun Li</author>
</authors>
<title>Exploiting timelines to enhance multidocument summarization.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>923--933</pages>
<contexts>
<context position="35223" citStr="Ng et al., 2014" startWordPosition="5830" endWordPosition="5833">tion based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al</context>
</contexts>
<marker>Ng, Chen, Kan, Li, 2014</marker>
<rawString>Jun-Ping Ng, Yan Chen, Min-Yen Kan, and Zhoujun Li. 2014. Exploiting timelines to enhance multidocument summarization. In ACL, pages 923–933.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Emily Chen</author>
<author>Weiwei Guo</author>
<author>Dolores Perin</author>
</authors>
<title>Automated pyramid scoring of summaries using distributional semantics.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>143--147</pages>
<contexts>
<context position="22813" citStr="Passonneau et al., 2013" startWordPosition="3853" endWordPosition="3856">he strict string matching, it ignores the semantic content units and performs better when larger sets of model summaries are available. In contrast to ROUGE, pyramid scoring is robust with as few as four model summaries (Nenkova and Passonneau, 2004). Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Pas</context>
<context position="24292" citStr="Passonneau et al., 2013" startWordPosition="4090" endWordPosition="4093">. In this paper, we adopt the same setting as in (Passonneau et al., 2013): a 100 dimension matrix factorization model is learned on a domain independent corpus, which is drawn from sense definitions of WordNet and Wiktionary2, and Brown corpus. We exper2http://en.wiktionary.org/ 1592 ROUGE-2 ROUGE-SU4 System P R F1 P R F1 Our 0.117 0.117 0.117 0.148 0.147 0.148 22 0.112 0.114 0.113 0.147 0.150 0.148 43 0.132 0.135 0.134 0.162 0.166 0.164 17 0.128 0.131 0.129 0.157 0.160 0.159 Table 3: Performance under ROUGE metric. iment with 2 threshold values, i.e., 0.6 and 0.65, similar to those used in (Passonneau et al., 2013). The top three systems in TAC 2011 evaluated with manual pyramid score were System 22 (Li et al., 2011), 43, and 17 (Ng et al., 2011). Table 2 shows the comparison with them under the automated pyramid evaluation. Our method achieves the best results in both thresholds, which means that our method is able to find more semantic content units (SCUs) than the state-of-the-art system in TAC 2011. In addition, paired t-test (with p &lt; 0.01) comparing our model with the best system in TAC 2011, i.e., System 22, shows that the performance of our model is significantly better. It is worth noting that </context>
</contexts>
<marker>Passonneau, Chen, Guo, Perin, 2013</marker>
<rawString>Rebecca J. Passonneau, Emily Chen, Weiwei Guo, and Dolores Perin. 2013. Automated pyramid scoring of summaries using distributional semantics. In ACL (2), pages 143–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Marco Cornolti</author>
<author>Enrique Alfonseca</author>
<author>Katja Filippova</author>
</authors>
<title>Modelling events through memory-based, open-ie patterns for abstractive summarization. In</title>
<date>2014</date>
<booktitle>ACL,</booktitle>
<pages>892--901</pages>
<contexts>
<context position="35745" citStr="Pighin et al., 2014" startWordPosition="5909" endWordPosition="5912">ntly, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more complementary semantic content units can be incorporated. The phrase selection and merging is done simultaneously to achieve global optimal. Meanwhile, the cons</context>
</contexts>
<marker>Pighin, Cornolti, Alfonseca, Filippova, 2014</marker>
<rawString>Daniele Pighin, Marco Cornolti, Enrique Alfonseca, and Katja Filippova. 2014. Modelling events through memory-based, open-ie patterns for abstractive summarization. In ACL, pages 892–901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Information status distinctions and referring expressions: An empirical study of references to people in news summaries.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="34969" citStr="Siddharthan et al., 2011" startWordPosition="5790" endWordPosition="5794">the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Chri</context>
</contexts>
<marker>Siddharthan, Nenkova, McKeown, 2011</marker>
<rawString>Advaith Siddharthan, Ani Nenkova, and Kathleen McKeown. 2011. Information status distinctions and referring expressions: An empirical study of references to people in news summaries. Comput. Linguist., 37(4):811–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruben Sipos</author>
<author>Pannaga Shivaswamy</author>
<author>Thorsten Joachims</author>
</authors>
<title>Large-margin learning of submodular summarization models.</title>
<date>2012</date>
<booktitle>In EACL,</booktitle>
<pages>224--233</pages>
<contexts>
<context position="33323" citStr="Sipos et al., 2012" startWordPosition="5548" endWordPosition="5551">, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integra</context>
</contexts>
<marker>Sipos, Shivaswamy, Joachims, 2012</marker>
<rawString>Ruben Sipos, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Large-margin learning of submodular summarization models. In EACL, pages 224– 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianmin Zhang</author>
</authors>
<title>Ctsum: Extracting more certain summaries for news articles.</title>
<date>2014</date>
<booktitle>In SIGIR,</booktitle>
<pages>787--796</pages>
<contexts>
<context position="35244" citStr="Wan and Zhang, 2014" startWordPosition="5834" endWordPosition="5837">ch that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., </context>
</contexts>
<marker>Wan, Zhang, 2014</marker>
<rawString>Xiaojun Wan and Jianmin Zhang. 2014. Ctsum: Extracting more certain summaries for news articles. In SIGIR, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Manifold-ranking based topic-focused multidocument summarization.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<pages>2903--2908</pages>
<contexts>
<context position="2175" citStr="Wan et al., 2007" startWordPosition="297" endWordPosition="300">e categories: extraction-based, compression-based and abstraction-based. Most ∗ The work described in this paper is substantially supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2013090068/TH138232) and the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that </context>
<context position="32629" citStr="Wan et al., 2007" startWordPosition="5439" endWordPosition="5442">utside”, “authorities said”, etc. In addition, the VP “killing five girls” of the original sentence with ID 64 is also excluded since it has significant redundancy with the summary sentence with ID 1. 5 Related Work Existing multi-document summarization (MDS) works can be classified into three categories: 1594 extraction-based approaches, compression-based approaches, and abstraction-based approaches. Extraction-based approaches are the most studied of the three. Early studies mainly followed a greedy strategy in sentence selection (C¸elikyilmaz and Hakkani-T¨ur, 2011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multidocument summarization. In IJCAI, pages 2903– 2908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic generation of story highlights.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>565--574</pages>
<contexts>
<context position="34061" citStr="Woodsend and Lapata, 2010" startWordPosition="5655" endWordPosition="5658">ularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (</context>
</contexts>
<marker>Woodsend, Lapata, 2010</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2010. Automatic generation of story highlights. In ACL, pages 565–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>233--243</pages>
<contexts>
<context position="8174" citStr="Woodsend and Lapata, 2012" startWordPosition="1260" endWordPosition="1264">n”, “walked into an Amish school, sent the boys outside and tied up and shot the girls, killing three of them”, “walked into an Amish school”, “sent the boys outside”, and “tied up and shot the girls, killing three of them”. 1 Because of the recursive operation, the extracted phrases may have overlaps. Later, we will show how to avoid such overlapping in phrase selection. A salience score is calculated for each phrase to indicate its importance. Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc. One key characteristic of our approach is that the considered basic units are phrases instead of sentences. Such finer granularity leaves more room for better global salience score by potentially covering more distinct facts. In our implementation, we adopt a concept-based weight incorporating the position information. The concept set is designated to be the union set of unigrams, bigrams, and named entities in the documents. We remove stopwords and perform lemmatization before extracting unigrams and bigrams. The position-based term frequency is </context>
<context position="17821" citStr="Woodsend and Lapata, 2012" startWordPosition="3011" endWordPosition="3014">Ni and Nj co-occur in the summary (i.e., αij = 1), then we have to include them individually (i.e., αi = 1 and αj = 1). The third constraint is the inverse of the first two. Similarly, the constraints for VPs are as follows: βij − βi ≤ 0, βij − βj ≤ 0, βi + βj − βij ≤ 1. Sentence number. In abstractive summarization, we do not prefer to generate many short sentences. This is controlled by: � αi ≤ K, (13) i where K is the maximum number of sentences. Short sentence avoidance. We do not select the VPs from very short sentences because a short sentence normally cannot convey a complete key fact (Woodsend and Lapata, 2012). if l(S) &lt; M, Vi ∈ S, then βi = 0, (14) where M is the threshold of the sentence length. Pronoun avoidance. We exclude the NPs that are pronouns from being selected as the subject of the new sentences. As previously observed (Woodsend and Lapata, 2012), pronouns are normally not used by human summary writers. It is because the summary is short and the narration relation of sentences is relatively simple so that pronouns are not needed. Moreover, in automatic summary, pronouns will cause ambiguity in the summary, especially when the sentence order is automatically determined. Therefore, we mod</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In EMNLP-CoNLL, pages 233–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Xiaojun Wan</author>
<author>Jahna Otterbacher</author>
<author>Liang Kong</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Evolutionary timeline summarization: A balanced optimization framework via iterative substitution.</title>
<date>2011</date>
<booktitle>In SIGIR,</booktitle>
<pages>745--754</pages>
<contexts>
<context position="35263" citStr="Yan et al., 2011" startWordPosition="5838" endWordPosition="5841">rmation items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep</context>
</contexts>
<marker>Yan, Wan, Otterbacher, Kong, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011. Evolutionary timeline summarization: A balanced optimization framework via iterative substitution. In SIGIR, pages 745–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Joshua Goodman</author>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
</authors>
<title>Multi-document summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<pages>1776--1782</pages>
<contexts>
<context position="8112" citStr="Yih et al., 2007" startWordPosition="1251" endWordPosition="1254">ding sentence is decomposed into phrases “An armed man”, “walked into an Amish school, sent the boys outside and tied up and shot the girls, killing three of them”, “walked into an Amish school”, “sent the boys outside”, and “tied up and shot the girls, killing three of them”. 1 Because of the recursive operation, the extracted phrases may have overlaps. Later, we will show how to avoid such overlapping in phrase selection. A salience score is calculated for each phrase to indicate its importance. Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc. One key characteristic of our approach is that the considered basic units are phrases instead of sentences. Such finer granularity leaves more room for better global salience score by potentially covering more distinct facts. In our implementation, we adopt a concept-based weight incorporating the position information. The concept set is designated to be the union set of unigrams, bigrams, and named entities in the documents. We remove stopwords and perform lemmatization before extracti</context>
<context position="33236" citStr="Yih et al., 2007" startWordPosition="5532" endWordPosition="5535">l., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the secon</context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and Hisami Suzuki. 2007. Multi-document summarization by maximizing informative content-words. In IJCAI, pages 1776–1782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Zajic</author>
<author>Bonnie J Dorr</author>
<author>Jimmy Lin</author>
<author>Richard Schwartz</author>
</authors>
<title>Sentence compression as a component of a multi-document summarization system.</title>
<date>2006</date>
<booktitle>In DUC at NLT/NAACL</booktitle>
<contexts>
<context position="2466" citStr="Zajic et al., 2006" startWordPosition="342" endWordPosition="345">ng Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come fro</context>
<context position="33757" citStr="Zajic et al., 2006" startWordPosition="5611" endWordPosition="5614">ncy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other han</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2006</marker>
<rawString>David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2006. Sentence compression as a component of a multi-document summarization system. In DUC at NLT/NAACL 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>