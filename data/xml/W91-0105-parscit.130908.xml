<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<sectionHeader confidence="0.7685375" genericHeader="abstract">
REVERSIBILITY AND MODULARITY IN
NATURAL LANGUAGE GENERATION
</sectionHeader>
<author confidence="0.48832">
Giinter Neumann
</author>
<affiliation confidence="0.475329">
Lehrstuhl fiir Computerlinguistik
</affiliation>
<address confidence="0.872173">
SFB 314, Projekt N3 BiLD
Universitat des Saarlandes
Im Stadtwald 15
D-6600 Saarbriicken 11, FRG
</address>
<email confidence="0.694275">
neumannOcoli.uni-sb.de
</email>
<sectionHeader confidence="0.977077" genericHeader="introduction">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.99981025">
A consequent use of reversible grammars within
natural language generation systems has strong
implications for the separation into strategic and
tactical components. A central goal of this paper
is to make plausible that a uniform architecture
for grammatical processing will serve as a basis
to achieve more flexible and efficient generation
systems.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958596153846">
In general, the goal of parsing is the derivation of
all possible grammatical structures defined by a
grammar of a given string a (i.e. especially the
determination of all possible logical forms of a)
and the goal of the corresponding generation task
is the computation of all possible strings defined
by a grammar of al given logical form (I) that are
logically equivalent to (1) (see also (Shieber, 1988),
(Calder et al., 1989)). Recently, there is a strong
tendency to use the same grammar for perform-
ing both tasks. Besides more practically moti-
vated reasons - obtaining more compact systems
or avoidance of inconsistencies between the input
and output of a syStem - there are also theoreti-
cal (a single model of language behaviour) and
psychological evidences (empirical evidence for
shared processors or facilities, cf. (Garrett, 1982),
(Frazier, 1982), (Jackendoff, 1987)) to adopt this
view.
From a formal point of view the main interest in
obtaining non—directional grammars is the spec-
ification of the relationship between strings and
logical forms.&apos; According to van Noord (1990),
a grammar is reversible if the parsing and gen-
eration problem is computable and the relation
between strings and logical forms is symmetric.
In this case parsing and generation are viewed as
mutually inverse processes.
Furthermore there are also approaches that as-
sume that it is possible to use the same algo-
rithm for processing the grammar in both direc-
tions (e.g. (Hasida and Isizaki, 1987), (Shieber,
1988), (Dymetman et al., 1990), (Emele and Za-
jac, 1990)). A great advantage of a uniform pro-
cess is that a discourse and task independent
module for grammatical processing is available.
This means that during performing both tasks
the same grammatical power is potentially dis-
posable (regardless of the actual language use).
Nevertheless, in most of the &apos;real&apos; generation
systems where all aspects of the generation pro-
cess of natural language utterances are consid-
ered, grammars are used that are especially de-
signed for generation purposes (cf. (Hovy, 1987),
(Dale, 1990), (Horacek, 1990), (McKeown et al.,
1990), (Reithinger, 1991)).2
The purpose of this paper is to show that the
use of a uniform architecture for grammatical pro-
cessing has important influences for the whole
generation task. A consequent use of a uniform
process within a natural language generation sys-
tem affects the separation into strategic and tacti-
</bodyText>
<footnote confidence="0.901109">
11 assume a notion of grammars that integrate phono-
logical, syntactical and semantics&apos; levels of description,
e.g., (Pollard and Sag, 1987).
2But it is important to note here, that most of the
proposed grammars are unification-based which is an im-
portant common property with respect to current parsing
granunars.
</footnote>
<page confidence="0.999807">
31
</page>
<bodyText confidence="0.999810727272727">
cal components. On the one hand, existing prob-
lems with this separation emerge, on the other
hand uniform architectures will serve as an im-
portant (linguistic) basis to achieve first solutions
for the problems.
In the next section I will discuss important
problems and restrictions with the modular de-
sign of current generation systems and will then
show why a uniform architecture as the gram-
matical basis can contribute to solutions of the
problems.
</bodyText>
<sectionHeader confidence="0.987212" genericHeader="method">
2 Modularity in Generation
Systems
</sectionHeader>
<bodyText confidence="0.997026666666667">
The Problem It is widely accepted to cut
down the problem of natural language generation
(NLG) into two subtasks:
</bodyText>
<listItem confidence="0.9992705">
• determination of the content of an utterance
• determination of its linguistic realization
</listItem>
<bodyText confidence="0.79758225">
This &apos;divide and conquer&apos; view of generation is
the base of current architectures of systems. With
few exceptions (e.g., (Appelt, 1985)) the following
two components are assumed:
</bodyText>
<listItem confidence="0.9999365">
• &apos;what to say&apos; part (strategic component)
• &apos;how to say it&apos; part (tactical component)
</listItem>
<bodyText confidence="0.984128636363637">
But, as it has been demonstrated by some au-
thors ((Appelt, 1985), (Hovy, 1987), (Rubinoff,
1988), (Neumann, 1991), (Reithinger, 1991)) it
is not possible to separate the two phases of the
generation process completely, e.g., in the case
of lexical gaps, choice between near synonyms or
paraphrases.
Currently, in systems where the separation is
advocated the problems are sometimes &apos;solved&apos;
in such a way that the strategic component has
to provide all information needed by the tactical
component to make decisions about lexical and
syntactic choices (McDonald, 1983), (McKeown,
1985), (Busemann, 1990), (Horacek, 1990). As a
consequence, this implies that the input for tac-
tical components is tailored to determine a good
sentence, making the use of powerful grammatical
processes redundant. In such approaches, tactical
components are only front–ends and the strategic
component needs detailed information about the
language to use.
Hence, they are not separate modules because
this implies that both components share the
grammar. As pointed out in Fodor (1983) one of
the characteristic properties of a module is that
it is computationally autonomous. But a rele-
vant consideration of computationally autonomy
is that modules do not share sources (in our case
the grammar).
Looking for More Symmetric Architec-
tures To maintain the modular design a more
symmetric division into strategic and tactical sep-
aration is needed:
</bodyText>
<listItem confidence="0.93102025">
• Strategic component —+ primarly concerned
with conceptual decisions
• Tactical component primarly concerned
with linguistic decisions
</listItem>
<bodyText confidence="0.9274438">
A consequence of this view is that the strate-
gic component has no detailed information about
the specific grammar and lexicon. This means
that in general a message which is constructed
precisely enough to satisfy the strategic compo-
nent&apos;s goal can be underspecified from the tactical
viewpoint. For example, if the strategic compo-
nent specifies as input to the tactical component
that &apos;Peter loves Maria&apos;, and &apos;Maria&apos; is the cur-
rent focus, then in German it is possible to utter:
</bodyText>
<listItem confidence="0.4465996">
1 Maria wird von Peter geliebt
&apos;Maria is loved by Peter&apos;
or
2 Maria liebt Peter
&apos;Maria, Peter loves&apos;
</listItem>
<bodyText confidence="0.998873875">
Of course, a &apos;real&apos; generation system needs to
choose between the possible paraphrases. An
adequate generation system should avoid to ut-
ter 2 because for this utterance there exists also
the unmarked reading that &apos;Maria loves Peter&apos;.
As long as the strategic component has no de-
tailed knowledge of a specific grammar it could
not express &apos;choose the passive form to avoid am-
biguity&apos;. But then the process can only choose
randomly between paraphrases during generation
and this means that the underlying message will
possibly not be conveyed.
There is also psychologically grounded evidence
to assume that the input to a tactical component
might not be necessary and sufficient to make lin-
guistic decisions. This is best observed in exam-
</bodyText>
<page confidence="0.998611">
32
</page>
<bodyText confidence="0.999894441176471">
pies of self—correction (Levelt, 1989). For exam-
ple, in the following utterance:3
&amp;quot;but aaa, bands like aaa- aaa- aaa- errr-
like groups, not bands, - groups, you
know what I mean like aaa.&amp;quot;
the speaker discovers two words (the near-
synonymous &apos;group&apos; and &apos;band&apos;) each of which
comes close to the underlying concept and has
problems to decide which one is the most suit-
able. In this case, the problem is because of a
mis—match between what the strategic compo-
nent want to express and what the language is
capable to express (Rubinoff, 1988).
Current Approaches In order to be able to
handle these problems, more flexible tactical com-
ponents are necessary that are able to handle e.g.
underspecified input. In (Hovy, 1987), (Finkler
and Neumann, 1989) and (Reithinger, 1991) ap-
proaches are described how such more flexible
components can be achieved. A major point of
these systems is to assume a bidirectional flow
of control between the strategic and the tactical
components.
The problem with systems where a high degree
of feedback between the strategic and the tactical
components is necessary in order to perform the
generation task is that one component could not
perform its specific task without the help of the
other. But when the mode of operation of e.g.
the tactical component is continuously influenced
by feedback from the strategic component then
the tactical component will lose its autonomy and
consequently this means that it is not a module
(see also (Levelt, 1989)).
</bodyText>
<sectionHeader confidence="0.9731625" genericHeader="method">
3 Integration of Parsing and
Generation
</sectionHeader>
<bodyText confidence="0.999408333333333">
A promising approach for achieving more au-
tonomous tactical components is to integrate gen-
eration and parsing in a more strict way. By this
</bodyText>
<listItem confidence="0.976603777777778">
I mean:
• the use of resulting structures of one direc-
tion directly in the other direction,
3This example is taken from Rubinoff (1988) and is
originally from a corpus of speech collected at the Univer-
sity of Pennsylvania.
• the use of one mode of operation (e.g., pars-
ing) for monitoring the other mode (e.g., gen-
eration).
</listItem>
<bodyText confidence="0.975980047619048">
A main thesis of this paper is that the best
way to achieve such integrated approach is to use
a uniform grammatical process as the linguistic
basis of a tactical component.
Use of Same Structures in Both Directions
If parsing and generation share the same data (i.e.
grammar and lexicon) then it is possible that re-
sulting structures of one direction could be used
directly in the other direction. For example, dur-
ing the generation of paraphrases of the ambigu-
ous utterance &apos;Remove the folder with the sys-
tem tools.&apos; the generator can use directly the
analysed structures of the two NPs &apos;the folder&apos;
and &apos;the system tools&apos; in the corresponding para-
phrases. In a similiar way parsing and generation
of elliptic utterances can also be performed more
efficiently. For example, consider the following
dialog between person A and B:
A: &apos;Peter comes to the party tonight.&apos;
B: &apos;Mary, too.&apos;
In order to be able to parse B&apos;s utterance A can
directly use parts of the grammatical structure of
his own utterance in order to complete the elliptic
structure.4
Adaptability to Language Use of Others
Another very important argument for the use of
uniform knowledge sources is the possibility to
model the assumption that during communica-
tion the use of language of one interlocutor is
influenced by means of the language use of the
others.
For example, in a uniform lexicon it does not
matter wether the lexeme was accessed during
parsing or generation. This means that the use of
linguistic elements of the interlocutor influences
the choice of lexical material during generation
if the frequency of lexemes will serve as a deci-
sion criterion. This will help to choose between
lexemes which are synonymous in the actual situ-
ation or when the semantic input cannot be suffi-
ciently specified. E.g. some drinking-devices can
be denoted either &apos;cup&apos; or &apos;mug&apos; because their
</bodyText>
<footnote confidence="0.917386">
41n this particular case, A can use the whole VP &apos;will
come to the party&apos;. In general the process is more compli-
cated e.g., if B&apos;s answer would be &apos;Mary and John, too&apos;.
</footnote>
<page confidence="0.998796">
33
</page>
<bodyText confidence="0.999551625">
shape cannot be interpreted unequivocally. An
appropriate choice would be to use the same lex-
eme that was previously used by the hearer (if no
other information is available), in order to ensure
that the same object will be denoted. In prin-
ciple this is also possible for the choice between
alternative syntactic structures.
This adaptability to the use of language of part-
ners in communication is one of the sources for
the fact that the global generation process of hu-
mans is flexible and efficient. Of course, adapt-
ability is also a kind of co—operative behaviour.
This is necessary if new ideas have to be expressed
for which no mutually known linguistic terms ex-
ist (e.g., during communication between experts
and novices). In this case adaptability to the use
of language of the hearer is necessary in order
to make possible that the hearer will be able to
understand the new information.
In principle this kind of adaptability means
that the structures of the input computed during
the understanding process carry some informa-
tion that can be used to parametrize the genera-
tion process. This leads to more flexibility: not
all necessary parameters need to be specified in
the input of a generator because decision points
can also be set during run—time.
This dynamic behaviour of a generation system
will increase efficiency, too. As McDonald et al.
(1987) define, one generator design is more effi-
cient than another, if it is able to solve the same
problem with fewer steps. They argue that&amp;quot;the
key element governing the difficulty of utterance
production is the degree of familiarity with the
situation&amp;quot;. The efficiency of the generation pro-
cess depends on the competence and experience
one has acquired for a particular situation. But
to have experience in the use of linguistic objects
that are adequate in a particular situation means
to be adaptable.
Monitoring As Levelt (1989) pointed out
&amp;quot;speakers monitor what they are saying and how
they are saying it&amp;quot;, i.e. they monitor not only for
meaning but also for linguistic well—formedness.
To be able to monitor what one is saying is very
important for processing of underspecified input
and hence for achieving a more balanced divison
of the generation task (see sec. 2). For exam-
ple to choose between the two paraphrases of the
example in sec. 2, the tactical component could
parse the resulting strings in order to decide to
choose the less ambiguous string &apos;Mary is loved
by Peter.&apos; It only needs to know from the strate-
gic component that unambiguous utterances are
preferred (as a pragmactic constraint).
In Levelt&apos;s model parsing and generation are
performed in an isolated way by means of two
different grammars. In such flow of control the
complete structure has to be generated again if
ambiguities are detected that have to be avoided.
If, for example an intelligent help-system that
supports a user by using an operation research
system (e.g. Unix, (Wilensky et al., 1984)), re-
ceives as input the utterance &amp;quot;Remove the folder
with the system tools&amp;quot; then the system is not able
to perform the corresponding action directly be-
cause it is ambiguous. But the system could ask
the user &amp;quot;Do you mean &apos;Remove the folder by
means of the system tools&apos; or &apos;Remove the folder
that contains the system tools&apos; &amp;quot;. This situation
is summarized in the following figure (LF and
LF symbolize two readings of S):
</bodyText>
<table confidence="0.886495571428571">
LF&apos; LF&amp;quot;
S: Remove the folder with
the system tools
S&apos;: Remove the folder by means of
the system tools
S&amp;quot;: Remove the folder that contains
the system tools
</table>
<figureCaption confidence="0.9835445">
Figure 1: Relationship between ambiguities and
paraphrases
</figureCaption>
<bodyText confidence="0.999121636363636">
If parsing and generation are performed in an
isolated way then generation of paraphrases can
be very inefficient, because the source of the am-
biguous utterance S is not used directly to guide
the generation process.
Generation of Paraphrases In order to clar-
ify, why an integrated approach can help to solve
the problem I will consider the problem of gener-
ation of paraphrases in more detail.
If a reversible grammar is used in both direc-
tions then the links between the strings and logi-
</bodyText>
<page confidence="0.991423">
34
</page>
<figure confidence="0.4333875">
phon: (remove the folder with the system tools)
synsem : S[imp
dtrs : [
phon: (remove) 1
synsem: V P[f in]
rphon : (the folder)
synsem : N P[acc]
adjunct : P P[&lt; with the system tools &gt;]
comp: (
head:
</figure>
<figureCaption confidence="0.9999555">
Figure 2: &apos;with the system tools&apos; in modifier position of the VP
Figure 3: The same PP as a modifier of the NP &apos;the folder&apos;
</figureCaption>
<figure confidence="0.5706107">
dtrs :
phon : (remove the folder with the system tools)
synsem: S[imp
head: [phon : (remove) ]
synsem : V P[f in]
phon : (the folder with the system tools)
synsem: N P[acc]
dirs r head: N P[&lt; the folder &gt;]
adjunct : PP[&lt; with the system tools &gt;
&apos;comp: (
</figure>
<figureCaption confidence="0.282776">
cal forms in fig. 1 are bidirectional.5
</figureCaption>
<bodyText confidence="0.971604125">
A first naive algorithm that performs genera-
tion of paraphrases using a reversible grammar
can be described as follows: Suppose S is the in-
put for the parser then the set
{(S,r LF&apos;), (S, LF&amp;quot;)}
is computed. Now LF&apos; respectively LF&amp;quot; is given
as input to the generator to compute possible
paraphrases. The sets
</bodyText>
<equation confidence="0.627147">
{(LF&apos;, S&apos;), (LF&apos;, S)}
respectively
{(LF&amp;quot;, S), (LF&amp;quot;, S&amp;quot;)}
</equation>
<bodyText confidence="0.979143805555555">
result. By means of comparison of the elements
of the sets obtained during generation with the
5It is not of central role here wether the &apos;competence&apos;
grammar is actually compiled in two efficient parsing and
generation grammars as long as the symmetry property is
not affected. This inherent property of a reversible grain-
mar is very important in the case of generation of para-
phrases because it ensures that the ambiguous structure
and the corresponding paraphrases are related together.
If this would not be ihe case then this would mean that
one is only able to generate the paraphrases but not the
ambiguous structure.
set obtained during parsing one can easily deter-
mine the two paraphrases S and S, because of
the relationship between strings and logical forms
defined by the grammar.
This algorithm is naive because of the assump-
tion that it is possible to generate all possi-
ble paraphrases at once. Although &apos;all—parses&apos;
algorithms are widley used during parsing in
natural language systems a corresponding &apos;all—
paraphrases&apos; strategy is not practicle because in
general the search space during generation is
much larger (which is a consequence of the mod-
ular design discussed in sec. 2).
Of course, from a formal point of view one is
interested in algorithms that compute all gram-
matically well—formed structures — at least poten-
tially. So most of the currently developed gener-
ators and uniform algorithms assume — more or
less explictly — an all—paraphrases strategy (e.g.,
(Shieber, 1988), (Calder et al., 1989), (Shieber et
al., 1989), (Dymetman et al., 1990), (Emele and
Zajac, 1990)). But from a practical point of view
they are not directly usable in such specific situ-
ations.
</bodyText>
<page confidence="0.998385">
35
</page>
<bodyText confidence="0.999311565656566">
More Suitable Strategies A more suitable
strategy would be to generate only one para-
phrase for each ambiguous logical form. As long
as parsing and generation are performed in an iso-
lated way the problem with this strategy is that
there is no control over the choice of paraphrases.
In order to make clear this point I will look closer
to the underlying structure of the example&apos;s ut-
terances.
The problem why there are two readings is that
the PP &apos;with the system folder&apos; can be attached
into modifier position of the NP &apos;the folder&apos; (ex-
pressing the semantic relation that &apos;folder&apos; con-
tains &apos;system tools&apos;) or of the verb &apos;remove&apos; (ex-
pressing semantically that &apos;folder&apos; is the instru-
ment of the described situation). Fig. 2 and 3
(see above) show the internal grammatical struc-
ture in a HPSG—style notation (omitting details,
that are not relevant in this context).
As long as the source of the ambiguity is not
known it is possible to generate in both cases the
utterance &apos;Remove the folder with the system—
tools&apos; as a paraphrase of itself. Of course, it is
possible to compare the resulting strings with the
input string S. But because the source of the am-
biguity is not known the loop between the iso-
lated processes must be performed several times
in general.
A better strategy would be to recognize rele-
vant sources of ambiguities during parsing and
to use this information to guide the generation
process. Meteer and Shaked (1988) propose an
approach where during the repeated parse of an
ambiguous utterance potential sources of ambigu-
ity can be detected. For example when in the case
of lexical ambiguity a noun can be associated to
two semantic classes a so called &apos;lexical ambigu-
ity specialist&apos; records the noun as the ambiguity
source and the two different classes. These two
classes are then explicitly used in the generator
input and are realized as e.g. modifiers for the
ambiguous noun.
The only common knowledge source for the
paraphraser is a high order intensional logic lan-
guage called World Model Language. It serves as
the interface between parser and generator. The
problem with this approach is that parsing and
generation are performed in an isolated way using
two different grammars. If an ambiguous utter-
ance S need to be paraphrased S has to be parsed
again. During this repeated parse all potential
ambiguities have to be recognized and recorded
(i.e. have to be monitored) by means of different
&apos;ambiguity specialists&apos;. The problem here is that
also local ambiguities have to be considered that
are not relevant for the whole structure.
An Alternative Approach I will now de-
scribe the basic idea of an approach that is based
on an integrated approach where both tasks share
the same grammar. The advantage of this ap-
proach is that no repeated parse is necessary to
compute potential ambiguity sources because the
different grammatical structures determined dur-
ing parsing are used directly to guide the gen-
eration process. By means of this strategy it is
also ensured that an ambiguous utterance is not
generated as a paraphrase of itself.
In principle the algorithm works as follows:
During the generation of paraphrases the gen-
eration process is monitored in such a way that
the monitor compares in each step the resulting
structures of the generation process with the cor-
responding structures from parsing maintained in
the alternative parse trees (I will now assume that
two parse trees Pi and P2 corresponding to the
structures given in fig. 2 and 3 are obtained dur-
ing parsing). Suppose that LF (cf. fig. 1) is
specified as the input to the generator. In the case
where the generator encounters alternative gram-
matical structures to be expanded, the monitor
guides the generator by means of inspection of
the corresponding parse trees. In the case where
actual considered parts pi and p2 of Pi and P2
(e.g., same NPs) are equal the generator has to
choose the same grammatical structure that was
used to build pi and 132 (or more efficiently the
generator can use the partial structure directly as
a kind of compiled knowledge). In the case where
a partial structure of e.g. parse tree Pi has no
correspondence in P2 (cf. fig. 2 and 3) an ambi-
guity source is detected. In this case an alterna-
tive grammatical structure has to be chosen.6
At this point it should be clear that the easiest
way in order to be able to generate &apos;along parsed
structures&apos; is to use the same grammar in both
directions. In this case grammatical structures
obtained during parsing can be used directly to
restrict the potential search space during genera-
tion.
</bodyText>
<footnote confidence="0.990465714285714">
60f course, the described algorithm is too restrictive,
in order to handle non—structural (e.g. contextual) para-
phrases. But, I assume that this approach is also appli-
cable in the case of lexical amibiguities prerequisite word
meanings are structurally described by means of lexical se-
mantics (e.g., Jackendoff&apos;s Lexical Conceptual Structures
(Jackendoff, 1990))
</footnote>
<page confidence="0.998548">
36
</page>
<bodyText confidence="0.999857333333333">
This approach is not only restricted in cases
where the input is ambiguous and the para-
phrases must contrast the different meanings. It
can also be used for self-monitoring when it has
to be checked wether a produced utterance S of
an input form LF is ambiguous. In this case S will
be parsed. If during parsing e.g., two readings LF
and LF&apos; are deduced LF is generated again along
the parse tree obtained for S. Now an utterance
S&apos; can be generated that has the same meaning
but differs with respect to the ambiguity source
of S.
</bodyText>
<sectionHeader confidence="0.995227" genericHeader="method">
4 Current Work
</sectionHeader>
<bodyText confidence="0.995184070422536">
We have now started a project called BiLD (short
for Bidirectional Linguistic Deduction) at the
University of Saarland (Saarbrficken) where it
will be investigated how an integrated approach
of parsing and generation can be realized effi-
ciently by means of a uniform architecture and
how such a model can be used for increasing flex-
ibility and efficiency during natural language pro-
cessing.
The main topic &apos;of the BiLD project is the de-
velopment of a uniform parametrized deduction
process for grammatical processing. This process
constitutes the core of a flexible and symetric tac-
tical module. In Order to realize the integrated
approach and to obtain a high degree of efficiency
in both directions we will develop methods for a
declarative encoding of information of control in
the lexicon and grammar.
We follow a sign-based approach for the de-
scription of linguistic entities based on Head-
driven Phrase Structure Grammar (Pollard and
Sag, 1987) and the variant described in Reape
(1989). Besides theoretical reasons there are also
reasons with respeet to system&apos;s design criterions
to adopt this vie*, because all levels of descrip-
tions (i.e. phonological, syntactic and semantic
structure) of linguistics entities (i.e. words and
phrases) are described simultanueous in a uni-
form way by means of partial information struc-
tures. None of the levels of description has a
privileged status but expresses possible mutually
co-ocurrence restrictions of structures of different
levels.
Furthermore a high degree of lexicalism is as-
sumed so that the lexicon as a complex hierachi-
cal ordered data structure plays a central role
in BiLD. As it has been shown this lexicalized
view supports reversibility (cf. (Newman, 1990),
(Dymetman et al., 1990)) and the performing
of specific processing strategies (e.g., incremental
and parallel generation, (Neumann and Finkler,
1990)).
The task of the deduction process during gener-
ation is to construct the graphemic form of a spec-
ified feature description of a semantic form. For
example, to yield the utterance &amp;quot;A man sings.&amp;quot;
the deduction process gets as input the semantic
feature structure
and deduces the graphematic structure
[ graph: (A_man_sings.)
by means of successive application of lexical and
grammatical information. In the same way the
deduction process computes from the graphe-
matic structure an appropriate semantic struc-
ture in parsing direction. A first prototype based
on head-driven bottom-up strategy is now under
development (cf. (van Noord, 1991)).
A major aspect of the BiLD project is that
a specific parametrization of the deduction pro-
cess is represented in the lexicon as well as in the
grammar to obtain efficient structures of control
(Uszkoreit, 1991). The main idea is that pref-
erence values are assigned to the elements (dis-
juncts or conjuncts) of feature descriptions. For
example, in HPSG all lexical entries are put to-
gether into one large disjunctive form. From a
purely declarative point of view these elements
are unordered. But a preference structure is used
during processing in order to guide the process
of lexical choice efficiently which itself influences
the grammatical process.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99995925">
A main thesis of this paper was to show that ex-
isting problems with the modular design of cur-
rent generation systems emerge when a reversible
grammar is used. In order to maintain the mod-
ular design I have proposed an approach that
is based on a strong integration of parsing and
generation of grammatical structures using a re-
versible grammar and monitoring mechanisms.
</bodyText>
<figure confidence="0.946979777777778">
101
agens :
restr : [pred : man&apos;
var :
rel : sing&apos;
quant : exists&apos;
var :
[
a
</figure>
<page confidence="0.996325">
37
</page>
<bodyText confidence="0.998525666666667">
By means of such an integrated approach per-
forming of e.g. generation of paraphrases can be
done more easier and efficently.
</bodyText>
<sectionHeader confidence="0.989589" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.7794475">
This research was supported by SFB 314, Project
N3 BiLD.
</bodyText>
<sectionHeader confidence="0.920793" genericHeader="references">
Bibliography
</sectionHeader>
<reference confidence="0.998359035714286">
Douglas E. Appelt. Planning English Sentences.
Cambridge University Press, Cambridge, 1985.
Stefan Busemann. Generierung natiirlicher
Sprache mit Generalisierten Phrasenstruktur-
Grammatiken. PhD thesis, University of Saar-
land (Saarbrficken), 1990.
Jonathan Calder, Mike Reape, and Henk Zeevat.
An algorithm for generation in unification cat-
egorial grammar. In Fourth Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 233-240, Manch-
ester, 1989.
Robert Dale. Generating receipes: An overview
of epicure. In Robert Dale, Chris Mellish,
and Michael Zock, editors, Current Research in
Natural Language Generation, pages 229-255.
Academic Press, London, 1990.
Marc Dymetman, Pierre Isabelle, and Francois
Perrault. A symmetrical approach to parsing
and generation. In Proceedings of the 13th In-
ternational Conference on Computational Lin-
guistics (COLING), pages 90-96, Helsinki,
1990.
Martin Emele and Remi Zajac. Typed unification
grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguis-
tics (COLING), pages 293-298, Helsinki, 1990.
Wolfgang Finkler and Gunter Neumann. Popel-
how: A distributed parallel model for incre-
mental natural language production with feed-
back. In Proceedings of the Eleventh Inter-
national Joint Conference on Artificial Intel-
ligence, pages 1518-1523, Detroit, 1989.
Jerry A. Fodor. The Modularity of Mind: An Es-
say on Faculty Psychology. A Bradford Book,
MIT Press, Cambridge, Massachusetts, 1983.
Lyn Frazier. Shared components of production
and perception. In M. A. Arbib et al., editor,
Neural Models of Language Processes, pages
225-236. Academic Press, New York, 1982.
Merrill F. Garrett. Remarks on the relation be-
tween language production and language com-
prehension systems. In M. A. Arbib et al.,
editor, Neural Models of Language Processes,
pages 209-224. Academic Press, New York,
1982.
K. Hasida and S. Isizaki. Dependency propa-
gation: A unified theory of sentence compre-
hension and generation. In Proceedings of the
Tenth International Joint Conference on Artifi-
cial Intelligence, pages 664-670, Mailand, 1987.
Helmut Horacek. The architecture of a genera-
tion component in a complete natural language
dialogue system. In Robert Dale, Chris Mel-
lish, and Michael Zock, editors, Current Re-
search in Natural Language Generation, pages
193 - 227. Academic Press, London, 1990.
Eduard. H. Hovy. Generating Natural Language
under Pragmatic Constraints. PhD thesis, Yale
University, 1987.
Ray Jackendoff. Consciousness and the Compu-
tational Mind. MIT Press, Cambridge, Mas-
sachusetts, 1987.
Ray Jackendoff. Semantic Structures. MIT
Press, Cambridge, Massachusetts, 1990.
Willem J. M. Levelt. Speaking: From Intention
to Articulation. MIT Press, Cambridge, Mas-
sachusetts, 1989.
David D. McDonald. Natural language genera-
tion as a computational problem: An intro-
duction. In M. Brady and C. Berwick, edi-
tors, Computational Models of Discourse. MIT
Press, Cambridge, Massachusetts, 1983.
David D. McDonald, Marie W. Meteer, and
James D. Pustejovsky. Factors contributing to
efficiency in natural language generation. In
K. Kempen, editor, Natural Language Gener-
ation: New Results in Artificial Intelligence,
Psychology and Linguistics, pages 159-182.
Martinus Nijhoff, Dordrecht, 1987.
Kathleen R. McKeown. Text Generation: Using
Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Cambridge
University Press, Cambridge, 1985.
</reference>
<page confidence="0.985329">
38
</page>
<reference confidence="0.99963584">
Kathleen R. McKeown, Michael Elhadad, Yu-
miko Fukomoto, Jong Lim, Christine Lom-
bardi, Jacques Robin, and Frank Smadja. Nat-
ural language generation in comet. In Robert
Dale, Chris Mellish, and Michel Zock, editors,
Current Research in Natural Language Genera-
tion, pages 103 — 139. Academic Press, London,
1990.
Marie Meteer and Varda Shaked. Strategies for
effective paraphrasing. In Proceedings of the
12th International Conference on Computa-
tional Linguistics (COLING), Budapest, 1988.
Gunter Neumann. A bidirectional model for nat-
ural language processing. In Fifth Conference
of the European Chapter of the Association
for Computational Linguistics, pages 245-250,
Berlin, 1991.
Gunter Neumann and Wolfgang Finkler. A head-
driven approach to incremental and parallel
generation of syntactic structures. In Proceed-
ings of the 13th International Conference on
Computational Linguistics (COLING), pages
288-293, Helsinki, 1990.
Paula Newman. Towards convenient bi—
directional grammar formalisms. In Proceed-
ings of the 13th International Conference on
Computational Linguistics (COLING), pages
294-298, Helsinki, 1990.
Carl Pollard and Ivan Sag. Information Based
Syntax and Semantics, Volume 1. Center for
the Study of Language and Information Stan-
ford, 1987.
Mike Reape. A logical treatment of semi—free
word order and bounded discontinuous con-
stituency. In Fourth Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics, pages 103-110, Manchester,
1989.
Norbert Reithinger. Popel: A parallel and in-
cremental natural language generation system.
In C. L. Paris et al., editor, Natural Language
Generation in Artificial Intelligence and Com-
putational Linguistics, pages 179-199. Kluwer,
1991.
Robert Rubinoff. A cooperative model of strat-
egy and tactics in generation. In Paper
presented at the&apos; Fourth International Work-
shop on Natuna Language Generation, Santa
Catalina Island, 1988.
Stuart M. Shieber. A uniform architecture for
parsing and generation. In Proceedings of the
12th International Conference on Computa-
tional Linguistics (COLING), Budapest, 1988.
Stuart M. Shieber, Gertjan van Noord, Robert C.
Moore, and Fernando C.N. Pereira. A
semantic-head-driven generation algorithm for
unification based formalisms. In 27th Annual
Meeting of the Association for Computational
Linguistics, Vancouver, 1989.
Hans Uszkoreit. Strategies for adding control in-
formation to declarative grammars. In 29th
Annual Meeting of the Association for Com-
putational Linguistics, Berkeley, 1991.
Gertjan van Noord. Reversible unification-based
machine translation. In Proceedings of the
13th International Conference on Computa-
tional Linguistics (COLING), Helsinki, 1990.
Gertjan van Noord. Towards uniform process-
ing for constraint-based categorial grammars.
In Proceedings of the ACL Workshop on Re-
versible Grammar in Natural Language Pro-
cessing, Berkeley, 1991.
R. Wilensky, Y. Arens, and D. Chin. Talking to
unix in english: An overview of uc. Communi-
cations of the ACM, pages 574 — 593, 1984.
</reference>
<page confidence="0.99953">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.028452">
<title confidence="0.992754">REVERSIBILITY AND MODULARITY NATURAL LANGUAGE GENERATION</title>
<author confidence="0.6670885">Giinter Neumann Lehrstuhl fiir</author>
<affiliation confidence="0.552406">SFB 314, Projekt N3 Universitat des</affiliation>
<address confidence="0.3534305">Im Stadtwald 15 D-6600 Saarbriicken 11,</address>
<email confidence="0.739207">neumannOcoli.uni-sb.de</email>
<abstract confidence="0.996249444444445">A consequent use of reversible grammars within natural language generation systems has strong implications for the separation into strategic and tactical components. A central goal of this paper is to make plausible that a uniform architecture for grammatical processing will serve as a basis to achieve more flexible and efficient generation systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning English Sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="4174" citStr="Appelt, 1985" startWordPosition="653" endWordPosition="654">e next section I will discuss important problems and restrictions with the modular design of current generation systems and will then show why a uniform architecture as the grammatical basis can contribute to solutions of the problems. 2 Modularity in Generation Systems The Problem It is widely accepted to cut down the problem of natural language generation (NLG) into two subtasks: • determination of the content of an utterance • determination of its linguistic realization This &apos;divide and conquer&apos; view of generation is the base of current architectures of systems. With few exceptions (e.g., (Appelt, 1985)) the following two components are assumed: • &apos;what to say&apos; part (strategic component) • &apos;how to say it&apos; part (tactical component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information need</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Douglas E. Appelt. Planning English Sentences. Cambridge University Press, Cambridge, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Busemann</author>
</authors>
<title>Generierung natiirlicher Sprache mit Generalisierten PhrasenstrukturGrammatiken.</title>
<date>1990</date>
<tech>PhD thesis,</tech>
<institution>University of Saarland (Saarbrficken),</institution>
<contexts>
<context position="4908" citStr="Busemann, 1990" startWordPosition="766" endWordPosition="767"> component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is tailored to determine a good sentence, making the use of powerful grammatical processes redundant. In such approaches, tactical components are only front–ends and the strategic component needs detailed information about the language to use. Hence, they are not separate modules because this implies that both components share the grammar. As pointed out in Fodor (1983) one of the characteristic properties of a module is that it is computationally autonomous. But a relevant consideration of computationally</context>
</contexts>
<marker>Busemann, 1990</marker>
<rawString>Stefan Busemann. Generierung natiirlicher Sprache mit Generalisierten PhrasenstrukturGrammatiken. PhD thesis, University of Saarland (Saarbrficken), 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Calder</author>
<author>Mike Reape</author>
<author>Henk Zeevat</author>
</authors>
<title>An algorithm for generation in unification categorial grammar.</title>
<date>1989</date>
<booktitle>In Fourth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>233--240</pages>
<location>Manchester,</location>
<contexts>
<context position="1034" citStr="Calder et al., 1989" startWordPosition="152" endWordPosition="155">goal of this paper is to make plausible that a uniform architecture for grammatical processing will serve as a basis to achieve more flexible and efficient generation systems. 1 Introduction In general, the goal of parsing is the derivation of all possible grammatical structures defined by a grammar of a given string a (i.e. especially the determination of all possible logical forms of a) and the goal of the corresponding generation task is the computation of all possible strings defined by a grammar of al given logical form (I) that are logically equivalent to (1) (see also (Shieber, 1988), (Calder et al., 1989)). Recently, there is a strong tendency to use the same grammar for performing both tasks. Besides more practically motivated reasons - obtaining more compact systems or avoidance of inconsistencies between the input and output of a syStem - there are also theoretical (a single model of language behaviour) and psychological evidences (empirical evidence for shared processors or facilities, cf. (Garrett, 1982), (Frazier, 1982), (Jackendoff, 1987)) to adopt this view. From a formal point of view the main interest in obtaining non—directional grammars is the specification of the relationship betw</context>
<context position="17781" citStr="Calder et al., 1989" startWordPosition="2936" endWordPosition="2939">nce. Although &apos;all—parses&apos; algorithms are widley used during parsing in natural language systems a corresponding &apos;all— paraphrases&apos; strategy is not practicle because in general the search space during generation is much larger (which is a consequence of the modular design discussed in sec. 2). Of course, from a formal point of view one is interested in algorithms that compute all grammatically well—formed structures — at least potentially. So most of the currently developed generators and uniform algorithms assume — more or less explictly — an all—paraphrases strategy (e.g., (Shieber, 1988), (Calder et al., 1989), (Shieber et al., 1989), (Dymetman et al., 1990), (Emele and Zajac, 1990)). But from a practical point of view they are not directly usable in such specific situations. 35 More Suitable Strategies A more suitable strategy would be to generate only one paraphrase for each ambiguous logical form. As long as parsing and generation are performed in an isolated way the problem with this strategy is that there is no control over the choice of paraphrases. In order to make clear this point I will look closer to the underlying structure of the example&apos;s utterances. The problem why there are two readi</context>
</contexts>
<marker>Calder, Reape, Zeevat, 1989</marker>
<rawString>Jonathan Calder, Mike Reape, and Henk Zeevat. An algorithm for generation in unification categorial grammar. In Fourth Conference of the European Chapter of the Association for Computational Linguistics, pages 233-240, Manchester, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating receipes: An overview of epicure.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>229--255</pages>
<editor>In Robert Dale, Chris Mellish, and Michael Zock, editors,</editor>
<publisher>Academic Press,</publisher>
<location>London,</location>
<contexts>
<context position="2675" citStr="Dale, 1990" startWordPosition="414" endWordPosition="415">(Hasida and Isizaki, 1987), (Shieber, 1988), (Dymetman et al., 1990), (Emele and Zajac, 1990)). A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the &apos;real&apos; generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 1990), (McKeown et al., 1990), (Reithinger, 1991)).2 The purpose of this paper is to show that the use of a uniform architecture for grammatical processing has important influences for the whole generation task. A consequent use of a uniform process within a natural language generation system affects the separation into strategic and tacti11 assume a notion of grammars that integrate phonological, syntactical and semantics&apos; levels of description, e.g., (Pollard and Sag, 1987). 2But it is important to note here, that most of the proposed grammars are unification-based which is an im</context>
</contexts>
<marker>Dale, 1990</marker>
<rawString>Robert Dale. Generating receipes: An overview of epicure. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation, pages 229-255. Academic Press, London, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
<author>Pierre Isabelle</author>
<author>Francois Perrault</author>
</authors>
<title>A symmetrical approach to parsing and generation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>90--96</pages>
<location>Helsinki,</location>
<contexts>
<context position="2132" citStr="Dymetman et al., 1990" startWordPosition="326" endWordPosition="329">a formal point of view the main interest in obtaining non—directional grammars is the specification of the relationship between strings and logical forms.&apos; According to van Noord (1990), a grammar is reversible if the parsing and generation problem is computable and the relation between strings and logical forms is symmetric. In this case parsing and generation are viewed as mutually inverse processes. Furthermore there are also approaches that assume that it is possible to use the same algorithm for processing the grammar in both directions (e.g. (Hasida and Isizaki, 1987), (Shieber, 1988), (Dymetman et al., 1990), (Emele and Zajac, 1990)). A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the &apos;real&apos; generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 1990), (McKeown et al., 1990), (Reithinger, 1</context>
<context position="17830" citStr="Dymetman et al., 1990" startWordPosition="2944" endWordPosition="2947">y used during parsing in natural language systems a corresponding &apos;all— paraphrases&apos; strategy is not practicle because in general the search space during generation is much larger (which is a consequence of the modular design discussed in sec. 2). Of course, from a formal point of view one is interested in algorithms that compute all grammatically well—formed structures — at least potentially. So most of the currently developed generators and uniform algorithms assume — more or less explictly — an all—paraphrases strategy (e.g., (Shieber, 1988), (Calder et al., 1989), (Shieber et al., 1989), (Dymetman et al., 1990), (Emele and Zajac, 1990)). But from a practical point of view they are not directly usable in such specific situations. 35 More Suitable Strategies A more suitable strategy would be to generate only one paraphrase for each ambiguous logical form. As long as parsing and generation are performed in an isolated way the problem with this strategy is that there is no control over the choice of paraphrases. In order to make clear this point I will look closer to the underlying structure of the example&apos;s utterances. The problem why there are two readings is that the PP &apos;with the system folder&apos; can b</context>
<context position="25180" citStr="Dymetman et al., 1990" startWordPosition="4179" endWordPosition="4182">els of descriptions (i.e. phonological, syntactic and semantic structure) of linguistics entities (i.e. words and phrases) are described simultanueous in a uniform way by means of partial information structures. None of the levels of description has a privileged status but expresses possible mutually co-ocurrence restrictions of structures of different levels. Furthermore a high degree of lexicalism is assumed so that the lexicon as a complex hierachical ordered data structure plays a central role in BiLD. As it has been shown this lexicalized view supports reversibility (cf. (Newman, 1990), (Dymetman et al., 1990)) and the performing of specific processing strategies (e.g., incremental and parallel generation, (Neumann and Finkler, 1990)). The task of the deduction process during generation is to construct the graphemic form of a specified feature description of a semantic form. For example, to yield the utterance &amp;quot;A man sings.&amp;quot; the deduction process gets as input the semantic feature structure and deduces the graphematic structure [ graph: (A_man_sings.) by means of successive application of lexical and grammatical information. In the same way the deduction process computes from the graphematic struct</context>
</contexts>
<marker>Dymetman, Isabelle, Perrault, 1990</marker>
<rawString>Marc Dymetman, Pierre Isabelle, and Francois Perrault. A symmetrical approach to parsing and generation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), pages 90-96, Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
<author>Remi Zajac</author>
</authors>
<title>Typed unification grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>293--298</pages>
<location>Helsinki,</location>
<contexts>
<context position="2157" citStr="Emele and Zajac, 1990" startWordPosition="330" endWordPosition="334">e main interest in obtaining non—directional grammars is the specification of the relationship between strings and logical forms.&apos; According to van Noord (1990), a grammar is reversible if the parsing and generation problem is computable and the relation between strings and logical forms is symmetric. In this case parsing and generation are viewed as mutually inverse processes. Furthermore there are also approaches that assume that it is possible to use the same algorithm for processing the grammar in both directions (e.g. (Hasida and Isizaki, 1987), (Shieber, 1988), (Dymetman et al., 1990), (Emele and Zajac, 1990)). A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the &apos;real&apos; generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 1990), (McKeown et al., 1990), (Reithinger, 1991)).2 The purpose of th</context>
<context position="17855" citStr="Emele and Zajac, 1990" startWordPosition="2948" endWordPosition="2951">natural language systems a corresponding &apos;all— paraphrases&apos; strategy is not practicle because in general the search space during generation is much larger (which is a consequence of the modular design discussed in sec. 2). Of course, from a formal point of view one is interested in algorithms that compute all grammatically well—formed structures — at least potentially. So most of the currently developed generators and uniform algorithms assume — more or less explictly — an all—paraphrases strategy (e.g., (Shieber, 1988), (Calder et al., 1989), (Shieber et al., 1989), (Dymetman et al., 1990), (Emele and Zajac, 1990)). But from a practical point of view they are not directly usable in such specific situations. 35 More Suitable Strategies A more suitable strategy would be to generate only one paraphrase for each ambiguous logical form. As long as parsing and generation are performed in an isolated way the problem with this strategy is that there is no control over the choice of paraphrases. In order to make clear this point I will look closer to the underlying structure of the example&apos;s utterances. The problem why there are two readings is that the PP &apos;with the system folder&apos; can be attached into modifier </context>
</contexts>
<marker>Emele, Zajac, 1990</marker>
<rawString>Martin Emele and Remi Zajac. Typed unification grammars. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), pages 293-298, Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Finkler</author>
<author>Gunter Neumann</author>
</authors>
<title>Popelhow: A distributed parallel model for incremental natural language production with feedback.</title>
<date>1989</date>
<booktitle>In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1518--1523</pages>
<location>Detroit,</location>
<contexts>
<context position="7932" citStr="Finkler and Neumann, 1989" startWordPosition="1255" endWordPosition="1258">rrlike groups, not bands, - groups, you know what I mean like aaa.&amp;quot; the speaker discovers two words (the nearsynonymous &apos;group&apos; and &apos;band&apos;) each of which comes close to the underlying concept and has problems to decide which one is the most suitable. In this case, the problem is because of a mis—match between what the strategic component want to express and what the language is capable to express (Rubinoff, 1988). Current Approaches In order to be able to handle these problems, more flexible tactical components are necessary that are able to handle e.g. underspecified input. In (Hovy, 1987), (Finkler and Neumann, 1989) and (Reithinger, 1991) approaches are described how such more flexible components can be achieved. A major point of these systems is to assume a bidirectional flow of control between the strategic and the tactical components. The problem with systems where a high degree of feedback between the strategic and the tactical components is necessary in order to perform the generation task is that one component could not perform its specific task without the help of the other. But when the mode of operation of e.g. the tactical component is continuously influenced by feedback from the strategic comp</context>
</contexts>
<marker>Finkler, Neumann, 1989</marker>
<rawString>Wolfgang Finkler and Gunter Neumann. Popelhow: A distributed parallel model for incremental natural language production with feedback. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pages 1518-1523, Detroit, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry A Fodor</author>
</authors>
<title>The Modularity of Mind: An Essay on Faculty Psychology. A Bradford Book,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="5369" citStr="Fodor (1983)" startWordPosition="836" endWordPosition="837"> information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is tailored to determine a good sentence, making the use of powerful grammatical processes redundant. In such approaches, tactical components are only front–ends and the strategic component needs detailed information about the language to use. Hence, they are not separate modules because this implies that both components share the grammar. As pointed out in Fodor (1983) one of the characteristic properties of a module is that it is computationally autonomous. But a relevant consideration of computationally autonomy is that modules do not share sources (in our case the grammar). Looking for More Symmetric Architectures To maintain the modular design a more symmetric division into strategic and tactical separation is needed: • Strategic component —+ primarly concerned with conceptual decisions • Tactical component primarly concerned with linguistic decisions A consequence of this view is that the strategic component has no detailed information about the specif</context>
</contexts>
<marker>Fodor, 1983</marker>
<rawString>Jerry A. Fodor. The Modularity of Mind: An Essay on Faculty Psychology. A Bradford Book, MIT Press, Cambridge, Massachusetts, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
</authors>
<title>Shared components of production and perception.</title>
<date>1982</date>
<booktitle>Neural Models of Language Processes,</booktitle>
<pages>225--236</pages>
<editor>In M. A. Arbib et al., editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="1463" citStr="Frazier, 1982" startWordPosition="220" endWordPosition="221">n task is the computation of all possible strings defined by a grammar of al given logical form (I) that are logically equivalent to (1) (see also (Shieber, 1988), (Calder et al., 1989)). Recently, there is a strong tendency to use the same grammar for performing both tasks. Besides more practically motivated reasons - obtaining more compact systems or avoidance of inconsistencies between the input and output of a syStem - there are also theoretical (a single model of language behaviour) and psychological evidences (empirical evidence for shared processors or facilities, cf. (Garrett, 1982), (Frazier, 1982), (Jackendoff, 1987)) to adopt this view. From a formal point of view the main interest in obtaining non—directional grammars is the specification of the relationship between strings and logical forms.&apos; According to van Noord (1990), a grammar is reversible if the parsing and generation problem is computable and the relation between strings and logical forms is symmetric. In this case parsing and generation are viewed as mutually inverse processes. Furthermore there are also approaches that assume that it is possible to use the same algorithm for processing the grammar in both directions (e.g.</context>
</contexts>
<marker>Frazier, 1982</marker>
<rawString>Lyn Frazier. Shared components of production and perception. In M. A. Arbib et al., editor, Neural Models of Language Processes, pages 225-236. Academic Press, New York, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Merrill F Garrett</author>
</authors>
<title>Remarks on the relation between language production and language comprehension systems.</title>
<date>1982</date>
<booktitle>Neural Models of Language Processes,</booktitle>
<pages>209--224</pages>
<editor>In M. A. Arbib et al., editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="1446" citStr="Garrett, 1982" startWordPosition="218" endWordPosition="219">ponding generation task is the computation of all possible strings defined by a grammar of al given logical form (I) that are logically equivalent to (1) (see also (Shieber, 1988), (Calder et al., 1989)). Recently, there is a strong tendency to use the same grammar for performing both tasks. Besides more practically motivated reasons - obtaining more compact systems or avoidance of inconsistencies between the input and output of a syStem - there are also theoretical (a single model of language behaviour) and psychological evidences (empirical evidence for shared processors or facilities, cf. (Garrett, 1982), (Frazier, 1982), (Jackendoff, 1987)) to adopt this view. From a formal point of view the main interest in obtaining non—directional grammars is the specification of the relationship between strings and logical forms.&apos; According to van Noord (1990), a grammar is reversible if the parsing and generation problem is computable and the relation between strings and logical forms is symmetric. In this case parsing and generation are viewed as mutually inverse processes. Furthermore there are also approaches that assume that it is possible to use the same algorithm for processing the grammar in both</context>
</contexts>
<marker>Garrett, 1982</marker>
<rawString>Merrill F. Garrett. Remarks on the relation between language production and language comprehension systems. In M. A. Arbib et al., editor, Neural Models of Language Processes, pages 209-224. Academic Press, New York, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hasida</author>
<author>S Isizaki</author>
</authors>
<title>Dependency propagation: A unified theory of sentence comprehension and generation.</title>
<date>1987</date>
<booktitle>In Proceedings of the Tenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>664--670</pages>
<location>Mailand,</location>
<contexts>
<context position="2090" citStr="Hasida and Isizaki, 1987" startWordPosition="320" endWordPosition="323">(Jackendoff, 1987)) to adopt this view. From a formal point of view the main interest in obtaining non—directional grammars is the specification of the relationship between strings and logical forms.&apos; According to van Noord (1990), a grammar is reversible if the parsing and generation problem is computable and the relation between strings and logical forms is symmetric. In this case parsing and generation are viewed as mutually inverse processes. Furthermore there are also approaches that assume that it is possible to use the same algorithm for processing the grammar in both directions (e.g. (Hasida and Isizaki, 1987), (Shieber, 1988), (Dymetman et al., 1990), (Emele and Zajac, 1990)). A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the &apos;real&apos; generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 199</context>
</contexts>
<marker>Hasida, Isizaki, 1987</marker>
<rawString>K. Hasida and S. Isizaki. Dependency propagation: A unified theory of sentence comprehension and generation. In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, pages 664-670, Mailand, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>The architecture of a generation component in a complete natural language dialogue system.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>193--227</pages>
<editor>In Robert Dale, Chris Mellish, and Michael Zock, editors,</editor>
<publisher>Academic Press,</publisher>
<location>London,</location>
<contexts>
<context position="2692" citStr="Horacek, 1990" startWordPosition="416" endWordPosition="417">izaki, 1987), (Shieber, 1988), (Dymetman et al., 1990), (Emele and Zajac, 1990)). A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the &apos;real&apos; generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 1990), (McKeown et al., 1990), (Reithinger, 1991)).2 The purpose of this paper is to show that the use of a uniform architecture for grammatical processing has important influences for the whole generation task. A consequent use of a uniform process within a natural language generation system affects the separation into strategic and tacti11 assume a notion of grammars that integrate phonological, syntactical and semantics&apos; levels of description, e.g., (Pollard and Sag, 1987). 2But it is important to note here, that most of the proposed grammars are unification-based which is an important common pr</context>
<context position="4925" citStr="Horacek, 1990" startWordPosition="768" endWordPosition="769">s it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is tailored to determine a good sentence, making the use of powerful grammatical processes redundant. In such approaches, tactical components are only front–ends and the strategic component needs detailed information about the language to use. Hence, they are not separate modules because this implies that both components share the grammar. As pointed out in Fodor (1983) one of the characteristic properties of a module is that it is computationally autonomous. But a relevant consideration of computationally autonomy is that</context>
</contexts>
<marker>Horacek, 1990</marker>
<rawString>Helmut Horacek. The architecture of a generation component in a complete natural language dialogue system. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation, pages 193 - 227. Academic Press, London, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hovy</author>
</authors>
<title>Generating Natural Language under Pragmatic Constraints.</title>
<date>1987</date>
<tech>PhD thesis,</tech>
<institution>Yale University,</institution>
<contexts>
<context position="2661" citStr="Hovy, 1987" startWordPosition="412" endWordPosition="413">ections (e.g. (Hasida and Isizaki, 1987), (Shieber, 1988), (Dymetman et al., 1990), (Emele and Zajac, 1990)). A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the &apos;real&apos; generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 1990), (McKeown et al., 1990), (Reithinger, 1991)).2 The purpose of this paper is to show that the use of a uniform architecture for grammatical processing has important influences for the whole generation task. A consequent use of a uniform process within a natural language generation system affects the separation into strategic and tacti11 assume a notion of grammars that integrate phonological, syntactical and semantics&apos; levels of description, e.g., (Pollard and Sag, 1987). 2But it is important to note here, that most of the proposed grammars are unification-based </context>
<context position="4383" citStr="Hovy, 1987" startWordPosition="688" endWordPosition="689">lutions of the problems. 2 Modularity in Generation Systems The Problem It is widely accepted to cut down the problem of natural language generation (NLG) into two subtasks: • determination of the content of an utterance • determination of its linguistic realization This &apos;divide and conquer&apos; view of generation is the base of current architectures of systems. With few exceptions (e.g., (Appelt, 1985)) the following two components are assumed: • &apos;what to say&apos; part (strategic component) • &apos;how to say it&apos; part (tactical component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactic</context>
<context position="7903" citStr="Hovy, 1987" startWordPosition="1253" endWordPosition="1254">- aaa- aaa- errrlike groups, not bands, - groups, you know what I mean like aaa.&amp;quot; the speaker discovers two words (the nearsynonymous &apos;group&apos; and &apos;band&apos;) each of which comes close to the underlying concept and has problems to decide which one is the most suitable. In this case, the problem is because of a mis—match between what the strategic component want to express and what the language is capable to express (Rubinoff, 1988). Current Approaches In order to be able to handle these problems, more flexible tactical components are necessary that are able to handle e.g. underspecified input. In (Hovy, 1987), (Finkler and Neumann, 1989) and (Reithinger, 1991) approaches are described how such more flexible components can be achieved. A major point of these systems is to assume a bidirectional flow of control between the strategic and the tactical components. The problem with systems where a high degree of feedback between the strategic and the tactical components is necessary in order to perform the generation task is that one component could not perform its specific task without the help of the other. But when the mode of operation of e.g. the tactical component is continuously influenced by fee</context>
</contexts>
<marker>Hovy, 1987</marker>
<rawString>Eduard. H. Hovy. Generating Natural Language under Pragmatic Constraints. PhD thesis, Yale University, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Consciousness and the Computational Mind.</title>
<date>1987</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="1483" citStr="Jackendoff, 1987" startWordPosition="222" endWordPosition="223">putation of all possible strings defined by a grammar of al given logical form (I) that are logically equivalent to (1) (see also (Shieber, 1988), (Calder et al., 1989)). Recently, there is a strong tendency to use the same grammar for performing both tasks. Besides more practically motivated reasons - obtaining more compact systems or avoidance of inconsistencies between the input and output of a syStem - there are also theoretical (a single model of language behaviour) and psychological evidences (empirical evidence for shared processors or facilities, cf. (Garrett, 1982), (Frazier, 1982), (Jackendoff, 1987)) to adopt this view. From a formal point of view the main interest in obtaining non—directional grammars is the specification of the relationship between strings and logical forms.&apos; According to van Noord (1990), a grammar is reversible if the parsing and generation problem is computable and the relation between strings and logical forms is symmetric. In this case parsing and generation are viewed as mutually inverse processes. Furthermore there are also approaches that assume that it is possible to use the same algorithm for processing the grammar in both directions (e.g. (Hasida and Isizaki</context>
</contexts>
<marker>Jackendoff, 1987</marker>
<rawString>Ray Jackendoff. Consciousness and the Computational Mind. MIT Press, Cambridge, Massachusetts, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="22885" citStr="Jackendoff, 1990" startWordPosition="3803" endWordPosition="3804">easiest way in order to be able to generate &apos;along parsed structures&apos; is to use the same grammar in both directions. In this case grammatical structures obtained during parsing can be used directly to restrict the potential search space during generation. 60f course, the described algorithm is too restrictive, in order to handle non—structural (e.g. contextual) paraphrases. But, I assume that this approach is also applicable in the case of lexical amibiguities prerequisite word meanings are structurally described by means of lexical semantics (e.g., Jackendoff&apos;s Lexical Conceptual Structures (Jackendoff, 1990)) 36 This approach is not only restricted in cases where the input is ambiguous and the paraphrases must contrast the different meanings. It can also be used for self-monitoring when it has to be checked wether a produced utterance S of an input form LF is ambiguous. In this case S will be parsed. If during parsing e.g., two readings LF and LF&apos; are deduced LF is generated again along the parse tree obtained for S. Now an utterance S&apos; can be generated that has the same meaning but differs with respect to the ambiguity source of S. 4 Current Work We have now started a project called BiLD (short </context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>Ray Jackendoff. Semantic Structures. MIT Press, Cambridge, Massachusetts, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem J M Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="7224" citStr="Levelt, 1989" startWordPosition="1135" endWordPosition="1136">ance there exists also the unmarked reading that &apos;Maria loves Peter&apos;. As long as the strategic component has no detailed knowledge of a specific grammar it could not express &apos;choose the passive form to avoid ambiguity&apos;. But then the process can only choose randomly between paraphrases during generation and this means that the underlying message will possibly not be conveyed. There is also psychologically grounded evidence to assume that the input to a tactical component might not be necessary and sufficient to make linguistic decisions. This is best observed in exam32 pies of self—correction (Levelt, 1989). For example, in the following utterance:3 &amp;quot;but aaa, bands like aaa- aaa- aaa- errrlike groups, not bands, - groups, you know what I mean like aaa.&amp;quot; the speaker discovers two words (the nearsynonymous &apos;group&apos; and &apos;band&apos;) each of which comes close to the underlying concept and has problems to decide which one is the most suitable. In this case, the problem is because of a mis—match between what the strategic component want to express and what the language is capable to express (Rubinoff, 1988). Current Approaches In order to be able to handle these problems, more flexible tactical components a</context>
<context position="8665" citStr="Levelt, 1989" startWordPosition="1377" endWordPosition="1378">se systems is to assume a bidirectional flow of control between the strategic and the tactical components. The problem with systems where a high degree of feedback between the strategic and the tactical components is necessary in order to perform the generation task is that one component could not perform its specific task without the help of the other. But when the mode of operation of e.g. the tactical component is continuously influenced by feedback from the strategic component then the tactical component will lose its autonomy and consequently this means that it is not a module (see also (Levelt, 1989)). 3 Integration of Parsing and Generation A promising approach for achieving more autonomous tactical components is to integrate generation and parsing in a more strict way. By this I mean: • the use of resulting structures of one direction directly in the other direction, 3This example is taken from Rubinoff (1988) and is originally from a corpus of speech collected at the University of Pennsylvania. • the use of one mode of operation (e.g., parsing) for monitoring the other mode (e.g., generation). A main thesis of this paper is that the best way to achieve such integrated approach is to us</context>
<context position="13093" citStr="Levelt (1989)" startWordPosition="2126" endWordPosition="2127">viour of a generation system will increase efficiency, too. As McDonald et al. (1987) define, one generator design is more efficient than another, if it is able to solve the same problem with fewer steps. They argue that&amp;quot;the key element governing the difficulty of utterance production is the degree of familiarity with the situation&amp;quot;. The efficiency of the generation process depends on the competence and experience one has acquired for a particular situation. But to have experience in the use of linguistic objects that are adequate in a particular situation means to be adaptable. Monitoring As Levelt (1989) pointed out &amp;quot;speakers monitor what they are saying and how they are saying it&amp;quot;, i.e. they monitor not only for meaning but also for linguistic well—formedness. To be able to monitor what one is saying is very important for processing of underspecified input and hence for achieving a more balanced divison of the generation task (see sec. 2). For example to choose between the two paraphrases of the example in sec. 2, the tactical component could parse the resulting strings in order to decide to choose the less ambiguous string &apos;Mary is loved by Peter.&apos; It only needs to know from the strategic c</context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>Willem J. M. Levelt. Speaking: From Intention to Articulation. MIT Press, Cambridge, Massachusetts, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>Natural language generation as a computational problem: An introduction.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse.</booktitle>
<editor>In M. Brady and C. Berwick, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="4873" citStr="McDonald, 1983" startWordPosition="762" endWordPosition="763">t) • &apos;how to say it&apos; part (tactical component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is tailored to determine a good sentence, making the use of powerful grammatical processes redundant. In such approaches, tactical components are only front–ends and the strategic component needs detailed information about the language to use. Hence, they are not separate modules because this implies that both components share the grammar. As pointed out in Fodor (1983) one of the characteristic properties of a module is that it is computationally autonomous. But a releva</context>
</contexts>
<marker>McDonald, 1983</marker>
<rawString>David D. McDonald. Natural language generation as a computational problem: An introduction. In M. Brady and C. Berwick, editors, Computational Models of Discourse. MIT Press, Cambridge, Massachusetts, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
<author>Marie W Meteer</author>
<author>James D Pustejovsky</author>
</authors>
<title>Factors contributing to efficiency in natural language generation. In</title>
<date>1987</date>
<booktitle>Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics,</booktitle>
<pages>159--182</pages>
<editor>K. Kempen, editor,</editor>
<location>Martinus Nijhoff, Dordrecht,</location>
<contexts>
<context position="12565" citStr="McDonald et al. (1987)" startWordPosition="2038" endWordPosition="2041"> adaptability to the use of language of the hearer is necessary in order to make possible that the hearer will be able to understand the new information. In principle this kind of adaptability means that the structures of the input computed during the understanding process carry some information that can be used to parametrize the generation process. This leads to more flexibility: not all necessary parameters need to be specified in the input of a generator because decision points can also be set during run—time. This dynamic behaviour of a generation system will increase efficiency, too. As McDonald et al. (1987) define, one generator design is more efficient than another, if it is able to solve the same problem with fewer steps. They argue that&amp;quot;the key element governing the difficulty of utterance production is the degree of familiarity with the situation&amp;quot;. The efficiency of the generation process depends on the competence and experience one has acquired for a particular situation. But to have experience in the use of linguistic objects that are adequate in a particular situation means to be adaptable. Monitoring As Levelt (1989) pointed out &amp;quot;speakers monitor what they are saying and how they are say</context>
</contexts>
<marker>McDonald, Meteer, Pustejovsky, 1987</marker>
<rawString>David D. McDonald, Marie W. Meteer, and James D. Pustejovsky. Factors contributing to efficiency in natural language generation. In K. Kempen, editor, Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, pages 159-182. Martinus Nijhoff, Dordrecht, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="4890" citStr="McKeown, 1985" startWordPosition="764" endWordPosition="765">t&apos; part (tactical component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is tailored to determine a good sentence, making the use of powerful grammatical processes redundant. In such approaches, tactical components are only front–ends and the strategic component needs detailed information about the language to use. Hence, they are not separate modules because this implies that both components share the grammar. As pointed out in Fodor (1983) one of the characteristic properties of a module is that it is computationally autonomous. But a relevant consideration </context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R. McKeown. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press, Cambridge, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Smadja. Natural language generation in comet.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>103--139</pages>
<editor>Michael Elhadad, Yumiko Fukomoto, Jong Lim, Christine Lombardi, Jacques Robin, and Frank</editor>
<publisher>Academic Press,</publisher>
<location>London,</location>
<marker>McKeown, 1990</marker>
<rawString>Kathleen R. McKeown, Michael Elhadad, Yumiko Fukomoto, Jong Lim, Christine Lombardi, Jacques Robin, and Frank Smadja. Natural language generation in comet. In Robert Dale, Chris Mellish, and Michel Zock, editors, Current Research in Natural Language Generation, pages 103 — 139. Academic Press, London, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Meteer</author>
<author>Varda Shaked</author>
</authors>
<title>Strategies for effective paraphrasing.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Budapest,</location>
<contexts>
<context position="19389" citStr="Meteer and Shaked (1988)" startWordPosition="3216" endWordPosition="3219">ting details, that are not relevant in this context). As long as the source of the ambiguity is not known it is possible to generate in both cases the utterance &apos;Remove the folder with the system— tools&apos; as a paraphrase of itself. Of course, it is possible to compare the resulting strings with the input string S. But because the source of the ambiguity is not known the loop between the isolated processes must be performed several times in general. A better strategy would be to recognize relevant sources of ambiguities during parsing and to use this information to guide the generation process. Meteer and Shaked (1988) propose an approach where during the repeated parse of an ambiguous utterance potential sources of ambiguity can be detected. For example when in the case of lexical ambiguity a noun can be associated to two semantic classes a so called &apos;lexical ambiguity specialist&apos; records the noun as the ambiguity source and the two different classes. These two classes are then explicitly used in the generator input and are realized as e.g. modifiers for the ambiguous noun. The only common knowledge source for the paraphraser is a high order intensional logic language called World Model Language. It serves</context>
</contexts>
<marker>Meteer, Shaked, 1988</marker>
<rawString>Marie Meteer and Varda Shaked. Strategies for effective paraphrasing. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), Budapest, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunter Neumann</author>
</authors>
<title>A bidirectional model for natural language processing.</title>
<date>1991</date>
<booktitle>In Fifth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>245--250</pages>
<location>Berlin,</location>
<contexts>
<context position="4418" citStr="Neumann, 1991" startWordPosition="692" endWordPosition="693">arity in Generation Systems The Problem It is widely accepted to cut down the problem of natural language generation (NLG) into two subtasks: • determination of the content of an utterance • determination of its linguistic realization This &apos;divide and conquer&apos; view of generation is the base of current architectures of systems. With few exceptions (e.g., (Appelt, 1985)) the following two components are assumed: • &apos;what to say&apos; part (strategic component) • &apos;how to say it&apos; part (tactical component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is tailored to determ</context>
</contexts>
<marker>Neumann, 1991</marker>
<rawString>Gunter Neumann. A bidirectional model for natural language processing. In Fifth Conference of the European Chapter of the Association for Computational Linguistics, pages 245-250, Berlin, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunter Neumann</author>
<author>Wolfgang Finkler</author>
</authors>
<title>A headdriven approach to incremental and parallel generation of syntactic structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>288--293</pages>
<location>Helsinki,</location>
<contexts>
<context position="25306" citStr="Neumann and Finkler, 1990" startWordPosition="4195" endWordPosition="4198"> are described simultanueous in a uniform way by means of partial information structures. None of the levels of description has a privileged status but expresses possible mutually co-ocurrence restrictions of structures of different levels. Furthermore a high degree of lexicalism is assumed so that the lexicon as a complex hierachical ordered data structure plays a central role in BiLD. As it has been shown this lexicalized view supports reversibility (cf. (Newman, 1990), (Dymetman et al., 1990)) and the performing of specific processing strategies (e.g., incremental and parallel generation, (Neumann and Finkler, 1990)). The task of the deduction process during generation is to construct the graphemic form of a specified feature description of a semantic form. For example, to yield the utterance &amp;quot;A man sings.&amp;quot; the deduction process gets as input the semantic feature structure and deduces the graphematic structure [ graph: (A_man_sings.) by means of successive application of lexical and grammatical information. In the same way the deduction process computes from the graphematic structure an appropriate semantic structure in parsing direction. A first prototype based on head-driven bottom-up strategy is now u</context>
</contexts>
<marker>Neumann, Finkler, 1990</marker>
<rawString>Gunter Neumann and Wolfgang Finkler. A headdriven approach to incremental and parallel generation of syntactic structures. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), pages 288-293, Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Newman</author>
</authors>
<title>Towards convenient bi— directional grammar formalisms.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>294--298</pages>
<location>Helsinki,</location>
<contexts>
<context position="25155" citStr="Newman, 1990" startWordPosition="4177" endWordPosition="4178"> because all levels of descriptions (i.e. phonological, syntactic and semantic structure) of linguistics entities (i.e. words and phrases) are described simultanueous in a uniform way by means of partial information structures. None of the levels of description has a privileged status but expresses possible mutually co-ocurrence restrictions of structures of different levels. Furthermore a high degree of lexicalism is assumed so that the lexicon as a complex hierachical ordered data structure plays a central role in BiLD. As it has been shown this lexicalized view supports reversibility (cf. (Newman, 1990), (Dymetman et al., 1990)) and the performing of specific processing strategies (e.g., incremental and parallel generation, (Neumann and Finkler, 1990)). The task of the deduction process during generation is to construct the graphemic form of a specified feature description of a semantic form. For example, to yield the utterance &amp;quot;A man sings.&amp;quot; the deduction process gets as input the semantic feature structure and deduces the graphematic structure [ graph: (A_man_sings.) by means of successive application of lexical and grammatical information. In the same way the deduction process computes fr</context>
</contexts>
<marker>Newman, 1990</marker>
<rawString>Paula Newman. Towards convenient bi— directional grammar formalisms. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), pages 294-298, Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Information Based Syntax and Semantics, Volume 1. Center for the Study of Language and Information Stanford,</title>
<date>1987</date>
<contexts>
<context position="3167" citStr="Pollard and Sag, 1987" startWordPosition="489" endWordPosition="492"> utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 1990), (McKeown et al., 1990), (Reithinger, 1991)).2 The purpose of this paper is to show that the use of a uniform architecture for grammatical processing has important influences for the whole generation task. A consequent use of a uniform process within a natural language generation system affects the separation into strategic and tacti11 assume a notion of grammars that integrate phonological, syntactical and semantics&apos; levels of description, e.g., (Pollard and Sag, 1987). 2But it is important to note here, that most of the proposed grammars are unification-based which is an important common property with respect to current parsing granunars. 31 cal components. On the one hand, existing problems with this separation emerge, on the other hand uniform architectures will serve as an important (linguistic) basis to achieve first solutions for the problems. In the next section I will discuss important problems and restrictions with the modular design of current generation systems and will then show why a uniform architecture as the grammatical basis can contribute </context>
<context position="24385" citStr="Pollard and Sag, 1987" startWordPosition="4056" endWordPosition="4059"> flexibility and efficiency during natural language processing. The main topic &apos;of the BiLD project is the development of a uniform parametrized deduction process for grammatical processing. This process constitutes the core of a flexible and symetric tactical module. In Order to realize the integrated approach and to obtain a high degree of efficiency in both directions we will develop methods for a declarative encoding of information of control in the lexicon and grammar. We follow a sign-based approach for the description of linguistic entities based on Headdriven Phrase Structure Grammar (Pollard and Sag, 1987) and the variant described in Reape (1989). Besides theoretical reasons there are also reasons with respeet to system&apos;s design criterions to adopt this vie*, because all levels of descriptions (i.e. phonological, syntactic and semantic structure) of linguistics entities (i.e. words and phrases) are described simultanueous in a uniform way by means of partial information structures. None of the levels of description has a privileged status but expresses possible mutually co-ocurrence restrictions of structures of different levels. Furthermore a high degree of lexicalism is assumed so that the l</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Carl Pollard and Ivan Sag. Information Based Syntax and Semantics, Volume 1. Center for the Study of Language and Information Stanford, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>A logical treatment of semi—free word order and bounded discontinuous constituency.</title>
<date>1989</date>
<booktitle>In Fourth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>103--110</pages>
<location>Manchester,</location>
<contexts>
<context position="24427" citStr="Reape (1989)" startWordPosition="4065" endWordPosition="4066">processing. The main topic &apos;of the BiLD project is the development of a uniform parametrized deduction process for grammatical processing. This process constitutes the core of a flexible and symetric tactical module. In Order to realize the integrated approach and to obtain a high degree of efficiency in both directions we will develop methods for a declarative encoding of information of control in the lexicon and grammar. We follow a sign-based approach for the description of linguistic entities based on Headdriven Phrase Structure Grammar (Pollard and Sag, 1987) and the variant described in Reape (1989). Besides theoretical reasons there are also reasons with respeet to system&apos;s design criterions to adopt this vie*, because all levels of descriptions (i.e. phonological, syntactic and semantic structure) of linguistics entities (i.e. words and phrases) are described simultanueous in a uniform way by means of partial information structures. None of the levels of description has a privileged status but expresses possible mutually co-ocurrence restrictions of structures of different levels. Furthermore a high degree of lexicalism is assumed so that the lexicon as a complex hierachical ordered da</context>
</contexts>
<marker>Reape, 1989</marker>
<rawString>Mike Reape. A logical treatment of semi—free word order and bounded discontinuous constituency. In Fourth Conference of the European Chapter of the Association for Computational Linguistics, pages 103-110, Manchester, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
</authors>
<title>Popel: A parallel and incremental natural language generation system.</title>
<date>1991</date>
<booktitle>Natural Language Generation in Artificial Intelligence and Computational Linguistics,</booktitle>
<pages>179--199</pages>
<editor>In C. L. Paris et al., editor,</editor>
<publisher>Kluwer,</publisher>
<contexts>
<context position="2736" citStr="Reithinger, 1991" startWordPosition="422" endWordPosition="423">et al., 1990), (Emele and Zajac, 1990)). A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the &apos;real&apos; generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes (cf. (Hovy, 1987), (Dale, 1990), (Horacek, 1990), (McKeown et al., 1990), (Reithinger, 1991)).2 The purpose of this paper is to show that the use of a uniform architecture for grammatical processing has important influences for the whole generation task. A consequent use of a uniform process within a natural language generation system affects the separation into strategic and tacti11 assume a notion of grammars that integrate phonological, syntactical and semantics&apos; levels of description, e.g., (Pollard and Sag, 1987). 2But it is important to note here, that most of the proposed grammars are unification-based which is an important common property with respect to current parsing granu</context>
<context position="4438" citStr="Reithinger, 1991" startWordPosition="694" endWordPosition="695">on Systems The Problem It is widely accepted to cut down the problem of natural language generation (NLG) into two subtasks: • determination of the content of an utterance • determination of its linguistic realization This &apos;divide and conquer&apos; view of generation is the base of current architectures of systems. With few exceptions (e.g., (Appelt, 1985)) the following two components are assumed: • &apos;what to say&apos; part (strategic component) • &apos;how to say it&apos; part (tactical component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is tailored to determine a good sentence,</context>
<context position="7955" citStr="Reithinger, 1991" startWordPosition="1260" endWordPosition="1261">ps, you know what I mean like aaa.&amp;quot; the speaker discovers two words (the nearsynonymous &apos;group&apos; and &apos;band&apos;) each of which comes close to the underlying concept and has problems to decide which one is the most suitable. In this case, the problem is because of a mis—match between what the strategic component want to express and what the language is capable to express (Rubinoff, 1988). Current Approaches In order to be able to handle these problems, more flexible tactical components are necessary that are able to handle e.g. underspecified input. In (Hovy, 1987), (Finkler and Neumann, 1989) and (Reithinger, 1991) approaches are described how such more flexible components can be achieved. A major point of these systems is to assume a bidirectional flow of control between the strategic and the tactical components. The problem with systems where a high degree of feedback between the strategic and the tactical components is necessary in order to perform the generation task is that one component could not perform its specific task without the help of the other. But when the mode of operation of e.g. the tactical component is continuously influenced by feedback from the strategic component then the tactical</context>
</contexts>
<marker>Reithinger, 1991</marker>
<rawString>Norbert Reithinger. Popel: A parallel and incremental natural language generation system. In C. L. Paris et al., editor, Natural Language Generation in Artificial Intelligence and Computational Linguistics, pages 179-199. Kluwer, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Rubinoff</author>
</authors>
<title>A cooperative model of strategy and tactics in generation. In Paper presented at the&apos;</title>
<date>1988</date>
<booktitle>Fourth International Workshop on Natuna Language Generation,</booktitle>
<location>Santa Catalina Island,</location>
<contexts>
<context position="4401" citStr="Rubinoff, 1988" startWordPosition="690" endWordPosition="691"> problems. 2 Modularity in Generation Systems The Problem It is widely accepted to cut down the problem of natural language generation (NLG) into two subtasks: • determination of the content of an utterance • determination of its linguistic realization This &apos;divide and conquer&apos; view of generation is the base of current architectures of systems. With few exceptions (e.g., (Appelt, 1985)) the following two components are assumed: • &apos;what to say&apos; part (strategic component) • &apos;how to say it&apos; part (tactical component) But, as it has been demonstrated by some authors ((Appelt, 1985), (Hovy, 1987), (Rubinoff, 1988), (Neumann, 1991), (Reithinger, 1991)) it is not possible to separate the two phases of the generation process completely, e.g., in the case of lexical gaps, choice between near synonyms or paraphrases. Currently, in systems where the separation is advocated the problems are sometimes &apos;solved&apos; in such a way that the strategic component has to provide all information needed by the tactical component to make decisions about lexical and syntactic choices (McDonald, 1983), (McKeown, 1985), (Busemann, 1990), (Horacek, 1990). As a consequence, this implies that the input for tactical components is t</context>
<context position="7722" citStr="Rubinoff, 1988" startWordPosition="1224" endWordPosition="1225">y and sufficient to make linguistic decisions. This is best observed in exam32 pies of self—correction (Levelt, 1989). For example, in the following utterance:3 &amp;quot;but aaa, bands like aaa- aaa- aaa- errrlike groups, not bands, - groups, you know what I mean like aaa.&amp;quot; the speaker discovers two words (the nearsynonymous &apos;group&apos; and &apos;band&apos;) each of which comes close to the underlying concept and has problems to decide which one is the most suitable. In this case, the problem is because of a mis—match between what the strategic component want to express and what the language is capable to express (Rubinoff, 1988). Current Approaches In order to be able to handle these problems, more flexible tactical components are necessary that are able to handle e.g. underspecified input. In (Hovy, 1987), (Finkler and Neumann, 1989) and (Reithinger, 1991) approaches are described how such more flexible components can be achieved. A major point of these systems is to assume a bidirectional flow of control between the strategic and the tactical components. The problem with systems where a high degree of feedback between the strategic and the tactical components is necessary in order to perform the generation task is </context>
<context position="8983" citStr="Rubinoff (1988)" startWordPosition="1431" endWordPosition="1432">fic task without the help of the other. But when the mode of operation of e.g. the tactical component is continuously influenced by feedback from the strategic component then the tactical component will lose its autonomy and consequently this means that it is not a module (see also (Levelt, 1989)). 3 Integration of Parsing and Generation A promising approach for achieving more autonomous tactical components is to integrate generation and parsing in a more strict way. By this I mean: • the use of resulting structures of one direction directly in the other direction, 3This example is taken from Rubinoff (1988) and is originally from a corpus of speech collected at the University of Pennsylvania. • the use of one mode of operation (e.g., parsing) for monitoring the other mode (e.g., generation). A main thesis of this paper is that the best way to achieve such integrated approach is to use a uniform grammatical process as the linguistic basis of a tactical component. Use of Same Structures in Both Directions If parsing and generation share the same data (i.e. grammar and lexicon) then it is possible that resulting structures of one direction could be used directly in the other direction. For example,</context>
</contexts>
<marker>Rubinoff, 1988</marker>
<rawString>Robert Rubinoff. A cooperative model of strategy and tactics in generation. In Paper presented at the&apos; Fourth International Workshop on Natuna Language Generation, Santa Catalina Island, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>A uniform architecture for parsing and generation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Budapest,</location>
<contexts>
<context position="1011" citStr="Shieber, 1988" startWordPosition="150" endWordPosition="151">nents. A central goal of this paper is to make plausible that a uniform architecture for grammatical processing will serve as a basis to achieve more flexible and efficient generation systems. 1 Introduction In general, the goal of parsing is the derivation of all possible grammatical structures defined by a grammar of a given string a (i.e. especially the determination of all possible logical forms of a) and the goal of the corresponding generation task is the computation of all possible strings defined by a grammar of al given logical form (I) that are logically equivalent to (1) (see also (Shieber, 1988), (Calder et al., 1989)). Recently, there is a strong tendency to use the same grammar for performing both tasks. Besides more practically motivated reasons - obtaining more compact systems or avoidance of inconsistencies between the input and output of a syStem - there are also theoretical (a single model of language behaviour) and psychological evidences (empirical evidence for shared processors or facilities, cf. (Garrett, 1982), (Frazier, 1982), (Jackendoff, 1987)) to adopt this view. From a formal point of view the main interest in obtaining non—directional grammars is the specification o</context>
<context position="17758" citStr="Shieber, 1988" startWordPosition="2934" endWordPosition="2935"> paraphrases at once. Although &apos;all—parses&apos; algorithms are widley used during parsing in natural language systems a corresponding &apos;all— paraphrases&apos; strategy is not practicle because in general the search space during generation is much larger (which is a consequence of the modular design discussed in sec. 2). Of course, from a formal point of view one is interested in algorithms that compute all grammatically well—formed structures — at least potentially. So most of the currently developed generators and uniform algorithms assume — more or less explictly — an all—paraphrases strategy (e.g., (Shieber, 1988), (Calder et al., 1989), (Shieber et al., 1989), (Dymetman et al., 1990), (Emele and Zajac, 1990)). But from a practical point of view they are not directly usable in such specific situations. 35 More Suitable Strategies A more suitable strategy would be to generate only one paraphrase for each ambiguous logical form. As long as parsing and generation are performed in an isolated way the problem with this strategy is that there is no control over the choice of paraphrases. In order to make clear this point I will look closer to the underlying structure of the example&apos;s utterances. The problem </context>
</contexts>
<marker>Shieber, 1988</marker>
<rawString>Stuart M. Shieber. A uniform architecture for parsing and generation. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), Budapest, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan van Noord</author>
<author>Robert C Moore</author>
<author>Fernando C N Pereira</author>
</authors>
<title>A semantic-head-driven generation algorithm for unification based formalisms.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Vancouver,</location>
<marker>Shieber, van Noord, Moore, Pereira, 1989</marker>
<rawString>Stuart M. Shieber, Gertjan van Noord, Robert C. Moore, and Fernando C.N. Pereira. A semantic-head-driven generation algorithm for unification based formalisms. In 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Uszkoreit</author>
</authors>
<title>Strategies for adding control information to declarative grammars.</title>
<date>1991</date>
<booktitle>In 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Berkeley,</location>
<contexts>
<context position="26158" citStr="Uszkoreit, 1991" startWordPosition="4332" endWordPosition="4333">semantic feature structure and deduces the graphematic structure [ graph: (A_man_sings.) by means of successive application of lexical and grammatical information. In the same way the deduction process computes from the graphematic structure an appropriate semantic structure in parsing direction. A first prototype based on head-driven bottom-up strategy is now under development (cf. (van Noord, 1991)). A major aspect of the BiLD project is that a specific parametrization of the deduction process is represented in the lexicon as well as in the grammar to obtain efficient structures of control (Uszkoreit, 1991). The main idea is that preference values are assigned to the elements (disjuncts or conjuncts) of feature descriptions. For example, in HPSG all lexical entries are put together into one large disjunctive form. From a purely declarative point of view these elements are unordered. But a preference structure is used during processing in order to guide the process of lexical choice efficiently which itself influences the grammatical process. 5 Conclusion A main thesis of this paper was to show that existing problems with the modular design of current generation systems emerge when a reversible g</context>
</contexts>
<marker>Uszkoreit, 1991</marker>
<rawString>Hans Uszkoreit. Strategies for adding control information to declarative grammars. In 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Reversible unification-based machine translation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Helsinki,</location>
<marker>van Noord, 1990</marker>
<rawString>Gertjan van Noord. Reversible unification-based machine translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Towards uniform processing for constraint-based categorial grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the ACL Workshop on Reversible Grammar in Natural Language Processing,</booktitle>
<location>Berkeley,</location>
<marker>van Noord, 1991</marker>
<rawString>Gertjan van Noord. Towards uniform processing for constraint-based categorial grammars. In Proceedings of the ACL Workshop on Reversible Grammar in Natural Language Processing, Berkeley, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
<author>Y Arens</author>
<author>D Chin</author>
</authors>
<title>Talking to unix in english: An overview of uc.</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<pages>574--593</pages>
<contexts>
<context position="14147" citStr="Wilensky et al., 1984" startWordPosition="2301" endWordPosition="2304"> component could parse the resulting strings in order to decide to choose the less ambiguous string &apos;Mary is loved by Peter.&apos; It only needs to know from the strategic component that unambiguous utterances are preferred (as a pragmactic constraint). In Levelt&apos;s model parsing and generation are performed in an isolated way by means of two different grammars. In such flow of control the complete structure has to be generated again if ambiguities are detected that have to be avoided. If, for example an intelligent help-system that supports a user by using an operation research system (e.g. Unix, (Wilensky et al., 1984)), receives as input the utterance &amp;quot;Remove the folder with the system tools&amp;quot; then the system is not able to perform the corresponding action directly because it is ambiguous. But the system could ask the user &amp;quot;Do you mean &apos;Remove the folder by means of the system tools&apos; or &apos;Remove the folder that contains the system tools&apos; &amp;quot;. This situation is summarized in the following figure (LF and LF symbolize two readings of S): LF&apos; LF&amp;quot; S: Remove the folder with the system tools S&apos;: Remove the folder by means of the system tools S&amp;quot;: Remove the folder that contains the system tools Figure 1: Relationship </context>
</contexts>
<marker>Wilensky, Arens, Chin, 1984</marker>
<rawString>R. Wilensky, Y. Arens, and D. Chin. Talking to unix in english: An overview of uc. Communications of the ACM, pages 574 — 593, 1984.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>