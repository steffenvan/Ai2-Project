<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011176">
<title confidence="0.999227">
POS induction with distributional and morphological information
using a distance-dependent Chinese restaurant process
</title>
<author confidence="0.989645">
Kairit Sirts
</author>
<affiliation confidence="0.9988925">
Institute of Cybernetics at
Tallinn University of Technology
</affiliation>
<email confidence="0.968759">
sirts@ioc.ee
</email>
<author confidence="0.9963">
Micha Elsner
</author>
<affiliation confidence="0.9963685">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.995543">
melsner0@gmail.com
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933875">
We present a new approach to inducing the
syntactic categories of words, combining
their distributional and morphological prop-
erties in a joint nonparametric Bayesian
model based on the distance-dependent
Chinese Restaurant Process. The prior
distribution over word clusterings uses a
log-linear model of morphological similar-
ity; the likelihood function is the probabil-
ity of generating vector word embeddings.
The weights of the morphology model
are learned jointly while inducing part-of-
speech clusters, encouraging them to co-
here with the distributional features. The
resulting algorithm outperforms competi-
tive alternatives on English POS induction.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989413611111111">
The morphosyntactic function of words is reflected
in two ways: their distributional properties, and
their morphological structure. Each information
source has its own advantages and disadvantages.
Distributional similarity varies smoothly with syn-
tactic function, so that words with similar syntactic
functions should have similar distributional proper-
ties. In contrast, there can be multiple paradigms
for a single morphological inflection (such as past
tense in English). But accurate computation of
distributional similarity requires large amounts of
data, which may not be available for rare words;
morphological rules can be applied to any word
regardless of how often it appears.
These observations suggest that a general ap-
proach to the induction of syntactic categories
should leverage both distributional and morpho-
logical features (Clark, 2003; Christodoulopoulos
</bodyText>
<author confidence="0.941639">
Jacob Eisenstein
</author>
<affiliation confidence="0.992676">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<email confidence="0.97479">
jacobe@gatech.edu
</email>
<author confidence="0.976172">
Sharon Goldwater
</author>
<affiliation confidence="0.998255">
ILCC, School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.984563">
sgwater@inf.ed.ac.uk
</email>
<bodyText confidence="0.99996102631579">
et al., 2010). But these features are difficult to
combine because of their disparate representations.
Distributional information is typically represented
in numerical vectors, and recent work has demon-
strated the utility of continuous vector represen-
tations, or “embeddings” (Mikolov et al., 2013;
Luong et al., 2013; Kim and de Marneffe, 2013;
Turian et al., 2010). In contrast, morphology is
often represented in terms of sparse, discrete fea-
tures (such as morphemes), or via pairwise mea-
sures such as string edit distance. Moreover, the
mapping between a surface form and morphology
is complex and nonlinear, so that simple metrics
such as edit distance will only weakly approximate
morphological similarity.
In this paper we present a new approach for in-
ducing part-of-speech (POS) classes, combining
morphological and distributional information in a
non-parametric Bayesian generative model based
on the distance-dependent Chinese restaurant pro-
cess (ddCRP; Blei and Frazier, 2011). In the dd-
CRP, each data point (word type) selects another
point to “follow”; this chain of following links
corresponds to a partition of the data points into
clusters. The probability of word w1 following w2
depends on two factors: 1) the distributional simi-
larity between all words in the proposed partition
containing w1 and w2, which is encoded using a
Gaussian likelihood function over the word embed-
dings; and 2) the morphological similarity between
w1 and w2, which acts as a prior distribution on the
induced clustering. We use a log-linear model to
capture suffix similarities between words, and learn
the feature weights by iterating between sampling
and weight learning.
We apply our model to the English section of
the the Multext-East corpus (Erjavec, 2004) in or-
der to evaluate both against the coarse-grained and
</bodyText>
<page confidence="0.971871">
265
</page>
<bodyText confidence="0.814771">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265–271,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
fine-grained tags, where the fine-grained tags en-
code detailed morphological classes. We find that
our model effectively combines morphological fea-
tures with distributional similarity, outperforming
comparable alternative approaches.
</bodyText>
<sectionHeader confidence="0.998697" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999942454545455">
Unsupervised POS tagging has a long history in
NLP. This paper focuses on the POS induction
problem (i.e., no tag dictionary is available), and
here we limit our discussion to very recent sys-
tems. A review and comparison of older systems
is provided by Christodoulopoulos et al. (2010),
who found that imposing a one-tag-per-word-type
constraint to reduce model flexibility tended to
improve system performance; like other recent
systems, we impose that constraint here. Recent
work also shows that the combination of morpho-
logical and distributional information yields the
best results, especially cross-linguistically (Clark,
2003; Berg-Kirkpatrick et al., 2010). Since then,
most systems have incorporated morphology in
some way, whether as an initial step to obtain pro-
totypes for clusters (Abend et al., 2010), or as
features in a generative model (Lee et al., 2010;
Christodoulopoulos et al., 2011; Sirts and Alum¨ae,
2012), or a representation-learning algorithm (Yat-
baz et al., 2012). Several of these systems use a
small fixed set of orthographic and/or suffix fea-
tures, sometimes obtained from an unsupervised
morphological segmentation system (Abend et al.,
2010; Lee et al., 2010; Christodoulopoulos et al.,
2011; Yatbaz et al., 2012). Blunsom and Cohn’s
(2011) model learns an n-gram character model
over the words in each cluster; we learn a log-
linear model, which can incorporate arbitrary fea-
tures. Berg-Kirkpatrick et al. (2010) also include
a log-linear model of morphology in POS induc-
tion, but they use morphology in the likelihood
term of a parametric sequence model, thereby en-
couraging all elements that share a tag to have the
same morphological features. In contrast, we use
pairwise morphological similarity as a prior in a
non-parametric clustering model. This means that
the membership of a word in a cluster requires only
morphological similarity to some other element in
the cluster, not to the cluster centroid; which may
be more appropriate for languages with multiple
morphological paradigms. Another difference is
that our non-parametric formulation makes it un-
necessary to know the number of tags in advance.
</bodyText>
<sectionHeader confidence="0.998817" genericHeader="method">
3 Distance-dependent CRP
</sectionHeader>
<bodyText confidence="0.954963153846154">
The ddCRP (Blei and Frazier, 2011) is an extension
of the CRP; like the CRP, it defines a distribution
over partitions (“table assignments”) of data points
(“customers”). Whereas in the regular CRP each
customer chooses a table with probability propor-
tional to the number of customers already sitting
there, in the ddCRP each customer chooses another
customer to follow, and sits at the same table with
that customer. By identifying the connected compo-
nents in this graph, the ddCRP equivalently defines
a prior over clusterings.
If cz is the index of the customer followed by
customer i, then the ddCRP prior can be written
</bodyText>
<equation confidence="0.985850333333333">
�
f(dzj) if i =� j (1)
α if i = j,
</equation>
<bodyText confidence="0.999908647058823">
where dzj is the distance between customers i and j
and f is a decay function. A ddCRP is sequential if
customers can only follow previous customers, i.e.,
dzj = oc when i &gt; j and f(oc) = 0. In this case,
if dzj = 1 for all i &lt; j then the ddCRP reduces to
the CRP.
Separating the distance and decay function
makes sense for “natural” distances (e.g., the num-
ber of words between word i and j in a document,
or the time between two events), but they can also
be collapsed into a single similarity function. We
wish to assign higher similarities to pairs of words
that share meaningful suffixes. Because we do not
know which suffixes are meaningful a priori, we
use a maximum entropy model whose features in-
clude all suffixes up to length three that are shared
by at least one pair of words. Our prior is then:
</bodyText>
<equation confidence="0.987709">
� ewTg(z,j) if i =� j
P(cz = j�w, α) a (2)
α if i = j,
</equation>
<bodyText confidence="0.9999425">
where gs(i, j) is 1 if suffix s is shared by ith and
jth words, and 0 otherwise.
We can create an infinite mixture model by com-
bining the ddCRP prior with a likelihood function
defining the probability of the data given the cluster
assignments. Since we are using continuous-valued
vectors (word embeddings) to represent the distri-
butional characteristics of words, we use a multi-
variate Gaussian likelihood. We will marginalize
over the mean µ and covariance E of each clus-
ter, which in turn are drawn from Gaussian and
inverse-Wishart (IW) priors respectively:
</bodyText>
<equation confidence="0.9999905">
E — IW (v0, A0) µ — N(µ0, Σ/κ0) (3)
P(cz = j) a
</equation>
<page confidence="0.840365">
266
</page>
<bodyText confidence="0.822078">
The full model is then:
</bodyText>
<equation confidence="0.997047285714286">
P(X,c, µ, E|Θ, w, α) (4)
K
=Y P(Σk|Θ)p(µk|Σk, Θ)
k=1
n
× Y (P(ci|w, α)P(xi|µzi, Σzi)),
i=1
</equation>
<bodyText confidence="0.9999732">
where Θ are the hyperparameters for (µ, E) and zi
is the (implicit) cluster assignment of the ith word
xi. With a CRP prior, this model would be an infi-
nite Gaussian mixture model (IGMM; Rasmussen,
2000), and we will use the IGMM as a baseline.
</bodyText>
<sectionHeader confidence="0.999696" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999764666666667">
The Gibbs sampler for the ddCRP integrates over
the Gaussian parameters, sampling only follower
variables. At each step, the follower link ci for a
single customer i is sampled, which can implicitly
shift the entire block of n customers fol(i) who fol-
low i into a new cluster. Since we marginalize over
the cluster parameters, computing P(ci = j) re-
quires computing the likelihood P(fol(i), Xj|Θ),
where Xj are the k customers already clustered
with j. However, if we do not merge fol(i)
with Xj, then we have P(Xj|Θ) in the overall
joint probability. Therefore, we can decompose
P(fol(i), Xj|Θ) = P(fol(i)|Xj, Θ)P(Xj|Θ) and
need only compute the change in likelihood due to
merging in fol(i):1:
</bodyText>
<equation confidence="0.97604975">
�νn+k+1−i�
Γ 2
�νk+1−i � , (5)
Γ 2
</equation>
<bodyText confidence="0.665361">
where the hyperparameters are updated as κn =
κ0 + n, νn = ν0 + n, and
</bodyText>
<equation confidence="0.97744725">
µn = κ0µ0 + x¯ (6)
κ0 + n
Λn = Λ0 + Q + κ0µ0µ0T − κnµnµTn, (7)
n T
</equation>
<bodyText confidence="0.993976666666667">
where Q = Pi=1 xixi .
Combining this likelihood term with the prior,
the probability of customer i following j is
</bodyText>
<equation confidence="0.841142">
P(ci = j|X,Θ, w, α)
∝ P(fol(i)|Xj, Θ)P(ci = j|w, α). (8)
1http://www.stats.ox.ac.uk/˜teh/re-
search/notes/GaussianInverseWishart.pdf
</equation>
<bodyText confidence="0.999907076923077">
Our non-sequential ddCRP introduces cycles
into the follower structure, which are handled in the
sampler as described by Socher et al. (2011). Also,
the block of customers being moved around can po-
tentially be very large, which makes it easy for the
likelihood term to swamp the prior. In practice we
found that introducing an additional parameter a
(used to exponentiate the prior) improved results—
although we report results without this exponent as
well. This technique was also used by Titov and
Klementiev (2012) and Elsner et al. (2012).
Inference also includes optimizing the feature
weights for the log-linear model in the ddCRP
prior (Titov and Klementiev, 2012). We interleave
L-BFGS optimization within sampling, as in Monte
Carlo Expectation-Maximization (Wei and Tanner,
1990). We do not apply the exponentiation parame-
ter a when training the weights because this proce-
dure affects the follower structure only, and we do
not have to worry about the magnitude of the like-
lihood. Before the first iteration we initialize the
follower structure: for each word, we choose ran-
domly a word to follow from amongst those with
the longest shared suffix of up to 3 characters. The
number of clusters starts around 750, but decreases
substantially after the first sampling iteration.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99886865">
Data For our experiments we used the English
word embeddings from the Polyglot project (Al-
Rfou’ et al., 2013)2, which provides embeddings
trained on Wikipedia texts for 100,000 of the most
frequent words in many languages.
We evaluate on the English part of the Multext-
East (MTE) corpus (Erjavec, 2004), which provides
both coarse-grained and fine-grained POS labels
for the text of Orwell’s “1984”. Coarse labels con-
sist of 11 main word classes, while the fine-grained
tags (104 for English) are sequences of detailed
morphological attributes. Some of these attributes
are not well-attested in English (e.g. gender) and
some are mostly distinguishable via semantic anal-
ysis (e.g. 1st and 2nd person verbs). Many tags are
assigned only to one or a few words. Scores for the
fine-grained tags will be lower for these reasons,
but we argue below that they are still informative.
Since Wikipedia and MTE are from different
domains their lexicons do not fully overlap; we
</bodyText>
<footnote confidence="0.858158">
2https://sites.google.com/site/rmyeid/
projects/polyglot
</footnote>
<equation confidence="0.982574714285714">
P(fol(i)|Xj, Θ) = π−nd/2 κ
κn+k|Λn+k|νn+k/2
d/2
d/2
k |Λk|νk/2
× Yd
i=1
</equation>
<page confidence="0.979115">
267
</page>
<table confidence="0.977696">
Wikipedia tokens 1843M
Multext-East tokens 118K
Multext-East types 9193
Multext-East &amp; Wiki types 7540
</table>
<tableCaption confidence="0.990084">
Table 1: Statistics for the English Polyglot word embeddings
</tableCaption>
<bodyText confidence="0.991215252747253">
and English part of MTE: number of Wikipedia tokens used
to train the embeddings, number of tokens/types in MTE, and
number of types shared by both datasets.
take the intersection of these two sets for training
and evaluation. Table 1 shows corpus statistics.
Evaluation With a few exceptions (Biemann,
2006; Van Gael et al., 2009), POS induction sys-
tems normally require the user to specify the num-
ber of desired clusters, and the systems are evalu-
ated with that number set to the number of tags in
the gold standard. For corpora such as MTE with
both fine-grained and coarse-grained tages, pre-
vious evaluations have scored against the coarse-
grained tags. Though coarse-grained tags have
their place (Petrov et al., 2012), in many cases
the distributional and morphological distinctions
between words are more closely aligned with the
fine-grained tagsets, which typically distinguish
between verb tenses, noun number and gender,
and adjectival scale (comparative, superlative, etc.),
so we feel that the evaluation against fine-grained
tagset is more relevant here. For better comparison
with previous work, we also evaluate against the
coarse-grained tags; however, these numbers are
not strictly comparable to other scores reported on
MTE because we are only able to train and evalu-
ate on the subset of words that also have Polyglot
embeddings. To provide some measure of the dif-
ficulty of the task, we report baseline scores using
K-means clustering, which is relatively strong base-
line in this task (Christodoulopoulos et al., 2011).
There are several measures commonly used for
unsupervised POS induction. We report greedy
one-to-one mapping accuracy (1-1) (Haghighi and
Klein, 2006) and the information-theoretic score V-
measure (V-m), which also varies from 0 to 100%
(Rosenberg and Hirschberg, 2007). In previous
work it has been common to also report many-to-
one (m-1) mapping but this measure is particularly
sensitive to the number of induced clusters (more
clusters yield higher scores), which is variable for
our models. V-m can be somewhat sensitive to the
number of clusters (Reichart and Rappoport, 2009)
but much less so than m-1 (Christodoulopoulos
et al., 2010). With different number of induced
and gold standard clusters the 1-1 measure suffers
because some induced clusters cannot be mapped
to gold clusters or vice versa. However, almost half
the gold standard clusters in MTE contain just a
few words and we do not expect our model to be
able to learn them anyway, so the 1-1 measure is
still useful for telling us how well the model learns
the bigger and more distinguishable classes.
In unsupervised POS induction it is standard to
report accuracy on tokens even when the model it-
self works on types. Here we report also type-based
measures because these can reveal differences in
model behavior even when token-based measures
are similar.
Experimental setup For baselines we use K-
means and the IGMM, which both only learn from
the word embeddings. The CRP prior in the IGMM
has one hyperparameter (the concentration param-
eter α); we report results for α = 5 and 20. Both
the IGMM and ddCRP have four hyperparameters
controlling the prior over the Gaussian cluster pa-
rameters: A0, µ0, v0 and n0. We set the prior scale
matrix A0 by using the average covariance from
a K-means run with K = 200. When setting the
average covariance as the expected value of the IW
distribution the suitable scale matrix can be com-
puted as A0 = E [X] (v0 − d − 1), where v0 is the
prior degrees of freedom (which we set to d + 10)
and d is the data dimensionality (64 for the Poly-
glot embeddings). We set the prior mean µ0 equal
to the sample mean of the data and n0 to 0.01.
We experiment with three different priors for the
ddCRP model. All our ddCRP models are non-
sequential (Socher et al., 2011), allowing cycles
to be formed. The simplest model, ddCRP uni-
form, uses a uniform prior that sets the distance
between any two words equal to one.3 The second
model, ddCRP learned, uses the log-linear prior
with weights learned between each two Gibbs iter-
ations as explained in section 4. The final model,
ddCRP exp, adds the prior exponentiation. The α
parameter for the ddCRP is set to 1 in all experi-
ments. For ddCRP exp, we report results with the
exponent a set to 5.
Results and discussion Table 2 presents all re-
sults. Each number is an average of 5 experiments
</bodyText>
<footnote confidence="0.9995625">
3In the sequential case this model would be equivalent to
the IGMM (Blei and Frazier, 2011). Due to the nonsequen-
tiality this equivalence does not hold, but we do expect to see
similar results to the IGMM.
</footnote>
<page confidence="0.982525">
268
</page>
<table confidence="0.999455875">
Model K Fine types Fine tokens Coarse tokens
Model K-means Model K-means Model K-means
K-means 104 or 11 16.1 / 47.3 - 39.2 / 62.0 - 44.4 / 45.5 -
IGMM, α = 5 55.6 41.0 / 45.9 23.1 / 49.5 48.0 / 64.8 37.2 / 61.0 48.3 / 58.3 40.8 / 55.0
IGMM, α = 20 121.2 35.0 / 47.1 14.7 / 46.9 50.6 / 67.8 44.7 / 65.5 48.7 / 60.0 48.3 / 57.9
ddCRP uniform 80.4 50.5 / 52.9 18.6 / 48.2 52.4 / 68.7 35.1 / 60.3 52.1 / 62.2 40.3 / 54.2
ddCRP learned 89.6 50.1 / 55.1 17.6 / 48.0 51.1 / 69.7 39.0 / 63.2 48.9 / 62.0 41.1 / 55.1
ddCRP exp, a = 5 47.2 64.0 / 60.3 25.0 / 50.3 55.1 / 66.4 33.0 / 59.1 47.8 / 55.1 36.9 / 53.1
</table>
<tableCaption confidence="0.99109675">
Table 2: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokens
using coarse-grained tags. For each model we present the number of induced clusters K (or fixed K for K-means) and 1-1 / V-m
scores. The second column under each evaluation setting gives the scores for K-means with K equal to the number of clusters
induced by the model in that row.
</tableCaption>
<bodyText confidence="0.999986366666667">
with different random initializations. For each eval-
uation setting we provide two sets of scores—first
are the 1-1 and V-m scores for the given model,
second are the comparable scores for K-means run
with the same number of clusters as induced by the
non-parametric model.
These results show that all non-parametric mod-
els perform better than K-means, which is a strong
baseline in this task (Christodoulopoulos et al.,
2011). The poor performace of K-means can be
explained by the fact that it tends to find clusters
of relatively equal size, although the POS clus-
ters are rarely of similar size. The common noun
singular class is by far the largest in English, con-
taining roughly a quarter of the word types. Non-
parametric models are able to produce cluster of
different sizes when the evidence indicates so, and
this is clearly the case here.
From the token-based evaluation it is hard to
say which IGMM hyperparameter value is better
even though the number of clusters induced differs
by a factor of 2. The type-base evaluation, how-
ever, clearly prefers the smaller value with fewer
clusters. Similar effects can be seen when com-
paring IGMM and ddCRP uniform. We expected
these two models perform on the same level, and
their token-based scores are similar, but on the type-
based evaluation the ddCRP is clearly superior. The
difference could be due to the non-sequentiality,
or becuase the samplers are different—IGMM en-
abling resampling only one item at a time, ddCRP
performing blocked sampling.
Further we can see that the ddCRP uniform and
learned perform roughly the same. Although the
prior in those models is different they work mainly
using the the likelihood. The ddCRP with learned
prior does produce nice follower structures within
each cluster but the prior is in general too weak
compared to the likelihood to influence the cluster-
ing decisions. Exponentiating the prior reduces the
number of induced clusters and improves results,
as it can change the cluster assignment for some
words where the likelihood strongly prefers one
cluster but the prior clearly indicates another.
The last column shows the token-based evalua-
tion against the coarse-grained tagset. This is the
most common evaluation framework used previ-
ously in the literature. Although our scores are not
directly comparable with the previous results, our
V-m scores are similar to the best published 60.5
(Christodoulopoulos et al., 2010) and 66.7 (Sirts
and Alum¨ae, 2012).
In preliminary experiments, we found that di-
rectly applying the best-performing English model
to other languages is not effective. Different lan-
guages may require different parametrizations of
the model. Further study is also needed to verify
that word embeddings effectively capture syntax
across languages, and to determine the amount of
unlabeled text necessary to learn good embeddings.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998513294117647">
This paper demonstrates that morphology and dis-
tributional features can be combined in a flexi-
ble, joint probabilistic model, using the distance-
dependent Chinese Restaurant Process. A key ad-
vantage of this framework is the ability to include
arbitrary features in the prior distribution. Future
work may exploit this advantage more thoroughly:
for example, by using features that incorporate
prior knowledge of the language’s morphological
structure. Another important goal is the evaluation
of this method on languages beyond English.
Acknowledgments: KS was supported by the
Tiger University program of the Estonian Infor-
mation Technology Foundation for Education. JE
was supported by a visiting fellowship from the
Scottish Informatics &amp; Computer Science Alliance.
We thank the reviewers for their helpful feedback.
</bodyText>
<page confidence="0.997268">
269
</page>
<sectionHeader confidence="0.98954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999698476635514">
Omri Abend, Roi Reichart, and Ari Rappoport. 2010.
Improved unsupervised pos induction through pro-
totype discovery. In Proceedings of the 48th An-
nual Meeting of the Association of Computational
Linguistics, pages 1298–1307.
Rami Al-Rfou’, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proceedings of the Thir-
teenth Annual Conference on Natural Language
Learning, pages 183–192, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre B. Cˆot´e, John
DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582–590.
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7–12.
David M Blei and Peter I Frazier. 2011. Distance
dependent chinese restaurant processes. Journal of
Machine Learning Research, 12:2461–2488.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association of Computational
Linguistics, pages 865–874.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for part-of-speech induction using multiple features.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the European chapter of the
ACL.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association of Computational
Linguistics.
Tomaˇz Erjavec. 2004. MULTEXT-East version 3:
Multilingual morphosyntactic specifications, lexi-
cons and corpora. In LREC.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013. Deriving adjectival scales from continuous
space word representations. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2010. Simple type-level unsupervised pos tag-
ging. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
853–861.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representations
with recursive neural networks for morphology. In
Proceedings of the Thirteenth Annual Conference on
Natural Language Learning.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of Human
Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 746–751.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Carl Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Process-
ing Systems 12, Cambridge, MA. MIT Press.
Roi Reichart and Ari Rappoport. 2009. The nvi cluster-
ing evaluation measure. In Proceedings of the Ninth
Annual Conference on Natural Language Learning,
pages 165–173.
A. Rosenberg and J. Hirschberg. 2007. V-measure:
A conditional entropy-based external cluster evalua-
tion measure. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 410–42.
Kairit Sirts and Tanel Alum¨ae. 2012. A hierarchi-
cal Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 407–416.
Richard Socher, Andrew L Maas, and Christopher D
Manning. 2011. Spectral chinese restaurant pro-
cesses: Nonparametric clustering based on similar-
ities. In Proceedings of the Fifteenth International
Conference on Artificial Intelligence and Statistics,
pages 698–706.
</reference>
<page confidence="0.947709">
270
</page>
<reference confidence="0.998880962962963">
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384–394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 678–687, Singapore.
Greg CG Wei and Martin A Tanner. 1990. A
monte carlo implementation of the em algorithm
and the poor man’s data augmentation algorithms.
Journal of the American Statistical Association,
85(411):699–704.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 940–951.
</reference>
<page confidence="0.997789">
271
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.449566">
<title confidence="0.9749765">POS induction with distributional and morphological using a distance-dependent Chinese restaurant process</title>
<author confidence="0.867361">Kairit</author>
<affiliation confidence="0.9964705">Institute of Cybernetics Tallinn University of</affiliation>
<email confidence="0.569106">sirts@ioc.ee</email>
<author confidence="0.999877">Micha Elsner</author>
<affiliation confidence="0.9999075">Department of Linguistics The Ohio State University</affiliation>
<email confidence="0.991981">melsner0@gmail.com</email>
<abstract confidence="0.996413">We present a new approach to inducing the syntactic categories of words, combining their distributional and morphological properties in a joint nonparametric Bayesian model based on the distance-dependent Chinese Restaurant Process. The prior distribution over word clusterings uses a log-linear model of morphological similarity; the likelihood function is the probability of generating vector word embeddings. The weights of the morphology model are learned jointly while inducing part-ofspeech clusters, encouraging them to cohere with the distributional features. The resulting algorithm outperforms competitive alternatives on English POS induction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Improved unsupervised pos induction through prototype discovery.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>1298--1307</pages>
<contexts>
<context position="5126" citStr="Abend et al., 2010" startWordPosition="733" endWordPosition="736">. A review and comparison of older systems is provided by Christodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features.</context>
</contexts>
<marker>Abend, Reichart, Rappoport, 2010</marker>
<rawString>Omri Abend, Roi Reichart, and Ari Rappoport. 2010. Improved unsupervised pos induction through prototype discovery. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics, pages 1298–1307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rami Al-Rfou’</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed word representations for multilingual nlp.</title>
<date>2013</date>
<booktitle>In Proceedings of the Thirteenth Annual Conference on Natural Language Learning,</booktitle>
<pages>183--192</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<marker>Al-Rfou’, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou’, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed word representations for multilingual nlp. In Proceedings of the Thirteenth Annual Conference on Natural Language Learning, pages 183–192, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre B Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>582--590</pages>
<marker>Berg-Kirkpatrick, Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre B. Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="12979" citStr="Biemann, 2006" startWordPosition="2070" endWordPosition="2071">rlap; we 2https://sites.google.com/site/rmyeid/ projects/polyglot P(fol(i)|Xj, Θ) = π−nd/2 κ κn+k|Λn+k|νn+k/2 d/2 d/2 k |Λk|νk/2 × Yd i=1 267 Wikipedia tokens 1843M Multext-East tokens 118K Multext-East types 9193 Multext-East &amp; Wiki types 7540 Table 1: Statistics for the English Polyglot word embeddings and English part of MTE: number of Wikipedia tokens used to train the embeddings, number of tokens/types in MTE, and number of types shared by both datasets. take the intersection of these two sets for training and evaluation. Table 1 shows corpus statistics. Evaluation With a few exceptions (Biemann, 2006; Van Gael et al., 2009), POS induction systems normally require the user to specify the number of desired clusters, and the systems are evaluated with that number set to the number of tags in the gold standard. For corpora such as MTE with both fine-grained and coarse-grained tages, previous evaluations have scored against the coarsegrained tags. Though coarse-grained tags have their place (Petrov et al., 2012), in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets, which typically distinguish between verb tenses, </context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Unsupervised part-of-speech tagging employing efficient graph clustering. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Peter I Frazier</author>
</authors>
<title>Distance dependent chinese restaurant processes.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2461</pages>
<contexts>
<context position="3019" citStr="Blei and Frazier, 2011" startWordPosition="415" endWordPosition="418">0). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Blei and Frazier, 2011). In the ddCRP, each data point (word type) selects another point to “follow”; this chain of following links corresponds to a partition of the data points into clusters. The probability of word w1 following w2 depends on two factors: 1) the distributional similarity between all words in the proposed partition containing w1 and w2, which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w1 and w2, which acts as a prior distribution on the induced clustering. We use a log-linear model to capture suffix similarities between words</context>
<context position="6515" citStr="Blei and Frazier, 2011" startWordPosition="952" endWordPosition="955">e model, thereby encouraging all elements that share a tag to have the same morphological features. In contrast, we use pairwise morphological similarity as a prior in a non-parametric clustering model. This means that the membership of a word in a cluster requires only morphological similarity to some other element in the cluster, not to the cluster centroid; which may be more appropriate for languages with multiple morphological paradigms. Another difference is that our non-parametric formulation makes it unnecessary to know the number of tags in advance. 3 Distance-dependent CRP The ddCRP (Blei and Frazier, 2011) is an extension of the CRP; like the CRP, it defines a distribution over partitions (“table assignments”) of data points (“customers”). Whereas in the regular CRP each customer chooses a table with probability proportional to the number of customers already sitting there, in the ddCRP each customer chooses another customer to follow, and sits at the same table with that customer. By identifying the connected components in this graph, the ddCRP equivalently defines a prior over clusterings. If cz is the index of the customer followed by customer i, then the ddCRP prior can be written � f(dzj) </context>
<context position="17137" citStr="Blei and Frazier, 2011" startWordPosition="2778" endWordPosition="2781">implest model, ddCRP uniform, uses a uniform prior that sets the distance between any two words equal to one.3 The second model, ddCRP learned, uses the log-linear prior with weights learned between each two Gibbs iterations as explained in section 4. The final model, ddCRP exp, adds the prior exponentiation. The α parameter for the ddCRP is set to 1 in all experiments. For ddCRP exp, we report results with the exponent a set to 5. Results and discussion Table 2 presents all results. Each number is an average of 5 experiments 3In the sequential case this model would be equivalent to the IGMM (Blei and Frazier, 2011). Due to the nonsequentiality this equivalence does not hold, but we do expect to see similar results to the IGMM. 268 Model K Fine types Fine tokens Coarse tokens Model K-means Model K-means Model K-means K-means 104 or 11 16.1 / 47.3 - 39.2 / 62.0 - 44.4 / 45.5 - IGMM, α = 5 55.6 41.0 / 45.9 23.1 / 49.5 48.0 / 64.8 37.2 / 61.0 48.3 / 58.3 40.8 / 55.0 IGMM, α = 20 121.2 35.0 / 47.1 14.7 / 46.9 50.6 / 67.8 44.7 / 65.5 48.7 / 60.0 48.3 / 57.9 ddCRP uniform 80.4 50.5 / 52.9 18.6 / 48.2 52.4 / 68.7 35.1 / 60.3 52.1 / 62.2 40.3 / 54.2 ddCRP learned 89.6 50.1 / 55.1 17.6 / 48.0 51.1 / 69.7 39.0 / 6</context>
</contexts>
<marker>Blei, Frazier, 2011</marker>
<rawString>David M Blei and Peter I Frazier. 2011. Distance dependent chinese restaurant processes. Journal of Machine Learning Research, 12:2461–2488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A hierarchical pitman-yor process hmm for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>865--874</pages>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A hierarchical pitman-yor process hmm for unsupervised part of speech induction. In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics, pages 865–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4597" citStr="Christodoulopoulos et al. (2010)" startWordPosition="658" endWordPosition="661">–271, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics fine-grained tags, where the fine-grained tags encode detailed morphological classes. We find that our model effectively combines morphological features with distributional similarity, outperforming comparable alternative approaches. 2 Related work Unsupervised POS tagging has a long history in NLP. This paper focuses on the POS induction problem (i.e., no tag dictionary is available), and here we limit our discussion to very recent systems. A review and comparison of older systems is provided by Christodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulop</context>
<context position="14852" citStr="Christodoulopoulos et al., 2010" startWordPosition="2363" endWordPosition="2366">e several measures commonly used for unsupervised POS induction. We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score Vmeasure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). In previous work it has been common to also report many-toone (m-1) mapping but this measure is particularly sensitive to the number of induced clusters (more clusters yield higher scores), which is variable for our models. V-m can be somewhat sensitive to the number of clusters (Reichart and Rappoport, 2009) but much less so than m-1 (Christodoulopoulos et al., 2010). With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa. However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes. In unsupervised POS induction it is standard to report accuracy on tokens even when the model itself works on types. Here we report also type-based measures because these</context>
<context position="20683" citStr="Christodoulopoulos et al., 2010" startWordPosition="3409" endWordPosition="3412">oo weak compared to the likelihood to influence the clustering decisions. Exponentiating the prior reduces the number of induced clusters and improves results, as it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another. The last column shows the token-based evaluation against the coarse-grained tagset. This is the most common evaluation framework used previously in the literature. Although our scores are not directly comparable with the previous results, our V-m scores are similar to the best published 60.5 (Christodoulopoulos et al., 2010) and 66.7 (Sirts and Alum¨ae, 2012). In preliminary experiments, we found that directly applying the best-performing English model to other languages is not effective. Different languages may require different parametrizations of the model. Further study is also needed to verify that word embeddings effectively capture syntax across languages, and to determine the amount of unlabeled text necessary to learn good embeddings. 6 Conclusion This paper demonstrates that morphology and distributional features can be combined in a flexible, joint probabilistic model, using the distancedependent Chine</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>A Bayesian mixture model for part-of-speech induction using multiple features.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5215" citStr="Christodoulopoulos et al., 2011" startWordPosition="748" endWordPosition="751">et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS indu</context>
<context position="14210" citStr="Christodoulopoulos et al., 2011" startWordPosition="2263" endWordPosition="2266"> noun number and gender, and adjectival scale (comparative, superlative, etc.), so we feel that the evaluation against fine-grained tagset is more relevant here. For better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings. To provide some measure of the difficulty of the task, we report baseline scores using K-means clustering, which is relatively strong baseline in this task (Christodoulopoulos et al., 2011). There are several measures commonly used for unsupervised POS induction. We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score Vmeasure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). In previous work it has been common to also report many-toone (m-1) mapping but this measure is particularly sensitive to the number of induced clusters (more clusters yield higher scores), which is variable for our models. V-m can be somewhat sensitive to the number of clusters (Reichart and Rappoport, 2009) but much less so </context>
<context position="18687" citStr="Christodoulopoulos et al., 2011" startWordPosition="3082" endWordPosition="3085">lusters K (or fixed K for K-means) and 1-1 / V-m scores. The second column under each evaluation setting gives the scores for K-means with K equal to the number of clusters induced by the model in that row. with different random initializations. For each evaluation setting we provide two sets of scores—first are the 1-1 and V-m scores for the given model, second are the comparable scores for K-means run with the same number of clusters as induced by the non-parametric model. These results show that all non-parametric models perform better than K-means, which is a strong baseline in this task (Christodoulopoulos et al., 2011). The poor performace of K-means can be explained by the fact that it tends to find clusters of relatively equal size, although the POS clusters are rarely of similar size. The common noun singular class is by far the largest in English, containing roughly a quarter of the word types. Nonparametric models are able to produce cluster of different sizes when the evidence indicates so, and this is clearly the case here. From the token-based evaluation it is hard to say which IGMM hyperparameter value is better even though the number of clusters induced differs by a factor of 2. The type-base eval</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2011</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2011. A Bayesian mixture model for part-of-speech induction using multiple features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the European chapter of the ACL.</booktitle>
<contexts>
<context position="1822" citStr="Clark, 2003" startWordPosition="249" endWordPosition="250">h syntactic function, so that words with similar syntactic functions should have similar distributional properties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos Jacob Eisenstein School of Interactive Computing Georgia Institute of Technology jacobe@gatech.edu Sharon Goldwater ILCC, School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morpholog</context>
<context position="4944" citStr="Clark, 2003" startWordPosition="706" endWordPosition="707">ing has a long history in NLP. This paper focuses on the POS induction problem (i.e., no tag dictionary is available), and here we limit our discussion to very recent systems. A review and comparison of older systems is provided by Christodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the European chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Sharon Goldwater</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Bootstrapping a unified model of lexical and phonetic acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="10649" citStr="Elsner et al. (2012)" startWordPosition="1701" endWordPosition="1704">8) 1http://www.stats.ox.ac.uk/˜teh/research/notes/GaussianInverseWishart.pdf Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from amongst those with the longest</context>
</contexts>
<marker>Elsner, Goldwater, Eisenstein, 2012</marker>
<rawString>Micha Elsner, Sharon Goldwater, and Jacob Eisenstein. 2012. Bootstrapping a unified model of lexical and phonetic acquisition. In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaˇz Erjavec</author>
</authors>
<title>MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="3790" citStr="Erjavec, 2004" startWordPosition="543" endWordPosition="544">to clusters. The probability of word w1 following w2 depends on two factors: 1) the distributional similarity between all words in the proposed partition containing w1 and w2, which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w1 and w2, which acts as a prior distribution on the induced clustering. We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning. We apply our model to the English section of the the Multext-East corpus (Erjavec, 2004) in order to evaluate both against the coarse-grained and 265 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265–271, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics fine-grained tags, where the fine-grained tags encode detailed morphological classes. We find that our model effectively combines morphological features with distributional similarity, outperforming comparable alternative approaches. 2 Related work Unsupervised POS tagging has a long history in NLP. This paper focuses on the </context>
<context position="11709" citStr="Erjavec, 2004" startWordPosition="1873" endWordPosition="1874">d. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from amongst those with the longest shared suffix of up to 3 characters. The number of clusters starts around 750, but decreases substantially after the first sampling iteration. 5 Experiments Data For our experiments we used the English word embeddings from the Polyglot project (AlRfou’ et al., 2013)2, which provides embeddings trained on Wikipedia texts for 100,000 of the most frequent words in many languages. We evaluate on the English part of the MultextEast (MTE) corpus (Erjavec, 2004), which provides both coarse-grained and fine-grained POS labels for the text of Orwell’s “1984”. Coarse labels consist of 11 main word classes, while the fine-grained tags (104 for English) are sequences of detailed morphological attributes. Some of these attributes are not well-attested in English (e.g. gender) and some are mostly distinguishable via semantic analysis (e.g. 1st and 2nd person verbs). Many tags are assigned only to one or a few words. Scores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative. Since Wikipedia and MTE a</context>
</contexts>
<marker>Erjavec, 2004</marker>
<rawString>Tomaˇz Erjavec. 2004. MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14362" citStr="Haghighi and Klein, 2006" startWordPosition="2284" endWordPosition="2287">e. For better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings. To provide some measure of the difficulty of the task, we report baseline scores using K-means clustering, which is relatively strong baseline in this task (Christodoulopoulos et al., 2011). There are several measures commonly used for unsupervised POS induction. We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score Vmeasure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). In previous work it has been common to also report many-toone (m-1) mapping but this measure is particularly sensitive to the number of induced clusters (more clusters yield higher scores), which is variable for our models. V-m can be somewhat sensitive to the number of clusters (Reichart and Rappoport, 2009) but much less so than m-1 (Christodoulopoulos et al., 2010). With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clu</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joo-Kyung Kim</author>
<author>Marie-Catherine de Marneffe</author>
</authors>
<title>Deriving adjectival scales from continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Kim, de Marneffe, 2013</marker>
<rawString>Joo-Kyung Kim and Marie-Catherine de Marneffe. 2013. Deriving adjectival scales from continuous space word representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised pos tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>853--861</pages>
<contexts>
<context position="5182" citStr="Lee et al., 2010" startWordPosition="744" endWordPosition="747">hristodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linea</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised pos tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 853–861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Thirteenth Annual Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="2349" citStr="Luong et al., 2013" startWordPosition="315" endWordPosition="318">ctic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos Jacob Eisenstein School of Interactive Computing Georgia Institute of Technology jacobe@gatech.edu Sharon Goldwater ILCC, School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distanc</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Thirteenth Annual Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="2329" citStr="Mikolov et al., 2013" startWordPosition="311" endWordPosition="314">the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos Jacob Eisenstein School of Interactive Computing Georgia Institute of Technology jacobe@gatech.edu Sharon Goldwater ILCC, School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<contexts>
<context position="13394" citStr="Petrov et al., 2012" startWordPosition="2139" endWordPosition="2142">es in MTE, and number of types shared by both datasets. take the intersection of these two sets for training and evaluation. Table 1 shows corpus statistics. Evaluation With a few exceptions (Biemann, 2006; Van Gael et al., 2009), POS induction systems normally require the user to specify the number of desired clusters, and the systems are evaluated with that number set to the number of tags in the gold standard. For corpora such as MTE with both fine-grained and coarse-grained tages, previous evaluations have scored against the coarsegrained tags. Though coarse-grained tags have their place (Petrov et al., 2012), in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets, which typically distinguish between verb tenses, noun number and gender, and adjectival scale (comparative, superlative, etc.), so we feel that the evaluation against fine-grained tagset is more relevant here. For better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Rasmussen</author>
</authors>
<title>The infinite Gaussian mixture model.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12,</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8938" citStr="Rasmussen, 2000" startWordPosition="1405" endWordPosition="1406">ent the distributional characteristics of words, we use a multivariate Gaussian likelihood. We will marginalize over the mean µ and covariance E of each cluster, which in turn are drawn from Gaussian and inverse-Wishart (IW) priors respectively: E — IW (v0, A0) µ — N(µ0, Σ/κ0) (3) P(cz = j) a 266 The full model is then: P(X,c, µ, E|Θ, w, α) (4) K =Y P(Σk|Θ)p(µk|Σk, Θ) k=1 n × Y (P(ci|w, α)P(xi|µzi, Σzi)), i=1 where Θ are the hyperparameters for (µ, E) and zi is the (implicit) cluster assignment of the ith word xi. With a CRP prior, this model would be an infinite Gaussian mixture model (IGMM; Rasmussen, 2000), and we will use the IGMM as a baseline. 4 Inference The Gibbs sampler for the ddCRP integrates over the Gaussian parameters, sampling only follower variables. At each step, the follower link ci for a single customer i is sampled, which can implicitly shift the entire block of n customers fol(i) who follow i into a new cluster. Since we marginalize over the cluster parameters, computing P(ci = j) requires computing the likelihood P(fol(i), Xj|Θ), where Xj are the k customers already clustered with j. However, if we do not merge fol(i) with Xj, then we have P(Xj|Θ) in the overall joint probabi</context>
</contexts>
<marker>Rasmussen, 2000</marker>
<rawString>Carl Rasmussen. 2000. The infinite Gaussian mixture model. In Advances in Neural Information Processing Systems 12, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>The nvi clustering evaluation measure.</title>
<date>2009</date>
<booktitle>In Proceedings of the Ninth Annual Conference on Natural Language Learning,</booktitle>
<pages>165--173</pages>
<contexts>
<context position="14792" citStr="Reichart and Rappoport, 2009" startWordPosition="2353" endWordPosition="2356"> in this task (Christodoulopoulos et al., 2011). There are several measures commonly used for unsupervised POS induction. We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score Vmeasure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). In previous work it has been common to also report many-toone (m-1) mapping but this measure is particularly sensitive to the number of induced clusters (more clusters yield higher scores), which is variable for our models. V-m can be somewhat sensitive to the number of clusters (Reichart and Rappoport, 2009) but much less so than m-1 (Christodoulopoulos et al., 2010). With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa. However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes. In unsupervised POS induction it is standard to report accuracy on tokens even when the model itself works on </context>
</contexts>
<marker>Reichart, Rappoport, 2009</marker>
<rawString>Roi Reichart and Ari Rappoport. 2009. The nvi clustering evaluation measure. In Proceedings of the Ninth Annual Conference on Natural Language Learning, pages 165–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosenberg</author>
<author>J Hirschberg</author>
</authors>
<title>V-measure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>410--42</pages>
<contexts>
<context position="14480" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="2302" endWordPosition="2305">mbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings. To provide some measure of the difficulty of the task, we report baseline scores using K-means clustering, which is relatively strong baseline in this task (Christodoulopoulos et al., 2011). There are several measures commonly used for unsupervised POS induction. We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score Vmeasure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). In previous work it has been common to also report many-toone (m-1) mapping but this measure is particularly sensitive to the number of induced clusters (more clusters yield higher scores), which is variable for our models. V-m can be somewhat sensitive to the number of clusters (Reichart and Rappoport, 2009) but much less so than m-1 (Christodoulopoulos et al., 2010). With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa. However, almost half the gold standard clusters in MTE contain </context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>A. Rosenberg and J. Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 410–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Tanel Alum¨ae</author>
</authors>
<title>A hierarchical Dirichlet process model for joint part-of-speech and morphology induction.</title>
<date>2012</date>
<booktitle>In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>407--416</pages>
<marker>Sirts, Alum¨ae, 2012</marker>
<rawString>Kairit Sirts and Tanel Alum¨ae. 2012. A hierarchical Dirichlet process model for joint part-of-speech and morphology induction. In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 407–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrew L Maas</author>
<author>Christopher D Manning</author>
</authors>
<title>Spectral chinese restaurant processes: Nonparametric clustering based on similarities.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>698--706</pages>
<contexts>
<context position="10247" citStr="Socher et al. (2011)" startWordPosition="1634" endWordPosition="1637">compute the change in likelihood due to merging in fol(i):1: �νn+k+1−i� Γ 2 �νk+1−i � , (5) Γ 2 where the hyperparameters are updated as κn = κ0 + n, νn = ν0 + n, and µn = κ0µ0 + x¯ (6) κ0 + n Λn = Λ0 + Q + κ0µ0µ0T − κnµnµTn, (7) n T where Q = Pi=1 xixi . Combining this likelihood term with the prior, the probability of customer i following j is P(ci = j|X,Θ, w, α) ∝ P(fol(i)|Xj, Θ)P(ci = j|w, α). (8) 1http://www.stats.ox.ac.uk/˜teh/research/notes/GaussianInverseWishart.pdf Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carl</context>
<context position="16477" citStr="Socher et al., 2011" startWordPosition="2660" endWordPosition="2663">uster parameters: A0, µ0, v0 and n0. We set the prior scale matrix A0 by using the average covariance from a K-means run with K = 200. When setting the average covariance as the expected value of the IW distribution the suitable scale matrix can be computed as A0 = E [X] (v0 − d − 1), where v0 is the prior degrees of freedom (which we set to d + 10) and d is the data dimensionality (64 for the Polyglot embeddings). We set the prior mean µ0 equal to the sample mean of the data and n0 to 0.01. We experiment with three different priors for the ddCRP model. All our ddCRP models are nonsequential (Socher et al., 2011), allowing cycles to be formed. The simplest model, ddCRP uniform, uses a uniform prior that sets the distance between any two words equal to one.3 The second model, ddCRP learned, uses the log-linear prior with weights learned between each two Gibbs iterations as explained in section 4. The final model, ddCRP exp, adds the prior exponentiation. The α parameter for the ddCRP is set to 1 in all experiments. For ddCRP exp, we report results with the exponent a set to 5. Results and discussion Table 2 presents all results. Each number is an average of 5 experiments 3In the sequential case this mo</context>
</contexts>
<marker>Socher, Maas, Manning, 2011</marker>
<rawString>Richard Socher, Andrew L Maas, and Christopher D Manning. 2011. Spectral chinese restaurant processes: Nonparametric clustering based on similarities. In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, pages 698–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10624" citStr="Titov and Klementiev (2012)" startWordPosition="1696" endWordPosition="1699">P(fol(i)|Xj, Θ)P(ci = j|w, α). (8) 1http://www.stats.ox.ac.uk/˜teh/research/notes/GaussianInverseWishart.pdf Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from among</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A bayesian approach to unsupervised semantic role induction. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2398" citStr="Turian et al., 2010" startWordPosition="324" endWordPosition="327">onal and morphological features (Clark, 2003; Christodoulopoulos Jacob Eisenstein School of Interactive Computing Georgia Institute of Technology jacobe@gatech.edu Sharon Goldwater ILCC, School of Informatics University of Edinburgh sgwater@inf.ed.ac.uk et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Bl</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>678--687</pages>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 678–687, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg CG Wei</author>
<author>Martin A Tanner</author>
</authors>
<title>A monte carlo implementation of the em algorithm and the poor man’s data augmentation algorithms.</title>
<date>1990</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>85</volume>
<issue>411</issue>
<contexts>
<context position="10896" citStr="Wei and Tanner, 1990" startWordPosition="1735" endWordPosition="1738">rs being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from amongst those with the longest shared suffix of up to 3 characters. The number of clusters starts around 750, but decreases substantially after the first sampling iteration. 5 Experiments Data For our experiments we used the English word embeddings from the Polyglot project (A</context>
</contexts>
<marker>Wei, Tanner, 1990</marker>
<rawString>Greg CG Wei and Martin A Tanner. 1990. A monte carlo implementation of the em algorithm and the poor man’s data augmentation algorithms. Journal of the American Statistical Association, 85(411):699–704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehmet Ali Yatbaz</author>
<author>Enis Sert</author>
<author>Deniz Yuret</author>
</authors>
<title>Learning syntactic categories using paradigmatic representations of word context.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>940--951</pages>
<contexts>
<context position="5303" citStr="Yatbaz et al., 2012" startWordPosition="760" endWordPosition="764">ty tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS induction, but they use morphology in the likelihood term of a parametric sequence model, th</context>
</contexts>
<marker>Yatbaz, Sert, Yuret, 2012</marker>
<rawString>Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012. Learning syntactic categories using paradigmatic representations of word context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 940–951.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>