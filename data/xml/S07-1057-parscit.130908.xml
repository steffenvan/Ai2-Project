<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001750">
<title confidence="0.9970435">
PNNL: A Supervised Maximum Entropy Approach to Word Sense
Disambiguation
</title>
<author confidence="0.88829">
Stephen Tratz, Antonio Sanfilippo, Michelle Gregory, Alan Chappell, Christian
Posse, Paul Whitney
</author>
<affiliation confidence="0.692401">
Pacific Northwest National Laboratory
</affiliation>
<address confidence="0.821542333333333">
902 Battelle Blvd, PO Box 999
Richland, WA 99352, USA
{stephen.tratz, antonio.sanfilippo, michelle, alan.chap-
</address>
<email confidence="0.941702">
pell, christian.posse, paul.whitney}@pnl.gov
</email>
<sectionHeader confidence="0.993794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99878075">
In this paper, we described the PNNL
Word Sense Disambiguation system as ap-
plied to the English all-word task in Se-
mEval 2007. We use a supervised learning
approach, employing a large number of
features and using Information Gain for di-
mension reduction. The rich feature set
combined with a Maximum Entropy classi-
fier produces results that are significantly
better than baseline and are the highest F-
score for the fined-grained English all-
words subtask of SemEval.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999897307692308">
Accurate word sense disambiguation (WSD) can
support many natural language processing and
knowledge management tasks. The main goal of
the PNNL WSD system is to support Semantic
Web applications, such as semantic-driven search
and navigation, through a reliable mapping of
words in naturally occurring text to ontological
classes. As described in Sanfilippo et al. (2006),
this goal is achieved by defining a WordNet-based
(Fellbaum, 1998) ontology that offers a manage-
able set of concept classes, provides an extensive
characterization of concept class in terms of lexical
instances, and integrates an automated class recog-
nition algorithm. We found that the same features
that are useful for predicting word classes are also
useful in distinguishing individual word senses.
Our main objective in this paper is to predict in-
dividual word senses using a large combination of
features including contextual, semantic, and syn-
tactic information. In our earlier paper (Sanfilippo
et al., 2006), we reported that the PNNL WSD sys-
tem exceeded the performance of the best perform-
ers for verbs in the SENSEVAL-3 English all-
words task dataset. SemEval 2007 is our first op-
portunity to enter a word sense disambiguation
competition.
</bodyText>
<sectionHeader confidence="0.989762" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.99984895">
While many unsupervised word sense disambigua-
tion systems have been created, supervised systems
have generally produced superior results (Snyder
and Palmer, 2004; Mihalcea et al., 2004). Our sys-
tem is based on a supervised WSD approach that
uses a Maximum Entropy classifier to predict
WordNet senses.
We use SemCor1, OMWE 1.0 (Chklovski and
Mihalcea, 2002), and example sentences in Word-
Net as the training corpus. We utilize the
OpenNLP MaxEnt implementation2 of the maxi-
mum entropy classification algorithm (Berger et
al., 1996) to train classification models for each
lemma and part-of-speech combination in the train-
ing corpus. These models are used to predict
WordNet senses for words found in natural text.
For lemma and part-of-speech combinations that
are not present in the training corpus, the PNNL
WSD system defaults to the most frequent Word-
Net sense.
</bodyText>
<subsectionHeader confidence="0.749166">
2.1 Features
</subsectionHeader>
<bodyText confidence="0.999852666666667">
We use a rich set of features to predict individual
word senses. A large number of features are ex-
tracted for each word sense instance in the training
data. Following Dang &amp; Palmer (2005) and Ko-
homban &amp; Lee (2005), we use contextual, syntac-
tic and semantic information to inform our word
</bodyText>
<footnote confidence="0.9982295">
1 http://www.cs.unt.edu/~rada/downloads.html.
2 http://maxent.sourceforge.net/.
</footnote>
<page confidence="0.975222">
264
</page>
<bodyText confidence="0.965165777777778">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 264–267,
Prague, June 2007. c�2007 Association for Computational Linguistics
sense disambiguation system. However, there are
significant differences between the specific types
of contextual, syntactic and semantic information
we use in our system and those proposed by Dang
&amp; Palmer (2005) and Kohomban &amp; Lee (2005).
More specifically, we employ novel features and
feature combinations, as described below.
</bodyText>
<listItem confidence="0.882114166666667">
• Contextual information. The contextual infor-
mation we use includes the word under analy-
sis plus the three tokens found on each side of
the word, within sentence boundaries. Tokens
include both words and punctuation.
• Syntactic information. We include grammatical
dependencies (e.g. subject, object) and mor-
pho-syntactic features such as part of speech,
case, number and tense. We use the Connexor
parser3 (Tapanainen and Järvinen, 1997) to ex-
tract lemma information, parts of speech, syn-
tactic dependencies, tense, case, and number
information. A sample output of a Connexor
parse is given in Table 1. Features are extract-
ed for all tokens that are related through no
more than 3 levels of dependency to the word
to be disambiguated.
• Semantic information. The semantic informa-
</listItem>
<bodyText confidence="0.868409952380952">
tion we incorporate includes named entity
types (e.g. PERSON, LOCATION, ORGANI-
ZATION) and hypernyms. We use OpenNLP4
and LingPipe5 to identify named entities, re-
placing the strings identified as named entities
(e.g., Joe Smith) with the corresponding entity
type (PERSON). We also substitute personal
pronouns that unambiguously denote people
with the entity type PERSON. Numbers in the
text are replaced with type label NUMBER.
Hypernyms are retrieved from WordNet and
added to the feature set for all noun tokens se-
lected by the contextual and syntactic rules. In
contrast to Dang &amp; Palmer (2005), we only in-
clude the hypernyms of the most frequent
sense, and we include the entire hypernym
chain (e.g. motor, machine, device, instrumen-
tality, artifact, object, whole, entity).
To address feature extraction processes specific
to noun and verbs, we add the following condi-
tions.
</bodyText>
<footnote confidence="0.993795">
3 http://www.connexor.com/.
4 http://opennlp.sourceforge.nt/.
5 http://www.alias-i.com/lingpipe/.
</footnote>
<listItem confidence="0.9841032">
• Syntactic information for verbs. If the verb
does not have a subject, the subject of the clos-
est ancestor verb in the syntax tree is used in-
stead.
• Syntactic information for nouns. The first verb
ancestor in the syntax tree is also used to gen-
erate features.
• Semantic information for nouns. A feature in-
dicating whether a token is capitalized for each
of the tokens used to generate features.
</listItem>
<bodyText confidence="0.933511">
A sample of the resulting feature vectors that are
used by the PNNL word sense disambiguation sys-
tem is presented in Table 2.
</bodyText>
<table confidence="0.9819828">
ID Word Lemma Grammatical Morphosyntactic
Dependen- Features
cies
1 the the det:&gt;2 @DN&gt; %&gt;N DET
2 engine engine subj:&gt;3 @SUBJ %NH N NOM SG
3 throbbe throb main:&gt;0 @+FMAINV %VA V PAST
4 d into goa:&gt;3 @ADVL %EH PREP
5 into life pcomp:&gt;4 @&lt;P %NH N NOM SG
6 life .
.
</table>
<tableCaption confidence="0.721386217391304">
Table 1. Connexor sample output for the sentence
“The engine throbbed into life”.
the pre:2:the, pre:2:pos:DET, det:the, det:pos:DET,
hassubj:det:
engine pre:1:instrumentality, pre:1:object, pre:1:artifact,
pre:1:device, pre:1:engine, pre:1:motor, pre:1:whole,
pre:1:entity, pre:1:machine, pre:1:pos:N,
pre:1:case:NOM,
pre:1:num:SG,subj:instrumentality,subj:object, subj:arti-
fact, subj:device, subj:engine, subj:motor, subj:whole,
subj:entity, subj:machine, subj:pos:N, hassubj:,
subj:case:NOM, subj:num:SG,
throbbed haspre:1:,haspre:2:,haspost:1:, haspost:2:, haspost:3:,
self:throb, self:pos:V, main:,throbbed, self:tense:PAST
into post:1:into, post:1:pos:PREP, goa:into, goa:pos:PREP,
life post:2:life, post:2:state, post:2:being, post:2:pos:N,
post:2:case:NOM, post:2:num:SG, hasgoa:, pcomp:life,
pcomp:state, pcomp:being, pcomp:pos:N,
hasgoa:pcomp:, goa:pcomp:case:NOM,
goa:pcomp:num:SG
. post:3:.
Table 2. Feature vector for throbbed in the sen-
tence “The engine throbbed into life”.
</tableCaption>
<bodyText confidence="0.999908142857143">
As the example in Table 2 indicates, the combi-
nation of contextual, syntactic, and semantic infor-
mation types results in a large number of features.
Inspection of the training data reveals that some
features may be more important than others in es-
tablishing word sense assignment for each choice
of word lemma. We use a feature selection proce-
</bodyText>
<page confidence="0.995593">
265
</page>
<bodyText confidence="0.999992125">
dure to reduce the full set of features to the feature
subset that is most relevant to word sense assign-
ment for each lemma. This practice improves the
efficiency of our word sense disambiguation algo-
rithm. The feature selection procedure we adopted
consists of scoring each potential feature according
to a particular feature selection metric, and then
taking the best k features.
We choose Information Gain as our feature se-
lection metric. Information Gain measures the de-
crease in entropy when the feature is given versus
when it is absent. Yang and Pederson (1997) report
that Information Gain outperformed other feature
selection approaches in their multi-class bench-
marks, and Foreman (2003) showed that it per-
formed amongst the best for his 2-class problems.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999789666666667">
To evaluate our approach and feature set, we ran
our model on the SENSEVAL-3 English all-words
task test data. Using data provided by the SENSE-
VAL website6, we were able to compare our results
for verbs to the top performers on verbs alone.
Upali S. Kohomban and Wee Sun Lee provided us
with the results file for the Simil-Prime system
(Kohomban and Lee, 2005). As reported in Sanfil-
ippo et al. (2006) and shown in table 3, our results
for verbs rival those of top performers. We had a
significant improvement (p-value&lt;0.05) over the
baseline of 52.9%, a marginal improvement over
the second best performer (SenseLearner) (Mihal-
cea and Faruque, 2004), and we were as good as
the top performer (GAMBL) (Decadt et al., 2004).7
</bodyText>
<table confidence="0.9995965">
System Precision Fraction of
Recall
Our system 61% 22%
GAMBL 59.0% 21.3%
SenseLearner 56.1% 20.2%
Baseline 52.9% 19.1%
</table>
<tableCaption confidence="0.990049">
Table 3. Results for verb sense disambiguation on
SENSEVAL-3 data, adapted from Sanfilippo et al.
</tableCaption>
<bodyText confidence="0.956217333333333">
(2006).
Since then, we have expanded our evaluation to
all parts of speech. Table 4 provides the evaluation
</bodyText>
<footnote confidence="0.85239925">
6 http://www.senseval.org/.
7 The 2% improvement in precision which our system
showed as compared to GAMBL was not statistically
significant (p=0.21).
</footnote>
<bodyText confidence="0.999371454545455">
of our system as compared to the three top per-
formers on the SENSEVAL-3 data and the baseline.
The baseline of 0.631 F-score8 was computed us-
ing the most frequent WordNet sense. The PNNL
WSD system performs significantly better than the
baseline (p-value&lt;0.05) and rivals the top perform-
ers. The performance of the PNNL WSD system
relative to the other three systems and the baseline
remains unchanged when the unknown sense an-
swers (denoted by a ‘U’) are excluded from the
evaluation.
</bodyText>
<table confidence="0.999192">
System Precision Recall
PNNL 0.670 0.670
Simil-Prime 0.661 0.663
GAMBL 0.652 0.652
SenseLearner 0.646 0.646
Baseline 0.631 0.631
</table>
<tableCaption confidence="0.995839">
Table 4. SENSEVAL-3 English all-words.
</tableCaption>
<table confidence="0.999784333333333">
System Recall Precision
PNNL 0.669 0.671
GAMBL 0.651 0.651
Simil-Prime 0.644 0.657
SenseLearner 0.642 0.651
Baseline 0.631 0.631
</table>
<tableCaption confidence="0.99864">
Table 5. SENSEVAL-3 English all-words, No “U”.
</tableCaption>
<sectionHeader confidence="0.8632465" genericHeader="method">
4 Experimental results on SemEval all-
words subtask
</sectionHeader>
<bodyText confidence="0.999901454545455">
This was our first opportunity to test our model in
a WSD competition. For this competition, we fo-
cused our efforts on the fine-grained English all-
words task because our system was set up to per-
form fine-grained WordNet sense prediction. We
are pleased that our system achieved the highest
score for this subtask. Our results for the SemEval
dataset as compared to baseline are reported in Ta-
ble 6. The PNNL WSD system did not assign the
unknown sense, ‘U’, to any word instances in the
SemEval dataset.
</bodyText>
<footnote confidence="0.928247">
8 This baseline is slightly higher than that reported by
others (Snyder and Palmer 2004).
</footnote>
<page confidence="0.989416">
266
</page>
<table confidence="0.99704175">
System F-score
PNNL 0.591
Baseline 0.514
p-value &lt;0.01
</table>
<tableCaption confidence="0.909623">
Table 6. SemEval Results.
</tableCaption>
<sectionHeader confidence="0.995175" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999948705882353">
Although these results are promising, there is still
much work to be done. For example, we need to
investigate the contribution of each feature to the
overall performance of the system in terms of pre-
cision and recall. Such a feature sensitivity analysis
will provide us with a better understanding of how
the algorithm can be further improved and/or made
more efficient by leaving out features whose con-
tribution is negligible.
Another important point to make is that, while
our system shows the best precision/recall results
overall, we can only claim statistical relevance
with reference to the baseline and results worse
than baseline. The size of the SemEval data set
(N=465) is too small to establish whether the dif-
ference in precision/recall results with the other top
systems is statistically significant.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999827">
We would like to thank Upali S. Kohomban and
Wee Sun Lee for providing us with their SENSE-
VAL-3 English all-words task results file for Simil-
Prime. Many thanks also to Patrick Paulson, Bob
Baddeley, Ryan Hohimer, and Amanda White for
their help in developing the word class disam-
biguation system on which the work presented in
this paper is based.
</bodyText>
<sectionHeader confidence="0.996976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99119749122807">
Berger, A., S. Della Pietra and V. Della Pietra (1996) A
Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, volume 22,
number 1, pages 39-71.
Chklovski, T. and R. Mihalcea (2002) Building a sense
tagged corpus with open mind word expert. In Pro-
ceedings of the ACL-02 workshop on Word sense dis-
ambiguation: recent successes and future directions.
Dang, H. T. and M. Palmer (2005) The Role of Semant-
ic Roles in Disambiguating Verb Senses. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, Ann Arbor MI, June
26-28, 2005.
Decadt, B., V. Hoste, W. Daelemans and A. Van den
Bosch (2004) GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. SENSEVAL-3: Third In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text. Barcelona, Spain.
Fellbaum, C., editor. (1998) WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Foreman, G. (2003) An Extensive Empirical Study of
Feature Selection Metrics for Text Classification.
Journal of Machine Learning Research, 3, pages
1289-1305.
Kohomban, U. and W. Lee (2005) Learning semantic
classes for word sense disambiguation. In Proceed-
ings of the 43rd Annual meeting of the Association for
Computational Linguistics, Ann Arbor, MI.
Mihalcea, R., T. Chklovski, and A. Kilgarriff (2004)
The SENSEVAL-3 English Lexical Sample Task,
SENSEVAL-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text. Barcelonna, Span.
Mihalcea, R. and E. Faruque (2004) SenseLearner:
Minimally supervised word sense disambiguation for
all words in open text. SENSEVAL-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text. Barcelona, Spain.
Sanfilippo, A., S. Tratz, M. Gregory, A. Chappell, P.
Whitney, C. Posse, P. Paulson, B. Baddeley, R. Hohi-
mer, A. White (2006) Automating Ontological An-
notation with WordNet. Proceedings to the Third In-
ternational WordNet Conference, Jan 22-26, Jeju Is-
land, Korea.
Snyder, B. and M. Palmer. 2004. The English All-
Words Task. SENSEVAL-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text. Barcelona, Spain.
Tapanainen, P. and Timo Järvinen (1997) A nonproject-
ive dependency parser. In Proceedings of the 5th
Conference on Applied Natural Language Processing,
pages 64–71, Washington D.C. Association for Com-
putational Linguistics.
Yang, Y. and J. O. Pedersen (1997) A Comparative
Study on Feature Selection in Text Categorization. In
Proceedings of the 14th International Conference on
Machine Learning (ICML), pages 412-420, 1997.
</reference>
<page confidence="0.997033">
267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.757457">
<title confidence="0.999433">PNNL: A Supervised Maximum Entropy Approach to Word Sense Disambiguation</title>
<author confidence="0.973741">Stephen Tratz</author>
<author confidence="0.973741">Antonio Sanfilippo</author>
<author confidence="0.973741">Michelle Gregory</author>
<author confidence="0.973741">Alan Chappell</author>
<author confidence="0.973741">Christian Posse</author>
<author confidence="0.973741">Paul Whitney</author>
<affiliation confidence="0.995452">Pacific Northwest National Laboratory</affiliation>
<address confidence="0.999691">902 Battelle Blvd, PO Box 999 Richland, WA 99352, USA</address>
<email confidence="0.942461">antonio.sanfilippo,michelle,pell,christian.posse,paul.whitney}@pnl.gov</email>
<abstract confidence="0.991763538461538">In this paper, we described the PNNL Word Sense Disambiguation system as applied to the English all-word task in SemEval 2007. We use a supervised learning approach, employing a large number of features and using Information Gain for dimension reduction. The rich feature set combined with a Maximum Entropy classifier produces results that are significantly better than baseline and are the highest Fscore for the fined-grained English allwords subtask of SemEval.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<pages>39--71</pages>
<contexts>
<context position="2618" citStr="Berger et al., 1996" startWordPosition="393" endWordPosition="396">07 is our first opportunity to enter a word sense disambiguation competition. 2 Approach While many unsupervised word sense disambiguation systems have been created, supervised systems have generally produced superior results (Snyder and Palmer, 2004; Mihalcea et al., 2004). Our system is based on a supervised WSD approach that uses a Maximum Entropy classifier to predict WordNet senses. We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in WordNet as the training corpus. We utilize the OpenNLP MaxEnt implementation2 of the maximum entropy classification algorithm (Berger et al., 1996) to train classification models for each lemma and part-of-speech combination in the training corpus. These models are used to predict WordNet senses for words found in natural text. For lemma and part-of-speech combinations that are not present in the training corpus, the PNNL WSD system defaults to the most frequent WordNet sense. 2.1 Features We use a rich set of features to predict individual word senses. A large number of features are extracted for each word sense instance in the training data. Following Dang &amp; Palmer (2005) and Kohomban &amp; Lee (2005), we use contextual, syntactic and sema</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A., S. Della Pietra and V. Della Pietra (1996) A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, volume 22, number 1, pages 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>R Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with open mind word expert.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Word</booktitle>
<contexts>
<context position="2444" citStr="Chklovski and Mihalcea, 2002" startWordPosition="366" endWordPosition="369">r (Sanfilippo et al., 2006), we reported that the PNNL WSD system exceeded the performance of the best performers for verbs in the SENSEVAL-3 English allwords task dataset. SemEval 2007 is our first opportunity to enter a word sense disambiguation competition. 2 Approach While many unsupervised word sense disambiguation systems have been created, supervised systems have generally produced superior results (Snyder and Palmer, 2004; Mihalcea et al., 2004). Our system is based on a supervised WSD approach that uses a Maximum Entropy classifier to predict WordNet senses. We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in WordNet as the training corpus. We utilize the OpenNLP MaxEnt implementation2 of the maximum entropy classification algorithm (Berger et al., 1996) to train classification models for each lemma and part-of-speech combination in the training corpus. These models are used to predict WordNet senses for words found in natural text. For lemma and part-of-speech combinations that are not present in the training corpus, the PNNL WSD system defaults to the most frequent WordNet sense. 2.1 Features We use a rich set of features to predict individual word senses. A large numbe</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Chklovski, T. and R. Mihalcea (2002) Building a sense tagged corpus with open mind word expert. In Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>The Role of Semantic Roles in Disambiguating Verb Senses.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor MI,</location>
<contexts>
<context position="3153" citStr="Dang &amp; Palmer (2005)" startWordPosition="483" endWordPosition="486">implementation2 of the maximum entropy classification algorithm (Berger et al., 1996) to train classification models for each lemma and part-of-speech combination in the training corpus. These models are used to predict WordNet senses for words found in natural text. For lemma and part-of-speech combinations that are not present in the training corpus, the PNNL WSD system defaults to the most frequent WordNet sense. 2.1 Features We use a rich set of features to predict individual word senses. A large number of features are extracted for each word sense instance in the training data. Following Dang &amp; Palmer (2005) and Kohomban &amp; Lee (2005), we use contextual, syntactic and semantic information to inform our word 1 http://www.cs.unt.edu/~rada/downloads.html. 2 http://maxent.sourceforge.net/. 264 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 264–267, Prague, June 2007. c�2007 Association for Computational Linguistics sense disambiguation system. However, there are significant differences between the specific types of contextual, syntactic and semantic information we use in our system and those proposed by Dang &amp; Palmer (2005) and Kohomban &amp; Lee (2005). More s</context>
<context position="5216" citStr="Dang &amp; Palmer (2005)" startWordPosition="793" endWordPosition="796">. The semantic information we incorporate includes named entity types (e.g. PERSON, LOCATION, ORGANIZATION) and hypernyms. We use OpenNLP4 and LingPipe5 to identify named entities, replacing the strings identified as named entities (e.g., Joe Smith) with the corresponding entity type (PERSON). We also substitute personal pronouns that unambiguously denote people with the entity type PERSON. Numbers in the text are replaced with type label NUMBER. Hypernyms are retrieved from WordNet and added to the feature set for all noun tokens selected by the contextual and syntactic rules. In contrast to Dang &amp; Palmer (2005), we only include the hypernyms of the most frequent sense, and we include the entire hypernym chain (e.g. motor, machine, device, instrumentality, artifact, object, whole, entity). To address feature extraction processes specific to noun and verbs, we add the following conditions. 3 http://www.connexor.com/. 4 http://opennlp.sourceforge.nt/. 5 http://www.alias-i.com/lingpipe/. • Syntactic information for verbs. If the verb does not have a subject, the subject of the closest ancestor verb in the syntax tree is used instead. • Syntactic information for nouns. The first verb ancestor in the synt</context>
</contexts>
<marker>Dang, Palmer, 2005</marker>
<rawString>Dang, H. T. and M. Palmer (2005) The Role of Semantic Roles in Disambiguating Verb Senses. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor MI, June 26-28, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Decadt</author>
<author>V Hoste</author>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
</authors>
<title>GAMBL, genetic algorithm optimization of memory-based WSD.</title>
<date>2004</date>
<booktitle>SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<location>Barcelona,</location>
<marker>Decadt, Hoste, Daelemans, Van den Bosch, 2004</marker>
<rawString>Decadt, B., V. Hoste, W. Daelemans and A. Van den Bosch (2004) GAMBL, genetic algorithm optimization of memory-based WSD. SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, C., editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Fellbaum, C., editor. (1998) WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foreman</author>
</authors>
<title>An Extensive Empirical Study of Feature Selection Metrics for Text Classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1289--1305</pages>
<contexts>
<context position="8417" citStr="Foreman (2003)" startWordPosition="1251" endWordPosition="1252"> to word sense assignment for each lemma. This practice improves the efficiency of our word sense disambiguation algorithm. The feature selection procedure we adopted consists of scoring each potential feature according to a particular feature selection metric, and then taking the best k features. We choose Information Gain as our feature selection metric. Information Gain measures the decrease in entropy when the feature is given versus when it is absent. Yang and Pederson (1997) report that Information Gain outperformed other feature selection approaches in their multi-class benchmarks, and Foreman (2003) showed that it performed amongst the best for his 2-class problems. 3 Evaluation To evaluate our approach and feature set, we ran our model on the SENSEVAL-3 English all-words task test data. Using data provided by the SENSEVAL website6, we were able to compare our results for verbs to the top performers on verbs alone. Upali S. Kohomban and Wee Sun Lee provided us with the results file for the Simil-Prime system (Kohomban and Lee, 2005). As reported in Sanfilippo et al. (2006) and shown in table 3, our results for verbs rival those of top performers. We had a significant improvement (p-value</context>
</contexts>
<marker>Foreman, 2003</marker>
<rawString>Foreman, G. (2003) An Extensive Empirical Study of Feature Selection Metrics for Text Classification. Journal of Machine Learning Research, 3, pages 1289-1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Kohomban</author>
<author>W Lee</author>
</authors>
<title>Learning semantic classes for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="8859" citStr="Kohomban and Lee, 2005" startWordPosition="1327" endWordPosition="1330">n versus when it is absent. Yang and Pederson (1997) report that Information Gain outperformed other feature selection approaches in their multi-class benchmarks, and Foreman (2003) showed that it performed amongst the best for his 2-class problems. 3 Evaluation To evaluate our approach and feature set, we ran our model on the SENSEVAL-3 English all-words task test data. Using data provided by the SENSEVAL website6, we were able to compare our results for verbs to the top performers on verbs alone. Upali S. Kohomban and Wee Sun Lee provided us with the results file for the Simil-Prime system (Kohomban and Lee, 2005). As reported in Sanfilippo et al. (2006) and shown in table 3, our results for verbs rival those of top performers. We had a significant improvement (p-value&lt;0.05) over the baseline of 52.9%, a marginal improvement over the second best performer (SenseLearner) (Mihalcea and Faruque, 2004), and we were as good as the top performer (GAMBL) (Decadt et al., 2004).7 System Precision Fraction of Recall Our system 61% 22% GAMBL 59.0% 21.3% SenseLearner 56.1% 20.2% Baseline 52.9% 19.1% Table 3. Results for verb sense disambiguation on SENSEVAL-3 data, adapted from Sanfilippo et al. (2006). Since then</context>
<context position="3179" citStr="Kohomban &amp; Lee (2005)" startWordPosition="488" endWordPosition="492">ximum entropy classification algorithm (Berger et al., 1996) to train classification models for each lemma and part-of-speech combination in the training corpus. These models are used to predict WordNet senses for words found in natural text. For lemma and part-of-speech combinations that are not present in the training corpus, the PNNL WSD system defaults to the most frequent WordNet sense. 2.1 Features We use a rich set of features to predict individual word senses. A large number of features are extracted for each word sense instance in the training data. Following Dang &amp; Palmer (2005) and Kohomban &amp; Lee (2005), we use contextual, syntactic and semantic information to inform our word 1 http://www.cs.unt.edu/~rada/downloads.html. 2 http://maxent.sourceforge.net/. 264 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 264–267, Prague, June 2007. c�2007 Association for Computational Linguistics sense disambiguation system. However, there are significant differences between the specific types of contextual, syntactic and semantic information we use in our system and those proposed by Dang &amp; Palmer (2005) and Kohomban &amp; Lee (2005). More specifically, we employ nov</context>
</contexts>
<marker>Kohomban, Lee, 2005</marker>
<rawString>Kohomban, U. and W. Lee (2005) Learning semantic classes for word sense disambiguation. In Proceedings of the 43rd Annual meeting of the Association for Computational Linguistics, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Chklovski</author>
<author>A Kilgarriff</author>
</authors>
<date>2004</date>
<booktitle>The SENSEVAL-3 English Lexical Sample Task, SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<location>Barcelonna, Span.</location>
<contexts>
<context position="2272" citStr="Mihalcea et al., 2004" startWordPosition="337" endWordPosition="340"> this paper is to predict individual word senses using a large combination of features including contextual, semantic, and syntactic information. In our earlier paper (Sanfilippo et al., 2006), we reported that the PNNL WSD system exceeded the performance of the best performers for verbs in the SENSEVAL-3 English allwords task dataset. SemEval 2007 is our first opportunity to enter a word sense disambiguation competition. 2 Approach While many unsupervised word sense disambiguation systems have been created, supervised systems have generally produced superior results (Snyder and Palmer, 2004; Mihalcea et al., 2004). Our system is based on a supervised WSD approach that uses a Maximum Entropy classifier to predict WordNet senses. We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in WordNet as the training corpus. We utilize the OpenNLP MaxEnt implementation2 of the maximum entropy classification algorithm (Berger et al., 1996) to train classification models for each lemma and part-of-speech combination in the training corpus. These models are used to predict WordNet senses for words found in natural text. For lemma and part-of-speech combinations that are not present in the t</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Mihalcea, R., T. Chklovski, and A. Kilgarriff (2004) The SENSEVAL-3 English Lexical Sample Task, SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Barcelonna, Span.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>E Faruque</author>
</authors>
<title>SenseLearner: Minimally supervised word sense disambiguation for all words in open text.</title>
<date>2004</date>
<booktitle>SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="9149" citStr="Mihalcea and Faruque, 2004" startWordPosition="1373" endWordPosition="1377">h and feature set, we ran our model on the SENSEVAL-3 English all-words task test data. Using data provided by the SENSEVAL website6, we were able to compare our results for verbs to the top performers on verbs alone. Upali S. Kohomban and Wee Sun Lee provided us with the results file for the Simil-Prime system (Kohomban and Lee, 2005). As reported in Sanfilippo et al. (2006) and shown in table 3, our results for verbs rival those of top performers. We had a significant improvement (p-value&lt;0.05) over the baseline of 52.9%, a marginal improvement over the second best performer (SenseLearner) (Mihalcea and Faruque, 2004), and we were as good as the top performer (GAMBL) (Decadt et al., 2004).7 System Precision Fraction of Recall Our system 61% 22% GAMBL 59.0% 21.3% SenseLearner 56.1% 20.2% Baseline 52.9% 19.1% Table 3. Results for verb sense disambiguation on SENSEVAL-3 data, adapted from Sanfilippo et al. (2006). Since then, we have expanded our evaluation to all parts of speech. Table 4 provides the evaluation 6 http://www.senseval.org/. 7 The 2% improvement in precision which our system showed as compared to GAMBL was not statistically significant (p=0.21). of our system as compared to the three top perfor</context>
</contexts>
<marker>Mihalcea, Faruque, 2004</marker>
<rawString>Mihalcea, R. and E. Faruque (2004) SenseLearner: Minimally supervised word sense disambiguation for all words in open text. SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sanfilippo</author>
<author>S Tratz</author>
<author>M Gregory</author>
<author>A Chappell</author>
<author>P Whitney</author>
<author>C Posse</author>
<author>P Paulson</author>
<author>B Baddeley</author>
<author>R Hohimer</author>
<author>A White</author>
</authors>
<title>Automating Ontological Annotation with WordNet.</title>
<date>2006</date>
<booktitle>Proceedings to the Third International WordNet Conference,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="1224" citStr="Sanfilippo et al. (2006)" startWordPosition="174" endWordPosition="177">n for dimension reduction. The rich feature set combined with a Maximum Entropy classifier produces results that are significantly better than baseline and are the highest Fscore for the fined-grained English allwords subtask of SemEval. 1 Introduction Accurate word sense disambiguation (WSD) can support many natural language processing and knowledge management tasks. The main goal of the PNNL WSD system is to support Semantic Web applications, such as semantic-driven search and navigation, through a reliable mapping of words in naturally occurring text to ontological classes. As described in Sanfilippo et al. (2006), this goal is achieved by defining a WordNet-based (Fellbaum, 1998) ontology that offers a manageable set of concept classes, provides an extensive characterization of concept class in terms of lexical instances, and integrates an automated class recognition algorithm. We found that the same features that are useful for predicting word classes are also useful in distinguishing individual word senses. Our main objective in this paper is to predict individual word senses using a large combination of features including contextual, semantic, and syntactic information. In our earlier paper (Sanfil</context>
<context position="8900" citStr="Sanfilippo et al. (2006)" startWordPosition="1334" endWordPosition="1338">erson (1997) report that Information Gain outperformed other feature selection approaches in their multi-class benchmarks, and Foreman (2003) showed that it performed amongst the best for his 2-class problems. 3 Evaluation To evaluate our approach and feature set, we ran our model on the SENSEVAL-3 English all-words task test data. Using data provided by the SENSEVAL website6, we were able to compare our results for verbs to the top performers on verbs alone. Upali S. Kohomban and Wee Sun Lee provided us with the results file for the Simil-Prime system (Kohomban and Lee, 2005). As reported in Sanfilippo et al. (2006) and shown in table 3, our results for verbs rival those of top performers. We had a significant improvement (p-value&lt;0.05) over the baseline of 52.9%, a marginal improvement over the second best performer (SenseLearner) (Mihalcea and Faruque, 2004), and we were as good as the top performer (GAMBL) (Decadt et al., 2004).7 System Precision Fraction of Recall Our system 61% 22% GAMBL 59.0% 21.3% SenseLearner 56.1% 20.2% Baseline 52.9% 19.1% Table 3. Results for verb sense disambiguation on SENSEVAL-3 data, adapted from Sanfilippo et al. (2006). Since then, we have expanded our evaluation to all </context>
</contexts>
<marker>Sanfilippo, Tratz, Gregory, Chappell, Whitney, Posse, Paulson, Baddeley, Hohimer, White, 2006</marker>
<rawString>Sanfilippo, A., S. Tratz, M. Gregory, A. Chappell, P. Whitney, C. Posse, P. Paulson, B. Baddeley, R. Hohimer, A. White (2006) Automating Ontological Annotation with WordNet. Proceedings to the Third International WordNet Conference, Jan 22-26, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>M Palmer</author>
</authors>
<title>The English AllWords Task.</title>
<date>2004</date>
<booktitle>SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2248" citStr="Snyder and Palmer, 2004" startWordPosition="333" endWordPosition="336">es. Our main objective in this paper is to predict individual word senses using a large combination of features including contextual, semantic, and syntactic information. In our earlier paper (Sanfilippo et al., 2006), we reported that the PNNL WSD system exceeded the performance of the best performers for verbs in the SENSEVAL-3 English allwords task dataset. SemEval 2007 is our first opportunity to enter a word sense disambiguation competition. 2 Approach While many unsupervised word sense disambiguation systems have been created, supervised systems have generally produced superior results (Snyder and Palmer, 2004; Mihalcea et al., 2004). Our system is based on a supervised WSD approach that uses a Maximum Entropy classifier to predict WordNet senses. We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in WordNet as the training corpus. We utilize the OpenNLP MaxEnt implementation2 of the maximum entropy classification algorithm (Berger et al., 1996) to train classification models for each lemma and part-of-speech combination in the training corpus. These models are used to predict WordNet senses for words found in natural text. For lemma and part-of-speech combinations that </context>
<context position="11173" citStr="Snyder and Palmer 2004" startWordPosition="1702" endWordPosition="1705">ts on SemEval allwords subtask This was our first opportunity to test our model in a WSD competition. For this competition, we focused our efforts on the fine-grained English allwords task because our system was set up to perform fine-grained WordNet sense prediction. We are pleased that our system achieved the highest score for this subtask. Our results for the SemEval dataset as compared to baseline are reported in Table 6. The PNNL WSD system did not assign the unknown sense, ‘U’, to any word instances in the SemEval dataset. 8 This baseline is slightly higher than that reported by others (Snyder and Palmer 2004). 266 System F-score PNNL 0.591 Baseline 0.514 p-value &lt;0.01 Table 6. SemEval Results. 5 Discussion Although these results are promising, there is still much work to be done. For example, we need to investigate the contribution of each feature to the overall performance of the system in terms of precision and recall. Such a feature sensitivity analysis will provide us with a better understanding of how the algorithm can be further improved and/or made more efficient by leaving out features whose contribution is negligible. Another important point to make is that, while our system shows the bes</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Snyder, B. and M. Palmer. 2004. The English AllWords Task. SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>Timo Järvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<institution>D.C. Association for Computational Linguistics.</institution>
<location>Washington</location>
<contexts>
<context position="4276" citStr="Tapanainen and Järvinen, 1997" startWordPosition="641" endWordPosition="644">formation we use in our system and those proposed by Dang &amp; Palmer (2005) and Kohomban &amp; Lee (2005). More specifically, we employ novel features and feature combinations, as described below. • Contextual information. The contextual information we use includes the word under analysis plus the three tokens found on each side of the word, within sentence boundaries. Tokens include both words and punctuation. • Syntactic information. We include grammatical dependencies (e.g. subject, object) and morpho-syntactic features such as part of speech, case, number and tense. We use the Connexor parser3 (Tapanainen and Järvinen, 1997) to extract lemma information, parts of speech, syntactic dependencies, tense, case, and number information. A sample output of a Connexor parse is given in Table 1. Features are extracted for all tokens that are related through no more than 3 levels of dependency to the word to be disambiguated. • Semantic information. The semantic information we incorporate includes named entity types (e.g. PERSON, LOCATION, ORGANIZATION) and hypernyms. We use OpenNLP4 and LingPipe5 to identify named entities, replacing the strings identified as named entities (e.g., Joe Smith) with the corresponding entity </context>
</contexts>
<marker>Tapanainen, Järvinen, 1997</marker>
<rawString>Tapanainen, P. and Timo Järvinen (1997) A nonprojective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–71, Washington D.C. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A Comparative Study on Feature Selection in Text Categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning (ICML),</booktitle>
<pages>412--420</pages>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yang, Y. and J. O. Pedersen (1997) A Comparative Study on Feature Selection in Text Categorization. In Proceedings of the 14th International Conference on Machine Learning (ICML), pages 412-420, 1997.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>