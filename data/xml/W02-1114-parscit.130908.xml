<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.083684">
<title confidence="0.9995755">
A Japanese Semantic Network built on a Pulsed Neural Network with
encoding Associative Concept Dictionaries
</title>
<author confidence="0.951266">
Takuya Sakaguchi
</author>
<affiliation confidence="0.9358785">
Media and Governance
Keio University
</affiliation>
<email confidence="0.997004">
sakataku@sfc.keio.ac.jp
</email>
<author confidence="0.924496">
Shun Ishizaki
</author>
<affiliation confidence="0.946277">
Environmental Information
Keio University
</affiliation>
<email confidence="0.997353">
ishizaki@sfc.keio.ac.jp
</email>
<sectionHeader confidence="0.996639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970416666667">
A semantic network with the dynamics
such as spreading activity is available for
various systems as well as the structure
model of knowledge. In this study we
suggest a practical dynamic semantic
network available for NLP, which has the
structure from associative concept
dictionaries and the dynamics from a
pulsed neural network. We built the
semantic network by means of
constructing the platform called &amp;quot;Brain
Memory Model&amp;quot; based on a pulsed neural
network first, then encoding data of
associative concept dictionaries into it.
We also constructed the module as one of
the applications of the semantic network
built on BMM to NLP, especially to
simile understanding. The outputs of the
module were understandable in general,
indicating the possibility of our semantic
network. We are now considering to
expand the scale of a semantic network
and to introduce the learning algorithm
for its self-organization.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988512">
A semantic network (Collins and Quillian, 1969)
is a kind of network model, describing a structure
of concepts preserved in a human brain. Basically
it is formed with a number of concepts as nodes
and relations between each concept as arks of a
network. Several models of a semantic network
have been suggested, some of them are dynamic
models mentioning to the dynamics of a network
such as spreading activity (Collins and Loftus,
1975). These models are available not only for
structure models about concepts but also for some
useful systems of associating, reasoning or natural
language processing (NLP).
When considering to design such a dynamic and
practical model of a semantic network, we have to
mention to at least two problems, i.e. how to
construct a large-scale structure and how to
provide a dynamics of a network. In this study, we
solved these problems by means of applying the
database called &amp;quot;Associative Concept Dictionary&amp;quot;
for its structure and the architecture constructed
with a pulsed neural network for its dynamics,
building a semantic network model and considered
its availability for NLP, especially for simile
understanding.
</bodyText>
<sectionHeader confidence="0.996506" genericHeader="introduction">
2 Purpose
</sectionHeader>
<bodyText confidence="0.999810583333333">
As mentioned above, the purpose of this study is
to build a semantic network with data of
associative concept dictionaries and architecture of
a pulsed neural network.
We would separate this study in three steps
shown below. First we constructed the architecture
based on a pulsed neural network, as the platform
in which a semantic network is built. A pulsed
neural network (Domany et al., 1994) is a kind of
neural network model often designed with
integrated and fire units, which is a more strict
model to a real neuron in terms of considering a
time course of a membrane potential of each
neuron as well as its firing. Since the membrane
potential is not reset but retained for some time
even after its increase in a pulsed neural network, it
is possible to realize spatio-temporal integration of
firings. We constructed the platform for a
semantic network with this neural network model,
which is named &amp;quot;Brain Memory Model&amp;quot; or simply
&amp;quot;BMM&amp;quot;.
As the second step of this study we built a
semantic network on BMM, by means of encoding
the data of associative concept dictionaries into it.
An associative concept dictionary (Okamoto and
Ishizaki, 1998) is a large-scale database about
concepts and their relations, gathered through
cognitive experiments where a human subject
answers some concepts associated from a stimulus
concept along a certain type of relation. Each
associated concept is registered to the database
with its type of relation, values of his/her reaction
time, reaction order and reaction rate and the
distance to the stimulus concept is calculated from
these values. Here we show a part of data of an
associative concept dictionary (See Figure 1).
</bodyText>
<table confidence="0.9962710625">
Stimulus Word Relation Associative Word Ans. Time Ans. Order Ans. Ratio Distance
Snow TSA Crystal 0.30 1.00 0.10 6.49
Snow TSA Weather 0.35 1.00 0.20 3.41
Snow TSA Water 0.29 1.00 0.20 3.41
Snow TSA Nature 0.13 1.00 0.20 3.41
Snow TSA Object 0.34 1.33 0.30 2.49
Snow HAS Avalanche 0.87 3.00 0.10 7.15
Snow HAS Hail 0.43 2.00 0.10 6.82
Snow HAS Snowstorm 0.68 2.50 0.20 3.91
Snow HAS Hailstone 0.48 2.50 0.20 3.91
Snow HAS Sleet 0.36 1.33 0.30 2.49
Snow HAS Powdery Snow 0.47 1.75 0.40 2.12
Snow CAN Melt 0.62 3.00 0.20 4.07
Snow CAN Fall 0.35 1.50 1.00 1.11
. . . . . . â€¢
.
</table>
<figureCaption confidence="0.999292">
Figure 1. Data of Associative Concept Dictionary.
</figureCaption>
<bodyText confidence="0.9999855">
As a result of encoding these data into BMM, it
is possible to build a semantic network with a rich
structure about concepts and a dynamics of spatio-
temporal integration.
Finally, we mentioned to the availability of this
semantic network for NLP. In this study we
constructed the system for simile understanding
which transfers a typical simile expression &amp;quot;The T
seems just like S&amp;quot; into a simile-understood
expression &amp;quot;The T is so r, with the help of
behaviors of the semantic network built on BMM.
Here we defined &amp;quot;T&amp;quot; and &amp;quot;S&amp;quot; in the simile
expression as a simple noun concept named &amp;quot;target
concept&amp;quot; and &amp;quot;source concept&amp;quot;, while &amp;quot;U&apos; in the
simile-understood expression as &amp;quot;goal concept&amp;quot;
which is output by the system.
</bodyText>
<sectionHeader confidence="0.998744" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.999928">
3.1 Constructing Brain Memory Model
</subsectionHeader>
<bodyText confidence="0.998225571428571">
The first step of this study is to construct Brain
Memory Model as the platform to build a semantic
network. We designed BMM as shown below,
which is composed with several modules (See
Figure 2). In this section we would explain the
architecture of BMM with mentioning to the roles
of these modules.
</bodyText>
<figure confidence="0.795732">
Neuron
Current fbr Job
</figure>
<figureCaption confidence="0.999877">
Figure 2. Module Composition of BMM.
</figureCaption>
<bodyText confidence="0.999930555555556">
Neural_Network module is one of the most
important modules, which is the base of BMM
describing its structure and dynamics. Note that
Neuron module composing Neural_Network
module is designed as integrated and fire unit, thus
Neural_Network module is regarded as a pulsed
neural network model. Neural_Network module is
designed as a multiple layered structure, and each
layer is possible to treat in a separate manner. It is
able to set the parameter such as a firing rule of
neurons, value of threshold and number of firing
neurons for each layer. It is also possible to knock
out any layer instantly, thus we could control the
area of network activity in BMM.
Table module plays the role to preserve the
information of concepts encoded in BMM. When
starting BMM program, Table module is built with
empty because there are no data encoded at this
moment. Thus we have to consider encoding data
of associative concept dictionaries into BMM,
where ACD_Loader module plays the important
role. This procedure would be explained in the
next section again.
Simile_Understanding module is the application
of BMM rather than BMM itself, which transfers a
simile expression into a simile-understood
expression with the help of the semantic network
</bodyText>
<figure confidence="0.996170545454546">
ACD Loader
Simile
nderstandin
Associative
Concept
Dictionary
Neural_
Network
Table
Current fbr Data
GUI
</figure>
<bodyText confidence="0.992809777777778">
built on BMM. The detail of this module would be
explained in section 3.3.
GUI module provides the interface of BMM. It
accepts various kinds of inputs about activities or
parameters, displaying the current of status or
condition in BMM on the other hand (See Figure
3).
Finally, the center BMM module controls and
organizes all other modules as a main module.
</bodyText>
<subsectionHeader confidence="0.99999">
3.2 Building a Semantic Network
</subsectionHeader>
<bodyText confidence="0.9991702">
In this section we would mention to the second
step of the study, i.e. the method to build a
semantic network with encoding associative
concept dictionaries into BMM architecture.
Encoding Concept and Distance: The core
idea of encoding is to assign one concept of
associative concept dictionaries into one neuron of
BMM, while the distance value between any two
concepts into the synapse weight between neurons
indicating those of two concepts (See Figure 4).
</bodyText>
<figure confidence="0.5769475">
Associative Concept Dictionary
Pulsed Neural Network(1. Layerl
</figure>
<figureCaption confidence="0.9181345">
Figure 4. Encoding Data of Associative
Concept Dictionaries into BMM.
</figureCaption>
<bodyText confidence="0.999805653846154">
For instance, if the data about concept &amp;quot;snow&amp;quot;,
concept &amp;quot;sleet&amp;quot; and the distance value between two
of them are loaded from the associative concept
dictionary into BMM, then two neurons labeled
&amp;quot;snow&amp;quot; and &amp;quot;sleet&amp;quot; are prepared in BMM and
connected them with each other by a synapse with
a certain weight calculated from the distance value.
In this way, concepts are expressed as neurons,
while relations are expressed as synapses in BMM.
This procedure of encoding is achieved by
ACD_Loader module and Table module. In detail,
when ACD_Loader module read one datum of a
concept from an associative concept dictionary,
first it searches whether the neuron assigned that
concept already exists in BMM, and if not,
prepares a new neuron to assign it. Secondly the
information of the assigned neuron number and the
concept name is paired and preserved in Table
module.
On the other hand, when the datum about
distance is read, ACD_Loader module also
searches and prepares the synapse of which the
relation is consistent, then calculates the
appropriate weight of the synapse from the
distance value. The equation for the calculation is
described as
</bodyText>
<equation confidence="0.978698">
wj = 1(11 (d ,j+ k2) (1)
</equation>
<bodyText confidence="0.998648730769231">
where wu is a synapse weight from neuron j to
neuron i. du is a distance between concept j and
concept i and la, k2 are both constant values
adjusting the order of synapse weights. The values
of synapse weights are preserved not as data in
Table module but as parameters in
Neural_Network module.
Encoding Type of Relation: We should
consider not only the distance of relation but also
the type of relation to encode. As mentioned
above, the data of associative concept dictionaries
are gathered through cognitive experiments, where
the type of relation for associated concepts to
answer has been regulated. In this way, there are
several types of relation registered in associative
concept dictionaries, e.g. generalized, specific,
material, attribute, action and so on (Okamoto and
Ishizaki, 1998). The easiest way to encode them is
to attach a relation tag to each synapse in BMM,
however, adding extra information in the
architecture might be cause to make the structure
more complicated. Thus in this study, we applied
a multiple layered structure of BMM to classify
those data, encoding concepts belonged in another
type of relation in another layer. For instance, we
could treat stimulus concepts itself, associated
</bodyText>
<figure confidence="0.998558888888889">
Brain Memory Model(Ver 2.0)
Ely Taktrya sakag,m(sekatakugs, keo jp)
Layer0 6248 neurons
a Fue ByTareshol. FT
C Fue In Order= In
NI. El Knock Out
Layera 122b neurons
0 Fue EsiThreshol, &amp;quot;FT
C Fue In Order= nn
n Knock.,
MLayer2 NS neurons
0 Flre By-faros., FT
0 Fue In Order= In
El Knock Out
Layer3 0 neurons
a Fue ByTaresholtls FT
C Fas In Or., In
Fl Knockout
</figure>
<figureCaption confidence="0.999913">
Figure 3. GUI Image of BMM.
</figureCaption>
<bodyText confidence="0.997326933333333">
concepts belonged in attribute and another type of
associated concepts belonged in action in a
separate manner, with preparing three layers to
encode each of them. This way of encoding
realizes not only a smart structure of BMM but
also more comfortable analysis, such as a knock
out simulation of each relation.
Outline of the Semantic Network: As a result
of encoding explained above, a semantic network
was built with the structure based on associative
concept dictionaries and the architecture based on
a pulsed neural network of BMM. This semantic
network consists of multiple layers, and concepts
in each layer have intra- and inter-layer
connections with other concepts (See Figure 5).
</bodyText>
<figureCaption confidence="0.962798">
Figure 5. Multiple Layer Structure
of Semantic Network.
</figureCaption>
<bodyText confidence="0.999863857142857">
We could regard a firing of the neuron as a
recalling of the concept assigned by that neuron.
Moreover, since the dynamics of BMM that a
firing of a neuron spreads to other neurons via
synapses is available, it is possible to associate
relative concepts triggered by a recalling of one
concept.
</bodyText>
<subsectionHeader confidence="0.997866">
3.3 Simile Understanding
</subsectionHeader>
<bodyText confidence="0.998173758620689">
Simile_Understanding module introduced in
section 3.1 is the independent module from BMM
itself, designed as one of the applications of the
semantic network built on BMM. The purpose of
this module is to understand simile expressions,
with transferring a typical simile expression &amp;quot;The T
seems just like S&amp;quot; into a simile-understood
expression &amp;quot;The T is so r with the help of the
semantic network on BMM. In this study we
treated only this syntax as a simile expression and
defined T and S are simple noun concepts, in order
to avoid mentioning to morphological analysis.
Procedure of Simile Understanding: There
are several hypotheses suggested for simile
understanding. Comparison theory (Ortony, 1979)
is one of them, which achieves simile
understanding by means of finding a common
concept between vehicle and topic. We followed
this hypothesis and considered the problem just as
a searching problem of goal concept U from target
concept T and source concept S, where U is
considered as the common concept connected with
both of T and S.
Simile_Understanding module solves this
searching problem on the semantic network with
the help of the dynamics to associate relative
concepts, which has been explained above. In
detail, the current of the procedure could be
divided by 7 processes as shown below:
</bodyText>
<listItem confidence="0.995872357142857">
(1) Recognize the simile expression input
(2) Search and stimulate target concept in the
semantic network
(3) The stimulus spreads to other concepts
connected with the target concept
(4) Search and stimulate source concept in the
semantic network
(5) The stimulus spreads to other concepts
connected with the source concept
(6) Two stimuli are integrated only in the concept
connected with both of target and source
concept and make it fire finally
(7) Output the concept fired as goal concept in the
simile-understood expression
</listItem>
<bodyText confidence="0.99601512962963">
Here we would trace the current of these
processes considering the case of a typical simile
expression &amp;quot;The skin seems just like snow&amp;quot;. When
a user input target concept &amp;quot;skin&amp;quot; and source
concept &amp;quot;snow&amp;quot; into the system, first it searches the
neuron indicating &amp;quot;snow&amp;quot; in BMM and stimulates
it strongly enough for its firing. At the next step,
the firing of the neuron spreads in a network and
stimulates other neurons connected with it, in this
case the neuron indicating &amp;quot;white&amp;quot; and the neuron
indicating &amp;quot;cold&amp;quot; are stimulated. On the other
hand, the system also fires the neuron of source
concept &amp;quot;skin&amp;quot;, causing the trigger of spreading
activity to stimulate other connected neurons,
indicating such as &amp;quot;white&amp;quot;, &amp;quot;beautiful&amp;quot; or &amp;quot;smooth&amp;quot;.
Note that the neuron of &amp;quot;white&amp;quot;, which is the only
neuron connected with both of &amp;quot;snow&amp;quot; and &amp;quot;skin&amp;quot;,
is stimulated by two neurons to increase its
membrane potential in two times. As a result, the
most excited neuron in BMM would be the one
indicating &amp;quot;white&amp;quot; and the system finally outputs
&amp;quot;white&amp;quot; as the primary goal concept.
Asymmetry of Target Concept and Source
Concept: In general, it is not a good idea to treat
target concept and source concept equally, since
the influence of those of two concepts for the
simile understanding are not equivalent. We could
realize this easily, considering that the simile
expression &amp;quot;The person seems just like a robot&amp;quot;
and the other expression &amp;quot;The robot seems just like
a person&amp;quot; give us quite different impressions,
though the differences between those of two
expressions are nothing but only target concept and
source concept have been exchanged.
Kusumi investigated about this problem, and
concluded that the influence of source concept to
simile understanding is about twice as strong as
that of target concept (Kusumi, 1987). In this
study we followed this investigation and
differentiated the influence of target concept and
source concept, by means of putting a short time
delay between the process of stimulating for target
concept (process (2)) and that for source concept
(process (4)). The dynamics of the semantic
network is given by BMM based on a pulsed
neural network model, where the membrane
potential of a neuron decays as time advances and
a stimulus in past becomes less and less influential.
We applied this aspect to make the later stimulus
of source concept more influential than the earlier
stimulus of target concept, putting 2 to 3 time steps
of delay between the process (2) and (4) that keeps
the best equilibrium of ratio 2:1 for simile
understanding.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="method">
4 Result
</sectionHeader>
<subsectionHeader confidence="0.999335">
4.1 Network Constructing
</subsectionHeader>
<bodyText confidence="0.999631264705882">
In order to build a semantic network on BMM,
first we considered to adopt two kinds of
associative concept dictionaries to load, i.e.
ACD50 and ACD10. ACD50 is the associative
concept dictionary constructed through cognitive
experiments with 50 human subjects for each
concept, containing about 60,000 entries with a
certain level of reliability. On the other hand,
ACD10 is constructed with 10 human subjects,
containing more than 70,000 entries but with less
reliability. Both of associative concept dictionaries
consist of only Japanese concepts. Loading these
multiple databases realized to build a richer
network with wide and deep knowledge on BMM.
Secondly, we considered data with answering rate
less than 20% for ACD10 and 4% for ACD50 as
junk data and ignored them when loading data into
BMM. And finally we prepared three layers in
BMM and separated concepts by their types of
relation with loading them in other layers, i.e.
Layer #0 for stimulus concepts and associated
concepts belonged in generalized or specific, Layer
#1 for associated concepts in action and Layer #2
for associated concepts in attribute. In associative
concept dictionaries, almost all of associated
concepts belonged in action are verb and concepts
in attribute are adjective, thus this proceeding has
the purpose to separate concepts by their parts of
speech.
The table shown below donates the properties of
the semantic network built on BMM, as a result of
loading two kinds of associative concept
dictionaries with the configurations explained
above (See Table 1).
</bodyText>
<table confidence="0.6226806">
N um of Concepts
_ayer 0 6 248
_ayer I I 336
_ayer 2 906
Total 8 490
</table>
<tableCaption confidence="0.994019">
Table 1. Number of Concepts in Semantic Network.
</tableCaption>
<subsectionHeader confidence="0.995932">
4.2 Simile Understanding
</subsectionHeader>
<bodyText confidence="0.999979363636364">
In this section, we would display the outputs of
Simile_Understanding module, which transfers a
simile expression input into a simile-understood
expression output.
First we selected several examples of simile
expressions for the demonstration of
Simile_Understanding module. Since it searches
goal concept with stimulating target concept and
source concept in the semantic network, the two of
concepts in a simile expression must be registered
in the semantic network first. Additionally, if there
is no relation between those concepts at all, the
module might not output the appropriate goal
concept. Taking into account these points enough,
we finally selected 10 examples which are
displayed in the table below (See Table 2). The
table also displays the output of the module for
each input. For instance, we could understand
from this table that the simile expression &amp;quot;The skin
seems just like snow&amp;quot; was input at Task #1 and the
simile-understood expression &amp;quot;The skin is so
white&amp;quot; was output by the module.
</bodyText>
<table confidence="0.999534727272727">
Task No.
Task #1 Skin Snow white
Task #2 Mind Snow cold
Task #3 Mind Flame hot
Task #4 Robot Person wise
Task #5 Person Robot cold
Task #6 personality Stone hard
Task #7 Thought Sky deep
Task #8 Dream Earth huge
Task #9 Word Needle sharp
Task #10 Life Violet flail
</table>
<tableCaption confidence="0.997061">
Table 2. Input/Output for Simile Understanding.
</tableCaption>
<bodyText confidence="0.9999248">
From the results of Task #1, Task #2 and Task
#3, we could find that the module certainly
referred to both of source concept and target
concept for simile understanding. On the other
hand, we could also find from the results of Task
#4 and #5 that the asymmetry of two concepts was
considered appropriately in the procedure. And
from the whole results, it might be possible to say
that the module could output a simile-understood
expression understandable in general for each
simile expression, as long as target concept and
source concept are registered and related in the
semantic network.
We finally concluded from these considerations
that the semantic network built on BMM with
encoding associative concept dictionaries has the
probability for the application to NLP such as
simile understanding.
For more evaluation of Simile_ Understanding
module, we could refer the study by Sakaguchi and
Ishizaki where the difference between outputs of
the module and answers by human beings has been
investigated, concluding that the module could
follow human beings in most simile expressions
(Sakaguchi and Ishizaki, 2000).
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999901325581395">
In this study, first we constructed BMM
architecture, which would be the platform in which
a semantic network was built. It was designed
based on a pulsed neural network, where a spatio-
temporal integration of firings is possible.
Secondly we considered the way of encoding data
of associative concept dictionaries into architecture
of BMM, i.e. concepts into neurons, distance
values into synapse weights and types of relation
into layers of BMM. We indeed encoded two
kinds of associative concept dictionaries into
BMM and built a semantic network containing
totally about 8,500 concepts. Finally, we
constructed the module for simile understanding,
which transfers a typical simile expression into an
appropriate simile-understood expression by means
of searching a common concept connected both of
target concept and source concept in the semantic
network built on BMM.
There might be some problems remained for
future works. The first problem is that the number
of concepts composing a semantic network is less
than 10,000, which is not enough for a practical
semantic network. We should encode more data of
associative concept dictionaries, or consider to
adopt another database such as WordNet (Miller et
al., 1993). If we succeed to expand the scale of a
semantic network with more concepts, it is
expected that much more simile expressions
become available for the evaluation of
Simile _Unde rstanding module.
The second problem we consider is that synaptic
weights are treated as constant value in this study.
We would like to improve this point with
introducing the learning algorithm for BMM. It
must make the semantic network much more
dynamic model, which is possible of learning, self-
organizing and optimizing its structure.
Finally, we would like to keep improving BMM
toward the complete model, explaining the whole
memory structure in the brain and realizing any
simulation of intellectual activity, that would be
called the &amp;quot;real&amp;quot; Brain Memory Model in the future.
</bodyText>
<sectionHeader confidence="0.999526" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891370370371">
Collins,A M and Loftus,E.F., &amp;quot;A Spreading-Activation
Theory of Semantic Processing&amp;quot;, In Psychological
Review, Vol.82-6, pp.407-428, 1975.
Collins,A M and Quillian,M.R., &amp;quot;Retrieval Time from
Semantic Memory&amp;quot;, In Journal of Verbal Learning
and Verbal Behavior, Vol.8, pp.240-247, 1969.
E.Domany, J.L.van Hemmen, K.Schulten, &amp;quot;Models of
Neural Networks II --- Temporal Aspects of Coding
and Information Processing in Biological Systems&amp;quot;,
Springer-Verlag, 1994.
Kusumi,T., &amp;quot;Effects of categorical dissimilarity and
affective similarity between constituent words on
metaphor appreciation&amp;quot;, Journal of Psycholinguistic
Research, 16, pp.577-595.
Miller,G.A., Beckwin,R., Fellbaum,C., Gross,D.,
Miller,K. and Tengi,R., &amp;quot;Five Papers on WordNet&amp;quot;,
Cs1 report 43, Cognitive science Laboratory
Princeton University, 1993.
Okamoto,J. and Ishizaki,S , &amp;quot;Construction of Electronic
Concept Dictionary and Quantification of Concept
Space&amp;quot;, SIG-NL-1301PSJ, 1998.
Ortony,A., &amp;quot;Beyond literal similarity&amp;quot;, Psychological
Review, 86, pp.161-280, 1979.
Sakaguchi,T. and Ishizaki,S., &amp;quot;Simile Understanding
System Using Associative Concept Dictionary and
Pulsed Neural Network&amp;quot;, Pacific Association for
Computational Linguistics, 2000.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.183594">
<title confidence="0.9992245">A Japanese Semantic Network built on a Pulsed Neural Network encoding Associative Concept Dictionaries</title>
<author confidence="0.809855">Takuya</author>
<affiliation confidence="0.86726">Media and Keio University</affiliation>
<email confidence="0.850958">sakataku@sfc.keio.ac.jp</email>
<author confidence="0.519562">Shun</author>
<affiliation confidence="0.767413">Environmental Keio University</affiliation>
<email confidence="0.9126">ishizaki@sfc.keio.ac.jp</email>
<abstract confidence="0.99915488">A semantic network with the dynamics such as spreading activity is available for various systems as well as the structure model of knowledge. In this study we suggest a practical dynamic semantic network available for NLP, which has the structure from associative concept dictionaries and the dynamics from a pulsed neural network. We built the semantic network by means of constructing the platform called &amp;quot;Brain Memory Model&amp;quot; based on a pulsed neural network first, then encoding data of associative concept dictionaries into it. We also constructed the module as one of the applications of the semantic network built on BMM to NLP, especially to simile understanding. The outputs of the module were understandable in general, indicating the possibility of our semantic network. We are now considering to expand the scale of a semantic network and to introduce the learning algorithm for its self-organization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>E F Loftus</author>
</authors>
<title>A Spreading-Activation Theory of Semantic Processing&amp;quot;,</title>
<date>1975</date>
<journal>In Psychological Review,</journal>
<volume>82</volume>
<pages>407--428</pages>
<contexts>
<context position="1640" citStr="Collins and Loftus, 1975" startWordPosition="246" endWordPosition="249">the possibility of our semantic network. We are now considering to expand the scale of a semantic network and to introduce the learning algorithm for its self-organization. 1 Introduction A semantic network (Collins and Quillian, 1969) is a kind of network model, describing a structure of concepts preserved in a human brain. Basically it is formed with a number of concepts as nodes and relations between each concept as arks of a network. Several models of a semantic network have been suggested, some of them are dynamic models mentioning to the dynamics of a network such as spreading activity (Collins and Loftus, 1975). These models are available not only for structure models about concepts but also for some useful systems of associating, reasoning or natural language processing (NLP). When considering to design such a dynamic and practical model of a semantic network, we have to mention to at least two problems, i.e. how to construct a large-scale structure and how to provide a dynamics of a network. In this study, we solved these problems by means of applying the database called &amp;quot;Associative Concept Dictionary&amp;quot; for its structure and the architecture constructed with a pulsed neural network for its dynamic</context>
</contexts>
<marker>Collins, Loftus, 1975</marker>
<rawString>Collins,A M and Loftus,E.F., &amp;quot;A Spreading-Activation Theory of Semantic Processing&amp;quot;, In Psychological Review, Vol.82-6, pp.407-428, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>M R Quillian</author>
</authors>
<title>Retrieval Time from Semantic Memory&amp;quot;,</title>
<date>1969</date>
<booktitle>In Journal of Verbal Learning and Verbal Behavior, Vol.8,</booktitle>
<pages>240--247</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="1250" citStr="Collins and Quillian, 1969" startWordPosition="179" endWordPosition="182">antic network by means of constructing the platform called &amp;quot;Brain Memory Model&amp;quot; based on a pulsed neural network first, then encoding data of associative concept dictionaries into it. We also constructed the module as one of the applications of the semantic network built on BMM to NLP, especially to simile understanding. The outputs of the module were understandable in general, indicating the possibility of our semantic network. We are now considering to expand the scale of a semantic network and to introduce the learning algorithm for its self-organization. 1 Introduction A semantic network (Collins and Quillian, 1969) is a kind of network model, describing a structure of concepts preserved in a human brain. Basically it is formed with a number of concepts as nodes and relations between each concept as arks of a network. Several models of a semantic network have been suggested, some of them are dynamic models mentioning to the dynamics of a network such as spreading activity (Collins and Loftus, 1975). These models are available not only for structure models about concepts but also for some useful systems of associating, reasoning or natural language processing (NLP). When considering to design such a dynam</context>
</contexts>
<marker>Collins, Quillian, 1969</marker>
<rawString>Collins,A M and Quillian,M.R., &amp;quot;Retrieval Time from Semantic Memory&amp;quot;, In Journal of Verbal Learning and Verbal Behavior, Vol.8, pp.240-247, 1969. E.Domany, J.L.van Hemmen, K.Schulten, &amp;quot;Models of Neural Networks II --- Temporal Aspects of Coding and Information Processing in Biological Systems&amp;quot;, Springer-Verlag, 1994.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Kusumi</author>
</authors>
<title>Effects of categorical dissimilarity and affective similarity between constituent words on metaphor appreciation&amp;quot;,</title>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>16</volume>
<pages>577--595</pages>
<marker>Kusumi, </marker>
<rawString>Kusumi,T., &amp;quot;Effects of categorical dissimilarity and affective similarity between constituent words on metaphor appreciation&amp;quot;, Journal of Psycholinguistic Research, 16, pp.577-595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwin</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
<author>R Tengi</author>
</authors>
<title>Five Papers on WordNet&amp;quot;,</title>
<date>1993</date>
<tech>Cs1 report 43,</tech>
<institution>Cognitive science Laboratory Princeton University,</institution>
<contexts>
<context position="21763" citStr="Miller et al., 1993" startWordPosition="3540" endWordPosition="3543"> constructed the module for simile understanding, which transfers a typical simile expression into an appropriate simile-understood expression by means of searching a common concept connected both of target concept and source concept in the semantic network built on BMM. There might be some problems remained for future works. The first problem is that the number of concepts composing a semantic network is less than 10,000, which is not enough for a practical semantic network. We should encode more data of associative concept dictionaries, or consider to adopt another database such as WordNet (Miller et al., 1993). If we succeed to expand the scale of a semantic network with more concepts, it is expected that much more simile expressions become available for the evaluation of Simile _Unde rstanding module. The second problem we consider is that synaptic weights are treated as constant value in this study. We would like to improve this point with introducing the learning algorithm for BMM. It must make the semantic network much more dynamic model, which is possible of learning, selforganizing and optimizing its structure. Finally, we would like to keep improving BMM toward the complete model, explaining</context>
</contexts>
<marker>Miller, Beckwin, Fellbaum, Gross, Miller, Tengi, 1993</marker>
<rawString>Miller,G.A., Beckwin,R., Fellbaum,C., Gross,D., Miller,K. and Tengi,R., &amp;quot;Five Papers on WordNet&amp;quot;, Cs1 report 43, Cognitive science Laboratory Princeton University, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Okamoto</author>
<author>S Ishizaki</author>
</authors>
<title>Construction of Electronic Concept Dictionary and Quantification of Concept Space&amp;quot;,</title>
<date>1301</date>
<marker>Okamoto, Ishizaki, 1301</marker>
<rawString>Okamoto,J. and Ishizaki,S , &amp;quot;Construction of Electronic Concept Dictionary and Quantification of Concept Space&amp;quot;, SIG-NL-1301PSJ, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ortony</author>
</authors>
<title>Beyond literal similarity&amp;quot;,</title>
<date>1979</date>
<journal>Psychological Review,</journal>
<volume>86</volume>
<pages>161--280</pages>
<contexts>
<context position="12662" citStr="Ortony, 1979" startWordPosition="2071" endWordPosition="2072">tself, designed as one of the applications of the semantic network built on BMM. The purpose of this module is to understand simile expressions, with transferring a typical simile expression &amp;quot;The T seems just like S&amp;quot; into a simile-understood expression &amp;quot;The T is so r with the help of the semantic network on BMM. In this study we treated only this syntax as a simile expression and defined T and S are simple noun concepts, in order to avoid mentioning to morphological analysis. Procedure of Simile Understanding: There are several hypotheses suggested for simile understanding. Comparison theory (Ortony, 1979) is one of them, which achieves simile understanding by means of finding a common concept between vehicle and topic. We followed this hypothesis and considered the problem just as a searching problem of goal concept U from target concept T and source concept S, where U is considered as the common concept connected with both of T and S. Simile_Understanding module solves this searching problem on the semantic network with the help of the dynamics to associate relative concepts, which has been explained above. In detail, the current of the procedure could be divided by 7 processes as shown below</context>
</contexts>
<marker>Ortony, 1979</marker>
<rawString>Ortony,A., &amp;quot;Beyond literal similarity&amp;quot;, Psychological Review, 86, pp.161-280, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sakaguchi</author>
<author>S Ishizaki</author>
</authors>
<title>Simile Understanding System Using Associative Concept Dictionary and Pulsed Neural Network&amp;quot;, Pacific Association for Computational Linguistics,</title>
<date>2000</date>
<contexts>
<context position="20527" citStr="Sakaguchi and Ishizaki, 2000" startWordPosition="3348" endWordPosition="3351">sion, as long as target concept and source concept are registered and related in the semantic network. We finally concluded from these considerations that the semantic network built on BMM with encoding associative concept dictionaries has the probability for the application to NLP such as simile understanding. For more evaluation of Simile_ Understanding module, we could refer the study by Sakaguchi and Ishizaki where the difference between outputs of the module and answers by human beings has been investigated, concluding that the module could follow human beings in most simile expressions (Sakaguchi and Ishizaki, 2000). 5 Conclusion In this study, first we constructed BMM architecture, which would be the platform in which a semantic network was built. It was designed based on a pulsed neural network, where a spatiotemporal integration of firings is possible. Secondly we considered the way of encoding data of associative concept dictionaries into architecture of BMM, i.e. concepts into neurons, distance values into synapse weights and types of relation into layers of BMM. We indeed encoded two kinds of associative concept dictionaries into BMM and built a semantic network containing totally about 8,500 conce</context>
</contexts>
<marker>Sakaguchi, Ishizaki, 2000</marker>
<rawString>Sakaguchi,T. and Ishizaki,S., &amp;quot;Simile Understanding System Using Associative Concept Dictionary and Pulsed Neural Network&amp;quot;, Pacific Association for Computational Linguistics, 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>