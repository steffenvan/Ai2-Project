<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000195">
<title confidence="0.997254">
Integrating Multiple Knowledge Sources to
Disambiguate Word Sense: An Exemplar-Based Approach
</title>
<author confidence="0.969655">
Hwee Tou Ng
</author>
<affiliation confidence="0.95547">
Defence Science Organisation
</affiliation>
<address confidence="0.9044925">
20 Science Park Drive
Singapore 118230
</address>
<email confidence="0.933415">
nhweetouOtrantor.dso.gov.sg
</email>
<author confidence="0.651775">
Hian Beng Lee
</author>
<affiliation confidence="0.654141">
Defence Science Organisation
</affiliation>
<address confidence="0.903422">
20 Science Park Drive
Singapore 118230
</address>
<email confidence="0.945219">
lhianbeneltrantor.dso.gov.sg
</email>
<sectionHeader confidence="0.991851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999582894736842">
In this paper, we present a new approach
for word sense disambiguation (WSD) us-
ing an exemplar-based learning algorithm.
This approach integrates a diverse set of
knowledge sources to disambiguate word
sense, including part of speech of neigh-
boring words, morphological form, the un-
ordered set of surrounding words, local
collocations, and verb-object syntactic re-
lation. We tested our WSD program,
named LEXAS, on both a common data
set used in previous work, as well as on
a large sense-tagged corpus that we sep-
arately constructed. LEXAS achieves a
higher accuracy on the common data set,
and performs better than the most frequent
heuristic on the highly ambiguous words
in the large corpus tagged with the refined
senses of WORDNET.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999801476190476">
One important problem of Natural Language Pro-
cessing (NLP) is figuring out what a word means
when it is used in a particular context. The different
meanings of a word are listed as its various senses in
a dictionary. The task of Word Sense Disambigua-
tion (WSD) is to identify the correct sense of a word
in context. Improvement in the accuracy of iden-
tifying the correct word sense will result in better
machine translation systems, information retrieval
systems, etc. For example, in machine translation,
knowing the correct word sense helps to select the
appropriate target words to use in order to translate
into a target language.
In this paper, we present a new approach for
WSD using an exemplar-based learning algorithm.
This approach integrates a diverse set of knowledge
sources to disambiguate word sense, including part
of speech (POS) of neighboring words, morphologi-
cal form, the unordered set of surrounding words,
local collocations, and verb-object syntactic rela-
tion. To evaluate our WSD program, named LEXAS
(LEXical Ambiguity-resolving System), we tested it
on a common data set involving the noun &amp;quot;interest&amp;quot;
used by Bruce and Wiebe (Bruce and Wiebe, 1994).
LEXAS achieves a mean accuracy of 87.4% on this
data set, which is higher than the accuracy of 78%
reported in (Bruce and Wiebe, 1994).
Moreover, to test the scalability of LEXAS, we have
acquired a corpus in which 192,800 word occurrences
have been manually tagged with senses from WORD-
NET, which is a public domain lexical database con-
taining about 95,000 word forms and 70,000 lexical
concepts (Miller, 1990). These sense tagged word
occurrences consist of 191 most frequently occur-
ring and most ambiguous nouns and verbs. When
tested on this large data set, LEXAS performs better
than the default strategy of picking the most fre-
quent sense. To our knowledge, this is the first time
that a WSD program has been tested on such a large
scale, and yielding results better than the most fre-
quent heuristic on highly ambiguous words with the
refined sense distinctions of WORDNET.
</bodyText>
<sectionHeader confidence="0.988757" genericHeader="general terms">
2 Task Description
</sectionHeader>
<bodyText confidence="0.98910185">
The input to a WSD program consists of unre-
stricted, real-world English sentences. In the out-
put, each word occurrence w is tagged with its cor-
rect sense (according to the context) in the form of
a sense number i, where i corresponds to the i-th
sense definition of w as given in some dictionary.
The choice of which sense definitions to use (and
according to which dictionary) is agreed upon in ad-
vance.
For our work, we use the sense definitions as given
in WORDNET, which is comparable to a good desk-
top printed dictionary in its coverage and sense dis-
tinction. Since WORDNET only provides sense def-
initions for content words, (i.e., words in the parts
of speech (POS) noun, verb, adjective, and adverb),
LEXAS is only concerned with disambiguating the
sense of content words. However, almost all existing
work in WSD deals only with disambiguating con-
tent words too.
LEXAS assumes that each word in an input sen-
</bodyText>
<page confidence="0.996974">
40
</page>
<bodyText confidence="0.991891111111111">
tence has been pre-tagged with its correct POS, so
that the possible senses to consider for a content
word w are only those associated with the particu-
lar POS of w in the sentence. For instance, given
the sentence &amp;quot;A reduction of principal and interest
is one way the problem may be solved.&amp;quot;, since the
word &amp;quot;interest&amp;quot; appears as a noun in this sentence,
LEXAS will only consider the noun senses of &amp;quot;inter-
est&amp;quot; but not its verb senses. That is, LEXAS is only
concerned with disambiguating senses of a word in
a given POS. Making such an assumption is reason-
able since POS taggers that can achieve accuracy
of 96% are readily available to assign POS to un-
restricted English sentences (Brill, 1992; Cutting et
al., 1992).
In addition, sense definitions are only available for
root words in a dictionary. These are words that
are not morphologically inflected, such as &amp;quot;interest&amp;quot;
(as opposed to the plural form &amp;quot;interests&amp;quot;), &amp;quot;fall&amp;quot;
(as opposed to the other inflected forms like &amp;quot;fell&amp;quot;,
&amp;quot;fallen&amp;quot;, &amp;quot;falling&amp;quot;, &amp;quot;falls&amp;quot;), etc. The sense of a mor-
phologically inflected content word is the sense of its
uninflected form. LEXAS follows this convention by
first converting each word in an input sentence into
its morphological root using the morphological ana-
lyzer of WORD NET, before assigning the appropriate
word sense to the root form.
</bodyText>
<sectionHeader confidence="0.993534" genericHeader="keywords">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.933760230769231">
LEXAS performs WSD by first learning from a train-
ing corpus of sentences in which words have been
pre-tagged with their correct senses. That is, it uses
supervised learning, in particular exemplar-based
learning, to achieve WSD. Our approach has been
fully implemented in the program LEXAS. Part of
the implementation uses PEBLS (Cost and Salzberg,
1993; Rachlin and Salzberg, 1993), a public domain
exemplar-based learning system.
LEXAS builds one exemplar-based classifier for
each content word w. It operates in two phases:
training phase and test phase. In the training phase,
LEXAS is given a set S of sentences in the training
corpus in which sense-tagged occurrences of w ap-
pear. For each training sentence with an occurrence
of w, LEXAS extracts the parts of speech (POS) of
words surrounding w, the morphological form of w,
the words that frequently co-occur with w in the
same sentence, and the local collocations containing
w. For disambiguating a noun w, the verb which
takes the current noun w as the object is also iden-
tified. This set of values form the features of an ex-
ample, with one training sentence contributing one
training example.
Subsequently, in the test phase, LEXAS is given
new, previously unseen sentences. For a new sen-
tence containing the word w, LEXAS extracts from
the new sentence the values for the same set of fea-
tures, including parts of speech of words surround-
ing w, the morphological form of w, the frequently
co-occurring words surrounding w, the local colloca-
tions containing w, and the verb that takes w as an
object (for the case when w is a noun). These values
form the features of a test example.
This test example is then compared to every train-
ing example. The sense of word w in the test exam-
ple is the sense of w in the closest matching train-
ing example, where there is a precise, computational
definition of &amp;quot;closest match&amp;quot; as explained later.
</bodyText>
<subsectionHeader confidence="0.999082">
3.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.927653285714286">
The first step of the algorithm is to extract a set F
of features such that each sentence containing an oc-
currence of iv will form a training example supplying
the necessary values for the set F of features.
Specifically, LEXAS uses the following set of fea-
tures to form a training example:
L3, L2, L1, R1, R2, R3, M, Ki, • • • , Km, C1, • • • C9, V
</bodyText>
<subsectionHeader confidence="0.7153895">
3.1.1 Part of Speech and Morphological
Form
</subsectionHeader>
<bodyText confidence="0.9997515">
The value of feature Li is the part of speech (POS)
of the word i-th position to the left of w. The value
of Ri is the POS of the word i-th position to the right
of w. Feature M denotes the morphological form of
w in the sentence s. For a noun, the value for this
feature is either singular or plural; for a verb, the
value is one of infinitive (as in the uninflected form
of a verb like &amp;quot;fall&amp;quot;), present-third-person-singular
(as in &amp;quot;falls&amp;quot;), past (as in &amp;quot;fell&amp;quot;), present-participle
(as in &amp;quot;falling&amp;quot;) or past-participle (as in &amp;quot;fallen&amp;quot;).
</bodyText>
<subsubsectionHeader confidence="0.534719">
3.1.2 Unordered Set of Surrounding Words
</subsubsectionHeader>
<equation confidence="0.503877">
K , K, are features corresponding to a set of
</equation>
<bodyText confidence="0.999288916666667">
keywords that frequently co-occur with word w in
the same sentence. For a sentence s, the value of
feature Ki is one if the keyword Ki appears some-
where in sentence s, else the value of K2 is zero.
The set of keywords K1, , Km are determined
based on conditional probability. All the word to-
kens other than the word occurrence w in a sen-
tence s are candidates for consideration as keywords.
These tokens are converted to lower case form before
being considered as candidates for keywords.
Let cp(ilk) denotes the conditional probability of
sense i of w given keyword k, where
</bodyText>
<equation confidence="0.4858345">
cp(ilk)&apos;k
Nk
</equation>
<footnote confidence="0.721567625">
Nk is the number of sentences in which keyword k co-
occurs with w, and Ni,k is the number of sentences
in which keyword k co-occurs with w where w has
sense i.
For a keyword k to be selected as a feature, it
must satisfy the following criteria:
1. cp(ilk) Mi for some sense i, where M1 is some
predefined minimum probability.
</footnote>
<page confidence="0.995805">
41
</page>
<listItem confidence="0.9627734">
2. The keyword k must occur at least M2 times
in some sense i, where M2 is some predefined
minimum value.
3. Select at most M3 number of keywords for a
given sense i if the number of keywords satisfy-
</listItem>
<bodyText confidence="0.947106">
ing the first two criteria for a given sense i ex-
ceeds M3. In this case, keywords that co-occur
more frequently (in terms of absolute frequency)
with sense i of word w are selected over those
co-occurring less frequently.
Condition 1 ensures that a selected keyword is in-
dicative of some sense i of w since cp(ijk) is at least
some minimum probability M1. Condition 2 reduces
the possibility of selecting a keyword based on spu-
rious occurrence. Condition 3 prefers keywords that
co-occur more frequently if there is a large number
of eligible keywords.
For example, M1 = 0.8, M2 7= 5, M3 := 5 when
LEXAS was tested on the common data set reported
in Section 4.1.
To illustrate, when disambiguating the noun &amp;quot;in-
terest&amp;quot;, some of the selected keywords are: ex-
pressed, acquiring, great, attracted, expressions,
pursue, best, conflict, served, short, minority, rates,
rate, bonds, lower, payments.
</bodyText>
<subsectionHeader confidence="0.926155">
3.1.3 Local Collocations
</subsectionHeader>
<bodyText confidence="0.999882741935484">
Local collocations are common expressions con-
taining the word to be disambiguated. For our pur-
pose, the term collocation does not imply idiomatic
usage, just words that are frequently adjacent to the
word to be disambiguated. Examples of local collo-
cations of the noun &amp;quot;interest&amp;quot; include &amp;quot;in the interest
of&amp;quot;, &amp;quot;principal and interest&amp;quot;, etc. When a word to
be disambiguated occurs as part of a collocation, its
sense can be frequently determined very reliably. For
example, the collocation &amp;quot;in the interest of&amp;quot; always
implies the &amp;quot;advantage, advancement, favor&amp;quot; sense
of the noun &amp;quot;interest&amp;quot; Note that the method for
extraction of keywords that we described earlier will
fail to find the words &amp;quot;in&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;of&amp;quot; as keywords,
since these words will appear in many different po-
sitions in a sentence for many senses of the noun
&amp;quot;interest&amp;quot;. It is only when these words appear in
the exact order &amp;quot;in the interest of&amp;quot; around the noun
&amp;quot;interest&amp;quot; that strongly implies the &amp;quot;advantage, ad-
vancement, favor&amp;quot; sense.
There are nine features related to collocations in
an example. Table 1 lists the nine features and some
collocation examples for the noun &amp;quot;interest&amp;quot;. For ex-
ample, the feature with left offset = -2 and right off-
set = 1 refers to the possible collocations beginning
at the word two positions to the left of &amp;quot;interest&amp;quot;
and ending at the word one position to the right of
&amp;quot;interest&amp;quot;. An example of such a collocation is &amp;quot;in
the interest of&amp;quot;.
The method for extraction of local collocations is
similar to that for extraction of keywords. For each
</bodyText>
<table confidence="0.9989939">
Left Offset Right Offset Collocation Example
-1 -1 accrued interest
1 1 interest rate
-2 -1 principal and interest
-1 1 national interest in
1 2 interest and dividends
-3 -1 sale of an interest
-2 1 in the interest of
-1 2 an interest in a
1 3 interest on the bonds
</table>
<tableCaption confidence="0.967268">
Table 1: Features for Collocations
</tableCaption>
<bodyText confidence="0.999776333333333">
of the nine collocation features, LEXAS concatenates
the words between the left and right offset positions.
Using similar conditional probability criteria for the
selection of keywords, collocations that are predic-
tive of a certain sense are selected to form the pos-
sible values for a collocation feature.
</bodyText>
<subsubsectionHeader confidence="0.675242">
3.1.4 Verb- Object Syntactic Relation
</subsubsectionHeader>
<bodyText confidence="0.999959">
LEXAS also makes use of the verb-object syntactic
relation as one feature V for the disambiguation of
nouns. If a noun to be disambiguated is the head of
a noun group, as indicated by its last position in a
noun group bracketing, and if the word immediately
preceding the opening noun group bracketing is a
verb, LEXAS takes such a verb-noun pair to be in a
verb-object syntactic relation. Again, using similar
conditional probability criteria for the selection of
keywords, verbs that are predictive of a certain sense
of the noun to be disambiguated are selected to form
the possible values for this verb-object feature V.
Since our training and test sentences come with
noun group bracketing, determining verb-object re-
lation using the above heuristic can be readily done.
In future work, we plan to incorporate more syntac-
tic relations including subject-verb, and adjective-
headnoun relations. We also plan to use verb-
object and subject-verb relations to disambiguate
verb senses.
</bodyText>
<subsectionHeader confidence="0.999349">
3.2 Training and Testing
</subsectionHeader>
<bodyText confidence="0.999948333333333">
The heart of exemplar-based learning is a measure
of the similarity, or distance, between two examples.
If the distance between two examples is small, then
the two examples are similar. We use the following
definition of distance between two symbolic values
Vi and v2 of a feature f:
</bodyText>
<equation confidence="0.92554725">
ni C2 i
=
d(Vi, V2)
Cl C2
</equation>
<footnote confidence="0.944756">
C1,i is the number of training examples with value
v1 for feature f that is classified as sense i in the
training corpus, and C1 is the number of training
examples with value v1 for feature f in any sense.
C2,i and C2 denote similar quantities for value v2 of
</footnote>
<page confidence="0.785662">
i=1
42
</page>
<bodyText confidence="0.99993092">
feature f. 7/ is the total number of senses for a word
w.
This metric for measuring distance is adopted
from (Cost and Salzberg, 1993), which in turn is
adapted from the value difference metric of the ear-
lier work of (Stanfill and Waltz, 1986). The distance
between two examples is the sum of the distances
between the values of all the features of the two ex-
amples.
During the training phase, the appropriate set of
features is extracted based on the method described
in Section 3.1. From the training examples formed,
the distance between any two values for a feature f
is computed based on the above formula.
During the test phase, a test example is compared
against all the training examples. LEXAS then deter-
mines the closest matching training example as the
one with the minimum distance to the test example.
The sense of w in the test example is the sense of w
in this closest matching training example.
If there is a tie among several training examples
with the same minimum distance to the test exam-
ple, LEXAS randomly selects one of these training
examples as the closet matching training example in
order to break the tie.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999975">
To evaluate the performance of LEXAS, we con-
ducted two tests, one on a common data set used in
(Bruce and Wiebe, 1994), and another on a larger
data set that we separately collected.
</bodyText>
<subsectionHeader confidence="0.997096">
4.1 Evaluation on a Common Data Set
</subsectionHeader>
<bodyText confidence="0.999865">
To our knowledge, very few of the existing work on
WSD has been tested and compared on a common
data set. This is in contrast to established practice
in the machine learning community. This is partly
because there are not many common data sets pub-
licly available for testing WSD programs.
One exception is the sense-tagged data set used
in (Bruce and Wiebe, 1994), which has been made
available in the public domain by Bruce and Wiebe.
This data set consists of 2369 sentences each con-
taining an occurrence of the noun &amp;quot;interest&amp;quot; (or its
plural form &amp;quot;interests&amp;quot;) with its correct sense man-
ually tagged. The noun &amp;quot;interest&amp;quot; occurs in six dif-
ferent senses in this data set. Table 2 shows the
distribution of sense tags from the data set that we
obtained. Note that the sense definitions used in this
data set are those from Longman Dictionary of Con-
temporary English (LDOCE) (Procter, 1978). This
does not pose any problem for LEXAS, since LEXAS
only requires that there be a division of senses into
different classes, regardless of how the sense classes
are defined or numbered.
POS of words are given in the data set, as well
as the bracketings of noun groups. These are used
to determine the POS of neighboring words and the
</bodyText>
<table confidence="0.9998085">
LDOCE sense Frequency Percent
1: readiness to give 361 15%
attention
2: quality of causing 11 &lt;1%
attention to be given
3: activity, subject, etc. 67 3%
which one gives time and
attention to
4: advantage, 178 8%
advancement, or favor
5: a share (in a company, 499 21%
business, etc.)
6: money paid for the use 1253 53%
of money
</table>
<tableCaption confidence="0.99741">
Table 2: Distribution of Sense Tags
</tableCaption>
<bodyText confidence="0.992475682926829">
verb-object syntactic relation to form the features of
examples.
In the results reported in (Bruce and Wiebe,
1994), they used a test set of 600 randomly selected
sentences from the 2369 sentences. Unfortunately,
in the data set made available in the public domain,
there is no indication of which sentences are used as
test sentences. As such, we conducted 100 random
trials, and in each trial, 600 sentences were randomly
selected to form the test set. LEXAS is trained on
the remaining 1769 sentences, and then tested on a
separate test set of sentences in each trial.
Note that in Bruce and Wiebe&apos;s test run, the pro-
portion of sentences in each sense in the test set is
approximately equal to their proportion in the whole
data set. Since we use random selection of test sen-
tences, the proportion of each sense in our test set is
also approximately equal to their proportion in the
whole data set in our random trials.
The average accuracy of LEXAS over 100 random
trials is 87.4%, and the standard deviation is 1.37%.
In each of our 100 random trials, the accuracy of
LEXAS is always higher than the accuracy of 78%
reported in (Bruce and Wiebe, 1994).
Bruce and Wiebe also performed a separate test
by using a subset of the &amp;quot;interest&amp;quot; data set with only
4 senses (sense 1, 4, 5, and 6), so as to compare their
results with previous work on WSD (Black, 1988;
Zernik, 1990; Yarowsky, 1992), which were tested
on 4 senses of the noun &amp;quot;interest&amp;quot;. However, the
work of (Black, 1988; Zernik, 1990; Yarowsky, 1992)
were not based on the present set of sentences, so
the comparison is only suggestive. We reproduced
in Table 3 the results of past work as well as the clas-
sification accuracy of LEXAS, which is 89.9% with a
standard deviation of 1.09% over 100 random trials.
In summary, when tested on the noun &amp;quot;interest&amp;quot;,
LEXAS gives higher classification accuracy than pre-
vious work on WSD.
In order to evaluate the relative contribution of
the knowledge sources, including (1) POS and mor-
</bodyText>
<page confidence="0.99938">
43
</page>
<table confidence="0.999260333333333">
WSD research Accuracy
Black (1988) 72%
Zernik (1990) 70%
Yarowsky (1992) 72%
Bruce &amp; Wiebe (1994) 79%
LEXAS (1996) 89%
</table>
<tableCaption confidence="0.999673">
Table 3: Comparison with previous results
</tableCaption>
<table confidence="0.999823">
Knowledge Source Mean Accuracy Std Dev
POS &amp; morpho 77.2% 1.44%
surrounding words 62.0% 1.82%
collocations 80.2% 1.55%
verb-object 43.5% 1.79%
</table>
<tableCaption confidence="0.966667">
Table 4: Relative Contribution of Knowledge
Sources
</tableCaption>
<bodyText confidence="0.931259962962963">
phological form; (2) unordered set of surrounding
words; (3) local collocations; and (4) verb to the left
(verb-object syntactic relation), we conducted 4 sep-
arate runs of 100 random trials each. In each run,
we utilized only one knowledge source and compute
the average classification accuracy and the standard
deviation. The results are given in Table 4.
Local collocation knowledge yields the highest ac-
curacy, followed by POS and morphological form.
Surrounding words give lower accuracy, perhaps be-
cause in our work, only the current sentence forms
the surrounding context, which averages about 20
words. Previous work on using the unordered set of
surrounding words have used a much larger window,
such as the 100-word window of (Yarowsky, 1992),
and the 2-sentence context of (Leacock et al., 1993).
Verb-object syntactic relation is the weakest knowl-
edge source.
Our experimental finding, that local collocations
are the most predictive, agrees with past observa-
tion that humans need a narrow window of only a
few words to perform WSD (Choueka and Lusignan,
1985).
The processing speed of LEXAS is satisfactory.
Running on an SGI Unix workstation, LEXAS can
process about 15 examples per second when tested
on the &amp;quot;interest&amp;quot; data set.
</bodyText>
<subsectionHeader confidence="0.997489">
4.2 Evaluation on a Large Data Set
</subsectionHeader>
<bodyText confidence="0.998801161764705">
Previous research on WSD tend to be tested only
on a dozen number of words, where each word fre-
quently has either two or a few senses. To test the
scalability of LEXAS, we have gathered a corpus in
which 192,800 word occurrences have been manually
tagged with senses from WORD NET 1.5. This data
set is almost two orders of magnitude larger in size
than the above &amp;quot;interest&amp;quot; data set. Manual tagging
was done by university undergraduates majoring in
Linguistics, and approximately one man-year of ef-
forts were expended in tagging our data set.
These 192,800 word occurrences consist of 121
nouns and 70 verbs which are the most frequently oc-
curring and most ambiguous words of English. The
121 nouns are:
action activity age air area art board
body book business car case center cen-
tury change child church city class college
community company condition cost coun-
try course day death development differ-
ence door effect effort end example experi-
ence face fact family field figure foot force
form girl government ground head history
home hour house information interest job
land law level life light line man mate-
rial matter member mind moment money
month name nation need number order
part party picture place plan point pol-
icy position power pressure problem pro-
cess program public purpose question rea-
son result right room school section sense
service side society stage state step student
study surface system table term thing time
town type use value voice water way word
work world
The 70 verbs are:
add appear ask become believe bring build
call carry change come consider continue
determine develop draw expect fall give
go grow happen help hold indicate involve
keep know lead leave lie like live look lose
mean meet move need open pay raise read
receive remember require return rise run
see seem send set show sit speak stand start
stop strike take talk tell think turn wait
walk want work write
For this set of nouns and verbs, the average num-
ber of senses per noun is 7.8, while the average num-
ber of senses per verb is 12.0. We draw our sen-
tences containing the occurrences of the 191 words
listed above from the combined corpus of the 1 mil-
lion word Brown corpus and the 2.5 million word
Wall Street Journal (WSJ) corpus. For every word
in the two lists, up to 1,500 sentences each con-
taining an occurrence of the word are extracted
from the combined corpus. In all, there are about
113,000 noun occurrences and about 79,800 verb oc-
currences. This set of 121 nouns accounts for about
20% of all occurrences of nouns that one expects to
encounter in any unrestricted English text. Simi-
larly, about 20% of all verb occurrences in any unre-
stricted text come from the set of 70 verbs chosen.
We estimate that there are 10-20% errors in our
sense-tagged data set. To get an idea of how the
sense assignments of our data set compare with
those provided by WORD NET linguists in SEMC OR,
the sense-tagged subset of Brown corpus prepared
by Miller et al. (Miller et al., 1994), we compare
</bodyText>
<page confidence="0.996795">
44
</page>
<table confidence="0.998796666666667">
Test set Sense 1 Most Frequent LEXAS
BC50 40.5% 47.1% 54.0%
WSJ6 44.8% 63.7% 68.6%
</table>
<tableCaption confidence="0.999871">
Table 5: Evaluation on a Large Data Set
</tableCaption>
<bodyText confidence="0.999427203703704">
a subset of the occurrences that overlap. Out of
5,317 occurrences that overlap, about 57% of the
sense assignments in our data set agree with those
in SEMCOR. This should not be too surprising, as
it is widely believed that sense tagging using the
full set of refined senses found in a large dictionary
like WORDNET involve making subtle human judg-
ments (Wilks et al., 1990; Bruce and Wiebe, 1994),
such that there are many genuine cases where two
humans will not agree fully on the best sense assign-
ments.
We evaluated LEXAS on this larger set of noisy,
sense-tagged data. We first set aside two subsets for
testing. The first test set, named BC50, consists of
7,119 occurrences of the 191 content words that oc-
cur in 50 text files of the Brown corpus. The second
test set, named WSJ6, consists of 14,139 occurrences
of the 191 content words that occur in 6 text files of
the WSJ corpus.
We compared the classification accuracy of LEXAS
against the default strategy of picking the most fre-
quent sense. This default strategy has been advo-
cated as the baseline performance level for compar-
ison with WSD programs (Gale et al., 1992). There
are two instantiations of this strategy in our current
evaluation. Since WORDNET orders its senses such
that sense 1 is the most frequent sense, one pos-
sibility is to always pick sense 1 as the best sense
assignment. This assignment method does not even
need to look at the training sentences. We call this
method &amp;quot;Sense 1&amp;quot; in Table 5. Another assignment
method is to determine the most frequently occur-
ring sense in the training sentences, and to assign
this sense to all test sentences. We call this method
&amp;quot;Most Frequent&amp;quot; in Table 5. The accuracy of LEXAS
on these two test sets is given in Table 5.
Our results indicate that exemplar-based classi-
fication of word senses scales up quite well when
tested on a large set of words. The classification
accuracy of LEXAS is always better than the default
strategy of picking the most frequent sense. We be-
lieve that our result is significant, especially when
the training data is noisy, and the words are highly
ambiguous with a large number of refined sense dis-
tinctions per word.
The accuracy on Brown corpus test files is lower
than that achieved on the Wall Street Journal test
files, primarily because the Brown corpus consists
of texts from a wide variety of genres, including
newspaper reports, newspaper editorial, biblical pas-
sages, science and mathematics articles, general fic-
tion, romance story, humor, etc. It is harder to dis-
ambiguate words coming from such a wide variety of
texts.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999969111111111">
There is now a large body of past work on WSD.
Early work on WSD, such as (Kelly and Stone, 1975;
Hirst, 1987) used hand-coding of knowledge to per-
form WSD. The knowledge acquisition process is la-
borious. In contrast, LEXAS learns from tagged sen-
tences, without human engineering of complex rules.
The recent emphasis on corpus based NLP has re-
sulted in much work on WSD of unconstrained real-
world texts. One line of research focuses on the use
of the knowledge contained in a machine-readable
dictionary to perform WSD, such as (Wilks et al.,
1990; Luk, 1995). In contrast, LEXAS uses super-
vised learning from tagged sentences, which is also
the approach taken by most recent work on WSD, in-
cluding (Bruce and Wiebe, 1994; Miller et al., 1994;
Leacock et al., 1993; Yarowsky, 1994; Yarowsky,
1993; Yarowsky, 1992).
The work of (Miller et al., 1994; Leacock et al.,
1993; Yarowsky, 1992) used only the unordered set of
surrounding words to perform WSD, and they used
statistical classifiers, neural networks, or IR-based
techniques. The work of (Bruce and Wiebe, 1994)
used parts of speech (POS) and morphological form,
in addition to surrounding words. However, the POS
used are abbreviated POS, and only in a window of
±2 words. No local collocation knowledge is used. A
probabilistic classifier is used in (Bruce and Wiebe,
1994).
That local collocation knowledge provides impor-
tant clues to WSD is pointed out in (Yarowsky,
1993), although it was demonstrated only on per-
forming binary (or very coarse) sense disambigua-
tion. The work of (Yarowsky, 1994) is perhaps the
most similar to our present work. However, his work
used decision list to perform classification, in which
only the single best disambiguating evidence that
matched a target context is used. In contrast, we
used exemplar-based learning, where the contribu-
tions of all features are summed up and taken into
account in coming up with a classification. We also
include verb-object syntactic relation as a feature,
which is not used in (Yarowsky, 1994). Although the
work of (Yarowsky, 1994) can be applied to WSD,
the results reported in (Yarowsky, 1994) only dealt
with accent restoration, which is a much simpler
problem. It is unclear how Yarowsky&apos;s method will
fare on WSD of a common test data set like the one
we used, nor has his method been tested on a large
data set with highly ambiguous words tagged with
the refined senses of WORDNET.
The work of (Miller et al., 1994) is the only prior
work we know of which attempted to evaluate WSD
on a large data set and using the refined sense dis-
tinction of WORDNET. However, their results show
</bodyText>
<page confidence="0.997609">
45
</page>
<bodyText confidence="0.9999633">
no improvement (in fact a slight degradation in per-
formance) when using surrounding words to perform
WSD as compared to the most frequent heuristic.
They attributed this to insufficient training data in
SEMCOR. In contrast, we adopt a different strategy
of collecting the training data set. Instead of tagging
every word in a running text, as is done in SEMCOR,
we only concentrate on the set of 191 most frequently
occurring and most ambiguous words, and collected
large enough training data for these words only. This
strategy yields better results, as indicated by a bet-
ter performance of LEXAS compared with the most
frequent heuristic on this set of words.
Most recently, Yarowsky used an unsupervised
learning procedure to perform WSD (Yarowsky,
1995), although this is only tested on disambiguat-
ing words into binary, coarse sense distinction. The
effectiveness of unsupervised learning on disam-
biguating words into the refined sense distinction of
WORDNET needs to be further investigated. The
work of (McRoy, 1992) pointed out that a diverse
set of knowledge sources are important to achieve
WSD, but no quantitative evaluation was given on
the relative importance of each knowledge source.
No previous work has reported any such evaluation
either. The work of (Cardie, 1993) used a case-based
approach that simultaneously learns part of speech,
word sense, and concept activation knowledge, al-
though the method is only tested on domain-specific
texts with domain-specific word senses.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999901">
In this paper, we have presented a new approach for
WSD using an exemplar based learning algorithm.
This approach integrates a diverse set of knowledge
sources to disambiguate word sense. When tested on
a common data set, our WSD program gives higher
classification accuracy than previous work on WSD.
When tested on a large, separately collected data
set, our program performs better than the default
strategy of picking the most frequent sense. To our
knowledge, this is the first time that a WSD program
has been tested on such a large scale, and yielding
results better than the most frequent heuristic on
highly ambiguous words with the refined senses of
WORDNET.
</bodyText>
<sectionHeader confidence="0.999076" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999867666666667">
We would like to thank: Dr Paul Wu for sharing
the Brown Corpus and Wall Street Journal Corpus;
Dr Christopher Ting for downloading and installing
WORDNET and SEMCOR, and for reformatting the
corpora; the 12 undergraduates from the Linguis-
tics Program of the National University of Singa-
pore for preparing the sense-tagged corpus; and Prof
K. P. Mohanan for his support of the sense-tagging
project.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776230769231">
Ezra Black. 1988. An experiment in computational
discrimination of English word senses. IBM Jour-
nal of Research and Development, 32(2):185-194.
Eric Brill. 1992. A simple rule-based part of speech
tagger. In Proceedings of the Third Conference on
Applied Natural Language Processing, pages 152-
155.
Rebecca Bruce and Janyce Wiebe. 1994. Word-
sense disambiguation using decomposable mod-
els. In Proceedings of the 32nd Annual Meeting
of the Association for Computational Linguistics,
Las Cruces, New Mexico.
Claire Cardie. 1993. A case-based approach to
knowledge acquisition for domain-specific sen-
tence analysis. In Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
798-803, Washington, DC.
Y. Choueka and S. Lusignan. 1985. Disambiguation
by short contexts. Computers and the Humani-
ties, 19:147-157.
Scott Cost and Steven Salzberg. 1993. A weighted
nearest neighbor algorithm for learning with sym-
bolic features. Machine Learning, 10(1):57-78.
Doug Cutting, Julian Kupiec, Jan Pedersen, and
Penelope Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the Third Conference on
Applied Natural Language Processing, pages 133-
140.
William Gale, Kenneth Ward Church, and David
Yarowsky. 1992. Estimating upper and lower
bounds on the performance of word-sense disam-
biguation programs. In Proceedings of the 30th
Annual Meeting of the Association for Computa-
tional Linguistics, Newark, Delaware.
Graeme Hirst. 1987. Semantic Interpretation and
the Resolution of Ambiguity. Cambridge Univer-
sity Press, Cambridge.
Edward Kelly and Phillip Stone. 1975. Com-
puter Recognition of English Word Senses. North-
Holland, Amsterdam.
Claudia Leacock, Geoffrey Towell, and Ellen
Voorhees. 1993. Corpus-based statistical sense
resolution. In Proceedings of the ARPA Human
Language Technology Workshop.
Alpha K. Luk. 1995. Statistical sense disambigua-
tion with relatively small corpora using dictio-
nary definitions. In Proceedings of the 33rd An-
nual Meeting of the Association for Computa-
tional Linguistics, Cambridge, Massachusetts.
Susan W. McRoy 1992. Using multiple knowledge
sources for word sense discrimination. Computa-
tional Linguistics, 18(1):1-30.
</reference>
<page confidence="0.989578">
46
</page>
<reference confidence="0.9984465">
George A. Miller, Ed. 1990. WordNet: An on-line
lexical database. International Journal of Lexi-
cography, 3(4):235-312.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994.
Using a semantic concordance for sense identifi-
cation. In Proceedings of the ARPA Human Lan-
guage Technology Workshop.
Paul Procter et al. 1978. Longman Dictionary of
Contemporary English.
John Rachlin and Steven Salzberg. 1993. PEBLS
3.0 User&apos;s Guide.
C Stanfill and David Waltz. 1986. Toward memory-
based reasoning. Communications of the ACM,
29(12):1213-1228.
Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E.
McDonald, Tony Plate, and Brian M. Slator.
1990. Providing machine tractable dictionary
tools. Machine Translation, 5(2):99-154.
David Yarowsky. 1992. Word-sense disambigua-
tion using statistical models of Roget&apos;s categories
trained on large corpora. In Proceedings of the
Fifteenth International Conference on Computa-
tional Linguistics, pages 454-460, Nantes, France.
David Yarowsky. 1993. One sense per colloca-
tion. In Proceedings of the ARPA Human Lan-
guage Technology Workshop.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restora-
tion in Spanish and French. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, Las Cruces, New Mexico.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, Cam-
bridge, Massachusetts.
Uri Zernik. 1990. Tagging word senses in corpus:
the needle in the haystack revisited. Technical
Report 90CRD198, GE R&amp;D Center.
</reference>
<page confidence="0.999485">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.402412">
<title confidence="0.995964">Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach</title>
<author confidence="0.967032">Hwee Tou Ng</author>
<affiliation confidence="0.995907">Defence Science Organisation</affiliation>
<address confidence="0.929366">20 Science Park Drive Singapore 118230</address>
<email confidence="0.889363">nhweetouOtrantor.dso.gov.sg</email>
<author confidence="0.653111">Hian Beng Lee</author>
<affiliation confidence="0.997941">Defence Science Organisation</affiliation>
<address confidence="0.938841">20 Science Park Drive Singapore 118230</address>
<email confidence="0.977241">lhianbeneltrantor.dso.gov.sg</email>
<abstract confidence="0.9982228">In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WORDNET.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ezra Black</author>
</authors>
<title>An experiment in computational discrimination of English word senses.</title>
<date>1988</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>32--2</pages>
<contexts>
<context position="18474" citStr="Black, 1988" startWordPosition="3187" endWordPosition="3188">andom selection of test sentences, the proportion of each sense in our test set is also approximately equal to their proportion in the whole data set in our random trials. The average accuracy of LEXAS over 100 random trials is 87.4%, and the standard deviation is 1.37%. In each of our 100 random trials, the accuracy of LEXAS is always higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). Bruce and Wiebe also performed a separate test by using a subset of the &amp;quot;interest&amp;quot; data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun &amp;quot;interest&amp;quot;. However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. We reproduced in Table 3 the results of past work as well as the classification accuracy of LEXAS, which is 89.9% with a standard deviation of 1.09% over 100 random trials. In summary, when tested on the noun &amp;quot;interest&amp;quot;, LEXAS gives higher classification accuracy than previous work on WSD. In order to evaluate the relative contribution of the knowledge sources</context>
</contexts>
<marker>Black, 1988</marker>
<rawString>Ezra Black. 1988. An experiment in computational discrimination of English word senses. IBM Journal of Research and Development, 32(2):185-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="4763" citStr="Brill, 1992" startWordPosition="787" endWordPosition="788">ider for a content word w are only those associated with the particular POS of w in the sentence. For instance, given the sentence &amp;quot;A reduction of principal and interest is one way the problem may be solved.&amp;quot;, since the word &amp;quot;interest&amp;quot; appears as a noun in this sentence, LEXAS will only consider the noun senses of &amp;quot;interest&amp;quot; but not its verb senses. That is, LEXAS is only concerned with disambiguating senses of a word in a given POS. Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al., 1992). In addition, sense definitions are only available for root words in a dictionary. These are words that are not morphologically inflected, such as &amp;quot;interest&amp;quot; (as opposed to the plural form &amp;quot;interests&amp;quot;), &amp;quot;fall&amp;quot; (as opposed to the other inflected forms like &amp;quot;fell&amp;quot;, &amp;quot;fallen&amp;quot;, &amp;quot;falling&amp;quot;, &amp;quot;falls&amp;quot;), etc. The sense of a morphologically inflected content word is the sense of its uninflected form. LEXAS follows this convention by first converting each word in an input sentence into its morphological root using the morphological analyzer of WORD NET, before assigning the appropri</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>Wordsense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="2250" citStr="Bruce and Wiebe, 1994" startWordPosition="346" endWordPosition="349">the appropriate target words to use in order to translate into a target language. In this paper, we present a new approach for WSD using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech (POS) of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. To evaluate our WSD program, named LEXAS (LEXical Ambiguity-resolving System), we tested it on a common data set involving the noun &amp;quot;interest&amp;quot; used by Bruce and Wiebe (Bruce and Wiebe, 1994). LEXAS achieves a mean accuracy of 87.4% on this data set, which is higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). Moreover, to test the scalability of LEXAS, we have acquired a corpus in which 192,800 word occurrences have been manually tagged with senses from WORDNET, which is a public domain lexical database containing about 95,000 word forms and 70,000 lexical concepts (Miller, 1990). These sense tagged word occurrences consist of 191 most frequently occurring and most ambiguous nouns and verbs. When tested on this large data set, LEXAS performs better than the defau</context>
<context position="15421" citStr="Bruce and Wiebe, 1994" startWordPosition="2640" endWordPosition="2643">xample is compared against all the training examples. LEXAS then determines the closest matching training example as the one with the minimum distance to the test example. The sense of w in the test example is the sense of w in this closest matching training example. If there is a tie among several training examples with the same minimum distance to the test example, LEXAS randomly selects one of these training examples as the closet matching training example in order to break the tie. 4 Evaluation To evaluate the performance of LEXAS, we conducted two tests, one on a common data set used in (Bruce and Wiebe, 1994), and another on a larger data set that we separately collected. 4.1 Evaluation on a Common Data Set To our knowledge, very few of the existing work on WSD has been tested and compared on a common data set. This is in contrast to established practice in the machine learning community. This is partly because there are not many common data sets publicly available for testing WSD programs. One exception is the sense-tagged data set used in (Bruce and Wiebe, 1994), which has been made available in the public domain by Bruce and Wiebe. This data set consists of 2369 sentences each containing an occ</context>
<context position="17227" citStr="Bruce and Wiebe, 1994" startWordPosition="2959" endWordPosition="2962">f words are given in the data set, as well as the bracketings of noun groups. These are used to determine the POS of neighboring words and the LDOCE sense Frequency Percent 1: readiness to give 361 15% attention 2: quality of causing 11 &lt;1% attention to be given 3: activity, subject, etc. 67 3% which one gives time and attention to 4: advantage, 178 8% advancement, or favor 5: a share (in a company, 499 21% business, etc.) 6: money paid for the use 1253 53% of money Table 2: Distribution of Sense Tags verb-object syntactic relation to form the features of examples. In the results reported in (Bruce and Wiebe, 1994), they used a test set of 600 randomly selected sentences from the 2369 sentences. Unfortunately, in the data set made available in the public domain, there is no indication of which sentences are used as test sentences. As such, we conducted 100 random trials, and in each trial, 600 sentences were randomly selected to form the test set. LEXAS is trained on the remaining 1769 sentences, and then tested on a separate test set of sentences in each trial. Note that in Bruce and Wiebe&apos;s test run, the proportion of sentences in each sense in the test set is approximately equal to their proportion i</context>
<context position="24255" citStr="Bruce and Wiebe, 1994" startWordPosition="4172" endWordPosition="4175">e sense-tagged subset of Brown corpus prepared by Miller et al. (Miller et al., 1994), we compare 44 Test set Sense 1 Most Frequent LEXAS BC50 40.5% 47.1% 54.0% WSJ6 44.8% 63.7% 68.6% Table 5: Evaluation on a Large Data Set a subset of the occurrences that overlap. Out of 5,317 occurrences that overlap, about 57% of the sense assignments in our data set agree with those in SEMCOR. This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al., 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will not agree fully on the best sense assignments. We evaluated LEXAS on this larger set of noisy, sense-tagged data. We first set aside two subsets for testing. The first test set, named BC50, consists of 7,119 occurrences of the 191 content words that occur in 50 text files of the Brown corpus. The second test set, named WSJ6, consists of 14,139 occurrences of the 191 content words that occur in 6 text files of the WSJ corpus. We compared the classification accuracy of LEXAS against the default strategy of picking the most frequent s</context>
<context position="27178" citStr="Bruce and Wiebe, 1994" startWordPosition="4680" endWordPosition="4683">st, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of ±2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wi</context>
<context position="19201" citStr="Bruce &amp; Wiebe (1994)" startWordPosition="3308" endWordPosition="3311"> (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. We reproduced in Table 3 the results of past work as well as the classification accuracy of LEXAS, which is 89.9% with a standard deviation of 1.09% over 100 random trials. In summary, when tested on the noun &amp;quot;interest&amp;quot;, LEXAS gives higher classification accuracy than previous work on WSD. In order to evaluate the relative contribution of the knowledge sources, including (1) POS and mor43 WSD research Accuracy Black (1988) 72% Zernik (1990) 70% Yarowsky (1992) 72% Bruce &amp; Wiebe (1994) 79% LEXAS (1996) 89% Table 3: Comparison with previous results Knowledge Source Mean Accuracy Std Dev POS &amp; morpho 77.2% 1.44% surrounding words 62.0% 1.82% collocations 80.2% 1.55% verb-object 43.5% 1.79% Table 4: Relative Contribution of Knowledge Sources phological form; (2) unordered set of surrounding words; (3) local collocations; and (4) verb to the left (verb-object syntactic relation), we conducted 4 separate runs of 100 random trials each. In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation. The results are gi</context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>Rebecca Bruce and Janyce Wiebe. 1994. Wordsense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
</authors>
<title>A case-based approach to knowledge acquisition for domain-specific sentence analysis.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence,</booktitle>
<pages>798--803</pages>
<location>Washington, DC.</location>
<contexts>
<context position="30360" citStr="Cardie, 1993" startWordPosition="5207" endWordPosition="5208">ntly, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction. The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WORDNET needs to be further investigated. The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source. No previous work has reported any such evaluation either. The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, word sense, and concept activation knowledge, although the method is only tested on domain-specific texts with domain-specific word senses. 6 Conclusion In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense. When tested on a common data set, our WSD program gives higher classification accuracy than previous work on WSD. When tested on a large, separately collected data set, our program perform</context>
</contexts>
<marker>Cardie, 1993</marker>
<rawString>Claire Cardie. 1993. A case-based approach to knowledge acquisition for domain-specific sentence analysis. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 798-803, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choueka</author>
<author>S Lusignan</author>
</authors>
<title>Disambiguation by short contexts.</title>
<date>1985</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>19--147</pages>
<contexts>
<context position="20531" citStr="Choueka and Lusignan, 1985" startWordPosition="3516" endWordPosition="3519">cal form. Surrounding words give lower accuracy, perhaps because in our work, only the current sentence forms the surrounding context, which averages about 20 words. Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). Verb-object syntactic relation is the weakest knowledge source. Our experimental finding, that local collocations are the most predictive, agrees with past observation that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). The processing speed of LEXAS is satisfactory. Running on an SGI Unix workstation, LEXAS can process about 15 examples per second when tested on the &amp;quot;interest&amp;quot; data set. 4.2 Evaluation on a Large Data Set Previous research on WSD tend to be tested only on a dozen number of words, where each word frequently has either two or a few senses. To test the scalability of LEXAS, we have gathered a corpus in which 192,800 word occurrences have been manually tagged with senses from WORD NET 1.5. This data set is almost two orders of magnitude larger in size than the above &amp;quot;interest&amp;quot; data set. Manual t</context>
</contexts>
<marker>Choueka, Lusignan, 1985</marker>
<rawString>Y. Choueka and S. Lusignan. 1985. Disambiguation by short contexts. Computers and the Humanities, 19:147-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Cost</author>
<author>Steven Salzberg</author>
</authors>
<title>A weighted nearest neighbor algorithm for learning with symbolic features.</title>
<date>1993</date>
<booktitle>Machine Learning,</booktitle>
<pages>10--1</pages>
<contexts>
<context position="5761" citStr="Cost and Salzberg, 1993" startWordPosition="943" endWordPosition="946">rd is the sense of its uninflected form. LEXAS follows this convention by first converting each word in an input sentence into its morphological root using the morphological analyzer of WORD NET, before assigning the appropriate word sense to the root form. 3 Algorithm LEXAS performs WSD by first learning from a training corpus of sentences in which words have been pre-tagged with their correct senses. That is, it uses supervised learning, in particular exemplar-based learning, to achieve WSD. Our approach has been fully implemented in the program LEXAS. Part of the implementation uses PEBLS (Cost and Salzberg, 1993; Rachlin and Salzberg, 1993), a public domain exemplar-based learning system. LEXAS builds one exemplar-based classifier for each content word w. It operates in two phases: training phase and test phase. In the training phase, LEXAS is given a set S of sentences in the training corpus in which sense-tagged occurrences of w appear. For each training sentence with an occurrence of w, LEXAS extracts the parts of speech (POS) of words surrounding w, the morphological form of w, the words that frequently co-occur with w in the same sentence, and the local collocations containing w. For disambiguat</context>
<context position="14290" citStr="Cost and Salzberg, 1993" startWordPosition="2439" endWordPosition="2442">en two examples. If the distance between two examples is small, then the two examples are similar. We use the following definition of distance between two symbolic values Vi and v2 of a feature f: ni C2 i = d(Vi, V2) Cl C2 C1,i is the number of training examples with value v1 for feature f that is classified as sense i in the training corpus, and C1 is the number of training examples with value v1 for feature f in any sense. C2,i and C2 denote similar quantities for value v2 of i=1 42 feature f. 7/ is the total number of senses for a word w. This metric for measuring distance is adopted from (Cost and Salzberg, 1993), which in turn is adapted from the value difference metric of the earlier work of (Stanfill and Waltz, 1986). The distance between two examples is the sum of the distances between the values of all the features of the two examples. During the training phase, the appropriate set of features is extracted based on the method described in Section 3.1. From the training examples formed, the distance between any two values for a feature f is computed based on the above formula. During the test phase, a test example is compared against all the training examples. LEXAS then determines the closest mat</context>
</contexts>
<marker>Cost, Salzberg, 1993</marker>
<rawString>Scott Cost and Steven Salzberg. 1993. A weighted nearest neighbor algorithm for learning with symbolic features. Machine Learning, 10(1):57-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="4786" citStr="Cutting et al., 1992" startWordPosition="789" endWordPosition="792">ntent word w are only those associated with the particular POS of w in the sentence. For instance, given the sentence &amp;quot;A reduction of principal and interest is one way the problem may be solved.&amp;quot;, since the word &amp;quot;interest&amp;quot; appears as a noun in this sentence, LEXAS will only consider the noun senses of &amp;quot;interest&amp;quot; but not its verb senses. That is, LEXAS is only concerned with disambiguating senses of a word in a given POS. Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al., 1992). In addition, sense definitions are only available for root words in a dictionary. These are words that are not morphologically inflected, such as &amp;quot;interest&amp;quot; (as opposed to the plural form &amp;quot;interests&amp;quot;), &amp;quot;fall&amp;quot; (as opposed to the other inflected forms like &amp;quot;fell&amp;quot;, &amp;quot;fallen&amp;quot;, &amp;quot;falling&amp;quot;, &amp;quot;falls&amp;quot;), etc. The sense of a morphologically inflected content word is the sense of its uninflected form. LEXAS follows this convention by first converting each word in an input sentence into its morphological root using the morphological analyzer of WORD NET, before assigning the appropriate word sense to the r</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Ward Church</author>
<author>David Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Newark, Delaware.</location>
<contexts>
<context position="24988" citStr="Gale et al., 1992" startWordPosition="4302" endWordPosition="4305">aluated LEXAS on this larger set of noisy, sense-tagged data. We first set aside two subsets for testing. The first test set, named BC50, consists of 7,119 occurrences of the 191 content words that occur in 50 text files of the Brown corpus. The second test set, named WSJ6, consists of 14,139 occurrences of the 191 content words that occur in 6 text files of the WSJ corpus. We compared the classification accuracy of LEXAS against the default strategy of picking the most frequent sense. This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al., 1992). There are two instantiations of this strategy in our current evaluation. Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. This assignment method does not even need to look at the training sentences. We call this method &amp;quot;Sense 1&amp;quot; in Table 5. Another assignment method is to determine the most frequently occurring sense in the training sentences, and to assign this sense to all test sentences. We call this method &amp;quot;Most Frequent&amp;quot; in Table 5. The accuracy of LEXAS on these two test sets is given i</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Ward Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="26566" citStr="Hirst, 1987" startWordPosition="4578" endWordPosition="4579">y ambiguous with a large number of refined sense distinctions per word. The accuracy on Brown corpus test files is lower than that achieved on the Wall Street Journal test files, primarily because the Brown corpus consists of texts from a wide variety of genres, including newspaper reports, newspaper editorial, biblical passages, science and mathematics articles, general fiction, romance story, humor, etc. It is harder to disambiguate words coming from such a wide variety of texts. 5 Related Work There is now a large body of past work on WSD. Early work on WSD, such as (Kelly and Stone, 1975; Hirst, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Graeme Hirst. 1987. Semantic Interpretation and the Resolution of Ambiguity. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Kelly</author>
<author>Phillip Stone</author>
</authors>
<title>Computer Recognition of English Word Senses.</title>
<date>1975</date>
<publisher>NorthHolland,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="26552" citStr="Kelly and Stone, 1975" startWordPosition="4574" endWordPosition="4577">and the words are highly ambiguous with a large number of refined sense distinctions per word. The accuracy on Brown corpus test files is lower than that achieved on the Wall Street Journal test files, primarily because the Brown corpus consists of texts from a wide variety of genres, including newspaper reports, newspaper editorial, biblical passages, science and mathematics articles, general fiction, romance story, humor, etc. It is harder to disambiguate words coming from such a wide variety of texts. 5 Related Work There is now a large body of past work on WSD. Early work on WSD, such as (Kelly and Stone, 1975; Hirst, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, includ</context>
</contexts>
<marker>Kelly, Stone, 1975</marker>
<rawString>Edward Kelly and Phillip Stone. 1975. Computer Recognition of English Word Senses. NorthHolland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Geoffrey Towell</author>
<author>Ellen Voorhees</author>
</authors>
<title>Corpus-based statistical sense resolution.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="20265" citStr="Leacock et al., 1993" startWordPosition="3474" endWordPosition="3477">rials each. In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation. The results are given in Table 4. Local collocation knowledge yields the highest accuracy, followed by POS and morphological form. Surrounding words give lower accuracy, perhaps because in our work, only the current sentence forms the surrounding context, which averages about 20 words. Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). Verb-object syntactic relation is the weakest knowledge source. Our experimental finding, that local collocations are the most predictive, agrees with past observation that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). The processing speed of LEXAS is satisfactory. Running on an SGI Unix workstation, LEXAS can process about 15 examples per second when tested on the &amp;quot;interest&amp;quot; data set. 4.2 Evaluation on a Large Data Set Previous research on WSD tend to be tested only on a dozen number of words, where each word frequently has either two or a few </context>
<context position="27221" citStr="Leacock et al., 1993" startWordPosition="4688" endWordPosition="4691">erform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of ±2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wiebe, 1994). That local collocation knowledg</context>
</contexts>
<marker>Leacock, Towell, Voorhees, 1993</marker>
<rawString>Claudia Leacock, Geoffrey Towell, and Ellen Voorhees. 1993. Corpus-based statistical sense resolution. In Proceedings of the ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alpha K Luk</author>
</authors>
<title>Statistical sense disambiguation with relatively small corpora using dictionary definitions.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="27016" citStr="Luk, 1995" startWordPosition="4654" endWordPosition="4655">from such a wide variety of texts. 5 Related Work There is now a large body of past work on WSD. Early work on WSD, such as (Kelly and Stone, 1975; Hirst, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. Howeve</context>
</contexts>
<marker>Luk, 1995</marker>
<rawString>Alpha K. Luk. 1995. Statistical sense disambiguation with relatively small corpora using dictionary definitions. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
</authors>
<title>Using multiple knowledge sources for word sense discrimination.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--1</pages>
<contexts>
<context position="30099" citStr="McRoy, 1992" startWordPosition="5165" endWordPosition="5166">ntly occurring and most ambiguous words, and collected large enough training data for these words only. This strategy yields better results, as indicated by a better performance of LEXAS compared with the most frequent heuristic on this set of words. Most recently, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction. The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WORDNET needs to be further investigated. The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source. No previous work has reported any such evaluation either. The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, word sense, and concept activation knowledge, although the method is only tested on domain-specific texts with domain-specific word senses. 6 Conclusion In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm. This approach i</context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>Susan W. McRoy 1992. Using multiple knowledge sources for word sense discrimination. Computational Linguistics, 18(1):1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Ed</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<marker>Miller, Ed, 1990</marker>
<rawString>George A. Miller, Ed. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Martin Chodorow</author>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Robert G Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="23718" citStr="Miller et al., 1994" startWordPosition="4076" endWordPosition="4079">ed corpus. In all, there are about 113,000 noun occurrences and about 79,800 verb occurrences. This set of 121 nouns accounts for about 20% of all occurrences of nouns that one expects to encounter in any unrestricted English text. Similarly, about 20% of all verb occurrences in any unrestricted text come from the set of 70 verbs chosen. We estimate that there are 10-20% errors in our sense-tagged data set. To get an idea of how the sense assignments of our data set compare with those provided by WORD NET linguists in SEMC OR, the sense-tagged subset of Brown corpus prepared by Miller et al. (Miller et al., 1994), we compare 44 Test set Sense 1 Most Frequent LEXAS BC50 40.5% 47.1% 54.0% WSJ6 44.8% 63.7% 68.6% Table 5: Evaluation on a Large Data Set a subset of the occurrences that overlap. Out of 5,317 occurrences that overlap, about 57% of the sense assignments in our data set agree with those in SEMCOR. This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al., 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will </context>
<context position="27199" citStr="Miller et al., 1994" startWordPosition="4684" endWordPosition="4687">ing of knowledge to perform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of ±2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wiebe, 1994). That loca</context>
<context position="28906" citStr="Miller et al., 1994" startWordPosition="4968" endWordPosition="4971">ons of all features are summed up and taken into account in coming up with a classification. We also include verb-object syntactic relation as a feature, which is not used in (Yarowsky, 1994). Although the work of (Yarowsky, 1994) can be applied to WSD, the results reported in (Yarowsky, 1994) only dealt with accent restoration, which is a much simpler problem. It is unclear how Yarowsky&apos;s method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET. The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET. However, their results show 45 no improvement (in fact a slight degradation in performance) when using surrounding words to perform WSD as compared to the most frequent heuristic. They attributed this to insufficient training data in SEMCOR. In contrast, we adopt a different strategy of collecting the training data set. Instead of tagging every word in a running text, as is done in SEMCOR, we only concentrate on the set of 191 most frequently occurring and </context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In Proceedings of the ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Procter</author>
</authors>
<date>1978</date>
<journal>Longman Dictionary of Contemporary English.</journal>
<contexts>
<context position="16410" citStr="Procter, 1978" startWordPosition="2815" endWordPosition="2816"> WSD programs. One exception is the sense-tagged data set used in (Bruce and Wiebe, 1994), which has been made available in the public domain by Bruce and Wiebe. This data set consists of 2369 sentences each containing an occurrence of the noun &amp;quot;interest&amp;quot; (or its plural form &amp;quot;interests&amp;quot;) with its correct sense manually tagged. The noun &amp;quot;interest&amp;quot; occurs in six different senses in this data set. Table 2 shows the distribution of sense tags from the data set that we obtained. Note that the sense definitions used in this data set are those from Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978). This does not pose any problem for LEXAS, since LEXAS only requires that there be a division of senses into different classes, regardless of how the sense classes are defined or numbered. POS of words are given in the data set, as well as the bracketings of noun groups. These are used to determine the POS of neighboring words and the LDOCE sense Frequency Percent 1: readiness to give 361 15% attention 2: quality of causing 11 &lt;1% attention to be given 3: activity, subject, etc. 67 3% which one gives time and attention to 4: advantage, 178 8% advancement, or favor 5: a share (in a company, 49</context>
</contexts>
<marker>Procter, 1978</marker>
<rawString>Paul Procter et al. 1978. Longman Dictionary of Contemporary English.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rachlin</author>
<author>Steven Salzberg</author>
</authors>
<date>1993</date>
<note>PEBLS 3.0 User&apos;s Guide.</note>
<contexts>
<context position="5790" citStr="Rachlin and Salzberg, 1993" startWordPosition="947" endWordPosition="950">inflected form. LEXAS follows this convention by first converting each word in an input sentence into its morphological root using the morphological analyzer of WORD NET, before assigning the appropriate word sense to the root form. 3 Algorithm LEXAS performs WSD by first learning from a training corpus of sentences in which words have been pre-tagged with their correct senses. That is, it uses supervised learning, in particular exemplar-based learning, to achieve WSD. Our approach has been fully implemented in the program LEXAS. Part of the implementation uses PEBLS (Cost and Salzberg, 1993; Rachlin and Salzberg, 1993), a public domain exemplar-based learning system. LEXAS builds one exemplar-based classifier for each content word w. It operates in two phases: training phase and test phase. In the training phase, LEXAS is given a set S of sentences in the training corpus in which sense-tagged occurrences of w appear. For each training sentence with an occurrence of w, LEXAS extracts the parts of speech (POS) of words surrounding w, the morphological form of w, the words that frequently co-occur with w in the same sentence, and the local collocations containing w. For disambiguating a noun w, the verb which </context>
</contexts>
<marker>Rachlin, Salzberg, 1993</marker>
<rawString>John Rachlin and Steven Salzberg. 1993. PEBLS 3.0 User&apos;s Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stanfill</author>
<author>David Waltz</author>
</authors>
<title>Toward memorybased reasoning.</title>
<date>1986</date>
<journal>Communications of the ACM,</journal>
<pages>29--12</pages>
<contexts>
<context position="14399" citStr="Stanfill and Waltz, 1986" startWordPosition="2459" endWordPosition="2462">e following definition of distance between two symbolic values Vi and v2 of a feature f: ni C2 i = d(Vi, V2) Cl C2 C1,i is the number of training examples with value v1 for feature f that is classified as sense i in the training corpus, and C1 is the number of training examples with value v1 for feature f in any sense. C2,i and C2 denote similar quantities for value v2 of i=1 42 feature f. 7/ is the total number of senses for a word w. This metric for measuring distance is adopted from (Cost and Salzberg, 1993), which in turn is adapted from the value difference metric of the earlier work of (Stanfill and Waltz, 1986). The distance between two examples is the sum of the distances between the values of all the features of the two examples. During the training phase, the appropriate set of features is extracted based on the method described in Section 3.1. From the training examples formed, the distance between any two values for a feature f is computed based on the above formula. During the test phase, a test example is compared against all the training examples. LEXAS then determines the closest matching training example as the one with the minimum distance to the test example. The sense of w in the test e</context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>C Stanfill and David Waltz. 1986. Toward memorybased reasoning. Communications of the ACM, 29(12):1213-1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Dan Fass</author>
<author>Cheng-Ming Guo</author>
<author>James E McDonald</author>
<author>Tony Plate</author>
<author>Brian M Slator</author>
</authors>
<title>Providing machine tractable dictionary tools.</title>
<date>1990</date>
<journal>Machine Translation,</journal>
<pages>5--2</pages>
<contexts>
<context position="24231" citStr="Wilks et al., 1990" startWordPosition="4168" endWordPosition="4171">uists in SEMC OR, the sense-tagged subset of Brown corpus prepared by Miller et al. (Miller et al., 1994), we compare 44 Test set Sense 1 Most Frequent LEXAS BC50 40.5% 47.1% 54.0% WSJ6 44.8% 63.7% 68.6% Table 5: Evaluation on a Large Data Set a subset of the occurrences that overlap. Out of 5,317 occurrences that overlap, about 57% of the sense assignments in our data set agree with those in SEMCOR. This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al., 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will not agree fully on the best sense assignments. We evaluated LEXAS on this larger set of noisy, sense-tagged data. We first set aside two subsets for testing. The first test set, named BC50, consists of 7,119 occurrences of the 191 content words that occur in 50 text files of the Brown corpus. The second test set, named WSJ6, consists of 14,139 occurrences of the 191 content words that occur in 6 text files of the WSJ corpus. We compared the classification accuracy of LEXAS against the default strategy of pic</context>
<context position="27004" citStr="Wilks et al., 1990" startWordPosition="4650" endWordPosition="4653">iguate words coming from such a wide variety of texts. 5 Related Work There is now a large body of past work on WSD. Early work on WSD, such as (Kelly and Stone, 1975; Hirst, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding w</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1990</marker>
<rawString>Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E. McDonald, Tony Plate, and Brian M. Slator. 1990. Providing machine tractable dictionary tools. Machine Translation, 5(2):99-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>454--460</pages>
<location>Nantes, France.</location>
<contexts>
<context position="18505" citStr="Yarowsky, 1992" startWordPosition="3191" endWordPosition="3192">tences, the proportion of each sense in our test set is also approximately equal to their proportion in the whole data set in our random trials. The average accuracy of LEXAS over 100 random trials is 87.4%, and the standard deviation is 1.37%. In each of our 100 random trials, the accuracy of LEXAS is always higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). Bruce and Wiebe also performed a separate test by using a subset of the &amp;quot;interest&amp;quot; data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun &amp;quot;interest&amp;quot;. However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. We reproduced in Table 3 the results of past work as well as the classification accuracy of LEXAS, which is 89.9% with a standard deviation of 1.09% over 100 random trials. In summary, when tested on the noun &amp;quot;interest&amp;quot;, LEXAS gives higher classification accuracy than previous work on WSD. In order to evaluate the relative contribution of the knowledge sources, including (1) POS and mor43 W</context>
<context position="20211" citStr="Yarowsky, 1992" startWordPosition="3467" endWordPosition="3468">n), we conducted 4 separate runs of 100 random trials each. In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation. The results are given in Table 4. Local collocation knowledge yields the highest accuracy, followed by POS and morphological form. Surrounding words give lower accuracy, perhaps because in our work, only the current sentence forms the surrounding context, which averages about 20 words. Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). Verb-object syntactic relation is the weakest knowledge source. Our experimental finding, that local collocations are the most predictive, agrees with past observation that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). The processing speed of LEXAS is satisfactory. Running on an SGI Unix workstation, LEXAS can process about 15 examples per second when tested on the &amp;quot;interest&amp;quot; data set. 4.2 Evaluation on a Large Data Set Previous research on WSD tend to be tested only on a dozen number of word</context>
<context position="27270" citStr="Yarowsky, 1992" startWordPosition="4696" endWordPosition="4697">ious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of ±2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wiebe, 1994). That local collocation knowledge provides important clues to WSD is pointed out </context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 454-460, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="27253" citStr="Yarowsky, 1993" startWordPosition="4694" endWordPosition="4695">process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of ±2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wiebe, 1994). That local collocation knowledge provides important clues to WS</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proceedings of the ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="27237" citStr="Yarowsky, 1994" startWordPosition="4692" endWordPosition="4693">dge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of ±2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wiebe, 1994). That local collocation knowledge provides impor</context>
<context position="28477" citStr="Yarowsky, 1994" startWordPosition="4892" endWordPosition="4893">ted out in (Yarowsky, 1993), although it was demonstrated only on performing binary (or very coarse) sense disambiguation. The work of (Yarowsky, 1994) is perhaps the most similar to our present work. However, his work used decision list to perform classification, in which only the single best disambiguating evidence that matched a target context is used. In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification. We also include verb-object syntactic relation as a feature, which is not used in (Yarowsky, 1994). Although the work of (Yarowsky, 1994) can be applied to WSD, the results reported in (Yarowsky, 1994) only dealt with accent restoration, which is a much simpler problem. It is unclear how Yarowsky&apos;s method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET. The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET. However, their results show 45 n</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>David Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="29833" citStr="Yarowsky, 1995" startWordPosition="5125" endWordPosition="5126">istic. They attributed this to insufficient training data in SEMCOR. In contrast, we adopt a different strategy of collecting the training data set. Instead of tagging every word in a running text, as is done in SEMCOR, we only concentrate on the set of 191 most frequently occurring and most ambiguous words, and collected large enough training data for these words only. This strategy yields better results, as indicated by a better performance of LEXAS compared with the most frequent heuristic on this set of words. Most recently, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction. The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WORDNET needs to be further investigated. The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source. No previous work has reported any such evaluation either. The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, wo</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Zernik</author>
</authors>
<title>Tagging word senses in corpus: the needle in the haystack revisited.</title>
<date>1990</date>
<tech>Technical Report 90CRD198, GE R&amp;D Center.</tech>
<contexts>
<context position="18488" citStr="Zernik, 1990" startWordPosition="3189" endWordPosition="3190">on of test sentences, the proportion of each sense in our test set is also approximately equal to their proportion in the whole data set in our random trials. The average accuracy of LEXAS over 100 random trials is 87.4%, and the standard deviation is 1.37%. In each of our 100 random trials, the accuracy of LEXAS is always higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). Bruce and Wiebe also performed a separate test by using a subset of the &amp;quot;interest&amp;quot; data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun &amp;quot;interest&amp;quot;. However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. We reproduced in Table 3 the results of past work as well as the classification accuracy of LEXAS, which is 89.9% with a standard deviation of 1.09% over 100 random trials. In summary, when tested on the noun &amp;quot;interest&amp;quot;, LEXAS gives higher classification accuracy than previous work on WSD. In order to evaluate the relative contribution of the knowledge sources, including (1</context>
</contexts>
<marker>Zernik, 1990</marker>
<rawString>Uri Zernik. 1990. Tagging word senses in corpus: the needle in the haystack revisited. Technical Report 90CRD198, GE R&amp;D Center.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>