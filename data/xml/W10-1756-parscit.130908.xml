<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.998986">
A Unified Approach to Minimum Risk Training and Decoding
</title>
<author confidence="0.998318">
Abhishek Arun, Barry Haddow and Philipp Koehn
</author>
<affiliation confidence="0.9980125">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.966386">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.997333">
a.arun@sms.ed.ac.uk, {bhaddow,pkoehn}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993826" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952055555555">
We present a unified approach to perform-
ing minimum risk training and minimum
Bayes risk (MBR) decoding with BLEU
in a phrase-based model. Key to our ap-
proach is the use of a Gibbs sampler that
allows us to explore the entire probabil-
ity distribution and maintain a strict prob-
abilistic formulation across the pipeline.
We also describe a new sampling algo-
rithm called corpus sampling which al-
lows us at training time to use BLEU in-
stead of an approximation thereof. Our
approach is theoretically sound and gives
better (up to +0.6%BLEU) and more sta-
ble results than the standard MERT opti-
mization algorithm. By comparing our ap-
proach to lattice MBR, we are also able to
gain crucial insights about both methods.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999470133333333">
According to statistical decision theory, the opti-
mal decision rule for any statistical model is the
solution that minimizes its risk (expected loss).
This solution is often referred to as the Minimum
Bayes Risk (MBR) solution (Kumar and Byrne,
2004). Since machine translation (MT) mod-
els are typically evaluated by BLEU (Papineni et
al., 2002), a loss function which rewards partial
matches, the MBR solution is to be preferred to
the Maximum A Posteriori (MAP) solution.
In most statistical MT (SMT) systems, MBR
is implemented as a reranker of a list1 of trans-
lations generated by a first-pass decoder. This de-
coder typically assigns unnormalised log probabil-
ities (known as scores) to each translation hypoth-
</bodyText>
<footnote confidence="0.702541">
1We use the term list to denote any enumerable represen-
tation of translation hypotheses e.g n-best list, translation lat-
tice or forest.
</footnote>
<bodyText confidence="0.999555">
esis, so these scores must be converted to proba-
bilities in order to apply MBR. In order to perform
this conversion, it is first necessary to compute the
normalization function Z. Since Z is defined as
an intractable sum over all possible translations, it
is approximated by summing over the translations
in the list. The second step is to find the correct
scale factor for the scores using a hyper-parameter
search over held-out data. This is needed because
the model parameters for the first-pass decoder are
normally learnt using MERT (Och, 2003), which
is invariant under scaling of the scores.
Both these steps are theoretically unsatisfactory
methods of estimating the posterior probability
distribution since the approximation to Z is an un-
bounded term and the scaling factor is an artificial
way of inducing a probability distribution.
Recently, (Tromble et al., 2008; Kumar et al.,
2009) have shown that using a search lattice to im-
prove the estimation of the true probability distri-
bution can lead to improved MBR performance.
However, these approaches still rely on MERT for
training the base model, and in fact introduce sev-
eral extra parameters which must also be estimated
using either grid search or a second MERT run.
The lattice pruning required to make these tech-
niques tractable is quite drastic, and is in addi-
tion to the pruning already performed during the
search. Such extensive pruning is liable to render
any probability estimates heavily biased (Blunsom
and Osborne, 2008; Bouchard-Cˆot´e et al., 2009).
Here, we present a unified approach to training
and decoding in a phrase-based translation model
(Koehn et al., 2003) which keeps the objective
constant across the translation pipeline and so ob-
viates the need for any extra hyper-parameter fit-
ting. We use the phrase-based Gibbs sampler of
Arun et al. (2009) at training time to compute the
gradient of our minimum risk training objective in
order to apply first-order optimization techniques,
</bodyText>
<page confidence="0.985178">
365
</page>
<note confidence="0.9618395">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365–374,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999526428571429">
and at test time we use it to estimate the posterior
distribution required by MBR (Section 3).
We experimented with two different objective
functions for training (Section 4). First, follow-
ing (Arun et al., 2009), we define our objective
at the sentence-level using a sentence-level variant
of BLEU. Then, in order to reduce the mismatch
between training and test loss functions, we also
tried directly optimising the expected corpus level
BLEU, where we introduce a novel sampling tech-
nique, which we call corpus sampling to calculate
the required expectations.
The methods presented in this paper are theo-
retically sound. Moreover, experimental evidence
on three language pairs shows that our training
regime is more stable than MERT, able to gener-
alize better and generally leads to improvement in
translation when used with sampling based MBR
(Section 5). An added benefit is that the trained
weights also lead to better performance when used
with a beam-search based decoder.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="introduction">
2 Inference methods for MT
</sectionHeader>
<bodyText confidence="0.9999835">
We assume a phrase-based machine translation
model, defined with a log-linear form, with feature
function vector h and parametrized by weight vec-
tor 0, as described in Koehn et al. (2003). The in-
put sentence, f, is segmented into phrases, which
are sequences of adjacent words. Each source
phrase is translated into the target language, to
produce an output sentence e and an alignment
a representing the mapping from source to target
phrases. Phrases are allowed to be reordered.
</bodyText>
<equation confidence="0.581412666666667">
p(e, a |f; 0) =
exp [0 - h(e, a, f)] (1)
E(e, a,) exp [0 - h(e&apos;, a&apos;, f)]
</equation>
<bodyText confidence="0.9906215">
MAP decoding under this model consists of
finding the most likely output string, e*:
</bodyText>
<equation confidence="0.9934335">
�e* = argmaxe p(e, a|f) (2)
aEA(e,f)
</equation>
<bodyText confidence="0.999979">
where A(e, f) is the set of all derivations of output
string e given source string f.
Summing over all the derivations is intractable,
making approximations necessary. The most com-
mon of these approximations is the Viterbi approx-
imation, which simply chooses the most likely
derivation (e*, a*). This approximation can be
computed in polynomial time via dynamic pro-
gramming (DP). Though fast and effective for
many problems, it has two serious drawbacks for
probabilistic inference. First, the error incurred
by the Viterbi maximum with respect to the true
model maximum is unbounded. Second, the DP
solution requires substantial pruning and restricts
the use of non-local features. The latter problem
persists even in the variational approximations of
Li et al. (2009), which attempt to solve the former.
</bodyText>
<subsectionHeader confidence="0.997059">
2.1 Gibbs sampling for phrase-based MT
</subsectionHeader>
<bodyText confidence="0.999477583333333">
An alternate approximate inference method for
phrase-based MT without any of the previously
mentioned drawbacks is the Gibbs sampler (Ge-
man and Geman, 1984) of Arun et al. (2009)
which draws samples from the posterior distribu-
tion of the translation model. For the work pre-
sented in this paper, we use this sampler.
The sampler produces a sequence of samples,
SN1 = (e1, a1) ... (eN, aN), that are drawn from
the distribution p(e, a|f). These samples can be
used to estimate the expectation of a function
h(e, a, f) as follows:
</bodyText>
<equation confidence="0.9973394">
N
1
L&apos;p(a,e|f)[h] = ��m
N�� N
i=1
</equation>
<sectionHeader confidence="0.991809" genericHeader="method">
3 Decoding
</sectionHeader>
<bodyText confidence="0.999526333333333">
In this work, we are interested in performing MBR
decoding with BLEU. We define the MBR decision
rule following Tromble et al. (2008):
</bodyText>
<equation confidence="0.999007666666667">
e* = arg max
eEex e&apos;EEE
� BLEUe(e&apos;)p(e&apos;|f) (4)
</equation>
<bodyText confidence="0.9997577">
where EH refers to the hypothesis space from
which translations are chosen, EE refers to the
evidence space used for calculating risk and
BLEUe(e&apos;) is a gain function that indicates the re-
ward of hypothesising e&apos; when the reference solu-
tion is e.
To perform MBR decoding using the sampler,
let the function h in Equation 3 be the indica-
tor function h = δ(a, 6)δ(e, e). Then, Equa-
tion 3 provides an estimate of p(a, 6|f), and using
h = δ(e, e) marginalizes over all derivations a&apos;,
yielding an estimate of p(e|f). MBR is computed
at the sentence-level while BLEU is a corpus-level
metric, so instead we use a sentence-level approx-
imation of BLEU.2
The sampler can be used to perform two other
decoding tasks: the mode of the estimated dis-
tribution p(a, e|f) is the maximum derivation
(MaxDeriv) solution while the mode of p(6|f) is
the maximum translation (MaxTrans) solution.
</bodyText>
<footnote confidence="0.7707575">
2The ngram precision counts are smoothed by adding 0.01
for n &gt; 1
</footnote>
<equation confidence="0.980818">
h(ai, ei, f) (3)
</equation>
<page confidence="0.99854">
366
</page>
<sectionHeader confidence="0.994451" genericHeader="method">
4 Minimum Risk Training
</sectionHeader>
<bodyText confidence="0.9999777">
In order to train models suitable for use with Max-
Trans or MBR decoding, we need to employ a
training method which takes account of the whole
distribution. To this end, we employ minimum risk
training to find weights θ for Equation 1 that mini-
mize the expected loss on the training set. We con-
sider two variants of minimum risk training: sen-
tence sampling optimizes an objective defined at
the sentence level and corpus sampling a corpus-
based objective.
</bodyText>
<subsectionHeader confidence="0.999684">
4.1 Sentence sampling
</subsectionHeader>
<bodyText confidence="0.998728">
Since BLEU, the metric we care about, is a gain
function, our objective function maximizes the ex-
pected gain of our model. The expected gain, !g
of a probabilistic translation model on a corpus D,
defined with respect to the gain function BLEUe(e)
is given by �
</bodyText>
<equation confidence="0.9918855">
!g =
he,fi∈D
</equation>
<bodyText confidence="0.99973875">
where e� is the reference translation, e is a hypoth-
esis translation and BLEU refers to the sentence-
level approximation of the metric.
Using the probabilistic formulation of Equation
1, the optimization of the objective in (5) is facil-
itated by the fact that it is continuous and differ-
entiable with respect to the model parameters θ to
give
</bodyText>
<equation confidence="0.981978">
(6)
= (hk − F-p(e,a|f)[hk]) p(e, a|f)
</equation>
<bodyText confidence="0.999763416666667">
Since the gradient is expressed in terms of ex-
pectations of feature values, it can easily be calcu-
lated using the sampler and then first-order opti-
mization techniques can be applied to find optimal
values of θ. Because of the noise introduced by
the sampler, we used stochastic gradient descent
(SGD), with a learning rate that gets updated after
each step proportionally to difference in succes-
sive gradients (Schraudolph, 1999).
While our initial formulation of minimum risk
training is similar to that of Arun et al. (2009), in
preliminary experiments we observed a tendency
for translation performance on held-out data to
quickly increase to a maximum and then plateau.
Hypothesizing that we were being trapped in lo-
cal maxima as !g is non-convex, we decided to
employ deterministic annealing (Rose, 1998) to
smooth the objective function to ensure that the
optimizer explored as large a region as possible of
the space before it settled on an optimal weight set.
Our instantiation of deterministic annealing (DA)
is based on the work of Smith and Eisner (2006),
and involves the addition of an entropic prior to
the objective in Equation 5 to give
</bodyText>
<equation confidence="0.9992056">
�0=J: p(e,a|f)BLEU6(e) + T.H(p)
f)E
e D ea
,
= (hk − F-p(e,a|f)[hk])p(e,a|f)
</equation>
<bodyText confidence="0.9999935">
A high value of T leads the optimizer to find
weights which describe a fairly flat distribution,
whereas a lower value of T pushes the optimizer
towards a more peaked distribution. We perform
10 to 20 iterations of SGD at each temperature.
In their deterministic annealing formulation,
(Smith and Eisner, 2006; Li and Eisner, 2009), ex-
press the parameterization of the distribution 0 as
γ0� (where γ is the scaling factor) and perform op-
timization in two steps, the first optimizing 0 and
the second optimizing γ. We experimented with
this two stage optimization process, but found that
simply performing an unconstrained optimization
on 0 gave better results.
</bodyText>
<subsectionHeader confidence="0.999314">
4.2 Corpus sampling
</subsectionHeader>
<bodyText confidence="0.999946583333333">
While the objective functions in Equations 5 and
4.1 use a sentence-level variant of BLEU, the
model’s test-time performance is evaluated with
corpus level BLEU. The lack of correlation be-
tween sentence-level BLEU and corpus BLEU is
well-known (Chiang et al., 2008a). Therefore, in
an effort to address this issue, we tried maximizing
expected corpus BLEU directly.
In other words, given a training corpus of the
form (CF, C E) where CF is a set of source sen-
tences and CE� its corresponding reference transla-
tions, we consider a gain function defined on the
</bodyText>
<figure confidence="0.525542777777778">
�=
h�e,fi
∈D
BLEUe(e) ∂θk
∂!g
∂θk
�
e,a
∂p
</figure>
<bodyText confidence="0.919213363636364">
∂p
where
∂θk
where H(p) is the entropy of the probability dis-
tribution p(e, a|f), and T &gt; 0 is a temperature
paramater which is gradually lowered as the opti-
mization progresses according to some annealing
schedule.
Differentiating with respect to θk then shows
that the annealed gradient is given by the follow-
ing expression:
</bodyText>
<figure confidence="0.531590666666667">
� p(e, a|f)BLEUe(e) (5) 1: : (BLEUe(e) − T (1 + log p)) ∂p
e,a he,fi e,a ∂θk
∈D
∂p
where
∂θk
</figure>
<page confidence="0.94323">
367
</page>
<bodyText confidence="0.8450495">
hypothesized translation CE of the input CF with
respect to C E.
</bodyText>
<equation confidence="0.889482333333333">
The objective in equation 5 therefore becomes:
9 = 1: P(CE|CF)BLEUC E(CE) (7)
CE
</equation>
<bodyText confidence="0.991251810344827">
The pair (CE, CF) is denoted as a cor-
pus sample corresponding to a sequence
(e1, a1), ... , (eN, aN) of derivations of the
corresponding source strings f1, ... , fN of
source corpus CF.
Although the sampler described in Section 2
generates samples at the sentence level, we can use
it to generate corpus samples by applying the fol-
lowing procedure (see Figure 1). For each source
sentence fZ in the corpus, we generate a sequence
of samples (eZ1, aZ1), ... , (eZn, aZn) using the sam-
pler. From each of these sequences of samples, we
then resample new sequences of derivation sam-
ples, one for each source sentence in the corpus.
The first corpus sample is then obtained by iter-
ating through the source sentences and taking the
first resampled derivation for each sentence, then
the second corpus sample by taking the second re-
sampled derivation, and so on. The resampling
step is necessary to eliminate any biases due to the
order of the generated samples.
The corpus sampling procedure invariably gen-
erates a set of samples which are all distinct and so
would give us a uniform estimate of the probabil-
ity distribution P(CE|CF). However this is not a
problem since we are not interested in evaluating
the actual distribution; we just need to calculate
expectations of feature values and BLEU scores
over the distribution. The feature values of a cor-
pus sample are the average of the feature values of
its constituting derivations and its BLEU score is
computed based on the yield of its derivations.
When training using corpus sampling we pro-
cess the training corpus in batches (CF, C E), treat-
ing each batch as a corpus in its own right, and
updating the weights after each batch.
The gradient for the objective function in (7) is:
∂P = (hCk − EP(CE|CF )[hCk�) P(CE|CF)
where
∂θk
where hCk is the k-th component of a corpus
sample feature vector.
During deterministic annealing for sentence
sampling, the entropy term is computed over the
Figure 1: Example illustrating the extraction of 2
corpus samples for a corpus of source sentences
f1, f2, f3. In the first step, we sample 5 deriva-
tions for each source sentence. We then resample
2 derivations from the empirical distributions of
each source sentence.
distribution p(e, a|f) of each individual sentence.
While corpus sampling, we are considering the
distribution P(CE|CF) but the estimated distribu-
tion is always uniform. So we define the entropic
prior term over the distribution p(e, a|f) of the
sentences making up the corpus sample.
The annealed corpus sampling objective is
therefore:
</bodyText>
<equation confidence="0.508998">
1: T 1:
CE P(CE|CF)BLEUC �E(CE)+ H(p(e,a|f))
|CF  |fECF
</equation>
<bodyText confidence="0.9996535">
The gradient of this objective is of similar form
to the sentence sampling gradient in Equation (6).
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999968">
5.1 Training Data and Preparation
</subsectionHeader>
<bodyText confidence="0.999217857142857">
The experiments in this section were performed
using the Europarl section of the French-English
and German-English parallel corpora from the
WMT09 shared translation task (Callison-Burch et
al., 2009), as well as 300k parallel Arabic-English
sentences from the NIST MT evaluation train-
ing data.3 For all language pairs, we constructed
</bodyText>
<footnote confidence="0.996579">
3The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
</footnote>
<figure confidence="0.996855317073171">
1: =
CE
BLEUC
E(CE) ∂θk
∂9
∂θk
∂P
f1 f2 f3
f1
A
A
C
B
B
G
f2
D
E
H
F
f3
M
K
L
L
L
SAMPLE FROM
P(e,a  |f)
f1
A
B
f2
F
E
Extract Corpus
Samples
f3
L
L
Corpus Sample 1 {A, F, L }
Corpus Sample 2 {B, E, L }
</figure>
<page confidence="0.986642">
368
</page>
<bodyText confidence="0.999698">
a phrase-based translation model as described in
Koehn et al. (2003), limiting the phrase length to
5. The target side of the parallel corpus was used
to train 3-gram language models. For the German
and French systems, the DEV2006 set was used
for model tuning and the first half of TEST2007
(in-domain) for heldout testing. Final testing was
performed on NEWS-DEV2009B (out-of-domain)
and the first half of TEST2008 (in-domain). For
the Arabic system, the MT02 set (10 reference
translations) was used for tuning and MT03 and
MT05 (4 reference translations, each) were used
for held-out testing and final testing respectively.
To reduce the size of the phrase table, we used the
association-score technique suggested by Johnson
et al. (2007). Translation quality is reported using
case-insensitive BLEU.
</bodyText>
<subsectionHeader confidence="0.952998">
5.2 Baseline
</subsectionHeader>
<bodyText confidence="0.99991652">
Our baseline system is phrase-based
Moses (Koehn et al., 2007) with feature weights
trained using MERT. Moses and the Gibbs
sampler use identical feature sets.4
The MERT optimization algorithm uses multi-
ple random restarts to avoid getting stuck in a poor
local optima. Therefore, every time MERT is run,
it produces a slightly different final weight vector
leading to varying test set results. While this char-
acteristic of MERT is typically ignored, we ac-
count for it by performing MERT training 10 times
for each of the 3 language pairs, decoding the test
sets with each of the 10 optimized weight sets. We
present the best and the worst test set results along
with the mean and the standard deviation (Q) of
these results in Table 1. We report results using
the Moses implementation of Viterbi, nbest MBR
and lattice MBR decoding (Kumar et al., 2009). 5
For both nbest and lattice MBR decoding, the hy-
pothesis set was composed of the top 1000 unique
translations produced by the Viterbi decoder, and
the same 1000 translations were used as evidence
set for nbest MBR.
As Table 1 shows, translation results using
MERT optimized weights vary markedly from one
</bodyText>
<footnote confidence="0.743198333333333">
sentences with confidence c &gt; 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
4We use 5 translation model scores, distance-based distor-
tion, language model and word penalty. The reordering limit
is set to 6 for all experiments.
5For nbest and lattice MBR decoding, we optimized for
the scaling factor using a grid-search on held-out data. For
lattice MBR decoding, we optimized the lattice density and
set the p and r parameters as per Tromble et al. (2008).
</footnote>
<bodyText confidence="0.999885222222222">
tuning run to the other, with results varying from
a range of 0.3% BLEU to 1.3% BLEU when using
Viterbi decoding. We also see that, bar in-domain
German to English, MBR decoding gives a small
improvement on all other datasets.
Surprisingly, lattice MBR only gives improve-
ments on two datasets and actually leads to a drop
in performance on the other 3 datasets. We discuss
possible reasons for this in Section 6.
</bodyText>
<subsectionHeader confidence="0.999084">
5.3 Sentence sampling
</subsectionHeader>
<bodyText confidence="0.999988535714286">
At training time, the optimization algorithm is ini-
tialized with zero weights and the sampler is ini-
tialized with a random derivation from Moses. To
get rid of any initialization biases, the first 100
samples are discarded.6 We then run the sampler
for 1000 iterations after which we perform reheat-
ing whereby the distribution is progressively flat-
tened. Samples are not collected during this pe-
riod. Reheating allows the sampler more mobil-
ity around the search space thus possibly escaping
any local optima it might be trapped in. We subse-
quently run the sampler for 1000 more iterations.
We denote this procedure as running 2 chains of
the sampler. We use batch sizes of 96 randomly
selected sentences for SGD optimization.
During DA, our cooling schedule is an exponen-
tially decaying one with decay rate set to 0.9, per-
forming 20 iterations of SGD optimization at each
temperature setting. Five training runs were per-
formed and the BLEU scores averaged. The fea-
ture weights were output every 50 iterations and
performance measured on the heldout set by run-
ning the sampler as a decoder. At decode time,
we use the same sampler configurations as during
training but run 2 chains each for 5000 iterations.
For MBR decoding, we use the entirety of this
sample set as our evidence set and use the top 1000
most probable translations as the hypothesis set.
</bodyText>
<subsectionHeader confidence="0.998323">
5.4 Corpus sampling
</subsectionHeader>
<bodyText confidence="0.99996425">
For our corpus sampling experiments, we sample
using the same procedure as in sentence sampling
but using 2 chains of 2000 iterations. We then
resample 2000 corpus samples from the empiri-
cal distribution estimated from the first 4000 sam-
ples. For Arabic-English training, we used batch
sizes of 100 randomly selected sentences for ex-
periments without DA and batches of 400 random
</bodyText>
<footnote confidence="0.993381">
6This procedure is referred to as burn-in in the MCMC
literature.
</footnote>
<page confidence="0.995179">
369
</page>
<table confidence="0.999753428571428">
Viterbi nMBR lMBR
min max mean v min max mean v min max mean v
AR-EN MT05 43.7 44.3 44.0 0.17 44.2 44.5 44.4 0.13 44.2 44.6 44.5 0.12
FR-EN In 33.1 33.4 33.3 0.10 33.2 33.6 33.4 0.12 32.3 32.7 32.6 0.13
FR-EN Out 19.1 19.6 19.4 0.18 19.3 19.7 19.5 0.12 19.1 19.4 19.3 0.12
DE-EN In 27.6 27.9 27.8 0.10 27.6 27.9 27.7 0.10 27.2 27.5 27.4 0.10
DE-EN Out 14.9 16.2 15.7 0.33 15.0 16.3 15.7 0.33 15.3 16.4 16.0 0.30
</table>
<tableCaption confidence="0.991361">
Table 1: Baseline results - MERT trained models decoded using Viterbi, nbest MBR (nMBR) and lattice
</tableCaption>
<figureCaption confidence="0.78602325">
MBR (lMBR). MERT was run 10 times for each language pair. We report minimum, maximum, mean
and standard deviation of test set BLEU scores across the 10 runs.
Figure 2: Heldout performance for German-English training averaged across 5 minimum risk training
runs. Best scores achieved are indicated by dotted line.
</figureCaption>
<figure confidence="0.994798069767442">
Sentence Sampling, Without DA Sentence Sampling, With DA Corpus Sampling, Without DA Corpus Sampling, With DA
200 400 600 800 100 300 500 700 50 100 150 200 250 50 150 250 350
Training iterations Training iterations Training iterations Training iterations
Bleu
15 20 25 30
●
MaxDeriv
MaxTrans
MBR
● 27.4 28.2
●
●
Bleu
15 20 25 30
●
●
●
●
●
●
MaxDeriv
MaxTrans
MBR
Bleu
15 20 25 30
28.1
● ● ● ● ● ● ● ●
●
MaxDeriv
MaxTrans
MBR
Bleu
15 20 25 30
● ●
● ● ● ● ● ● ● ● ● ● ● ●
●
●
●
●
MaxDeriv
MaxTrans
MBR
28.5
</figure>
<bodyText confidence="0.9806351">
sentences with DA. The size of the batches cor-
responds to the number of sentences that form a
corpus sample. For German/French to English ex-
periments, we used batches of 100 random sen-
tences for training with and without DA. We per-
form 10 optimizations at each temperature setting
during deterministic annealing. Test time condi-
tions are identical to the sentence sampling ones
and we measure performance on a held-out set af-
ter every 20 iterations of the learner.
</bodyText>
<sectionHeader confidence="0.803066" genericHeader="method">
5.5 Results
</sectionHeader>
<bodyText confidence="0.999823228571429">
Figures 2 and 3 show the scores on the German-
English and Arabic-English held-out sets respec-
tively comparing all four training regimes: corpus
vs sentence sampling, DA vs without DA. Results
for French-English training are similar.
We focus our analysis on the Arabic-English ex-
perimental setup. Without deterministic anneal-
ing, the learner converges quickly, usually after
just 20 iterations, after which performance de-
grades steadily. The magnitudes of the weights
are large, sharpening the distribution. There is
not much diversity amongst the sampled deriva-
tions, i.e. the entropy of the sample set is low.
Therefore, all 3 decoding regimes give very simi-
lar results. With the addition of the entropic prior,
the model is slow to converge before the so-called
phase transition occurs (usually after around 50
iterations), after which performance goes up to
reach a peak (45.2 BLEU) higher than that without
the prior (44.2 BLEU), before steadily declining.
The entropic prior encourages diversity among the
sample set, especially at high temperature settings.
In the presence of diversity, the benefits of
marginalization over derivations is clear: Max-
Trans does better than MaxDeriv and MBR does
best, confirm recent findings of (Blunsom et al.,
2008; Arun et al., 2009) that MaxTrans improves
over MaxDeriv decoding for models trained to ac-
count for multiple derivations. As the temperature
decreases to zero, the model sharpens, effectively
intent on maximizing one-best performance and
thus voiding the benefits of MaxTrans and MBR.
Figures 2 and 3 also show that corpus sampling
improves over sentence sampling, although not by
much (+ 0.3 BLEU).
</bodyText>
<subsectionHeader confidence="0.999557">
5.6 Comparison with MERT baseline
</subsectionHeader>
<bodyText confidence="0.999967222222222">
Having established the superiority of the pipeline
of expected corpus BLEU training with DA fol-
lowed by MBR decoding over other alternatives
considered, we compare it to the best results ob-
tained with MERT optimized Moses (bold scores
from Table 1). To account for sampler variance
during both training and decoding, we average
scores across 50 runs; 10 decoding runs each using
the best weight set from 5 training runs. Results
</bodyText>
<page confidence="0.992279">
370
</page>
<note confidence="0.703379">
Sentence Sampling, Without DA Sentence Sampling With DA Corpus Sampling, Without DA Corpus Sampling, With DA
</note>
<figure confidence="0.99925625">
Bleu
30 35 40 45 50
●
MaxDeriv
● MaxTrans
MBR
●
44.2
Bleu
30 35 40 45 50
●
● ● ● ● ● ● ● ●
●
●
●
MaxDeriv
● MaxTrans
MBR
45.2
Bleu
30 35 40 45 50
● ●
●
●
MaxDeriv
● MaxTrans
MBR
44.5
Bleu
30 35 40 45 50
●
●
45.5
●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
MaxDeriv
● MaxTrans
MBR
0 200 600 1000 1400 0 500 1000 1500 20 40 60 80 100 140 100 200 300 400
Training iterations Training iterations Training iterations Training iterations
</figure>
<figureCaption confidence="0.884991333333333">
Figure 3: Heldout performance for Arabic-English training averaged across 5 minimum risk training
runs. Best scores achieved are indicated by dotted line.
are shown in Table 2.7
</figureCaption>
<bodyText confidence="0.9995423125">
We observe that on 3 out of 5 datasets, the sam-
pler results are much more stable than MERT and
as stable on the other 2 datasets. We attribute the
improved stability to the more powerful optimiza-
tion algorithm used by the sampler which uses gra-
dient information to steer the model towards better
weights. MERT, alternatively, optimizes one fea-
ture at a time using line search and therefore does
not explore the full feature space as thoroughly.
Translation results with the sampler are better
than with MERT on 2 datasets, are equal on an-
other 2 and worse in one case. The improvements
withe the sampler are obtained in the case of out-
of-domain data suggesting that the minimum risk
training objective generalizes better than the 1-
best objective of MERT.
</bodyText>
<table confidence="0.999618714285714">
Test set MERT/Moses Sampler
Best v MBR v
AR-EN MT05 44.5 (lMBR) 0.12 44.5 0.14
FR-EN In 33.4 (nMBR) 0.12 33.2 0.06
FR-EN Out 19.5 (nMBR) 0.12 19.8 0.05
DE-EN In 27.8 (Viterbi) 0.10 27.8 0.11
DE-EN Out 16.0 (lMBR) 0.30 16.6 0.12
</table>
<tableCaption confidence="0.957001">
Table 2: Final results comparing MERT/Moses
</tableCaption>
<bodyText confidence="0.780373875">
pipeline with unified sampler pipeline. Sampler
uses corpus sampling during training and MBR
decoding at test time. Moses results are aver-
aged across decoding runs using weights from
10 MERT runs and sampler results are averaged
across 10 decoding runs for each of 5 different
training runs. We report BLEU scores and standard
deviation (a).
</bodyText>
<footnote confidence="0.978974">
7The MBR decoding times, averaged over 10 decoding
runs of 50 sentences each, are 10 secs/sent for Moses nbest
MBR, 40 secs/sent for Moses lattice MBR and 180 secs/sent
for the sampler.
</footnote>
<table confidence="0.999955571428571">
Viterbi nMBR lMBR Sampler
MBR
AR-EN MT05 44.2 44.4 44.8 44.8
FR-EN In 33.1 33.2 33.3 33.3
FR-EN Out 19.6 19.8 19.9 19.9
DE-EN In 27.7 27.9 28.0 28.0
DE-EN Out 16.0 16.3 16.6 16.6
</table>
<tableCaption confidence="0.999654">
Table 3: Comparison of decoding methods using
</tableCaption>
<bodyText confidence="0.8808106">
expected BLEU trained weights. We report Viterbi,
nbest MBR (nMBR) and lattice MBR (lMBR) de-
coding scores vs best sampler MBR decoding per-
formance. We selected the best weight set based
on performance on heldout data.
</bodyText>
<subsectionHeader confidence="0.998291">
5.7 Moses with expected BLEU weights
</subsectionHeader>
<bodyText confidence="0.999877714285714">
In a final set of experiments, we reran the Moses
decoder this time using weights obtained through
expected BLEU optimization. Here, for each lan-
guage pair, we picked the weight set that gave the
best results on held-out data. Note that the results
which we show in Table 3 are over one run only,
so are not strictly comparable to those in Table 2
which are averaged over several training and de-
coding runs. We also report the best results ob-
tained with the sampler MBR decoder using these
weights.
In contrast to Table 1, here we see a consistent
improvement across all test-sets when going from
Viterbi decoding to n-best then to lattice MBR.
Except for in-domain French-English, the transla-
tion results are superior to the best scores shown
(in bold) in Table 1, confirming that the minimum
risk training objective is able to find good weight
sets. Interestingly, we also observe that sampler
MBR gets the same exact results for all test sets as
lattice MBR.
</bodyText>
<page confidence="0.997948">
371
</page>
<sectionHeader confidence="0.999576" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99998965882353">
We have shown that the sampler of Arun et al.
(2009) can be used to perform minimum risk train-
ing over an unpruned search space. Our pro-
posed corpus sampling technique, like MERT, is
able to optimize corpus BLEU directly whereas
alternate parameter estimation techniques usually
employed in SMT optimize approximations of
BLEU. Chiang et al. (2008b) accounts for the on-
line nature of the MIRA optimization algorithm
by smoothing the sentence-level BLEU precision
counts of a translation with a weighted average of
the precision counts of previously decoded sen-
tences, thus approximating corpus BLEU. As
for minimum risk training, prior implementations
have either used sentence-level BLEU (Zens et al.,
2007) or a linear approximation to BLEU (Smith
and Eisner, 2006; Li and Eisner, 2009).
At test time, the sampler works best as an MBR
decoder, but also allows us to verify past claims
about the benefits of marginalizing over align-
ments during decoding. We compare the sam-
pler MBR decoder’s performance against MERT-
optimized Moses run under three different decod-
ing regimes, finding that the sampler does as well
or better on 4 out of 5 datasets.
Our training and testing pipeline has the advan-
tage of being able to handle a large number of both
local and global features so we expect in the future
to outperform the standard MERT and dynamic
programming-based search pipeline further.
As shown in Section 5.2, lattice MBR in some
cases leads to a marked drop in performance. (Ku-
mar et al., 2009) mention that the linear approx-
imation to BLEU used in their lattice MBR algo-
rithm is not guaranteed to match corpus BLEU, es-
pecially on unseen test sets. To account for these
cases, they allow their algorithm to back-off to the
MAP solution. One possible reason for the drop
in performance in our lattice MBR experiments is
that the implementation we use does not employ
this back-off strategy.
Table 3 provides valuable insights as to the mer-
its of the lattice MBR approach versus our own
sampling based pipeline. Firstly, whereas with
MERT optimized weights, the benefits of lattice
MBR are debatable (Table 1), running Moses with
minimum risk trained weights gives results that
are in line with what we would expect - lattice
MBR does systematically better than competing
decoding algorithms. This suggests that the unbi-
ased minimum risk training criterion used by the
sampler is a better fit for lattice MBR than the
MERT criterion, and also that the mismatch be-
tween linear and corpus BLEU mentioned before
might not be the reason for the results in Table 1.
Secondly, we find that sampling MBR matches
lattice MBR on the minimum risk trained weights.
The MBR sampler uses samples drawn from the
distribution as hypothesis and evidence sets, typi-
cally 1000 samples for the former and 10000 sam-
ples for the latter. In the lattice MBR experiments
of Tromble et al. (2008), it is shown that this size
of hypothesis set is sufficient. Their evidence set,
however, is significantly larger than ours.8Table 3
suggests that, since it is not biased by heuris-
tic pruning, the sampler’s limited evidence set is
enough to give a good estimate of the probabil-
ity distribution whereas beam-search based MBR
needs to scale from using n-best lists to lattices to
get equivalent results.
Sampling the phrase-based model is expensive,
meaning that lattice MBR is still faster (around
4x) to run than sampler MBR. However, due to
the unified nature of the training and decoding cri-
terion in our approach, the minimum risk trained
weights can be plugged directly into the sam-
pler MBR decoder, whereas lattice MBR requires
an additional expensive step of tuning the model
hyper-parameters (Kumar et al., 2009).
In future work, we also intend to look at more
efficient ways of generating samples. One pos-
sibility is to interleave Gibbs sampling steps us-
ing low order ngram language model distributions
with Metropolis-Hasting steps that use higher or-
der language model distributions.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.996893461538461">
Expected BLEU training for phrase-based models
has been successfully attempted by (Smith and
Eisner, 2006; Zens et al., 2007), however they both
used biased n-best lists to approximate the pos-
terior distribution. Li and Eisner (2009) present
work on performing expected BLEU training with
deterministic annealing on translation forests gen-
erated by Hiero (Chiang, 2007). Since BLEU does
not factorize over the search graph, they use the
linear approximation of Tromble et al. (2008) in-
stead.
Pauls et al. (2009) present an alternate training
criterion over translation forests called CoBLEU,
</bodyText>
<footnote confidence="0.725889">
8up to 1081 as per Tromble et al. (2008)
</footnote>
<page confidence="0.995169">
372
</page>
<bodyText confidence="0.999974133333333">
similar in spirit to expected BLEU training, but
aimed to maximize the expected counts of n-grams
appearing in reference translations. This training
criterion is used in conjunction with consensus de-
coding (DeNero et al., 2009), a linear-time ap-
proximation of MBR.
In contrast to the approaches above, the algo-
rithms presented in this paper are able to explore
an unpruned search space. By using corpus sam-
pling, we can perform minimum risk training with
corpus BLEU rather than any approximations of
this metric. Also, since we maintain a probabilis-
tic formulation across training and decoding, our
approach does not require a grid-search for a scal-
ing factor as in Tromble et al. (2008).
</bodyText>
<sectionHeader confidence="0.999418" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999983380952381">
We have presented a unified approach to the task
of parameter estimation and decoding for a phrase-
based system using the standard translation eval-
uation metric, BLEU. Using a Gibbs sampler to
explore the entire probability distribution allows
us to implement two probabilistic sound algo-
rithms, minimum risk training and its equivalent,
MBR decoding, in an unbiased way. The proba-
bilistic formulation also allows us to use gradient
based optimization techniques which produce sta-
ble model parameters. At decoding time, we show
the benefits of marginalizing over derivations and
that MBR gives better results than other decoding
criteria.
Since our optimization algorithm can cope with
a large number of features, in future work, we
plan to incorporate more expressive features in
the model. We use a Gibbs sampler for inference
so there is scope for exploring non-local features
which might not easily be added to dynamic pro-
gramming based models.
</bodyText>
<sectionHeader confidence="0.998365" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.971480555555556">
This research was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-001; and by the EuroMa-
trix project funded by the European Commission (6th
Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
</bodyText>
<sectionHeader confidence="0.999311" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990426984127">
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom,
Adam Lopez, and Philipp Koehn. 2009. Monte carlo in-
ference and maximization for phrase-based translation. In
Proceedings of CoNLL, pages 102–110.
Phil Blunsom and Miles Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP 2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
Alexandre Bouchard-Cˆot´e, Slav Petrov, and Dan Klein.
2009. Randomized pruning: Efficiently calculating ex-
pectations in large dynamic programs. In Advances in
Neural Information Processing Systems 22, pages 144–
152.
Chris Callison-Burch, Philipp Koehn, Christoph Monz, and
Josh Schroeder, editors. 2009. Proc. of Workshop on Ma-
chine Translations.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient algo-
rithms. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages 610–
619, Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008b. On-
line large-margin training of syntactic and structural trans-
lation features. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Processing,
pages 224–233, Honolulu, Hawaii, October. Association
for Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
John DeNero, David Chiang, and Kevin Knight. 2009. Fast
consensus decoding over translation forests. In Proceed-
ings of ACL/AFNLP, pages 567–575.
Stuart Geman and Donald Geman. 1984. Stochastic relax-
ation, Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 6:721–741.
J.H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most of the
phrasetable. In Proc. of EMNLP-CoNLL, Prague.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48–54,
Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL Demos, pages 177–
180.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decod-
ing for statistical machine translation. In Processings of
HLT-NAACL.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz
Och. 2009. Efficient minimum error rate training and
minimum bayes-risk decoding for translation hypergraphs
and lattices. In Proceedings of ACL/AFNLP, pages 163–
171.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-risk
training on translation forests. In Proceedings of EMNLP,
pages 40–51.
</reference>
<page confidence="0.990853">
373
</page>
<reference confidence="0.999439896551724">
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Vari-
ational decoding for statistical machine translation. In
Proceedings of ACL/AFNLP, pages 593–601.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of ACL, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311–318.
Adam Pauls, John Denero, and Dan Klein. 2009. Consensus
training for consensus decoding in machine translation. In
Proceedings of EMNLP, pages 1418–1427.
Kenneth Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related opti-
mization problems. In Proceedings of the IEEE, pages
2210–2239.
Nicol N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-09-
99, IDSIA.
David A. Smith and Jason Eisner. 2006. Minimum risk an-
nealing for training log-linear models. In Proceedings of
COLING-ACL, pages 787–794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice Minimum Bayes-Risk decod-
ing for statistical machine translation. In Proceedings of
EMNLP, pages 620–629.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007. A sys-
tematic comparison of training criteria for statistical ma-
chine translation. In Proceedings of EMNLP, pages 524–
532.
</reference>
<page confidence="0.999015">
374
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.481351">
<title confidence="0.999666">A Unified Approach to Minimum Risk Training and Decoding</title>
<author confidence="0.976345">Abhishek Arun</author>
<author confidence="0.976345">Barry Haddow</author>
<author confidence="0.976345">Philipp</author>
<affiliation confidence="0.9951525">School of University of</affiliation>
<address confidence="0.508648">Edinburgh, EH8 9AB,</address>
<abstract confidence="0.998078315789474">We present a unified approach to performing minimum risk training and minimum risk (MBR) decoding with in a phrase-based model. Key to our approach is the use of a Gibbs sampler that us to explore the probabildistribution maintain a strict probabilistic formulation across the pipeline. We also describe a new sampling algocalled sampling alus at training time to use instead of an approximation thereof. Our approach is theoretically sound and gives (up to and more stable results than the standard MERT optimization algorithm. By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Chris Dyer</author>
<author>Barry Haddow</author>
<author>Phil Blunsom</author>
<author>Adam Lopez</author>
<author>Philipp Koehn</author>
</authors>
<title>Monte carlo inference and maximization for phrase-based translation.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>102--110</pages>
<contexts>
<context position="3647" citStr="Arun et al. (2009)" startWordPosition="588" endWordPosition="591">ond MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 2008; Bouchard-Cˆot´e et al., 2009). Here, we present a unified approach to training and decoding in a phrase-based translation model (Koehn et al., 2003) which keeps the objective constant across the translation pipeline and so obviates the need for any extra hyper-parameter fitting. We use the phrase-based Gibbs sampler of Arun et al. (2009) at training time to compute the gradient of our minimum risk training objective in order to apply first-order optimization techniques, 365 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365–374, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics and at test time we use it to estimate the posterior distribution required by MBR (Section 3). We experimented with two different objective functions for training (Section 4). First, following (Arun et al., 2009), we define our objective at the sentence-level using a sent</context>
<context position="6682" citStr="Arun et al. (2009)" startWordPosition="1076" endWordPosition="1079"> for many problems, it has two serious drawbacks for probabilistic inference. First, the error incurred by the Viterbi maximum with respect to the true model maximum is unbounded. Second, the DP solution requires substantial pruning and restricts the use of non-local features. The latter problem persists even in the variational approximations of Li et al. (2009), which attempt to solve the former. 2.1 Gibbs sampling for phrase-based MT An alternate approximate inference method for phrase-based MT without any of the previously mentioned drawbacks is the Gibbs sampler (Geman and Geman, 1984) of Arun et al. (2009) which draws samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, SN1 = (e1, a1) ... (eN, aN), that are drawn from the distribution p(e, a|f). These samples can be used to estimate the expectation of a function h(e, a, f) as follows: N 1 L&apos;p(a,e|f)[h] = ��m N�� N i=1 3 Decoding In this work, we are interested in performing MBR decoding with BLEU. We define the MBR decision rule following Tromble et al. (2008): e* = arg max eEex e&apos;EEE � BLEUe(e&apos;)p(e&apos;|f) (4) where EH refers to the </context>
<context position="9903" citStr="Arun et al. (2009)" startWordPosition="1641" endWordPosition="1644">tiable with respect to the model parameters θ to give (6) = (hk − F-p(e,a|f)[hk]) p(e, a|f) Since the gradient is expressed in terms of expectations of feature values, it can easily be calculated using the sampler and then first-order optimization techniques can be applied to find optimal values of θ. Because of the noise introduced by the sampler, we used stochastic gradient descent (SGD), with a learning rate that gets updated after each step proportionally to difference in successive gradients (Schraudolph, 1999). While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as !g is non-convex, we decided to employ deterministic annealing (Rose, 1998) to smooth the objective function to ensure that the optimizer explored as large a region as possible of the space before it settled on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on the work of Smith and Eisner (2006), and involves the addition of an entropic prior to the objec</context>
<context position="23712" citStr="Arun et al., 2009" startWordPosition="4008" endWordPosition="4011">milar results. With the addition of the entropic prior, the model is slow to converge before the so-called phase transition occurs (usually after around 50 iterations), after which performance goes up to reach a peak (45.2 BLEU) higher than that without the prior (44.2 BLEU), before steadily declining. The entropic prior encourages diversity among the sample set, especially at high temperature settings. In the presence of diversity, the benefits of marginalization over derivations is clear: MaxTrans does better than MaxDeriv and MBR does best, confirm recent findings of (Blunsom et al., 2008; Arun et al., 2009) that MaxTrans improves over MaxDeriv decoding for models trained to account for multiple derivations. As the temperature decreases to zero, the model sharpens, effectively intent on maximizing one-best performance and thus voiding the benefits of MaxTrans and MBR. Figures 2 and 3 also show that corpus sampling improves over sentence sampling, although not by much (+ 0.3 BLEU). 5.6 Comparison with MERT baseline Having established the superiority of the pipeline of expected corpus BLEU training with DA followed by MBR decoding over other alternatives considered, we compare it to the best result</context>
<context position="28333" citStr="Arun et al. (2009)" startWordPosition="4827" endWordPosition="4830">so report the best results obtained with the sampler MBR decoder using these weights. In contrast to Table 1, here we see a consistent improvement across all test-sets when going from Viterbi decoding to n-best then to lattice MBR. Except for in-domain French-English, the translation results are superior to the best scores shown (in bold) in Table 1, confirming that the minimum risk training objective is able to find good weight sets. Interestingly, we also observe that sampler MBR gets the same exact results for all test sets as lattice MBR. 371 6 Discussion We have shown that the sampler of Arun et al. (2009) can be used to perform minimum risk training over an unpruned search space. Our proposed corpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU. Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations </context>
</contexts>
<marker>Arun, Dyer, Haddow, Blunsom, Lopez, Koehn, 2009</marker>
<rawString>Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom, Adam Lopez, and Philipp Koehn. 2009. Monte carlo inference and maximization for phrase-based translation. In Proceedings of CoNLL, pages 102–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="3306" citStr="Blunsom and Osborne, 2008" startWordPosition="533" endWordPosition="536">008; Kumar et al., 2009) have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance. However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 2008; Bouchard-Cˆot´e et al., 2009). Here, we present a unified approach to training and decoding in a phrase-based translation model (Koehn et al., 2003) which keeps the objective constant across the translation pipeline and so obviates the need for any extra hyper-parameter fitting. We use the phrase-based Gibbs sampler of Arun et al. (2009) at training time to compute the gradient of our minimum risk training objective in order to apply first-order optimization techniques, 365 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365–374, Uppsala, Swede</context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>Phil Blunsom and Miles Osborne. 2008. Probabilistic inference for machine translation. In Proc. of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="23692" citStr="Blunsom et al., 2008" startWordPosition="4004" endWordPosition="4007">g regimes give very similar results. With the addition of the entropic prior, the model is slow to converge before the so-called phase transition occurs (usually after around 50 iterations), after which performance goes up to reach a peak (45.2 BLEU) higher than that without the prior (44.2 BLEU), before steadily declining. The entropic prior encourages diversity among the sample set, especially at high temperature settings. In the presence of diversity, the benefits of marginalization over derivations is clear: MaxTrans does better than MaxDeriv and MBR does best, confirm recent findings of (Blunsom et al., 2008; Arun et al., 2009) that MaxTrans improves over MaxDeriv decoding for models trained to account for multiple derivations. As the temperature decreases to zero, the model sharpens, effectively intent on maximizing one-best performance and thus voiding the benefits of MaxTrans and MBR. Figures 2 and 3 also show that corpus sampling improves over sentence sampling, although not by much (+ 0.3 BLEU). 5.6 Comparison with MERT baseline Having established the superiority of the pipeline of expected corpus BLEU training with DA followed by MBR decoding over other alternatives considered, we compare i</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Randomized pruning: Efficiently calculating expectations in large dynamic programs.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>144--152</pages>
<marker>Bouchard-Cˆot´e, Petrov, Klein, 2009</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Slav Petrov, and Dan Klein. 2009. Randomized pruning: Efficiently calculating expectations in large dynamic programs. In Advances in Neural Information Processing Systems 22, pages 144– 152.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>Proc. of Workshop on Machine Translations.</booktitle>
<editor>Chris Callison-Burch, Philipp Koehn, Christoph Monz, and Josh Schroeder, editors.</editor>
<contexts>
<context position="3647" citStr="(2009)" startWordPosition="591" endWordPosition="591">. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 2008; Bouchard-Cˆot´e et al., 2009). Here, we present a unified approach to training and decoding in a phrase-based translation model (Koehn et al., 2003) which keeps the objective constant across the translation pipeline and so obviates the need for any extra hyper-parameter fitting. We use the phrase-based Gibbs sampler of Arun et al. (2009) at training time to compute the gradient of our minimum risk training objective in order to apply first-order optimization techniques, 365 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365–374, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics and at test time we use it to estimate the posterior distribution required by MBR (Section 3). We experimented with two different objective functions for training (Section 4). First, following (Arun et al., 2009), we define our objective at the sentence-level using a sent</context>
<context position="6428" citStr="(2009)" startWordPosition="1038" endWordPosition="1038">ary. The most common of these approximations is the Viterbi approximation, which simply chooses the most likely derivation (e*, a*). This approximation can be computed in polynomial time via dynamic programming (DP). Though fast and effective for many problems, it has two serious drawbacks for probabilistic inference. First, the error incurred by the Viterbi maximum with respect to the true model maximum is unbounded. Second, the DP solution requires substantial pruning and restricts the use of non-local features. The latter problem persists even in the variational approximations of Li et al. (2009), which attempt to solve the former. 2.1 Gibbs sampling for phrase-based MT An alternate approximate inference method for phrase-based MT without any of the previously mentioned drawbacks is the Gibbs sampler (Geman and Geman, 1984) of Arun et al. (2009) which draws samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, SN1 = (e1, a1) ... (eN, aN), that are drawn from the distribution p(e, a|f). These samples can be used to estimate the expectation of a function h(e, a, f) as follo</context>
<context position="9903" citStr="(2009)" startWordPosition="1644" endWordPosition="1644">respect to the model parameters θ to give (6) = (hk − F-p(e,a|f)[hk]) p(e, a|f) Since the gradient is expressed in terms of expectations of feature values, it can easily be calculated using the sampler and then first-order optimization techniques can be applied to find optimal values of θ. Because of the noise introduced by the sampler, we used stochastic gradient descent (SGD), with a learning rate that gets updated after each step proportionally to difference in successive gradients (Schraudolph, 1999). While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as !g is non-convex, we decided to employ deterministic annealing (Rose, 1998) to smooth the objective function to ensure that the optimizer explored as large a region as possible of the space before it settled on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on the work of Smith and Eisner (2006), and involves the addition of an entropic prior to the objec</context>
<context position="28333" citStr="(2009)" startWordPosition="4830" endWordPosition="4830">e best results obtained with the sampler MBR decoder using these weights. In contrast to Table 1, here we see a consistent improvement across all test-sets when going from Viterbi decoding to n-best then to lattice MBR. Except for in-domain French-English, the translation results are superior to the best scores shown (in bold) in Table 1, confirming that the minimum risk training objective is able to find good weight sets. Interestingly, we also observe that sampler MBR gets the same exact results for all test sets as lattice MBR. 371 6 Discussion We have shown that the sampler of Arun et al. (2009) can be used to perform minimum risk training over an unpruned search space. Our proposed corpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU. Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations </context>
<context position="32496" citStr="(2009)" startWordPosition="5519" endWordPosition="5519"> additional expensive step of tuning the model hyper-parameters (Kumar et al., 2009). In future work, we also intend to look at more efficient ways of generating samples. One possibility is to interleave Gibbs sampling steps using low order ngram language model distributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus</context>
</contexts>
<marker>2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christoph Monz, and Josh Schroeder, editors. 2009. Proc. of Workshop on Machine Translations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Decomposability of translation metrics for improved evaluation and efficient algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>610--619</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="11552" citStr="Chiang et al., 2008" startWordPosition="1911" endWordPosition="1914">2009), express the parameterization of the distribution 0 as γ0� (where γ is the scaling factor) and perform optimization in two steps, the first optimizing 0 and the second optimizing γ. We experimented with this two stage optimization process, but found that simply performing an unconstrained optimization on 0 gave better results. 4.2 Corpus sampling While the objective functions in Equations 5 and 4.1 use a sentence-level variant of BLEU, the model’s test-time performance is evaluated with corpus level BLEU. The lack of correlation between sentence-level BLEU and corpus BLEU is well-known (Chiang et al., 2008a). Therefore, in an effort to address this issue, we tried maximizing expected corpus BLEU directly. In other words, given a training corpus of the form (CF, C E) where CF is a set of source sentences and CE� its corresponding reference translations, we consider a gain function defined on the �= h�e,fi ∈D BLEUe(e) ∂θk ∂!g ∂θk � e,a ∂p ∂p where ∂θk where H(p) is the entropy of the probability distribution p(e, a|f), and T &gt; 0 is a temperature paramater which is gradually lowered as the optimization progresses according to some annealing schedule. Differentiating with respect to θk then shows t</context>
<context position="28628" citStr="Chiang et al. (2008" startWordPosition="4873" endWordPosition="4876">re superior to the best scores shown (in bold) in Table 1, confirming that the minimum risk training objective is able to find good weight sets. Interestingly, we also observe that sampler MBR gets the same exact results for all test sets as lattice MBR. 371 6 Discussion We have shown that the sampler of Arun et al. (2009) can be used to perform minimum risk training over an unpruned search space. Our proposed corpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU. Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations have either used sentence-level BLEU (Zens et al., 2007) or a linear approximation to BLEU (Smith and Eisner, 2006; Li and Eisner, 2009). At test time, the sampler works best as an MBR decoder, but also allows us to verify past claims about the benefits of marginalizing over alignments during d</context>
</contexts>
<marker>Chiang, DeNeefe, Chan, Ng, 2008</marker>
<rawString>David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng. 2008a. Decomposability of translation metrics for improved evaluation and efficient algorithms. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610– 619, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="11552" citStr="Chiang et al., 2008" startWordPosition="1911" endWordPosition="1914">2009), express the parameterization of the distribution 0 as γ0� (where γ is the scaling factor) and perform optimization in two steps, the first optimizing 0 and the second optimizing γ. We experimented with this two stage optimization process, but found that simply performing an unconstrained optimization on 0 gave better results. 4.2 Corpus sampling While the objective functions in Equations 5 and 4.1 use a sentence-level variant of BLEU, the model’s test-time performance is evaluated with corpus level BLEU. The lack of correlation between sentence-level BLEU and corpus BLEU is well-known (Chiang et al., 2008a). Therefore, in an effort to address this issue, we tried maximizing expected corpus BLEU directly. In other words, given a training corpus of the form (CF, C E) where CF is a set of source sentences and CE� its corresponding reference translations, we consider a gain function defined on the �= h�e,fi ∈D BLEUe(e) ∂θk ∂!g ∂θk � e,a ∂p ∂p where ∂θk where H(p) is the entropy of the probability distribution p(e, a|f), and T &gt; 0 is a temperature paramater which is gradually lowered as the optimization progresses according to some annealing schedule. Differentiating with respect to θk then shows t</context>
<context position="28628" citStr="Chiang et al. (2008" startWordPosition="4873" endWordPosition="4876">re superior to the best scores shown (in bold) in Table 1, confirming that the minimum risk training objective is able to find good weight sets. Interestingly, we also observe that sampler MBR gets the same exact results for all test sets as lattice MBR. 371 6 Discussion We have shown that the sampler of Arun et al. (2009) can be used to perform minimum risk training over an unpruned search space. Our proposed corpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU. Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations have either used sentence-level BLEU (Zens et al., 2007) or a linear approximation to BLEU (Smith and Eisner, 2006; Li and Eisner, 2009). At test time, the sampler works best as an MBR decoder, but also allows us to verify past claims about the benefits of marginalizing over alignments during d</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008b. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224–233, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="32632" citStr="Chiang, 2007" startWordPosition="5537" endWordPosition="5538">ore efficient ways of generating samples. One possibility is to interleave Gibbs sampling steps using low order ngram language model distributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in th</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP,</booktitle>
<pages>567--575</pages>
<contexts>
<context position="33127" citStr="DeNero et al., 2009" startWordPosition="5615" endWordPosition="5618">rk on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in this paper are able to explore an unpruned search space. By using corpus sampling, we can perform minimum risk training with corpus BLEU rather than any approximations of this metric. Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al. (2008). 8 Conclusions We have presented a unified approach to the task of parameter estimation and decoding for a phrasebased system using the st</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In Proceedings of ACL/AFNLP, pages 567–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="6660" citStr="Geman and Geman, 1984" startWordPosition="1070" endWordPosition="1074"> Though fast and effective for many problems, it has two serious drawbacks for probabilistic inference. First, the error incurred by the Viterbi maximum with respect to the true model maximum is unbounded. Second, the DP solution requires substantial pruning and restricts the use of non-local features. The latter problem persists even in the variational approximations of Li et al. (2009), which attempt to solve the former. 2.1 Gibbs sampling for phrase-based MT An alternate approximate inference method for phrase-based MT without any of the previously mentioned drawbacks is the Gibbs sampler (Geman and Geman, 1984) of Arun et al. (2009) which draws samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, SN1 = (e1, a1) ... (eN, aN), that are drawn from the distribution p(e, a|f). These samples can be used to estimate the expectation of a function h(e, a, f) as follows: N 1 L&apos;p(a,e|f)[h] = ��m N�� N i=1 3 Decoding In this work, we are interested in performing MBR decoding with BLEU. We define the MBR decision rule following Tromble et al. (2008): e* = arg max eEex e&apos;EEE � BLEUe(e&apos;)p(e&apos;|f) (4) w</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Johnson</author>
<author>J Martin</author>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<location>Prague.</location>
<contexts>
<context position="16646" citStr="Johnson et al. (2007)" startWordPosition="2794" endWordPosition="2797"> the parallel corpus was used to train 3-gram language models. For the German and French systems, the DEV2006 set was used for model tuning and the first half of TEST2007 (in-domain) for heldout testing. Final testing was performed on NEWS-DEV2009B (out-of-domain) and the first half of TEST2008 (in-domain). For the Arabic system, the MT02 set (10 reference translations) was used for tuning and MT03 and MT05 (4 reference translations, each) were used for held-out testing and final testing respectively. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007). Translation quality is reported using case-insensitive BLEU. 5.2 Baseline Our baseline system is phrase-based Moses (Koehn et al., 2007) with feature weights trained using MERT. Moses and the Gibbs sampler use identical feature sets.4 The MERT optimization algorithm uses multiple random restarts to avoid getting stuck in a poor local optima. Therefore, every time MERT is run, it produces a slightly different final weight vector leading to varying test set results. While this characteristic of MERT is typically ignored, we account for it by performing MERT training 10 times for each of the 3 </context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>J.H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proc. of EMNLP-CoNLL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>48--54</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3456" citStr="Koehn et al., 2003" startWordPosition="556" endWordPosition="559">ormance. However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 2008; Bouchard-Cˆot´e et al., 2009). Here, we present a unified approach to training and decoding in a phrase-based translation model (Koehn et al., 2003) which keeps the objective constant across the translation pipeline and so obviates the need for any extra hyper-parameter fitting. We use the phrase-based Gibbs sampler of Arun et al. (2009) at training time to compute the gradient of our minimum risk training objective in order to apply first-order optimization techniques, 365 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365–374, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics and at test time we use it to estimate the posterior distribution required by MBR</context>
<context position="5170" citStr="Koehn et al. (2003)" startWordPosition="825" endWordPosition="828">resented in this paper are theoretically sound. Moreover, experimental evidence on three language pairs shows that our training regime is more stable than MERT, able to generalize better and generally leads to improvement in translation when used with sampling based MBR (Section 5). An added benefit is that the trained weights also lead to better performance when used with a beam-search based decoder. 2 Inference methods for MT We assume a phrase-based machine translation model, defined with a log-linear form, with feature function vector h and parametrized by weight vector 0, as described in Koehn et al. (2003). The input sentence, f, is segmented into phrases, which are sequences of adjacent words. Each source phrase is translated into the target language, to produce an output sentence e and an alignment a representing the mapping from source to target phrases. Phrases are allowed to be reordered. p(e, a |f; 0) = exp [0 - h(e, a, f)] (1) E(e, a,) exp [0 - h(e&apos;, a&apos;, f)] MAP decoding under this model consists of finding the most likely output string, e*: �e* = argmaxe p(e, a|f) (2) aEA(e,f) where A(e, f) is the set of all derivations of output string e given source string f. Summing over all the deri</context>
<context position="15972" citStr="Koehn et al. (2003)" startWordPosition="2686" endWordPosition="2689">T09 shared translation task (Callison-Burch et al., 2009), as well as 300k parallel Arabic-English sentences from the NIST MT evaluation training data.3 For all language pairs, we constructed 3The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the 1: = CE BLEUC E(CE) ∂θk ∂9 ∂θk ∂P f1 f2 f3 f1 A A C B B G f2 D E H F f3 M K L L L SAMPLE FROM P(e,a |f) f1 A B f2 F E Extract Corpus Samples f3 L L Corpus Sample 1 {A, F, L } Corpus Sample 2 {B, E, L } 368 a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train 3-gram language models. For the German and French systems, the DEV2006 set was used for model tuning and the first half of TEST2007 (in-domain) for heldout testing. Final testing was performed on NEWS-DEV2009B (out-of-domain) and the first half of TEST2008 (in-domain). For the Arabic system, the MT02 set (10 reference translations) was used for tuning and MT03 and MT05 (4 reference translations, each) were used for held-out testing and final testing respectively. To reduce the size of the phrase table, </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrasebased translation. In Proc. of HLT-NAACL, pages 48–54, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch Mayne</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demos,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="16784" citStr="Koehn et al., 2007" startWordPosition="2813" endWordPosition="2816">and the first half of TEST2007 (in-domain) for heldout testing. Final testing was performed on NEWS-DEV2009B (out-of-domain) and the first half of TEST2008 (in-domain). For the Arabic system, the MT02 set (10 reference translations) was used for tuning and MT03 and MT05 (4 reference translations, each) were used for held-out testing and final testing respectively. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007). Translation quality is reported using case-insensitive BLEU. 5.2 Baseline Our baseline system is phrase-based Moses (Koehn et al., 2007) with feature weights trained using MERT. Moses and the Gibbs sampler use identical feature sets.4 The MERT optimization algorithm uses multiple random restarts to avoid getting stuck in a poor local optima. Therefore, every time MERT is run, it produces a slightly different final weight vector leading to varying test set results. While this characteristic of MERT is typically ignored, we account for it by performing MERT training 10 times for each of the 3 language pairs, decoding the test sets with each of the 10 optimized weight sets. We present the best and the worst test set results along</context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL Demos, pages 177– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Processings of HLT-NAACL.</booktitle>
<contexts>
<context position="1210" citStr="Kumar and Byrne, 2004" startWordPosition="191" endWordPosition="194">rithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereof. Our approach is theoretically sound and gives better (up to +0.6%BLEU) and more stable results than the standard MERT optimization algorithm. By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods. 1 Introduction According to statistical decision theory, the optimal decision rule for any statistical model is the solution that minimizes its risk (expected loss). This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne, 2004). Since machine translation (MT) models are typically evaluated by BLEU (Papineni et al., 2002), a loss function which rewards partial matches, the MBR solution is to be preferred to the Maximum A Posteriori (MAP) solution. In most statistical MT (SMT) systems, MBR is implemented as a reranker of a list1 of translations generated by a first-pass decoder. This decoder typically assigns unnormalised log probabilities (known as scores) to each translation hypoth1We use the term list to denote any enumerable representation of translation hypotheses e.g n-best list, translation lattice or forest. e</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Processings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="2705" citStr="Kumar et al., 2009" startWordPosition="433" endWordPosition="436">mming over the translations in the list. The second step is to find the correct scale factor for the scores using a hyper-parameter search over held-out data. This is needed because the model parameters for the first-pass decoder are normally learnt using MERT (Och, 2003), which is invariant under scaling of the scores. Both these steps are theoretically unsatisfactory methods of estimating the posterior probability distribution since the approximation to Z is an unbounded term and the scaling factor is an artificial way of inducing a probability distribution. Recently, (Tromble et al., 2008; Kumar et al., 2009) have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance. However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 200</context>
<context position="17575" citStr="Kumar et al., 2009" startWordPosition="2949" endWordPosition="2952">tting stuck in a poor local optima. Therefore, every time MERT is run, it produces a slightly different final weight vector leading to varying test set results. While this characteristic of MERT is typically ignored, we account for it by performing MERT training 10 times for each of the 3 language pairs, decoding the test sets with each of the 10 optimized weight sets. We present the best and the worst test set results along with the mean and the standard deviation (Q) of these results in Table 1. We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al., 2009). 5 For both nbest and lattice MBR decoding, the hypothesis set was composed of the top 1000 unique translations produced by the Viterbi decoder, and the same 1000 translations were used as evidence set for nbest MBR. As Table 1 shows, translation results using MERT optimized weights vary markedly from one sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). 4We use 5 translation model scores, distance-based distortion, language model and word penalty. The reordering limit is set to 6 for all experiments. 5For nbest and lattice MBR decoding, </context>
<context position="29779" citStr="Kumar et al., 2009" startWordPosition="5065" endWordPosition="5069"> claims about the benefits of marginalizing over alignments during decoding. We compare the sampler MBR decoder’s performance against MERToptimized Moses run under three different decoding regimes, finding that the sampler does as well or better on 4 out of 5 datasets. Our training and testing pipeline has the advantage of being able to handle a large number of both local and global features so we expect in the future to outperform the standard MERT and dynamic programming-based search pipeline further. As shown in Section 5.2, lattice MBR in some cases leads to a marked drop in performance. (Kumar et al., 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. To account for these cases, they allow their algorithm to back-off to the MAP solution. One possible reason for the drop in performance in our lattice MBR experiments is that the implementation we use does not employ this back-off strategy. Table 3 provides valuable insights as to the merits of the lattice MBR approach versus our own sampling based pipeline. Firstly, whereas with MERT optimized weights, the benefits of lattice MBR are debata</context>
<context position="31974" citStr="Kumar et al., 2009" startWordPosition="5434" endWordPosition="5437">pler’s limited evidence set is enough to give a good estimate of the probability distribution whereas beam-search based MBR needs to scale from using n-best lists to lattices to get equivalent results. Sampling the phrase-based model is expensive, meaning that lattice MBR is still faster (around 4x) to run than sampler MBR. However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al., 2009). In future work, we also intend to look at more efficient ways of generating samples. One possibility is to interleave Gibbs sampling steps using low order ngram language model distributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealin</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of ACL/AFNLP, pages 163– 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>40--51</pages>
<contexts>
<context position="10938" citStr="Li and Eisner, 2009" startWordPosition="1813" endWordPosition="1816"> on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on the work of Smith and Eisner (2006), and involves the addition of an entropic prior to the objective in Equation 5 to give �0=J: p(e,a|f)BLEU6(e) + T.H(p) f)E e D ea , = (hk − F-p(e,a|f)[hk])p(e,a|f) A high value of T leads the optimizer to find weights which describe a fairly flat distribution, whereas a lower value of T pushes the optimizer towards a more peaked distribution. We perform 10 to 20 iterations of SGD at each temperature. In their deterministic annealing formulation, (Smith and Eisner, 2006; Li and Eisner, 2009), express the parameterization of the distribution 0 as γ0� (where γ is the scaling factor) and perform optimization in two steps, the first optimizing 0 and the second optimizing γ. We experimented with this two stage optimization process, but found that simply performing an unconstrained optimization on 0 gave better results. 4.2 Corpus sampling While the objective functions in Equations 5 and 4.1 use a sentence-level variant of BLEU, the model’s test-time performance is evaluated with corpus level BLEU. The lack of correlation between sentence-level BLEU and corpus BLEU is well-known (Chian</context>
<context position="29069" citStr="Li and Eisner, 2009" startWordPosition="4941" endWordPosition="4944"> like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU. Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations have either used sentence-level BLEU (Zens et al., 2007) or a linear approximation to BLEU (Smith and Eisner, 2006; Li and Eisner, 2009). At test time, the sampler works best as an MBR decoder, but also allows us to verify past claims about the benefits of marginalizing over alignments during decoding. We compare the sampler MBR decoder’s performance against MERToptimized Moses run under three different decoding regimes, finding that the sampler does as well or better on 4 out of 5 datasets. Our training and testing pipeline has the advantage of being able to handle a large number of both local and global features so we expect in the future to outperform the standard MERT and dynamic programming-based search pipeline further. </context>
<context position="32496" citStr="Li and Eisner (2009)" startWordPosition="5516" endWordPosition="5519">BR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al., 2009). In future work, we also intend to look at more efficient ways of generating samples. One possibility is to interleave Gibbs sampling steps using low order ngram language model distributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of EMNLP, pages 40–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP,</booktitle>
<pages>593--601</pages>
<contexts>
<context position="6428" citStr="Li et al. (2009)" startWordPosition="1035" endWordPosition="1038">ons necessary. The most common of these approximations is the Viterbi approximation, which simply chooses the most likely derivation (e*, a*). This approximation can be computed in polynomial time via dynamic programming (DP). Though fast and effective for many problems, it has two serious drawbacks for probabilistic inference. First, the error incurred by the Viterbi maximum with respect to the true model maximum is unbounded. Second, the DP solution requires substantial pruning and restricts the use of non-local features. The latter problem persists even in the variational approximations of Li et al. (2009), which attempt to solve the former. 2.1 Gibbs sampling for phrase-based MT An alternate approximate inference method for phrase-based MT without any of the previously mentioned drawbacks is the Gibbs sampler (Geman and Geman, 1984) of Arun et al. (2009) which draws samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, SN1 = (e1, a1) ... (eN, aN), that are drawn from the distribution p(e, a|f). These samples can be used to estimate the expectation of a function h(e, a, f) as follo</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Variational decoding for statistical machine translation. In Proceedings of ACL/AFNLP, pages 593–601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2358" citStr="Och, 2003" startWordPosition="382" endWordPosition="383">n hypotheses e.g n-best list, translation lattice or forest. esis, so these scores must be converted to probabilities in order to apply MBR. In order to perform this conversion, it is first necessary to compute the normalization function Z. Since Z is defined as an intractable sum over all possible translations, it is approximated by summing over the translations in the list. The second step is to find the correct scale factor for the scores using a hyper-parameter search over held-out data. This is needed because the model parameters for the first-pass decoder are normally learnt using MERT (Och, 2003), which is invariant under scaling of the scores. Both these steps are theoretically unsatisfactory methods of estimating the posterior probability distribution since the approximation to Z is an unbounded term and the scaling factor is an artificial way of inducing a probability distribution. Recently, (Tromble et al., 2008; Kumar et al., 2009) have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance. However, these approaches still rely on MERT for training the base model, and in fact introduce several extra pa</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1305" citStr="Papineni et al., 2002" startWordPosition="206" endWordPosition="209">mation thereof. Our approach is theoretically sound and gives better (up to +0.6%BLEU) and more stable results than the standard MERT optimization algorithm. By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods. 1 Introduction According to statistical decision theory, the optimal decision rule for any statistical model is the solution that minimizes its risk (expected loss). This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne, 2004). Since machine translation (MT) models are typically evaluated by BLEU (Papineni et al., 2002), a loss function which rewards partial matches, the MBR solution is to be preferred to the Maximum A Posteriori (MAP) solution. In most statistical MT (SMT) systems, MBR is implemented as a reranker of a list1 of translations generated by a first-pass decoder. This decoder typically assigns unnormalised log probabilities (known as scores) to each translation hypoth1We use the term list to denote any enumerable representation of translation hypotheses e.g n-best list, translation lattice or forest. esis, so these scores must be converted to probabilities in order to apply MBR. In order to perf</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>John Denero</author>
<author>Dan Klein</author>
</authors>
<title>Consensus training for consensus decoding in machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1418--1427</pages>
<contexts>
<context position="32774" citStr="Pauls et al. (2009)" startWordPosition="5560" endWordPosition="5563">tributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in this paper are able to explore an unpruned search space. By using corpus sampling, we can perform minimum risk training with corpus BLEU rather </context>
</contexts>
<marker>Pauls, Denero, Klein, 2009</marker>
<rawString>Adam Pauls, John Denero, and Dan Klein. 2009. Consensus training for consensus decoding in machine translation. In Proceedings of EMNLP, pages 1418–1427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression, and related optimization problems.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>2210--2239</pages>
<contexts>
<context position="10186" citStr="Rose, 1998" startWordPosition="1687" endWordPosition="1688">values of θ. Because of the noise introduced by the sampler, we used stochastic gradient descent (SGD), with a learning rate that gets updated after each step proportionally to difference in successive gradients (Schraudolph, 1999). While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as !g is non-convex, we decided to employ deterministic annealing (Rose, 1998) to smooth the objective function to ensure that the optimizer explored as large a region as possible of the space before it settled on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on the work of Smith and Eisner (2006), and involves the addition of an entropic prior to the objective in Equation 5 to give �0=J: p(e,a|f)BLEU6(e) + T.H(p) f)E e D ea , = (hk − F-p(e,a|f)[hk])p(e,a|f) A high value of T leads the optimizer to find weights which describe a fairly flat distribution, whereas a lower value of T pushes the optimizer towards a more peaked distribution</context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>Kenneth Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. In Proceedings of the IEEE, pages 2210–2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol N Schraudolph</author>
</authors>
<title>Local gain adaptation in stochastic gradient descent.</title>
<date>1999</date>
<tech>Technical Report IDSIA-09-99, IDSIA.</tech>
<contexts>
<context position="9806" citStr="Schraudolph, 1999" startWordPosition="1626" endWordPosition="1627">ptimization of the objective in (5) is facilitated by the fact that it is continuous and differentiable with respect to the model parameters θ to give (6) = (hk − F-p(e,a|f)[hk]) p(e, a|f) Since the gradient is expressed in terms of expectations of feature values, it can easily be calculated using the sampler and then first-order optimization techniques can be applied to find optimal values of θ. Because of the noise introduced by the sampler, we used stochastic gradient descent (SGD), with a learning rate that gets updated after each step proportionally to difference in successive gradients (Schraudolph, 1999). While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as !g is non-convex, we decided to employ deterministic annealing (Rose, 1998) to smooth the objective function to ensure that the optimizer explored as large a region as possible of the space before it settled on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on</context>
</contexts>
<marker>Schraudolph, 1999</marker>
<rawString>Nicol N. Schraudolph. 1999. Local gain adaptation in stochastic gradient descent. Technical Report IDSIA-09-99, IDSIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>787--794</pages>
<contexts>
<context position="10442" citStr="Smith and Eisner (2006)" startWordPosition="1729" endWordPosition="1732">nitial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as !g is non-convex, we decided to employ deterministic annealing (Rose, 1998) to smooth the objective function to ensure that the optimizer explored as large a region as possible of the space before it settled on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on the work of Smith and Eisner (2006), and involves the addition of an entropic prior to the objective in Equation 5 to give �0=J: p(e,a|f)BLEU6(e) + T.H(p) f)E e D ea , = (hk − F-p(e,a|f)[hk])p(e,a|f) A high value of T leads the optimizer to find weights which describe a fairly flat distribution, whereas a lower value of T pushes the optimizer towards a more peaked distribution. We perform 10 to 20 iterations of SGD at each temperature. In their deterministic annealing formulation, (Smith and Eisner, 2006; Li and Eisner, 2009), express the parameterization of the distribution 0 as γ0� (where γ is the scaling factor) and perform </context>
<context position="29047" citStr="Smith and Eisner, 2006" startWordPosition="4937" endWordPosition="4940">rpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU. Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations have either used sentence-level BLEU (Zens et al., 2007) or a linear approximation to BLEU (Smith and Eisner, 2006; Li and Eisner, 2009). At test time, the sampler works best as an MBR decoder, but also allows us to verify past claims about the benefits of marginalizing over alignments during decoding. We compare the sampler MBR decoder’s performance against MERToptimized Moses run under three different decoding regimes, finding that the sampler does as well or better on 4 out of 5 datasets. Our training and testing pipeline has the advantage of being able to handle a large number of both local and global features so we expect in the future to outperform the standard MERT and dynamic programming-based sea</context>
<context position="32368" citStr="Smith and Eisner, 2006" startWordPosition="5495" endWordPosition="5498">riterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al., 2009). In future work, we also intend to look at more efficient ways of generating samples. One possibility is to interleave Gibbs sampling steps using low order ngram language model distributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize t</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proceedings of COLING-ACL, pages 787–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="2684" citStr="Tromble et al., 2008" startWordPosition="429" endWordPosition="432"> is approximated by summing over the translations in the list. The second step is to find the correct scale factor for the scores using a hyper-parameter search over held-out data. This is needed because the model parameters for the first-pass decoder are normally learnt using MERT (Och, 2003), which is invariant under scaling of the scores. Both these steps are theoretically unsatisfactory methods of estimating the posterior probability distribution since the approximation to Z is an unbounded term and the scaling factor is an artificial way of inducing a probability distribution. Recently, (Tromble et al., 2008; Kumar et al., 2009) have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance. However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blu</context>
<context position="7210" citStr="Tromble et al. (2008)" startWordPosition="1172" endWordPosition="1175">iously mentioned drawbacks is the Gibbs sampler (Geman and Geman, 1984) of Arun et al. (2009) which draws samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, SN1 = (e1, a1) ... (eN, aN), that are drawn from the distribution p(e, a|f). These samples can be used to estimate the expectation of a function h(e, a, f) as follows: N 1 L&apos;p(a,e|f)[h] = ��m N�� N i=1 3 Decoding In this work, we are interested in performing MBR decoding with BLEU. We define the MBR decision rule following Tromble et al. (2008): e* = arg max eEex e&apos;EEE � BLEUe(e&apos;)p(e&apos;|f) (4) where EH refers to the hypothesis space from which translations are chosen, EE refers to the evidence space used for calculating risk and BLEUe(e&apos;) is a gain function that indicates the reward of hypothesising e&apos; when the reference solution is e. To perform MBR decoding using the sampler, let the function h in Equation 3 be the indicator function h = δ(a, 6)δ(e, e). Then, Equation 3 provides an estimate of p(a, 6|f), and using h = δ(e, e) marginalizes over all derivations a&apos;, yielding an estimate of p(e|f). MBR is computed at the sentence-level </context>
<context position="18367" citStr="Tromble et al. (2008)" startWordPosition="3080" endWordPosition="3083">tions were used as evidence set for nbest MBR. As Table 1 shows, translation results using MERT optimized weights vary markedly from one sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). 4We use 5 translation model scores, distance-based distortion, language model and word penalty. The reordering limit is set to 6 for all experiments. 5For nbest and lattice MBR decoding, we optimized for the scaling factor using a grid-search on held-out data. For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al. (2008). tuning run to the other, with results varying from a range of 0.3% BLEU to 1.3% BLEU when using Viterbi decoding. We also see that, bar in-domain German to English, MBR decoding gives a small improvement on all other datasets. Surprisingly, lattice MBR only gives improvements on two datasets and actually leads to a drop in performance on the other 3 datasets. We discuss possible reasons for this in Section 6. 5.3 Sentence sampling At training time, the optimization algorithm is initialized with zero weights and the sampler is initialized with a random derivation from Moses. To get rid of any</context>
<context position="31154" citStr="Tromble et al. (2008)" startWordPosition="5300" endWordPosition="5303">better than competing decoding algorithms. This suggests that the unbiased minimum risk training criterion used by the sampler is a better fit for lattice MBR than the MERT criterion, and also that the mismatch between linear and corpus BLEU mentioned before might not be the reason for the results in Table 1. Secondly, we find that sampling MBR matches lattice MBR on the minimum risk trained weights. The MBR sampler uses samples drawn from the distribution as hypothesis and evidence sets, typically 1000 samples for the former and 10000 samples for the latter. In the lattice MBR experiments of Tromble et al. (2008), it is shown that this size of hypothesis set is sufficient. Their evidence set, however, is significantly larger than ours.8Table 3 suggests that, since it is not biased by heuristic pruning, the sampler’s limited evidence set is enough to give a good estimate of the probability distribution whereas beam-search based MBR needs to scale from using n-best lists to lattices to get equivalent results. Sampling the phrase-based model is expensive, meaning that lattice MBR is still faster (around 4x) to run than sampler MBR. However, due to the unified nature of the training and decoding criterion</context>
<context position="32745" citStr="Tromble et al. (2008)" startWordPosition="5554" endWordPosition="5557"> order ngram language model distributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in this paper are able to explore an unpruned search space. By using corpus sampling, we can perform minimum risk trai</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proceedings of EMNLP, pages 620–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Sasa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of training criteria for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>524--532</pages>
<contexts>
<context position="28989" citStr="Zens et al., 2007" startWordPosition="4927" endWordPosition="4930">raining over an unpruned search space. Our proposed corpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU. Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations have either used sentence-level BLEU (Zens et al., 2007) or a linear approximation to BLEU (Smith and Eisner, 2006; Li and Eisner, 2009). At test time, the sampler works best as an MBR decoder, but also allows us to verify past claims about the benefits of marginalizing over alignments during decoding. We compare the sampler MBR decoder’s performance against MERToptimized Moses run under three different decoding regimes, finding that the sampler does as well or better on 4 out of 5 datasets. Our training and testing pipeline has the advantage of being able to handle a large number of both local and global features so we expect in the future to outp</context>
<context position="32388" citStr="Zens et al., 2007" startWordPosition="5499" endWordPosition="5502">, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al., 2009). In future work, we also intend to look at more efficient ways of generating samples. One possibility is to interleave Gibbs sampling steps using low order ngram language model distributions with Metropolis-Hasting steps that use higher order language model distributions. 7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8up to 1081 as per Tromble et al. (2008) 372 similar in spirit to expected BLEU training, but aimed to maximize the expected counts o</context>
</contexts>
<marker>Zens, Hasan, Ney, 2007</marker>
<rawString>Richard Zens, Sasa Hasan, and Hermann Ney. 2007. A systematic comparison of training criteria for statistical machine translation. In Proceedings of EMNLP, pages 524– 532.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>