<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033470">
<title confidence="0.99094">
Text Categorization Using Automatically Acquired Domain Ontology
</title>
<author confidence="0.999416">
Shih-Hung Wu, Tzong-Han Tsai, Wen-Lian Hsu
</author>
<affiliation confidence="0.999066">
Institute of Information Science
</affiliation>
<address confidence="0.6483255">
Academia Sinica
Nankang, Taipei, Taiwan, R.O.C.
</address>
<email confidence="0.987556">
shwu@iis.sinica.edu.tw, thtsai@iis.sinica.edu.tw, hsu@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.994963" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899333333333">
In this paper, we describe ontology-based
text categorization in which the domain
ontologies are automatically acquired
through morphological rules and statistical
methods. The ontology-based approach is
a promising way for general information
retrieval applications such as knowledge
management or knowledge discovery. As
a way to evaluate the quality of domain
ontologies, we test our method through
several experiments. Automatically
acquired domain ontologies, with or
without manual editing, have been used
for text categorization. The results are
quite satisfactory. Furthermore, we have
developed an automatic method to
evaluate the quality of our domain
ontology.
</bodyText>
<sectionHeader confidence="0.997803" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999969212121212">
Domain ontology, consisting of important
concepts and relationships of the concepts in the
domain, is useful in a variety of applications
(Gruber, 1993). However, evaluating the quality of
domain ontologies is not straightforward. Reusing
an ontology for several applications can be a
practical method for evaluating domain ontology.
Since text categorization is a general tool for
information retrieval, knowledge management and
knowledge discovery, we test the ability of
domain ontology to categorize news clips in this
paper.
Traditional IR methods use keyword
distribution form a training corpus to assign
testing document. However, using only keywords
in a training set cannot guarantee satisfactory
results since authors may use different keywords.
We believe that, news clip events are categorized
by concepts, not just keywords. Previous works
shows that the latent semantic index (LSI) method
and the n-gram method give good results for
Chinese news categorization (Wu et al., 1998).
However, the indices of LSI and n-grams are less
meaningful semantically. The implicit rules
acquired by these methods can be understood by
computers, not humans. Thus, manual editing for
exceptions and personalization are not possible
and it is difficult to further reuse these indices for
knowledge management.
With good domain ontology we can identify
the concept structure of sentences in a document.
Our idea is to compile the concepts within
documents in a training set and use these concepts
to understand documents in a testing set. However,
building rigorous domain ontology is laborious
and time-consuming. Previous works suggest that
ontology acquisition is an iterative process, which
includes keyword collection and structure
reorganization. The ontology is revised, refined,
and accumulated by a human editor at each
iteration (Noy and McGuinness, 2001). For
example, in order to find a hyponym of a keyword,
the human editor must observe sentences
containing this keyword and its related hyponyms
(Hearst, 1992). The editor then deduces rules for
finding more hyponyms of this keyword. At each
iteration the editor refines the rules to obtain better
quality pairs of keyword-hyponyms. To speed up
the above labor-intensive approach, semi-
automatic approaches have been designed in
which a human editor only has to verify the results
of the acquisition (Maedche and Staab, 2000).
A knowledge representation framework,
Information Map (InfoMap) in our previous work
(Hsu et al., 2001), has been designed to integrate
various linguistic, common-sense and domain
knowledge. InfoMap is designed to perform
natural language understanding, and applied to
many application domains, such as question
answering (QA), knowledge management and
organization memory (Wu et al., 2002), and shows
good results. An important characteristic of
InfoMap is that it extracts events from a sentence
by capturing the topic words, usually subject-verb
pairs or hypernym-hyponym pairs, which are
defined in the domain ontology.
</bodyText>
<figureCaption confidence="0.977161">
Figure 1. Ontology Structure for CNA News
</figureCaption>
<bodyText confidence="0.999936285714286">
We shall review the InfoMap ontology
framework in Section 2. The ontology acquisition
process and extraction rules will be introduced in
Section 3. We describe ontology-based text
categorization in Section 4. Experimental results
are reported in Section 5. We conclude our work
in Section 6.
</bodyText>
<sectionHeader confidence="0.994794" genericHeader="method">
2. Information Map
</sectionHeader>
<bodyText confidence="0.999836857142857">
InfoMap can serve as domain ontology as well as
an inference engine. InfoMap is designed for NLP
applications; its basic function is to identify the
event structure of a sentence. We shall briefly
describe InfoMap in this section. Figure 1 gives
example ontology of the Central News Agency
(CNA), the target in our experiment.
</bodyText>
<subsectionHeader confidence="0.979677">
2.1 InfoMap Structure Format
</subsectionHeader>
<bodyText confidence="0.9998567">
As a domain ontology, InfoMap consists of
domain concepts and their related sub-concepts
such as categories, attributes, activities. The
relationships of a concept and its associated sub-
concepts form a tree-like taxonomy. InfoMap also
defines references to connect nodes from different
branches which serves to integrate these
hierarchical concepts into a network. InfoMap not
only classifies concepts, but also connects the
concepts by defining the relationships among them.
</bodyText>
<figureCaption confidence="0.999199">
Figure 2. Skeleton of the Ontology Structure of
</figureCaption>
<figure confidence="0.998426647058824">
InfoMap
Concept A
Category
Concept A&apos;
(Sub-concept of
concept A)
Attribute
Concept B
(relavant but not
belong to concept A)
Action
Concept C
(An activity of
concept A)
Legend
Function node
Concept node
</figure>
<bodyText confidence="0.998984285714286">
In InfoMap, concept nodes represent concepts
and function nodes represent the relationships
between concepts. The root node of a domain is
the name of the domain. Following the root node,
important topics are stored in a hierarchical order.
These topics have sub-categories that list related
sub-topics in a recursive fashion. Figure 1 is a
partial view of the domain ontology of the CNA.
Under each domain there are several topics and
each topic might have sub-concepts and associated
attributes. In this example, note that, the domain
ontology is automatically acquired from a domain
corpus, hence the quality is poor. Figure 2 shows
the skeleton order of a concept using InfoMap.
</bodyText>
<subsectionHeader confidence="0.99982">
2.2 Event Structure
</subsectionHeader>
<bodyText confidence="0.999983782608695">
Since concepts that are semantically related are
often clustered together, one can use InfoMap to
discern the main event structure in a natural
language sentence. The process of identifying the
event structure, we call a firing mechanism, which
matches words in a sentence to both concepts and
relationships in InfoMap.
Suppose keywords of concept A and its sub-
concept B (or its hyponyms) appear in a sentence.
It is likely that the author is describing an event “B
of A”. For example, when the words “tire” and
“car” appear in a sentence, normally this sentence
would be about the tire of a car (not tire in the
sense of fatigue). Therefore, a word-pair with a
semantic relationship can give more concrete
information than two words without a semantic
relationship. Of course, certain syntactic
constraints also need to be satisfied. This can be
extended to a noun-verb pair or a combination of
noun, verb and adjective. We call such words in a
sentence an event structure. This mechanism
seems to be especially effective for Chinese
sentences.
</bodyText>
<subsectionHeader confidence="0.999025">
2.3 Domain Speculation
</subsectionHeader>
<bodyText confidence="0.999527823529412">
With the help of domain ontologies, one can
categorize a piece of text into a specific domain by
categorizing each individual sentence within the
text. There are many different ways to use domain
ontology to categorize text. It can be used as a
dictionary, as a keyword lists and as a structure to
identify NL events. Take a single sentence for
example. We first use InfoMap as a dictionary to
do word segmentation (necessary for Chinese
sentences) in which the ambiguity can be resolved
by checking the domain topic in the ontology.
After words are segmented, we can examine the
distribution of these words in the ontology and
effectively identify the densest cluster. Thus, we
can use InfoMap to identify the domains of the
sentences and their associated keywords. Section
4.1 will further elaborate on this.
</bodyText>
<sectionHeader confidence="0.870625" genericHeader="method">
3. Automatic Ontology Acquisition
</sectionHeader>
<bodyText confidence="0.99961">
The automatically domain ontology acquisition
from a domain corpus has three steps:
</bodyText>
<listItem confidence="0.998671333333333">
1. Identify the domain keywords.
2. Find the relative concepts.
3. Merge the correlated activities.
</listItem>
<subsectionHeader confidence="0.996996">
3.1 Domain Keyword Identification
</subsectionHeader>
<bodyText confidence="0.999968066666667">
The first step of automatic domain ontology
acquisition is to identify domain keywords.
Identifying Chinese unknown words is difficult
since the word boundary is not marked in Chinese
corpus. According to an inspection of a 5 million
word Chinese corpus (Chen et al., 1996), 3.51% of
words are not listed in the CKIP lexicon (a
Chinese lexicon with more than 80,000 entries).
We use reoccurrence frequency and fan-out
numbers to characterize words and their
boundaries according to PAT-tree (Chien, 1999).
We then adopt the TF/IDF classifier to choose
domain keywords. The domain keywords serve as
the seed topics in the ontology. We then apply
SOAT to automatically obtain related concepts.
</bodyText>
<subsectionHeader confidence="0.992925">
3.2 SOAT
</subsectionHeader>
<bodyText confidence="0.970870771428571">
To build the domain ontology for a new domain,
we need to collect domain keywords and concepts
by finding relationships among keywords. We
adopt a semi-automatic domain ontology
acquisition tool (SOAT, Wu et al., 2002), to
construct a new ontology from a domain corpus.
With a given domain corpus, SOAT can build a
prototype of the domain ontology.
InfoMap uses two major relationships among
concepts: taxonomic relationships (category and
synonym) and non-taxonomic relationships
(attribute and action). SOAT defines rules, which
consist of patterns of keywords and variables, to
capture these relationships. The extraction rules in
SOAT are morphological rules constructed from
part-of-speech (POS) tagged phrase structure.
Here we briefly introduce the SOAT process:
Input: domain corpus with the POS tag
Output: domain ontology prototype
Steps:
1 Select a keyword (usually the name of
the domain) in the corpus as the seed to
form a potential root set R
2 Begin the following recursive process:
2.1 Pick a keyword A as the root from R
2.2 Find a new related keyword B of the
root A by extraction rules and add it
into the domain ontology according to
the rules
2.3 If there is no more related keywords,
remove A from R
2.4 Put B into the potential root set
Repeat step 2 until either R becomes
empty or the total number of nodes reach
a threshold
</bodyText>
<subsectionHeader confidence="0.998617">
3.3 Morphological Rules
</subsectionHeader>
<bodyText confidence="0.9999095">
To find the relative words of a keyword, we check
the context in the sentence from which the
keyword appears. We can then find attributes or
hyponyms of the keyword. For example, in a
sentence, we find a noun in front of a keyword
(say, computer) may form a specific kind of
concept (say, quantum computer). A noun (say,
connector) followed by “of” and a keyword may
be an attribute of the keyword, (say, connector of
computer). See (Wu et al., 2002) for details.
</bodyText>
<subsectionHeader confidence="0.950385">
3.4 Ontology Merging
</subsectionHeader>
<bodyText confidence="0.999897230769231">
Ontologies can be created by merging different
resources. One NLP resource that we will merge
into our domain ontology is the noun-verb event
frame (NVEF) database (Tsai and Hsu, 2002).
NVEF is a collection of permissible noun-verb
sense-pairs that appear in general domain corpora.
The noun will be the subject or object of the verb.
This noun-verb sense-pair collection is domain
independent. We can use nouns as domain
keywords and find their correlated verbs. Adding
these verbs into the domain ontology makes the
ontology more suitable for NLP. The correlated
verbs are added under the action function node.
</bodyText>
<sectionHeader confidence="0.968326" genericHeader="method">
4. Ontology-Based Text Categorization
</sectionHeader>
<bodyText confidence="0.9999465">
To incorporate the domain ontology into a text
categorization, we have to adjust both the training
process and testing process. Section 4.1 describes
how to make use of the ontology and the event
structure during the training process. Section 4.2
describes how to use ontology to perform domain
speculation. Section 4.3 describes how to
categorize news clippings.
</bodyText>
<subsectionHeader confidence="0.976248">
4.1 Feature and Threshold Selection
</subsectionHeader>
<bodyText confidence="0.998288666666667">
With the event structure matched (fired) in the
domain ontology, we have more features with
which to index a text. To select useful features and
a proper threshold, we apply Microsoft Decision
Tree Algorithm to determine a path’s relevance as
this algorithm can extract human interpretable
rules (Soni et al., 2000).
Features of the event structure include event
structure score, node score, fired node level, and
node type. During the training process, we record
all features of the event structure fired by the news
clippings in the domain-categorized training
corpus. The decision tree shows that a threshold of
0.85 is sufficient to evaluate event structure scores.
We use event structure score to determine if the
path is relevant. According to Figure 3, if the
threshold of true probability is 85%, then the event
structure score (Pathscore in the figure) should be
65.75. And the relevance of a path p is true if p
falls in a node on the decision tree whose ratio of true
instance is greater than λ .
</bodyText>
<figureCaption confidence="0.974182">
Figure 3. Threshold selection using decision
</figureCaption>
<bodyText confidence="0.670777">
tree
</bodyText>
<subsectionHeader confidence="0.981216">
4.2 Domain Speculation
</subsectionHeader>
<bodyText confidence="0.999635857142857">
The goal of domain speculation is to categorize a
sentence S into a domain Dj according to the
combined score of the keywords and the event
structure in sentence S. We first calculate the
similarity score of S and Dj. The keyword score
and the event structure score are calculated
independently.
</bodyText>
<equation confidence="0.970518666666667">
SimScore(Dj , S) = Keyword_ Score(Dj , S) +
α * EventStructure
_ Score(Dj,S)
</equation>
<bodyText confidence="0.999978307692308">
We use the TF/IDF classifier (Salton, 1989) to
calculate the Keyword_Score of a sentence as
follows. First, we use a segmentation module to
split a Chinese sentence into words. The TF/IDF
classifier represents a domain as a weighted vector,
Dj =( wj1, wj2,..., wjn), where n is the number of
words in this domain and wk is the weight of word
k. wk is defined as nfjk * idfjk, where nfjk is the term
frequency (i.e., the number of times the word wk
occurs in the domain j). Let DFk be the number of
domains in which word k appears and |D |the total
number of domains. idfk, the inverse document
frequency, is given by:
</bodyText>
<equation confidence="0.9976028">
 ||
D
idf = log( ) .
k DF
k
</equation>
<bodyText confidence="0.992704714285714">
This weighting function assigns high values to
domain-specific words, i.e. words which appear
frequently in one domain and infrequently in
others. Conversely, it will assign low weights to
words appearing in many domains. The similarity
between a domain j and a sentence represented by
a vector Di is measured by the following cosine:
</bodyText>
<equation confidence="0.983037181818182">
Score D S
( , )
j
= Sim D D
( ,
j i )
∑ n
k jk ik
w w
=1
) 2 ∑k=1 (wik) 2
</equation>
<bodyText confidence="0.999895">
The event structure score is calculated by
InfoMap Engine. First, find all the nodes in
ontology that match the words in the sentence.
Then determine if there is any concept-attribute
pair, or hypernym-hyponym pair. Finally, assign a
score to each fired event structure according to the
string length of words that match the nodes in the
ontology. The selected event structure is the one
with the highest score.
</bodyText>
<equation confidence="0.9782095">
EventStructure_ Score(Dj , S)
= ∑
max StringLength(keywords(Dj ∩ S))
Event
</equation>
<subsectionHeader confidence="0.949291">
4.3 News Categorization
</subsectionHeader>
<bodyText confidence="0.999964111111111">
Upon receiving a news clipping C, we split it into
sentences Si. The sentences are scored and
categorized according to domains. Thus, every
sentence has an individual score for each domain
Score(D, Si). We add up these scores of every
sentence in the text according to domain, giving us
total domain scores for the entire text. The
domain which has the highest score is the domain
into which the text is categorized.
</bodyText>
<equation confidence="0.99191875">
Domain ( ) arg max (∑ ( ,
C = Score D Si
D S C
∈ ))
</equation>
<sectionHeader confidence="0.8716145" genericHeader="method">
5. Refining Ontology through the Text
Categorization Application
</sectionHeader>
<bodyText confidence="0.999942153846154">
The advantage of ontology compared to other
implicit knowledge representation mechanism is
that it can be read, interpreted and edited by
human. Noise and errors can be detected and
refined, especially for the automatically acquired
ontology, in order to obtain a better ontology.
Another advantage of allowing human editing is
that the ontology produced can be shared by
various applications, such as from a QA system to
a knowledge management system. In contrast, the
implicit knowledge represented in LSI or other
representations is difficult to port from one
application to another.
</bodyText>
<equation confidence="0.913009">
Keyword
_
=
∑ n (w
k jk
= 1
</equation>
<bodyText confidence="0.983461913043479">
In this section, we show how the human
editing feature improves news categorization. First,
we can identify a common error type: ambiguity;
then, depending on the degree of categorization
ambiguity, the system can report to a human editor
the possible errors of certain concepts in the
domain ontology as clues.
Consider the following common error type:
event structure ambiguity. Some event structures
are located in several domains due to the noise of
training data. We define two formulas to find such
event structures. The ambiguity of an event
structure E(Si) is proportional to the number of
domains in which it appears, and inversely
proportional to its event score, where Si are the
sentences that fire event E.
GlobalCategorizationAmiguityFactor(E(Si) )
= number of domains fired by
Si/average( EventScore(Si) )
We also measure the similarity between every
two event structures by calculating the co-
occurrence multiplied by the global categorization
ambiguity factor.
</bodyText>
<equation confidence="0.983749666666667">
GlobalCategorizationAmbiguityij (E i, E j)
=Co-occurrence(E i, E j) *
GlobalCategorizationAmbiguityFactor(E j)
</equation>
<bodyText confidence="0.9989415">
When the GlobalCategorizationAmbiguity of an
event structure E i exceeds a threshold, the system
will suggest that the human editor refine the
ontology.
</bodyText>
<sectionHeader confidence="0.997019" genericHeader="evaluation">
6. Experiments
</sectionHeader>
<bodyText confidence="0.99994925">
To assess the power of domain identification of
ontology, we test the text categorization ability on
two different corpora. The ontology of the first
experiment is edited manually; the ontology of the
second experiment is automatically acquired. And
we also conduct an experiment on the effect of
human editing of the automatically acquired
ontology.
</bodyText>
<subsectionHeader confidence="0.99877">
6.1 Single Sentence Test
</subsectionHeader>
<bodyText confidence="0.997791">
We test 9,143 sentences, edited manually for a QA
system. The accuracy is 94%. These sentences are
questions in the financial domain. Because the
sentence topics are quite focused, the accuracy is
very high. See Table 1.
</bodyText>
<tableCaption confidence="0.99078">
Table 1. Sentence Categorization Accuracy
</tableCaption>
<table confidence="0.806871">
Domain # Sentence # Accuracy
24 9143 94.01%
</table>
<subsectionHeader confidence="0.996924">
6.2 News Clippings Collection
</subsectionHeader>
<bodyText confidence="0.999800428571429">
The second experiment that we conduct is news
categorization. We collect daily news from China
News Agency (CNA) ranging from 1991 to 1999.
Each news clipping is short with 352 Chinese
characters (about 150 words) on the average.
There are more than thirty domains and we choose
10 major categories for the experiment.
</bodyText>
<subsectionHeader confidence="0.995308">
6.3 10 Categories News Categorization
</subsectionHeader>
<bodyText confidence="0.9999918">
Our ten categories are: domestic arts and education
(DD), foreign affairs (FA), finance report (FX),
domestic health (HD), Taiwan local news (LD),
Taiwan sports (LD), domestic military (MD),
domestic politics (PD), Taiwan stock markets (SD),
and weather report (WE). From each category, we
choose the first 100 news clippings as the training
set and the following 100 news clippings as the
testing set. After data cleansing, the total training
set has 979 news clippings, with 27,951 nodes and
less than 10,000 distinct words. The training set
for which domain ontologies are automatically
acquired is shown in Table 2. A partial view of
this ontology is in Figure 1.
The result of text categorization based on this
automatically acquired domain ontology is shown
in Table 5, which contains the recall and precision
for each domain. Note that, without the help of the
event structure, the macro average f-score is
85.16%. Even the total number of domain key
concepts is less than 10,000 words (instead of
100,000 words in standard dictionary), we can still
obtain a good categorization result. With the help
of event structure, the macro average f-score is
85.55%.
</bodyText>
<subsectionHeader confidence="0.999505">
6.4 Human Editing
</subsectionHeader>
<bodyText confidence="0.999432576923077">
To verify the refinement method, we conduct
an experiment to compare the result of using
automatically acquired domain ontology and that
of limited human editing (on only one domain
ontology). After the training process, we use
domain ontologies to classify the training data,
and to calculate the global categorization
ambiguity factor formula in order to obtain
ambiguous event structure pairs as candidates for
human editing. For simplicity, we restrict the
action of refinement to deletion. It takes a human
editor one half day to finish the task and delete
0.62% nodes (172 out of 27,951 nodes). In the
testing phase, we select 928 new news clippings as
the testing set. Table 3 shows the results from
before and after human editing. Due to time
constraints, we only edit the part of the ontology
that might affect domain DD. The recall and
precision of domain DD increase as well as both
the average recall and average precision. In
addition, the recall of domains having higher
correlation with DD, such as PD and FA,
decreases. Apparently, the event structures that
mislead the categorization system to theses
domain have mostly been deleted. The experiment
result is very consistent with our intuition.
</bodyText>
<tableCaption confidence="0.997108">
Table 2. Ten Category training set CNA news
</tableCaption>
<table confidence="0.999631461538462">
Domain Training set size
Doc# Char#
DD 98 41870
FA 97 38143
FX 100 30771
HD 96 39818
JD 107 35381
LD 96 36957
MD 89 32903
PD 100 43152
SD 109 33030
WE 87 30457
total 979 362,482
</table>
<sectionHeader confidence="0.85149" genericHeader="conclusions">
7. Discussions and Conclusions
</sectionHeader>
<bodyText confidence="0.998883976190476">
Compared to an ordinary n-gram dictionary, our
ontology dictionary is quite small (roughly 10%)
but records certain important relations between
keywords.
Our goal is to generate rules that are human
readable via ontology. The experiment result
shows that event structure enhances text
categorization, even when the domain ontology is
automatically acquired without human verification.
To improve our ontological approach, our future
work are: 1. human editing in more domains; 2.
enlarge our dictionary by merging existing
ontologies, e.g., the names of countries, capitals
and important persons, which are absent from the
training corpus; 3. incorporate more sense pairs
such as N-A (noun-adjective), Adv-V (adverb-
verb); 4. use machine learning model on the
weighting of the ontological features.
Previous research shows that some NLP
techniques can improve information retrieval.
Ontology-based IR is one of them. However, the
construction of domain ontology is too costly.
Thus, automatic acquisition of domain ontology is
becoming an interesting research topic. Previous
research shows that implicit rules (such as LSI, N-
gram dictionaries) learned from a training corpus
give better results than explicit rules generated by
humans. However, it is hard to use these implicit
rules or to combine them with other resources for
further refinement. With the help of domain
ontology, we can automatically generate rules that
humans can understand. Since humans and
machines can maintain ontology independently,
the ontological approach can be applied more
easily to other IR applications. Ontologies from
different sources can be merged into the domain
ontology. The system should include an editing
interface that human thoughts can be incorporated
to complement statistical rules. With semi-
automatically acquired domain ontology, text
categorization can be adapted to personal
preferences.
</bodyText>
<sectionHeader confidence="0.924992" genericHeader="references">
8. References
</sectionHeader>
<reference confidence="0.999123240740741">
Chen, K.J., C.R. Huang, L.P. Chang &amp; H.L. Hsu,
SINICA CORPUS: Design Methodology for
Balanced Corpora, in Proceedings of PACLIC
11th Conference, pp.167-176, 1996.
Chien, L.F., PAT-tree-based Adaptive keyphrase
extraction for Intelligent Chinese Information
Retrieval, Information Processing and
Management, Vol. 35, pp. 501-521, 1999.
Gruber, T.R. (1993), A translation approach to
portable ontologies. Knowledge Acquisition,
5(2), pp. 199-220, 1993.
Hearst, M.A. (1992), Automatic acquisition of
hyponyms from large text corpora. In
COLING-92, pp. 539-545.
Hsu, W.L., Wu, S.H. and Chen, Y.S., Event
Identification Based On The Information Map -
INFOMAP, in Natural Language Processing
and Knowledge Engineering Symposium of the
IEEE Systems, Man, and Cybernetics
Conference, Tucson, Arizona, USA, 2001.
Maedche, A. and Staab, S. (2000), Discovering
Conceptual Relationships from Text. In: Horn,
W. (ed.): ECAI 2000. Proceedings of the 14th
European Conference on Artificial Intelligence,
IOS Press, Amsterdam.
Noy, N.F. and McGuinness D.L. (2001), Ontology
Development 101: A Guide to Creating Your
First Ontology, SMI technical report SMI-
2001-0880, Stanford Medical Informatics.
Salton, G., Automatic Text Processing, Addison-
Wesley, Massachusetts, 1989.
Soni, S, Tang, Z. and Yang, J., “Microsoft
Performance Study of Microsoft Data Mining
Algorithms”, UniSys, 2000/12.
Tsai, J.L. and Hsu, W.L., “Applying an NVEF
Word-Pair Identifier to the Chinese Syllable-to-
Word Conversion Problem,” COLING-02,
Taipei, ACM press, 2002.
Wu, S.H. and Hsu, W.L., SOAT: A Semi-
Automatic Domain Ontology Acquisition Tool
from Chinese Corpus, COLING-02, Taipei,
ACM press, 2002.
Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu, W.L.,
FAQ-centered Organizational Memory, in
Nada Matta and Rose Dieng-Kuntz (ed.),
Knowledge Management and Organizational
Memories, Kluwer Academic Publishers,
Boston, 2002.
Wu, S.H., Yang, P.C. and Soo, V.W., An
Assessment on Character-based Chinese News
Filtering Using Latent Semantic Indexing,
Computational Linguistics &amp; Chinese
Language Processing, Vol. 3, no.2, August
1998.
</reference>
<tableCaption confidence="0.996053">
Table 3. Experiment result of CNA news categorization
</tableCaption>
<table confidence="0.999471970588235">
Domain # of nodes #of nodes TF/IDF(baseline) TF/IDF+Event TF/IDF+Event Structure The different between
automatically deleted in Structure(first with Human Editing (second improvement) and
acquired human editing improvement) (second improvement) (first improvement)
Before After # % P% R% F% P% R% F% P% R% F% P+% R+% F+%
DD 4616 4574 42 0.91 72.9 82.9 77.6 74.04 81.91 77.78 74.29 82.98 78.39 0.25 1.07 0.61
FA 8352 8348 4 0.05 0 8 1 71.32 95.83 81.78 76.67 95.83 85.19
FX 44 44 0 0.00 75.8 94.7 84.2 100 100 100 100 100 100
HD 3357 3348 9 0.27 3 9 6 80.21 87.50 83.70 78.79 88.64 83.42
JD 1854 1846 8 0.43 100 100 100 87.18 73.91 80 87.84 70.65 78.31
LD 2925 2831 94 3.21 78.7 88.6 83.4 90.36 77.32 83.33 88.51 79.38 83.70
MD 2010 1999 11 0.55 9 4 2 95.71 68.37 79.76 97.26 72.45 83.04
PD 3199 3195 4 0.13 88 71.7 79.0 70.43 72.32 71.37 66.67 69.64 68.12
SD 585 585 0 0.00 87.6 4 4 100 100 100 100 100 100
WE 1009 1009 0 0.00 4 80.4 83.8 95.74 100 97.83 95.74 100 97.83
95.5 1 7
9 66.3 78.3
65.8 3 1
1 68.7 67.2
100 5 5
95.7 4 100 100
100 97.8
3
5.35 0.00 3.41
0.00 0.00 0.00
-1.42 1.14 -0.28
0.66 -3.26 -1.69
-1.85 2.06 0.37
1.55 4.08 3.28
-3.76 -2.68 -3.25
0.00 0.00 0.00
0.00 0.00 0.00
Total 27951 27779 172 0.62
Macro 86.0 85.3 85.1 86.50 85.72 85.55 86.58 85.96 85.80 0.08 0.24 0.25
Average 3 6 6
</table>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539100">
<title confidence="0.983054">Text Categorization Using Automatically Acquired Domain Ontology</title>
<author confidence="0.997189">Shih-Hung Wu</author>
<author confidence="0.997189">Tzong-Han Tsai</author>
<author confidence="0.997189">Wen-Lian</author>
<affiliation confidence="0.8328885">Institute of Information Academia</affiliation>
<address confidence="0.953533">Nankang, Taipei, Taiwan,</address>
<email confidence="0.988816">shwu@iis.sinica.edu.tw,thtsai@iis.sinica.edu.tw,hsu@iis.sinica.edu.tw</email>
<abstract confidence="0.992764473684211">In this paper, we describe ontology-based text categorization in which the domain ontologies are automatically acquired through morphological rules and statistical methods. The ontology-based approach is a promising way for general information retrieval applications such as knowledge management or knowledge discovery. As a way to evaluate the quality of domain ontologies, we test our method through several experiments. Automatically acquired domain ontologies, with or without manual editing, have been used for text categorization. The results are quite satisfactory. Furthermore, we have developed an automatic method to evaluate the quality of our domain ontology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K J Chen</author>
<author>C R Huang</author>
<author>L P Chang</author>
<author>H L Hsu</author>
</authors>
<date>1996</date>
<booktitle>SINICA CORPUS: Design Methodology for Balanced Corpora, in Proceedings of PACLIC 11th Conference,</booktitle>
<pages>167--176</pages>
<contexts>
<context position="8512" citStr="Chen et al., 1996" startWordPosition="1296" endWordPosition="1299"> of the sentences and their associated keywords. Section 4.1 will further elaborate on this. 3. Automatic Ontology Acquisition The automatically domain ontology acquisition from a domain corpus has three steps: 1. Identify the domain keywords. 2. Find the relative concepts. 3. Merge the correlated activities. 3.1 Domain Keyword Identification The first step of automatic domain ontology acquisition is to identify domain keywords. Identifying Chinese unknown words is difficult since the word boundary is not marked in Chinese corpus. According to an inspection of a 5 million word Chinese corpus (Chen et al., 1996), 3.51% of words are not listed in the CKIP lexicon (a Chinese lexicon with more than 80,000 entries). We use reoccurrence frequency and fan-out numbers to characterize words and their boundaries according to PAT-tree (Chien, 1999). We then adopt the TF/IDF classifier to choose domain keywords. The domain keywords serve as the seed topics in the ontology. We then apply SOAT to automatically obtain related concepts. 3.2 SOAT To build the domain ontology for a new domain, we need to collect domain keywords and concepts by finding relationships among keywords. We adopt a semi-automatic domain ont</context>
</contexts>
<marker>Chen, Huang, Chang, Hsu, 1996</marker>
<rawString>Chen, K.J., C.R. Huang, L.P. Chang &amp; H.L. Hsu, SINICA CORPUS: Design Methodology for Balanced Corpora, in Proceedings of PACLIC 11th Conference, pp.167-176, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Chien</author>
</authors>
<title>PAT-tree-based Adaptive keyphrase extraction for Intelligent Chinese Information Retrieval,</title>
<date>1999</date>
<journal>Information Processing and Management,</journal>
<volume>35</volume>
<pages>501--521</pages>
<contexts>
<context position="8743" citStr="Chien, 1999" startWordPosition="1334" endWordPosition="1335">words. 2. Find the relative concepts. 3. Merge the correlated activities. 3.1 Domain Keyword Identification The first step of automatic domain ontology acquisition is to identify domain keywords. Identifying Chinese unknown words is difficult since the word boundary is not marked in Chinese corpus. According to an inspection of a 5 million word Chinese corpus (Chen et al., 1996), 3.51% of words are not listed in the CKIP lexicon (a Chinese lexicon with more than 80,000 entries). We use reoccurrence frequency and fan-out numbers to characterize words and their boundaries according to PAT-tree (Chien, 1999). We then adopt the TF/IDF classifier to choose domain keywords. The domain keywords serve as the seed topics in the ontology. We then apply SOAT to automatically obtain related concepts. 3.2 SOAT To build the domain ontology for a new domain, we need to collect domain keywords and concepts by finding relationships among keywords. We adopt a semi-automatic domain ontology acquisition tool (SOAT, Wu et al., 2002), to construct a new ontology from a domain corpus. With a given domain corpus, SOAT can build a prototype of the domain ontology. InfoMap uses two major relationships among concepts: t</context>
</contexts>
<marker>Chien, 1999</marker>
<rawString>Chien, L.F., PAT-tree-based Adaptive keyphrase extraction for Intelligent Chinese Information Retrieval, Information Processing and Management, Vol. 35, pp. 501-521, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R Gruber</author>
</authors>
<title>A translation approach to portable ontologies.</title>
<date>1993</date>
<journal>Knowledge Acquisition,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>199--220</pages>
<contexts>
<context position="1110" citStr="Gruber, 1993" startWordPosition="143" endWordPosition="144">eral information retrieval applications such as knowledge management or knowledge discovery. As a way to evaluate the quality of domain ontologies, we test our method through several experiments. Automatically acquired domain ontologies, with or without manual editing, have been used for text categorization. The results are quite satisfactory. Furthermore, we have developed an automatic method to evaluate the quality of our domain ontology. 1. Introduction Domain ontology, consisting of important concepts and relationships of the concepts in the domain, is useful in a variety of applications (Gruber, 1993). However, evaluating the quality of domain ontologies is not straightforward. Reusing an ontology for several applications can be a practical method for evaluating domain ontology. Since text categorization is a general tool for information retrieval, knowledge management and knowledge discovery, we test the ability of domain ontology to categorize news clips in this paper. Traditional IR methods use keyword distribution form a training corpus to assign testing document. However, using only keywords in a training set cannot guarantee satisfactory results since authors may use different keywor</context>
</contexts>
<marker>Gruber, 1993</marker>
<rawString>Gruber, T.R. (1993), A translation approach to portable ontologies. Knowledge Acquisition, 5(2), pp. 199-220, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING-92,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="2978" citStr="Hearst, 1992" startWordPosition="423" endWordPosition="424"> is to compile the concepts within documents in a training set and use these concepts to understand documents in a testing set. However, building rigorous domain ontology is laborious and time-consuming. Previous works suggest that ontology acquisition is an iterative process, which includes keyword collection and structure reorganization. The ontology is revised, refined, and accumulated by a human editor at each iteration (Noy and McGuinness, 2001). For example, in order to find a hyponym of a keyword, the human editor must observe sentences containing this keyword and its related hyponyms (Hearst, 1992). The editor then deduces rules for finding more hyponyms of this keyword. At each iteration the editor refines the rules to obtain better quality pairs of keyword-hyponyms. To speed up the above labor-intensive approach, semiautomatic approaches have been designed in which a human editor only has to verify the results of the acquisition (Maedche and Staab, 2000). A knowledge representation framework, Information Map (InfoMap) in our previous work (Hsu et al., 2001), has been designed to integrate various linguistic, common-sense and domain knowledge. InfoMap is designed to perform natural lan</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M.A. (1992), Automatic acquisition of hyponyms from large text corpora. In COLING-92, pp. 539-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Hsu</author>
<author>S H Wu</author>
<author>Y S Chen</author>
</authors>
<title>Event Identification Based On The Information Map -INFOMAP,</title>
<date>2001</date>
<booktitle>in Natural Language Processing and Knowledge Engineering Symposium of the IEEE Systems, Man, and Cybernetics Conference,</booktitle>
<location>Tucson, Arizona, USA,</location>
<contexts>
<context position="3448" citStr="Hsu et al., 2001" startWordPosition="494" endWordPosition="497">e, in order to find a hyponym of a keyword, the human editor must observe sentences containing this keyword and its related hyponyms (Hearst, 1992). The editor then deduces rules for finding more hyponyms of this keyword. At each iteration the editor refines the rules to obtain better quality pairs of keyword-hyponyms. To speed up the above labor-intensive approach, semiautomatic approaches have been designed in which a human editor only has to verify the results of the acquisition (Maedche and Staab, 2000). A knowledge representation framework, Information Map (InfoMap) in our previous work (Hsu et al., 2001), has been designed to integrate various linguistic, common-sense and domain knowledge. InfoMap is designed to perform natural language understanding, and applied to many application domains, such as question answering (QA), knowledge management and organization memory (Wu et al., 2002), and shows good results. An important characteristic of InfoMap is that it extracts events from a sentence by capturing the topic words, usually subject-verb pairs or hypernym-hyponym pairs, which are defined in the domain ontology. Figure 1. Ontology Structure for CNA News We shall review the InfoMap ontology </context>
</contexts>
<marker>Hsu, Wu, Chen, 2001</marker>
<rawString>Hsu, W.L., Wu, S.H. and Chen, Y.S., Event Identification Based On The Information Map -INFOMAP, in Natural Language Processing and Knowledge Engineering Symposium of the IEEE Systems, Man, and Cybernetics Conference, Tucson, Arizona, USA, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maedche</author>
<author>S Staab</author>
</authors>
<title>Discovering Conceptual Relationships from Text. In:</title>
<date>2000</date>
<booktitle>ECAI 2000. Proceedings of the 14th European Conference on Artificial Intelligence,</booktitle>
<editor>Horn, W. (ed.):</editor>
<publisher>IOS Press,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="3343" citStr="Maedche and Staab, 2000" startWordPosition="479" endWordPosition="482"> is revised, refined, and accumulated by a human editor at each iteration (Noy and McGuinness, 2001). For example, in order to find a hyponym of a keyword, the human editor must observe sentences containing this keyword and its related hyponyms (Hearst, 1992). The editor then deduces rules for finding more hyponyms of this keyword. At each iteration the editor refines the rules to obtain better quality pairs of keyword-hyponyms. To speed up the above labor-intensive approach, semiautomatic approaches have been designed in which a human editor only has to verify the results of the acquisition (Maedche and Staab, 2000). A knowledge representation framework, Information Map (InfoMap) in our previous work (Hsu et al., 2001), has been designed to integrate various linguistic, common-sense and domain knowledge. InfoMap is designed to perform natural language understanding, and applied to many application domains, such as question answering (QA), knowledge management and organization memory (Wu et al., 2002), and shows good results. An important characteristic of InfoMap is that it extracts events from a sentence by capturing the topic words, usually subject-verb pairs or hypernym-hyponym pairs, which are define</context>
</contexts>
<marker>Maedche, Staab, 2000</marker>
<rawString>Maedche, A. and Staab, S. (2000), Discovering Conceptual Relationships from Text. In: Horn, W. (ed.): ECAI 2000. Proceedings of the 14th European Conference on Artificial Intelligence, IOS Press, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N F Noy</author>
<author>D L McGuinness</author>
</authors>
<title>Ontology Development 101: A Guide to Creating Your First Ontology, SMI technical report SMI2001-0880, Stanford Medical Informatics.</title>
<date>2001</date>
<contexts>
<context position="2819" citStr="Noy and McGuinness, 2001" startWordPosition="395" endWordPosition="398"> is difficult to further reuse these indices for knowledge management. With good domain ontology we can identify the concept structure of sentences in a document. Our idea is to compile the concepts within documents in a training set and use these concepts to understand documents in a testing set. However, building rigorous domain ontology is laborious and time-consuming. Previous works suggest that ontology acquisition is an iterative process, which includes keyword collection and structure reorganization. The ontology is revised, refined, and accumulated by a human editor at each iteration (Noy and McGuinness, 2001). For example, in order to find a hyponym of a keyword, the human editor must observe sentences containing this keyword and its related hyponyms (Hearst, 1992). The editor then deduces rules for finding more hyponyms of this keyword. At each iteration the editor refines the rules to obtain better quality pairs of keyword-hyponyms. To speed up the above labor-intensive approach, semiautomatic approaches have been designed in which a human editor only has to verify the results of the acquisition (Maedche and Staab, 2000). A knowledge representation framework, Information Map (InfoMap) in our pre</context>
</contexts>
<marker>Noy, McGuinness, 2001</marker>
<rawString>Noy, N.F. and McGuinness D.L. (2001), Ontology Development 101: A Guide to Creating Your First Ontology, SMI technical report SMI2001-0880, Stanford Medical Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing,</title>
<date>1989</date>
<publisher>AddisonWesley,</publisher>
<location>Massachusetts,</location>
<contexts>
<context position="13348" citStr="Salton, 1989" startWordPosition="2099" endWordPosition="2100">he relevance of a path p is true if p falls in a node on the decision tree whose ratio of true instance is greater than λ . Figure 3. Threshold selection using decision tree 4.2 Domain Speculation The goal of domain speculation is to categorize a sentence S into a domain Dj according to the combined score of the keywords and the event structure in sentence S. We first calculate the similarity score of S and Dj. The keyword score and the event structure score are calculated independently. SimScore(Dj , S) = Keyword_ Score(Dj , S) + α * EventStructure _ Score(Dj,S) We use the TF/IDF classifier (Salton, 1989) to calculate the Keyword_Score of a sentence as follows. First, we use a segmentation module to split a Chinese sentence into words. The TF/IDF classifier represents a domain as a weighted vector, Dj =( wj1, wj2,..., wjn), where n is the number of words in this domain and wk is the weight of word k. wk is defined as nfjk * idfjk, where nfjk is the term frequency (i.e., the number of times the word wk occurs in the domain j). Let DFk be the number of domains in which word k appears and |D |the total number of domains. idfk, the inverse document frequency, is given by: || D idf = log( ) . k DF </context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, G., Automatic Text Processing, AddisonWesley, Massachusetts, 1989.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Soni</author>
<author>Z Tang</author>
<author>J Yang</author>
</authors>
<journal>Microsoft Performance Study of Microsoft Data Mining Algorithms”, UniSys,</journal>
<pages>2000--12</pages>
<marker>Soni, Tang, Yang, </marker>
<rawString>Soni, S, Tang, Z. and Yang, J., “Microsoft Performance Study of Microsoft Data Mining Algorithms”, UniSys, 2000/12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Tsai</author>
<author>W L Hsu</author>
</authors>
<date>2002</date>
<booktitle>Applying an NVEF Word-Pair Identifier to the Chinese Syllable-toWord Conversion Problem,” COLING-02,</booktitle>
<publisher>ACM press,</publisher>
<location>Taipei,</location>
<contexts>
<context position="10984" citStr="Tsai and Hsu, 2002" startWordPosition="1710" endWordPosition="1713">the context in the sentence from which the keyword appears. We can then find attributes or hyponyms of the keyword. For example, in a sentence, we find a noun in front of a keyword (say, computer) may form a specific kind of concept (say, quantum computer). A noun (say, connector) followed by “of” and a keyword may be an attribute of the keyword, (say, connector of computer). See (Wu et al., 2002) for details. 3.4 Ontology Merging Ontologies can be created by merging different resources. One NLP resource that we will merge into our domain ontology is the noun-verb event frame (NVEF) database (Tsai and Hsu, 2002). NVEF is a collection of permissible noun-verb sense-pairs that appear in general domain corpora. The noun will be the subject or object of the verb. This noun-verb sense-pair collection is domain independent. We can use nouns as domain keywords and find their correlated verbs. Adding these verbs into the domain ontology makes the ontology more suitable for NLP. The correlated verbs are added under the action function node. 4. Ontology-Based Text Categorization To incorporate the domain ontology into a text categorization, we have to adjust both the training process and testing process. Secti</context>
</contexts>
<marker>Tsai, Hsu, 2002</marker>
<rawString>Tsai, J.L. and Hsu, W.L., “Applying an NVEF Word-Pair Identifier to the Chinese Syllable-toWord Conversion Problem,” COLING-02, Taipei, ACM press, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Wu</author>
<author>W L Hsu</author>
</authors>
<title>SOAT: A SemiAutomatic Domain Ontology Acquisition Tool from Chinese Corpus,</title>
<date>2002</date>
<publisher>ACM press,</publisher>
<location>COLING-02, Taipei,</location>
<marker>Wu, Hsu, 2002</marker>
<rawString>Wu, S.H. and Hsu, W.L., SOAT: A SemiAutomatic Domain Ontology Acquisition Tool from Chinese Corpus, COLING-02, Taipei, ACM press, 2002.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>W.L., FAQ-centered Organizational Memory, in Nada Matta and Rose Dieng-Kuntz (ed.), Knowledge Management and Organizational Memories,</booktitle>
<editor>Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston,</location>
<marker>2002</marker>
<rawString>Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu, W.L., FAQ-centered Organizational Memory, in Nada Matta and Rose Dieng-Kuntz (ed.), Knowledge Management and Organizational Memories, Kluwer Academic Publishers, Boston, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Wu</author>
<author>P C Yang</author>
<author>V W Soo</author>
</authors>
<title>An Assessment on Character-based Chinese News Filtering Using</title>
<date>1998</date>
<booktitle>Latent Semantic Indexing, Computational Linguistics &amp; Chinese Language Processing,</booktitle>
<volume>3</volume>
<pages>2</pages>
<contexts>
<context position="1950" citStr="Wu et al., 1998" startWordPosition="265" endWordPosition="268">l for information retrieval, knowledge management and knowledge discovery, we test the ability of domain ontology to categorize news clips in this paper. Traditional IR methods use keyword distribution form a training corpus to assign testing document. However, using only keywords in a training set cannot guarantee satisfactory results since authors may use different keywords. We believe that, news clip events are categorized by concepts, not just keywords. Previous works shows that the latent semantic index (LSI) method and the n-gram method give good results for Chinese news categorization (Wu et al., 1998). However, the indices of LSI and n-grams are less meaningful semantically. The implicit rules acquired by these methods can be understood by computers, not humans. Thus, manual editing for exceptions and personalization are not possible and it is difficult to further reuse these indices for knowledge management. With good domain ontology we can identify the concept structure of sentences in a document. Our idea is to compile the concepts within documents in a training set and use these concepts to understand documents in a testing set. However, building rigorous domain ontology is laborious a</context>
</contexts>
<marker>Wu, Yang, Soo, 1998</marker>
<rawString>Wu, S.H., Yang, P.C. and Soo, V.W., An Assessment on Character-based Chinese News Filtering Using Latent Semantic Indexing, Computational Linguistics &amp; Chinese Language Processing, Vol. 3, no.2, August 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>