<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002166">
<title confidence="0.9939745">
Unsupervised learning of word sense disambiguation rules by estimating an
optimum iteration number in the EM algorithm
</title>
<author confidence="0.99702">
Hiroyuki Shinnou
</author>
<affiliation confidence="0.9986">
Department of Systems Engineering,
Ibaraki University
</affiliation>
<address confidence="0.921936">
4-12-1 Nakanarusawa, Hitachi, Ibaraki
316-8511 JAPAN
</address>
<email confidence="0.999138">
shinnou@dse.ibaraki.ac.jp
</email>
<author confidence="0.982091">
Minoru Sasaki
</author>
<affiliation confidence="0.958737">
Department of Computer and
Information Sciences,
Ibaraki University
</affiliation>
<address confidence="0.922152">
4-12-1 Nakanarusawa, Hitachi, Ibaraki
316-8511 JAPAN
</address>
<email confidence="0.999213">
sasaki@cis.ibaraki.ac.jp
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999064">
In this paper, we improve an unsuper-
vised learning method using the Expectation-
Maximization (EM) algorithm proposed by
Nigam et al. for text classification problems
in order to apply it to word sense disambigua-
tion (WSD) problems. The improved method
stops the EM algorithm at the optimum itera-
tion number. To estimate that number, we pro-
pose two methods. In experiments, we solved
50 noun WSD problems in the Japanese Dic-
tionary Task in SENSEVAL2. The score of our
method is a match for the best public score of
this task. Furthermore, our methods were con-
firmed to be effective also for verb WSD prob-
lems.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959981132076">
In this paper, we improve an unsupervised learning
method using the Expectation-Maximization (EM) algo-
rithm proposed by (Nigam et al., 2000) for text classifi-
cation problems in order to apply it to word sense disam-
biguation (WSD) problems. The original method works
well, but often causes worse classification for WSD. To
avoid this, we propose two methods to estimate the opti-
mum iteration number in the EM algorithm.
Many problems in natural language processing can be
converted into classification problems, and be solved by
an inductive learning method. This strategy has been very
successful, but it has a serious problem in that an in-
ductive learning method requires labeled data, which is
expensive because it must be made manually. To over-
come this problem, unsupervised learning methods using
huge unlabeled data to boost the performance of rules
learned by small labeled data have been proposed re-
cently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park
et al., 2000)(Li and Li, 2002). Among these methods,
the method using the EM algorithm proposed by the pa-
per(Nigam et al., 2000), which is referred to as the EM
method in this paper, is the state of the art. However, the
target of the EM method is text classification. It is hoped
that this method can be applied to WSD, because WSD is
the most important problem in natural language process-
ing.
The EM method works well in text classification,
but often causes worse classification in WSD. The EM
method is expected to improve the accuracy of learned
rules step by step in proportion to the iteration number in
the EM algorithm. However, this rarely happens in prac-
tice, and in many cases, the accuracy falls after a certain
iteration number in the EM algorithm. In the worst case,
the accuracy of the rule learned through only labeled data
is degraded by using unlabeled data. To overcome this
problem, we estimate an optimum iteration number in the
EM algorithm, and in actual learning, we stop the itera-
tion of the EM algorithm at the estimated number. If the
estimated number is 0, it means that the EM method is
not used. To estimate the optimum iteration number, we
propose two methods: one uses cross validation and the
other uses two heuristics besides cross validation. In this
paper, we refer to the former method as CV-EM and the
latter method as CV-EM2.
In experiments, we solved 50 noun WSD problems in
the Japanese Dictionary Task in SENSEVAL2(Kurohashi
and Shirai, 2001). The original EM method failed to
boost the precision (76.78%) of the rule learned through
only labeled data. On the other hand, CV-EM and CV-
EM2 boosted the precision to 77.88% and 78.56%. The
score of CV-EM2 is a match for the best public score of
this task. Furthermore, these methods were confirmed to
be effective also for verb WSD problems.
</bodyText>
<sectionHeader confidence="0.787132" genericHeader="method">
2 WSD by Naive Bayes
</sectionHeader>
<bodyText confidence="0.999455571428571">
In a classification problem, let C = {c1, c2, · · ·, c,,,,} be
a set of classes. An instance x is represented as a feature
that the comma and the period are defined as a kind of
content words in this paper. Next we look up the the-
saurus ID of the word ‘saikou’, and find 3.1920_4 3.
In our thesaurus, as shown in Figure 1, a higher number
corresponds to a higher level meaning.
</bodyText>
<equation confidence="0.9851265">
list
x = (f1, f2, ··· , fn).
</equation>
<bodyText confidence="0.646243333333333">
We can solve the classification problem by estimating the
probability P(c|x). Actually, the class cx of x, is given
by
</bodyText>
<equation confidence="0.991167">
cx = arg max P(c|x).
cEC
Bayes theorem shows that
P (c)P (x|c)
P (c|x) = P (x) .
As a result, we get
cx = arg max
cEC
P(c)P(x|c).
</equation>
<bodyText confidence="0.998527">
In the above equation, P(c) is estimated easily; the ques-
tion is how to estimate P(x|c). Naive Bayes models as-
sume the following:
</bodyText>
<equation confidence="0.997194666666667">
n
P(x|c) = P(fi|c). (1)
i=1
</equation>
<bodyText confidence="0.9911427">
The estimation of P(fi|c) is easy, so we can estimate
P (x|c)(Mitchell, 1997). In order to use Naive Bayes ef-
fectively, we must select features that satisfy the equation
1 as much as possible. In text classification tasks, the
appearance of each word corresponds to each feature.
In this paper, we use following six attributes (e1 to e6)
for WSD. Suppose that the target word is wi which is the
i-th word in the sentence.
e1: the word wi_1
e2: the word wi+1
</bodyText>
<listItem confidence="0.778312">
e3: two content words in front of wi
e4: two content words behind wi
e5: thesaurus ID number of e3
e6: thesaurus ID number of e4
</listItem>
<bodyText confidence="0.9815064">
For example, we make features from the following sen-
tence 1 in which the target word is ‘kiroku’2.
kako/saikou/wo/kiroku/suru/ta/.
Because the word to the left of the word ‘kiroku’ is ‘wo’,
we get ‘e1=wo’. In the same way, we get ‘e2=suru’.
Content words to the left of the word ‘kiroku’ are the
word ‘kako’ and the word ‘saikou’. We select two words
from them in the order of proximity to the target word.
Thus, we get ‘e3=kako’ and ‘e3=saikou’. In the
same way, we get ‘e4=suru’ and ‘e4=.’. Note
</bodyText>
<footnote confidence="0.999219333333333">
1A sentence is segmented into words, and each word is
transformed to its original form by morphological analysis.
2‘kiroku’ has at least two meanings: ‘memo’ and ‘record’.
</footnote>
<figureCaption confidence="0.999887">
Figure 1: Japanese thesaurus: Bunrui-goi-hyou
</figureCaption>
<bodyText confidence="0.9727621875">
In this paper, we use a four-digit number and
a five-digit number of a thesaurus ID. As a re-
sult, for ‘e3=saikou’, we get ‘e5=3192’ and
‘e5=31920’. In the same way, for ‘e3=kako’, we
get ‘e5=1164’ and ‘e5=11642’. Following this pro-
cedure, we should look up the thesaurus ID of the word
‘suru’. However, we do not look up the thesaurus ID for
a word that consists of hiragana characters, because such
words are too ambiguous, that is, they have too many the-
saurus IDs. When a word has multiple thesaurus IDs, we
create a feature for each ID.
As a result, we get following ten features from the
above example sentence:
e1=wo, e2=suru, e3=saikou, e3=kako,
e4=suru, e4=., e5=3192, e5=31920,
e5=1164, e5=11642.
</bodyText>
<sectionHeader confidence="0.963661" genericHeader="method">
3 Unsupervised learning using EM
algorithm
</sectionHeader>
<bodyText confidence="0.9998295">
We can use the EM method if we use Naive Bayes for
classification problems. In this paper, we show only key
equations and the key algorithm of this method(Nigam et
al., 2000).
Basically the method computes P(fi|cj) where fi is a
feature and cj is a class. This probability is given by4
</bodyText>
<equation confidence="0.94157775">
1 + E|D|
k=1 N(fi, dk)P(cj|dk)
|F |+ E|m  |1 ED11 N(fm, dk)P(cj  |dk)
(2)
</equation>
<footnote confidence="0.99414725">
3In this paper we use the bunrui-goi-hyou as a Japanese the-
saurus.
4This equation is smoothed by taking into account the fre-
quency 0.
</footnote>
<equation confidence="0.990075">
P(fi|cj) =
</equation>
<bodyText confidence="0.993601076923077">
D: all data consisting of labeled data and unla-
beled data
dk: an element in D
F: the set of all features
fm: an element in F
N(fi, dk): the number of fi in the instance dk.
In our problem, N(fi, dk) is 0 or 1, and almost all of
them are 0. If dk is labeled, P(cj|dk) is 0 or 1. If dk is
unlabeled, P(cj|dk) is initially 0, and is updated to an ap-
propriate value step by step in proportion to the iteration
of the EM algorithm.
By using equation 2, the following classifier is con-
structed:
</bodyText>
<equation confidence="0.996139">
P(cj) 11fn∈Kdi P(fn|cj)
P (cj|di) =7�7 / (3)
EI°11 PI/ P(CI) 11f,,∈Kdi P(Mcr)
</equation>
<bodyText confidence="0.9961332">
In this equation, Kdi is the set of features in the instance
di.
The EM algorithm computes P(cj|di) by using equa-
tion 3 (E-step). Next, by using equation 2, P (fi|cj)
is computed (M-step). By iterating E-step and M-step,
P(fi|cj) and P(cj|di) converge. In our experiment,
when the difference between the current P (fi|cj) and the
updated P(fi|cj) comes to less than 8 · 10−6 or the itera-
tion number reaches 10 times, we judge that the algorithm
has converged.
4 Estimation of the optimum iteration
number
In this paper, we propose two methods (CV-EM and CV-
EM2) to estimate the optimum iteration number in the
EM algorithm.
The CV-EM method is cross validation. First of all, we
divide labeled data into three parts, one of which is used
as test data and the others are used as new labeled data.
By using this new labeled data and huge unlabeled data,
we conduct the EM method. After each iteration in the
EM algorithm, the learned rules at the time are evaluated
by using test data. This experiment is conducted three
times by changing the labeled data and test data. The
precision of each iteration number is given by the mean
of three experiments. The optimum iteration number is
estimated to be the iteration number at which the highest
precision is achieved.
The CV-EM2 method also uses cross validation, but
estimates the optimum iteration number by ad-hoc mech-
anism.
First, we judge whether we can use the EM method
without modification or not. To do this, we compare the
precision at convergence with the precision of the itera-
tion number 1. If the former is higher than the latter, we
judge that we can use the EM method without modifica-
tion. In this case, the optimum iteration number is esti-
mated to be the converged number. On the other hand, if
the former is not higher than the latter, we go to the sec-
ond judgment, namely whether the EM method should be
used or not. To judge this, we compare the two precisions
of the iteration number 0 and 1. The iteration number 0
means that the EM method is not used. If the precision of
the iteration number 0 is higher than the precision of the
iteration number 1, we judge that the EM method should
not be used. In this case, the optimum iteration number
is estimated to be 0. Conversely, if the precision of the
iteration number 1 is higher than the precision of the it-
eration number 0, we judge that the EM method should
be used. In this case, the optimum iteration number is
estimated to be the number obtained by CV-EM.
In the many cases, the CV-EM2 outputs the same num-
ber as the CV-EM. However, the basic idea is different.
Roughly speaking, the CV-EM2 relies on two heuris-
tics: (1) Basically we only have to judge whether the EM
method can be used or not, because the EM algorithm
improves or degrades the precision monotonically. (2)
Whether the EM algorithm succeeds correlates closely
with whether the precision is improved by the first iter-
ation of the EM algorithm. Therefore, we estimate the
optimum iteration number by comparing three precisions,
the precision of the iteration number 0, 1 and at conver-
gence.
The figure 2 shows a typical case that the CV-EM2 dif-
fers from the CV-EM. In the cross validation, the preci-
sion is degraded by the first iteration of the EM algorithm,
and then it is improved by iteration, and the maximum
precision is achieved at the k-th iteration, but the preci-
sion converges to the lower point than the precision of the
iteration number 1. In this case, the CV-EM gives k as the
estimation, but the CV-EM2 gives 0.
</bodyText>
<footnote confidence="0.476297">
0 1 k
iteration
</footnote>
<figureCaption confidence="0.892557">
Figure 2: Typical difference between CV-EM and CV-
</figureCaption>
<equation confidence="0.9385757">
EM2
P(cj) is computed by
1 + E|D|
k=1 P(cj|dk)
P(cj) =
|C |+ |D|
. (4)
CV-EM =&gt; k
CV-EM2 =&gt; 0
precision
</equation>
<sectionHeader confidence="0.997005" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999837086956522">
To confirm the effectiveness of our methods, we tested
with 50 nouns of the Japanese Dictionary Task in SEN-
SEVAL2(Kurohashi and Shirai, 2001).
The Japanese Dictionary Task is a standard WSD prob-
lem. As the evaluation words, 50 noun words and 50 verb
words are provided. These words are selected so as to
balance the difficulty of WSD. The number of labeled in-
stances for nouns is 177.4 on average, and for verbs is
172.7 on average. The number of test instances for each
evaluation word is 100, so the number of test instances
of noun and verb evaluation words is 5,000 respectively.
However, unlabeled data are not provided. Note that we
cannot use simple raw texts including the target word, be-
cause we must use the same dictionary and part of speech
set as labeled data. Therefore, we use Mainichi newspa-
per articles for 1995 with word segmentations provided
by RWC. This data is the origin of labeled data. As a re-
sult, we gathered 7585.5 and 6571.9 unlabeled instances
for per noun and per verb evaluation word on average,
respectively.
Table 1 shows the results of experiments for noun eval-
uation words. In this table, NB means Naive Bayes,
EM the EM method, and ideal the EM method
stopping at the ideal iteration number. Note that the pre-
cision is computed by mixed-gained scoring(Kurohashi
and Shirai, 2001) which gives partial points in some
cases.
The precision of Naive Bayes which learns through
only labeled data was 76.58%. The EM method failed to
boost it, and degraded it to 73.56%. On the other hand, by
using CV-EM the precision was boosted to 77.88%. Fur-
thermore, CV-EM2 boosted it to 78.56%. This score is a
match for the best public score of this task. As success-
ful results in this task, two researches are reported. One
used Naive Bayes with various attributes, and achieved
78.22% precision(Murata et al., 2001). Another used
Adaboost of decision trees, and achieved 78.47% preci-
sion(Nakano and Hirai, 2002). Our score is higher than
these scores 5. Furthermore, their methods used syntactic
analysis, but our methods do not need it.
In the same way, we performed experiments for verb
evaluation words. Table 2 shows the results. In the ex-
periment, Naive Bayes achieved 78.16% precision. The
EM method boosted it to 78.74%. Furthermore, CV-EM
and CV-EM2 boosted it to 79.22% and 79.26% respec-
tively. CV-EM2 is marginally higher than CV-EM.
</bodyText>
<footnote confidence="0.985365">
5The best score for the total of noun words and verb words
is reported to be 79.33% in (Murata et al., 2001).
</footnote>
<tableCaption confidence="0.993777">
Table 1: Results of experiments (Noun)
</tableCaption>
<table confidence="0.999968641509434">
Word NB EM CV-EM CV-EM2 ideal
(%) (%) (%) (%) (%)
aida 81.0 80.0 82.0 82.0 82.0
atama 60.0 64.0 60.0 64.0 66.0
ippan 88.0 86.0 89.0 89.0 89.0
ippou 82.0 88.0 88.0 88.0 89.0
ima 90.0 90.0 90.0 90.0 90.0
imi 45.0 53.0 53.0 53.0 53.0
utagai 100.0 95.0 98.0 98.0 100.0
otoko 92.0 89.0 92.0 92.0 92.0
kaihatsu 62.0 63.0 62.0 62.0 63.0
kaku n 71.0 77.0 71.0 77.0 81.0
kankei 85.0 90.0 90.0 90.0 90.0
kimochi 65.0 65.0 65.0 65.0 66.0
kiroku 74.0 71.0 73.0 73.0 77.0
gijutsu 96.0 92.0 96.0 96.0 96.0
genzai 97.0 09.0 98.0 98.0 98.0
koushou 100.0 88.0 100.0 100.0 100.0
kokunai 46.0 58.0 46.0 46.0 58.0
kotoba 45.0 40.0 40.0 40.0 45.0
kodomo 67.0 73.0 72.0 72.0 73.0
gogo 77.0 65.0 86.0 86.0 86.0
shijo 77.0 55.0 77.0 77.0 77.0
shimin 67.0 63.0 67.0 67.0 67.0
shakai 82.0 83.0 83.0 83.0 83.0
shonen 92.0 90.0 90.0 90.0 92.0
jikan 54.0 15.0 54.0 54.0 54.0
jigyou 69.0 70.0 69.0 69.0 71.0
jidai 72.0 77.0 77.0 77.0 78.0
jibun 100.0 100.0 100.0 100.0 100.0
joho 77.0 64.0 77.0 77.0 77.0
sugata 55.0 63.0 61.0 61.0 63.0
seishin 65.0 66.0 66.0 66.0 66.0
taishou 98.0 98.0 98.0 98.0 98.0
daihyou 85.0 95.0 96.0 96.0 98.0
chikaku 74.0 87.0 87.0 87.0 87.0
chihou 70.0 72.0 70.0 70.0 72.0
chushin 98.0 98.0 98.0 98.0 98.0
te 47.0 48.0 47.0 48.0 48.0
teido 100.0 100.0 100.0 100.0 100.0
denwa 84.0 65.0 83.0 83.0 85.0
doujitsu 81.0 51.0 57.0 81.0 81.0
hana 99.0 97.0 99.0 99.0 99.0
hantai 97.0 97.0 97.0 97.0 97.0
baai 82.0 91.0 91.0 91.0 92.0
mae 86.0 91.0 92.0 91.0 92.0
minkan 100.0 100.0 100.0 100.0 100.0
musume 88.0 88.0 88.0 88.0 88.0
mune 71.0 77.0 77.0 77.0 79.0
me 18.0 17.0 18.0 18.0 18.0
mono 31.0 27.0 27.0 27.0 31.0
mondai 97.0 97.0 97.0 97.0 97.0
average 76.78 73.56 77.88 78.56 79.64
</table>
<tableCaption confidence="0.983224">
Table 2: Results of experiments (Verb)
</tableCaption>
<table confidence="0.999887886792453">
Word NB EM CV-EM CV-EM2 ideal
(%) (%) (%) (%) (%)
ataeru 71.0 78.0 78.0 78.0 78.0
iu 94.0 94.0 94.0 94.0 94.0
ukeru 59.0 64.0 59.0 64.0 64.0
uttaeru 84.0 87.0 87.0 87.0 88.0
umareru 69.0 83.0 82.0 83.0 83.0
egaku 58.0 56.0 56.0 56.0 58.0
omou 90.0 89.0 89.0 89.0 90.0
kau 83.0 83.0 83.0 83.0 83.0
kakaru 58.0 57.0 58.0 58.0 58.0
kaku v 72.0 66.0 72.0 72.0 72.0
kawaru 92.0 92.0 92.0 92.0 92.0
kangaeru 99.0 99.0 99.0 99.0 99.0
kiku 56.0 55.0 55.0 55.0 56.0
kimaru 96.0 96.0 96.0 96.0 96.0
kimeru 93.0 93.0 93.0 93.0 93.0
kuru 84.0 85.0 86.0 85.0 86.0
kuwaeru 89.0 89.0 89.0 89.0 89.0
koeru 78.0 82.0 85.0 82.0 88.0
shiru 97.0 97.0 97.0 97.0 97.0
susumu 49.0 50.0 50.0 50.0 50.0
susumeru 97.0 95.0 97.0 97.0 97.0
dasu 35.0 29.0 35.0 35.0 36.0
chigau 100.0 100.0 100.0 100.0 100.0
tsukau 97.0 97.0 97.0 97.0 97.0
tsukuru 69.0 75.0 78.0 75.0 78.0
tsutaeru 75.0 76.0 76.0 76.0 76.0
dekiru 81.0 81.0 81.0 81.0 81.0
deru 59.0 64.0 64.0 64.0 64.0
tou 69.0 79.0 79.0 79.0 79.0
toru 32.0 34.0 32.0 34.0 37.0
nerau 99.0 99.0 99.0 99.0 99.0
nokosu 79.0 79.0 79.0 79.0 79.0
noru 54.0 54.0 54.0 54.0 54.0
hairu 36.0 36.0 36.0 36.0 36.0
hakaru 92.0 92.0 92.0 92.0 92.0
hanasu 100.0 87.0 100.0 100.0 100.0
hiraku 86.0 94.0 94.0 94.0 94.0
fukumu 99.0 99.0 99.0 99.0 99.0
matsu 52.0 50.0 51.0 51.0 52.0
matomeru 79.0 80.0 80.0 80.0 80.0
mamoru 79.0 71.0 70.0 71.0 79.0
miseru 98.0 98.0 98.0 98.0 98.0
mitomeru 89.0 89.0 89.0 89.0 89.0
miru 73.0 71.0 73.0 73.0 73.0
mukaeru 89.0 89.0 89.0 89.0 89.0
motsu 57.0 62.0 57.0 57.0 62.0
motomeru 87.0 87.0 87.0 87.0 87.0
yomu 88.0 88.0 88.0 88.0 88.0
yoru 97.0 97.0 97.0 97.0 97.0
wakaru 90.0 90.0 90.0 90.0 90.0
average 78.16 78.74 79.22 79.26 79.92
</table>
<sectionHeader confidence="0.999703" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999924">
6.1 Cause of failure of the EM method
</subsectionHeader>
<bodyText confidence="0.999924470588235">
Why does the EM method often fail to boost the perfor-
mance? One reason may be the difference among class
distributions of labeled data L, unlabeled data U and test
data T. Practically L, U and T are the same because they
consist of random samples from all data. However, there
are differences among them.
Intuitively, learning by combining labeled data and un-
labeled data is regarded as learning from the distribution
of L + U. It is expected that the EM method is effective
if d = d(L, T) − d(L + U, T) &gt; 0, and is counterproduc-
tive if d &lt; 0, in which d(·, ·) means the distance of two
distributions.
To confirm the above expectation, we conduct an ex-
periment by using Kullback-Leibler divergence as d(·, ·).
The distribution of L + U can be obtained from Equation
4 when the EM algorithm converges. The result of the
experiment is shown in Table 3.
</bodyText>
<tableCaption confidence="0.998679">
Table 3: Effects of the distribution of meanings
</tableCaption>
<table confidence="0.859208666666667">
d &gt; 0 d &lt; 0
improvement 6 7
deterioration 2 8
</table>
<bodyText confidence="0.999543529411765">
The columns of the table are divided into positive
(d &gt; 0) and negative (d &lt; 0). Positive means that L + U
gets close to T and negative means that L + U goes away
from T. The rows of the table are divided into improve-
ment of precision and deterioration of precision. In this
paper, improvement of precision is when the precision is
improved by over 5%, and deterioration of precision is
when the precision is degraded by over 5%.
This result indicates that there is a weak correlation be-
tween whether L + U gets close to T or goes away from
T and whether the EM method is effective or not, but we
cannot conclude they are completely dependent. How-
ever, the evaluation word ‘genzai’ whose precision falls
most by the EM method is precisely the above case. The
d for this word is the smallest, –0.30, among all evalua-
tion words. Further investigation of the causes of failure
of the EM method is our future work.
</bodyText>
<subsectionHeader confidence="0.999972">
6.2 Effectiveness of estimation of CV-EM2
</subsectionHeader>
<bodyText confidence="0.999962205882353">
CV-EM2 achieved ideal estimation for 29 of 50 evalua-
tion words, that is 58%. Furthermore, for 15 of the other
21 evaluation words, the difference between the preci-
sion through our method and that through ideal estima-
tion did not exceed 2%. Therefore, estimation of CV-
EM2 is mostly effective.
The words ‘kokunai’ and ‘kotoba’ are typical cases
where estimation fails. The difference between the pre-
cision of CV-EM2 and that through ideal estimation ex-
ceeded 5%. The failure of estimation for these two words
reduced the whole precision.
Figure 3 compares the precision for cross validation
and that for actual evaluation for the word ‘kokunai’. In
the same way, Figure 3 shows the case of the word ‘ko-
toba’. In these figures, the x-axis shows the iteration
number of the EM algorithm. To clarify the change of
precision, the initial precision is set to 0, and the y-axis
shows the difference (%) between the actual and initial
precision.
In the case of ‘kokunai’, the precision got worse in
cross validation, but the precision got better in the ac-
tual evaluation. This means that cross validation is use-
less, so it is difficult to estimate an optimum iteration
number in the EM algorithm. However, such cases are
rare. In the experiment, this case arises for only this word
‘kokunai’. Consider next the case of ‘kotoba’. In cross
validation, the precision improved in the first iteration of
the EM algorithm, but got worse step by step thereafter.
On the other hand, in the actual evaluation, the precision
got worse even in the first iteration of the EM algorithm.
The difference of these results in the first iteration of the
EM algorithm causes our estimation to fail. In future,
we must improve our method by further investigation of
these words.
</bodyText>
<equation confidence="0.782617">
0 2 4 6 8 10
iteration
</equation>
<bodyText confidence="0.999485461538462">
point is selected in cross validation, that is 3. On the other
hand, CV-EM2 estimates 0 by using the relation of three
precisions: the initial precision, the precision for the iter-
ation 1 and the precision at convergence.
Let’s count the number of words for which CV-EM2
is better or worse than CV-EM. For one word ‘mae’ in
nouns and three words ‘kuru’, ‘koeru’ and ‘tukuru’ in
verbs, CV-EM was superior to CV-EM2. On the other
hand, for four words ‘atama’, ‘kaku n’, ‘te’ and ‘dou-
jitsu’ in nouns and four words ‘ukeru’, ‘umareru’, ‘toru’
and ‘mamortu’ in verbs, CV-EM2 was better to CV-EM.
These numbers show that our method is somewhat supe-
rior to CV-EM.
</bodyText>
<figure confidence="0.9971521">
difference from base precision (%)
10
-5
5
0
REAL
CROSS-VAL
difference from base precision (%)
-1
-2
-3
-4
-5
-6
2
0
1
CROSS-VAL
0 2 4 6 8 10
iteration
</figure>
<figureCaption confidence="0.978903">
Figure 3: Comparison between cross validation and ac-
tual evaluation (‘kokunai’)
</figureCaption>
<figure confidence="0.9905055">
0 2 4 6 8 10
iteration
</figure>
<figureCaption confidence="0.9884235">
Figure 4: Comparison between cross validation and ac-
tual evaluation (‘kotoba’)
</figureCaption>
<subsectionHeader confidence="0.999292">
6.3 Comparison of CV-EM and CV-EM2
</subsectionHeader>
<bodyText confidence="0.998201875">
CV-EM2 is slightly superior to CV-EM. In the evaluation
word ‘doujitu’, there is a remarkable difference between
the two methods.
Figure 5 shows the change of the precision for ‘dou-
jitsu’ in cross validation, and Figure 6 shows that in ac-
tual evaluation.
The precision goes up in cross validation, but goes
down largely in actual evaluation. In CV-EM, the best
</bodyText>
<figure confidence="0.9303505">
0 2 4 6 8 10
iteration
</figure>
<figureCaption confidence="0.996592">
Figure 6: Actual evaluation in ‘doujitsu’
</figureCaption>
<subsectionHeader confidence="0.941504">
6.4 Unsupervised learning for verb WSD
</subsectionHeader>
<bodyText confidence="0.999980111111111">
In the experiments, CV-EM and CV-EM2 improved the
EM method for both noun words and verb words. The
effectiveness of these methods was large for noun words,
but was small for verb words. We believe that the cause
of this difference is the difficulty of unsupervised learning
for verb WSD. In ideal estimation, the precision for noun
words was boosted from 76.78% to 79.64% by the EM
method, that is 1.037 times. On the other hand, the preci-
sion for verb words was boosted from 78.16% to 79.92%
</bodyText>
<figure confidence="0.991203818181818">
difference from base precision (%)
-1
-2
-3
-4
-5
-6
0
1
REAL
CROSS-VAL
</figure>
<figureCaption confidence="0.900605">
Figure 5: Cross validation in ‘doujitsu’
</figureCaption>
<figure confidence="0.994282444444444">
0
-5
-10
-15
-20
-25
-30
difference from base precision (%)
CROSS-VAL
</figure>
<bodyText confidence="0.989684862068966">
by the EM method, that is 1.022 times. This shows that
the EM method does not work so well for verb words.
We consider that feature independence plays a key role
in unsupervised learning. Suppose the instance x con-
sists of two features f1 and f2. When class c., of x is
judged from feature f1, the probability P(c.,|f2) is tuned
to be larger. The question is whether it is actually right
or not to increase P(c.,|f2). If it is right, unsupervised
learning works well, but if it is not, unsupervised learn-
ing fails. Intuitively, feature independence warrants in-
creasing P(c.,|f2). In noun WSD, the left context of the
target word corresponds to the words modifying the tar-
get word, and the right context of the target word cor-
responds to the verb word whose case slot can have the
target word. Both the left context and right context can
judge the meaning of the target word by itself, and are in-
dependent. Left context and right context act as indepen-
dent features. On the other hand, we cannot find such an
opportune interpretation for the features of verbs (Shin-
nou, 2002). Therefore, the EM method is not so effective
for verb words.
Naive Bayes assumes the independence of features,
too. However, this assumption is not so rigid in practice.
We believe that the improvement by the EM method for
verb words depends on the robustness of Naive Bayes. In
our experiments, the EM method for noun words failed
to boost the precision. We think that the cause is the im-
balance of labeled data, unlabeled data and test data. We
should investigate this in a future study.
</bodyText>
<sectionHeader confidence="0.827833" genericHeader="method">
6.5 Related works
</sectionHeader>
<bodyText confidence="0.999918357142857">
Co-training(Blum and Mitchell, 1998) is a powerful un-
supervised learning method. In Co-training, if we can
find two independent feature sets for the target problem,
any supervised learning method can be used. Further-
more, it is reported that Co-training is superior to the
EM method if complete independent feature sets can be
used(Nigam and Ghani, 2000). However, Co-training
requires consistency besides independence for two fea-
ture sets. This condition makes it difficult to apply Co-
training to multiclass classification problems. On the
other hand, the EM method requires Naive Bayes to be
used as the supervised learning method, but can be ap-
plied to multiclass classification problems without any
modification. Therefore, the EM method is more prac-
tical than Co-training.
Yarowsky proposed the unsupervised learning method
for WSD(Yarowsky, 1995). His method is reported to be
a special case of Co-training(Blum and Mitchell, 1998).
As two independent feature sets, one is the context sur-
rounding the target word and the other is the heuristic of
‘one sense per discourse’. However, it is unknown how
valid this heuristic is for granularity of meanings of our
evaluation words. Furthermore, this method needs doc-
uments in which the target word appears multiple times,
as unlabeled data. Therefore, it is not so easy to gather
unlabeled data. On the other hand, the EM method does
not have such problem because it uses sentences includ-
ing the target word as unlabeled data.
</bodyText>
<subsectionHeader confidence="0.848226">
6.6 Future works
</subsectionHeader>
<bodyText confidence="0.999948818181818">
We have three future works. First, we must raise the pre-
cision for verb words, which may be impossible unless
we use other features, so we need to investigate other fea-
tures. Second, we must improve the estimation method of
the optimum iteration number in the EM algorithm. The
difference between the precision through our estimation
and that through the ideal estimation is large. We can im-
prove the accuracy by improving the estimation method.
Finally, we will investigate the reason for the failure of
the EM method, which may be the key to unsupervised
learning.
</bodyText>
<sectionHeader confidence="0.999744" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999979625">
In this paper, we improved the EM method proposed by
Nigam et al. for text classification problems in order to
apply it to WSD problems. To avoid some failures in the
original EM method, we proposed two methods to esti-
mate the optimum iteration number in the EM algorithm.
In experiments, we tested with 50 noun WSD problems in
the Japanese Dictionary Task in SENSEVAL2. Our two
methods greatly improved the original EM method. Es-
pecially, the score of noun evaluation words was equiva-
lent to the best public score of this task. Furthermore, our
methods were also effective for verb WSD problems. In
future, we will tackle three works: (1) To find other effec-
tive features for unsupervised learning of verb WSD, (2)
To improve the estimation method of the optimum itera-
tion number in the EM algorithm, and (3) To investigate
the reason for the failure of the EM method.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997402295454546">
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In 11th
Annual Conference on Computational Learning The-
ory (COLT-98), pages 92–100.
Sadao Kurohashi and Kiyoaki Shirai. 2001.
SENSEVAL-2 Japanese Tasks (in Japansese). In
Technical Report ofIEICE, NLC-36–48, pages 1–8.
Cong Li and Hang Li. 2002. Word Translation Dis-
ambiguation Using Bilingual Bootstrapping. In 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-02), pages 343–351.
Tom Mitchell. 1997. Machine Learning. McGraw-Hill
Companies.
Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto,
Qing Ma, and Hitoshi Isahara. 2001. CRL at Japanese
dictionary-based task of SENSEVAL-2 (in Japanese).
In Technical Report ofIEICE, NLC-36–48, pages 31–
38.
Keigo Nakano and Yuuzou Hirai. 2002. AdaBoost wo
motiita gogi no aimaisei kaisyou (in Japanese). In 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 659–662.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the
effectiveness and applicability of co-training. In 9th
International Conference on Information and Knowl-
edge Management, pages 86–93.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 2000. Text Classification from Labeled
and Unlabeled Documents using EM. Machine Learn-
ing, 39(2/3):103–134.
Seong-Bae Park, Byoung-Tak Zhang, and Yung Taek
Kim. 2000. Word sense disambiguation by learning
from unlabeled data. In 38th Annual Meeting of the
Association for Computational Linguistics (ACL-00),
pages 547–554.
Hiroyuki Shinnou. 2002. Learning of word sense
disambiguation rules by Co-training, checking co-
occurrence of features. In 3rd international conference
on Language resources and evaluation (LREC-2002),
pages 1380–1384.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In 33th
Annual Meeting of the Association for Computational
Linguistics (ACL-95), pages 189–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.125952">
<title confidence="0.937152">Unsupervised learning of word sense disambiguation rules by estimating an optimum iteration number in the EM algorithm</title>
<author confidence="0.952779">Hiroyuki</author>
<affiliation confidence="0.914438">Department of Systems Ibaraki</affiliation>
<address confidence="0.8730335">4-12-1 Nakanarusawa, Hitachi, 316-8511</address>
<email confidence="0.975453">shinnou@dse.ibaraki.ac.jp</email>
<author confidence="0.51561">Minoru</author>
<affiliation confidence="0.92369">Department of Computer Information Ibaraki</affiliation>
<address confidence="0.879935">4-12-1 Nakanarusawa, Hitachi, 316-8511</address>
<email confidence="0.983697">sasaki@cis.ibaraki.ac.jp</email>
<abstract confidence="0.9865051875">In this paper, we improve an unsupervised learning method using the Expectation- Maximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation (WSD) problems. The improved method stops the EM algorithm at the optimum iteration number. To estimate that number, we propose two methods. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2. The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for verb WSD problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In 11th Annual Conference on Computational Learning Theory (COLT-98),</booktitle>
<pages>92--100</pages>
<contexts>
<context position="1999" citStr="Blum and Mitchell, 1998" startWordPosition="303" endWordPosition="307">WSD. To avoid this, we propose two methods to estimate the optimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park et al., 2000)(Li and Li, 2002). Among these methods, the method using the EM algorithm proposed by the paper(Nigam et al., 2000), which is referred to as the EM method in this paper, is the state of the art. However, the target of the EM method is text classification. It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing. The EM method works well in text classification, but often causes worse classification in WSD. The EM method is expected to improve the accuracy of learned rules step by step in propo</context>
<context position="24929" citStr="Blum and Mitchell, 1998" startWordPosition="4441" endWordPosition="4444">ind such an opportune interpretation for the features of verbs (Shinnou, 2002). Therefore, the EM method is not so effective for verb words. Naive Bayes assumes the independence of features, too. However, this assumption is not so rigid in practice. We believe that the improvement by the EM method for verb words depends on the robustness of Naive Bayes. In our experiments, the EM method for noun words failed to boost the precision. We think that the cause is the imbalance of labeled data, unlabeled data and test data. We should investigate this in a future study. 6.5 Related works Co-training(Blum and Mitchell, 1998) is a powerful unsupervised learning method. In Co-training, if we can find two independent feature sets for the target problem, any supervised learning method can be used. Furthermore, it is reported that Co-training is superior to the EM method if complete independent feature sets can be used(Nigam and Ghani, 2000). However, Co-training requires consistency besides independence for two feature sets. This condition makes it difficult to apply Cotraining to multiclass classification problems. On the other hand, the EM method requires Naive Bayes to be used as the supervised learning method, bu</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In 11th Annual Conference on Computational Learning Theory (COLT-98), pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Kiyoaki Shirai</author>
</authors>
<date>2001</date>
<booktitle>SENSEVAL-2 Japanese Tasks (in Japansese). In Technical Report ofIEICE, NLC-36–48,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3506" citStr="Kurohashi and Shirai, 2001" startWordPosition="565" endWordPosition="568">ing unlabeled data. To overcome this problem, we estimate an optimum iteration number in the EM algorithm, and in actual learning, we stop the iteration of the EM algorithm at the estimated number. If the estimated number is 0, it means that the EM method is not used. To estimate the optimum iteration number, we propose two methods: one uses cross validation and the other uses two heuristics besides cross validation. In this paper, we refer to the former method as CV-EM and the latter method as CV-EM2. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2(Kurohashi and Shirai, 2001). The original EM method failed to boost the precision (76.78%) of the rule learned through only labeled data. On the other hand, CV-EM and CVEM2 boosted the precision to 77.88% and 78.56%. The score of CV-EM2 is a match for the best public score of this task. Furthermore, these methods were confirmed to be effective also for verb WSD problems. 2 WSD by Naive Bayes In a classification problem, let C = {c1, c2, · · ·, c,,,,} be a set of classes. An instance x is represented as a feature that the comma and the period are defined as a kind of content words in this paper. Next we look up the thesa</context>
<context position="11649" citStr="Kurohashi and Shirai, 2001" startWordPosition="2047" endWordPosition="2051">irst iteration of the EM algorithm, and then it is improved by iteration, and the maximum precision is achieved at the k-th iteration, but the precision converges to the lower point than the precision of the iteration number 1. In this case, the CV-EM gives k as the estimation, but the CV-EM2 gives 0. 0 1 k iteration Figure 2: Typical difference between CV-EM and CVEM2 P(cj) is computed by 1 + E|D| k=1 P(cj|dk) P(cj) = |C |+ |D| . (4) CV-EM =&gt; k CV-EM2 =&gt; 0 precision 5 Experiments To confirm the effectiveness of our methods, we tested with 50 nouns of the Japanese Dictionary Task in SENSEVAL2(Kurohashi and Shirai, 2001). The Japanese Dictionary Task is a standard WSD problem. As the evaluation words, 50 noun words and 50 verb words are provided. These words are selected so as to balance the difficulty of WSD. The number of labeled instances for nouns is 177.4 on average, and for verbs is 172.7 on average. The number of test instances for each evaluation word is 100, so the number of test instances of noun and verb evaluation words is 5,000 respectively. However, unlabeled data are not provided. Note that we cannot use simple raw texts including the target word, because we must use the same dictionary and par</context>
</contexts>
<marker>Kurohashi, Shirai, 2001</marker>
<rawString>Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2 Japanese Tasks (in Japansese). In Technical Report ofIEICE, NLC-36–48, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Li</author>
<author>Hang Li</author>
</authors>
<title>Word Translation Disambiguation Using Bilingual Bootstrapping.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>343--351</pages>
<contexts>
<context position="2051" citStr="Li and Li, 2002" startWordPosition="311" endWordPosition="314">ptimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park et al., 2000)(Li and Li, 2002). Among these methods, the method using the EM algorithm proposed by the paper(Nigam et al., 2000), which is referred to as the EM method in this paper, is the state of the art. However, the target of the EM method is text classification. It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing. The EM method works well in text classification, but often causes worse classification in WSD. The EM method is expected to improve the accuracy of learned rules step by step in proportion to the iteration number in the EM algorithm. H</context>
</contexts>
<marker>Li, Li, 2002</marker>
<rawString>Cong Li and Hang Li. 2002. Word Translation Disambiguation Using Bilingual Bootstrapping. In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<date>1997</date>
<journal>Machine Learning. McGraw-Hill Companies.</journal>
<contexts>
<context position="4765" citStr="Mitchell, 1997" startWordPosition="809" endWordPosition="810">0_4 3. In our thesaurus, as shown in Figure 1, a higher number corresponds to a higher level meaning. list x = (f1, f2, ··· , fn). We can solve the classification problem by estimating the probability P(c|x). Actually, the class cx of x, is given by cx = arg max P(c|x). cEC Bayes theorem shows that P (c)P (x|c) P (c|x) = P (x) . As a result, we get cx = arg max cEC P(c)P(x|c). In the above equation, P(c) is estimated easily; the question is how to estimate P(x|c). Naive Bayes models assume the following: n P(x|c) = P(fi|c). (1) i=1 The estimation of P(fi|c) is easy, so we can estimate P (x|c)(Mitchell, 1997). In order to use Naive Bayes effectively, we must select features that satisfy the equation 1 as much as possible. In text classification tasks, the appearance of each word corresponds to each feature. In this paper, we use following six attributes (e1 to e6) for WSD. Suppose that the target word is wi which is the i-th word in the sentence. e1: the word wi_1 e2: the word wi+1 e3: two content words in front of wi e4: two content words behind wi e5: thesaurus ID number of e3 e6: thesaurus ID number of e4 For example, we make features from the following sentence 1 in which the target word is ‘k</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom Mitchell. 1997. Machine Learning. McGraw-Hill Companies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Masao Utiyama</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<date>2001</date>
<booktitle>CRL at Japanese dictionary-based task of SENSEVAL-2 (in Japanese). In Technical Report ofIEICE, NLC-36–48,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="13344" citStr="Murata et al., 2001" startWordPosition="2344" endWordPosition="2347">tion number. Note that the precision is computed by mixed-gained scoring(Kurohashi and Shirai, 2001) which gives partial points in some cases. The precision of Naive Bayes which learns through only labeled data was 76.58%. The EM method failed to boost it, and degraded it to 73.56%. On the other hand, by using CV-EM the precision was boosted to 77.88%. Furthermore, CV-EM2 boosted it to 78.56%. This score is a match for the best public score of this task. As successful results in this task, two researches are reported. One used Naive Bayes with various attributes, and achieved 78.22% precision(Murata et al., 2001). Another used Adaboost of decision trees, and achieved 78.47% precision(Nakano and Hirai, 2002). Our score is higher than these scores 5. Furthermore, their methods used syntactic analysis, but our methods do not need it. In the same way, we performed experiments for verb evaluation words. Table 2 shows the results. In the experiment, Naive Bayes achieved 78.16% precision. The EM method boosted it to 78.74%. Furthermore, CV-EM and CV-EM2 boosted it to 79.22% and 79.26% respectively. CV-EM2 is marginally higher than CV-EM. 5The best score for the total of noun words and verb words is reported </context>
</contexts>
<marker>Murata, Utiyama, Uchimoto, Ma, Isahara, 2001</marker>
<rawString>Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara. 2001. CRL at Japanese dictionary-based task of SENSEVAL-2 (in Japanese). In Technical Report ofIEICE, NLC-36–48, pages 31– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keigo Nakano</author>
<author>Yuuzou Hirai</author>
</authors>
<title>AdaBoost wo motiita gogi no aimaisei kaisyou (in Japanese).</title>
<date>2002</date>
<booktitle>In 8th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>659--662</pages>
<contexts>
<context position="13440" citStr="Nakano and Hirai, 2002" startWordPosition="2357" endWordPosition="2361">, 2001) which gives partial points in some cases. The precision of Naive Bayes which learns through only labeled data was 76.58%. The EM method failed to boost it, and degraded it to 73.56%. On the other hand, by using CV-EM the precision was boosted to 77.88%. Furthermore, CV-EM2 boosted it to 78.56%. This score is a match for the best public score of this task. As successful results in this task, two researches are reported. One used Naive Bayes with various attributes, and achieved 78.22% precision(Murata et al., 2001). Another used Adaboost of decision trees, and achieved 78.47% precision(Nakano and Hirai, 2002). Our score is higher than these scores 5. Furthermore, their methods used syntactic analysis, but our methods do not need it. In the same way, we performed experiments for verb evaluation words. Table 2 shows the results. In the experiment, Naive Bayes achieved 78.16% precision. The EM method boosted it to 78.74%. Furthermore, CV-EM and CV-EM2 boosted it to 79.22% and 79.26% respectively. CV-EM2 is marginally higher than CV-EM. 5The best score for the total of noun words and verb words is reported to be 79.33% in (Murata et al., 2001). Table 1: Results of experiments (Noun) Word NB EM CV-EM C</context>
</contexts>
<marker>Nakano, Hirai, 2002</marker>
<rawString>Keigo Nakano and Yuuzou Hirai. 2002. AdaBoost wo motiita gogi no aimaisei kaisyou (in Japanese). In 8th Annual Meeting of the Association for Natural Language Processing, pages 659–662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In 9th International Conference on Information and Knowledge Management,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="25247" citStr="Nigam and Ghani, 2000" startWordPosition="4493" endWordPosition="4496">s on the robustness of Naive Bayes. In our experiments, the EM method for noun words failed to boost the precision. We think that the cause is the imbalance of labeled data, unlabeled data and test data. We should investigate this in a future study. 6.5 Related works Co-training(Blum and Mitchell, 1998) is a powerful unsupervised learning method. In Co-training, if we can find two independent feature sets for the target problem, any supervised learning method can be used. Furthermore, it is reported that Co-training is superior to the EM method if complete independent feature sets can be used(Nigam and Ghani, 2000). However, Co-training requires consistency besides independence for two feature sets. This condition makes it difficult to apply Cotraining to multiclass classification problems. On the other hand, the EM method requires Naive Bayes to be used as the supervised learning method, but can be applied to multiclass classification problems without any modification. Therefore, the EM method is more practical than Co-training. Yarowsky proposed the unsupervised learning method for WSD(Yarowsky, 1995). His method is reported to be a special case of Co-training(Blum and Mitchell, 1998). As two independ</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In 9th International Conference on Information and Knowledge Management, pages 86–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="1201" citStr="Nigam et al., 2000" startWordPosition="175" endWordPosition="178">ssification problems in order to apply it to word sense disambiguation (WSD) problems. The improved method stops the EM algorithm at the optimum iteration number. To estimate that number, we propose two methods. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2. The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for verb WSD problems. 1 Introduction In this paper, we improve an unsupervised learning method using the Expectation-Maximization (EM) algorithm proposed by (Nigam et al., 2000) for text classification problems in order to apply it to word sense disambiguation (WSD) problems. The original method works well, but often causes worse classification for WSD. To avoid this, we propose two methods to estimate the optimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. </context>
<context position="6914" citStr="Nigam et al., 2000" startWordPosition="1184" endWordPosition="1187">e do not look up the thesaurus ID for a word that consists of hiragana characters, because such words are too ambiguous, that is, they have too many thesaurus IDs. When a word has multiple thesaurus IDs, we create a feature for each ID. As a result, we get following ten features from the above example sentence: e1=wo, e2=suru, e3=saikou, e3=kako, e4=suru, e4=., e5=3192, e5=31920, e5=1164, e5=11642. 3 Unsupervised learning using EM algorithm We can use the EM method if we use Naive Bayes for classification problems. In this paper, we show only key equations and the key algorithm of this method(Nigam et al., 2000). Basically the method computes P(fi|cj) where fi is a feature and cj is a class. This probability is given by4 1 + E|D| k=1 N(fi, dk)P(cj|dk) |F |+ E|m |1 ED11 N(fm, dk)P(cj |dk) (2) 3In this paper we use the bunrui-goi-hyou as a Japanese thesaurus. 4This equation is smoothed by taking into account the frequency 0. P(fi|cj) = D: all data consisting of labeled data and unlabeled data dk: an element in D F: the set of all features fm: an element in F N(fi, dk): the number of fi in the instance dk. In our problem, N(fi, dk) is 0 or 1, and almost all of them are 0. If dk is labeled, P(cj|dk) is 0</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seong-Bae Park</author>
<author>Byoung-Tak Zhang</author>
<author>Yung Taek Kim</author>
</authors>
<title>Word sense disambiguation by learning from unlabeled data.</title>
<date>2000</date>
<booktitle>In 38th Annual Meeting of the Association for Computational Linguistics (ACL-00),</booktitle>
<pages>547--554</pages>
<contexts>
<context position="2034" citStr="Park et al., 2000" startWordPosition="308" endWordPosition="311">s to estimate the optimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park et al., 2000)(Li and Li, 2002). Among these methods, the method using the EM algorithm proposed by the paper(Nigam et al., 2000), which is referred to as the EM method in this paper, is the state of the art. However, the target of the EM method is text classification. It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing. The EM method works well in text classification, but often causes worse classification in WSD. The EM method is expected to improve the accuracy of learned rules step by step in proportion to the iteration number in th</context>
</contexts>
<marker>Park, Zhang, Kim, 2000</marker>
<rawString>Seong-Bae Park, Byoung-Tak Zhang, and Yung Taek Kim. 2000. Word sense disambiguation by learning from unlabeled data. In 38th Annual Meeting of the Association for Computational Linguistics (ACL-00), pages 547–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shinnou</author>
</authors>
<title>Learning of word sense disambiguation rules by Co-training, checking cooccurrence of features.</title>
<date>2002</date>
<booktitle>In 3rd international conference on Language resources and evaluation (LREC-2002),</booktitle>
<pages>1380--1384</pages>
<contexts>
<context position="24383" citStr="Shinnou, 2002" startWordPosition="4349" endWordPosition="4351">s well, but if it is not, unsupervised learning fails. Intuitively, feature independence warrants increasing P(c.,|f2). In noun WSD, the left context of the target word corresponds to the words modifying the target word, and the right context of the target word corresponds to the verb word whose case slot can have the target word. Both the left context and right context can judge the meaning of the target word by itself, and are independent. Left context and right context act as independent features. On the other hand, we cannot find such an opportune interpretation for the features of verbs (Shinnou, 2002). Therefore, the EM method is not so effective for verb words. Naive Bayes assumes the independence of features, too. However, this assumption is not so rigid in practice. We believe that the improvement by the EM method for verb words depends on the robustness of Naive Bayes. In our experiments, the EM method for noun words failed to boost the precision. We think that the cause is the imbalance of labeled data, unlabeled data and test data. We should investigate this in a future study. 6.5 Related works Co-training(Blum and Mitchell, 1998) is a powerful unsupervised learning method. In Co-tra</context>
</contexts>
<marker>Shinnou, 2002</marker>
<rawString>Hiroyuki Shinnou. 2002. Learning of word sense disambiguation rules by Co-training, checking cooccurrence of features. In 3rd international conference on Language resources and evaluation (LREC-2002), pages 1380–1384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In 33th Annual Meeting of the Association for Computational Linguistics (ACL-95),</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2015" citStr="Yarowsky, 1995" startWordPosition="307" endWordPosition="308">opose two methods to estimate the optimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park et al., 2000)(Li and Li, 2002). Among these methods, the method using the EM algorithm proposed by the paper(Nigam et al., 2000), which is referred to as the EM method in this paper, is the state of the art. However, the target of the EM method is text classification. It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing. The EM method works well in text classification, but often causes worse classification in WSD. The EM method is expected to improve the accuracy of learned rules step by step in proportion to the ite</context>
<context position="25745" citStr="Yarowsky, 1995" startWordPosition="4568" endWordPosition="4569">that Co-training is superior to the EM method if complete independent feature sets can be used(Nigam and Ghani, 2000). However, Co-training requires consistency besides independence for two feature sets. This condition makes it difficult to apply Cotraining to multiclass classification problems. On the other hand, the EM method requires Naive Bayes to be used as the supervised learning method, but can be applied to multiclass classification problems without any modification. Therefore, the EM method is more practical than Co-training. Yarowsky proposed the unsupervised learning method for WSD(Yarowsky, 1995). His method is reported to be a special case of Co-training(Blum and Mitchell, 1998). As two independent feature sets, one is the context surrounding the target word and the other is the heuristic of ‘one sense per discourse’. However, it is unknown how valid this heuristic is for granularity of meanings of our evaluation words. Furthermore, this method needs documents in which the target word appears multiple times, as unlabeled data. Therefore, it is not so easy to gather unlabeled data. On the other hand, the EM method does not have such problem because it uses sentences including the targ</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In 33th Annual Meeting of the Association for Computational Linguistics (ACL-95), pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>