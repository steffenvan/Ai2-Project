<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.989441">
Incremental Joint Extraction of Entity Mentions and Relations
</title>
<author confidence="0.999562">
Qi Li Heng Ji
</author>
<affiliation confidence="0.855039">
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180, USA
</affiliation>
<email confidence="0.99866">
{liq7,jih}@rpi.edu
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999094">
We present an incremental joint frame-
work to simultaneously extract entity men-
tions and relations using structured per-
ceptron with efficient beam-search. A
segment-based decoder based on the idea
of semi-Markov chain is adopted to the
new framework as opposed to traditional
token-based tagging. In addition, by virtue
of the inexact search, we developed a num-
ber of new and effective global features
as soft constraints to capture the inter-
dependency among entity mentions and
relations. Experiments on Automatic Con-
tent Extraction (ACE)1 corpora demon-
strate that our joint model significantly
outperforms a strong pipelined baseline,
which attains better performance than the
best-reported end-to-end system.
</bodyText>
<sectionHeader confidence="0.999522" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999204833333333">
The goal of end-to-end entity mention and re-
lation extraction is to discover relational struc-
tures of entity mentions from unstructured texts.
This problem has been artificially broken down
into several components such as entity mention
boundary identification, entity type classification
and relation extraction. Although adopting such
a pipelined approach would make a system com-
paratively easy to assemble, it has some limita-
tions: First, it prohibits the interactions between
components. Errors in the upstream components
are propagated to the downstream components
without any feedback. Second, it over-simplifies
the problem as multiple local classification steps
without modeling long-distance and cross-task de-
pendencies. By contrast, we re-formulate this
task as a structured prediction problem to reveal
the linguistic and logical properties of the hidden
</bodyText>
<footnote confidence="0.923806">
1http://www.itl.nist.gov/iad/mig//tests/ace
</footnote>
<bodyText confidence="0.999974853658537">
structures. For example, in Figure 1, the output
structure of each sentence can be interpreted as a
graph in which entity mentions are nodes and re-
lations are directed arcs with relation types. By
jointly predicting the structures, we aim to address
the aforementioned limitations by capturing: (i)
The interactions between two tasks. For exam-
ple, in Figure 1a, although it may be difficult for
a mention extractor to predict “1,400” as a Per-
son (PER) mention, the context word “employs”
between “tire maker” and “1,400” strongly in-
dicates an Employment-Organization (EMP-ORG)
relation which must involve a PER mention. (ii)
The global features of the hidden structure. Var-
ious entity mentions and relations share linguis-
tic and logical constraints. For example, we
can use the triangle feature in Figure 1b to en-
sure that the relations between “forces”, and each
of the entity mentions “Somalia/GPE”, “Haiti/GPE”
and “Kosovo/GPE”, are of the same type (Physical
(PHYS), in this case).
Following the above intuitions, we introduce
a joint framework based on structured percep-
tron (Collins, 2002; Collins and Roark, 2004) with
beam-search to extract entity mentions and rela-
tions simultaneously. With the benefit of inexact
search, we are also able to use arbitrary global
features with low cost. The underlying learning
algorithm has been successfully applied to some
other Natural Language Processing (NLP) tasks.
Our task differs from dependency parsing (such as
(Huang and Sagae, 2010)) in that relation struc-
tures are more flexible, where each node can have
arbitrary relation arcs. Our previous work (Li et
al., 2013) used perceptron model with token-based
tagging to jointly extract event triggers and argu-
ments. By contrast, we aim to address a more chal-
lenging task: identifying mention boundaries and
types together with relations, which raises the is-
sue that assignments for the same sentence with
different mention boundaries are difficult to syn-
</bodyText>
<page confidence="0.976351">
402
</page>
<note confidence="0.880184">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 402–412,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.9673391">
PHYS
EMP-ORG
The tire maker
\ Y J
ORG
still employs 1,400
\YJ
PER
conj and
EMP-ORG
PER
US
|{z}
� GPE
in Somalia
 |{z }
GPE
� Haiti
|{z}
GPE
and Kosovo
 |{z }
GPE
 |forces
{z
PER
conj and
GPE
GPE
(a) Interactions between Two Tasks (b) Example of Global Feature
</figure>
<figureCaption confidence="0.999972">
Figure 1: End-to-End Entity Mention and Relation Extraction.
</figureCaption>
<bodyText confidence="0.994434">
chronize during search. To tackle this problem,
we adopt a segment-based decoding algorithm de-
rived from (Sarawagi and Cohen, 2004; Zhang and
Clark, 2008) based on the idea of semi-Markov
chain (a.k.a, multiple-beam search algorithm).
Most previous attempts on joint inference of en-
tity mentions and relations (such as (Roth and Yih,
2004; Roth and Yih, 2007)) assumed that entity
mention boundaries were given, and the classifiers
of mentions and relations are separately learned.
As a key difference, we incrementally extract en-
tity mentions together with relations using a single
model. The main contributions of this paper are as
follows:
</bodyText>
<listItem confidence="0.99917225">
1. This is the first work to incrementally predict
entity mentions and relations using a single
joint model (Section 3).
2. Predicting mention boundaries in the joint
</listItem>
<bodyText confidence="0.9228705">
framework raises the challenge of synchroniz-
ing different assignments in the same beam. We
solve this problem by detecting entity mentions
on segment-level instead of traditional token-
based approaches (Section 3.1.1).
3. We design a set of novel global features based
on soft constraints over the entire output graph
structure with low cost (Section 4).
Experimental results show that the proposed
framework achieves better performance than
pipelined approaches, and global features provide
further significant gains.
</bodyText>
<sectionHeader confidence="0.981314" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.964544">
2.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.998881333333333">
The entity mention extraction and relation
extraction tasks we are addressing are those
of the Automatic Content Extraction (ACE)
program2. ACE defined 7 main entity types
including Person (PER), Organization (ORG),
Geographical Entities (GPE), Location (LOC),
</bodyText>
<footnote confidence="0.861469">
2http://www.nist.gov/speech/tests/ace
</footnote>
<listItem confidence="0.511859071428571">
Facility (FAC), Weapon (WEA) and Vehicle
(VEH). The goal of relation extraction3 is to
extract semantic relations of the targeted types
between a pair of entity mentions which ap-
pear in the same sentence. ACE’04 defined 7
main relation types: Physical (PHYS), Person-
Social (PER-SOC), Employment-Organization
(EMP-ORG), Agent-Artifact (ART), PER/ORG
Affiliation (Other-AFF), GPE-Affiliation
(GPE-AFF) and Discourse (DISC). ACE’05 kept
PER-SOC, ART and GPE-AFF, split PHYS into
PHYS and a new relation type Part-Whole,
removed DISC, and merged EMP-ORG and
Other-AFF into EMP-ORG.
</listItem>
<bodyText confidence="0.9997489">
Throughout this paper, we use ⊥ to denote non-
entity or non-relation classes. We consider rela-
tion asymmetric. The same relation type with op-
posite directions is considered to be two classes,
which we refer to as directed relation types.
Most previous research on relation extraction
assumed that entity mentions were given In this
work we aim to address the problem of end-to-end
entity mention and relation extraction from raw
texts.
</bodyText>
<subsectionHeader confidence="0.999197">
2.2 Baseline System
</subsectionHeader>
<bodyText confidence="0.999689357142857">
In order to develop a baseline system repre-
senting state-of-the-art pipelined approaches, we
trained a linear-chain Conditional Random Fields
model (Lafferty et al., 2001) for entity mention ex-
traction and a Maximum Entropy model for rela-
tion extraction.
Entity Mention Extraction Model We re-cast
the problem of entity mention extraction as a se-
quential token tagging task as in the state-of-the-
art system (Florian et al., 2006). We applied the
BILOU scheme, where each tag means a token is
the Beginning, Inside, Last, Outside, and Unit of
an entity mention, respectively. Most of our fea-
tures are similar to the work of (Florian et al.,
</bodyText>
<footnote confidence="0.987726">
3Throughout this paper we refer to relation mention as re-
lation since we do not consider relation mention coreference.
</footnote>
<page confidence="0.999121">
403
</page>
<bodyText confidence="0.998953">
2004; Florian et al., 2006) except that we do not
have their gazetteers and outputs from other men-
tion detection systems as features. Our additional
features are as follows:
</bodyText>
<listItem confidence="0.99378">
• Governor word of the current token based on de-
pendency parsing (Marneffe et al., 2006).
• Prefix of each word in Brown clusters learned
from TDT5 corpus (Sun et al., 2011).
</listItem>
<sectionHeader confidence="0.517848" genericHeader="method">
Relation Extraction Model Given a sentence
</sectionHeader>
<bodyText confidence="0.998339714285714">
with entity mention annotations, the goal of base-
line relation extraction is to classify each mention
pair into one of the pre-defined relation types with
direction or L (non-relation). Most of our relation
extraction features are based on the previous work
of (Zhou et al., 2005) and (Kambhatla, 2004). We
designed the following additional features:
</bodyText>
<listItem confidence="0.983119875">
• The label sequence of phrases covering the two
mentions. For example, for the sentence in Fig-
ure 1a, the sequence is “NP VP NP”. We also
augment it by head words of each phrase.
• Four syntactico - semantic patterns described in
(Chan and Roth, 2010).
• We replicated each lexical feature by replacing
each word with its Brown cluster.
</listItem>
<sectionHeader confidence="0.990009" genericHeader="method">
3 Algorithm
</sectionHeader>
<subsectionHeader confidence="0.985168">
3.1 The Model
</subsectionHeader>
<bodyText confidence="0.999935714285714">
Our goal is to predict the hidden structure of
each sentence based on arbitrary features and con-
straints. Let x E X be an input sentence, y&apos; E Y
be a candidate structure, and f(x, y&apos;) be the fea-
ture vector that characterizes the entire structure.
We use the following linear model to predict the
most probable structure yˆ for x:
</bodyText>
<equation confidence="0.9985185">
yˆ = argmax f(x, y&apos;) · w (1)
y&apos;EY(x)
</equation>
<bodyText confidence="0.9999739">
where the score of each candidate assignment is
defined as the inner product of the feature vector
f(x, y&apos;) and feature weights w.
Since the structures contain both entity men-
tions relations, and we also aim to exploit global
features. There does not exist a polynomial-time
algorithm to find the best structure. In practice
we apply beam-search to expand partial configu-
rations for the input sentence incrementally to find
the structure with the highest score.
</bodyText>
<subsectionHeader confidence="0.812823">
3.1.1 Joint Decoding Algorithm
</subsectionHeader>
<bodyText confidence="0.999993">
One main challenge to search for entity mentions
and relations incrementally is the alignment of dif-
ferent assignments. Assignments for the same sen-
tence can have different numbers of entity men-
tions and relation arcs. The entity mention ex-
traction task is often re-cast as a token-level se-
quential labeling problem with BIO or BILOU
scheme (Ratinov and Roth, 2009; Florian et al.,
2006). A naive solution to our task is to adopt this
strategy by treating each token as a state. How-
ever, different assignments for the same sentence
can have various mention boundaries. It is un-
fair to compare the model scores of a partial men-
tion and a complete mention. It is also difficult to
synchronize the search process of relations. For
example, consider the two hypotheses ending at
“York” for the same sentence:
</bodyText>
<equation confidence="0.9500165">
PHYS
AllanU-PER from1 NewB-GPE YorkL-GPE Stock Exchange
AllanU-PER from1 NewB-ORG YorkI-ORG Stock Exchange
PHYS
</equation>
<bodyText confidence="0.985075896551724">
The model would bias towards the incorrect as-
signment “New/B-GPE York/L-GPE” since it can
have more informative features as a complete
mention (e.g., a binary feature indicating if the
entire mention appears in a GPE gazetter). Fur-
thermore, the predictions of the two PHYS rela-
tions cannot be synchronized since “New/B-FAC
York/I-FAC” is not yet a complete mention.
To tackle these problems, we employ the idea of
semi-Markov chain (Sarawagi and Cohen, 2004),
in which each state corresponds to a segment
of the input sequence. They presented a vari-
ant of Viterbi algorithm for exact inference in
semi-Markov chain. We relax the max operation
by beam-search, resulting in a segment-based de-
coder similar to the multiple-beam algorithm in
(Zhang and Clark, 2008). Let dˆ be the upper bound
of entity mention length. The k-best partial assign-
ments ending at the i-th token can be calculated as:
B[i] = k-BEST f(x, y&apos;) · w
y&apos;E{y[1..i]|y[1:i−d]EB[i−d], d=1... ˆd}
where y[1:i−d] stands for a partial configuration
ending at the (i-d)-th token, and y[i−d+1,i] corre-
sponds to the structure of a new segment (i.e., sub-
sequence of x) x[i−d+1,i]. Our joint decoding algo-
rithm is shown in Figure 2. For each token index
i, it maintains a beam for the partial assignments
whose last segments end at the i-th token. There
are two types of actions during the search:
</bodyText>
<page confidence="0.995062">
404
</page>
<bodyText confidence="0.942456666666667">
Input: input sentence x = (x1,x2, ..., xm).
k: beam size.
T U {1}: entity mention type alphabet.
R U {1}: directed relation type alphabet.4
dt: max length of type-t segment, t E T U {⊥}.
Output: best configuration yˆ for x
</bodyText>
<figure confidence="0.955421529411765">
1 initialize m empty beams B[1..m]
2 for i +- 1...m do
3 for t E T U {1} do
4 ford +- 1...dt, y&apos; E B[i − d] do
5 k +- i − d + 1
6 B[i] +- B[i] U APPEND(y&apos;, t, k, i)
7 B[i] +- k-BEST(B[i])
8 for j +- (i − 1)...1 do
9 buf +- ∅
10 for y&apos; E B[i] do
11 if HASPAIR(y&apos;, i, j) then
12 for r E R U {1} do
13 buf +- buf U LINK(y&apos;, r, i, j)
14 else
15 buf +- buf U {y&apos;}
16 B[i] +- k-BEST(buf)
17 return B[m][0]
</figure>
<figureCaption confidence="0.995973">
Figure 2: Joint Decoding for Entity Men-
</figureCaption>
<bodyText confidence="0.916271125">
tions and Relations. HASPAIR(y&apos;, i, j) checks
if there are two entity mentions in y&apos; that
end at token xi and token xj, respectively.
APPEND(y&apos;, t, k, i) appends y&apos; with a type-t
segment spanning from xk to xi. Similarly
LINK(y&apos;, r, i, j) augments y&apos; by assigning a di-
rected relation r to the pair of entity mentions
ending at xi and xj respectively.
</bodyText>
<listItem confidence="0.955361733333334">
1. APPEND (Lines 3-7). First, the algorithm
enumerates all possible segments (i.e., subse-
quences) of x ending at the current token with
various entity types. A special type of seg-
ment is a single token with non-entity label (1).
Each segment is then appended to existing par-
tial assignments in one of the previous beams to
form new assignments. Finally the top k results
are recorded in the current beam.
2. LINK (Lines 8-16). After each step of APPEND,
the algorithm looks backward to link the newly
identified entity mentions and previous ones (if
any) with relation arcs. At the j-th sub-step,
it only considers the previous mention ending
at the j-th previous token. Therefore different
</listItem>
<footnote confidence="0.501758">
4The same relation type with opposite directions is con-
sidered to be two classes in R.
</footnote>
<bodyText confidence="0.999873285714286">
configurations are guaranteed to have the same
number of sub-steps. Finally, all assignments
are re-ranked with new relation information.
There are m APPEND actions, each is followed by
at most (i−1) LINK actions (line 8). Therefore the
worst-case time complexity is O(ˆd·k ·m2), where
dˆ is the upper bound of segment length.
</bodyText>
<figure confidence="0.85293275">
3.1.2 Example Demonstration
ORG
PER
r mak r m 00
</figure>
<figureCaption confidence="0.996179">
Figure 3: Example of decoding steps. x-axis
</figureCaption>
<bodyText confidence="0.97609128">
and y-axis represent the input sentence and en-
tity types, respectively. The rectangles denote seg-
ments with entity types, among which the shaded
ones are three competing hypotheses ending at
“1,400”. The solid lines and arrows indicate cor-
rect APPEND and LINK actions respectively, while
the dashed indicate incorrect actions.
Here we demonstrate a simple but concrete ex-
ample by considering again the sentence described
in Figure 1a. Suppose we are at the token “1,400”.
At this point we can propose multiple entity men-
tions with various lengths. Assuming “1,400/PER”,
“1,400/1” and “(employs 1,400)/PER” are possi-
ble assignments, the algorithm appends these new
segments to the partial assignments in the beams
of the tokens “employs” and “still”, respectively.
Figure 3 illustrates this process. For simplicity,
only a small part of the search space is presented.
The algorithm then links the newly identified men-
tions to the previous ones in the same configu-
ration. In this example, the only previous men-
tion is “(tire maker)/ORG”. Finally, “1,400/PER” will
be preferred by the model since there are more
indicative context features for EMP-ORG relation
between “(tire maker)/PER” and “1,400/PER”.
</bodyText>
<page confidence="0.995418">
405
</page>
<subsectionHeader confidence="0.991879">
3.2 Structured-Perceptron Learning
</subsectionHeader>
<bodyText confidence="0.999965833333333">
To estimate the feature weights, we use struc-
tured perceptron (Collins, 2002), an extension
of the standard perceptron for structured pre-
diction, as the learning framework. Huang et
al. (2012) proved the convergency of structured
perceptron when inexact search is applied with
violation-fixing update methods such as early-
update (Collins and Roark, 2004). Since we use
beam-search in this work, we apply early-update.
In addition, we use averaged parameters to reduce
overfitting as in (Collins, 2002).
Figure 4 shows the pseudocode for struc-
tured perceptron training with early-update. Here
BEAMSEARCH is identical to the decoding algo-
rithm described in Figure 2 except that if y&apos;, the
prefix of the gold standard y, falls out of the beam
after each execution of the k-BEST function (line 7
and 16), then the top assignment z and y&apos; are re-
turned for parameter update. It is worth noting that
this can only happen if the gold-standard has a seg-
ment ending at the current token. For instance, in
the example of Figure 1a, B[2] cannot trigger any
early-update since the gold standard does not con-
tain any segment ending at the second token.
</bodyText>
<figure confidence="0.8318411">
Input: training set D = {(x(j), y(j))}Ni=1,
maximum iteration number T
Output: model parameters w
1 initialize w +— 0
2 for t +— 1...T do
3 foreach (x, y) E D do
4 (x, y&apos;, z) +— BEAMSEARCH (x, y, w)
5 if z =� y then
6 w +— w + f(x, y&apos;) — f(x, z)
7 return w
</figure>
<figureCaption confidence="0.912902">
Figure 4: Perceptron algorithm with beam-
search and early-update. y&apos; is the prefix of the
gold-standard and z is the top assignment.
</figureCaption>
<subsectionHeader confidence="0.995138">
3.3 Entity Type Constraints
</subsectionHeader>
<bodyText confidence="0.999731230769231">
Entity type constraints have been shown effective
in predicting relations (Roth and Yih, 2007; Chan
and Roth, 2010). We automatically collect a map-
ping table of permissible entity types for each rela-
tion type from our training data. Instead of apply-
ing the constraints in post-processing inference,
we prune the branches that violate the type con-
straints during search. This type of pruning can
reduce search space as well as make the input for
parameter update less noisy. In our experiments,
only 7 relation mentions (0.5%) in the dev set and
5 relation mentions (0.3%) in the test set violate
the constraints collected from the training data.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.9998672">
An advantage of our framework is that we can
easily exploit arbitrary features across the two
tasks. This section describes the local features
(Section 4.1) and global features (Section 4.2) we
developed in this work.
</bodyText>
<subsectionHeader confidence="0.99365">
4.1 Local Features
</subsectionHeader>
<bodyText confidence="0.99924078125">
We design segment-based features to directly eval-
uate the properties of an entity mention instead of
the individual tokens it contains. Let yˆ be a pre-
dicted structure of a sentence x. The entity seg-
ments of yˆ can be expressed as a list of triples
(e1, ..., em), where each segment ei = (ui, vi, ti)
is a triple of start index ui, end index vi, and entity
type ti. The following is an example of segment-
based feature:
This feature is triggered if the labels of the (i —1)-
th and the i-th segments are “+,ORG”, and the text
of the i-th segment is “tire maker”. Our segment-
based features are described as follows:
Gazetteer features Entity type of each segment
based on matching a number of gazetteers includ-
ing persons, countries, cities and organizations.
Case features Whether a segment’s words are
initial-capitalized, all lower cased, or mixture.
Contextual features Unigrams and bigrams of
the text and part-of-speech tags in a segment’s
contextual window of size 2.
Parsing-based features Features derived from
constituent parsing trees, including (a) the phrase
type of the lowest common ancestor of the tokens
contained in the segment, (b) the depth of the low-
est common ancestor, (c) a binary feature indicat-
ing if the segment is a base phrase or a suffix of a
base phrase, and (d) the head words of the segment
and its neighbor phrases.
In addition, we convert each triple (ui, vi, ti) to
BTLOU tags for the tokens it contains to imple-
ment token-based features. The token-based men-
</bodyText>
<table confidence="0.318896">
f001(x, ˆy,i) = ⎧ 1 if x[ˆy ui,ˆy vi] = tire maker
⎨⎪ ˆy.t(i−1), ˆy.ti = +,ORG
⎪⎩ 0 otherwise
</table>
<page confidence="0.986447">
406
</page>
<bodyText confidence="0.9902455">
tion features and local relation features are identi-
cal to those of our pipelined system (Section 2.2).
</bodyText>
<subsectionHeader confidence="0.99249">
4.2 Global Entity Mention Features
</subsectionHeader>
<bodyText confidence="0.999810090909091">
By virtue of the efficient inexact search, we are
able to use arbitrary features from the entire
structure of yˆ to capture long-distance dependen-
cies. The following features between related entity
mentions are extracted once a new segment is ap-
pended during decoding.
Coreference consistency Coreferential entity
mentions should be assigned the same entity type.
We determine high-recall coreference links be-
tween two segments in the same sentence using
some simple heuristic rules:
</bodyText>
<listItem confidence="0.868972181818182">
• Two segments exactly or partially string match.
• A pronoun (e.g., “their”,“it”) refers to previous
entity mentions. For example, in “they have
no insurance on their cars”, “they” and “their”
should have the same entity type.
• A relative pronoun (e.g., “which”,“that”, and
“who”) refers to the noun phrase it modifies in
the parsing tree. For example, in “the starting
kicker is nikita kargalskiy, who may be 5,000
miles from his hometown”, “nikita kargalskiy”
and “who” should both be labeled as persons.
</listItem>
<bodyText confidence="0.999961034482759">
Then we encode a global feature to check
whether two coreferential segments share the same
entity type. This feature is particularly effective
for pronouns because their contexts alone are of-
ten not informative.
Neighbor coherence Neighboring entity men-
tions tend to have coherent entity types. For ex-
ample, in “Barbara Starr was reporting from the
Pentagon”, “Barbara Starr” and “Pentagon” are
connected by a dependency link prep from and
thus they are unlikely to be a pair of PER men-
tions. Two types of neighbor are considered: (i)
the first entity mention before the current segment,
and (ii) the segment which is connected by a sin-
gle word or a dependency link with the current
segment. We take the entity types of the two seg-
ments and the linkage together as a global feature.
For instance, “PER prep from PER” is a feature
for the above example when “Barbara Starr” and
“Pentagon” are both labeled as PER mentions.
Part-of-whole consistency If an entity men-
tion is semantically part of another mention (con-
nected by a prep of dependency link), they should
be assigned the same entity type. For example,
in “some of Iraq’s exiles”, “some” and “exiles”
are both PER mentions; in “one of the town’s two
meat-packing plants”, “one” and “plants” are both
FAC mentions; in “the rest ofAmerica”, “rest” and
“America” are both GPE mentions.
</bodyText>
<subsectionHeader confidence="0.996442">
4.3 Global Relation Features
</subsectionHeader>
<bodyText confidence="0.999939392857143">
Relation arcs can also share inter-dependencies or
obey soft constraints. We extract the following
relation-centric global features when a new rela-
tion hypothesis is made during decoding.
Role coherence If an entity mention is involved
in multiple relations with the same type, then its
roles should be coherent. For example, a PER
mention is unlikely to have more than one em-
ployer. However, a GPE mention can be a physical
location for multiple entity mentions. We combine
the relation type and the entity mention’s argument
roles as a global feature, as shown in Figure 5a.
Triangle constraint Multiple entity mentions
are unlikely to be fully connected with the same
relation type. We use a negative feature to penalize
any configuration that contains this type of struc-
ture. An example is shown in Figure 5b.
Inter-dependent compatibility If two entity
mentions are connected by a dependency link, they
tend to have compatible relations with other enti-
ties. For example, in Figure 5c, the conj and de-
pendency link between “Somalia” and “Kosovo”
indicates they may share the same relation type
with the third entity mention “forces”.
Neighbor coherence Similar to the entity men-
tion neighbor coherence feature, we also combine
the types of two neighbor relations in the same
sentence as a bigram feature.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99853">
5.1 Data and Scoring Metric
</subsectionHeader>
<bodyText confidence="0.999975857142857">
Most previous work on ACE relation extraction
has reported results on ACE’04 data set. As
we will show later in our experiments, ACE’05
made significant improvement on both relation
type definition and annotation quality. Therefore
we present the overall performance on ACE’05
data. We removed two small subsets in informal
genres - cts and un, and then randomly split the re-
maining 511 documents into 3 parts: 351 for train-
ing, 80 for development, and the rest 80 for blind
test. In order to compare with state-of-the-art we
also performed the same 5-fold cross-validation on
bnews and nwire subsets of ACE’04 corpus as in
previous work. The statistics of these data sets
</bodyText>
<page confidence="0.992437">
407
</page>
<figure confidence="0.998086">
(a) (b) (c)
</figure>
<figureCaption confidence="0.995931">
Figure 5: Examples of Global Relation Features.
</figureCaption>
<figure confidence="0.999783363636364">
(PER forces)
conj and
(GPE Kosovo)
PHYS
(GPE US)
(GPE Haiti)
(GPE Somalia)
(PER forces)
(GPE Somalia)
(PER forces)
(GPE Somalia)
0.700 5 10 15 20 25
# of training iterations
(a) Entity Mention Performance
0.55
0.50
0.45
0.40
0.35
0.300 5 10 15 20 25
# of training iterations
(b) Relation Performance
</figure>
<figureCaption confidence="0.79597325">
Figure 6: Learning Curves on Development Set.
are summarized in Table 1. We ran the Stanford
CoreNLP toolkit5 to automatically recover the true
cases for lowercased documents.
</figureCaption>
<table confidence="0.999866">
Data Set # sentences # mentions # relations
ACE’05 Train 7,273 26,470 4,779
Dev 1,765 6,421 1,179
Test 1,535 5,476 1,147
ACE’04 6,789 22,740 4,368
</table>
<tableCaption confidence="0.999588">
Table 1: Data Sets.
</tableCaption>
<bodyText confidence="0.999954125">
We use the standard F1 measure to evaluate the
performance of entity mention extraction and re-
lation extraction. An entity mention is considered
correct if its entity type is correct and the offsets
of its mention head are correct. A relation men-
tion is considered correct if its relation type is
correct, and the head offsets of two entity men-
tion arguments are both correct. As in Chan and
</bodyText>
<footnote confidence="0.755677">
5http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<bodyText confidence="0.999292166666667">
Roth (2011), we excluded the DISC relation type,
and removed relations in the system output which
are implicitly correct via coreference links for fair
comparison. Furthermore, we combine these two
criteria to evaluate the performance of end-to-end
entity mention and relation extraction.
</bodyText>
<subsectionHeader confidence="0.999394">
5.2 Development Results
</subsectionHeader>
<bodyText confidence="0.9999897">
In general a larger beam size can yield better per-
formance but increase training and decoding time.
As a tradeoff, we set the beam size as 8 through-
out the experiments. Figure 6 shows the learn-
ing curves on the development set, and compares
the performance with and without global features.
From these figures we can clearly see that global
features consistently improve the extraction per-
formance of both tasks. We set the number of
training iterations as 22 based on these curves.
</bodyText>
<subsectionHeader confidence="0.999193">
5.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.999915391304348">
Table 2 shows the overall performance of various
methods on the ACE’05 test data. We compare
our proposed method (Joint w/ Global) with the
pipelined system (Pipeline), the joint model with
only local features (Joint w/ Local), and two hu-
man annotators who annotated 73 documents in
ACE’05 corpus.
We can see that our approach significantly out-
performs the pipelined approach for both tasks. As
a real example, for the partial sentence “a marcher
from Florida” from the test data, the pipelined ap-
proach failed to identify “marcher” as a PER men-
tion, and thus missed the GEN-AFF relation be-
tween “marcher” and “Florida”. Our joint model
correctly identified the entity mentions and their
relation. Figure 7 shows the details when the
joint model is applied to this sentence. At the
token “marcher”, the top hypothesis in the beam
is “(1,1)”, while the correct one is ranked sec-
ond best. After the decoder processes the token
“Florida”, the correct hypothesis is promoted to
the top in the beam by the Neighbor Coherence
features for PER-GPE pair. Furthermore, after
</bodyText>
<figure confidence="0.979977090909091">
mention local+global
mention local
0.80
0.78
F_1 score
0.76
0.74
0.72
F_1 score
relation local+global
relation local
</figure>
<page confidence="0.972386">
408
</page>
<table confidence="0.999296625">
Model Entity Mention (%) Relation (%) Entity Mention + Relation (%)
Score P R Fl P R Fl P R Fl
Pipeline 83.2 73.6 78.1 67.5 39.4 49.8 65.1 38.1 48.0
Joint w/ Local 84.5 76.0 80.0 68.4 40.1 50.6 65.3 38.3 48.3
Joint w/ Global 85.2 76.9 80.8 68.9 41.9 52.1 65.4 39.8 49.5
Annotator 1 91.8 89.9 90.9 71.9 69.0 70.4 69.5 66.7 68.1
Annotator 2 88.7 88.3 88.5 65.2 63.6 64.4 61.8 60.2 61.0
Inter-Agreement 85.8 87.3 86.5 55.4 54.7 55.0 52.3 51.6 51.9
</table>
<tableCaption confidence="0.978223">
Table 2: Overall performance on ACE’05 corpus.
</tableCaption>
<table confidence="0.99744325">
ra k
(a) ha? marcher?i 1
ha? marcherPE�i 2
( ) 1
ha? marcher? from?i
ha? marcherPEE from?i
( ) ha? marcherPE� from? FloridaGPEi 1
ha? marcher? from? FloridaGPEi 2
GEN-AFF
( ) 1
ha? marcherPE� from? FloridaGPEi
ha? marcher? from? FloridaGPEi
</table>
<figureCaption confidence="0.91372">
Figure 7: Two competing hypotheses for “a
marcher from Florida” during decoding.
</figureCaption>
<bodyText confidence="0.999868090909091">
linking the two mentions by GEN-AFF relation,
the ranking of the incorrect hypothesis “(1,1)”
is dropped to the 4-th place in the beam, resulting
in a large margin from the correct hypothesis.
The human F1 score on end-to-end relation ex-
traction is only about 70%, which indicates it is a
very challenging task. Furthermore, the F1 score
of the inter-annotator agreement is 51.9%, which
is only 2.4% above that of our proposed method.
Compared to human annotators, the bottleneck
of automatic approaches is the low recall of rela-
tion extraction. Among the 631 remaining miss-
ing relations, 318 (50.3%) of them were caused
by missing entity mention arguments. A lot of
nominal mention heads rarely appear in the train-
ing data, such as persons (“supremo”, “shep-
herd”, “oligarchs”, “rich”), geo-political entity
mentions (“stateside”), facilities (“roadblocks”,
“cells”), weapons (“sim lant”, “nukes”) and ve-
hicles (“prams”). In addition, relations are often
implicitly expressed in a variety of forms. Some
examples are as follows:
</bodyText>
<listItem confidence="0.977220888888889">
• “Rice has been chosen by President Bush to
become the new Secretary of State” indicates
“Rice” has a PER-SOC relation with “Bush”.
• “U.S. troops are now knocking on the door of
Baghdad” indicates “troops” has a PHYS rela-
tion with “Baghdad”.
• “Russia and France sent planes to Baghdad” in-
dicates “Russia” and “France” are involved in
an ART relation with “planes” as owners.
</listItem>
<bodyText confidence="0.997586666666667">
In addition to contextual features, deeper se-
mantic knowledge is required to capture such im-
plicit semantic relations.
</bodyText>
<subsectionHeader confidence="0.999831">
5.4 Comparison with State-of-the-art
</subsectionHeader>
<bodyText confidence="0.999946705882353">
Table 3 compares the performance on ACE’04
corpus. For entity mention extraction, our joint
model achieved 79.7% on 5-fold cross-validation,
which is comparable with the best F1 score 79.2%
reported by (Florian et al., 2006) on single-
fold. However, Florian et al. (2006) used some
gazetteers and the output of other Information Ex-
traction (IE) models as additional features, which
provided significant gains ((Florian et al., 2004)).
Since these gazetteers, additional data sets and ex-
ternal IE models are all not publicly available, it is
not fair to directly compare our joint model with
their results.
For end-to-end entity mention and relation ex-
traction, both the joint approach and the pipelined
baseline outperform the best results reported
by (Chan and Roth, 2011) under the same setting.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9561241">
Entity mention extraction (e.g., (Florian et al.,
2004; Florian et al., 2006; Florian et al., 2010; Zi-
touni and Florian, 2008; Ohta et al., 2012)) and
relation extraction (e.g., (Reichartz et al., 2009;
Sun et al., 2011; Jiang and Zhai, 2007; Bunescu
and Mooney, 2005; Zhao and Grishman, 2005;
Culotta and Sorensen, 2004; Zhou et al., 2007;
Qian and Zhou, 2010; Qian et al., 2008; Chan
and Roth, 2011; Plank and Moschitti, 2013)) have
drawn much attention in recent years but were
</bodyText>
<page confidence="0.996536">
409
</page>
<table confidence="0.998844833333333">
Model Entity Mention (%) Relation (%) Entity Mention + Relation (%)
Score P R Fl P R Fl P R Fl
Chan and Roth (2011) - 42.9 38.9 40.8 -
Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9
Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1
Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3
</table>
<tableCaption confidence="0.996898">
Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant
</tableCaption>
<bodyText confidence="0.988973023255814">
improvement as measured by paired t-test (p &lt; 0.01)
usually studied separately. Most relation extrac-
tion work assumed that entity mention boundaries
and/or types were given. Chan and Roth (2011) re-
ported the best results using predicted entity men-
tions.
Some previous work used relations and en-
tity mentions to enhance each other in joint
inference frameworks, including re-ranking (Ji
and Grishman, 2005), Integer Linear Program-
ming (ILP) (Roth and Yih, 2004; Roth and Yih,
2007; Yang and Cardie, 2013), and Card-pyramid
Parsing (Kate and Mooney, 2010). All these
work noted the advantage of exploiting cross-
component interactions and richer knowledge.
However, they relied on models separately learned
for each subtask. As a key difference, our ap-
proach jointly extracts entity mentions and rela-
tions using a single model, in which arbitrary soft
constraints can be easily incorporated. Some other
work applied probabilistic graphical models for
joint extraction (e.g., (Singh et al., 2013; Yu and
Lam, 2010)). By contrast, our work employs an
efficient joint search algorithm without modeling
joint distribution over numerous variables, there-
fore it is more flexible and computationally sim-
pler. In addition, (Singh et al., 2013) used gold-
standard mention boundaries.
Our previous work (Li et al., 2013) used struc-
tured perceptron with token-based decoder to
jointly predict event triggers and arguments based
on the assumption that entity mentions and other
argument candidates are given as part of the in-
put. In this paper, we solve a more challeng-
ing problem: take raw texts as input and identify
the boundaries, types of entity mentions and rela-
tions all together in a single model. Sarawagi and
Cohen (2004) proposed a segment-based CRFs
model for name tagging. Zhang and Clark (2008)
used a segment-based decoder for word segmenta-
tion and pos tagging. We extended the similar idea
to our end-to-end task by incrementally predicting
relations along with entity mention segments.
</bodyText>
<sectionHeader confidence="0.997852" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999392809523809">
In this paper we introduced a new architecture
for more powerful end-to-end entity mention and
relation extraction. For the first time, we ad-
dressed this challenging task by an incremental
beam-search algorithm in conjunction with struc-
tured perceptron. While detecting mention bound-
aries jointly with other components raises the chal-
lenge of synchronizing multiple assignments in
the same beam, a simple yet effective segment-
based decoder is adopted to solve this problem.
More importantly, we exploited a set of global fea-
tures based on linguistic and logical properties of
the two tasks to predict more coherent structures.
Experiments demonstrated our approach signifi-
cantly outperformed pipelined approaches for both
tasks and dramatically advanced state-of-the-art.
In future work, we plan to explore more soft and
hard constraints to reduce search space as well as
improve accuracy. In addition, we aim to incorpo-
rate other IE components such as event extraction
into the joint model.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999971176470588">
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award and RPI faculty start-up
grant. The views and conclusions contained in
this document are those of the authors and should
not be interpreted as representing the official poli-
cies, either expressed or implied, of the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on.
</bodyText>
<page confidence="0.996965">
410
</page>
<sectionHeader confidence="0.995852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999672784313725">
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proc. HLT/EMNLP, pages 724–731.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In Proc.
COLING, pages 152–160.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551–560.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111–118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1–8.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL,
pages 423–429.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In Proc. HLT-NAACL, pages 1–8.
Radu Florian, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex models:
A case study in mention detection. In Proc. ACL.
Radu Florian, John F. Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection robust-
ness to noisy input. In Proc. EMNLP, pages 335–
345.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
ACL, pages 1077–1086.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142–151.
Heng Ji and Ralph Grishman. 2005. Improving name
tagging by reference resolution and relation detec-
tion. In Proc. ACL, pages 411–418.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extrac-
tion. In Proc. HLT-NAACL.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proc. ACL,
pages 178–181.
Rohit J. Kate and Raymond Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proc. ACL, pages 203–212.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML, pages 282–289.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73–82.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 449,454.
Tomoko Ohta, Sampo Pyysalo, Jun’ichi Tsujii, and
Sophia Ananiadou. 2012. Open-domain anatomi-
cal entity mention detection. In Proc. ACL Work-
shop on Detecting Structure in Scholarly Discourse,
pages 27–36.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proc.
ACL, pages 1498–1507.
Longhua Qian and Guodong Zhou. 2010. Clustering-
based stratified seed sampling for semi-supervised
relation classification. In Proc. EMNLP, pages 346–
355.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proc. COLING, pages 697–704.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. CONLL, pages 147–155.
Frank Reichartz, Hannes Korte, and Gerhard Paass.
2009. Composite kernels for relation extraction. In
Proc. ACL-IJCNLP (Short Papers), pages 365–368.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proc. CoNLL.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proc. NIPS.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proc. ACL, pages 521–529.
</reference>
<page confidence="0.981343">
411
</page>
<reference confidence="0.989967166666667">
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640–1649.
Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Proc. COLING
(Posters), pages 1399–1407.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proc. ACL, pages 1147–1157.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL, pages 419–426.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL, pages 427–434.
Guodong Zhou, Min Zhang, Dong-Hong Ji, and
Qiaoming Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proc. EMNLP-CoNLL, pages
728–736.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proc. EMNLP,
pages 600–609.
</reference>
<page confidence="0.997627">
412
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970343">
<title confidence="0.998563">Incremental Joint Extraction of Entity Mentions and Relations</title>
<author confidence="0.999946">Qi Li Heng Ji</author>
<affiliation confidence="0.997612">Computer Science Rensselaer Polytechnic</affiliation>
<address confidence="0.993381">Troy, NY 12180,</address>
<abstract confidence="0.999051210526316">We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search. A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a number of new and effective global features as soft constraints to capture the interdependency among entity mentions and relations. Experiments on Automatic Con- Extraction corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP,</booktitle>
<pages>724--731</pages>
<contexts>
<context position="30621" citStr="Bunescu and Mooney, 2005" startWordPosition="5033" endWordPosition="5036">s, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicat</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proc. HLT/EMNLP, pages 724–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting background knowledge for relation extraction.</title>
<date>2010</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>152--160</pages>
<contexts>
<context position="8747" citStr="Chan and Roth, 2010" startWordPosition="1341" endWordPosition="1344">Given a sentence with entity mention annotations, the goal of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or L (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment it by head words of each phrase. • Four syntactico - semantic patterns described in (Chan and Roth, 2010). • We replicated each lexical feature by replacing each word with its Brown cluster. 3 Algorithm 3.1 The Model Our goal is to predict the hidden structure of each sentence based on arbitrary features and constraints. Let x E X be an input sentence, y&apos; E Y be a candidate structure, and f(x, y&apos;) be the feature vector that characterizes the entire structure. We use the following linear model to predict the most probable structure yˆ for x: yˆ = argmax f(x, y&apos;) · w (1) y&apos;EY(x) where the score of each candidate assignment is defined as the inner product of the feature vector f(x, y&apos;) and feature w</context>
<context position="17117" citStr="Chan and Roth, 2010" startWordPosition="2794" endWordPosition="2797">update since the gold standard does not contain any segment ending at the second token. Input: training set D = {(x(j), y(j))}Ni=1, maximum iteration number T Output: model parameters w 1 initialize w +— 0 2 for t +— 1...T do 3 foreach (x, y) E D do 4 (x, y&apos;, z) +— BEAMSEARCH (x, y, w) 5 if z =� y then 6 w +— w + f(x, y&apos;) — f(x, z) 7 return w Figure 4: Perceptron algorithm with beamsearch and early-update. y&apos; is the prefix of the gold-standard and z is the top assignment. 3.3 Entity Type Constraints Entity type constraints have been shown effective in predicting relations (Roth and Yih, 2007; Chan and Roth, 2010). We automatically collect a mapping table of permissible entity types for each relation type from our training data. Instead of applying the constraints in post-processing inference, we prune the branches that violate the type constraints during search. This type of pruning can reduce search space as well as make the input for parameter update less noisy. In our experiments, only 7 relation mentions (0.5%) in the dev set and 5 relation mentions (0.3%) in the test set violate the constraints collected from the training data. 4 Features An advantage of our framework is that we can easily exploi</context>
</contexts>
<marker>Chan, Roth, 2010</marker>
<rawString>Yee Seng Chan and Dan Roth. 2010. Exploiting background knowledge for relation extraction. In Proc. COLING, pages 152–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting syntactico-semantic structures for relation extraction.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="30314" citStr="Chan and Roth, 2011" startWordPosition="4981" endWordPosition="4984">mparable with the best F1 score 79.2% reported by (Florian et al., 2006) on singlefold. However, Florian et al. (2006) used some gazetteers and the output of other Information Extraction (IE) models as additional features, which provided significant gains ((Florian et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P</context>
</contexts>
<marker>Chan, Roth, 2011</marker>
<rawString>Yee Seng Chan and Dan Roth. 2011. Exploiting syntactico-semantic structures for relation extraction. In Proc. ACL, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="2941" citStr="Collins and Roark, 2004" startWordPosition="429" endWordPosition="432">“tire maker” and “1,400” strongly indicates an Employment-Organization (EMP-ORG) relation which must involve a PER mention. (ii) The global features of the hidden structure. Various entity mentions and relations share linguistic and logical constraints. For example, we can use the triangle feature in Figure 1b to ensure that the relations between “forces”, and each of the entity mentions “Somalia/GPE”, “Haiti/GPE” and “Kosovo/GPE”, are of the same type (Physical (PHYS), in this case). Following the above intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event triggers and arguments. B</context>
<context position="15800" citStr="Collins and Roark, 2004" startWordPosition="2552" endWordPosition="2555">n this example, the only previous mention is “(tire maker)/ORG”. Finally, “1,400/PER” will be preferred by the model since there are more indicative context features for EMP-ORG relation between “(tire maker)/PER” and “1,400/PER”. 405 3.2 Structured-Perceptron Learning To estimate the feature weights, we use structured perceptron (Collins, 2002), an extension of the standard perceptron for structured prediction, as the learning framework. Huang et al. (2012) proved the convergency of structured perceptron when inexact search is applied with violation-fixing update methods such as earlyupdate (Collins and Roark, 2004). Since we use beam-search in this work, we apply early-update. In addition, we use averaged parameters to reduce overfitting as in (Collins, 2002). Figure 4 shows the pseudocode for structured perceptron training with early-update. Here BEAMSEARCH is identical to the decoding algorithm described in Figure 2 except that if y&apos;, the prefix of the gold standard y, falls out of the beam after each execution of the k-BEST function (line 7 and 16), then the top assignment z and y&apos; are returned for parameter update. It is worth noting that this can only happen if the gold-standard has a segment endin</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. ACL, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2915" citStr="Collins, 2002" startWordPosition="427" endWordPosition="428">ploys” between “tire maker” and “1,400” strongly indicates an Employment-Organization (EMP-ORG) relation which must involve a PER mention. (ii) The global features of the hidden structure. Various entity mentions and relations share linguistic and logical constraints. For example, we can use the triangle feature in Figure 1b to ensure that the relations between “forces”, and each of the entity mentions “Somalia/GPE”, “Haiti/GPE” and “Kosovo/GPE”, are of the same type (Physical (PHYS), in this case). Following the above intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event</context>
<context position="15523" citStr="Collins, 2002" startWordPosition="2513" endWordPosition="2514">he beams of the tokens “employs” and “still”, respectively. Figure 3 illustrates this process. For simplicity, only a small part of the search space is presented. The algorithm then links the newly identified mentions to the previous ones in the same configuration. In this example, the only previous mention is “(tire maker)/ORG”. Finally, “1,400/PER” will be preferred by the model since there are more indicative context features for EMP-ORG relation between “(tire maker)/PER” and “1,400/PER”. 405 3.2 Structured-Perceptron Learning To estimate the feature weights, we use structured perceptron (Collins, 2002), an extension of the standard perceptron for structured prediction, as the learning framework. Huang et al. (2012) proved the convergency of structured perceptron when inexact search is applied with violation-fixing update methods such as earlyupdate (Collins and Roark, 2004). Since we use beam-search in this work, we apply early-update. In addition, we use averaged parameters to reduce overfitting as in (Collins, 2002). Figure 4 shows the pseudocode for structured perceptron training with early-update. Here BEAMSEARCH is identical to the decoding algorithm described in Figure 2 except that i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="30674" citStr="Culotta and Sorensen, 2004" startWordPosition="5041" endWordPosition="5044">all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measu</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proc. ACL, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hany Hassan</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaoqiang Luo</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="29973" citStr="Florian et al., 2004" startWordPosition="4926" endWordPosition="4929">relation with “planes” as owners. In addition to contextual features, deeper semantic knowledge is required to capture such implicit semantic relations. 5.4 Comparison with State-of-the-art Table 3 compares the performance on ACE’04 corpus. For entity mention extraction, our joint model achieved 79.7% on 5-fold cross-validation, which is comparable with the best F1 score 79.2% reported by (Florian et al., 2006) on singlefold. However, Florian et al. (2006) used some gazetteers and the output of other Information Extraction (IE) models as additional features, which provided significant gains ((Florian et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proc. HLT-NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Imed Zitouni</author>
</authors>
<title>Factorizing complex models: A case study in mention detection.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7418" citStr="Florian et al., 2006" startWordPosition="1114" endWordPosition="1117">relation extraction assumed that entity mentions were given In this work we aim to address the problem of end-to-end entity mention and relation extraction from raw texts. 2.2 Baseline System In order to develop a baseline system representing state-of-the-art pipelined approaches, we trained a linear-chain Conditional Random Fields model (Lafferty et al., 2001) for entity mention extraction and a Maximum Entropy model for relation extraction. Entity Mention Extraction Model We re-cast the problem of entity mention extraction as a sequential token tagging task as in the state-of-theart system (Florian et al., 2006). We applied the BILOU scheme, where each tag means a token is the Beginning, Inside, Last, Outside, and Unit of an entity mention, respectively. Most of our features are similar to the work of (Florian et al., 3Throughout this paper we refer to relation mention as relation since we do not consider relation mention coreference. 403 2004; Florian et al., 2006) except that we do not have their gazetteers and outputs from other mention detection systems as features. Our additional features are as follows: • Governor word of the current token based on dependency parsing (Marneffe et al., 2006). • </context>
<context position="10106" citStr="Florian et al., 2006" startWordPosition="1573" endWordPosition="1576">polynomial-time algorithm to find the best structure. In practice we apply beam-search to expand partial configurations for the input sentence incrementally to find the structure with the highest score. 3.1.1 Joint Decoding Algorithm One main challenge to search for entity mentions and relations incrementally is the alignment of different assignments. Assignments for the same sentence can have different numbers of entity mentions and relation arcs. The entity mention extraction task is often re-cast as a token-level sequential labeling problem with BIO or BILOU scheme (Ratinov and Roth, 2009; Florian et al., 2006). A naive solution to our task is to adopt this strategy by treating each token as a state. However, different assignments for the same sentence can have various mention boundaries. It is unfair to compare the model scores of a partial mention and a complete mention. It is also difficult to synchronize the search process of relations. For example, consider the two hypotheses ending at “York” for the same sentence: PHYS AllanU-PER from1 NewB-GPE YorkL-GPE Stock Exchange AllanU-PER from1 NewB-ORG YorkI-ORG Stock Exchange PHYS The model would bias towards the incorrect assignment “New/B-GPE York/</context>
<context position="29766" citStr="Florian et al., 2006" startWordPosition="4894" endWordPosition="4897">.S. troops are now knocking on the door of Baghdad” indicates “troops” has a PHYS relation with “Baghdad”. • “Russia and France sent planes to Baghdad” indicates “Russia” and “France” are involved in an ART relation with “planes” as owners. In addition to contextual features, deeper semantic knowledge is required to capture such implicit semantic relations. 5.4 Comparison with State-of-the-art Table 3 compares the performance on ACE’04 corpus. For entity mention extraction, our joint model achieved 79.7% on 5-fold cross-validation, which is comparable with the best F1 score 79.2% reported by (Florian et al., 2006) on singlefold. However, Florian et al. (2006) used some gazetteers and the output of other Information Extraction (IE) models as additional features, which provided significant gains ((Florian et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity menti</context>
</contexts>
<marker>Florian, Jing, Kambhatla, Zitouni, 2006</marker>
<rawString>Radu Florian, Hongyan Jing, Nanda Kambhatla, and Imed Zitouni. 2006. Factorizing complex models: A case study in mention detection. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>John F Pitrelli</author>
<author>Salim Roukos</author>
<author>Imed Zitouni</author>
</authors>
<title>Improving mention detection robustness to noisy input.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>335--345</pages>
<contexts>
<context position="30452" citStr="Florian et al., 2010" startWordPosition="5004" endWordPosition="5007">s and the output of other Information Extraction (IE) models as additional features, which provided significant gains ((Florian et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2</context>
</contexts>
<marker>Florian, Pitrelli, Roukos, Zitouni, 2010</marker>
<rawString>Radu Florian, John F. Pitrelli, Salim Roukos, and Imed Zitouni. 2010. Improving mention detection robustness to noisy input. In Proc. EMNLP, pages 335– 345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="3309" citStr="Huang and Sagae, 2010" startWordPosition="485" endWordPosition="488"> the entity mentions “Somalia/GPE”, “Haiti/GPE” and “Kosovo/GPE”, are of the same type (Physical (PHYS), in this case). Following the above intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event triggers and arguments. By contrast, we aim to address a more challenging task: identifying mention boundaries and types together with relations, which raises the issue that assignments for the same sentence with different mention boundaries are difficult to syn402 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 402–412, Baltimore, Maryland, US</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In ACL, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="15638" citStr="Huang et al. (2012)" startWordPosition="2529" endWordPosition="2532"> only a small part of the search space is presented. The algorithm then links the newly identified mentions to the previous ones in the same configuration. In this example, the only previous mention is “(tire maker)/ORG”. Finally, “1,400/PER” will be preferred by the model since there are more indicative context features for EMP-ORG relation between “(tire maker)/PER” and “1,400/PER”. 405 3.2 Structured-Perceptron Learning To estimate the feature weights, we use structured perceptron (Collins, 2002), an extension of the standard perceptron for structured prediction, as the learning framework. Huang et al. (2012) proved the convergency of structured perceptron when inexact search is applied with violation-fixing update methods such as earlyupdate (Collins and Roark, 2004). Since we use beam-search in this work, we apply early-update. In addition, we use averaged parameters to reduce overfitting as in (Collins, 2002). Figure 4 shows the pseudocode for structured perceptron training with early-update. Here BEAMSEARCH is identical to the decoding algorithm described in Figure 2 except that if y&apos;, the prefix of the gold standard y, falls out of the beam after each execution of the k-BEST function (line 7 </context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proc. HLT-NAACL, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Improving name tagging by reference resolution and relation detection.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="31659" citStr="Ji and Grishman, 2005" startWordPosition="5211" endWordPosition="5214">Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually studied separately. Most relation extraction work assumed that entity mention boundaries and/or types were given. Chan and Roth (2011) reported the best results using predicted entity mentions. Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, </context>
</contexts>
<marker>Ji, Grishman, 2005</marker>
<rawString>Heng Ji and Ralph Grishman. 2005. Improving name tagging by reference resolution and relation detection. In Proc. ACL, pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction. In</title>
<date>2007</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="30595" citStr="Jiang and Zhai, 2007" startWordPosition="5029" endWordPosition="5032"> Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 cor</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>178--181</pages>
<contexts>
<context position="8446" citStr="Kambhatla, 2004" startWordPosition="1290" endWordPosition="1291">tputs from other mention detection systems as features. Our additional features are as follows: • Governor word of the current token based on dependency parsing (Marneffe et al., 2006). • Prefix of each word in Brown clusters learned from TDT5 corpus (Sun et al., 2011). Relation Extraction Model Given a sentence with entity mention annotations, the goal of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or L (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment it by head words of each phrase. • Four syntactico - semantic patterns described in (Chan and Roth, 2010). • We replicated each lexical feature by replacing each word with its Brown cluster. 3 Algorithm 3.1 The Model Our goal is to predict the hidden structure of each sentence based on arbitrary features and constraints. Let x E X be an input sentence, y&apos; E Y be a candidate structure, and f(x, y&apos;) be </context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In Proc. ACL, pages 178–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond Mooney</author>
</authors>
<title>Joint entity and relation extraction using card-pyramid parsing.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>203--212</pages>
<contexts>
<context position="31807" citStr="Kate and Mooney, 2010" startWordPosition="5235" endWordPosition="5238">ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually studied separately. Most relation extraction work assumed that entity mention boundaries and/or types were given. Chan and Roth (2011) reported the best results using predicted entity mentions. Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint distribution over numerous variables, therefore it </context>
</contexts>
<marker>Kate, Mooney, 2010</marker>
<rawString>Rohit J. Kate and Raymond Mooney. 2010. Joint entity and relation extraction using card-pyramid parsing. In Proc. ACL, pages 203–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7160" citStr="Lafferty et al., 2001" startWordPosition="1071" endWordPosition="1074">ughout this paper, we use ⊥ to denote nonentity or non-relation classes. We consider relation asymmetric. The same relation type with opposite directions is considered to be two classes, which we refer to as directed relation types. Most previous research on relation extraction assumed that entity mentions were given In this work we aim to address the problem of end-to-end entity mention and relation extraction from raw texts. 2.2 Baseline System In order to develop a baseline system representing state-of-the-art pipelined approaches, we trained a linear-chain Conditional Random Fields model (Lafferty et al., 2001) for entity mention extraction and a Maximum Entropy model for relation extraction. Entity Mention Extraction Model We re-cast the problem of entity mention extraction as a sequential token tagging task as in the state-of-theart system (Florian et al., 2006). We applied the BILOU scheme, where each tag means a token is the Beginning, Inside, Last, Outside, and Unit of an entity mention, respectively. Most of our features are similar to the work of (Florian et al., 3Throughout this paper we refer to relation mention as relation since we do not consider relation mention coreference. 403 2004; Fl</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="3443" citStr="Li et al., 2013" startWordPosition="508" endWordPosition="511">intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event triggers and arguments. By contrast, we aim to address a more challenging task: identifying mention boundaries and types together with relations, which raises the issue that assignments for the same sentence with different mention boundaries are difficult to syn402 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 402–412, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics PHYS EMP-ORG The tire maker \ Y J ORG still employs 1,400 \YJ PER</context>
<context position="32560" citStr="Li et al., 2013" startWordPosition="5350" endWordPosition="5353">ately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint distribution over numerous variables, therefore it is more flexible and computationally simpler. In addition, (Singh et al., 2013) used goldstandard mention boundaries. Our previous work (Li et al., 2013) used structured perceptron with token-based decoder to jointly predict event triggers and arguments based on the assumption that entity mentions and other argument candidates are given as part of the input. In this paper, we solve a more challenging problem: take raw texts as input and identify the boundaries, types of entity mentions and relations all together in a single model. Sarawagi and Cohen (2004) proposed a segment-based CRFs model for name tagging. Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. We extended the similar idea to our end-to-en</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proc. ACL, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>449--454</pages>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. LREC, pages 449,454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Open-domain anatomical entity mention detection.</title>
<date>2012</date>
<booktitle>In Proc. ACL Workshop on Detecting Structure in Scholarly Discourse,</booktitle>
<pages>27--36</pages>
<contexts>
<context position="30499" citStr="Ohta et al., 2012" startWordPosition="5013" endWordPosition="5016">(IE) models as additional features, which provided significant gains ((Florian et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Gl</context>
</contexts>
<marker>Ohta, Pyysalo, Tsujii, Ananiadou, 2012</marker>
<rawString>Tomoko Ohta, Sampo Pyysalo, Jun’ichi Tsujii, and Sophia Ananiadou. 2012. Open-domain anatomical entity mention detection. In Proc. ACL Workshop on Detecting Structure in Scholarly Discourse, pages 27–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1498--1507</pages>
<contexts>
<context position="30782" citStr="Plank and Moschitti, 2013" startWordPosition="5061" endWordPosition="5064">o-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually studied separately. Most relation extraction work assumed that entit</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In Proc. ACL, pages 1498–1507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
</authors>
<title>Clusteringbased stratified seed sampling for semi-supervised relation classification.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>346--355</pages>
<contexts>
<context position="30714" citStr="Qian and Zhou, 2010" startWordPosition="5049" endWordPosition="5052">irectly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually </context>
</contexts>
<marker>Qian, Zhou, 2010</marker>
<rawString>Longhua Qian and Guodong Zhou. 2010. Clusteringbased stratified seed sampling for semi-supervised relation classification. In Proc. EMNLP, pages 346– 355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction.</title>
<date>2008</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>697--704</pages>
<contexts>
<context position="30733" citStr="Qian et al., 2008" startWordPosition="5053" endWordPosition="5056">oint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually studied separately.</context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. In Proc. COLING, pages 697–704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc. CONLL,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="10083" citStr="Ratinov and Roth, 2009" startWordPosition="1569" endWordPosition="1572"> There does not exist a polynomial-time algorithm to find the best structure. In practice we apply beam-search to expand partial configurations for the input sentence incrementally to find the structure with the highest score. 3.1.1 Joint Decoding Algorithm One main challenge to search for entity mentions and relations incrementally is the alignment of different assignments. Assignments for the same sentence can have different numbers of entity mentions and relation arcs. The entity mention extraction task is often re-cast as a token-level sequential labeling problem with BIO or BILOU scheme (Ratinov and Roth, 2009; Florian et al., 2006). A naive solution to our task is to adopt this strategy by treating each token as a state. However, different assignments for the same sentence can have various mention boundaries. It is unfair to compare the model scores of a partial mention and a complete mention. It is also difficult to synchronize the search process of relations. For example, consider the two hypotheses ending at “York” for the same sentence: PHYS AllanU-PER from1 NewB-GPE YorkL-GPE Stock Exchange AllanU-PER from1 NewB-ORG YorkI-ORG Stock Exchange PHYS The model would bias towards the incorrect assi</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proc. CONLL, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Reichartz</author>
<author>Hannes Korte</author>
<author>Gerhard Paass</author>
</authors>
<title>Composite kernels for relation extraction.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP (Short Papers),</booktitle>
<pages>365--368</pages>
<contexts>
<context position="30555" citStr="Reichartz et al., 2009" startWordPosition="5021" endWordPosition="5024">ignificant gains ((Florian et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table </context>
</contexts>
<marker>Reichartz, Korte, Paass, 2009</marker>
<rawString>Frank Reichartz, Hannes Korte, and Gerhard Paass. 2009. Composite kernels for relation extraction. In Proc. ACL-IJCNLP (Short Papers), pages 365–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proc. CoNLL.</booktitle>
<contexts>
<context position="4634" citStr="Roth and Yih, 2004" startWordPosition="700" endWordPosition="703">still employs 1,400 \YJ PER conj and EMP-ORG PER US |{z} � GPE in Somalia |{z } GPE � Haiti |{z} GPE and Kosovo |{z } GPE |forces {z PER conj and GPE GPE (a) Interactions between Two Tasks (b) Example of Global Feature Figure 1: End-to-End Entity Mention and Relation Extraction. chronize during search. To tackle this problem, we adopt a segment-based decoding algorithm derived from (Sarawagi and Cohen, 2004; Zhang and Clark, 2008) based on the idea of semi-Markov chain (a.k.a, multiple-beam search algorithm). Most previous attempts on joint inference of entity mentions and relations (such as (Roth and Yih, 2004; Roth and Yih, 2007)) assumed that entity mention boundaries were given, and the classifiers of mentions and relations are separately learned. As a key difference, we incrementally extract entity mentions together with relations using a single model. The main contributions of this paper are as follows: 1. This is the first work to incrementally predict entity mentions and relations using a single joint model (Section 3). 2. Predicting mention boundaries in the joint framework raises the challenge of synchronizing different assignments in the same beam. We solve this problem by detecting entit</context>
<context position="31713" citStr="Roth and Yih, 2004" startWordPosition="5220" endWordPosition="5223">/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually studied separately. Most relation extraction work assumed that entity mention boundaries and/or types were given. Chan and Roth (2011) reported the best results using predicted entity mentions. Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joi</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Global inference for entity and relation identification via a lin- ear programming formulation. In Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<publisher>MIT.</publisher>
<contexts>
<context position="4655" citStr="Roth and Yih, 2007" startWordPosition="704" endWordPosition="707">\YJ PER conj and EMP-ORG PER US |{z} � GPE in Somalia |{z } GPE � Haiti |{z} GPE and Kosovo |{z } GPE |forces {z PER conj and GPE GPE (a) Interactions between Two Tasks (b) Example of Global Feature Figure 1: End-to-End Entity Mention and Relation Extraction. chronize during search. To tackle this problem, we adopt a segment-based decoding algorithm derived from (Sarawagi and Cohen, 2004; Zhang and Clark, 2008) based on the idea of semi-Markov chain (a.k.a, multiple-beam search algorithm). Most previous attempts on joint inference of entity mentions and relations (such as (Roth and Yih, 2004; Roth and Yih, 2007)) assumed that entity mention boundaries were given, and the classifiers of mentions and relations are separately learned. As a key difference, we incrementally extract entity mentions together with relations using a single model. The main contributions of this paper are as follows: 1. This is the first work to incrementally predict entity mentions and relations using a single joint model (Section 3). 2. Predicting mention boundaries in the joint framework raises the challenge of synchronizing different assignments in the same beam. We solve this problem by detecting entity mentions on segment</context>
<context position="17095" citStr="Roth and Yih, 2007" startWordPosition="2790" endWordPosition="2793">t trigger any early-update since the gold standard does not contain any segment ending at the second token. Input: training set D = {(x(j), y(j))}Ni=1, maximum iteration number T Output: model parameters w 1 initialize w +— 0 2 for t +— 1...T do 3 foreach (x, y) E D do 4 (x, y&apos;, z) +— BEAMSEARCH (x, y, w) 5 if z =� y then 6 w +— w + f(x, y&apos;) — f(x, z) 7 return w Figure 4: Perceptron algorithm with beamsearch and early-update. y&apos; is the prefix of the gold-standard and z is the top assignment. 3.3 Entity Type Constraints Entity type constraints have been shown effective in predicting relations (Roth and Yih, 2007; Chan and Roth, 2010). We automatically collect a mapping table of permissible entity types for each relation type from our training data. Instead of applying the constraints in post-processing inference, we prune the branches that violate the type constraints during search. This type of pruning can reduce search space as well as make the input for parameter update less noisy. In our experiments, only 7 relation mentions (0.5%) in the dev set and 5 relation mentions (0.3%) in the test set violate the constraints collected from the training data. 4 Features An advantage of our framework is tha</context>
<context position="31733" citStr="Roth and Yih, 2007" startWordPosition="5224" endWordPosition="5227">9.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually studied separately. Most relation extraction work assumed that entity mention boundaries and/or types were given. Chan and Roth (2011) reported the best results using predicted entity mentions. Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm </context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>Dan Roth and Wen-tau Yih. 2007. Global inference for entity and relation identification via a lin- ear programming formulation. In Introduction to Statistical Relational Learning. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="4426" citStr="Sarawagi and Cohen, 2004" startWordPosition="667" endWordPosition="670">2nd Annual Meeting of the Association for Computational Linguistics, pages 402–412, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics PHYS EMP-ORG The tire maker \ Y J ORG still employs 1,400 \YJ PER conj and EMP-ORG PER US |{z} � GPE in Somalia |{z } GPE � Haiti |{z} GPE and Kosovo |{z } GPE |forces {z PER conj and GPE GPE (a) Interactions between Two Tasks (b) Example of Global Feature Figure 1: End-to-End Entity Mention and Relation Extraction. chronize during search. To tackle this problem, we adopt a segment-based decoding algorithm derived from (Sarawagi and Cohen, 2004; Zhang and Clark, 2008) based on the idea of semi-Markov chain (a.k.a, multiple-beam search algorithm). Most previous attempts on joint inference of entity mentions and relations (such as (Roth and Yih, 2004; Roth and Yih, 2007)) assumed that entity mention boundaries were given, and the classifiers of mentions and relations are separately learned. As a key difference, we incrementally extract entity mentions together with relations using a single model. The main contributions of this paper are as follows: 1. This is the first work to incrementally predict entity mentions and relations using </context>
<context position="11094" citStr="Sarawagi and Cohen, 2004" startWordPosition="1733" endWordPosition="1736">two hypotheses ending at “York” for the same sentence: PHYS AllanU-PER from1 NewB-GPE YorkL-GPE Stock Exchange AllanU-PER from1 NewB-ORG YorkI-ORG Stock Exchange PHYS The model would bias towards the incorrect assignment “New/B-GPE York/L-GPE” since it can have more informative features as a complete mention (e.g., a binary feature indicating if the entire mention appears in a GPE gazetter). Furthermore, the predictions of the two PHYS relations cannot be synchronized since “New/B-FAC York/I-FAC” is not yet a complete mention. To tackle these problems, we employ the idea of semi-Markov chain (Sarawagi and Cohen, 2004), in which each state corresponds to a segment of the input sequence. They presented a variant of Viterbi algorithm for exact inference in semi-Markov chain. We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). Let dˆ be the upper bound of entity mention length. The k-best partial assignments ending at the i-th token can be calculated as: B[i] = k-BEST f(x, y&apos;) · w y&apos;E{y[1..i]|y[1:i−d]EB[i−d], d=1... ˆd} where y[1:i−d] stands for a partial configuration ending at the (i-d)-th token, and y[i−d+1,i] cor</context>
<context position="32969" citStr="Sarawagi and Cohen (2004)" startWordPosition="5419" endWordPosition="5422">eling joint distribution over numerous variables, therefore it is more flexible and computationally simpler. In addition, (Singh et al., 2013) used goldstandard mention boundaries. Our previous work (Li et al., 2013) used structured perceptron with token-based decoder to jointly predict event triggers and arguments based on the assumption that entity mentions and other argument candidates are given as part of the input. In this paper, we solve a more challenging problem: take raw texts as input and identify the boundaries, types of entity mentions and relations all together in a single model. Sarawagi and Cohen (2004) proposed a segment-based CRFs model for name tagging. Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. We extended the similar idea to our end-to-end task by incrementally predicting relations along with entity mention segments. 7 Conclusions and Future Work In this paper we introduced a new architecture for more powerful end-to-end entity mention and relation extraction. For the first time, we addressed this challenging task by an incremental beam-search algorithm in conjunction with structured perceptron. While detecting mention boundaries jointly w</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Sebastian Riedel</author>
<author>Brian Martin</author>
<author>Jiaping Zheng</author>
<author>Andrew McCallum</author>
</authors>
<title>Joint inference of entities, relations, and coreference.</title>
<date>2013</date>
<booktitle>In Proc. CIKM Workshop on Automated Knowledge Base Construction.</booktitle>
<contexts>
<context position="32245" citStr="Singh et al., 2013" startWordPosition="5301" endWordPosition="5304">-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint distribution over numerous variables, therefore it is more flexible and computationally simpler. In addition, (Singh et al., 2013) used goldstandard mention boundaries. Our previous work (Li et al., 2013) used structured perceptron with token-based decoder to jointly predict event triggers and arguments based on the assumption that entity mentions and other argument candidates are given as part of the input. In this paper, we solve a more challenging problem: take raw texts as input a</context>
</contexts>
<marker>Singh, Riedel, Martin, Zheng, McCallum, 2013</marker>
<rawString>Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint inference of entities, relations, and coreference. In Proc. CIKM Workshop on Automated Knowledge Base Construction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Satoshi Sekine</author>
</authors>
<title>Semi-supervised relation extraction with large-scale word clustering.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>521--529</pages>
<contexts>
<context position="8099" citStr="Sun et al., 2011" startWordPosition="1233" endWordPosition="1236">he Beginning, Inside, Last, Outside, and Unit of an entity mention, respectively. Most of our features are similar to the work of (Florian et al., 3Throughout this paper we refer to relation mention as relation since we do not consider relation mention coreference. 403 2004; Florian et al., 2006) except that we do not have their gazetteers and outputs from other mention detection systems as features. Our additional features are as follows: • Governor word of the current token based on dependency parsing (Marneffe et al., 2006). • Prefix of each word in Brown clusters learned from TDT5 corpus (Sun et al., 2011). Relation Extraction Model Given a sentence with entity mention annotations, the goal of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or L (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment it by head words of each phrase. • Four syntactico - sema</context>
<context position="30573" citStr="Sun et al., 2011" startWordPosition="5025" endWordPosition="5028">an et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-va</context>
</contexts>
<marker>Sun, Grishman, Sekine, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011. Semi-supervised relation extraction with large-scale word clustering. In Proc. ACL, pages 521–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1640--1649</pages>
<contexts>
<context position="31757" citStr="Yang and Cardie, 2013" startWordPosition="5228" endWordPosition="5231">0.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) usually studied separately. Most relation extraction work assumed that entity mention boundaries and/or types were given. Chan and Roth (2011) reported the best results using predicted entity mentions. Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint d</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proc. ACL, pages 1640–1649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yu</author>
<author>Wai Lam</author>
</authors>
<title>Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach.</title>
<date>2010</date>
<booktitle>In Proc. COLING (Posters),</booktitle>
<pages>1399--1407</pages>
<contexts>
<context position="32264" citStr="Yu and Lam, 2010" startWordPosition="5305" endWordPosition="5308">shman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint distribution over numerous variables, therefore it is more flexible and computationally simpler. In addition, (Singh et al., 2013) used goldstandard mention boundaries. Our previous work (Li et al., 2013) used structured perceptron with token-based decoder to jointly predict event triggers and arguments based on the assumption that entity mentions and other argument candidates are given as part of the input. In this paper, we solve a more challenging problem: take raw texts as input and identify the bou</context>
</contexts>
<marker>Yu, Lam, 2010</marker>
<rawString>Xiaofeng Yu and Wai Lam. 2010. Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach. In Proc. COLING (Posters), pages 1399–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and pos tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1147--1157</pages>
<contexts>
<context position="4450" citStr="Zhang and Clark, 2008" startWordPosition="671" endWordPosition="674">Association for Computational Linguistics, pages 402–412, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics PHYS EMP-ORG The tire maker \ Y J ORG still employs 1,400 \YJ PER conj and EMP-ORG PER US |{z} � GPE in Somalia |{z } GPE � Haiti |{z} GPE and Kosovo |{z } GPE |forces {z PER conj and GPE GPE (a) Interactions between Two Tasks (b) Example of Global Feature Figure 1: End-to-End Entity Mention and Relation Extraction. chronize during search. To tackle this problem, we adopt a segment-based decoding algorithm derived from (Sarawagi and Cohen, 2004; Zhang and Clark, 2008) based on the idea of semi-Markov chain (a.k.a, multiple-beam search algorithm). Most previous attempts on joint inference of entity mentions and relations (such as (Roth and Yih, 2004; Roth and Yih, 2007)) assumed that entity mention boundaries were given, and the classifiers of mentions and relations are separately learned. As a key difference, we incrementally extract entity mentions together with relations using a single model. The main contributions of this paper are as follows: 1. This is the first work to incrementally predict entity mentions and relations using a single joint model (Se</context>
<context position="11397" citStr="Zhang and Clark, 2008" startWordPosition="1782" endWordPosition="1785">ion (e.g., a binary feature indicating if the entire mention appears in a GPE gazetter). Furthermore, the predictions of the two PHYS relations cannot be synchronized since “New/B-FAC York/I-FAC” is not yet a complete mention. To tackle these problems, we employ the idea of semi-Markov chain (Sarawagi and Cohen, 2004), in which each state corresponds to a segment of the input sequence. They presented a variant of Viterbi algorithm for exact inference in semi-Markov chain. We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). Let dˆ be the upper bound of entity mention length. The k-best partial assignments ending at the i-th token can be calculated as: B[i] = k-BEST f(x, y&apos;) · w y&apos;E{y[1..i]|y[1:i−d]EB[i−d], d=1... ˆd} where y[1:i−d] stands for a partial configuration ending at the (i-d)-th token, and y[i−d+1,i] corresponds to the structure of a new segment (i.e., subsequence of x) x[i−d+1,i]. Our joint decoding algorithm is shown in Figure 2. For each token index i, it maintains a beam for the partial assignments whose last segments end at the i-th token. There are two types of actions during the search: 404 Inp</context>
<context position="33046" citStr="Zhang and Clark (2008)" startWordPosition="5431" endWordPosition="5434">and computationally simpler. In addition, (Singh et al., 2013) used goldstandard mention boundaries. Our previous work (Li et al., 2013) used structured perceptron with token-based decoder to jointly predict event triggers and arguments based on the assumption that entity mentions and other argument candidates are given as part of the input. In this paper, we solve a more challenging problem: take raw texts as input and identify the boundaries, types of entity mentions and relations all together in a single model. Sarawagi and Cohen (2004) proposed a segment-based CRFs model for name tagging. Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. We extended the similar idea to our end-to-end task by incrementally predicting relations along with entity mention segments. 7 Conclusions and Future Work In this paper we introduced a new architecture for more powerful end-to-end entity mention and relation extraction. For the first time, we addressed this challenging task by an incremental beam-search algorithm in conjunction with structured perceptron. While detecting mention boundaries jointly with other components raises the challenge of synchronizing multiple assignmen</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and pos tagging using a single perceptron. In Proc. ACL, pages 1147–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>419--426</pages>
<contexts>
<context position="30646" citStr="Zhao and Grishman, 2005" startWordPosition="5037" endWordPosition="5040">d external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical sign</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proc. ACL, pages 419–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="8424" citStr="Zhou et al., 2005" startWordPosition="1285" endWordPosition="1288"> their gazetteers and outputs from other mention detection systems as features. Our additional features are as follows: • Governor word of the current token based on dependency parsing (Marneffe et al., 2006). • Prefix of each word in Brown clusters learned from TDT5 corpus (Sun et al., 2011). Relation Extraction Model Given a sentence with entity mention annotations, the goal of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or L (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment it by head words of each phrase. • Four syntactico - semantic patterns described in (Chan and Roth, 2010). • We replicated each lexical feature by replacing each word with its Brown cluster. 3 Algorithm 3.1 The Model Our goal is to predict the hidden structure of each sentence based on arbitrary features and constraints. Let x E X be an input sentence, y&apos; E Y be a candidate struc</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proc. ACL, pages 427–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Min Zhang</author>
<author>Dong-Hong Ji</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL,</booktitle>
<pages>728--736</pages>
<contexts>
<context position="30693" citStr="Zhou et al., 2007" startWordPosition="5045" endWordPosition="5048">it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1 Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-tes</context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>Guodong Zhou, Min Zhang, Dong-Hong Ji, and Qiaoming Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proc. EMNLP-CoNLL, pages 728–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Radu Florian</author>
</authors>
<title>Mention detection crossing the language barrier.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>600--609</pages>
<contexts>
<context position="30479" citStr="Zitouni and Florian, 2008" startWordPosition="5008" endWordPosition="5012">her Information Extraction (IE) models as additional features, which provided significant gains ((Florian et al., 2004)). Since these gazetteers, additional data sets and external IE models are all not publicly available, it is not fair to directly compare our joint model with their results. For end-to-end entity mention and relation extraction, both the joint approach and the pipelined baseline outperform the best results reported by (Chan and Roth, 2011) under the same setting. 6 Related Work Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012)) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013)) have drawn much attention in recent years but were 409 Model Entity Mention (%) Relation (%) Entity Mention + Relation (%) Score P R Fl P R Fl P R Fl Chan and Roth (2011) - 42.9 38.9 40.8 - Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9 Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 3</context>
</contexts>
<marker>Zitouni, Florian, 2008</marker>
<rawString>Imed Zitouni and Radu Florian. 2008. Mention detection crossing the language barrier. In Proc. EMNLP, pages 600–609.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>