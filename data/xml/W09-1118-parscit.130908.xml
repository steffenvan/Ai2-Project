<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000996">
<title confidence="0.985998">
An Intrinsic Stopping Criterion for Committee-Based Active Learning
</title>
<author confidence="0.657849">
Fredrik Olsson
</author>
<affiliation confidence="0.268133">
SICS
</affiliation>
<address confidence="0.462092">
Box 1263
SE-164 29 Kista, Sweden
</address>
<email confidence="0.986587">
fredrik.olsson@sics.se
</email>
<author confidence="0.991626">
Katrin Tomanek
</author>
<affiliation confidence="0.8040385">
Jena University Language &amp; Information Engineering Lab
Friedrich-Schiller-Universit¨at Jena
</affiliation>
<address confidence="0.588544">
F¨urstengraben 30, D-07743 Jena, Germany
</address>
<email confidence="0.614189">
katrin.tomanek@uni-jena.de
</email>
<sectionHeader confidence="0.985738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880545454545">
As supervised machine learning methods are
increasingly used in language technology, the
need for high-quality annotated language data
becomes imminent. Active learning (AL) is
a means to alleviate the burden of annotation.
This paper addresses the problem of knowing
when to stop the AL process without having
the human annotator make an explicit deci-
sion on the matter. We propose and evaluate
an intrinsic criterion for committee-based AL
of named entity recognizers.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999898929824561">
With the increasing popularity of supervised ma-
chine learning methods in language processing, the
need for high-quality labeled text becomes immi-
nent. On the one hand, the amount of readily avail-
able texts is huge, while on the other hand the la-
beling and creation of corpora based on such texts is
tedious, error prone and expensive.
Active learning (AL) is one way of approaching
the challenge of classifier creation and data annota-
tion. Examples of AL used in language engineering
include named entity recognition (Shen et al., 2004;
Tomanek et al., 2007), text categorization (Lewis
and Gale, 1994; Hoi et al., 2006), part-of-speech
tagging (Ringger et al., 2007), and parsing (Thomp-
son et al., 1999; Becker and Osborne, 2005).
AL is a supervised machine learning technique in
which the learner is in control of the data used for
learning – the control is used to query an oracle, typ-
ically a human, for the correct label of the unlabeled
training instances for which the classifier learned so
far makes unreliable predictions.
The AL process takes as input a set of labeled in-
stances and a larger set of unlabeled instances, and
produces a classifier and a relatively small set of
newly labeled data. The overall goal is to obtain
as good a classifier as possible, without having to
mark-up and supply the learner with more than nec-
essary data. The learning process aims at keeping
the human annotation effort to a minimum, only ask-
ing for advice where the training utility of the result
of such a query is high.
The approaches taken to AL in this paper are
based on committees of classifiers with access to
pools of data. Figure 1 outlines a prototypical
committee-based AL loop. In this paper we focus
on the question when AL-driven annotation should
be stopped (Item 7 in Figure 1).
Usually, the progress of AL is illustrated by
means of a learning curve which depicts how the
classifier’s performance changes as a result of in-
creasingly more labeled training data being avail-
able. A learning curve might be used to address
the issue of knowing when to stop the learning pro-
cess – once the curve has leveled out, that is, when
additional training data does not contribute (much)
to increase the performance of the classifier, the AL
process may be terminated. While in a random se-
lection scenario, classifier performance can be esti-
mated by cross-validation on the labeled data, AL
requires a held-out annotated reference corpus. In
AL, the performance of the classifier cannot be re-
liably estimated using the data labeled in the pro-
cess since sampling strategies for estimating per-
formance assume independently and identically dis-
tributed examples (Sch¨utze et al., 2006). The whole
point in AL is to obtain a distribution of instances
that is skewed in favor of the base learner used.
</bodyText>
<page confidence="0.967786">
138
</page>
<note confidence="0.98546">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 138–146,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<listItem confidence="0.976527352941176">
1. Initialize the process by applying EnsembleGeneration-
Method using base learner B on labeled training data set
DL to obtain a committee of classifiers C.
2. Have each classifier in C predict a label for every instance
in the unlabeled data set DU, obtain labeled set DU ′.
3. From DU ′, select the most informative n instances to
learn from, obtaining DU ′′.
4. Ask the teacher for classifications of the instances I in
DU ′′.
5. Move I, with supplied classifications, from DU to DL.
6. Re-train using EnsembleGenerationMethod and base
learner B on the newly extended DL to obtain a new com-
mittee, C.
7. Repeat steps 2 through 6 until DU is empty or some stop-
ping criterion is met.
8. Output classifier learned using EnsembleGeneration-
Method and base learner B on DL.
</listItem>
<figureCaption confidence="0.99837">
Figure 1: A prototypical query by committee algorithm.
</figureCaption>
<bodyText confidence="0.999990684210527">
In practice, however, an annotated reference cor-
pus is rarely available and its creation would be in-
consistent with the goal of creating a classifier with
as little human effort as possible. Thus, other ways
of deciding when to stop AL are needed. In this pa-
per, we propose an intrinsic stopping-criterion for
committee-based AL of named entity recognizers.
It is intrinsic in that it relies on the characteristics of
the data and the base learner1 rather than on exter-
nal parameters, i.e., the stopping criterion does not
require any pre-defined thresholds.
The paper is structured as follows. Section 2
sketches interpretations of ideal stopping points and
describes the idea behind our stopping criterion.
Section 3 outlines related work. Section 4 describes
the experiments we have conducted concerning a
named entity recognition scenario, while Section 5
presents the results which are then discussed in Sec-
tion 6. Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.730848" genericHeader="method">
2 A stopping criterion for active learning
</sectionHeader>
<bodyText confidence="0.922402">
What is the ideal stopping point for AL? Obviously,
annotation should be stopped at the latest when the
</bodyText>
<footnote confidence="0.975261666666667">
1The term base learner (configuration) refers to the combi-
nation of base learner, parameter settings, and data representa-
tion.
</footnote>
<bodyText confidence="0.998780208333334">
best classifier for a scenario is yielded. However, de-
pending on the scenario at hand, the “best” classifier
could have different interpretations. In many papers
on AL and stopping criteria, the best (or optimal)
classifier is the one that yields the highest perfor-
mance on a test set. It is assumed that AL-based
annotation should be stopped as soon as this per-
formance is reached. This could be generalized as
stopping criteria based on maximal classifier perfor-
mance. In practice, the trade-off between annota-
tion effort and classifier performance is related to the
achievable performance given the learner configura-
tion and data under scrutiny: For instance, would we
invest many hours of additional annotation effort just
to possibly increase the classifier performance by a
fraction of a percent? In this context, a stopping cri-
terion may be based on classifier performance con-
vergence, and consequently, we can define the best
possible classifier to be one which cannot learn more
from the remaining pool of data.
The intrinsic stopping criterion (ISC) we propose
here focuses on the latter aspect of the ideal stop-
ping point described above – exhaustiveness of the
AL pool. We suggest to stop the annotation process
of the data from a given pool when the base learner
cannot learn (much) more from it. The definition of
our intrinsic stopping criterion for committee-based
AL builds on the notions of Selection Agreement
(Tomanek et al., 2007), and Validation Set Agree-
ment (Tomanek and Hahn, 2008).
The Selection Agreement (SA) is the agreement
among the members of a decision committee re-
garding the classification of the most informative in-
stance selected from the pool of unlabeled data in
each AL round. The intuition underlying the SA is
that the committee will agree more on the hard in-
stances selected from the remaining set of unlabeled
data as the AL process proceeds. When the mem-
bers of the committee are in complete agreement,
AL should be aborted since it no longer contributes
to the overall learning process – in this case, AL is
but a computationally expensive counterpart of ran-
dom sampling. However, as pointed out by Tomanek
et al. (2007), the SA hardly ever signals complete
agreement and can thus not be used as the sole in-
dicator of AL having reached the point at which it
should be aborted.
The Validation Set Agreement (VSA) is the agree-
</bodyText>
<page confidence="0.99821">
139
</page>
<bodyText confidence="0.999982390243903">
ment among the members of the decision commit-
tee concerning the classification of a held-out, unan-
notated data set (the validation set). The validation
set stays the same throughout the entire AL process.
Thus, the VSA is mainly affected by the perfor-
mance of the committee, which in turn, is grounded
in the information contained in the most informative
instances in the pool of unlabeled data. Tomanek
and colleagues argue that the VSA is thus a good
approximation of the (progression of the) learning
curve and can be employed as decision support for
knowing when to stop annotating – from the slope of
the VSA curve one can read whether further annota-
tion will result in increased classifier performance.
We combine the SA and the VSA into a single
stopping criterion by relating the agreement of the
committee on a held-out validation set with that on
the (remaining) pool of unlabeled data. If the SA
is larger than the VSA, it is a signal that the deci-
sion committee is more in agreement concerning the
most informative instances in the (diminishing) un-
labeled pool than it is concerning the validation set.
This, in turn, implies that the committee would learn
more from a random sample2 from the validation set
(or from a data source exhibiting the same distribu-
tion of instances), than it would from the unlabeled
data pool. Based on this argument, a stopping crite-
rion for committee-based AL can be formulated as:
Active learning may be terminated when
the Selection Agreement is larger than, or
equal to, the Validation Set Agreement.
In relation to the stopping criterion based solely
on SA proposed by Tomanek et al. (2007), the above
defined criterion comes into effect earlier in the
AL process. Furthermore, while it was claimed in
(Tomanek and Hahn, 2008) that one can observe the
classifier convergence from the VSA curve (as it ap-
proximated the progression of the learning curve),
that requires a threshold to be specified for the ac-
tual stopping point. The ISC is completely intrinsic
and does thus not require any thresholds to be set.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.997214">
Schohn and Cohn (2000) report on document clas-
sification using AL with Support Vector Machines.
</bodyText>
<footnote confidence="0.6795525">
2The sample has to be large enough to mimic the distribution
of instances in the original unlabeled pool.
</footnote>
<bodyText confidence="0.999986666666667">
If the most informative instance is no closer to the
decision hyperplane than any of the support vectors,
the margin has been exhausted and AL is terminated.
Vlachos (2008) suggests to use classifier confi-
dence to define a stopping criterion for uncertainty-
based sampling. The idea is to stop learning when
the confidence of the classifier, on an external, pos-
sibly unannotated test set, remains at the same level
or drops for a number of consecutive iterations dur-
ing the AL process. Vlachos shows that the criterion
indeed is applicable to the tasks he investigates.
Zhu and colleagues (Zhu and Hovy, 2007;
Zhu et al., 2008a; Zhu et al., 2008b) introduce
max-confidence, min-error, minimum expected er-
ror strategy, overall-uncertainty, and classification-
change as means to terminate AL. They primar-
ily use a single-classifier approach to word sense
disambiguation and text classification in their ex-
periments. Max-confidence seeks to terminate AL
once the classifier is most confident in its predic-
tions. In the min-error strategy, the learning is halted
when there is no difference between the classifier’s
predictions and those labels provided by a human
annotator. The minimum expected error strategy
involves estimating the classification error on fu-
ture unlabeled instances and stop the learning when
the expected error is as low as possible. Overall-
uncertainty is similar to max-confidence, but unlike
the latter, overall-uncertainty takes into account all
data remaining in the unlabeled pool when estimat-
ing the uncertainty of the classifier. Classification-
change builds on the assumption that the most in-
formative instance is the one which causes the clas-
sifier to change the predicted label of the instance.
Classification-change-based stopping is realized by
Zhu and colleagues such that AL is terminated once
no predicted label of the instances in the unlabeled
pool change during two consecutive AL iterations.
Laws and Sch¨utze (2008) investigate three ways
of terminating uncertainty-based AL for named en-
tity recognition – minimal absolute performance,
maximum possible performance, and convergence.
The minimal absolute performance of the system
is set by the user prior to starting the AL process.
The classifier then estimates its own performance
using a held-out unlabeled data set. Once the per-
formance is reached, the learning is terminated. The
maximum possible performance strategy refers to
</bodyText>
<page confidence="0.981324">
140
</page>
<bodyText confidence="0.999990738095238">
the optimal performance of the classifier given the
data. Once the optimal performance is achieved, the
process is aborted. Finally, the convergence crite-
rion aims to stop the learning process when the pool
of available data does not contribute to the classi-
fier’s performance. The convergence is calculated
as the gradient of the classifier’s estimated perfor-
mance or uncertainty. Laws and Sch¨utze conclude
that both gradient-based approaches, that is, conver-
gence, can be used as stopping criteria relative to the
optimal performance achievable on a given pool of
data. They also show that while their method lends
itself to acceptable estimates of accuracy, it is much
harder to estimate the recall of the classifier. Thus,
the stopping criteria based on minimal absolute or
maximum possible performance are not reliable.
The work most related to ours is that of Tomanek
and colleagues (Tomanek et al., 2007; Tomanek and
Hahn, 2008) who define and evaluate the Selection
Agreement (SA) and the Validation Set Agreement
(VSA) already introduced in Section 2. Tomanek
and Hahn (2008) conclude that monitoring the
progress of AL should be based on a separate vali-
dation set instead of the data directly affected by the
learning process – thus, VSA is preferred over SA.
Further, they find that the VSA curve approximates
the progression of the learning curve and thus clas-
sifier performance convergence could be estimated.
However, to actually find where to stop the annota-
tion, a threshold needs to be set.
Our proposed intrinsic stopping criterion is
unique in several ways: The ISC is intrinsic, relying
only on the characteristics of the base learner and
the data at hand in order to decide when the AL pro-
cess may be terminated. The ISC does not require
the user to set any external parameters prior to ini-
tiating the AL process. Further, the ISC is designed
to work with committees of classifiers, and as such,
it is independent of how the disagreement between
the committee members is quantified. The ISC does
neither rely on a particular base learner, nor on a par-
ticular way of creating the decision committee.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999973666666667">
To challenge the definition of the ISC, we conducted
two types of experiments concerning named entity
recognition. The primary focus of the first type
of experiment is on creating classifiers (classifier-
centric), while the second type is concerned with the
creation of annotated documents (data-centric). In
all experiments, the agreement among the decision
committee members is quantified by the Vote En-
tropy measure (Engelson and Dagan, 1996):
</bodyText>
<equation confidence="0.9944916">
1 � V (l, e)
k logV (l, e)
V E(e) = − (1)
log k k
l
</equation>
<bodyText confidence="0.999962666666667">
where k is the number of members in the committee,
and V (l, e) is the number of members assigning la-
bel l to instance e. If an instance obtains a low Vote
Entropy value, it means that the committee members
are in high agreement concerning its classification,
and thus also that it is less a informative one.
</bodyText>
<subsectionHeader confidence="0.99287">
4.1 Classifier-centric experimental settings
</subsectionHeader>
<bodyText confidence="0.999984387096774">
In common AL scenarios, the main goal of us-
ing AL is to create a good classifier with min-
imal label complexity. To follow this idea, we
select sentences that are assumed to be useful
for classifier training. We decided to select
complete sentences – instead of, e.g., single to-
kens – as in practice annotators must see the
context of words to decide on their entity labels.
Our experimental setting is based on the AL ap-
proach described by Tomanek et al. (2007): The
committee consists of k = 3 Maximum Entropy
(ME) classifiers (Berger et al., 1996). In each AL
iteration, each classifier is trained on a randomly
drawn (sampling without replacement) subset L′ C
L with L′ = 23L, L being the set of all instances la-
beled so far (cf. EnsembleGenerationMethod in Fig-
ure 1). Usefulness of a sentence is estimated as the
average token Vote Entropy (cf. Equation 1). In each
AL iteration, the 20 most useful sentences are se-
lected (n = 20 in Step 3 in Figure 1). AL is started
from a randomly chosen seed of 20 sentences.
While we made use of ME classifiers during the
selection, we employed an NE tagger based on Con-
ditional Random Fields (CRF) (Lafferty et al., 2001)
during evaluation time to determine the learning
curves. CRFs have a significantly higher tagging
performance, so the final classifier we are aiming
at should be a CRF model. We have shown be-
fore (Tomanek et al., 2007) that MEs are well apt as
selectors with the advantage of much shorter train-
ing times than CRFs. For both MEs and CRFs the
</bodyText>
<page confidence="0.993368">
141
</page>
<bodyText confidence="0.99997775">
same features were employed which comprised or-
thographical (based mainly on regular expressions),
lexical and morphological (suffixed/prefixed, word
itself), syntactic (POS tags), as well as contextual
(features of neighboring tokens) ones.
The experiments on classifier-centric AL have
been performed on the English data set of cor-
pus used in the CoNLL-2003 shared task (Tjong
Kim Sang and Meulder, 2003). This corpus con-
sists of newspaper articles annotated with respect to
person, location, and organisation entities. As AL
pool we took the training set which consists of about
14,000 sentences (≈ 200, 000 tokens). As valida-
tion set and as gold standard for plotting the learn-
ing curve we used CoNLL’s evaluation corpus which
sums up to 3,453 sentences.
</bodyText>
<subsectionHeader confidence="0.982665">
4.2 Data-centric experimental settings
</subsectionHeader>
<bodyText confidence="0.999966209302326">
While AL is commonly used to create as good
classifiers as possible, with the amount of human
effort kept to a minimum, it may result in frag-
mented and possibly non re-usable annotations (e.g.,
a collection of documents in which only some of
the names are marked up). This experiment con-
cerns a method of orchestrating AL in a way ben-
eficial for the bootstrapping of annotated data (Ols-
son, 2008). The bootstrapping proper is realized by
means of AL for selecting documents to annotate, as
opposed to sentences. This way the annotated data
set is comprised of entire documents thus promot-
ing data creation. As in the classifier-centric setting,
the task is to recognize names – persons, organiza-
tions, locations, times, dates, monetary expressions,
and percentages – in news wire texts. The texts
used are part of the MUC-7 corpus (Linguistic Data
Consortium, 2001) and consists of 100 documents,
3,480 sentences, and 90,790 tokens. The task is ap-
proached using the IOB tagging scheme proposed
by, e.g., Ramshaw and Marcus (1995), turning the
original 7-class task into a 15-class task. Each to-
ken is represented using a fairly standard menagerie
of features, including such stemming from the sur-
face appearance of the token (e.g., Contains dollar?
Length in characters), calculated based on linguis-
tic pre-processing made with the English Functional
Dependency Grammar (Tapanainen and J¨arvinen,
1997) (e.g., Case, Part-of-speech), fetched from pre-
compiled lists of information (e.g., Is first name?),
and features based on predictions concerning the
context of the token (e.g, Class ofprevious token).
The decision committee is made up from 10
boosted decision trees using MultiBoostAB (Webb,
2000) (cf. EnsembleGenerationMethod in Figure 1).
Each classifier is created by the REPTree decision
tree learner described by Witten and Frank (2005).
The informativeness of a document is calculated by
means of average token Vote Entropy (cf. Equa-
tion 1). The seed set of the AL process consists of
five randomly selected documents. In each AL iter-
ation, one document is selected for annotation from
the corpus (n = 1 in Step 3 in Figure 1).
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999992095238095">
Two different scenarios were used to illustrate the
applicability of the proposed intrinsic stopping cri-
terion. In the first scenario, we assumed that the
pool of unlabeled data was static and fairly large.
In the second scenario, we assumed that the unla-
beled data would be collected in smaller batches as
it was made available on a stream, for instance, from
a news feed. Both the classifier-centric and the data-
centric experiments were carried out within the first
scenario. Only the classifier-centric experiment was
conducted in the stream-based scenario.
In the classifier-centric setting, the SA is defined
as (1 − Vote Entropy) for the most informative in-
stances in the unlabeled pool, that is, the per-token
average Vote Entropy on the most informative sen-
tences. Analogously, in the data-centric setting, the
SA is defined as (1 − Vote Entropy) for the most in-
formative document – here too, the informativeness
is calculated as the per-token average Vote Entropy.
In both settings, the VSA is the per-token average
Vote Entropy on the validation set.
</bodyText>
<subsectionHeader confidence="0.980722">
5.1 AL on static pools
</subsectionHeader>
<bodyText confidence="0.999686285714286">
The intersection of the SA and VSA agreement
curves indicates a point at which the AL process
may be terminated without (a significant) loss in
classifier performance. For both AL scenarios (data-
and classifier-centric) we plot both the learning
curves for AL and random selection, as well as the
SA and VSA curve for AL. In both scenarios, these
</bodyText>
<page confidence="0.986285">
142
</page>
<figure confidence="0.997217875">
0 25000 50000 75000 100000 125000 150000
Baseline
Query by committee
Selection agreement
Validation set agreement
C
0 25000 50000 75000 100000 125000 150000
Number of tokens in the training set
</figure>
<figureCaption confidence="0.875819">
Figure 2: Classifier-centric AL experiments on the
CoNLL corpus. The intersection, C, corresponds to the
point where (almost) no further improvement in terms
of classifier performance can be expected. The baseline
learning curve shows the results of learning from ran-
domly sampled data.
</figureCaption>
<bodyText confidence="0.993831190476191">
curves are averages over several runs.3
The results from the classifier-centric experiment
on the CoNLL corpus are presented in Figure 2.
AL clearly outperforms random selection. The AL
curve converges at a maximum performance of F ≈
84% after about 125,000 tokens. As expected, the
SA curve drops from high values in the beginning
down to very low values in the end where hardly
any interesting instances are left in the pool. The
intersection (C) with the VSA curve is very close to
the point (125,000 tokens) where no further increase
of performance can be reached by additional anno-
tation making it a good stopping point.
The results from the data-centric experiment are
available in Figure 3. The bottom part shows the
SA and VSA curves. The ISC occurs at the inter-
section of the SA and VSA curves (C), which corre-
sponds to a point well beyond the steepest part of the
learning curve. While stopping the learning at C re-
sults in a classifier with performance inferior what is
maximally achievable, stopping at C arguably corre-
</bodyText>
<footnote confidence="0.635726">
3The classifier-centric experiments are averages over three
independent runs. The data-centric experiments are averages
over ten independent runs.
0 10 20 30 40 50 60 70 80 90
Number of documents in the training set
</footnote>
<figureCaption confidence="0.9969412">
Figure 3: Data-centric AL experiments on the MUC-7
corpus. The intersection, C, corresponds to a point at
which the AL curve has almost leveled out. The base-
line learning curve shows the results of learning from ran-
domly sampled data.
</figureCaption>
<bodyText confidence="0.990621285714286">
sponds to a plausible place to abort the learning. The
optimal performance is F ≈ 83.5%, while the ISC
corresponds to F ≈ 82%.
Keep in mind that the learning curves with which
the ISC are compared are not available in a practical
situation, they are included in Figures 2 and 3 for the
sake of clarity only.
</bodyText>
<subsectionHeader confidence="0.997379">
5.2 AL on streamed data
</subsectionHeader>
<bodyText confidence="0.999957333333333">
One way of paraphrasing the ISC is: Once the in-
tersection between the SA and VSA curves has been
reached, the most informative instances remaining
in the pool of unlabeled data are less informative to
the classifier than the instances in the held-out, unla-
beled validation set are on average. This means that
the classifier would learn more from a sufficiently
large sample taken from the validation set than it
would if the AL process continued on the remain-
ing unlabeled pool.4
As an illustration of the practical applicability of
the ISC consider the following scenario. Assume
</bodyText>
<footnote confidence="0.956243">
4Note however, that the classifier might still learn from the
instances in the unlabeled pool – applying the ISC only means
that the classifier would learn more from a validation set-like
distribution of instances.
</footnote>
<figure confidence="0.999641800000001">
0 10 20 30 40 50 60 70 80 90
Baseline
Query by boosting
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.14
0.13
0.12
0.1
0.1
0.09
0.08
0.07
0.06
0.05
1
Selection agreement
Validation set agreement
C
Vote Entropy F-score 0.85
0.8
0.75
0.7
0.65
0.6
0.3
0.25
0.2
0.15
0.1
0.05
0
Vote Entropy F-score
</figure>
<page confidence="0.517123">
143
</page>
<figure confidence="0.992274">
0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000
Number of tokens in the training set
</figure>
<figureCaption confidence="0.999784833333333">
Figure 4: AL curves for the four partitions used in the ex-
periments on streamed data. Ci denotes a point at which
the AL is terminated for partition i and a new partition is
employed instead. C1 corresponds to the ISC plotted in
the graph labeled Partition 1 in Figure 5, C2 to the ISC in
Partition 2, and C3 to the ISC in Partition 3.
</figureCaption>
<bodyText confidence="0.999957655172414">
that we are collecting data from a stream, for in-
stance items taken from a news feed. Thus, the data
is not available on the form of a closed set, but rather
an open one which grows over time. To make the
most of the human annotators in this scenario, we
want them to operate on batches of data instead of
annotating individual news items as they are pub-
lished. The purpose of the annotation is to mark up
names in the texts in order to train a named entity
recognizer. To do so, we wait until there has ap-
peared a given number of sentences on the stream,
and then collect those sentences. The problem is,
how do we know when the AL-based annotation
process for each such batch should be terminated?
We clearly do not want the annotators to annotate
all sentences, and we cannot have the annotators
set new thresholds pertaining to the absolute per-
formance of the named entity recognizer for each
new batch of data available. By using the ISC, we
are able to automatically issue a halting of the AL
process (and thus also the annotation process) and
proceed to the next batch of data without losing too
much in performance, and without having the anno-
tators mark up too much of the available data. To
this end, the ISC seems like a reasonable trade-off
between annotation effort and performance gain.
To carry out this experiment we took a sub sample
of 10% (1,400 sentences) from the original AL pool
of the CoNLL corpus as validation set.5 The rest of
</bodyText>
<footnote confidence="0.903347">
5Note that the original CoNLL test set was not used in this
</footnote>
<figureCaption confidence="0.9680945">
Figure 5: The SA and VSA curves for the four data par-
titions used in the experiment on streamed data. Each
intersection – ISC – corresponds to a point where AL is
terminated.
</figureCaption>
<bodyText confidence="0.99225776">
this pool was split into batches of about 500 con-
secutive sentences. Classifier-centric AL was now
run taking the first batch as pool to select from. At
the point where the SA and VSA curve crossed, we
continued AL selection from the next batch and so
forth. Figure 4 shows the learning curve for a simu-
lation of the scenario described above. The inter-
section between the SA and VSA curves for par-
tition 1 as depicted in Figure 5 corresponds to the
first “step” (ending in C1) in the stair-like learning
curve in Figure 4. The step occurs after 4,641 to-
kens. Analogously, the other steps (ending in C2 and
C3, respectively) in the learning curve corresponds
the intersection between the SA and VSA curves for
partitions 2 and 3 in Figure 5. The intersection for
partition 4 corresponds to the point were we would
have turned to the next partition. This experiment
was stopped after 4 partitions.
Table 1 shows the accumulated number of sen-
tences and tokens (center columns) that required an-
notation in order to reach the ISC for each partition.
In addition, the last column in the table shows the
number of sentences (of the 500 collected for inclu-
experiment, thus the F-score reported in Figure 4 cannot be
compared to that in Figure 2.
</bodyText>
<figure confidence="0.997235575">
C1
C2
C3
Partition 1
Partition 2
Partition 3
Partition 4
Partition 1
SA
VSA
1000 4000 7000
Tokens
Partition 3
SA
VSA
8000 14000
Tokens
Partition 2
SA
VSA
5000 8000
Tokens
Partition 4
14000 18000
Tokens
Vote Entropy 0.0 0.2
Vote Entropy 0.00 0.10 0.20
SA
VSA
0.00 0.10 0.20
Vote Entropy
0.00 0.10 0.20
Vote Entropy
F-score 0.8
0.75
0.7
0.65
0.6
0.55
0.5
</figure>
<page confidence="0.993104">
144
</page>
<tableCaption confidence="0.865174">
Table 1: The number of tokens and sentences required to
reach the ISC for each partition.
</tableCaption>
<bodyText confidence="0.999908">
sion in each partition) needed to reach the ISC – each
new partition contributes less to the increase in per-
formance than the preceding ones.
</bodyText>
<sectionHeader confidence="0.999741" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999924777777778">
We have argued that one interpretation of the ISC
is that it constitutes the point where the informa-
tiveness on the remaining part of the AL pool is
lower than the informativeness on a different and
independent data set with the same distribution. In
the first AL scenario where there is one static pool
to select from, reaching this point can be inter-
preted as an overall stopping point for annotation.
Here, the ISC represents a trade-off between the
amount of data annotated and the classifier perfor-
mance obtained such that the resulting classifier is
nearly optimal with respect to the data at hand. In
the second, stream-based AL scenario where several
smaller partitions are consecutively made available
to the learner, the ISC serves as an indicator that the
annotation of one batch should be terminated, and
that the mark-up should proceed with the next batch.
The ISC constitutes an intrinsic way of determin-
ing when to stop the learning process. It does not
require any external parameters such as pre-defined
thresholds to be set, and it depends only on the char-
acteristics of the data and base learner at hand. The
ISC can be utilized to relate the performance of the
classifier to the performance that is possible to ob-
tain by the data and learner at hand.
The ISC can not be used to estimate the perfor-
mance of the classifier. Consequently, it can not be
used to relate the classifier’s performance to an ex-
ternally set level, such as a particular F-score pro-
vided by the user. In this sense, the ISC may serve as
a complement to stopping criteria requiring the clas-
sifier to achieve absolute performance measures be-
fore the learning process is aborted, for instance the
max-confidence proposed by Zhu and Hovy (2007),
and the minimal absolute performance introduced
by Laws and Sch¨utze (2008).
</bodyText>
<sectionHeader confidence="0.99127" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999987514285714">
We have defined and empirically tested an intrinsic
stopping criterion (ISC) for committee-based AL.
The results of our experiments in two named en-
tity recognition scenarios show that the stopping cri-
terion is indeed a viable one, which represents a
fair trade-off between data use and classifier perfor-
mance. In a setting in which the unlabeled pool of
data used for learning is static, terminating the learn-
ing process by means of the ISC results in a nearly
optimal classifier. The ISC can also be used for de-
ciding when the pool of unlabeled data needs to be
refreshed.
We have focused on challenging the ISC with re-
spect to named entity recognition, approached in
two very different settings; future work includes ex-
periments using the ISC for other tasks. We be-
lieve that the ISC is likely to work in AL-based ap-
proaches to, e.g., part-of-speech tagging, and chunk-
ing as well. It should be kept in mind that while
the types of experiments conducted here concern
the same task, the ways they are realized differ in
many respects: the ways the decision committees
are formed, the data sets used, the representation of
instances, the relation between the sample size and
the instance size, as well as the pre-processing tools
used. Despite these differences, which outnumbers
the similarities, the ISC proves a viable stopping cri-
terion.
An assumption underlying the ISC is that the ini-
tial distribution of instances in the pool of unlabeled
data used for learning, and the distribution of in-
stances in the validation set are the same (or at least
very similar). Future work also includes investiga-
tions of automatic ways to ensure that this assump-
tion is met.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99899325">
The first author was funded by the EC project
COMPANIONS (IST-FP6-034434), the second au-
thor was funded by the EC projects BOOTStrep
(FP6-028099) and CALBC (FP7-231727).
</bodyText>
<figure confidence="0.989531666666667">
Partition
1
2
3
4
Sents Toks
320 4,641
580 7,932
840 13,444
1070 16,751
Sentences per partition
320
260
260
230
</figure>
<page confidence="0.992796">
145
</page>
<sectionHeader confidence="0.995252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953276595745">
Markus Becker and Miles Osborne. 2005. A Two-Stage
Method for Active Learning of Statistical Grammars.
In Proc 19th IJCAI, Edinburgh, Scotland, UK.
Adam Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 22(1):39–71.
Sean P. Engelson and Ido Dagan. 1996. Minimizing
Manual Annotation Cost In Supervised Training From
Corpora. In Proc 34th ACL, Santa Cruz, California,
USA.
Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006.
Large-Scale Text Categorization by Batch Mode Ac-
tive Learning. In Proc 15th WWW, Edinburgh, Scot-
land.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proc 18th ICML, Williamstown, Massachusetts, USA.
Florian Laws and Hinrich Sch¨utze. 2008. Stopping Cri-
teria for Active Learning of Named Entity Recogni-
tion. In Proc 22nd COLING, Manchester, England.
David D. Lewis and William A. Gale. 1994. A Sequen-
tial Algorithm for Training Text Classifiers. In Proc
17th ACM-SIGIR, Dublin, Ireland.
Linguistic Data Consortium. 2001. Message understand-
ing conference (muc) 7. LDC2001T02. FTP FILE.
Philadelphia: Linguistic Data Consortium.
Fredrik Olsson. 2008. Bootstrapping Named Entity An-
notation by means of Active Machine Learning – A
Method for Creating Corpora. Ph.D. thesis, Depart-
ment of Swedish, University of Gothenburg.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
Chunking using Transformation Based Learning. In
Proc 3rd VLC, Massachusetts Institute of Technology,
Cambridge, Massachusetts, USA.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active Learning for Part-of-
Speech Tagging: Accelerating Corpus Annotation. In
Proc Linguistic Annotation Workshop, Prague, Czech
Republic.
Greg Schohn and David Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In Proc
17th ICML, Stanford University, Stanford, California,
USA.
Hinrich Sch¨utze, Emre Velipasaoglu, and Jan O. Peder-
sen. 2006. Performance Thresholding in Practical
Text Classification. In Proc 15th CIKM, Arlington,
Virginia, USA.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-Criteria-based Active Learn-
ing for Named Entity Recognition. In Proc 42nd ACL,
Barcelona, Spain.
Pasi Tapanainen and Timo J¨arvinen. 1997. A Non-
Projective Dependency Parser. In Proc 5th ANLP,
Washington DC, USA.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active Learning for Natural
Language Parsing and Information Extraction. In Proc
16th ICML, Bled, Slovenia.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 Shared Task: Language
Independent Named Entity Recognition. In Proc 7th
CoNLL, Edmonton, Alberta, Canada.
Katrin Tomanek and Udo Hahn. 2008. Approximating
Learning Curves for Active-Learning-Driven Annota-
tion. In Proc 6th LREC, Marrakech, Morocco.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An Approach to Text Corpus Construction
which Cuts Annotation Costs and Maintains Reusabil-
ity of Annotated Data. In Proc Joint EMNLP-CoNLL,
Prague, Czech Republic.
Andreas Vlachos. 2008. A Stopping Criterion for
Active Learning. Computer, Speech and Language,
22(3):295–312, July.
Geoffrey I. Webb. 2000. MultiBoosting: A Tech-
nique for Combining Boosting and Wagging. Machine
Learning, 40(2):159–196, August.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools with Java Implementa-
tions. 2nd Edition. Morgan Kaufmann, San Fransisco.
Jingbo Zhu and Eduard Hovy. 2007. Active Learning
for Word Sense Disambiguation with Methods for Ad-
dressing the Class Imbalance Problem. In Proc Joint
EMNLP-CoNLL, Prague, Czech Republic.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008a.
Learning a Stopping Criterion for Active Learning for
Word Sense Disambiguation and Text Classification.
In Proc 3rd IJCNLP, Hyderabad, India.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008b.
Multi-Criteria-based Strategy to Stop Active Learning
for Data Annotation. In Proc 22nd COLING, Manch-
ester, England.
</reference>
<page confidence="0.99882">
146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.043348">
<title confidence="0.999834">An Intrinsic Stopping Criterion for Committee-Based Active Learning</title>
<author confidence="0.746455">Fredrik Olsson</author>
<affiliation confidence="0.221896">Box</affiliation>
<address confidence="0.520119">SE-164 29 Kista,</address>
<email confidence="0.980618">fredrik.olsson@sics.se</email>
<author confidence="0.990562">Katrin Tomanek</author>
<affiliation confidence="0.6721115">Jena University Language &amp; Information Engineering Friedrich-Schiller-Universit¨at</affiliation>
<address confidence="0.367311">F¨urstengraben 30, D-07743 Jena,</address>
<email confidence="0.990977">katrin.tomanek@uni-jena.de</email>
<abstract confidence="0.998651333333333">As supervised machine learning methods are increasingly used in language technology, the need for high-quality annotated language data becomes imminent. Active learning (AL) is a means to alleviate the burden of annotation. This paper addresses the problem of knowing when to stop the AL process without having the human annotator make an explicit decision on the matter. We propose and evaluate an intrinsic criterion for committee-based AL of named entity recognizers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Markus Becker</author>
<author>Miles Osborne</author>
</authors>
<title>A Two-Stage Method for Active Learning of Statistical Grammars.</title>
<date>2005</date>
<booktitle>In Proc 19th IJCAI,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="1544" citStr="Becker and Osborne, 2005" startWordPosition="226" endWordPosition="229">r high-quality labeled text becomes imminent. On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive. Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation. Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). AL is a supervised machine learning technique in which the learner is in control of the data used for learning – the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions. The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and produces a classifier and a relatively small set of newly labeled data. The overall goal is to obtain as good a classifier as possible, without having to mark-up and supply the learner with more t</context>
</contexts>
<marker>Becker, Osborne, 2005</marker>
<rawString>Markus Becker and Miles Osborne. 2005. A Two-Stage Method for Active Learning of Statistical Grammars. In Proc 19th IJCAI, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="16457" citStr="Berger et al., 1996" startWordPosition="2704" endWordPosition="2707">s less a informative one. 4.1 Classifier-centric experimental settings In common AL scenarios, the main goal of using AL is to create a good classifier with minimal label complexity. To follow this idea, we select sentences that are assumed to be useful for classifier training. We decided to select complete sentences – instead of, e.g., single tokens – as in practice annotators must see the context of words to decide on their entity labels. Our experimental setting is based on the AL approach described by Tomanek et al. (2007): The committee consists of k = 3 Maximum Entropy (ME) classifiers (Berger et al., 1996). In each AL iteration, each classifier is trained on a randomly drawn (sampling without replacement) subset L′ C L with L′ = 23L, L being the set of all instances labeled so far (cf. EnsembleGenerationMethod in Figure 1). Usefulness of a sentence is estimated as the average token Vote Entropy (cf. Equation 1). In each AL iteration, the 20 most useful sentences are selected (n = 20 in Step 3 in Figure 1). AL is started from a randomly chosen seed of 20 sentences. While we made use of ME classifiers during the selection, we employed an NE tagger based on Conditional Random Fields (CRF) (Laffert</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing Manual Annotation Cost In Supervised Training From Corpora.</title>
<date>1996</date>
<booktitle>In Proc 34th ACL,</booktitle>
<location>Santa Cruz, California, USA.</location>
<contexts>
<context position="15500" citStr="Engelson and Dagan, 1996" startWordPosition="2525" endWordPosition="2528">ement between the committee members is quantified. The ISC does neither rely on a particular base learner, nor on a particular way of creating the decision committee. 4 Experiments To challenge the definition of the ISC, we conducted two types of experiments concerning named entity recognition. The primary focus of the first type of experiment is on creating classifiers (classifiercentric), while the second type is concerned with the creation of annotated documents (data-centric). In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996): 1 � V (l, e) k logV (l, e) V E(e) = − (1) log k k l where k is the number of members in the committee, and V (l, e) is the number of members assigning label l to instance e. If an instance obtains a low Vote Entropy value, it means that the committee members are in high agreement concerning its classification, and thus also that it is less a informative one. 4.1 Classifier-centric experimental settings In common AL scenarios, the main goal of using AL is to create a good classifier with minimal label complexity. To follow this idea, we select sentences that are assumed to be useful for class</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. Minimizing Manual Annotation Cost In Supervised Training From Corpora. In Proc 34th ACL, Santa Cruz, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven C H Hoi</author>
<author>Rong Jin</author>
<author>Michael R Lyu</author>
</authors>
<title>Large-Scale Text Categorization by Batch Mode Active Learning.</title>
<date>2006</date>
<booktitle>In Proc 15th WWW,</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1434" citStr="Hoi et al., 2006" startWordPosition="209" endWordPosition="212">h the increasing popularity of supervised machine learning methods in language processing, the need for high-quality labeled text becomes imminent. On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive. Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation. Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). AL is a supervised machine learning technique in which the learner is in control of the data used for learning – the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions. The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and produces a classifier and a relatively small set of newly labeled data. The overall go</context>
</contexts>
<marker>Hoi, Jin, Lyu, 2006</marker>
<rawString>Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006. Large-Scale Text Categorization by Batch Mode Active Learning. In Proc 15th WWW, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proc 18th ICML,</booktitle>
<location>Williamstown, Massachusetts, USA.</location>
<contexts>
<context position="17072" citStr="Lafferty et al., 2001" startWordPosition="2817" endWordPosition="2820">, 1996). In each AL iteration, each classifier is trained on a randomly drawn (sampling without replacement) subset L′ C L with L′ = 23L, L being the set of all instances labeled so far (cf. EnsembleGenerationMethod in Figure 1). Usefulness of a sentence is estimated as the average token Vote Entropy (cf. Equation 1). In each AL iteration, the 20 most useful sentences are selected (n = 20 in Step 3 in Figure 1). AL is started from a randomly chosen seed of 20 sentences. While we made use of ME classifiers during the selection, we employed an NE tagger based on Conditional Random Fields (CRF) (Lafferty et al., 2001) during evaluation time to determine the learning curves. CRFs have a significantly higher tagging performance, so the final classifier we are aiming at should be a CRF model. We have shown before (Tomanek et al., 2007) that MEs are well apt as selectors with the advantage of much shorter training times than CRFs. For both MEs and CRFs the 141 same features were employed which comprised orthographical (based mainly on regular expressions), lexical and morphological (suffixed/prefixed, word itself), syntactic (POS tags), as well as contextual (features of neighboring tokens) ones. The experimen</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc 18th ICML, Williamstown, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Laws</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Stopping Criteria for Active Learning of Named Entity Recognition.</title>
<date>2008</date>
<booktitle>In Proc 22nd COLING,</booktitle>
<location>Manchester, England.</location>
<marker>Laws, Sch¨utze, 2008</marker>
<rawString>Florian Laws and Hinrich Sch¨utze. 2008. Stopping Criteria for Active Learning of Named Entity Recognition. In Proc 22nd COLING, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A Sequential Algorithm for Training Text Classifiers.</title>
<date>1994</date>
<booktitle>In Proc 17th ACM-SIGIR,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1415" citStr="Lewis and Gale, 1994" startWordPosition="205" endWordPosition="208">rs. 1 Introduction With the increasing popularity of supervised machine learning methods in language processing, the need for high-quality labeled text becomes imminent. On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive. Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation. Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). AL is a supervised machine learning technique in which the learner is in control of the data used for learning – the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions. The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and produces a classifier and a relatively small set of newly labeled d</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A Sequential Algorithm for Training Text Classifiers. In Proc 17th ACM-SIGIR, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>Message understanding conference (muc) 7. LDC2001T02. FTP FILE. Philadelphia: Linguistic Data Consortium.</title>
<date>2001</date>
<contexts>
<context position="19080" citStr="Consortium, 2001" startWordPosition="3148" endWordPosition="3149">mes are marked up). This experiment concerns a method of orchestrating AL in a way beneficial for the bootstrapping of annotated data (Olsson, 2008). The bootstrapping proper is realized by means of AL for selecting documents to annotate, as opposed to sentences. This way the annotated data set is comprised of entire documents thus promoting data creation. As in the classifier-centric setting, the task is to recognize names – persons, organizations, locations, times, dates, monetary expressions, and percentages – in news wire texts. The texts used are part of the MUC-7 corpus (Linguistic Data Consortium, 2001) and consists of 100 documents, 3,480 sentences, and 90,790 tokens. The task is approached using the IOB tagging scheme proposed by, e.g., Ramshaw and Marcus (1995), turning the original 7-class task into a 15-class task. Each token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g., Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar (Tapanainen and J¨arvinen, 1997) (e.g., Case, Part-of-speech), fetched from precompiled lists o</context>
</contexts>
<marker>Consortium, 2001</marker>
<rawString>Linguistic Data Consortium. 2001. Message understanding conference (muc) 7. LDC2001T02. FTP FILE. Philadelphia: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrik Olsson</author>
</authors>
<title>Bootstrapping Named Entity Annotation by means of Active Machine Learning – A Method for Creating Corpora.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Swedish, University of Gothenburg.</institution>
<contexts>
<context position="18611" citStr="Olsson, 2008" startWordPosition="3073" endWordPosition="3075"> 14,000 sentences (≈ 200, 000 tokens). As validation set and as gold standard for plotting the learning curve we used CoNLL’s evaluation corpus which sums up to 3,453 sentences. 4.2 Data-centric experimental settings While AL is commonly used to create as good classifiers as possible, with the amount of human effort kept to a minimum, it may result in fragmented and possibly non re-usable annotations (e.g., a collection of documents in which only some of the names are marked up). This experiment concerns a method of orchestrating AL in a way beneficial for the bootstrapping of annotated data (Olsson, 2008). The bootstrapping proper is realized by means of AL for selecting documents to annotate, as opposed to sentences. This way the annotated data set is comprised of entire documents thus promoting data creation. As in the classifier-centric setting, the task is to recognize names – persons, organizations, locations, times, dates, monetary expressions, and percentages – in news wire texts. The texts used are part of the MUC-7 corpus (Linguistic Data Consortium, 2001) and consists of 100 documents, 3,480 sentences, and 90,790 tokens. The task is approached using the IOB tagging scheme proposed by</context>
</contexts>
<marker>Olsson, 2008</marker>
<rawString>Fredrik Olsson. 2008. Bootstrapping Named Entity Annotation by means of Active Machine Learning – A Method for Creating Corpora. Ph.D. thesis, Department of Swedish, University of Gothenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text Chunking using Transformation Based Learning.</title>
<date>1995</date>
<booktitle>In Proc 3rd VLC, Massachusetts Institute of Technology,</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="19244" citStr="Ramshaw and Marcus (1995)" startWordPosition="3173" endWordPosition="3176">otstrapping proper is realized by means of AL for selecting documents to annotate, as opposed to sentences. This way the annotated data set is comprised of entire documents thus promoting data creation. As in the classifier-centric setting, the task is to recognize names – persons, organizations, locations, times, dates, monetary expressions, and percentages – in news wire texts. The texts used are part of the MUC-7 corpus (Linguistic Data Consortium, 2001) and consists of 100 documents, 3,480 sentences, and 90,790 tokens. The task is approached using the IOB tagging scheme proposed by, e.g., Ramshaw and Marcus (1995), turning the original 7-class task into a 15-class task. Each token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g., Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar (Tapanainen and J¨arvinen, 1997) (e.g., Case, Part-of-speech), fetched from precompiled lists of information (e.g., Is first name?), and features based on predictions concerning the context of the token (e.g, Class ofprevious token). The decision committee is</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text Chunking using Transformation Based Learning. In Proc 3rd VLC, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Ringger</author>
<author>Peter McClanahan</author>
<author>Robbie Haertel</author>
<author>George Busby</author>
<author>Marc Carmen</author>
<author>James Carroll</author>
<author>Kevin Seppi</author>
<author>Deryle Lonsdale</author>
</authors>
<title>Active Learning for Part-ofSpeech Tagging: Accelerating Corpus Annotation.</title>
<date>2007</date>
<booktitle>In Proc Linguistic Annotation Workshop,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1481" citStr="Ringger et al., 2007" startWordPosition="215" endWordPosition="218">achine learning methods in language processing, the need for high-quality labeled text becomes imminent. On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive. Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation. Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). AL is a supervised machine learning technique in which the learner is in control of the data used for learning – the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions. The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and produces a classifier and a relatively small set of newly labeled data. The overall goal is to obtain as good a classifier as possibl</context>
</contexts>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, 2007</marker>
<rawString>Eric Ringger, Peter McClanahan, Robbie Haertel, George Busby, Marc Carmen, James Carroll, Kevin Seppi, and Deryle Lonsdale. 2007. Active Learning for Part-ofSpeech Tagging: Accelerating Corpus Annotation. In Proc Linguistic Annotation Workshop, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Schohn</author>
<author>David Cohn</author>
</authors>
<title>Less is More: Active Learning with Support Vector Machines.</title>
<date>2000</date>
<booktitle>In Proc 17th ICML,</booktitle>
<institution>Stanford University,</institution>
<location>Stanford, California, USA.</location>
<contexts>
<context position="10319" citStr="Schohn and Cohn (2000)" startWordPosition="1702" endWordPosition="1705">tion Agreement is larger than, or equal to, the Validation Set Agreement. In relation to the stopping criterion based solely on SA proposed by Tomanek et al. (2007), the above defined criterion comes into effect earlier in the AL process. Furthermore, while it was claimed in (Tomanek and Hahn, 2008) that one can observe the classifier convergence from the VSA curve (as it approximated the progression of the learning curve), that requires a threshold to be specified for the actual stopping point. The ISC is completely intrinsic and does thus not require any thresholds to be set. 3 Related work Schohn and Cohn (2000) report on document classification using AL with Support Vector Machines. 2The sample has to be large enough to mimic the distribution of instances in the original unlabeled pool. If the most informative instance is no closer to the decision hyperplane than any of the support vectors, the margin has been exhausted and AL is terminated. Vlachos (2008) suggests to use classifier confidence to define a stopping criterion for uncertaintybased sampling. The idea is to stop learning when the confidence of the classifier, on an external, possibly unannotated test set, remains at the same level or dro</context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Greg Schohn and David Cohn. 2000. Less is More: Active Learning with Support Vector Machines. In Proc 17th ICML, Stanford University, Stanford, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Emre Velipasaoglu</author>
<author>Jan O Pedersen</author>
</authors>
<title>Performance Thresholding in Practical Text Classification.</title>
<date>2006</date>
<booktitle>In Proc 15th CIKM,</booktitle>
<location>Arlington, Virginia, USA.</location>
<marker>Sch¨utze, Velipasaoglu, Pedersen, 2006</marker>
<rawString>Hinrich Sch¨utze, Emre Velipasaoglu, and Jan O. Pedersen. 2006. Performance Thresholding in Practical Text Classification. In Proc 15th CIKM, Arlington, Virginia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>ChewLim Tan</author>
</authors>
<title>Multi-Criteria-based Active Learning for Named Entity Recognition.</title>
<date>2004</date>
<booktitle>In Proc 42nd ACL,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1349" citStr="Shen et al., 2004" startWordPosition="195" endWordPosition="198">nsic criterion for committee-based AL of named entity recognizers. 1 Introduction With the increasing popularity of supervised machine learning methods in language processing, the need for high-quality labeled text becomes imminent. On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive. Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation. Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). AL is a supervised machine learning technique in which the learner is in control of the data used for learning – the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions. The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and p</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and ChewLim Tan. 2004. Multi-Criteria-based Active Learning for Named Entity Recognition. In Proc 42nd ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>A NonProjective Dependency Parser.</title>
<date>1997</date>
<booktitle>In Proc 5th ANLP,</booktitle>
<location>Washington DC, USA.</location>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo J¨arvinen. 1997. A NonProjective Dependency Parser. In Proc 5th ANLP, Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active Learning for Natural Language Parsing and Information Extraction.</title>
<date>1999</date>
<booktitle>In Proc 16th ICML,</booktitle>
<location>Bled, Slovenia.</location>
<contexts>
<context position="1517" citStr="Thompson et al., 1999" startWordPosition="221" endWordPosition="225">processing, the need for high-quality labeled text becomes imminent. On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive. Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation. Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). AL is a supervised machine learning technique in which the learner is in control of the data used for learning – the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions. The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and produces a classifier and a relatively small set of newly labeled data. The overall goal is to obtain as good a classifier as possible, without having to mark-up and sup</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1999. Active Learning for Natural Language Parsing and Information Extraction. In Proc 16th ICML, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proc 7th CoNLL,</booktitle>
<location>Edmonton, Alberta, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language Independent Named Entity Recognition. In Proc 7th CoNLL, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
</authors>
<title>Approximating Learning Curves for Active-Learning-Driven Annotation. In</title>
<date>2008</date>
<booktitle>Proc 6th LREC,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="7356" citStr="Tomanek and Hahn, 2008" startWordPosition="1192" endWordPosition="1195">vergence, and consequently, we can define the best possible classifier to be one which cannot learn more from the remaining pool of data. The intrinsic stopping criterion (ISC) we propose here focuses on the latter aspect of the ideal stopping point described above – exhaustiveness of the AL pool. We suggest to stop the annotation process of the data from a given pool when the base learner cannot learn (much) more from it. The definition of our intrinsic stopping criterion for committee-based AL builds on the notions of Selection Agreement (Tomanek et al., 2007), and Validation Set Agreement (Tomanek and Hahn, 2008). The Selection Agreement (SA) is the agreement among the members of a decision committee regarding the classification of the most informative instance selected from the pool of unlabeled data in each AL round. The intuition underlying the SA is that the committee will agree more on the hard instances selected from the remaining set of unlabeled data as the AL process proceeds. When the members of the committee are in complete agreement, AL should be aborted since it no longer contributes to the overall learning process – in this case, AL is but a computationally expensive counterpart of rando</context>
<context position="9997" citStr="Tomanek and Hahn, 2008" startWordPosition="1646" endWordPosition="1649">mittee would learn more from a random sample2 from the validation set (or from a data source exhibiting the same distribution of instances), than it would from the unlabeled data pool. Based on this argument, a stopping criterion for committee-based AL can be formulated as: Active learning may be terminated when the Selection Agreement is larger than, or equal to, the Validation Set Agreement. In relation to the stopping criterion based solely on SA proposed by Tomanek et al. (2007), the above defined criterion comes into effect earlier in the AL process. Furthermore, while it was claimed in (Tomanek and Hahn, 2008) that one can observe the classifier convergence from the VSA curve (as it approximated the progression of the learning curve), that requires a threshold to be specified for the actual stopping point. The ISC is completely intrinsic and does thus not require any thresholds to be set. 3 Related work Schohn and Cohn (2000) report on document classification using AL with Support Vector Machines. 2The sample has to be large enough to mimic the distribution of instances in the original unlabeled pool. If the most informative instance is no closer to the decision hyperplane than any of the support v</context>
<context position="13857" citStr="Tomanek and Hahn, 2008" startWordPosition="2254" endWordPosition="2257">e gradient of the classifier’s estimated performance or uncertainty. Laws and Sch¨utze conclude that both gradient-based approaches, that is, convergence, can be used as stopping criteria relative to the optimal performance achievable on a given pool of data. They also show that while their method lends itself to acceptable estimates of accuracy, it is much harder to estimate the recall of the classifier. Thus, the stopping criteria based on minimal absolute or maximum possible performance are not reliable. The work most related to ours is that of Tomanek and colleagues (Tomanek et al., 2007; Tomanek and Hahn, 2008) who define and evaluate the Selection Agreement (SA) and the Validation Set Agreement (VSA) already introduced in Section 2. Tomanek and Hahn (2008) conclude that monitoring the progress of AL should be based on a separate validation set instead of the data directly affected by the learning process – thus, VSA is preferred over SA. Further, they find that the VSA curve approximates the progression of the learning curve and thus classifier performance convergence could be estimated. However, to actually find where to stop the annotation, a threshold needs to be set. Our proposed intrinsic stop</context>
</contexts>
<marker>Tomanek, Hahn, 2008</marker>
<rawString>Katrin Tomanek and Udo Hahn. 2008. Approximating Learning Curves for Active-Learning-Driven Annotation. In Proc 6th LREC, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>An Approach to Text Corpus Construction which Cuts Annotation Costs and Maintains Reusability of Annotated Data.</title>
<date>2007</date>
<booktitle>In Proc Joint EMNLP-CoNLL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1372" citStr="Tomanek et al., 2007" startWordPosition="199" endWordPosition="202">committee-based AL of named entity recognizers. 1 Introduction With the increasing popularity of supervised machine learning methods in language processing, the need for high-quality labeled text becomes imminent. On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive. Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation. Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). AL is a supervised machine learning technique in which the learner is in control of the data used for learning – the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions. The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and produces a classifier an</context>
<context position="7301" citStr="Tomanek et al., 2007" startWordPosition="1183" endWordPosition="1186"> criterion may be based on classifier performance convergence, and consequently, we can define the best possible classifier to be one which cannot learn more from the remaining pool of data. The intrinsic stopping criterion (ISC) we propose here focuses on the latter aspect of the ideal stopping point described above – exhaustiveness of the AL pool. We suggest to stop the annotation process of the data from a given pool when the base learner cannot learn (much) more from it. The definition of our intrinsic stopping criterion for committee-based AL builds on the notions of Selection Agreement (Tomanek et al., 2007), and Validation Set Agreement (Tomanek and Hahn, 2008). The Selection Agreement (SA) is the agreement among the members of a decision committee regarding the classification of the most informative instance selected from the pool of unlabeled data in each AL round. The intuition underlying the SA is that the committee will agree more on the hard instances selected from the remaining set of unlabeled data as the AL process proceeds. When the members of the committee are in complete agreement, AL should be aborted since it no longer contributes to the overall learning process – in this case, AL </context>
<context position="9861" citStr="Tomanek et al. (2007)" startWordPosition="1624" endWordPosition="1627">nformative instances in the (diminishing) unlabeled pool than it is concerning the validation set. This, in turn, implies that the committee would learn more from a random sample2 from the validation set (or from a data source exhibiting the same distribution of instances), than it would from the unlabeled data pool. Based on this argument, a stopping criterion for committee-based AL can be formulated as: Active learning may be terminated when the Selection Agreement is larger than, or equal to, the Validation Set Agreement. In relation to the stopping criterion based solely on SA proposed by Tomanek et al. (2007), the above defined criterion comes into effect earlier in the AL process. Furthermore, while it was claimed in (Tomanek and Hahn, 2008) that one can observe the classifier convergence from the VSA curve (as it approximated the progression of the learning curve), that requires a threshold to be specified for the actual stopping point. The ISC is completely intrinsic and does thus not require any thresholds to be set. 3 Related work Schohn and Cohn (2000) report on document classification using AL with Support Vector Machines. 2The sample has to be large enough to mimic the distribution of inst</context>
<context position="13832" citStr="Tomanek et al., 2007" startWordPosition="2250" endWordPosition="2253">ce is calculated as the gradient of the classifier’s estimated performance or uncertainty. Laws and Sch¨utze conclude that both gradient-based approaches, that is, convergence, can be used as stopping criteria relative to the optimal performance achievable on a given pool of data. They also show that while their method lends itself to acceptable estimates of accuracy, it is much harder to estimate the recall of the classifier. Thus, the stopping criteria based on minimal absolute or maximum possible performance are not reliable. The work most related to ours is that of Tomanek and colleagues (Tomanek et al., 2007; Tomanek and Hahn, 2008) who define and evaluate the Selection Agreement (SA) and the Validation Set Agreement (VSA) already introduced in Section 2. Tomanek and Hahn (2008) conclude that monitoring the progress of AL should be based on a separate validation set instead of the data directly affected by the learning process – thus, VSA is preferred over SA. Further, they find that the VSA curve approximates the progression of the learning curve and thus classifier performance convergence could be estimated. However, to actually find where to stop the annotation, a threshold needs to be set. Ou</context>
<context position="16369" citStr="Tomanek et al. (2007)" startWordPosition="2689" endWordPosition="2692">ttee members are in high agreement concerning its classification, and thus also that it is less a informative one. 4.1 Classifier-centric experimental settings In common AL scenarios, the main goal of using AL is to create a good classifier with minimal label complexity. To follow this idea, we select sentences that are assumed to be useful for classifier training. We decided to select complete sentences – instead of, e.g., single tokens – as in practice annotators must see the context of words to decide on their entity labels. Our experimental setting is based on the AL approach described by Tomanek et al. (2007): The committee consists of k = 3 Maximum Entropy (ME) classifiers (Berger et al., 1996). In each AL iteration, each classifier is trained on a randomly drawn (sampling without replacement) subset L′ C L with L′ = 23L, L being the set of all instances labeled so far (cf. EnsembleGenerationMethod in Figure 1). Usefulness of a sentence is estimated as the average token Vote Entropy (cf. Equation 1). In each AL iteration, the 20 most useful sentences are selected (n = 20 in Step 3 in Figure 1). AL is started from a randomly chosen seed of 20 sentences. While we made use of ME classifiers during t</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. An Approach to Text Corpus Construction which Cuts Annotation Costs and Maintains Reusability of Annotated Data. In Proc Joint EMNLP-CoNLL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
</authors>
<title>A Stopping Criterion for Active Learning.</title>
<date>2008</date>
<journal>Computer, Speech and Language,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="10671" citStr="Vlachos (2008)" startWordPosition="1762" endWordPosition="1763">A curve (as it approximated the progression of the learning curve), that requires a threshold to be specified for the actual stopping point. The ISC is completely intrinsic and does thus not require any thresholds to be set. 3 Related work Schohn and Cohn (2000) report on document classification using AL with Support Vector Machines. 2The sample has to be large enough to mimic the distribution of instances in the original unlabeled pool. If the most informative instance is no closer to the decision hyperplane than any of the support vectors, the margin has been exhausted and AL is terminated. Vlachos (2008) suggests to use classifier confidence to define a stopping criterion for uncertaintybased sampling. The idea is to stop learning when the confidence of the classifier, on an external, possibly unannotated test set, remains at the same level or drops for a number of consecutive iterations during the AL process. Vlachos shows that the criterion indeed is applicable to the tasks he investigates. Zhu and colleagues (Zhu and Hovy, 2007; Zhu et al., 2008a; Zhu et al., 2008b) introduce max-confidence, min-error, minimum expected error strategy, overall-uncertainty, and classificationchange as means </context>
</contexts>
<marker>Vlachos, 2008</marker>
<rawString>Andreas Vlachos. 2008. A Stopping Criterion for Active Learning. Computer, Speech and Language, 22(3):295–312, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey I Webb</author>
</authors>
<title>MultiBoosting: A Technique for Combining Boosting and Wagging.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="19915" citStr="Webb, 2000" startWordPosition="3274" endWordPosition="3275">ch token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g., Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar (Tapanainen and J¨arvinen, 1997) (e.g., Case, Part-of-speech), fetched from precompiled lists of information (e.g., Is first name?), and features based on predictions concerning the context of the token (e.g, Class ofprevious token). The decision committee is made up from 10 boosted decision trees using MultiBoostAB (Webb, 2000) (cf. EnsembleGenerationMethod in Figure 1). Each classifier is created by the REPTree decision tree learner described by Witten and Frank (2005). The informativeness of a document is calculated by means of average token Vote Entropy (cf. Equation 1). The seed set of the AL process consists of five randomly selected documents. In each AL iteration, one document is selected for annotation from the corpus (n = 1 in Step 3 in Figure 1). 5 Results Two different scenarios were used to illustrate the applicability of the proposed intrinsic stopping criterion. In the first scenario, we assumed that t</context>
</contexts>
<marker>Webb, 2000</marker>
<rawString>Geoffrey I. Webb. 2000. MultiBoosting: A Technique for Combining Boosting and Wagging. Machine Learning, 40(2):159–196, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools with Java Implementations. 2nd Edition.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Fransisco.</location>
<contexts>
<context position="20060" citStr="Witten and Frank (2005)" startWordPosition="3293" endWordPosition="3296">en (e.g., Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar (Tapanainen and J¨arvinen, 1997) (e.g., Case, Part-of-speech), fetched from precompiled lists of information (e.g., Is first name?), and features based on predictions concerning the context of the token (e.g, Class ofprevious token). The decision committee is made up from 10 boosted decision trees using MultiBoostAB (Webb, 2000) (cf. EnsembleGenerationMethod in Figure 1). Each classifier is created by the REPTree decision tree learner described by Witten and Frank (2005). The informativeness of a document is calculated by means of average token Vote Entropy (cf. Equation 1). The seed set of the AL process consists of five randomly selected documents. In each AL iteration, one document is selected for annotation from the corpus (n = 1 in Step 3 in Figure 1). 5 Results Two different scenarios were used to illustrate the applicability of the proposed intrinsic stopping criterion. In the first scenario, we assumed that the pool of unlabeled data was static and fairly large. In the second scenario, we assumed that the unlabeled data would be collected in smaller b</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools with Java Implementations. 2nd Edition. Morgan Kaufmann, San Fransisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Eduard Hovy</author>
</authors>
<title>Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem.</title>
<date>2007</date>
<booktitle>In Proc Joint EMNLP-CoNLL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11106" citStr="Zhu and Hovy, 2007" startWordPosition="1833" endWordPosition="1836">eled pool. If the most informative instance is no closer to the decision hyperplane than any of the support vectors, the margin has been exhausted and AL is terminated. Vlachos (2008) suggests to use classifier confidence to define a stopping criterion for uncertaintybased sampling. The idea is to stop learning when the confidence of the classifier, on an external, possibly unannotated test set, remains at the same level or drops for a number of consecutive iterations during the AL process. Vlachos shows that the criterion indeed is applicable to the tasks he investigates. Zhu and colleagues (Zhu and Hovy, 2007; Zhu et al., 2008a; Zhu et al., 2008b) introduce max-confidence, min-error, minimum expected error strategy, overall-uncertainty, and classificationchange as means to terminate AL. They primarily use a single-classifier approach to word sense disambiguation and text classification in their experiments. Max-confidence seeks to terminate AL once the classifier is most confident in its predictions. In the min-error strategy, the learning is halted when there is no difference between the classifier’s predictions and those labels provided by a human annotator. The minimum expected error strategy i</context>
<context position="30876" citStr="Zhu and Hovy (2007)" startWordPosition="5193" endWordPosition="5196">r at hand. The ISC can be utilized to relate the performance of the classifier to the performance that is possible to obtain by the data and learner at hand. The ISC can not be used to estimate the performance of the classifier. Consequently, it can not be used to relate the classifier’s performance to an externally set level, such as a particular F-score provided by the user. In this sense, the ISC may serve as a complement to stopping criteria requiring the classifier to achieve absolute performance measures before the learning process is aborted, for instance the max-confidence proposed by Zhu and Hovy (2007), and the minimal absolute performance introduced by Laws and Sch¨utze (2008). 7 Conclusions and Future Work We have defined and empirically tested an intrinsic stopping criterion (ISC) for committee-based AL. The results of our experiments in two named entity recognition scenarios show that the stopping criterion is indeed a viable one, which represents a fair trade-off between data use and classifier performance. In a setting in which the unlabeled pool of data used for learning is static, terminating the learning process by means of the ISC results in a nearly optimal classifier. The ISC ca</context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>Jingbo Zhu and Eduard Hovy. 2007. Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem. In Proc Joint EMNLP-CoNLL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Huizhen Wang</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning a Stopping Criterion for Active Learning for Word Sense Disambiguation and Text Classification.</title>
<date>2008</date>
<booktitle>In Proc 3rd IJCNLP,</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="11124" citStr="Zhu et al., 2008" startWordPosition="1837" endWordPosition="1840">st informative instance is no closer to the decision hyperplane than any of the support vectors, the margin has been exhausted and AL is terminated. Vlachos (2008) suggests to use classifier confidence to define a stopping criterion for uncertaintybased sampling. The idea is to stop learning when the confidence of the classifier, on an external, possibly unannotated test set, remains at the same level or drops for a number of consecutive iterations during the AL process. Vlachos shows that the criterion indeed is applicable to the tasks he investigates. Zhu and colleagues (Zhu and Hovy, 2007; Zhu et al., 2008a; Zhu et al., 2008b) introduce max-confidence, min-error, minimum expected error strategy, overall-uncertainty, and classificationchange as means to terminate AL. They primarily use a single-classifier approach to word sense disambiguation and text classification in their experiments. Max-confidence seeks to terminate AL once the classifier is most confident in its predictions. In the min-error strategy, the learning is halted when there is no difference between the classifier’s predictions and those labels provided by a human annotator. The minimum expected error strategy involves estimating</context>
</contexts>
<marker>Zhu, Wang, Hovy, 2008</marker>
<rawString>Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008a. Learning a Stopping Criterion for Active Learning for Word Sense Disambiguation and Text Classification. In Proc 3rd IJCNLP, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Huizhen Wang</author>
<author>Eduard Hovy</author>
</authors>
<title>Multi-Criteria-based Strategy to Stop Active Learning for Data Annotation.</title>
<date>2008</date>
<booktitle>In Proc 22nd COLING,</booktitle>
<location>Manchester, England.</location>
<contexts>
<context position="11124" citStr="Zhu et al., 2008" startWordPosition="1837" endWordPosition="1840">st informative instance is no closer to the decision hyperplane than any of the support vectors, the margin has been exhausted and AL is terminated. Vlachos (2008) suggests to use classifier confidence to define a stopping criterion for uncertaintybased sampling. The idea is to stop learning when the confidence of the classifier, on an external, possibly unannotated test set, remains at the same level or drops for a number of consecutive iterations during the AL process. Vlachos shows that the criterion indeed is applicable to the tasks he investigates. Zhu and colleagues (Zhu and Hovy, 2007; Zhu et al., 2008a; Zhu et al., 2008b) introduce max-confidence, min-error, minimum expected error strategy, overall-uncertainty, and classificationchange as means to terminate AL. They primarily use a single-classifier approach to word sense disambiguation and text classification in their experiments. Max-confidence seeks to terminate AL once the classifier is most confident in its predictions. In the min-error strategy, the learning is halted when there is no difference between the classifier’s predictions and those labels provided by a human annotator. The minimum expected error strategy involves estimating</context>
</contexts>
<marker>Zhu, Wang, Hovy, 2008</marker>
<rawString>Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008b. Multi-Criteria-based Strategy to Stop Active Learning for Data Annotation. In Proc 22nd COLING, Manchester, England.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>