<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.980458">
n-Best Parsing Revisited∗
</title>
<author confidence="0.995707">
Matthias B¨uchse and Daniel Geisler and Torsten St¨uber and Heiko Vogler
</author>
<affiliation confidence="0.9024505">
Faculty of Computer Science
Technische Universit¨at Dresden
</affiliation>
<address confidence="0.762774">
01062 Dresden
</address>
<email confidence="0.998335">
{buechse,geisler,stueber,vogler}@tcs.inf.tu-dresden.de
</email>
<sectionHeader confidence="0.997381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901352941177">
We derive and implement an algorithm
similar to (Huang and Chiang, 2005) for
finding the n best derivations in a weighted
hypergraph. We prove the correctness and
termination of the algorithm and we show
experimental results concerning its run-
time. Our work is different from the afore-
mentioned one in the following respects:
we consider labeled hypergraphs, allowing
for tree-based language models (Maletti
and Satta, 2009); we specifically handle
the case of cyclic hypergraphs; we admit
structured weight domains, allowing for
multiple features to be processed; we use
the paradigm of functional programming
together with lazy evaluation, achieving
concise algorithmic descriptions.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974882352941">
In statistical natural language processing, proba-
bilistic models play an important role which can
be used to assign to some input sentence a set of
analyses, each carrying a probability. For instance,
an analysis can be a parse tree or a possible trans-
lation. Due to the ambiguity of natural language,
the number of analyses for one input sentence can
be very large. Some models even assign an infinite
number of analyses to an input sentence.
In many cases however, the set of analyses can
in fact be represented in a finite and compact way.
While such a representation is space-efficient, it
may be incompatible with subsequent operations.
In these cases a finite subset is used as an approx-
imation, consisting of n best analyses, i. e. n anal-
yses with highest probability. For example, this
approach has the following two applications.
</bodyText>
<listItem confidence="0.9645685">
(1) Reranking: when log-linear models (Och
and Ney, 2002) are employed, some features may
</listItem>
<bodyText confidence="0.9835332">
∗ This research was financially supported by DFG VO
1101/5-1.
not permit an efficient evaluation during the com-
putation of the analyses. These features are com-
puted using individual analyses from said approx-
imation, leading to a reranking amongst them.
(2) Spurious ambiguity: many models produce
analyses which may be too fine-grained for further
processing (Li et al., 2009). As an example, con-
sider context-free grammars, where several left-
most derivations may exist for the same terminal
string. The weight of the terminal string is ob-
tained by summing over these derivations. The
n best leftmost derivations may be used to approx-
imate this sum.
In this paper, we consider the case where the
finite, compact representation has the form of a
weighted hypergraph (with labeled hyperedges)
and the analyses are derivations of the hypergraph.
This covers many parsing applications (Klein and
Manning, 2001), including weighted deductive
systems (Goodman, 1999; Nederhof, 2003), and
also applications in machine translation (May and
Knight, 2006).
In the nomenclature of (Huang and Chiang,
2005), which we adopt here, a derivation of a hy-
pergraph is a tree which is obtained in the follow-
ing way. Starting from some node, an ingoing hy-
peredge is picked and recorded as the label of the
root of the tree. Then, for the subtrees, one con-
tinues with the source nodes of said hyperedge in
the same way. In other words, a derivation can be
understood as an unfolding of the hypergraph.
The n-best-derivations problem then amounts
to finding n derivations which are best with re-
spect to the weights induced by the weighted hy-
pergraph.1 Among others, weighted hypergraphs
with labeled hyperedges subsume the following
two concepts.
(I) probabilistic context-free grammars (pcfgs).
</bodyText>
<footnote confidence="0.884520333333333">
1Note that this problem is different from the n-best-
hyperpaths problem described by Nielsen et al. (2005), as
already argued in (Huang and Chiang, 2005, Section 2).
</footnote>
<page confidence="0.99016">
46
</page>
<note confidence="0.982722">
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 46–54,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999873891891892">
In this case, nodes correspond to nonterminals,
hyperedges are labeled with productions, and the
derivations are exactly the abstract syntax trees
(ASTs) of the grammar (which are closely related
the parse trees). Note that, unless the pcfg is un-
ambiguous, a given word may have several cor-
responding ASTs, and its weight is obtained by
summing over the weights of the ASTs. Hence,
the n best derivations need not coincide with the
n best words (cf. application (2) above).
(II) weighted tree automata (wta) (Alexandrakis
and Bozapalidis, 1987; Berstel and Reutenauer,
1982; ´Esik and Kuich, 2003; F¨ul¨op and Vogler,
2009). These automata serve both as a tree-based
language model and as a data structure for the
parse forests obtained from that language model
by applying the Bar-Hillel construction (Maletti
and Satta, 2009). It is well known that context-free
grammars and tree automata are weakly equiv-
alent (Thatcher, 1967; ´Esik and Kuich, 2003).
However, unlike the former formalism, the latter
one has the ability to model non-local dependen-
cies in parse trees.
In the case of wta, nodes correspond to states,
hyperedges are labeled with input symbols, and
the derivations are exactly the runs of the automa-
ton. Since, due to ambiguity, a given tree may
have several accepting runs, the n best derivations
need not coincide with the n best trees. As for
the pcfgs, this is an example of spurious ambigu-
ity, which can be tackled as indicated by appli-
cation (2) above. Alternatively, one can attempt
to find an equivalent deterministic wta (May and
Knight, 2006; B¨uchse et al., 2009).
Next, we briefly discuss four known algorithms
which solve the n-best-derivations problem or
subproblems thereof.
</bodyText>
<listItem confidence="0.998521285714286">
• The Viterbi algorithm solves the 1-best-
derivation problem for acyclic hypergraphs. It is
based on a topological sort of the hypergraph.
• Knuth (1977) generalizes Dijkstra’s algorithm
(for finding the single-source shortest paths in a
graph) to hypergraphs, thus solving the case n = 1
even if the hypergraph contains cycles. Knuth as-
sumes the weights to be real numbers, and he re-
quires weight functions to be monotone and supe-
rior in order to guarantee that a best derivation ex-
ists. (The superiority property corresponds to Di-
jkstra’s requirement that edge weights—or, more
generally, cycle weights—are nonnegative.)
• Huang and Chiang (2005) show that the n-
</listItem>
<bodyText confidence="0.990391818181818">
best-derivations problem can be solved efficiently
by first solving the 1-best-derivation problem and
then extending that solution in a lazy manner.
Huang and Chiang assume weighted unlabeled hy-
pergraphs with weights computed in the reals, and
they require the weight functions to be monotone.
Moreover they assume that the 1-best-
derivation problem be solved using the Viterbi
algorithm, which implies that the hypergraph must
be acyclic. However they conjecture that their
second phase also works for cyclic hypergraphs.
</bodyText>
<listItem confidence="0.97553075">
• Pauls and Klein (2009) propose a variation
of the algorithm of Huang and Chiang (2005) in
which the 1-best-derivation problem is computed
via an A∗-based exploration of the 1-best charts.
</listItem>
<bodyText confidence="0.8384115">
In this paper, we also present an algorithm
for solving the n-best-derivations problem. Ulti-
mately it uses the same algorithmic ideas as the
one of Huang and Chiang (2005); however, it is
different in the following sense:
1. we consider labeled hypergraphs, allowing
for wta to be used in parsing;
2. we specifically handle the case of cyclic
hypergraphs, thus supporting the conjecture of
Huang and Chiang; for this we impose on the
weight functions the same requirements as Knuth
and use his algorithm;
</bodyText>
<listItem confidence="0.92915525">
3. by using the concept of linear pre-orders (and
not only linear orders on the set of reals) our ap-
proach can handle structured weights such as vec-
tors over frequencies, probabilities, and reals;
4. we present our algorithm in the framework
of functional programming (and not in that of im-
perative programming); this framework allows to
decribe algorithms in a more abstract and concise,
yet natural way;
5. due to the lazy evaluation paradigm often
found in functional programming, we obtain the
laziness on which the algorithm of Huang and Chi-
ang (2005) is based for free;
6. exploiting the abstract level of description
(see point 4) we are able to prove the correctness
and termination of our algorithm.
</listItem>
<bodyText confidence="0.9994595">
At the end of this paper, we will discuss experi-
ments which have been performed with an imple-
mentation of our algorithm in the functional pro-
gramming language HASKELL.
</bodyText>
<sectionHeader confidence="0.81242" genericHeader="method">
2 The n-best-derivations problem
</sectionHeader>
<bodyText confidence="0.743754">
In this section, we state the n-best-derivations
problem formally, and we give a comprehensive
</bodyText>
<page confidence="0.998955">
47
</page>
<bodyText confidence="0.953886485714286">
example. First, we introduce some basic notions.
Trees and hypergraphs The definition of
ranked trees commonly used in formal tree lan-
guage theory will serve us as the basis for defining
derivations.
A ranked alphabet is a finite set E (of symbols)
where every symbol carries a rank (a nonnegative
integer). By E(k) we denote the set of those sym-
bols having rank k. The set of trees over E, de-
noted by TE, is the smallest set T such that for
every k ∈ N, σ ∈ E(k), and ξ1, ... , ξk ∈ T,
also σ(ξ1, ... , ξk) ∈ T;2 for σ ∈ E(0) we ab-
breviate σ() by σ. For every k ∈ N, σ ∈
E(k) and subsets T1, ... , Tk ⊆ TE we define
the top-concatenation (with σ) σ(T1, ... , Tk) =
{σ(ξ1,...,ξk)  |ξ1 ∈ T1,...,ξk ∈ Tk}.
A E-hypergraph is a pair H = (V, E) where
V is a finite set (of vertices or nodes) and E ⊆
V ∗ ×E×V is a finite set (of hyperedges) such that
for every (v1 ... vk, σ, v) ∈ E we have that σ ∈
E(k).3 We interpret E as a ranked alphabet where
the rank of each edge is carried over from its label
in E. The family (Hv  |v ∈ V ) of derivations of H
is the smallest family (Pv  |v ∈ V ) of subsets
of TE such that e(Pv1, ... , Pvk) ⊆ Pv for every
e = (v1 ... vk, σ, v) ∈ E.
A E-hypergraph (V, E) is cyclic if there
are hyperedges (v11 ... v1k1, σ1, v1), ... ,
(vl1 ... vl k�, σl, vl) ∈ E such that vj−1 occurs
in vj1 ... vjk, for every j ∈ {2,... , l} and vl occurs
in v11 ... v1k1. It is called acyclic if it is not cyclic.
Example 1 Consider the ranked alphabet E =
E(0)∪E(1)∪E(2) with E(0) = {α, β}, E(1) = {γ},
and E(2) = {σ}, and the E-hypergraph H =
(V, E) where
</bodyText>
<listItem confidence="0.996959666666667">
• V={0,1}and
• E = {(ε, α, 1), (ε, β, 1), (1, γ, 1), (11, σ, 0),
(1, γ, 0)}.
</listItem>
<bodyText confidence="0.9989525">
A graphical representation of this hypergraph is
shown in Fig. 1. Note that this hypergraph is cyclic
because of the edge (1, γ, 1).
We indicate the derivations of H, assuming that
e1, ...,e5 are the edges in E in the order given
above:
</bodyText>
<footnote confidence="0.860217833333333">
2The term v(6, ... , �k) is usually understood as a string
composed of the symbol v, an opening parenthesis, the
string �1, a comma, and so on.
3The hypergraphs defined here are essentially nondeter-
ministic tree automata, where V is the set of states and E is
the set of transitions.
</footnote>
<figureCaption confidence="0.997548">
Figure 1: Hypergraph of Example 1.
</figureCaption>
<listItem confidence="0.825279">
• H1 = {e1, e2, e3(e1), e3(e2), e3(e3(e1)),... }
and
• H0 = e4(H1, H1) ∪ e5(H1) where, e. g.,
e4(H1, H1) is the top-concatenation of H1,
H1 with e4, and thus
</listItem>
<equation confidence="0.998937">
e4(H1,H1) = {e4(e1, e1), e4(e1, e2),
e4(e1, e3(e1)), e4(e3(e1), e1),... } .
</equation>
<bodyText confidence="0.99984185">
Next we give an example of ambiguity in hyper-
graphs with labeled hyperedges. Suppose that E
contains an additional hyperedge e6 = (0, γ, 0).
Then H0 would contain the derivations e6(e5(e1))
and e5(e3(e1)), which describe the same E-tree,
viz. γ(γ(α)) (obtained by the node-wise projec-
tion to the second component). ❑
In the sequel, let H = (V, E) be a E-hypergraph.
Ordering Usually an ordering is induced on the
set of derivations by means of probabilities or,
more generally, weights. In the following, we will
abstract from the weights by using a binary rela-
tion directly on derivations, where we will in-
terpret the fact ξ1 ξ2 as “ξ1 is better than or
equal toξ2”.
Example 2 (Ex. 1 contd.) First we show how an
ordering is induced on derivations by means of
weights. To this end, we associate an operation
over the set R of reals with every hyperedge (re-
specting its arity) by means of a mapping θ:
</bodyText>
<equation confidence="0.999961">
θ(e1)() = 4 θ(e2)() = 3
θ(e3)(x1) = x1 + 1 θ(e4)(x1,x2) = x1 + x2
θ(e5)(x1) = x1 + 0.5
</equation>
<bodyText confidence="0.946614444444444">
The weight h(ξ) of a tree ξ ∈ TE is obtained by
interpreting the symbols at each node using θ, e. g.
h(e3(e2)) = θ(e3)(θ(e2)()) = θ(e2)() + 1 = 4.
Then the natural order ≤ on R induces the bi-
nary relation over TE as follows: for every
ξ1, ξ2 ∈ TE we let ξ1 ξ2 iff h(ξ1) ≤ h(ξ2),
meaning that trees with smaller weights are con-
sidered better. (This is, e. g., the case when calcu-
lating probabilites in the image of − log x.) Note
</bodyText>
<figure confidence="0.989145363636364">
e5
e4
0 1
γ
σ
α
β
γ
e2
e1
e3
</figure>
<page confidence="0.997226">
48
</page>
<bodyText confidence="0.993362121212121">
that we could just as well have defined - with the
inverted order.
Since addition is commutative, we obtain
for every ξ1, ξ2 E TE that h(e4(ξ1, ξ2)) =
h(e4(ξ2, ξ1)) and thus e4(ξ1, ξ2) - e4(ξ2, ξ1) and
vice versa. Thus, for two different trees (e4(ξ1, ξ2)
and e4(ξ2, ξ1)) having the same weight, - should
not prefer any of them. That is, - need not be
antisymmetric.
As another example, the mapping θ could as-
sign to each symbol an operation over real-valued
vectors, where each component represents one
feature of a log-linear model such as frequencies,
probabilities, reals, etc. Then the ordering could
be defined by means of a linear combination of the
feature weights. ❑
We use the concept of a linear pre-order to cap-
ture the orderings which are obtained this way.
Let S be a set. A pre-order (on S) is a binary
relation - C_ S x S such that (i) s - s for ev-
ery s E S (reflexivity) and (ii) s1 - s2 and s2 - s3
implies s1 - s3 for every s1, s2, s3 E S (transi-
tivity). A pre-order - is called linear if s1 - s2
or s2 - s1 for every s1, s2 E S. For instance, the
binary relation - on TE as defined in Ex. 2 is a
linear pre-order.
We will restrict our considerations to a class
of linear pre-orders which admit efficient algo-
rithms. For this, we will always assume a lin-
ear pre-order - with the following two properties
(cf. Knuth (1977)).4
SP (subtree property) For every e(ξ1, ... , ξk) E
TE and i E {1, ... , k} we have ξi -
e(ξ1,... ,ξk).5
CP (compatibility) For every pair e(ξ1, . . . , ξk),
e(ξ.,..., ξk) E TE with ξ1 - ξ� 1, . . . ,
ξk - ξk we have that e(ξ1, . . . , ξk) -
e(ξ�1,... , ξk).
It is easy to verify that the linear pre-order - of
Ex. 2 has the aforementioned properties.
In the sequel, let - be a linear pre-order
on TE fulfilling SP and CP.
4Originally, these properties were called “superiority” and
“monotonicity” because they were viewed as properties of
the weight functions. We use the terms “subtree property”
and “compatibility” respectively, because we view them as
properties of the linear pre-order.
5This strong property is used here for simplicity. It suf-
fices to require that for every v E V and pair���′ E H. we
have � - �′ if � is a subtree of �′.
Before we state the n-best-derivations problem
formally, we define the operation minn, which
maps every subset T of TE to the set of all se-
quences of n best elements of T. To this end, let
T C_ TE and n &lt; IT I. We define minn(T) to be
the set of all sequences (ξ1, ... , ξn) E Tn of pair-
wise distinct elements such that ξ1 - ... - ξn and
for every ξ E T {ξ1, ... , ξkI we have ξn - ξ.
For every n &gt; IT we set minn(T) = minJTJ(T).
In addition, we set min&lt;n(T) = Uni=0 mini(T).
n-best-derivations problem The n-best-
derivations problem amounts to the following.
Given a E-hypergraph H = (V, E), a vertex v E
V , and a linear pre-order - on TE fulfilling
SP and CP,
compute an element of minn(Hv).
</bodyText>
<sectionHeader confidence="0.99957" genericHeader="method">
3 Functional Programming
</sectionHeader>
<bodyText confidence="0.9999731875">
We will describe our main algorithm as a func-
tional program. In essence, such a program is a
system of (recursive) equations that defines sev-
eral functions (as shown in Fig. 2). As a conse-
quence the main computational paradigm for eval-
uating the application (f a) of a function f to an
argument a is to choose an appropriate defining
equation f x = r and then evaluate (f a) to r&apos;
which is obtained from r by substituting every oc-
currence of x by a.
We assume a lazy (and in particular, call-by-
need) evaluation strategy, as in the functional pro-
gramming language HASKELL. Roughly speak-
ing, this amounts to evaluating the arguments of
a function only as needed to evaluate the its body
(i. e. for branching). If an argument occurs multi-
ple times in the body, it is evaluated only once.
We use HASKELL notation and functions for
dealing with lists, i. e. we denote the empty list by
[] and list construction by x:xs (where an ele-
ment x is prepended to a list xs), and we use the
functions head (line 01), tail (line 02), and take
(lines 03 and 04), which return the first element in
a list, a list without its first element, and a prefix
of a list, respectively.
In fact, the functions shown in Fig. 2 will be
used in our main algorithm (cf. Fig. 4). Thus,
we explain the functions merge (lines 05–07) and
e(l1, ... ,lk) (lines 08–10) a bit more in detail.
The merge function takes a set L of pairwise
disjoint lists of derivations, each one in ascend-
ing order with respect to -, and merges them into
</bodyText>
<page confidence="0.99615">
49
</page>
<figure confidence="0.9457781875">
-- standard Haskell functions: list deconstructors, take operation
01 head (x:xs) = x
02 tail (x:xs) = xs
03 take n xs = [] if n == 0 or xs == []
04 take n xs = (head xs):take (n-1) (tail xs)
-- merge operation (lists in L should be disjoint)
05 merge L = [] if L \ {[]} = ∅
06 merge L = m:merge ({tail l  |l ∈ L, l != [], head l == m} ∪
{l  |l ∈ L, l != [], head l != m})
07 where m = min{head l  |l ∈ L, l != []}
-- top concatenation
08 e(l1, ... ,lk) = [] if lz == [] for some i ∈ {1,... , k}
09 e(l1, ... ,lk) = e(head l1, ... , head lk):merge {e(lz1, ... ,lzk)  |i ∈ {1, ... , k}}
10 where l� = I lj if j &lt; i
tail lj if j = i
[head lj] if j &gt; i
</figure>
<figureCaption confidence="0.999397">
Figure 2: Some useful functions specified in a functional programming style.
</figureCaption>
<bodyText confidence="0.998026461538462">
one list with the same property (as known from the
merge sort algorithm).
Note that the minimum used in line 07 is based
on the linear pre-order For this reason, it
need not be uniquely determined. However, in an
implementation this function is deterministic, de-
pending on the the data structures.
The function e(l1, ... ,lk) implements the top-
concatenation with e on lists of derivations. It is
defined for every e = (v1 ... vk, Q, v) ∈ E and
takes lists l1, ... , lk of derivations, each in as-
cending order as for merge. The resulting list is
also in ascending order.
</bodyText>
<sectionHeader confidence="0.998366" genericHeader="method">
4 Algorithm
</sectionHeader>
<bodyText confidence="0.977893736842105">
In this section, we develop our algorithm for solv-
ing the n-best-derivations problem. We begin by
motivating our general approach, which amounts
to solving the 1-best-derivation problem first and
then extending that solution to a solution of the n-
best-derivations problem.
It can be shown that for every m ≥ n, the
set mine(Hv) is equal to the set of all prefixes of
length n of elements of minor(Hv). According
to this observation, we will develop a function p
mapping every v ∈ V to a (possibly infinite) list
such that the prefix of length n is in mine(Hv)
for every n. Then, by virtue of lazy evaluation, a
solution to the n-best-derivations problem can be
obtained by evaluating the term
take n (p v)
where take is specified in lines 03–04 of Fig. 2.
Thus, what is left to be done is to specify p appro-
priately.
</bodyText>
<subsectionHeader confidence="0.997596">
4.1 A provisional specification of p
</subsectionHeader>
<bodyText confidence="0.857555666666667">
Consider the following provisional specification
of p:
p v = merge {e(p vl, ... ,p vk) |
</bodyText>
<equation confidence="0.976887">
e = (vl ... vk, Q, v) ∈ E} (†)
</equation>
<bodyText confidence="0.999258">
where the functions merge and e(l1, ... ,lk) are
specified in lines 05–07 and lines 08–10 of Fig. 2,
respectively. This specification models exactly the
trivial equation
</bodyText>
<equation confidence="0.990142">
Hv = U e(Hvl, ... , Hvk)
e=(v1...vk,v,v)∈E
</equation>
<bodyText confidence="0.999449571428571">
for every v ∈ V , where the union and the top-
concatenation have been implemented for lists via
the functions merge and e(l1, ... ,lk).
This specification is adequate if H is acyclic.
For cyclic hypergraphs however, it can not even
solve the 1-best-derivation problem. To illustrate
this, we consider the hypergraph of Ex. 2 and cal-
</bodyText>
<page confidence="0.860835">
50
</page>
<equation confidence="0.8410595">
culate6
take 1 (p 1)
</equation>
<bodyText confidence="0.886899">
= (head (p 1)):take 0 (tail (p 1)) (04)
= head (p 1) (03)
= head (merge {e1(),e2(),e3(p 1)}) (†)
= min{head e1(),head e2(),head e3(p 1)}
</bodyText>
<equation confidence="0.970886666666667">
(01, 06, 07)
= min{head e1(),head e2(), e3(head (p 1))}.
(09)
</equation>
<bodyText confidence="0.999994142857143">
Note that the infinite regress occurs because the
computation of the head element head (p 1) de-
pends on itself. This leads us to the idea of
“pulling” this head element (which is the solu-
tion to the 1-best-derivation problem) “out” of the
merge in (†). Applying this idea to our particular
example, we reach the following equation for p 1:
</bodyText>
<equation confidence="0.83645875">
p 1 = e2: merge {e1(), e3(p 1)}
because e2 is the best derivation in H1. Then, in
order to evaluate merge we have to compute
min{head e1(),head e3(p 1)}
= min{e1, e3(head (p 1))}
= min{e1, e3(e2)}.
Since h(e1) = h(e3(e2)) = 4, we can choose any
of them, say e1, and continue:
e2: merge {e1(),e3(p 1)}
= e2: e1: merge {tail e1(), e3(p 1)}
= e2: e1: e3(e2): merge {tail e3(p 1)}
= ...
</equation>
<bodyText confidence="0.982554625">
Generalizing this example, the function p could
be specified as follows:
p 1 = (b 1) : merge {exp} (††)
where b 1 evaluates the 1-best derivation in H1
and exp “somehow” calculates the next best
derivations. In the following subsection, we elabo-
rate this approach. First, we develop an algorithm
for solving the 1-best-derivation problem.
</bodyText>
<subsectionHeader confidence="0.997192">
4.2 Solving the 1-best-derivation problem
</subsectionHeader>
<bodyText confidence="0.999853">
Using SP and CP, it can be shown that for ev-
ery v E V such that Hv =� 0 there is a mini-
mal derivation in Hv which does not contain any
subderivation in Hv (apart from itself). In other
words, it is not necessary to consider cycles when
solving the 1-best-derivation problem.
</bodyText>
<footnote confidence="0.878566">
6Please note that e1O is an application of the function in
lines 08–10 of Fig. 2 while e1 is a derivation.
</footnote>
<bodyText confidence="0.99975275">
We can exploit this knowledge in a program by
keeping a set U of visited nodes, taking care not to
consider edges which lead us back to those nodes.
Consider the following function:
</bodyText>
<equation confidence="0.9798245">
b v U = min{e(b v1 U&apos;, ... , b vk U&apos;) |
e = (v1 ... vk, σ, v) E E,
</equation>
<bodyText confidence="0.969521083333333">
{v1,...,vk} n U&apos; = 0}
where U&apos; = U U {v}
The argument U is the set of visited nodes. The
term b v 0 evaluates to a minimal element of Hv,
or to min 0 if Hv = 0. The problem of this divide-
and-conquer (or top-down) approach is that man-
aging a separate set U for every recursive call in-
curs a big overhead in the computation.
This overhead can be avoided by using a
dynamic programming (or bottom-up) approach
where each node is visited only once, and nodes
are visited in the order of their respective best
derivations.
To be more precise, we maintain a family (Pv |
v E V ) of already found best derivations (where
Pv E min&lt;1(Hv) and initially empty) and a set C
of candidate derivations, where candidates for all
vertices are considered at the same time. In each
iteration, a minimal candidate with respect to is
selected. This candidate is then declared the best
derivation of its respective node.
The following lemma shows that the bottom-up
approach is sound.
Lemma 3 Let (Pv  |v E V ) be a family such that
</bodyText>
<equation confidence="0.916856">
Pv E min&lt;1(Hv). We define
C = Ue=(v1...vk,v,v)EE, e(Pv1, ... , Pvk) .
Pv=0
</equation>
<bodyText confidence="0.972126764705882">
Then (i) for every ξ E UvEV,Pv=0 Hv there is a
ξ′ E C such that ξ′ ξ, and (ii) for every v E V
and ξ E C n Hv the following implication holds:
if ξ G ξ′ for every ξ′ E C, then ξ E min1(Hv).
An algorithm based on this lemma is shown in
Fig. 3. Its key function iter uses the notion of ac-
cumulating parameters. The parameter q is a map-
ping corresponding to the family (Pv  |v E V ) of
the lemma, i. e., q v = Pv; the parameter c is a
set corresponding to C. We begin in line 01 with
the function q0 mapping every vertex to the empty
list. According to the lemma, the candidates then
consist of the nullary edges.
As long as there are candidates left (line 04),
in a recursive call of iter the parameter q is up-
dated with the newly found pair (v, [ξ]) of ver-
tex v and (list of) best derivation ξ (expressed by
</bodyText>
<page confidence="0.996442">
51
</page>
<bodyText confidence="0.86710725">
Require E-hypergraph H = (V, E), linear pre-
order fulfilling SP and CP.
Ensure b v E min1(Hv) for every v E V
such that if b v == [e( 1, ... , �k)] for some
</bodyText>
<figure confidence="0.7441405">
e = (v1 ... vk, Q, v) E E, then b vi == [ti] for
every i E {1, . . . , k}.
01 b = iter q0 {(e, α, v) E E α E E(0)}
02 q0 v = []
03 iter q 0 = q
04 iter q c = iter (q//(v,[�])) c&apos;
05 where
06 � = min c and E Hv
07 c&apos; = Ue=(v1...vk,v,v)∈E e(q v1,...,q vk)
q v == ❑
</figure>
<figureCaption confidence="0.9840205">
Figure 3: Algorithm solving the 1-best-derivation
problem.
</figureCaption>
<bodyText confidence="0.982904851851852">
q//(v,[�])) and the candidate set is recomputed
accordingly. When the candidate set is exhausted
(line 03), then q is returned.
Correctness and completeness of the algorithm
follow from Statements (ii) and (i) of Lemma 3,
respectively. Now we show termination. In every
iteration a new next best derivation is determined
and the candidate set is recomputed. This set only
contains candidates for vertices v E V such that
q v == []. Hence, after at most V iterations
the candidates must be depleted, and the algorithm
terminates.
We note that the algorithm is very similar to that
of Knuth (1977). However, in contrast to the latter,
(i) it admits Hv = 0 for some v E V and (ii) it
computes some minimal derivation instead of the
weight of some minimal derivation.
Runtime According to the literature, the run-
time of Knuth’s algorithm is in O( E · log V )
(Knuth, 1977). This statement relies on a number
of optimizations which are beyond our scope. We
just sketch two optimizations: (i) the candidate set
can be implemented in a way which admits ob-
taining its minimum in O(log C ), and (ii) for the
computation of candidates, each edge needs to be
considered only once during the whole run of the
algorithm.
</bodyText>
<subsectionHeader confidence="0.999589">
4.3 Solving the n-best-derivations problem
</subsectionHeader>
<bodyText confidence="0.885005190476191">
Being able to solve the 1-best-derivation problem,
we can now refine our specification of p. The re-
fined algorithm is given in Fig. 4; for the func-
tions not given there, please refer to Fig. 3 (func-
tion b) and to Fig. 2 (functions merge, tail, and
the top-concatenation). In particular, line 02 of
Fig. 4 shows the general way of “pulling out” the
head element as it was indicated in Section 4.1 via
an example. We also remark that the definition of
the top-concatenation (lines 08–09 of Fig. 2) cor-
responds to the way in which mult�k was sped up
in Fig. 4 of (Huang and Chiang, 2005).
Theorem 4 The algorithm in Fig. 4 is correct with
respect to its require/ensure specification and it
terminates for every input.
PROOF (SKETCH). We indicate how induction on n
can be used for the proof. If n = 0, then the statement
is trivially true. Let n &gt; 0. If b v == [], then the
statement is trivially true as well. Now we consider the
converse case. To this end, we use the following three
auxiliary statements.
</bodyText>
<figure confidence="0.6255298">
(1) take n (merge {l1,... ,lk}) =
take n (merge {take n l1,...,take n lk}),
(2) take n e(l1, ... ,lk) =
take n e(take n l1, ... ,take n lk),
(3) take n (tail l) = tail (take (n+1) l).
</figure>
<bodyText confidence="0.955125222222222">
Using these statements, line 04 of Fig. 2, and line 02
of Fig. 4, we are able to “pull” the take of take n (p
v) “into” the right-hand side of p v, ultimately yield-
ing terms of the form take n (p vj) in the first line
of the merge application and take (n-1) (p v′j) in
the second one.
Then we can show the following statement by induc-
tion on m (note that the n is still fixed from the outer
induction): for every m E N we have that if the tree
in b v has at most height m, then take n (p v) E
minn(Hv). To this end, we use the following two aux-
iliary statements.
(4) For every sequence of pairwise disjoint sub-
sets P1, ... , Pk C Uv∈V Hv, sequence of nat-
ural numbers n1, . . . , nk E N, and lists l1 E
minn1(P1), . . . ,lk E minnk(Pk) such that
nj &gt; n for every j E {1, ... , k} we have that
take n (merge {l1,...,lk}) E minn(P1U...UPk).
</bodyText>
<listItem confidence="0.61182825">
(5) For every edge e = (v1 ... vk, Q, v) E E, subsets
P1, ... , Pk C Uv∈V Hv, and lists l1 E minn(P1), ... ,
lk E minn(Pk) we have that take n e(l1, ... , lk) E
minn(e(P1, ... , Pk)).
</listItem>
<bodyText confidence="0.738139">
Using these statements, it remains to show that
</bodyText>
<equation confidence="0.7746445">
{e(�1, ... , �k)} o minn−1((e(Hv1, ... , Hvk) �
{e(�1,...,�k)}) U U k)) C
</equation>
<bodyText confidence="0.966089">
e′6=e e′(Hv′ 1,...,Hv′
minn(Hv) where b v = [e(�1, ... , �k)] and o
denotes language concatenation. This can be shown by
using the definition of minn.
Termination of the algorithm now follows from the
fact that every finite prefix of p v is well defined. 0
</bodyText>
<page confidence="0.993837">
52
</page>
<bodyText confidence="0.8515005">
Require Σ-hypergraph H = (V, E), linear pre-order fulfilling SP and CP.
Ensure (take n (p v)) E min,,,(H„) for every v E V and n E N.
01 p v = ❑ ifbv== ��
02 p v = e(ξ1, ... , ξk):merge ({tail e(p v1, ... , p vk)  |e = (v1 ... vk, σ, v) E E} U
{e′(p v′1, ... , p v′k)  |e′ = (v′1 ... v′k, σ′, v) E E, e′ =� e})
if b v == �e(ξ1, . . . , ξk)�
</bodyText>
<figureCaption confidence="0.978938">
Figure 4: Algorithm solving the n-best-derivations problem.
</figureCaption>
<subsectionHeader confidence="0.9567455">
4.4 Implementation, Complexity, and
Experiments
</subsectionHeader>
<bodyText confidence="0.9954868">
We have implemented the algorithm (consisting
of Figs. 3 and 4 and the auxiliary functions of
Fig. 2) in HASKELL. The implementation is
rather straightforward except for the following
three points.
</bodyText>
<listItem confidence="0.679100272727273">
(1) Weights: we assume that is defined by
means of weights (cf. Ex. 2), and that comparing
these weights is in O(1) (which often holds be-
cause of limited precision). Hence, we store with
each derivation its weight so that comparison ac-
cording to is in O(1) as well.
(2) Memoization: we use a memoization tech-
nique to ensure that no derivation occurring in p v
is computed twice.
(3) Merge: the merge operation deserves some
consideration because it is used in a nested fash-
</listItem>
<bodyText confidence="0.998706739130435">
ion, yielding trees of merge applications. This
leads to an undesirable runtime complexity be-
cause these trees need not be balanced. Thus, in-
stead of actually computing the merge in p and in
the top-concatenation, we just return a data struc-
ture describing what should be merged. That data
structure consists of a best element and a list of
lists of derivations to be merged (cf. lines 06 and
09 in Fig. 2). We use a higher-order function to
manage these data structures on a heap, perform-
ing the merge in a nonnested way.
Runtime Here we consider the n-best part of the
algorithm, i. e. we assume the computation of the
mapping b to take constant time. Note however
that due to memoization, b is only computed once.
Then the runtime complexity of our implementa-
tion is in O(|E |+ |V  |· n · log(|E |+ n)). This can
be seen as follows.
By line 02 in Fig. 4, the initial heaps in the
higher-order merge described under (3) have a to-
tal of |E |elements. Building these heaps is thus
in O(|E|). By line 09 in Fig. 2, each newly found
derivation spawns at most as many new candidates
</bodyText>
<table confidence="0.9775705">
n total time [s] time for n-best part [s]
1 8.713 —
25 000 10.832 2.119
50 000 12.815 4.102
100 000 16.542 7.739
200 000 24.216 15.503
</table>
<tableCaption confidence="0.999849">
Table 1: Experimental results
</tableCaption>
<bodyText confidence="0.999866424242424">
on the heap as the maximum rank in Σ. We assume
this to be constant. Moreover, at most n deriva-
tions are computed for each node, that is, at most
|V |·n in total. Hence, the size of the heap of anode
is in O(|E |+n). For each derivation we compute,
we have to pop the minimal element off the heap
(cf. line 07 in Fig. 2), which is in O(log(|E|+n)),
and we have to compute the union of the remaining
heap with the newly spawned candidates, which
has the same complexity.
We give another estimate for the total number
of derivations computed by the algorithm, which
is based on the following observation. When pop-
ping a new derivation ξ off the heap, new next best
candidates are computed. This involves comput-
ing at most as many new derivations as the number
of nodes of ξ, because for each hyperedge occur-
ring in ξ we have to consider the next best alter-
native. Since we pop off at most n elements from
the heap belonging to the target node, we arrive at
the estimate d·n, where d is the size of the biggest
derivation of said node.
A slight improvement of the runtime complex-
ity can be obtained by restricting the heap size to
n best elements, as argued by Huang and Chiang
(2005). This way, they are able to obtain the com-
plexity O(|E |+ d · n · log n).
We have conducted experiments on an Intel
Core Duo 1200 MHz with 2 GB of RAM using
a cyclic hypergraph containing 671 vertices and
12136 edges. The results are shown in Table 1.
This table indicates that the runtime of the n-best
part is roughly linear in n.
</bodyText>
<page confidence="0.998247">
53
</page>
<sectionHeader confidence="0.998337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999344333333334">
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene’s theorem.
Inform. Process. Lett., 24(1):1–4.
Jean Berstel and Christophe Reutenauer. 1982. Recog-
nizable formal power series on trees. Theoret. Com-
put. Sci., 18(2):115–148.
Matthias B¨uchse, Jonathan May, and Heiko Vogler.
2009. Determinization of weighted tree automata
using factorizations. Talk presented at FSMNLP 09
in Pretoria, South Africa.
Zolt´an ´Esik and Werner Kuich. 2003. Formal tree se-
ries. J. Autom. Lang. Comb., 8(2):219–285.
Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9. Springer.
Joshua Goodman. 1999. Semiring parsing. Comp.
Ling., 25(4):573–605.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Parsing ’05: Proceedings of the
Ninth International Workshop on Parsing Technol-
ogy, pages 53–64. ACL.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT, pages
123–134.
Donald E. Knuth. 1977. A Generalization of Dijkstra’s
Algorithm. Inform. Process. Lett., 6(1):1–5, Febru-
ary.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL-IJCNLP ’09, pages 593–601.
ACL.
Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Conf. Parsing Technologies, pages 1–12. ACL.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In Proc. HLT, pages 351–358. ACL.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Comp. Ling., 29(1):135–
143.
Lars Relund Nielsen, Kim Allan Andersen, and
Daniele Pretolani. 2005. Finding the k shortest hy-
perpaths. Comput. Oper. Res., 32(6):1477–1497.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295–302.
Adam Pauls and Dan Klein. 2009. k-best a* parsing.
In Proc. ACL-IJCNLP ’09, pages 958–966, Morris-
town, NJ, USA. ACL.
J. W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization
of finite automata theory. J. Comput. Syst. Sci.,
1(4):317–322.
</reference>
<page confidence="0.999009">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885707">
<title confidence="0.999702">Parsing</title>
<author confidence="0.966828">B¨uchse Geisler St¨uber</author>
<affiliation confidence="0.9758265">Faculty of Computer Technische Universit¨at</affiliation>
<address confidence="0.953639">01062</address>
<email confidence="0.996113">buechse@tcs.inf.tu-dresden.de</email>
<email confidence="0.996113">geisler@tcs.inf.tu-dresden.de</email>
<email confidence="0.996113">stueber@tcs.inf.tu-dresden.de</email>
<email confidence="0.996113">vogler@tcs.inf.tu-dresden.de</email>
<abstract confidence="0.999557666666667">We derive and implement an algorithm similar to (Huang and Chiang, 2005) for the derivations in a weighted hypergraph. We prove the correctness and termination of the algorithm and we show experimental results concerning its runtime. Our work is different from the aforementioned one in the following respects: we consider labeled hypergraphs, allowing for tree-based language models (Maletti and Satta, 2009); we specifically handle the case of cyclic hypergraphs; we admit structured weight domains, allowing for multiple features to be processed; we use the paradigm of functional programming together with lazy evaluation, achieving concise algorithmic descriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Athanasios Alexandrakis</author>
<author>Symeon Bozapalidis</author>
</authors>
<title>Weighted grammars and Kleene’s theorem.</title>
<date>1987</date>
<journal>Inform. Process. Lett.,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="4552" citStr="Alexandrakis and Bozapalidis, 1987" startWordPosition="709" endWordPosition="712">ges 46–54, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics In this case, nodes correspond to nonterminals, hyperedges are labeled with productions, and the derivations are exactly the abstract syntax trees (ASTs) of the grammar (which are closely related the parse trees). Note that, unless the pcfg is unambiguous, a given word may have several corresponding ASTs, and its weight is obtained by summing over the weights of the ASTs. Hence, the n best derivations need not coincide with the n best words (cf. application (2) above). (II) weighted tree automata (wta) (Alexandrakis and Bozapalidis, 1987; Berstel and Reutenauer, 1982; ´Esik and Kuich, 2003; F¨ul¨op and Vogler, 2009). These automata serve both as a tree-based language model and as a data structure for the parse forests obtained from that language model by applying the Bar-Hillel construction (Maletti and Satta, 2009). It is well known that context-free grammars and tree automata are weakly equivalent (Thatcher, 1967; ´Esik and Kuich, 2003). However, unlike the former formalism, the latter one has the ability to model non-local dependencies in parse trees. In the case of wta, nodes correspond to states, hyperedges are labeled w</context>
</contexts>
<marker>Alexandrakis, Bozapalidis, 1987</marker>
<rawString>Athanasios Alexandrakis and Symeon Bozapalidis. 1987. Weighted grammars and Kleene’s theorem. Inform. Process. Lett., 24(1):1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Berstel</author>
<author>Christophe Reutenauer</author>
</authors>
<title>Recognizable formal power series on trees.</title>
<date>1982</date>
<journal>Theoret. Comput. Sci.,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="4582" citStr="Berstel and Reutenauer, 1982" startWordPosition="713" endWordPosition="716">2010. c�2010 Association for Computational Linguistics In this case, nodes correspond to nonterminals, hyperedges are labeled with productions, and the derivations are exactly the abstract syntax trees (ASTs) of the grammar (which are closely related the parse trees). Note that, unless the pcfg is unambiguous, a given word may have several corresponding ASTs, and its weight is obtained by summing over the weights of the ASTs. Hence, the n best derivations need not coincide with the n best words (cf. application (2) above). (II) weighted tree automata (wta) (Alexandrakis and Bozapalidis, 1987; Berstel and Reutenauer, 1982; ´Esik and Kuich, 2003; F¨ul¨op and Vogler, 2009). These automata serve both as a tree-based language model and as a data structure for the parse forests obtained from that language model by applying the Bar-Hillel construction (Maletti and Satta, 2009). It is well known that context-free grammars and tree automata are weakly equivalent (Thatcher, 1967; ´Esik and Kuich, 2003). However, unlike the former formalism, the latter one has the ability to model non-local dependencies in parse trees. In the case of wta, nodes correspond to states, hyperedges are labeled with input symbols, and the der</context>
</contexts>
<marker>Berstel, Reutenauer, 1982</marker>
<rawString>Jean Berstel and Christophe Reutenauer. 1982. Recognizable formal power series on trees. Theoret. Comput. Sci., 18(2):115–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias B¨uchse</author>
<author>Jonathan May</author>
<author>Heiko Vogler</author>
</authors>
<title>Determinization of weighted tree automata using factorizations. Talk presented at FSMNLP 09 in</title>
<date>2009</date>
<location>Pretoria, South Africa.</location>
<marker>B¨uchse, May, Vogler, 2009</marker>
<rawString>Matthias B¨uchse, Jonathan May, and Heiko Vogler. 2009. Determinization of weighted tree automata using factorizations. Talk presented at FSMNLP 09 in Pretoria, South Africa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an ´Esik</author>
<author>Werner Kuich</author>
</authors>
<title>Formal tree series.</title>
<date>2003</date>
<journal>J. Autom. Lang. Comb.,</journal>
<volume>8</volume>
<issue>2</issue>
<marker>´Esik, Kuich, 2003</marker>
<rawString>Zolt´an ´Esik and Werner Kuich. 2003. Formal tree series. J. Autom. Lang. Comb., 8(2):219–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an F¨ul¨op</author>
<author>Heiko Vogler</author>
</authors>
<title>Weighted tree automata and tree transducers.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, chapter 9.</booktitle>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer.</publisher>
<marker>F¨ul¨op, Vogler, 2009</marker>
<rawString>Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree automata and tree transducers. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 9. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Comp. Ling.,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2826" citStr="Goodman, 1999" startWordPosition="432" endWordPosition="433">r processing (Li et al., 2009). As an example, consider context-free grammars, where several leftmost derivations may exist for the same terminal string. The weight of the terminal string is obtained by summing over these derivations. The n best leftmost derivations may be used to approximate this sum. In this paper, we consider the case where the finite, compact representation has the form of a weighted hypergraph (with labeled hyperedges) and the analyses are derivations of the hypergraph. This covers many parsing applications (Klein and Manning, 2001), including weighted deductive systems (Goodman, 1999; Nederhof, 2003), and also applications in machine translation (May and Knight, 2006). In the nomenclature of (Huang and Chiang, 2005), which we adopt here, a derivation of a hypergraph is a tree which is obtained in the following way. Starting from some node, an ingoing hyperedge is picked and recorded as the label of the root of the tree. Then, for the subtrees, one continues with the source nodes of said hyperedge in the same way. In other words, a derivation can be understood as an unfolding of the hypergraph. The n-best-derivations problem then amounts to finding n derivations which are </context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Comp. Ling., 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better kbest parsing.</title>
<date>2005</date>
<booktitle>In Parsing ’05: Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>53--64</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2961" citStr="Huang and Chiang, 2005" startWordPosition="450" endWordPosition="453">or the same terminal string. The weight of the terminal string is obtained by summing over these derivations. The n best leftmost derivations may be used to approximate this sum. In this paper, we consider the case where the finite, compact representation has the form of a weighted hypergraph (with labeled hyperedges) and the analyses are derivations of the hypergraph. This covers many parsing applications (Klein and Manning, 2001), including weighted deductive systems (Goodman, 1999; Nederhof, 2003), and also applications in machine translation (May and Knight, 2006). In the nomenclature of (Huang and Chiang, 2005), which we adopt here, a derivation of a hypergraph is a tree which is obtained in the following way. Starting from some node, an ingoing hyperedge is picked and recorded as the label of the root of the tree. Then, for the subtrees, one continues with the source nodes of said hyperedge in the same way. In other words, a derivation can be understood as an unfolding of the hypergraph. The n-best-derivations problem then amounts to finding n derivations which are best with respect to the weights induced by the weighted hypergraph.1 Among others, weighted hypergraphs with labeled hyperedges subsum</context>
<context position="6363" citStr="Huang and Chiang (2005)" startWordPosition="1002" endWordPosition="1005">solves the 1-bestderivation problem for acyclic hypergraphs. It is based on a topological sort of the hypergraph. • Knuth (1977) generalizes Dijkstra’s algorithm (for finding the single-source shortest paths in a graph) to hypergraphs, thus solving the case n = 1 even if the hypergraph contains cycles. Knuth assumes the weights to be real numbers, and he requires weight functions to be monotone and superior in order to guarantee that a best derivation exists. (The superiority property corresponds to Dijkstra’s requirement that edge weights—or, more generally, cycle weights—are nonnegative.) • Huang and Chiang (2005) show that the nbest-derivations problem can be solved efficiently by first solving the 1-best-derivation problem and then extending that solution in a lazy manner. Huang and Chiang assume weighted unlabeled hypergraphs with weights computed in the reals, and they require the weight functions to be monotone. Moreover they assume that the 1-bestderivation problem be solved using the Viterbi algorithm, which implies that the hypergraph must be acyclic. However they conjecture that their second phase also works for cyclic hypergraphs. • Pauls and Klein (2009) propose a variation of the algorithm </context>
<context position="8152" citStr="Huang and Chiang (2005)" startWordPosition="1292" endWordPosition="1296">ht functions the same requirements as Knuth and use his algorithm; 3. by using the concept of linear pre-orders (and not only linear orders on the set of reals) our approach can handle structured weights such as vectors over frequencies, probabilities, and reals; 4. we present our algorithm in the framework of functional programming (and not in that of imperative programming); this framework allows to decribe algorithms in a more abstract and concise, yet natural way; 5. due to the lazy evaluation paradigm often found in functional programming, we obtain the laziness on which the algorithm of Huang and Chiang (2005) is based for free; 6. exploiting the abstract level of description (see point 4) we are able to prove the correctness and termination of our algorithm. At the end of this paper, we will discuss experiments which have been performed with an implementation of our algorithm in the functional programming language HASKELL. 2 The n-best-derivations problem In this section, we state the n-best-derivations problem formally, and we give a comprehensive 47 example. First, we introduce some basic notions. Trees and hypergraphs The definition of ranked trees commonly used in formal tree language theory w</context>
<context position="25881" citStr="Huang and Chiang, 2005" startWordPosition="4774" endWordPosition="4777">Solving the n-best-derivations problem Being able to solve the 1-best-derivation problem, we can now refine our specification of p. The refined algorithm is given in Fig. 4; for the functions not given there, please refer to Fig. 3 (function b) and to Fig. 2 (functions merge, tail, and the top-concatenation). In particular, line 02 of Fig. 4 shows the general way of “pulling out” the head element as it was indicated in Section 4.1 via an example. We also remark that the definition of the top-concatenation (lines 08–09 of Fig. 2) corresponds to the way in which mult�k was sped up in Fig. 4 of (Huang and Chiang, 2005). Theorem 4 The algorithm in Fig. 4 is correct with respect to its require/ensure specification and it terminates for every input. PROOF (SKETCH). We indicate how induction on n can be used for the proof. If n = 0, then the statement is trivially true. Let n &gt; 0. If b v == [], then the statement is trivially true as well. Now we consider the converse case. To this end, we use the following three auxiliary statements. (1) take n (merge {l1,... ,lk}) = take n (merge {take n l1,...,take n lk}), (2) take n e(l1, ... ,lk) = take n e(take n l1, ... ,take n lk), (3) take n (tail l) = tail (take (n+1)</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better kbest parsing. In Parsing ’05: Proceedings of the Ninth International Workshop on Parsing Technology, pages 53–64. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>123--134</pages>
<contexts>
<context position="2773" citStr="Klein and Manning, 2001" startWordPosition="424" endWordPosition="427">models produce analyses which may be too fine-grained for further processing (Li et al., 2009). As an example, consider context-free grammars, where several leftmost derivations may exist for the same terminal string. The weight of the terminal string is obtained by summing over these derivations. The n best leftmost derivations may be used to approximate this sum. In this paper, we consider the case where the finite, compact representation has the form of a weighted hypergraph (with labeled hyperedges) and the analyses are derivations of the hypergraph. This covers many parsing applications (Klein and Manning, 2001), including weighted deductive systems (Goodman, 1999; Nederhof, 2003), and also applications in machine translation (May and Knight, 2006). In the nomenclature of (Huang and Chiang, 2005), which we adopt here, a derivation of a hypergraph is a tree which is obtained in the following way. Starting from some node, an ingoing hyperedge is picked and recorded as the label of the root of the tree. Then, for the subtrees, one continues with the source nodes of said hyperedge in the same way. In other words, a derivation can be understood as an unfolding of the hypergraph. The n-best-derivations pro</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings of IWPT, pages 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<date>1977</date>
<journal>A Generalization of Dijkstra’s Algorithm. Inform. Process. Lett.,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="5868" citStr="Knuth (1977)" startWordPosition="925" endWordPosition="926"> given tree may have several accepting runs, the n best derivations need not coincide with the n best trees. As for the pcfgs, this is an example of spurious ambiguity, which can be tackled as indicated by application (2) above. Alternatively, one can attempt to find an equivalent deterministic wta (May and Knight, 2006; B¨uchse et al., 2009). Next, we briefly discuss four known algorithms which solve the n-best-derivations problem or subproblems thereof. • The Viterbi algorithm solves the 1-bestderivation problem for acyclic hypergraphs. It is based on a topological sort of the hypergraph. • Knuth (1977) generalizes Dijkstra’s algorithm (for finding the single-source shortest paths in a graph) to hypergraphs, thus solving the case n = 1 even if the hypergraph contains cycles. Knuth assumes the weights to be real numbers, and he requires weight functions to be monotone and superior in order to guarantee that a best derivation exists. (The superiority property corresponds to Dijkstra’s requirement that edge weights—or, more generally, cycle weights—are nonnegative.) • Huang and Chiang (2005) show that the nbest-derivations problem can be solved efficiently by first solving the 1-best-derivation</context>
<context position="13825" citStr="Knuth (1977)" startWordPosition="2425" endWordPosition="2426">the orderings which are obtained this way. Let S be a set. A pre-order (on S) is a binary relation - C_ S x S such that (i) s - s for every s E S (reflexivity) and (ii) s1 - s2 and s2 - s3 implies s1 - s3 for every s1, s2, s3 E S (transitivity). A pre-order - is called linear if s1 - s2 or s2 - s1 for every s1, s2 E S. For instance, the binary relation - on TE as defined in Ex. 2 is a linear pre-order. We will restrict our considerations to a class of linear pre-orders which admit efficient algorithms. For this, we will always assume a linear pre-order - with the following two properties (cf. Knuth (1977)).4 SP (subtree property) For every e(ξ1, ... , ξk) E TE and i E {1, ... , k} we have ξi - e(ξ1,... ,ξk).5 CP (compatibility) For every pair e(ξ1, . . . , ξk), e(ξ.,..., ξk) E TE with ξ1 - ξ� 1, . . . , ξk - ξk we have that e(ξ1, . . . , ξk) - e(ξ�1,... , ξk). It is easy to verify that the linear pre-order - of Ex. 2 has the aforementioned properties. In the sequel, let - be a linear pre-order on TE fulfilling SP and CP. 4Originally, these properties were called “superiority” and “monotonicity” because they were viewed as properties of the weight functions. We use the terms “subtree property” </context>
<context position="24641" citStr="Knuth (1977)" startWordPosition="4549" endWordPosition="4550">ivation problem. q//(v,[�])) and the candidate set is recomputed accordingly. When the candidate set is exhausted (line 03), then q is returned. Correctness and completeness of the algorithm follow from Statements (ii) and (i) of Lemma 3, respectively. Now we show termination. In every iteration a new next best derivation is determined and the candidate set is recomputed. This set only contains candidates for vertices v E V such that q v == []. Hence, after at most V iterations the candidates must be depleted, and the algorithm terminates. We note that the algorithm is very similar to that of Knuth (1977). However, in contrast to the latter, (i) it admits Hv = 0 for some v E V and (ii) it computes some minimal derivation instead of the weight of some minimal derivation. Runtime According to the literature, the runtime of Knuth’s algorithm is in O( E · log V ) (Knuth, 1977). This statement relies on a number of optimizations which are beyond our scope. We just sketch two optimizations: (i) the candidate set can be implemented in a way which admits obtaining its minimum in O(log C ), and (ii) for the computation of candidates, each edge needs to be considered only once during the whole run of th</context>
</contexts>
<marker>Knuth, 1977</marker>
<rawString>Donald E. Knuth. 1977. A Generalization of Dijkstra’s Algorithm. Inform. Process. Lett., 6(1):1–5, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP ’09,</booktitle>
<pages>593--601</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2243" citStr="Li et al., 2009" startWordPosition="339" endWordPosition="342">proximation, consisting of n best analyses, i. e. n analyses with highest probability. For example, this approach has the following two applications. (1) Reranking: when log-linear models (Och and Ney, 2002) are employed, some features may ∗ This research was financially supported by DFG VO 1101/5-1. not permit an efficient evaluation during the computation of the analyses. These features are computed using individual analyses from said approximation, leading to a reranking amongst them. (2) Spurious ambiguity: many models produce analyses which may be too fine-grained for further processing (Li et al., 2009). As an example, consider context-free grammars, where several leftmost derivations may exist for the same terminal string. The weight of the terminal string is obtained by summing over these derivations. The n best leftmost derivations may be used to approximate this sum. In this paper, we consider the case where the finite, compact representation has the form of a weighted hypergraph (with labeled hyperedges) and the analyses are derivations of the hypergraph. This covers many parsing applications (Klein and Manning, 2001), including weighted deductive systems (Goodman, 1999; Nederhof, 2003)</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Variational decoding for statistical machine translation. In Proc. ACL-IJCNLP ’09, pages 593–601. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
<author>Giorgio Satta</author>
</authors>
<title>Parsing algorithms based on tree automata.</title>
<date>2009</date>
<booktitle>In Proc. 11th Int. Conf. Parsing Technologies,</booktitle>
<pages>1--12</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="661" citStr="Maletti and Satta, 2009" startWordPosition="87" endWordPosition="90">uchse and Daniel Geisler and Torsten St¨uber and Heiko Vogler Faculty of Computer Science Technische Universit¨at Dresden 01062 Dresden {buechse,geisler,stueber,vogler}@tcs.inf.tu-dresden.de Abstract We derive and implement an algorithm similar to (Huang and Chiang, 2005) for finding the n best derivations in a weighted hypergraph. We prove the correctness and termination of the algorithm and we show experimental results concerning its runtime. Our work is different from the aforementioned one in the following respects: we consider labeled hypergraphs, allowing for tree-based language models (Maletti and Satta, 2009); we specifically handle the case of cyclic hypergraphs; we admit structured weight domains, allowing for multiple features to be processed; we use the paradigm of functional programming together with lazy evaluation, achieving concise algorithmic descriptions. 1 Introduction In statistical natural language processing, probabilistic models play an important role which can be used to assign to some input sentence a set of analyses, each carrying a probability. For instance, an analysis can be a parse tree or a possible translation. Due to the ambiguity of natural language, the number of analyse</context>
<context position="4836" citStr="Maletti and Satta, 2009" startWordPosition="753" endWordPosition="756">se trees). Note that, unless the pcfg is unambiguous, a given word may have several corresponding ASTs, and its weight is obtained by summing over the weights of the ASTs. Hence, the n best derivations need not coincide with the n best words (cf. application (2) above). (II) weighted tree automata (wta) (Alexandrakis and Bozapalidis, 1987; Berstel and Reutenauer, 1982; ´Esik and Kuich, 2003; F¨ul¨op and Vogler, 2009). These automata serve both as a tree-based language model and as a data structure for the parse forests obtained from that language model by applying the Bar-Hillel construction (Maletti and Satta, 2009). It is well known that context-free grammars and tree automata are weakly equivalent (Thatcher, 1967; ´Esik and Kuich, 2003). However, unlike the former formalism, the latter one has the ability to model non-local dependencies in parse trees. In the case of wta, nodes correspond to states, hyperedges are labeled with input symbols, and the derivations are exactly the runs of the automaton. Since, due to ambiguity, a given tree may have several accepting runs, the n best derivations need not coincide with the n best trees. As for the pcfgs, this is an example of spurious ambiguity, which can b</context>
</contexts>
<marker>Maletti, Satta, 2009</marker>
<rawString>Andreas Maletti and Giorgio Satta. 2009. Parsing algorithms based on tree automata. In Proc. 11th Int. Conf. Parsing Technologies, pages 1–12. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>A better n-best list: practical determinization of weighted finite tree automata.</title>
<date>2006</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>351--358</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2912" citStr="May and Knight, 2006" startWordPosition="442" endWordPosition="445"> where several leftmost derivations may exist for the same terminal string. The weight of the terminal string is obtained by summing over these derivations. The n best leftmost derivations may be used to approximate this sum. In this paper, we consider the case where the finite, compact representation has the form of a weighted hypergraph (with labeled hyperedges) and the analyses are derivations of the hypergraph. This covers many parsing applications (Klein and Manning, 2001), including weighted deductive systems (Goodman, 1999; Nederhof, 2003), and also applications in machine translation (May and Knight, 2006). In the nomenclature of (Huang and Chiang, 2005), which we adopt here, a derivation of a hypergraph is a tree which is obtained in the following way. Starting from some node, an ingoing hyperedge is picked and recorded as the label of the root of the tree. Then, for the subtrees, one continues with the source nodes of said hyperedge in the same way. In other words, a derivation can be understood as an unfolding of the hypergraph. The n-best-derivations problem then amounts to finding n derivations which are best with respect to the weights induced by the weighted hypergraph.1 Among others, we</context>
<context position="5577" citStr="May and Knight, 2006" startWordPosition="879" endWordPosition="882">003). However, unlike the former formalism, the latter one has the ability to model non-local dependencies in parse trees. In the case of wta, nodes correspond to states, hyperedges are labeled with input symbols, and the derivations are exactly the runs of the automaton. Since, due to ambiguity, a given tree may have several accepting runs, the n best derivations need not coincide with the n best trees. As for the pcfgs, this is an example of spurious ambiguity, which can be tackled as indicated by application (2) above. Alternatively, one can attempt to find an equivalent deterministic wta (May and Knight, 2006; B¨uchse et al., 2009). Next, we briefly discuss four known algorithms which solve the n-best-derivations problem or subproblems thereof. • The Viterbi algorithm solves the 1-bestderivation problem for acyclic hypergraphs. It is based on a topological sort of the hypergraph. • Knuth (1977) generalizes Dijkstra’s algorithm (for finding the single-source shortest paths in a graph) to hypergraphs, thus solving the case n = 1 even if the hypergraph contains cycles. Knuth assumes the weights to be real numbers, and he requires weight functions to be monotone and superior in order to guarantee that</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. A better n-best list: practical determinization of weighted finite tree automata. In Proc. HLT, pages 351–358. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Comp. Ling.,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>143</pages>
<contexts>
<context position="2843" citStr="Nederhof, 2003" startWordPosition="434" endWordPosition="435">i et al., 2009). As an example, consider context-free grammars, where several leftmost derivations may exist for the same terminal string. The weight of the terminal string is obtained by summing over these derivations. The n best leftmost derivations may be used to approximate this sum. In this paper, we consider the case where the finite, compact representation has the form of a weighted hypergraph (with labeled hyperedges) and the analyses are derivations of the hypergraph. This covers many parsing applications (Klein and Manning, 2001), including weighted deductive systems (Goodman, 1999; Nederhof, 2003), and also applications in machine translation (May and Knight, 2006). In the nomenclature of (Huang and Chiang, 2005), which we adopt here, a derivation of a hypergraph is a tree which is obtained in the following way. Starting from some node, an ingoing hyperedge is picked and recorded as the label of the root of the tree. Then, for the subtrees, one continues with the source nodes of said hyperedge in the same way. In other words, a derivation can be understood as an unfolding of the hypergraph. The n-best-derivations problem then amounts to finding n derivations which are best with respect</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Comp. Ling., 29(1):135– 143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Relund Nielsen</author>
<author>Kim Allan Andersen</author>
<author>Daniele Pretolani</author>
</authors>
<title>Finding the k shortest hyperpaths.</title>
<date>2005</date>
<journal>Comput. Oper. Res.,</journal>
<volume>32</volume>
<issue>6</issue>
<contexts>
<context position="3745" citStr="Nielsen et al. (2005)" startWordPosition="582" endWordPosition="585">ecorded as the label of the root of the tree. Then, for the subtrees, one continues with the source nodes of said hyperedge in the same way. In other words, a derivation can be understood as an unfolding of the hypergraph. The n-best-derivations problem then amounts to finding n derivations which are best with respect to the weights induced by the weighted hypergraph.1 Among others, weighted hypergraphs with labeled hyperedges subsume the following two concepts. (I) probabilistic context-free grammars (pcfgs). 1Note that this problem is different from the n-besthyperpaths problem described by Nielsen et al. (2005), as already argued in (Huang and Chiang, 2005, Section 2). 46 Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 46–54, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics In this case, nodes correspond to nonterminals, hyperedges are labeled with productions, and the derivations are exactly the abstract syntax trees (ASTs) of the grammar (which are closely related the parse trees). Note that, unless the pcfg is unambiguous, a given word may have several corresponding ASTs, and its weight is obtained by</context>
</contexts>
<marker>Nielsen, Andersen, Pretolani, 2005</marker>
<rawString>Lars Relund Nielsen, Kim Allan Andersen, and Daniele Pretolani. 2005. Finding the k shortest hyperpaths. Comput. Oper. Res., 32(6):1477–1497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="1834" citStr="Och and Ney, 2002" startWordPosition="275" endWordPosition="278">ty of natural language, the number of analyses for one input sentence can be very large. Some models even assign an infinite number of analyses to an input sentence. In many cases however, the set of analyses can in fact be represented in a finite and compact way. While such a representation is space-efficient, it may be incompatible with subsequent operations. In these cases a finite subset is used as an approximation, consisting of n best analyses, i. e. n analyses with highest probability. For example, this approach has the following two applications. (1) Reranking: when log-linear models (Och and Ney, 2002) are employed, some features may ∗ This research was financially supported by DFG VO 1101/5-1. not permit an efficient evaluation during the computation of the analyses. These features are computed using individual analyses from said approximation, leading to a reranking amongst them. (2) Spurious ambiguity: many models produce analyses which may be too fine-grained for further processing (Li et al., 2009). As an example, consider context-free grammars, where several leftmost derivations may exist for the same terminal string. The weight of the terminal string is obtained by summing over these</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>k-best a* parsing.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP ’09,</booktitle>
<pages>958--966</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6925" citStr="Pauls and Klein (2009)" startWordPosition="1089" endWordPosition="1092">y, cycle weights—are nonnegative.) • Huang and Chiang (2005) show that the nbest-derivations problem can be solved efficiently by first solving the 1-best-derivation problem and then extending that solution in a lazy manner. Huang and Chiang assume weighted unlabeled hypergraphs with weights computed in the reals, and they require the weight functions to be monotone. Moreover they assume that the 1-bestderivation problem be solved using the Viterbi algorithm, which implies that the hypergraph must be acyclic. However they conjecture that their second phase also works for cyclic hypergraphs. • Pauls and Klein (2009) propose a variation of the algorithm of Huang and Chiang (2005) in which the 1-best-derivation problem is computed via an A∗-based exploration of the 1-best charts. In this paper, we also present an algorithm for solving the n-best-derivations problem. Ultimately it uses the same algorithmic ideas as the one of Huang and Chiang (2005); however, it is different in the following sense: 1. we consider labeled hypergraphs, allowing for wta to be used in parsing; 2. we specifically handle the case of cyclic hypergraphs, thus supporting the conjecture of Huang and Chiang; for this we impose on the </context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>Adam Pauls and Dan Klein. 2009. k-best a* parsing. In Proc. ACL-IJCNLP ’09, pages 958–966, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Thatcher</author>
</authors>
<title>Characterizing derivation trees of context-free grammars through a generalization of finite automata theory.</title>
<date>1967</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="4937" citStr="Thatcher, 1967" startWordPosition="771" endWordPosition="772"> weight is obtained by summing over the weights of the ASTs. Hence, the n best derivations need not coincide with the n best words (cf. application (2) above). (II) weighted tree automata (wta) (Alexandrakis and Bozapalidis, 1987; Berstel and Reutenauer, 1982; ´Esik and Kuich, 2003; F¨ul¨op and Vogler, 2009). These automata serve both as a tree-based language model and as a data structure for the parse forests obtained from that language model by applying the Bar-Hillel construction (Maletti and Satta, 2009). It is well known that context-free grammars and tree automata are weakly equivalent (Thatcher, 1967; ´Esik and Kuich, 2003). However, unlike the former formalism, the latter one has the ability to model non-local dependencies in parse trees. In the case of wta, nodes correspond to states, hyperedges are labeled with input symbols, and the derivations are exactly the runs of the automaton. Since, due to ambiguity, a given tree may have several accepting runs, the n best derivations need not coincide with the n best trees. As for the pcfgs, this is an example of spurious ambiguity, which can be tackled as indicated by application (2) above. Alternatively, one can attempt to find an equivalent</context>
</contexts>
<marker>Thatcher, 1967</marker>
<rawString>J. W. Thatcher. 1967. Characterizing derivation trees of context-free grammars through a generalization of finite automata theory. J. Comput. Syst. Sci., 1(4):317–322.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>