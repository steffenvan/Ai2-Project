<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052786">
<note confidence="0.568561">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 172-179.
Association for Computational Linguistics.
</note>
<bodyText confidence="0.99940452">
least two categories (e.g. &amp;quot;rescuers&amp;quot; Workers
Responding and &amp;quot;victim&amp;quot; Affected People).
However, the predicate structure of the sentence
emphasizes the rescuers, and this particular im-
age was labeled as a member of the Workers
Responding category, although you can also see
wreckage and a victim within the image.
On the other hand, consider an image with
a different caption, reading &amp;quot;A fire victim who
perished in a blaze at a Manila disco is carried
by Philippine rescuers.&amp;quot; This caption suggests a
focus on the victim as opposed to the rescuers,
which implies that the image would be more ap-
propriate for the Affected People category. How-
ever, the words in the caption are nearly identi-
cal. A typical bag of words approach does not
have the capacity to distinguish between this hy-
pothetical image and the example shown; each
word is either present a certain number of times
or it is not, and there is no way to capture pred-
icate structure. For certain tasks involving cate-
gories such as the ones we are dealing with here,
some linguistic analysis is necessary.
No pre-existing system that we tested was
able to perform well on these categories. We
eventually became convinced that the main sub-
ject and verb of the first sentence of the caption
are particularly important in determining the
category of an image.&apos; These words correspond
to the object in the image and to what that
object is doing. For example, the most help-
ful words in the caption of the image shown in
Figure 1 are &amp;quot;rescuers&amp;quot; and &amp;quot;carry&amp;quot;. The other
words are not helpful, and some, such as &amp;quot;vic-
tim&amp;quot;, can even be misleading.
This paper will first describe an experiment
carried out with human volunteers who viewed
captions under varying conditions which we feel
supports our hypothesis that consideration of
syntax is necessary for optimal performance for
our task. It then describes a system we de-
veloped that uses a shallow parser to extract
subjects and verbs automatically, together with
a novel measure of word-to-word similarity, to
place images into our categories. We will show
&apos;Typically, captions contain two or three sentences
with the first sentence describing the image and the rest
giving background information about the related story.
that this system outperforms seven competing
systems which we have tested for this task.
</bodyText>
<sectionHeader confidence="0.809415" genericHeader="method">
2 The Task
</sectionHeader>
<bodyText confidence="0.999806864864865">
The task discussed in this paper arose naturally
in the course of our research, and only after ini-
tial attempts applying standard text categoriza-
tion systems led to poor performance did we be-
gin to consider the use of NLP techniques. The
raw data from our corpus consists of news post-
ings from a variety of Usenet newsgroups over
a three year period, some of which contain an
image with an associated caption. In previous
research, human evaluators labeled those news
documents which contain images into the cat-
egories Disaster, Struggle, Politics, Crime, and
Other. For the experiments discussed in this pa-
per, we started with the 296 images embedded
in Disaster documents. We chose the Disaster
category, approximately defined to cover natu-
ral disasters and accidents, because our previous
system achieves almost perfect precision and re-
call for this category.
We defined four categories to apply to these
images: Workers Responding, Affected People,
Wreckage, and Other. The categories were de-
fined to be mutually exclusive, and for images
that seemed to fit into multiple categories, we
asked human evaluators to choose the best fit
based on the main focus of the image.2 Each
image was categorized by the first author of this
paper and one volunteer who were shown both
the image and the caption, and those with agree-
ment were used for the experiments discussed in
this paper. There was agreement for 248 images.
98 (39.5%) were classified as Workers Respond-
ing, 72 (29.0%) were classified as Affected Peo-
ple, 55 (22.2%) were classified as Wreckage, and
23 (9.3%) were classified as Other. The final
data set was randomly divided into a training
set and a test set, each containing 124 images.
</bodyText>
<sectionHeader confidence="0.992201" genericHeader="method">
3 Initial Experiments
</sectionHeader>
<bodyText confidence="0.9995875">
Our original plan was to use our own classi-
fier, which relies on bins to empirically estimate
</bodyText>
<footnote confidence="0.988878333333333">
2Instructions provided to the evaluators, includ-
ing definitions of our categories, can be seen at http://
www cs. columbia. edur sableiresearchiinstructions.html.
</footnote>
<bodyText confidence="0.999684">
term weights as described in (Sable and Church,
2001), to place images into these categories.
However, we quickly found that the performance
was not adequate. We then tested several alter-
native systems and found that they all had sim-
ilar performance. Table 1 shows the results of
all systems tested. The first six systems in the
table comprise the publicly available Rainbow
package (McCallum, 1996), and the last is our
own bin-based system. The performance of the
systems ranged from 54.0% to 59.7%. Choos-
ing the largest category every time would give
a baseline performance of 39.5%. While all sys-
tems beat the baseline, we did not feel that they
were doing as well as possible.
</bodyText>
<table confidence="0.99855125">
System Performance
Naive Bayes 55.6%
Rocchio/TF*IDF 54.0%
K-Nearest Neighbor 54.0%
Probabilistic Indexing 59.7%
Maximum Entropy 58.1%
Support Vector Machines 54.8%
Bins 56.5%
</table>
<tableCaption confidence="0.999965">
Table 1: The initial results were low for all systems.
</tableCaption>
<bodyText confidence="0.99997165">
In order to decide in which category to place
an image, it is important to determine what is in
the image and what that thing is doing. In the
sample image shown in Figure 1, for example,
we see rescuers carrying, and that focus of the
image places it in the Workers Responding cat-
egory. Words in the caption such as &amp;quot;disco&amp;quot; and
&amp;quot;victim&amp;quot; refer to items in the image which are
indicative of other categories such as Wreckage
and Affected People, but they do not refer to the
focus of the image. We formed the hypothesis
that the main subject and verb of the first sen-
tence of the caption should play a pivotal role in
determining an image&apos;s category; if this is cor-
rect, it is likely that a system relying on deeper
NLP techniques should be able to outperform
typical systems for our task. Typical systems
relying on bag of words approaches can not ac-
count for the predicate argument relationships
in the captions.
</bodyText>
<sectionHeader confidence="0.97406" genericHeader="method">
4 Experiments with Humans
</sectionHeader>
<bodyText confidence="0.999767571428571">
To test our hypothesis, we randomly divided our
data set of 248 images into four equally sized
subsets and recruited four volunteers to view
text associated with our images under four con-
ditions. Each volunteer was a native speaker of
English, and none had any connection to this or
any related research. The four conditions were:
</bodyText>
<listItem confidence="0.994167818181818">
• Sent: The full first sentence of the caption.
• Rand: The words from the first sentence of
the caption in random order.
• IDF: The top two words, not including
proper nouns, from the first sentence of
the caption, according to TF*IDF weights
(Salton and Buckley, 1988; Salton, 1989).
• S-V: The two words, manually extracted,
best representing the main subject and
verb. If the subject was a proper noun, only
the token &amp;quot;NAME&amp;quot; was provided.
</listItem>
<table confidence="0.95232">
Sent Philippine rescuers carry a fire victim
March 19 who perished in a blaze
at a Manila disco.
Rand at perished disco who Manila a a in
19 carry Philippine blaze victim a
rescuers March fire
IDF disco rescuers
S-V subject = &amp;quot;rescuers&amp;quot;, verb = &amp;quot;carry&amp;quot;
</table>
<tableCaption confidence="0.92427525">
Table 2: The subject and verb make it clear that the
category for the sample image is Workers Responding.
Other words such as &amp;quot;disco&amp;quot; and &amp;quot;victim&amp;quot; are not helpful
and can be misleading.
</tableCaption>
<bodyText confidence="0.999296">
Table 2 illustrates the four conditions for the
sample image shown in Figure 1. As was the case
with many images, the subject and verb alone
(&amp;quot;rescuers carry&amp;quot;) are enough to confidently pre-
dict the category of the image. The top two
TF*IDF words might be enough, since &amp;quot;res-
cuers&amp;quot; happened to be one of them, but &amp;quot;disco&amp;quot;
is not helpful. If &amp;quot;victim&amp;quot; had happened to show
up instead of &amp;quot;rescuers&amp;quot;, this condition would
have been misleading. Viewing all the words in
random order is confusing; there are mixed sig-
nals here, and unless you take the time to un-
scramble the words and regain some syntactic
clues, you are forced to guess.
A web interface was set up which allows volun-
teers to predict each categories of images. Each
volunteer was tested with a different condition
for each of the four subsets of our data, and each
subset was presented to our four volunteers with
the four different conditions. In this way, a pre-
diction was recorded for every image under each
condition once, every volunteer was tested under
all conditions, and no volunteer was presented
with the same image twice.
</bodyText>
<table confidence="0.999434333333333">
Volunteer Sent Rand IDF S-V
#1 95.2% 83.9% 50.0% 64.5%
#2 95.2% 75.8% 46.8% 74.2%
#3 83.9% 62.9% 56.5% 64.5%
#4 90.3% 75.8% 61.3% 83.9%
Avg 91.1% 74.6% 53.6% 71.8%
</table>
<tableCaption confidence="0.801744666666667">
Table 3: Subject and verb alone performed almost as
well as all words in random order, and much better than
the top two TF*IDF words.
</tableCaption>
<bodyText confidence="0.997998272727273">
Table 3 shows the performance of each volun-
teer under each condition as well as the overall
performance for each condition. All volunteers
were reasonably consistent. In summary, Sent
&gt;&gt; Rand &gt; S-V &gt;&gt; IDF. That is, (1) more words
(Sent, Rand) are better than fewer words (S-V,
IDF), and (2) NLP helps (Sent is better than
Rand and S-V is better than IDF). The NLP
effect is remarkably strong and almost compen-
sates for the other effect; i.e. Rand is only
slightly better than S-V (for most volunteers).
</bodyText>
<table confidence="0.973902">
Condition Average Time
Rand 68.1
Sent 34.3
IDF 99.7
S-V 20.3
</table>
<tableCaption confidence="0.98612">
Table 4: Volunteers spent the most time making deci-
sions when presented all words in random order.
</tableCaption>
<bodyText confidence="0.99991828">
In addition to measuring performance, our in-
terface also keeps track of how long each de-
cision takes. Table 4 shows the average time
of decisions in seconds under each of the four
conditions. As can be seen, volunteers took the
longest, by far, to make decisions with the Rand
condition. Comparatively, with the S-V condi-
tion, they took less than one third of the time.
Examination of these results led us to the
conclusion that syntax clearly matters for this
task. All volunteers performed much better
when shown the full first sentence with words in
their original order than when the same words
were shown in random order, and the task took
approximately half the time. Therefore, any bag
of words approach is likely limited by a signifi-
cantly lower upper bound than one which uses
NLP techniques. In particular, the main sub-
ject and verb from the sentence were impor-
tant. Given only these two words, volunteers
performed almost as well as when they had all
the words in random order, and much better
than when they were given the top two words
according to TF*IDF weights, a very common
measure of word importance in IR literature.
</bodyText>
<table confidence="0.959618181818182">
5 Using Only Subjects and Verbs
with Standard Systems
System Performance
Sent S-V
Naive Bayes 55.6% 54.8%
Rocchio/TF*IDF 54.0% 54.0%
K-Nearest Neighbor 54.0% 54.8%
Probabilistic Indexing 59.7% 54.0%
Maximum Entropy 58.1% 53.2%
Support Vector Machines 54.8% 54.0%
Bins 56.5% 53.2%
</table>
<tableCaption confidence="0.964055666666667">
Table 5: Systems performed almost as well using single
word subjects and verbs as they did when provided with
the entire first sentence.
</tableCaption>
<bodyText confidence="0.999985733333333">
We next decided to test how the standard text
categorization systems we had previously tested
would fair if only subjects and verbs were pro-
vided. At this point, we were still using man-
ually extracted words. Table 5 shows how the
results using only subjects and verbs compared
to results using the entire first sentence of the
caption (the first column of results is the same
as that from Table 1). As can be seen, the per-
formance was slightly worse for five of the seven
systems, slightly better for one, and the same
for another. As with humans, results were al-
most as high using just two specifically chosen
words as when all words in the sentence (not
accounting for syntax) were used.
</bodyText>
<sectionHeader confidence="0.985977" genericHeader="method">
6 NLP Based System
</sectionHeader>
<bodyText confidence="0.999991066666667">
With the results of our experiment with humans
in mind, we set out to create a fully automatic
text categorization system that takes advantage
of our findings. First, our system tries to ex-
tract the single words best representing the main
subject and verb from the first sentence of each
caption in our training set, and these comprise
lists of subjects and verbs which are representa-
tive of our categories. Next, for each test image,
the subject and verb from its caption are ex-
tracted, and these are compared to those from
the training set using a measure of word-to-word
similarity. A score is generated for every cate-
gory based on these similarities, and the cate-
gory with the highest score is predicted.
</bodyText>
<subsectionHeader confidence="0.998806">
6.1 Extracting Subjects and Verbs
</subsectionHeader>
<bodyText confidence="0.999877722222222">
Subjects and verbs are automatically extracted
using a three step process. First, Church&apos;s
statistical part-of-speech tagger, POS (Church,
1988), assigns a grammatical category to every
word in each caption. Second, the shallow parser
CASS (Abney, 1997) parses each tagged caption.
Third, a final script operates on the output of
CASS, extracting the heads of the appropriate
noun phrase and verb phrase to obtain the sin-
gle words assumed to best represent the subject
and verb of the sentence. (If CASS considers the
head of the noun phrase to be a name, the to-
ken &amp;quot;NAME&amp;quot; is used instead). On our test set,
this process leads to an accuracy of 83.9% for
subjects and 80.6% for verbs, according to our
manually extracted words. WordNet (Fellbaum,
1998) is used to convert each extracted subject
and verb to its morphological base-word.
</bodyText>
<subsectionHeader confidence="0.993019">
6.2 Word Similarity
</subsectionHeader>
<bodyText confidence="0.999980615384616">
In order to compare subjects and verbs ex-
tracted from test captions to those from the
training set, we examined a large &amp;quot;extended&amp;quot;
corpus consisting of thousands of news articles
and captions taken from the same newsgroups
as the images discussed in this paper. Using
the same method of extraction as discussed in
Section 6.1, the single words best representing
the subjects and verbs were extracted from ev-
ery sentence of every article and caption in the
extended corpus. When dealing with text cat-
egorization, the creation of the corpus is gener-
ally one of the most time consuming tasks, since
documents usually need to be manually labeled
for the training set, but for the purposes of word
similarity as we are doing it, this extended cor-
pus is unlabeled and easily obtainable.
Based on these extracted subject/verb pairs,
we defined the similarity between two subjects
to be the percentage of verbs they share in com-
mon, and the similarity between two verbs to
be the percentage of subjects they share in com-
mon. The idea was that two subjects should be
considered similar if they often partake in sim-
ilar actions, and that two verbs should be con-
sidered similar if they represent actions that are
often executed by similar entities. This is not
necessarily a good measure of word similarity
for other tasks, but we thought it might work
well for this domain. For example, let&apos;s say that
the word &amp;quot;fireman&amp;quot; never appears in the training
set of the corpus, but words such as &amp;quot;policeman&amp;quot;
and &amp;quot;volunteer&amp;quot; do, in captions from images be-
longing to the category Workers Responding;
these subjects likely share a higher percentage of
verbs in common than most randomly selected
pairs of subjects, and would therefore have a
relatively high similarity. In addition, for our
current domain, they are representative of the
same category (Workers Responding). By our
definitions, the similarity between any subject
or verb and itself comes out to be one, and the
similarity between any two non-identical words
is generally much less.
We also defined the similarity between a sub-
ject and a verb to be twice the number of times
they appear together divided by the total num-
ber of times each appears. Therefore, if the sub-
ject/verb pair always appears together, the sim-
ilarity between the two words would be one, and
otherwise it would be less. The idea is that sub-
jects which are likely to perform actions seen
as representative of a category should in and of
themselves be considered representative of the
category. The same is true for verbs which rep-
resent actions that are likely to be performed by
subjects that are representative of a category.
For example, let&apos;s say that the word &amp;quot;fireman&amp;quot;
never appears in the training set of the corpus,
but verbs such as &amp;quot;help&amp;quot; and &amp;quot;rescue&amp;quot; do, in
captions from images belonging to the category
Workers Responding. Since a &amp;quot;fireman&amp;quot; is more
likely to &amp;quot;help&amp;quot; and &amp;quot;rescue&amp;quot; than perform other
activities, it will contribute more to the Workers
Responding category than to others.
</bodyText>
<subsectionHeader confidence="0.999162">
6.3 Choosing a Category
</subsectionHeader>
<bodyText confidence="0.999951611111111">
To choose a category for some specified image,
the single word subject and verb from the first
sentence of its caption are extracted, all relevant
similarities are added together for each category,
and the category with the highest score is then
predicted. More formally, let C be the set of
categories, and for some specific category c, let
S, and V, be the set of subjects and verbs ex-
tracted from training instances of c, respectively.
For a particular test image d, let sd and vd be
the single word subject and verb extracted, re-
spectively. For any two words tvi and tv2, re-
gardless of whether they are subjects or verbs,
let Sirou,1,2 be the similarity between the two
words as defined in the previous subsection (any
similarity involving a &amp;quot;NAME&amp;quot; token is defined
to be 0). Let T(c1d) be the total score for a
category c given a test document d. Then:
</bodyText>
<equation confidence="0.9104465">
Sirn,„d,sj
T (cid) = E„Eijsim,„,„ + )
</equation>
<bodyText confidence="0.990226416666667">
For a document d which does not have a
&amp;quot;NAME&amp;quot; token extracted as the subject, the
chosen category is simply:
argroax[T(cld)]
cEe
In order to take &amp;quot;NAME&amp;quot; tokens into account
when they are extracted (this occurs in 16 of the
124 test cases), we decided to multiply the score
for each category by the a-priori probability of
the category, based on the training set, given
that a &amp;quot;NAME&amp;quot; token is extracted. For exam-
ple, in the training set, 56.0% of the &amp;quot;NAME&amp;quot;
tokens come from the Affected People category,
whereas only 33.9% of the training images be-
long to this category overall, so the final score
for the Affect People category is multiplied by
0.56 if a &amp;quot;NAME&amp;quot; token is extracted to account
for the new skew. More formally, let P(N1c)
be the estimated probability of a &amp;quot;NAME&amp;quot; to-
ken given a category, based on the training set.
Then the category chosen for a document d that
has a &amp;quot;NAME&amp;quot; token extracted is:
argroax[T(cld)x P(N1c)]
cEe
</bodyText>
<sectionHeader confidence="0.938744" genericHeader="evaluation">
7 Results and Evaluation
</sectionHeader>
<table confidence="0.9989752">
System Performance
Sent S-V
Naive Bayes 55.6% 54.8%
Rocchio/TF*IDF 54.0% 54.0%
K-Nearest Neighbor 54.0% 54.8%
Probabilistic Indexing 59.7% 54.0%
Maximum Entropy 58.1% 53.2%
Support Vector Machines 54.8% 54.0%
Bins 56.5% 53.2%
NLP Based system 65.3%
</table>
<tableCaption confidence="0.9455425">
Table 6: Our NLP based system outperforms seven
standard systems by a considerable margin.
</tableCaption>
<bodyText confidence="0.9996452">
The final line of Table 6, which is otherwise
the same as Table 5, shows the performance of
our NLP based system described in the previous
section. As can be seen, the system&apos;s accuracy
of 65.3% is at least 10% higher than the seven
standard systems achieved when using only sub-
jects and verbs (manually extracted for the stan-
dard systems), and it is at least 5% higher than
the seven standard systems achieved when given
the entire first sentence of each caption. Looking
back at Section 4, we see that humans given only
the subject and verb of each sentence achieved,
on average, a 71.8% accuracy. We consider this
a reasonable upper bound for the accuracy that
a system such as ours might achieve.
</bodyText>
<sectionHeader confidence="0.996457" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999993315068493">
There has long been a lot of interest in combin-
ing NLP and IR,. Some of the recent work by
various researchers in this area is summarized
in (Strzalkowski et al., 1998) and (Strzalkowski,
1999), and as can be seen, the results have been
mixed, at best. Recently, there has been some
success using NLP to aid in the retrieval of
images. Smeaton and Quigley (1996) showed
some improvement using WordNet to compute
noun to noun similarities which were then used
to compare queries with captions. Elworthy
(2000) showed improvement using an NLP tech-
nique he calls &amp;quot;phrase matching&amp;quot; which first con-
verts queries and captions to &amp;quot;dependency struc-
tures&amp;quot;. In both of these cases, the researchers
manually constructed captions for their images,
and in the case of Smeaton and Quigley, they
manually disambiguated all words.
Working on domain-specific text categoriza-
tion tasks involving full length news articles,
Riloff has created the system AutoSlog-TS
(Riloff and Lorenzen, 1999) which relies on NLP
techniques to fully-automatically create dic-
tionaries of &amp;quot;augmented relevancy signatures&amp;quot;
which can then be used to improve results for
binary text categorization tasks. She found that
her system, which labels a document in a cat-
egory if any augmented relevancy signature as-
sociated with the category is found in the doc-
ument, performs about as well with automat-
ically constructed dictionaries as it does with
hand constructed dictionaries and much better
than when no dictionary is used at all. No com-
parison was made to other standard text cate-
gorization techniques.
With IR tasks such as query expansion and
word sense disambiguation in mind, there have
been previous attempts at measuring word-to-
word similarity. The research discussed in
(Sussna, 1993), (Resnik, 1999), and (Richardson
et al., 1994) concerns using WordNet link struc-
ture to determine semantic similarity between
nouns. Our task also requires us to compute
similarity between verbs with each other and
verbs with nouns, so the techniques discussed in
these papers do not apply. Our approach is sim-
pler, and not necessarily appropriate for general
tasks, but it serves our intended purpose well
and leads to positive results in our experiments.
Other commonly used metrics to measure
word-to-word similarity for use with NLP ap-
plications include the Jaccard Coefficient and
the Dice Coefficient (Radecki, 1982; van Rijs-
bergen, 1979; Smadja et al., 1996). These mea-
sures are related to the ratio of the frequency
with which two words appear together (i.e. near
each other) in text to the frequencies of the two
words independently. While simple and general,
they do not apply well to our specific task and
domain. For example, &amp;quot;rescuers&amp;quot; and &amp;quot;victim&amp;quot;
might often appear together in text, as they do
in the caption of the sample image in Figure 1,
but for our current categorization task, as sub-
jects they would be indicative of two different
categories ( Workers Responding versus Affected
People), and we do not want them to be consid-
ered similar. On the other hand, words such as
&amp;quot;firefighter&amp;quot; and &amp;quot;fireman&amp;quot; may hardly ever ap-
pear together, since an author will likely use one
or the other consistently, but for our task, they
should be considered very similar. Our method
of measuring word-to-word similarity takes these
problems into account.
</bodyText>
<sectionHeader confidence="0.996937" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999992571428571">
We have shown that NLP is important for a
particular text categorization task. We believe
that this importance depends on both the task
and domain. NLP becomes helpful when we are
dealing with tasks that rely on focus, perspec-
tive, point of view, etc. Admittedly, most of
the standard IR test collections are not like this,
and bag of words approaches work well for them.
However, we believe that tasks such as the one
described in this paper, which arose naturally in
the course of our research, will continue to ap-
pear, and when they do, approaches similar to
ours will be useful.
The categories discussed in this paper are not
nominal categories determined by the presence
or absence of any specific object in an image.
These categories deal with predicate argument
relationships that can only be determined using
linguistic analysis. Looking back, once again,
at Figure 1, we see that the subject and verb
of the first sentence of the caption refer to the
object of focus in the image and the action tak-
ing place. The phrase &amp;quot;rescuers carry&amp;quot; is a clear
indication of the Workers Responding category,
whereas other words that might have high IDF
weights, such as &amp;quot;disco&amp;quot; and &amp;quot;victims&amp;quot;, would
not be helpful and may even be misleading to
any system using a bag of words approach. We
have verified the importance of NLP for our task
by presenting evidence from experiments with
human subjects, and we have described a new
NLP based system which considerably outper-
forms seven standard systems. This is a positive
result which shows promise for combining NLP
and IR, in the future, at least for certain tasks.
</bodyText>
<sectionHeader confidence="0.998063" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736246753247">
S. Abney. 1997. The scol manual (version 0.1b).
K. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In Pro-
ceedings of the Second Conference on Applied Nat-
ural Language Processing (ANLP-88).
D. Elworthy. 2000. Retrieval from captioned image
databases using natural language processing. In
Proceedings of the 9th International Conference on
Information Knowledge and Management (CIKM-
00).
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
A. McCallum. 1996. Bow: A toolkit for statis-
tical language modeling, text retrieval, classifi-
cation, and clustering. http://www.cs.cmu.edui
mccallum/bow.
T. Radecki. 1982. Similarity measures for boolean
search request formulations. Journal of the Amer-
ican Society for Information Science, 33(1):8-17.
P. Resnik. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its ap-
plication to problems of ambiguity in natural lan-
guage. Journal of Artificial Intelligence Research,
11:95-130.
R. Richardson, A. F. Smeaton, and J. Murphy. 1994.
Using WordNet as a knowledge base for measur-
ing semantic similarity between words. Technical
Report CA-1294, Dublin City University.
E. Riloff and J. Lorenzen. 1999. Extraction-based
text categorization: Generating domain specific
role relationships automatically. In T. Strza-
lkowski, editor, Natural Language Information Re-
trieval, chapter 7. Kluwer Academic Publishers.
C. Sable and K. W. Church. 2001. Using bins to
empirically estimate term weights for text catego-
rization. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-01).
G. Salton and C. Buckley. 1988. Term weighting ap-
proaches in automatic text retrieval. Information
Processing and Management, 24(5):513-523.
G. Salton. 1989. Automatic &apos;Text Processing: The
Transformation, Analysis, and Retrieval of Infor-
mation by Computer. Addison-Wesley.
F. Z. Smadja, K. McKeown, and V. Hatzivassiloglou.
1996. Translating collocations for bilingual lexi-
cons: A statistical approach. Computational Lin-
guistics, 22 ( 1) : 1-38.
A. F. Smeaton and I. Quigley. 1996. Experi-
ments on using semantic distances between words
in image caption retrieval. In Proceedings of the
19th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR-96).
A. F. Smeaton. 1999. Using NLP or NLP resources
for information retrieval tasks. In T. Strzalkowski,
editor, Natural Language Information Retrieval,
chapter 4. Kluwer Academic Publishers.
T. Strzalkowski, F. Lin, and J. Perez-Carballo. 1998.
Natural language information retrieval: TREC-6
report. In The Sixth Text Retrieval Conference
(TREC-6). NIST Special Publication 500-240.
T. Strzalkowski, editor. 1999. Natural Language In-
formation Retrieval. Kluwer Academic Publish-
ers.
M. Sussna. 1993. Word sense disambiguation for
free-text indexing using a massive semantic net-
work. In Proceedings of the 2nd International
Conference on Information Knowledge and Man-
agement (CIKM-93).
C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, London, 2nd edition.
E. M. Voorhees. 1993. Using WordNet to disam-
biguate word senses for text retrieval. In Proceed-
ings of the Sixteenth Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR-93).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000011">
<note confidence="0.938209666666667">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 172-179. Association for Computational Linguistics.</note>
<abstract confidence="0.997346504587156">two categories (e.g. &amp;quot;rescuers&amp;quot; &amp;quot;victim&amp;quot; People). However, the predicate structure of the sentence emphasizes the rescuers, and this particular imwas labeled as a member of the although you can also see wreckage and a victim within the image. On the other hand, consider an image with different caption, reading fire victim who perished in a blaze at a Manila disco is carried Philippine rescuers.&amp;quot; caption suggests a focus on the victim as opposed to the rescuers, which implies that the image would be more apfor the People However, the words in the caption are nearly identical. A typical bag of words approach does not have the capacity to distinguish between this hypothetical image and the example shown; each word is either present a certain number of times or it is not, and there is no way to capture predicate structure. For certain tasks involving categories such as the ones we are dealing with here, some linguistic analysis is necessary. No pre-existing system that we tested was able to perform well on these categories. We eventually became convinced that the main subject and verb of the first sentence of the caption are particularly important in determining the category of an image.&apos; These words correspond to the object in the image and to what that object is doing. For example, the most helpful words in the caption of the image shown in Figure 1 are &amp;quot;rescuers&amp;quot; and &amp;quot;carry&amp;quot;. The other words are not helpful, and some, such as &amp;quot;victim&amp;quot;, can even be misleading. This paper will first describe an experiment carried out with human volunteers who viewed captions under varying conditions which we feel supports our hypothesis that consideration of syntax is necessary for optimal performance for our task. It then describes a system we developed that uses a shallow parser to extract subjects and verbs automatically, together with a novel measure of word-to-word similarity, to place images into our categories. We will show &apos;Typically, captions contain two or three sentences with the first sentence describing the image and the rest giving background information about the related story. that this system outperforms seven competing systems which we have tested for this task. 2 The Task The task discussed in this paper arose naturally in the course of our research, and only after initial attempts applying standard text categorization systems led to poor performance did we begin to consider the use of NLP techniques. The raw data from our corpus consists of news postings from a variety of Usenet newsgroups over a three year period, some of which contain an image with an associated caption. In previous research, human evaluators labeled those news documents which contain images into the cat- Struggle, Politics, Crime, the experiments discussed in this paper, we started with the 296 images embedded We chose the category, approximately defined to cover natural disasters and accidents, because our previous system achieves almost perfect precision and recall for this category. We defined four categories to apply to these Responding, Affected People, categories were defined to be mutually exclusive, and for images that seemed to fit into multiple categories, we asked human evaluators to choose the best fit on the main focus of the Each image was categorized by the first author of this paper and one volunteer who were shown both the image and the caption, and those with agreement were used for the experiments discussed in this paper. There was agreement for 248 images. (39.5%) were classified as Respond- (29.0%) were classified as Peo- (22.2%) were classified as (9.3%) were classified as final data set was randomly divided into a training set and a test set, each containing 124 images. 3 Initial Experiments Our original plan was to use our own classifier, which relies on bins to empirically estimate provided to the evaluators, including definitions of our categories, can be seen at http:// www cs. columbia. edur sableiresearchiinstructions.html. term weights as described in (Sable and Church, 2001), to place images into these categories. However, we quickly found that the performance was not adequate. We then tested several alternative systems and found that they all had similar performance. Table 1 shows the results of all systems tested. The first six systems in the table comprise the publicly available Rainbow package (McCallum, 1996), and the last is our own bin-based system. The performance of the systems ranged from 54.0% to 59.7%. Choosing the largest category every time would give a baseline performance of 39.5%. While all systems beat the baseline, we did not feel that they were doing as well as possible.</abstract>
<title confidence="0.803344571428571">System Performance Naive Bayes 55.6% Rocchio/TF*IDF 54.0% K-Nearest Neighbor 54.0% Probabilistic Indexing 59.7% Maximum Entropy 58.1% Support Vector Machines 54.8%</title>
<abstract confidence="0.989530112359551">Bins 56.5% 1: initial results were low for all systems. In order to decide in which category to place an image, it is important to determine what is in the image and what that thing is doing. In the sample image shown in Figure 1, for example, see carrying, that focus of the places it in the Responding category. Words in the caption such as &amp;quot;disco&amp;quot; and &amp;quot;victim&amp;quot; refer to items in the image which are of other categories such as People, they do not refer to the focus of the image. We formed the hypothesis that the main subject and verb of the first sentence of the caption should play a pivotal role in determining an image&apos;s category; if this is correct, it is likely that a system relying on deeper NLP techniques should be able to outperform typical systems for our task. Typical systems relying on bag of words approaches can not account for the predicate argument relationships in the captions. 4 Experiments with Humans To test our hypothesis, we randomly divided our data set of 248 images into four equally sized subsets and recruited four volunteers to view text associated with our images under four conditions. Each volunteer was a native speaker of English, and none had any connection to this or any related research. The four conditions were: • Sent: The full first sentence of the caption. • Rand: The words from the first sentence of the caption in random order. • IDF: The top two words, not including proper nouns, from the first sentence of the caption, according to TF*IDF weights (Salton and Buckley, 1988; Salton, 1989). • S-V: The two words, manually extracted, best representing the main subject and verb. If the subject was a proper noun, only the token &amp;quot;NAME&amp;quot; was provided. Sent Philippine rescuers carry a fire victim March 19 who perished in a blaze at a Manila disco. Rand at perished disco who Manila a a in 19 carry Philippine blaze victim a rescuers March fire IDF disco rescuers S-V subject = &amp;quot;rescuers&amp;quot;, verb = &amp;quot;carry&amp;quot; 2: subject and verb make it clear that the for the sample image is Responding. Other words such as &amp;quot;disco&amp;quot; and &amp;quot;victim&amp;quot; are not helpful and can be misleading. Table 2 illustrates the four conditions for the sample image shown in Figure 1. As was the case with many images, the subject and verb alone (&amp;quot;rescuers carry&amp;quot;) are enough to confidently predict the category of the image. The top two TF*IDF words might be enough, since &amp;quot;rescuers&amp;quot; happened to be one of them, but &amp;quot;disco&amp;quot; is not helpful. If &amp;quot;victim&amp;quot; had happened to show up instead of &amp;quot;rescuers&amp;quot;, this condition would have been misleading. Viewing all the words in random order is confusing; there are mixed signals here, and unless you take the time to unscramble the words and regain some syntactic clues, you are forced to guess. A web interface was set up which allows volunteers to predict each categories of images. Each volunteer was tested with a different condition for each of the four subsets of our data, and each subset was presented to our four volunteers with the four different conditions. In this way, a prediction was recorded for every image under each condition once, every volunteer was tested under all conditions, and no volunteer was presented with the same image twice. Volunteer Sent Rand IDF S-V Avg 91.1% 74.6% 53.6% 71.8% 3: and verb alone performed almost as well as all words in random order, and much better than the top two TF*IDF words. Table 3 shows the performance of each volunteer under each condition as well as the overall performance for each condition. All volunteers were reasonably consistent. In summary, Sent &gt;&gt; Rand &gt; S-V &gt;&gt; IDF. That is, (1) more words (Sent, Rand) are better than fewer words (S-V, IDF), and (2) NLP helps (Sent is better than Rand and S-V is better than IDF). The NLP effect is remarkably strong and almost compensates for the other effect; i.e. Rand is only slightly better than S-V (for most volunteers).</abstract>
<title confidence="0.952368">Condition Average Time</title>
<author confidence="0.910055">Rand</author>
<abstract confidence="0.9690901">Sent 34.3 IDF 99.7 S-V 20.3 4: spent the most time making decisions when presented all words in random order. In addition to measuring performance, our interface also keeps track of how long each decision takes. Table 4 shows the average time of decisions in seconds under each of the four conditions. As can be seen, volunteers took the longest, by far, to make decisions with the Rand condition. Comparatively, with the S-V condition, they took less than one third of the time. Examination of these results led us to the conclusion that syntax clearly matters for this task. All volunteers performed much better when shown the full first sentence with words in their original order than when the same words were shown in random order, and the task took approximately half the time. Therefore, any bag of words approach is likely limited by a significantly lower upper bound than one which uses NLP techniques. In particular, the main subject and verb from the sentence were important. Given only these two words, volunteers performed almost as well as when they had all the words in random order, and much better than when they were given the top two words according to TF*IDF weights, a very common measure of word importance in IR literature.</abstract>
<title confidence="0.782139">5 Using Only Subjects and Verbs with Standard Systems System Performance Sent S-V Naive Bayes 55.6% 54.8%</title>
<abstract confidence="0.997030207100592">Rocchio/TF*IDF 54.0% 54.0% K-Nearest Neighbor 54.0% 54.8% Probabilistic Indexing 59.7% 54.0% Maximum Entropy 58.1% 53.2% Support Vector Machines 54.8% 54.0% Bins 56.5% 53.2% 5: performed almost as well using single word subjects and verbs as they did when provided with the entire first sentence. We next decided to test how the standard text categorization systems we had previously tested would fair if only subjects and verbs were provided. At this point, we were still using manually extracted words. Table 5 shows how the results using only subjects and verbs compared to results using the entire first sentence of the caption (the first column of results is the same as that from Table 1). As can be seen, the performance was slightly worse for five of the seven systems, slightly better for one, and the same for another. As with humans, results were almost as high using just two specifically chosen words as when all words in the sentence (not accounting for syntax) were used. 6 NLP Based System With the results of our experiment with humans in mind, we set out to create a fully automatic text categorization system that takes advantage of our findings. First, our system tries to extract the single words best representing the main subject and verb from the first sentence of each caption in our training set, and these comprise lists of subjects and verbs which are representative of our categories. Next, for each test image, the subject and verb from its caption are extracted, and these are compared to those from the training set using a measure of word-to-word similarity. A score is generated for every category based on these similarities, and the category with the highest score is predicted. Subjects and Verbs Subjects and verbs are automatically extracted using a three step process. First, Church&apos;s statistical part-of-speech tagger, POS (Church, 1988), assigns a grammatical category to every word in each caption. Second, the shallow parser CASS (Abney, 1997) parses each tagged caption. Third, a final script operates on the output of CASS, extracting the heads of the appropriate noun phrase and verb phrase to obtain the single words assumed to best represent the subject and verb of the sentence. (If CASS considers the head of the noun phrase to be a name, the token &amp;quot;NAME&amp;quot; is used instead). On our test set, this process leads to an accuracy of 83.9% for subjects and 80.6% for verbs, according to our manually extracted words. WordNet (Fellbaum, 1998) is used to convert each extracted subject and verb to its morphological base-word. 6.2 Word Similarity In order to compare subjects and verbs extracted from test captions to those from the training set, we examined a large &amp;quot;extended&amp;quot; corpus consisting of thousands of news articles and captions taken from the same newsgroups as the images discussed in this paper. Using the same method of extraction as discussed in Section 6.1, the single words best representing the subjects and verbs were extracted from every sentence of every article and caption in the extended corpus. When dealing with text categorization, the creation of the corpus is generally one of the most time consuming tasks, since documents usually need to be manually labeled for the training set, but for the purposes of word similarity as we are doing it, this extended corpus is unlabeled and easily obtainable. Based on these extracted subject/verb pairs, we defined the similarity between two subjects to be the percentage of verbs they share in common, and the similarity between two verbs to be the percentage of subjects they share in common. The idea was that two subjects should be considered similar if they often partake in similar actions, and that two verbs should be considered similar if they represent actions that are often executed by similar entities. This is not necessarily a good measure of word similarity for other tasks, but we thought it might work well for this domain. For example, let&apos;s say that the word &amp;quot;fireman&amp;quot; never appears in the training set of the corpus, but words such as &amp;quot;policeman&amp;quot; and &amp;quot;volunteer&amp;quot; do, in captions from images beto the category Responding; these subjects likely share a higher percentage of verbs in common than most randomly selected pairs of subjects, and would therefore have a relatively high similarity. In addition, for our current domain, they are representative of the category Responding). our definitions, the similarity between any subject or verb and itself comes out to be one, and the similarity between any two non-identical words is generally much less. We also defined the similarity between a subject and a verb to be twice the number of times they appear together divided by the total number of times each appears. Therefore, if the subject/verb pair always appears together, the similarity between the two words would be one, and otherwise it would be less. The idea is that subjects which are likely to perform actions seen as representative of a category should in and of themselves be considered representative of the category. The same is true for verbs which represent actions that are likely to be performed by subjects that are representative of a category. For example, let&apos;s say that the word &amp;quot;fireman&amp;quot; never appears in the training set of the corpus, but verbs such as &amp;quot;help&amp;quot; and &amp;quot;rescue&amp;quot; do, in captions from images belonging to the category Responding. a &amp;quot;fireman&amp;quot; is more likely to &amp;quot;help&amp;quot; and &amp;quot;rescue&amp;quot; than perform other it will contribute more to the than to others. 6.3 Choosing a Category To choose a category for some specified image, the single word subject and verb from the first sentence of its caption are extracted, all relevant similarities are added together for each category, and the category with the highest score is then More formally, let the set of and for some specific category V, be the set of subjects and verbs exfrom training instances of a particular test image vd be the single word subject and verb extracted, re- For any two words and regardless of whether they are subjects or verbs, be the similarity between the two words as defined in the previous subsection (any similarity involving a &amp;quot;NAME&amp;quot; token is defined be 0). Let the total score for a a test document (cid) = ) a document does not have a &amp;quot;NAME&amp;quot; token extracted as the subject, the chosen category is simply: argroax[T(cld)] cEe In order to take &amp;quot;NAME&amp;quot; tokens into account when they are extracted (this occurs in 16 of the 124 test cases), we decided to multiply the score for each category by the a-priori probability of the category, based on the training set, given that a &amp;quot;NAME&amp;quot; token is extracted. For example, in the training set, 56.0% of the &amp;quot;NAME&amp;quot; come from the People whereas only 33.9% of the training images belong to this category overall, so the final score the People is multiplied by 0.56 if a &amp;quot;NAME&amp;quot; token is extracted to account the new skew. More formally, let be the estimated probability of a &amp;quot;NAME&amp;quot; token given a category, based on the training set. the category chosen for a document has a &amp;quot;NAME&amp;quot; token extracted is: argroax[T(cld)x P(N1c)] cEe</abstract>
<title confidence="0.64173675">7 Results and Evaluation System Performance Sent S-V Naive Bayes 55.6% 54.8%</title>
<abstract confidence="0.997595305970149">Rocchio/TF*IDF 54.0% 54.0% K-Nearest Neighbor 54.0% 54.8% Probabilistic Indexing 59.7% 54.0% Maximum Entropy 58.1% 53.2% Support Vector Machines 54.8% 54.0% Bins 56.5% 53.2% NLP Based system 65.3% 6: NLP based system outperforms seven standard systems by a considerable margin. The final line of Table 6, which is otherwise the same as Table 5, shows the performance of our NLP based system described in the previous section. As can be seen, the system&apos;s accuracy of 65.3% is at least 10% higher than the seven standard systems achieved when using only subjects and verbs (manually extracted for the standard systems), and it is at least 5% higher than the seven standard systems achieved when given the entire first sentence of each caption. Looking back at Section 4, we see that humans given only the subject and verb of each sentence achieved, on average, a 71.8% accuracy. We consider this a reasonable upper bound for the accuracy that a system such as ours might achieve. 8 Related Work There has long been a lot of interest in combining NLP and IR,. Some of the recent work by various researchers in this area is summarized in (Strzalkowski et al., 1998) and (Strzalkowski, 1999), and as can be seen, the results have been mixed, at best. Recently, there has been some success using NLP to aid in the retrieval of images. Smeaton and Quigley (1996) showed some improvement using WordNet to compute noun to noun similarities which were then used to compare queries with captions. Elworthy (2000) showed improvement using an NLP technique he calls &amp;quot;phrase matching&amp;quot; which first converts queries and captions to &amp;quot;dependency structures&amp;quot;. In both of these cases, the researchers manually constructed captions for their images, and in the case of Smeaton and Quigley, they manually disambiguated all words. Working on domain-specific text categorization tasks involving full length news articles, Riloff has created the system AutoSlog-TS (Riloff and Lorenzen, 1999) which relies on NLP techniques to fully-automatically create dictionaries of &amp;quot;augmented relevancy signatures&amp;quot; which can then be used to improve results for binary text categorization tasks. She found that her system, which labels a document in a category if any augmented relevancy signature associated with the category is found in the document, performs about as well with automatically constructed dictionaries as it does with hand constructed dictionaries and much better than when no dictionary is used at all. No comparison was made to other standard text categorization techniques. With IR tasks such as query expansion and word sense disambiguation in mind, there have previous attempts at measuring word-toword similarity. The research discussed (Sussna, 1993), (Resnik, 1999), and (Richardson et al., 1994) concerns using WordNet link structure to determine semantic similarity between nouns. Our task also requires us to compute similarity between verbs with each other and verbs with nouns, so the techniques discussed in papers do not apply. Our approach is simpler, and not necessarily appropriate for general tasks, but it serves our intended purpose well and leads to positive results in our experiments. Other commonly used metrics to measure word-to-word similarity for use with NLP applications include the Jaccard Coefficient and the Dice Coefficient (Radecki, 1982; van Rijsbergen, 1979; Smadja et al., 1996). These measures are related to the ratio of the frequency with which two words appear together (i.e. near each other) in text to the frequencies of the two words independently. While simple and general, they do not apply well to our specific task and domain. For example, &amp;quot;rescuers&amp;quot; and &amp;quot;victim&amp;quot; might often appear together in text, as they do in the caption of the sample image in Figure 1, but for our current categorization task, as subjects they would be indicative of two different ( Responding we do not want them to be considered similar. On the other hand, words such as &amp;quot;firefighter&amp;quot; and &amp;quot;fireman&amp;quot; may hardly ever appear together, since an author will likely use one or the other consistently, but for our task, they should be considered very similar. Our method of measuring word-to-word similarity takes these problems into account. 9 Conclusions We have shown that NLP is important for a particular text categorization task. We believe that this importance depends on both the task and domain. NLP becomes helpful when we are dealing with tasks that rely on focus, perspective, point of view, etc. Admittedly, most of the standard IR test collections are not like this, and bag of words approaches work well for them. However, we believe that tasks such as the one described in this paper, which arose naturally in the course of our research, will continue to appear, and when they do, approaches similar to ours will be useful. The categories discussed in this paper are not nominal categories determined by the presence or absence of any specific object in an image. These categories deal with predicate argument relationships that can only be determined using linguistic analysis. Looking back, once again, at Figure 1, we see that the subject and verb of the first sentence of the caption refer to the object of focus in the image and the action taking place. The phrase &amp;quot;rescuers carry&amp;quot; is a clear of the Responding whereas other words that might have high IDF weights, such as &amp;quot;disco&amp;quot; and &amp;quot;victims&amp;quot;, would not be helpful and may even be misleading to any system using a bag of words approach. We have verified the importance of NLP for our task by presenting evidence from experiments with human subjects, and we have described a new NLP based system which considerably outperforms seven standard systems. This is a positive result which shows promise for combining NLP the future, at least for certain tasks.</abstract>
<note confidence="0.927984214285715">References S. Abney. 1997. The scol manual (version 0.1b). K. Church. 1988. A stochastic parts program and phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-88). D. Elworthy. 2000. Retrieval from captioned image databases using natural language processing. In Proceedings of the 9th International Conference on Information Knowledge and Management (CIKM- 00). Fellbaum, editor. 1998. Database. Press. A. McCallum. 1996. Bow: A toolkit for statis-</note>
<abstract confidence="0.772448111111111">tical language modeling, text retrieval, classification, and clustering. http://www.cs.cmu.edui mccallum/bow. T. Radecki. 1982. Similarity measures for boolean request formulations. of the Amer- Society for Information Science, P. Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural lanof Artificial Intelligence Research, 11:95-130. R. Richardson, A. F. Smeaton, and J. Murphy. 1994. Using WordNet as a knowledge base for measuring semantic similarity between words. Technical Report CA-1294, Dublin City University. E. Riloff and J. Lorenzen. 1999. Extraction-based text categorization: Generating domain specific role relationships automatically. In T. Strzaeditor, Language Information Re- 7. Kluwer Academic Publishers. C. Sable and K. W. Church. 2001. Using bins to empirically estimate term weights for text catego- In of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-01). G. Salton and C. Buckley. 1988. Term weighting apin automatic text retrieval.</abstract>
<note confidence="0.831959486486486">and Management, Salton. 1989. &apos;Text Processing: The Transformation, Analysis, and Retrieval of Inforby Computer. F. Z. Smadja, K. McKeown, and V. Hatzivassiloglou. 1996. Translating collocations for bilingual lexi- A statistical approach. Lin- ( 1) : 1-38. A. F. Smeaton and I. Quigley. 1996. Experiments on using semantic distances between words image caption retrieval. In of the 19th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-96). A. F. Smeaton. 1999. Using NLP or NLP resources for information retrieval tasks. In T. Strzalkowski, Language Information Retrieval, chapter 4. Kluwer Academic Publishers. T. Strzalkowski, F. Lin, and J. Perez-Carballo. 1998. Natural language information retrieval: TREC-6 In Sixth Text Retrieval Conference Special Publication 500-240. Strzalkowski, editor. 1999. Language In- Retrieval. Academic Publishers. M. Sussna. 1993. Word sense disambiguation for free-text indexing using a massive semantic net- In of the 2nd International Conference on Information Knowledge and Management (CIKM-93). J. van Rijsbergen. 1979. Retrieval. Butterworths, London, 2nd edition. E. M. Voorhees. 1993. Using WordNet to disamword senses for text retrieval. In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-93).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>The scol manual (version 0.1b).</title>
<date>1997</date>
<contexts>
<context position="12851" citStr="Abney, 1997" startWordPosition="2183" endWordPosition="2184">tive of our categories. Next, for each test image, the subject and verb from its caption are extracted, and these are compared to those from the training set using a measure of word-to-word similarity. A score is generated for every category based on these similarities, and the category with the highest score is predicted. 6.1 Extracting Subjects and Verbs Subjects and verbs are automatically extracted using a three step process. First, Church&apos;s statistical part-of-speech tagger, POS (Church, 1988), assigns a grammatical category to every word in each caption. Second, the shallow parser CASS (Abney, 1997) parses each tagged caption. Third, a final script operates on the output of CASS, extracting the heads of the appropriate noun phrase and verb phrase to obtain the single words assumed to best represent the subject and verb of the sentence. (If CASS considers the head of the noun phrase to be a name, the token &amp;quot;NAME&amp;quot; is used instead). On our test set, this process leads to an accuracy of 83.9% for subjects and 80.6% for verbs, according to our manually extracted words. WordNet (Fellbaum, 1998) is used to convert each extracted subject and verb to its morphological base-word. 6.2 Word Similari</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>S. Abney. 1997. The scol manual (version 0.1b).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-88).</booktitle>
<contexts>
<context position="12742" citStr="Church, 1988" startWordPosition="2166" endWordPosition="2167">tence of each caption in our training set, and these comprise lists of subjects and verbs which are representative of our categories. Next, for each test image, the subject and verb from its caption are extracted, and these are compared to those from the training set using a measure of word-to-word similarity. A score is generated for every category based on these similarities, and the category with the highest score is predicted. 6.1 Extracting Subjects and Verbs Subjects and verbs are automatically extracted using a three step process. First, Church&apos;s statistical part-of-speech tagger, POS (Church, 1988), assigns a grammatical category to every word in each caption. Second, the shallow parser CASS (Abney, 1997) parses each tagged caption. Third, a final script operates on the output of CASS, extracting the heads of the appropriate noun phrase and verb phrase to obtain the single words assumed to best represent the subject and verb of the sentence. (If CASS considers the head of the noun phrase to be a name, the token &amp;quot;NAME&amp;quot; is used instead). On our test set, this process leads to an accuracy of 83.9% for subjects and 80.6% for verbs, according to our manually extracted words. WordNet (Fellbau</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-88).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Retrieval from captioned image databases using natural language processing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th International Conference on Information Knowledge and Management (CIKM00).</booktitle>
<contexts>
<context position="19912" citStr="Elworthy (2000)" startWordPosition="3400" endWordPosition="3401">er this a reasonable upper bound for the accuracy that a system such as ours might achieve. 8 Related Work There has long been a lot of interest in combining NLP and IR,. Some of the recent work by various researchers in this area is summarized in (Strzalkowski et al., 1998) and (Strzalkowski, 1999), and as can be seen, the results have been mixed, at best. Recently, there has been some success using NLP to aid in the retrieval of images. Smeaton and Quigley (1996) showed some improvement using WordNet to compute noun to noun similarities which were then used to compare queries with captions. Elworthy (2000) showed improvement using an NLP technique he calls &amp;quot;phrase matching&amp;quot; which first converts queries and captions to &amp;quot;dependency structures&amp;quot;. In both of these cases, the researchers manually constructed captions for their images, and in the case of Smeaton and Quigley, they manually disambiguated all words. Working on domain-specific text categorization tasks involving full length news articles, Riloff has created the system AutoSlog-TS (Riloff and Lorenzen, 1999) which relies on NLP techniques to fully-automatically create dictionaries of &amp;quot;augmented relevancy signatures&amp;quot; which can then be used </context>
</contexts>
<marker>Elworthy, 2000</marker>
<rawString>D. Elworthy. 2000. Retrieval from captioned image databases using natural language processing. In Proceedings of the 9th International Conference on Information Knowledge and Management (CIKM00).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Bow: A toolkit for statistical language modeling, text retrieval, classification, and clustering. http://www.cs.cmu.edui mccallum/bow.</title>
<date>1996</date>
<contexts>
<context position="4872" citStr="McCallum, 1996" startWordPosition="799" endWordPosition="800">sifier, which relies on bins to empirically estimate 2Instructions provided to the evaluators, including definitions of our categories, can be seen at http:// www cs. columbia. edur sableiresearchiinstructions.html. term weights as described in (Sable and Church, 2001), to place images into these categories. However, we quickly found that the performance was not adequate. We then tested several alternative systems and found that they all had similar performance. Table 1 shows the results of all systems tested. The first six systems in the table comprise the publicly available Rainbow package (McCallum, 1996), and the last is our own bin-based system. The performance of the systems ranged from 54.0% to 59.7%. Choosing the largest category every time would give a baseline performance of 39.5%. While all systems beat the baseline, we did not feel that they were doing as well as possible. System Performance Naive Bayes 55.6% Rocchio/TF*IDF 54.0% K-Nearest Neighbor 54.0% Probabilistic Indexing 59.7% Maximum Entropy 58.1% Support Vector Machines 54.8% Bins 56.5% Table 1: The initial results were low for all systems. In order to decide in which category to place an image, it is important to determine wh</context>
</contexts>
<marker>McCallum, 1996</marker>
<rawString>A. McCallum. 1996. Bow: A toolkit for statistical language modeling, text retrieval, classification, and clustering. http://www.cs.cmu.edui mccallum/bow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Radecki</author>
</authors>
<title>Similarity measures for boolean search request formulations.</title>
<date>1982</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>33--1</pages>
<contexts>
<context position="21778" citStr="Radecki, 1982" startWordPosition="3691" endWordPosition="3692">, 1999), and (Richardson et al., 1994) concerns using WordNet link structure to determine semantic similarity between nouns. Our task also requires us to compute similarity between verbs with each other and verbs with nouns, so the techniques discussed in these papers do not apply. Our approach is simpler, and not necessarily appropriate for general tasks, but it serves our intended purpose well and leads to positive results in our experiments. Other commonly used metrics to measure word-to-word similarity for use with NLP applications include the Jaccard Coefficient and the Dice Coefficient (Radecki, 1982; van Rijsbergen, 1979; Smadja et al., 1996). These measures are related to the ratio of the frequency with which two words appear together (i.e. near each other) in text to the frequencies of the two words independently. While simple and general, they do not apply well to our specific task and domain. For example, &amp;quot;rescuers&amp;quot; and &amp;quot;victim&amp;quot; might often appear together in text, as they do in the caption of the sample image in Figure 1, but for our current categorization task, as subjects they would be indicative of two different categories ( Workers Responding versus Affected People), and we do n</context>
</contexts>
<marker>Radecki, 1982</marker>
<rawString>T. Radecki. 1982. Similarity measures for boolean search request formulations. Journal of the American Society for Information Science, 33(1):8-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--95</pages>
<contexts>
<context position="21172" citStr="Resnik, 1999" startWordPosition="3596" endWordPosition="3597">on tasks. She found that her system, which labels a document in a category if any augmented relevancy signature associated with the category is found in the document, performs about as well with automatically constructed dictionaries as it does with hand constructed dictionaries and much better than when no dictionary is used at all. No comparison was made to other standard text categorization techniques. With IR tasks such as query expansion and word sense disambiguation in mind, there have been previous attempts at measuring word-toword similarity. The research discussed in (Sussna, 1993), (Resnik, 1999), and (Richardson et al., 1994) concerns using WordNet link structure to determine semantic similarity between nouns. Our task also requires us to compute similarity between verbs with each other and verbs with nouns, so the techniques discussed in these papers do not apply. Our approach is simpler, and not necessarily appropriate for general tasks, but it serves our intended purpose well and leads to positive results in our experiments. Other commonly used metrics to measure word-to-word similarity for use with NLP applications include the Jaccard Coefficient and the Dice Coefficient (Radecki</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>P. Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 11:95-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Richardson</author>
<author>A F Smeaton</author>
<author>J Murphy</author>
</authors>
<title>Using WordNet as a knowledge base for measuring semantic similarity between words.</title>
<date>1994</date>
<tech>Technical Report CA-1294,</tech>
<institution>Dublin City University.</institution>
<contexts>
<context position="21203" citStr="Richardson et al., 1994" startWordPosition="3599" endWordPosition="3602">that her system, which labels a document in a category if any augmented relevancy signature associated with the category is found in the document, performs about as well with automatically constructed dictionaries as it does with hand constructed dictionaries and much better than when no dictionary is used at all. No comparison was made to other standard text categorization techniques. With IR tasks such as query expansion and word sense disambiguation in mind, there have been previous attempts at measuring word-toword similarity. The research discussed in (Sussna, 1993), (Resnik, 1999), and (Richardson et al., 1994) concerns using WordNet link structure to determine semantic similarity between nouns. Our task also requires us to compute similarity between verbs with each other and verbs with nouns, so the techniques discussed in these papers do not apply. Our approach is simpler, and not necessarily appropriate for general tasks, but it serves our intended purpose well and leads to positive results in our experiments. Other commonly used metrics to measure word-to-word similarity for use with NLP applications include the Jaccard Coefficient and the Dice Coefficient (Radecki, 1982; van Rijsbergen, 1979; S</context>
</contexts>
<marker>Richardson, Smeaton, Murphy, 1994</marker>
<rawString>R. Richardson, A. F. Smeaton, and J. Murphy. 1994. Using WordNet as a knowledge base for measuring semantic similarity between words. Technical Report CA-1294, Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Lorenzen</author>
</authors>
<title>Extraction-based text categorization: Generating domain specific role relationships automatically.</title>
<date>1999</date>
<booktitle>Natural Language Information Retrieval, chapter 7.</booktitle>
<editor>In T. Strzalkowski, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="20378" citStr="Riloff and Lorenzen, 1999" startWordPosition="3468" endWordPosition="3471">Quigley (1996) showed some improvement using WordNet to compute noun to noun similarities which were then used to compare queries with captions. Elworthy (2000) showed improvement using an NLP technique he calls &amp;quot;phrase matching&amp;quot; which first converts queries and captions to &amp;quot;dependency structures&amp;quot;. In both of these cases, the researchers manually constructed captions for their images, and in the case of Smeaton and Quigley, they manually disambiguated all words. Working on domain-specific text categorization tasks involving full length news articles, Riloff has created the system AutoSlog-TS (Riloff and Lorenzen, 1999) which relies on NLP techniques to fully-automatically create dictionaries of &amp;quot;augmented relevancy signatures&amp;quot; which can then be used to improve results for binary text categorization tasks. She found that her system, which labels a document in a category if any augmented relevancy signature associated with the category is found in the document, performs about as well with automatically constructed dictionaries as it does with hand constructed dictionaries and much better than when no dictionary is used at all. No comparison was made to other standard text categorization techniques. With IR ta</context>
</contexts>
<marker>Riloff, Lorenzen, 1999</marker>
<rawString>E. Riloff and J. Lorenzen. 1999. Extraction-based text categorization: Generating domain specific role relationships automatically. In T. Strzalkowski, editor, Natural Language Information Retrieval, chapter 7. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sable</author>
<author>K W Church</author>
</authors>
<title>Using bins to empirically estimate term weights for text categorization.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-01).</booktitle>
<contexts>
<context position="4526" citStr="Sable and Church, 2001" startWordPosition="741" endWordPosition="744">8 images. 98 (39.5%) were classified as Workers Responding, 72 (29.0%) were classified as Affected People, 55 (22.2%) were classified as Wreckage, and 23 (9.3%) were classified as Other. The final data set was randomly divided into a training set and a test set, each containing 124 images. 3 Initial Experiments Our original plan was to use our own classifier, which relies on bins to empirically estimate 2Instructions provided to the evaluators, including definitions of our categories, can be seen at http:// www cs. columbia. edur sableiresearchiinstructions.html. term weights as described in (Sable and Church, 2001), to place images into these categories. However, we quickly found that the performance was not adequate. We then tested several alternative systems and found that they all had similar performance. Table 1 shows the results of all systems tested. The first six systems in the table comprise the publicly available Rainbow package (McCallum, 1996), and the last is our own bin-based system. The performance of the systems ranged from 54.0% to 59.7%. Choosing the largest category every time would give a baseline performance of 39.5%. While all systems beat the baseline, we did not feel that they wer</context>
</contexts>
<marker>Sable, Church, 2001</marker>
<rawString>C. Sable and K. W. Church. 2001. Using bins to empirically estimate term weights for text categorization. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-01).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="6925" citStr="Salton and Buckley, 1988" startWordPosition="1155" endWordPosition="1158">Experiments with Humans To test our hypothesis, we randomly divided our data set of 248 images into four equally sized subsets and recruited four volunteers to view text associated with our images under four conditions. Each volunteer was a native speaker of English, and none had any connection to this or any related research. The four conditions were: • Sent: The full first sentence of the caption. • Rand: The words from the first sentence of the caption in random order. • IDF: The top two words, not including proper nouns, from the first sentence of the caption, according to TF*IDF weights (Salton and Buckley, 1988; Salton, 1989). • S-V: The two words, manually extracted, best representing the main subject and verb. If the subject was a proper noun, only the token &amp;quot;NAME&amp;quot; was provided. Sent Philippine rescuers carry a fire victim March 19 who perished in a blaze at a Manila disco. Rand at perished disco who Manila a a in 19 carry Philippine blaze victim a rescuers March fire IDF disco rescuers S-V subject = &amp;quot;rescuers&amp;quot;, verb = &amp;quot;carry&amp;quot; Table 2: The subject and verb make it clear that the category for the sample image is Workers Responding. Other words such as &amp;quot;disco&amp;quot; and &amp;quot;victim&amp;quot; are not helpful and can be</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic &apos;Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="6940" citStr="Salton, 1989" startWordPosition="1159" endWordPosition="1160"> test our hypothesis, we randomly divided our data set of 248 images into four equally sized subsets and recruited four volunteers to view text associated with our images under four conditions. Each volunteer was a native speaker of English, and none had any connection to this or any related research. The four conditions were: • Sent: The full first sentence of the caption. • Rand: The words from the first sentence of the caption in random order. • IDF: The top two words, not including proper nouns, from the first sentence of the caption, according to TF*IDF weights (Salton and Buckley, 1988; Salton, 1989). • S-V: The two words, manually extracted, best representing the main subject and verb. If the subject was a proper noun, only the token &amp;quot;NAME&amp;quot; was provided. Sent Philippine rescuers carry a fire victim March 19 who perished in a blaze at a Manila disco. Rand at perished disco who Manila a a in 19 carry Philippine blaze victim a rescuers March fire IDF disco rescuers S-V subject = &amp;quot;rescuers&amp;quot;, verb = &amp;quot;carry&amp;quot; Table 2: The subject and verb make it clear that the category for the sample image is Workers Responding. Other words such as &amp;quot;disco&amp;quot; and &amp;quot;victim&amp;quot; are not helpful and can be misleading. Ta</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>G. Salton. 1989. Automatic &apos;Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Z Smadja</author>
<author>K McKeown</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<pages>1--38</pages>
<contexts>
<context position="21822" citStr="Smadja et al., 1996" startWordPosition="3697" endWordPosition="3700">) concerns using WordNet link structure to determine semantic similarity between nouns. Our task also requires us to compute similarity between verbs with each other and verbs with nouns, so the techniques discussed in these papers do not apply. Our approach is simpler, and not necessarily appropriate for general tasks, but it serves our intended purpose well and leads to positive results in our experiments. Other commonly used metrics to measure word-to-word similarity for use with NLP applications include the Jaccard Coefficient and the Dice Coefficient (Radecki, 1982; van Rijsbergen, 1979; Smadja et al., 1996). These measures are related to the ratio of the frequency with which two words appear together (i.e. near each other) in text to the frequencies of the two words independently. While simple and general, they do not apply well to our specific task and domain. For example, &amp;quot;rescuers&amp;quot; and &amp;quot;victim&amp;quot; might often appear together in text, as they do in the caption of the sample image in Figure 1, but for our current categorization task, as subjects they would be indicative of two different categories ( Workers Responding versus Affected People), and we do not want them to be considered similar. On th</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>F. Z. Smadja, K. McKeown, and V. Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22 ( 1) : 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Smeaton</author>
<author>I Quigley</author>
</authors>
<title>Experiments on using semantic distances between words in image caption retrieval.</title>
<date>1996</date>
<booktitle>In Proceedings of the 19th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-96).</booktitle>
<contexts>
<context position="19766" citStr="Smeaton and Quigley (1996)" startWordPosition="3376" endWordPosition="3379">ach caption. Looking back at Section 4, we see that humans given only the subject and verb of each sentence achieved, on average, a 71.8% accuracy. We consider this a reasonable upper bound for the accuracy that a system such as ours might achieve. 8 Related Work There has long been a lot of interest in combining NLP and IR,. Some of the recent work by various researchers in this area is summarized in (Strzalkowski et al., 1998) and (Strzalkowski, 1999), and as can be seen, the results have been mixed, at best. Recently, there has been some success using NLP to aid in the retrieval of images. Smeaton and Quigley (1996) showed some improvement using WordNet to compute noun to noun similarities which were then used to compare queries with captions. Elworthy (2000) showed improvement using an NLP technique he calls &amp;quot;phrase matching&amp;quot; which first converts queries and captions to &amp;quot;dependency structures&amp;quot;. In both of these cases, the researchers manually constructed captions for their images, and in the case of Smeaton and Quigley, they manually disambiguated all words. Working on domain-specific text categorization tasks involving full length news articles, Riloff has created the system AutoSlog-TS (Riloff and Lor</context>
</contexts>
<marker>Smeaton, Quigley, 1996</marker>
<rawString>A. F. Smeaton and I. Quigley. 1996. Experiments on using semantic distances between words in image caption retrieval. In Proceedings of the 19th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Smeaton</author>
</authors>
<title>Using NLP or NLP resources for information retrieval tasks. In</title>
<date>1999</date>
<booktitle>Natural Language Information Retrieval, chapter 4.</booktitle>
<editor>T. Strzalkowski, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Smeaton, 1999</marker>
<rawString>A. F. Smeaton. 1999. Using NLP or NLP resources for information retrieval tasks. In T. Strzalkowski, editor, Natural Language Information Retrieval, chapter 4. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>F Lin</author>
<author>J Perez-Carballo</author>
</authors>
<title>Natural language information retrieval: TREC-6 report.</title>
<date>1998</date>
<booktitle>In The Sixth Text Retrieval Conference (TREC-6). NIST Special Publication</booktitle>
<pages>500--240</pages>
<contexts>
<context position="19572" citStr="Strzalkowski et al., 1998" startWordPosition="3342" endWordPosition="3345"> when using only subjects and verbs (manually extracted for the standard systems), and it is at least 5% higher than the seven standard systems achieved when given the entire first sentence of each caption. Looking back at Section 4, we see that humans given only the subject and verb of each sentence achieved, on average, a 71.8% accuracy. We consider this a reasonable upper bound for the accuracy that a system such as ours might achieve. 8 Related Work There has long been a lot of interest in combining NLP and IR,. Some of the recent work by various researchers in this area is summarized in (Strzalkowski et al., 1998) and (Strzalkowski, 1999), and as can be seen, the results have been mixed, at best. Recently, there has been some success using NLP to aid in the retrieval of images. Smeaton and Quigley (1996) showed some improvement using WordNet to compute noun to noun similarities which were then used to compare queries with captions. Elworthy (2000) showed improvement using an NLP technique he calls &amp;quot;phrase matching&amp;quot; which first converts queries and captions to &amp;quot;dependency structures&amp;quot;. In both of these cases, the researchers manually constructed captions for their images, and in the case of Smeaton and Q</context>
</contexts>
<marker>Strzalkowski, Lin, Perez-Carballo, 1998</marker>
<rawString>T. Strzalkowski, F. Lin, and J. Perez-Carballo. 1998. Natural language information retrieval: TREC-6 report. In The Sixth Text Retrieval Conference (TREC-6). NIST Special Publication 500-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>editor</author>
</authors>
<title>Natural Language Information Retrieval.</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Strzalkowski, editor, 1999</marker>
<rawString>T. Strzalkowski, editor. 1999. Natural Language Information Retrieval. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sussna</author>
</authors>
<title>Word sense disambiguation for free-text indexing using a massive semantic network.</title>
<date>1993</date>
<booktitle>In Proceedings of the 2nd International Conference on Information Knowledge and Management (CIKM-93).</booktitle>
<contexts>
<context position="21156" citStr="Sussna, 1993" startWordPosition="3594" endWordPosition="3595">ext categorization tasks. She found that her system, which labels a document in a category if any augmented relevancy signature associated with the category is found in the document, performs about as well with automatically constructed dictionaries as it does with hand constructed dictionaries and much better than when no dictionary is used at all. No comparison was made to other standard text categorization techniques. With IR tasks such as query expansion and word sense disambiguation in mind, there have been previous attempts at measuring word-toword similarity. The research discussed in (Sussna, 1993), (Resnik, 1999), and (Richardson et al., 1994) concerns using WordNet link structure to determine semantic similarity between nouns. Our task also requires us to compute similarity between verbs with each other and verbs with nouns, so the techniques discussed in these papers do not apply. Our approach is simpler, and not necessarily appropriate for general tasks, but it serves our intended purpose well and leads to positive results in our experiments. Other commonly used metrics to measure word-to-word similarity for use with NLP applications include the Jaccard Coefficient and the Dice Coef</context>
</contexts>
<marker>Sussna, 1993</marker>
<rawString>M. Sussna. 1993. Word sense disambiguation for free-text indexing using a massive semantic network. In Proceedings of the 2nd International Conference on Information Knowledge and Management (CIKM-93).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London,</location>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworths, London, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Using WordNet to disambiguate word senses for text retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-93).</booktitle>
<marker>Voorhees, 1993</marker>
<rawString>E. M. Voorhees. 1993. Using WordNet to disambiguate word senses for text retrieval. In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-93).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>