<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003539">
<title confidence="0.9434105">
IHS R&amp;D Belarus: Cross-domain Extraction of Product Features
using Conditional Random Fields
</title>
<author confidence="0.500162">
Maryna Chernyshevich
</author>
<affiliation confidence="0.189708">
IHS Inc. / IHS Global Belarus
</affiliation>
<address confidence="0.4064845">
131 Starovilenskaya St.
220123, Minsk, Belarus
</address>
<email confidence="0.994767">
Marina.Chernyshevich@ihs.com
</email>
<sectionHeader confidence="0.993755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976425">
This paper describes the aspect extraction
system submitted by IHS R&amp;D Belarus
team at the SemEval-2014 shared task re-
lated to Aspect-Based Sentiment Analy-
sis. Our system is based on IHS Goldfire
linguistic processor and uses a rich set of
lexical, syntactic and statistical features
in CRF model. We participated in two
domain-specific tasks – restaurants and
laptops – with the same system trained on
a mixed corpus of reviews. Among sub-
missions of constrained systems from 28
teams, our submission was ranked first in
laptop domain and fourth in restaurant
domain for the subtask A devoted to as-
pect extraction.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995360156862745">
With a rapid growth of the blogs, forums, review
sites and social networks, more and more people
express their personal views about products on
the Internet in form of reviews, ratings, or rec-
ommendations. This is a great source of data
used by many researchers and commercial appli-
cations that are focused on the sentiment analy-
sis to determine customer opinions.
Sentiment analysis can be done on document,
sentence, and phrase level (Jagtap, V. S., Ka-
rishma Pawar, 2013). Earlier works were focused
mainly on the document (Turney, 2002; Pang,
Lee and Vaithyanathan, 2002) and the sentence
level (Kim and Hovy, 2004). However, this in-
formation can be insufficient for customers who
are seeking opinions on specific product features
(aspects) such as design, battery life, or screen.
This fine-grained classification is a topic of as-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons. org/licenses/by/4.0/
pect-based sentiment analysis (Moghaddam and
Ester, 2012).
Traditional approaches to aspect extraction are
based on frequently used nouns and noun phrases
(Popescu and Etzioni, 2005; Blair-Goldensohn et
al., 2008), exploiting opinions (Zhuang et al.,
2006; Kobayashi, 2006), and supervised learning
(Mukherjee and Liu, 2012).
In this paper, we describe a system
(IHS_RD_Belarus in official results) developed
to participate in the international shared task or-
ganized by the Conference on Semantic Evalua-
tion Exercises (SemEval-2014) and focused on
the phrase-level sentiment classification, namely
aspect extraction (Pontiki et al., 2014). An aspect
term means particular feature of a product or ser-
vice used in opinion-bearing sentences (My
phone has amazing screen), as well as in
neutral sentences (The screen brightness
automatically adjusts).
The organizers of SemEval-2014 task have
provided a dataset of customer reviews with an-
notated aspects of the target entities from two
domains: restaurants (3041 sentences) and lap-
tops (3045 sentences). The results were evaluat-
ed separately in each domain. Table 1 shows the
distribution of the provided data for each domain
dataset, training and testing set, with number of
sentences and aspects.
</bodyText>
<table confidence="0.986728714285714">
Laptops Restaurants
Training
Sentences 3045 3041
Aspects 2358 3693
Testing
Sentences 800 800
Aspects 654 1134
</table>
<tableCaption confidence="0.999652">
Table 1. Distribution of the provided data.
</tableCaption>
<bodyText confidence="0.9988774">
Many studies showed that sentiment analysis
is very sensitive to the source domain (training
corpus domain) and performs poorly on data
from other domain (Jakob and Gurevych, 2010).
This restriction limits the applicability of in-
</bodyText>
<page confidence="0.991222">
309
</page>
<note confidence="0.7304615">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 309–313,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999734545454545">
domain models to a wide domain diversity of
reviews. One of the common approaches to de-
velop a cross-domain system is training on a
mixture of labeled data from different domains
(Aue and Gamon, 2005). Cross-domain approach
has the advantage of better portability, but it suf-
fers from lower accuracy compared to in-domain
aspect extraction. Our cross-domain system is
trained on mixed training data, and the same
model was used unchanged for classification of
both domain-specific test datasets.
</bodyText>
<sectionHeader confidence="0.972166" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999738">
Aspect extraction may be considered as a se-
quence labeling task because the product aspects
occur at a sequence in a sentence (Liu, 2014).
One of the state-of-the-art methods used for se-
quence labeling is Conditional Random Fields
(CRF) (Lafferty, 2001). This method takes as an
input a sequence of tokens, calculates the proba-
bilities of the various possible labelings and
chooses the one with the maximum probability.
We decided to deviate from Inside-Outside-
Begin (IOB) scheme used by Jakob and
Gurevych (Jakob and Gurevych, 2010) and Li
(Li et al., 2010) and introduced the following
labels: FA for the attribute word preceding head
word of a noun group; FH for the head word of a
noun group; FPA for attribute word after head
word of a noun group (Microsoft Office
2003), and O for other non-aspect tokens. The
following is an example of our suggested tag-
ging: I/O want/O to/O unplug/O
the/O external/FA keyboard/FH.
Our experiments showed that the words used
in aspect terms are easier to recognize when they
are always tagged with the same tags. For exam-
ple, let’s consider the tagging of the word “cam-
era” in the following cases: “camera” and “com-
pact camera”. We propose the FH tag for both
examples, while the IOB scheme assumes the FB
tag for the first example and the FI tag for the
second.
</bodyText>
<subsectionHeader confidence="0.998818">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.970908952380953">
To facilitate feature generation for supervised
CRF learning, sentences were pre-processed with
IHS Goldfire linguistic processor that performs
the following operations: slang and misspelling
correction (“excelent” → &amp;quot;excellent&amp;quot; , “amazin”
→ “amazing”, “wouldnt” → “wouldn’t”), part-
of-speech tagging, parsing, noun phrase extrac-
tion, semantic role labeling within expanded
Subject-Action-Object (eSAO) relations
(Todhunter et al., 2013), named entity recogni-
tion, labeling for predictive question-answering
including rule-based sentiment analysis
(Todhunter et al., 2014).
In addition, we designed some simple rules to
detect entity boundaries that take precedence
over CRF labeling. For example, in the sentence
“I run Final Cut Pro 7 and a few
other applications”, our boundary detector
recognizes “Final Cut Pro 7” as an entity
represented by a single token (Tkachenko and
Simanovsky, 2012).
</bodyText>
<subsectionHeader confidence="0.974927">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999879666666667">
Below we will describe the features used in CRF
model to represent the current token, two previ-
ous and two next tokens.
</bodyText>
<listItem confidence="0.95419072972973">
Word features:
• Token feature represents a base form of a
token (word or entity) normalized by case
folding. The vocabulary of terms is pretty
compact within one domain, so this feature
can have considerable impact on terms ex-
traction performance.
• Part of speech feature represents the part-
of-speech tag of the current token with
slight generalization, for example, the NNS
tag (plural noun) is mapped to NN (singu-
lar noun).
• Named entity feature labels named entities,
e.g., people, organizations, locations, etc.
• Semantic category denotes the presence of
the token in manually crafted domain-
independent word-lists – sets of words hav-
ing a common semantic meaning – such as
parameter (characteristics of object, e.g.,
“durability”), process (e.g., “charging”),
sentiment-bearing word (e.g., “problem”),
person (e.g., “sister”), doer of an action
(someone or something that performs an
action, e.g., “organizer”), temporal word
(date- or time-related words, e.g., “Mon-
day”), nationality, word of reasoning (e.g.,
“decision”, “reason”), etc.
• Semantic orientation (SO) score of token
represents a low, mean or high SO score as
separate feature values (the thresholds were
determined experimentally). The SO of a
word indicates the strength of its associa-
tion with positive and negative reviews.
We calculated SO of each word w using
Pointwise Mutual Information (PMI)
measures as
SO (w) = PMI(w, pr) – PMI(w, nr),
</listItem>
<page confidence="0.986591">
310
</page>
<bodyText confidence="0.9998013">
where PMI is the amount of information
that we acquire about the presence of the
word in positive pr or negative reviews nr
(Turney, 2002). For the calculation of SO
score, we used rated reviews from
Epinions.com, Amazon.com and TripAdvi-
sor.com. To make corpus more precise, we
included only 5-star reviews in our positive
corpus, and 1-star reviews in our negative
corpus.
</bodyText>
<listItem confidence="0.91097696969697">
• Frequency of token occurrence is repre-
sented by five values ranging from very
frequent to very rare words with an exper-
imentally determined threshold. The fre-
quency was obtained by dividing the num-
ber of reviews containing the token by the
total number of reviews. The reason of us-
ing this as a feature is that people usually
comment on the same product aspects and
the vocabulary that they use usually con-
verges (Liu, 2012).
• Opinion target feature is a binary feature
that indicates whether a token is a part of
an item which opinions are expressed on
and comes from the rule-based sentiment
analysis integrated in the predictive ques-
tion-answering component of the IHS
Goldfire linguistic processor. Opinion tar-
get can be a product feature as well as a
product itself.
Noun phrase features:
• Role of a token in a noun phrase: head
word or attribute word.
• Noun phrase introduction feature marks all
tokens of noun phrase beginning with pos-
sessive pronoun, demonstrative pronoun,
definite or indefinite article.
• Number of attributes with SO score higher
than the experimentally chosen threshold.
This feature labels all words in a noun
group. Our research showed that people of-
ten use sentiment-bearing adjectives to de-
scribe an aspect, e.g., “My phone has a
great camera”.
• List feature was added to designate the
availability of list indicators (“and” or
comma) in the noun group, e.g., “The
leather carrying case, keyboard
and mouse arrived in two days”.
• Leaves-up feature denotes the number of
of-phrases in a noun phrase before the to-
ken under consideration. For example, the
token &amp;quot;battery&amp;quot; has one preceding of-
phrase in the phrase &amp;quot;durability of battery&amp;quot;.
• Leaves-down feature denotes the number of
of-phrases in a noun phrase after the token
under consideration.
SAO features:
• Semantic label feature represents the role
of the token in eSAO relation: subject, ac-
tion, adjective, object, preposition, indirect
object or adverb.
• SAO feature labels all words presented in
an eSAO relation. We used a set of eSAO
patterns to determine basic relations be-
tween words. To form a SAO pattern, each
non-empty component of an eSAO relation
was mapped to an abstract value, e.g.,
proper noun phrases to “PNP”, common
noun phrases to “CNP”, predicates are left
in their canonical form. For example, the
sentence &amp;quot;The restaurant Tal of-
fers authentic chongqing hot-
pot.&amp;quot; is represented by the SAO pattern
“PNP offer CNP”. All words from eSAO
are marked with the same SAO feature.
</listItem>
<subsectionHeader confidence="0.927971">
2.3 Results and Experiments
</subsectionHeader>
<bodyText confidence="0.9997612">
Our CRF model was trained on the mixed set of
6086 sentences with annotated aspect terms
(3045 from the laptop domain and 3041 from the
restaurant domain). The same model was applied
unchanged to the test dataset from laptop domain
(800 sentences) and restaurant domain (800 sen-
tences). We evaluated our system using 5-fold
cross-validation: in each of the five iterations of
the cross-validation, we used 80% of the provid-
ed training data for learning, and 20% for testing.
</bodyText>
<table confidence="0.5987524">
laptops restaurants
training set 0.707 0.7784
development set 0.7214 0.7865
test set 0.7455 0.7962
baseline 0.3564 0.4715
</table>
<tableCaption confidence="0.904637">
Table 2. Performance on different datasets (F1-
score).
</tableCaption>
<bodyText confidence="0.999953416666667">
The Table 2 shows the model performance (F1-
score) obtained on the training set (using 5-fold
cross validation), on the development set (we
used a part of the training set as development
set), on the final test set and the baseline provid-
ed by the task organizers.
To evaluate the individual contribution of dif-
ferent feature sets, we performed ablation exper-
iment, presented in Table 3. This test involves
removing one of the following feature sets at a
time: current token and its POS tag (TOK), com-
binations with two previous and two next tokens
</bodyText>
<page confidence="0.997743">
311
</page>
<bodyText confidence="0.999824888888889">
and their POS tags (CONT), named entity (NE),
semantic category (SC), semantic orientation
(SO), word frequency (WF), opinion target (OT),
noun phrase related features (NP_F), and SAO
pattern and semantic label (SAO_F). Some fea-
tures complement each other, so that despite
small individual contribution, a cumulative im-
provement is generally achieved by using them
in a set.
</bodyText>
<table confidence="0.999920571428571">
Dev set Test set
lap rest lap rest
overall 0.7214 0.7865 0.7455 0.7962
-TOK 0.6642 0.7244 0.692 0.7445
(-7.9%) (-7.9%) (-7.2%) (-6.4%)
-CONT 0.7101 0.77 0.7323 0.7811
(-1.6%) (-2.1%) (-1.8%) (-1.9%)
-SC 0.6982 0.7854 0.7048 0.7864
(-3.3%) (-0.1%) (-5.8%) (-1.2%)
-SO 0.709 0.7815 0.7442 0.7937
(-1.7%) (-0.6%) (-0.2%) (-0.3%)
-OT 0.7026 0.7812 0.7381 0.7973
(-2.6%) (-0.7%) (-1%) (0.1%)
-NP_F 0.717 0.777 0.7303 0.7801
(-0.6%) (-1.2%) (-2%) (-2%)
-WF 0.716 0.788 0.7399 0.7937
(-0.8%) (0.2%) (-0.7%) (-0.3%)
-SAO_F 0.7198 0.7854 0.7297 0.7981
(-0.2%) (-0.1%) (-2.1%) (0.2%)
-NE 0.7191 0.7836 0.7444 0.7961
(-0.3%) (-0.4%) (-0.1%) (0)
</table>
<tableCaption confidence="0.999849">
Table 3. Ablation experiment (F1-score).
</tableCaption>
<bodyText confidence="0.999631260869565">
The importance of a feature set is measured by
F1-score on development and testing datasets for
both domains separately.
Feature sets are listed in descending order of
their impact on overall performance. The analy-
sis shows that the most important feature set is
the combination of Token and POS features.
Other features contribute to the performance to a
smaller degree.
As can be seen, the relative influence of fea-
tures on F1-score is similar on test and develop-
ment sets, showing that our model effectively
overcomes the overfitting problem.
We conducted several experiments on the
training data to prove the domain portability of
our CRF model. The results are shown in Table 4.
As can be seen, the training on single-domain
data improves the performance of in-domain
classification by about 2%, but lowers the per-
formance of cross-domain classification by about
40%. The training on the mixed dataset demon-
strates acceptable accuracy on both domain-
specific test sets.
</bodyText>
<table confidence="0.9922424">
Training Results on Results on
dataset laptops dataset restaurants dataset
laptops 0.7667 0.3778
restaurants 0.2961 0.8223
mixed 0.7455 0.7962
</table>
<tableCaption confidence="0.9899425">
Table 4. Results of classification with different
training datasets (F1-score).
</tableCaption>
<subsectionHeader confidence="0.998073">
2.4 Error Analysis and Further Work
</subsectionHeader>
<bodyText confidence="0.999399">
The error analysis showed three main error
types: not recognized, excessively recognized
and partially recognized aspect terms (head word
is recognized correctly, e.g., “separate RAM
memory” instead of “RAM memory”). While
first types are recall and precision errors respec-
tively, partial aspect extraction yields both recall
and precision errors. A summary of the errors on
test dataset is presented in Table 5.
</bodyText>
<table confidence="0.993995">
laptops restaurants
not recognized 68% 58%
partially 18% 30%
recognized
excessively 14% 12%
recognized
</table>
<tableCaption confidence="0.996727">
Table 5. Error types distribution.
</tableCaption>
<bodyText confidence="0.999926153846154">
From Table 5, we can see that a major source
of errors is related to not recognized aspect
terms. In the future, we would like to experiment
with additional techniques to overcome recall
problem, e.g., using dictionaries or concept tax-
onomies and employ skip-chain CRF, proposed
by Li et al. (2010). Further improvements can also
be made by tuning parameters of CRF learning.
To verify the cross-domain portability of the
system, we are going to test it on a third domain
test dataset without including additional instanc-
es in the training corpus, as proposed by Aue and
Gamon (2005).
</bodyText>
<sectionHeader confidence="0.999289" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999898375">
In this paper, we have presented a CRF-based
learning technique applied to the aspect extrac-
tion task. We implemented rich set of lexical,
syntactic and statistical features and showed that
our approach has good domain portability and
performance ranked first out of 28 participating
teams in the laptop domain and fourth in restau-
rant domain.
</bodyText>
<page confidence="0.997128">
312
</page>
<sectionHeader confidence="0.989725" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998116697916667">
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case
study. In Proceedings of Recent Advances in Natu-
ral Language Processing, RANLP-2005.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan
McDonald, Tyler Neylon, George A. Reis, and Jeff
Reynar. 2008. Building a sentiment summarizer for
local service reviews. In Proceedings of WWW-
2008 workshop on NLP in the Information Explo-
sion Era.
V. S. Jagtap and Karishma Pawar. 2012. Analysis of
different approaches to Sentence-Level Sentiment
Classification. International Journal of Scientific
Engineering and Technology, Volume 2, Issue 3.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain set-
ting with conditional random fields. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP’10.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of In-
terntional Conference on Computational Linguis-
tics, COLING’04.
Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and Yuji
Matsumoto. 2006. Opinion mining on the Web by
extracting subject-attribute-value relations. In Pro-
ceedings of AAAI-CAAW ’06.
John Lafferty, Andrew McCallum, and Fernando Pe-
reira. 2001. Conditional Random Fields: Probabil-
istic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning,
ICML’01.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Struc-
ture-aware review mining and summarization. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING’10.
Bing Liu. 2012. Sentiment Analysis and Opinion
Mining.
Samaneh Moghaddam and Martin Ester. 2012. As-
pect-based opinion mining from online reviews.
Tutorial at SIGIR Conference.
Arjun Mukherjee and Bing Liu. 2012. Aspect Extrac-
tion through Semi-Supervised Modeling. In Pro-
ceedings of 50th Anunal Meeting of Association for
Computational Linguistics, ACL’12.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP’02.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Conference on Empirical Methods
in Natural Language Processing, EMNLP’05.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of the IEEE, 77(2): p.
257-286.
Maksim Tkachenko and Andrey Simanovsky. 2012.
Named Entity Recognition: Exploring Features. In
Proceedings of KONVENS’12.
James Todhunter, Igor Sovpel and Dzi-
anis Pastanohau. System and method for automatic
semantic labeling of natural language texts. U.S.
Patent 8 583 422, November 12, 2013.
James Todhunter, Igor Sovpel and Dzi-
anis Pastanohau. Question-answering system and
method based on semantic labeling of text docu-
ments and user questions. U.S. Patent 8 666 730,
September 16, 2014.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual
Meeting of the Association for Computational Lin-
guistics, ACL’02.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
Human Language Technology Conference and the
Conference on Empirical Methods in Natural Lan-
guage Processing, HLT/EMNLP’05.
Lei Zhang and Bing Liu. 2014. Aspect and Entity
Extraction for Opinion Mining. Data Mining and
Knowledge Discovery for Big Data.
Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie
review mining and summarization. In Proceedings
of ACM International Conference on Information
and Knowledge Management, CIKM’06.
Maria Pontiki, Dimitrios Galanis, John Pavlopoulos,
Haris Papageorgiou, Ion Androutsopoulos,
and Suresh Manandhar. 2014. SemEval-2014 Task
4: Aspect Based Sentiment Analysis. In Proceed-
ings of the 8th International Workshop on Seman-
tic Evaluation (SemEval 2014), Dublin, Ireland.
</reference>
<page confidence="0.999498">
313
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.730656">
<title confidence="0.9601325">IHS R&amp;D Belarus: Cross-domain Extraction of Product using Conditional Random Fields</title>
<author confidence="0.907357">Maryna</author>
<affiliation confidence="0.998012">IHS Inc. / IHS Global</affiliation>
<address confidence="0.9819705">131 Starovilenskaya 220123, Minsk, Belarus</address>
<email confidence="0.998435">Marina.Chernyshevich@ihs.com</email>
<abstract confidence="0.994029823529412">This paper describes the aspect extraction system submitted by IHS R&amp;D Belarus team at the SemEval-2014 shared task related to Aspect-Based Sentiment Analysis. Our system is based on IHS Goldfire linguistic processor and uses a rich set of lexical, syntactic and statistical features in CRF model. We participated in two tasks and the same system trained on a mixed corpus of reviews. Among submissions of constrained systems from 28 teams, our submission was ranked first in laptop domain and fourth in restaurant domain for the subtask A devoted to aspect extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: a case study.</title>
<date>2005</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing, RANLP-2005.</booktitle>
<contexts>
<context position="3876" citStr="Aue and Gamon, 2005" startWordPosition="585" endWordPosition="588"> 1134 Table 1. Distribution of the provided data. Many studies showed that sentiment analysis is very sensitive to the source domain (training corpus domain) and performs poorly on data from other domain (Jakob and Gurevych, 2010). This restriction limits the applicability of in309 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 309–313, Dublin, Ireland, August 23-24, 2014. domain models to a wide domain diversity of reviews. One of the common approaches to develop a cross-domain system is training on a mixture of labeled data from different domains (Aue and Gamon, 2005). Cross-domain approach has the advantage of better portability, but it suffers from lower accuracy compared to in-domain aspect extraction. Our cross-domain system is trained on mixed training data, and the same model was used unchanged for classification of both domain-specific test datasets. 2 System Description Aspect extraction may be considered as a sequence labeling task because the product aspects occur at a sequence in a sentence (Liu, 2014). One of the state-of-the-art methods used for sequence labeling is Conditional Random Fields (CRF) (Lafferty, 2001). This method takes as an inpu</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case study. In Proceedings of Recent Advances in Natural Language Processing, RANLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
<author>Tyler Neylon</author>
<author>George A Reis</author>
<author>Jeff Reynar</author>
</authors>
<title>Building a sentiment summarizer for local service reviews.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW2008 workshop on NLP in the Information Explosion Era.</booktitle>
<contexts>
<context position="2119" citStr="Blair-Goldensohn et al., 2008" startWordPosition="319" endWordPosition="322">his information can be insufficient for customers who are seeking opinions on specific product features (aspects) such as design, battery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etzioni, 2005; Blair-Goldensohn et al., 2008), exploiting opinions (Zhuang et al., 2006; Kobayashi, 2006), and supervised learning (Mukherjee and Liu, 2012). In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises (SemEval-2014) and focused on the phrase-level sentiment classification, namely aspect extraction (Pontiki et al., 2014). An aspect term means particular feature of a product or service used in opinion-bearing sentences (My phone has amazing screen), as well as in neutral sentences (The screen </context>
</contexts>
<marker>Blair-Goldensohn, Hannan, McDonald, Neylon, Reis, Reynar, 2008</marker>
<rawString>Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDonald, Tyler Neylon, George A. Reis, and Jeff Reynar. 2008. Building a sentiment summarizer for local service reviews. In Proceedings of WWW2008 workshop on NLP in the Information Explosion Era.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V S Jagtap</author>
<author>Karishma Pawar</author>
</authors>
<title>Analysis of different approaches to Sentence-Level Sentiment Classification.</title>
<date>2012</date>
<journal>International Journal of Scientific Engineering and Technology,</journal>
<volume>2</volume>
<marker>Jagtap, Pawar, 2012</marker>
<rawString>V. S. Jagtap and Karishma Pawar. 2012. Analysis of different approaches to Sentence-Level Sentiment Classification. International Journal of Scientific Engineering and Technology, Volume 2, Issue 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting opinion targets in a single- and cross-domain setting with conditional random fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP’10.</booktitle>
<contexts>
<context position="3486" citStr="Jakob and Gurevych, 2010" startWordPosition="524" endWordPosition="527">he target entities from two domains: restaurants (3041 sentences) and laptops (3045 sentences). The results were evaluated separately in each domain. Table 1 shows the distribution of the provided data for each domain dataset, training and testing set, with number of sentences and aspects. Laptops Restaurants Training Sentences 3045 3041 Aspects 2358 3693 Testing Sentences 800 800 Aspects 654 1134 Table 1. Distribution of the provided data. Many studies showed that sentiment analysis is very sensitive to the source domain (training corpus domain) and performs poorly on data from other domain (Jakob and Gurevych, 2010). This restriction limits the applicability of in309 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 309–313, Dublin, Ireland, August 23-24, 2014. domain models to a wide domain diversity of reviews. One of the common approaches to develop a cross-domain system is training on a mixture of labeled data from different domains (Aue and Gamon, 2005). Cross-domain approach has the advantage of better portability, but it suffers from lower accuracy compared to in-domain aspect extraction. Our cross-domain system is trained on mixed training data, and the sa</context>
<context position="4726" citStr="Jakob and Gurevych, 2010" startWordPosition="717" endWordPosition="720">ed unchanged for classification of both domain-specific test datasets. 2 System Description Aspect extraction may be considered as a sequence labeling task because the product aspects occur at a sequence in a sentence (Liu, 2014). One of the state-of-the-art methods used for sequence labeling is Conditional Random Fields (CRF) (Lafferty, 2001). This method takes as an input a sequence of tokens, calculates the probabilities of the various possible labelings and chooses the one with the maximum probability. We decided to deviate from Inside-OutsideBegin (IOB) scheme used by Jakob and Gurevych (Jakob and Gurevych, 2010) and Li (Li et al., 2010) and introduced the following labels: FA for the attribute word preceding head word of a noun group; FH for the head word of a noun group; FPA for attribute word after head word of a noun group (Microsoft Office 2003), and O for other non-aspect tokens. The following is an example of our suggested tagging: I/O want/O to/O unplug/O the/O external/FA keyboard/FH. Our experiments showed that the words used in aspect terms are easier to recognize when they are always tagged with the same tags. For example, let’s consider the tagging of the word “camera” in the following ca</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single- and cross-domain setting with conditional random fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of Interntional Conference on Computational Linguistics, COLING’04.</booktitle>
<contexts>
<context position="1477" citStr="Kim and Hovy, 2004" startWordPosition="229" endWordPosition="232"> With a rapid growth of the blogs, forums, review sites and social networks, more and more people express their personal views about products on the Internet in form of reviews, ratings, or recommendations. This is a great source of data used by many researchers and commercial applications that are focused on the sentiment analysis to determine customer opinions. Sentiment analysis can be done on document, sentence, and phrase level (Jagtap, V. S., Karishma Pawar, 2013). Earlier works were focused mainly on the document (Turney, 2002; Pang, Lee and Vaithyanathan, 2002) and the sentence level (Kim and Hovy, 2004). However, this information can be insufficient for customers who are seeking opinions on specific product features (aspects) such as design, battery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etz</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of Interntional Conference on Computational Linguistics, COLING’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Opinion mining on the Web by extracting subject-attribute-value relations.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-CAAW ’06.</booktitle>
<marker>Kobayashi, Iida, Inui, Matsumoto, 2006</marker>
<rawString>Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Opinion mining on the Web by extracting subject-attribute-value relations. In Proceedings of AAAI-CAAW ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML’01.</booktitle>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Chao Han</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
<author>Ying-Ju Xia</author>
<author>Shu Zhang</author>
<author>Hao Yu</author>
</authors>
<title>Structure-aware review mining and summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING’10.</booktitle>
<contexts>
<context position="4751" citStr="Li et al., 2010" startWordPosition="723" endWordPosition="726"> both domain-specific test datasets. 2 System Description Aspect extraction may be considered as a sequence labeling task because the product aspects occur at a sequence in a sentence (Liu, 2014). One of the state-of-the-art methods used for sequence labeling is Conditional Random Fields (CRF) (Lafferty, 2001). This method takes as an input a sequence of tokens, calculates the probabilities of the various possible labelings and chooses the one with the maximum probability. We decided to deviate from Inside-OutsideBegin (IOB) scheme used by Jakob and Gurevych (Jakob and Gurevych, 2010) and Li (Li et al., 2010) and introduced the following labels: FA for the attribute word preceding head word of a noun group; FH for the head word of a noun group; FPA for attribute word after head word of a noun group (Microsoft Office 2003), and O for other non-aspect tokens. The following is an example of our suggested tagging: I/O want/O to/O unplug/O the/O external/FA keyboard/FH. Our experiments showed that the words used in aspect terms are easier to recognize when they are always tagged with the same tags. For example, let’s consider the tagging of the word “camera” in the following cases: “camera” and “compac</context>
<context position="15198" citStr="Li et al. (2010)" startWordPosition="2394" endWordPosition="2397">s are recall and precision errors respectively, partial aspect extraction yields both recall and precision errors. A summary of the errors on test dataset is presented in Table 5. laptops restaurants not recognized 68% 58% partially 18% 30% recognized excessively 14% 12% recognized Table 5. Error types distribution. From Table 5, we can see that a major source of errors is related to not recognized aspect terms. In the future, we would like to experiment with additional techniques to overcome recall problem, e.g., using dictionaries or concept taxonomies and employ skip-chain CRF, proposed by Li et al. (2010). Further improvements can also be made by tuning parameters of CRF learning. To verify the cross-domain portability of the system, we are going to test it on a third domain test dataset without including additional instances in the training corpus, as proposed by Aue and Gamon (2005). 3 Conclusion In this paper, we have presented a CRF-based learning technique applied to the aspect extraction task. We implemented rich set of lexical, syntactic and statistical features and showed that our approach has good domain portability and performance ranked first out of 28 participating teams in the lap</context>
</contexts>
<marker>Li, Han, Huang, Zhu, Xia, Zhang, Yu, 2010</marker>
<rawString>Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware review mining and summarization. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2012</date>
<contexts>
<context position="2230" citStr="Liu, 2012" startWordPosition="336" endWordPosition="337">attery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etzioni, 2005; Blair-Goldensohn et al., 2008), exploiting opinions (Zhuang et al., 2006; Kobayashi, 2006), and supervised learning (Mukherjee and Liu, 2012). In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises (SemEval-2014) and focused on the phrase-level sentiment classification, namely aspect extraction (Pontiki et al., 2014). An aspect term means particular feature of a product or service used in opinion-bearing sentences (My phone has amazing screen), as well as in neutral sentences (The screen brightness automatically adjusts). The organizers of SemEval-2014 task have provided a dataset of customer revi</context>
<context position="8767" citStr="Liu, 2012" startWordPosition="1369" endWordPosition="1370">iews from Epinions.com, Amazon.com and TripAdvisor.com. To make corpus more precise, we included only 5-star reviews in our positive corpus, and 1-star reviews in our negative corpus. • Frequency of token occurrence is represented by five values ranging from very frequent to very rare words with an experimentally determined threshold. The frequency was obtained by dividing the number of reviews containing the token by the total number of reviews. The reason of using this as a feature is that people usually comment on the same product aspects and the vocabulary that they use usually converges (Liu, 2012). • Opinion target feature is a binary feature that indicates whether a token is a part of an item which opinions are expressed on and comes from the rule-based sentiment analysis integrated in the predictive question-answering component of the IHS Goldfire linguistic processor. Opinion target can be a product feature as well as a product itself. Noun phrase features: • Role of a token in a noun phrase: head word or attribute word. • Noun phrase introduction feature marks all tokens of noun phrase beginning with possessive pronoun, demonstrative pronoun, definite or indefinite article. • Numbe</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samaneh Moghaddam</author>
<author>Martin Ester</author>
</authors>
<title>Aspect-based opinion mining from online reviews. Tutorial at SIGIR Conference.</title>
<date>2012</date>
<contexts>
<context position="1963" citStr="Moghaddam and Ester, 2012" startWordPosition="297" endWordPosition="300">lier works were focused mainly on the document (Turney, 2002; Pang, Lee and Vaithyanathan, 2002) and the sentence level (Kim and Hovy, 2004). However, this information can be insufficient for customers who are seeking opinions on specific product features (aspects) such as design, battery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etzioni, 2005; Blair-Goldensohn et al., 2008), exploiting opinions (Zhuang et al., 2006; Kobayashi, 2006), and supervised learning (Mukherjee and Liu, 2012). In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises (SemEval-2014) and focused on the phrase-level sentiment classification, namely aspect extraction (Pontiki et al., 2014). An aspect term me</context>
</contexts>
<marker>Moghaddam, Ester, 2012</marker>
<rawString>Samaneh Moghaddam and Martin Ester. 2012. Aspect-based opinion mining from online reviews. Tutorial at SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Aspect Extraction through Semi-Supervised Modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of 50th Anunal Meeting of Association for Computational Linguistics, ACL’12.</booktitle>
<contexts>
<context position="2230" citStr="Mukherjee and Liu, 2012" startWordPosition="334" endWordPosition="337">h as design, battery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etzioni, 2005; Blair-Goldensohn et al., 2008), exploiting opinions (Zhuang et al., 2006; Kobayashi, 2006), and supervised learning (Mukherjee and Liu, 2012). In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises (SemEval-2014) and focused on the phrase-level sentiment classification, namely aspect extraction (Pontiki et al., 2014). An aspect term means particular feature of a product or service used in opinion-bearing sentences (My phone has amazing screen), as well as in neutral sentences (The screen brightness automatically adjusts). The organizers of SemEval-2014 task have provided a dataset of customer revi</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2012. Aspect Extraction through Semi-Supervised Modeling. In Proceedings of 50th Anunal Meeting of Association for Computational Linguistics, ACL’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP’02.</booktitle>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP’05.</booktitle>
<contexts>
<context position="2087" citStr="Popescu and Etzioni, 2005" startWordPosition="315" endWordPosition="318">and Hovy, 2004). However, this information can be insufficient for customers who are seeking opinions on specific product features (aspects) such as design, battery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etzioni, 2005; Blair-Goldensohn et al., 2008), exploiting opinions (Zhuang et al., 2006; Kobayashi, 2006), and supervised learning (Mukherjee and Liu, 2012). In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises (SemEval-2014) and focused on the phrase-level sentiment classification, namely aspect extraction (Pontiki et al., 2014). An aspect term means particular feature of a product or service used in opinion-bearing sentences (My phone has amazing screen), as well as i</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--286</pages>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. In Proceedings of the IEEE, 77(2): p. 257-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maksim Tkachenko</author>
<author>Andrey Simanovsky</author>
</authors>
<title>Named Entity Recognition: Exploring Features.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS’12.</booktitle>
<contexts>
<context position="6408" citStr="Tkachenko and Simanovsky, 2012" startWordPosition="983" endWordPosition="986"> → “wouldn’t”), partof-speech tagging, parsing, noun phrase extraction, semantic role labeling within expanded Subject-Action-Object (eSAO) relations (Todhunter et al., 2013), named entity recognition, labeling for predictive question-answering including rule-based sentiment analysis (Todhunter et al., 2014). In addition, we designed some simple rules to detect entity boundaries that take precedence over CRF labeling. For example, in the sentence “I run Final Cut Pro 7 and a few other applications”, our boundary detector recognizes “Final Cut Pro 7” as an entity represented by a single token (Tkachenko and Simanovsky, 2012). 2.2 Features Below we will describe the features used in CRF model to represent the current token, two previous and two next tokens. Word features: • Token feature represents a base form of a token (word or entity) normalized by case folding. The vocabulary of terms is pretty compact within one domain, so this feature can have considerable impact on terms extraction performance. • Part of speech feature represents the partof-speech tag of the current token with slight generalization, for example, the NNS tag (plural noun) is mapped to NN (singular noun). • Named entity feature labels named e</context>
</contexts>
<marker>Tkachenko, Simanovsky, 2012</marker>
<rawString>Maksim Tkachenko and Andrey Simanovsky. 2012. Named Entity Recognition: Exploring Features. In Proceedings of KONVENS’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Todhunter</author>
</authors>
<title>Igor Sovpel and Dzianis Pastanohau. System and method for automatic semantic labeling of natural language texts.</title>
<date>2013</date>
<journal>U.S. Patent</journal>
<volume>8</volume>
<pages>422</pages>
<marker>Todhunter, 2013</marker>
<rawString>James Todhunter, Igor Sovpel and Dzianis Pastanohau. System and method for automatic semantic labeling of natural language texts. U.S. Patent 8 583 422, November 12, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Todhunter</author>
</authors>
<title>Igor Sovpel and Dzianis Pastanohau. Question-answering system and method based on semantic labeling of text documents and user questions.</title>
<date>2014</date>
<journal>U.S. Patent</journal>
<volume>8</volume>
<pages>730</pages>
<marker>Todhunter, 2014</marker>
<rawString>James Todhunter, Igor Sovpel and Dzianis Pastanohau. Question-answering system and method based on semantic labeling of text documents and user questions. U.S. Patent 8 666 730, September 16, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics, ACL’02.</booktitle>
<contexts>
<context position="1397" citStr="Turney, 2002" startWordPosition="218" endWordPosition="219">ant domain for the subtask A devoted to aspect extraction. 1 Introduction With a rapid growth of the blogs, forums, review sites and social networks, more and more people express their personal views about products on the Internet in form of reviews, ratings, or recommendations. This is a great source of data used by many researchers and commercial applications that are focused on the sentiment analysis to determine customer opinions. Sentiment analysis can be done on document, sentence, and phrase level (Jagtap, V. S., Karishma Pawar, 2013). Earlier works were focused mainly on the document (Turney, 2002; Pang, Lee and Vaithyanathan, 2002) and the sentence level (Kim and Hovy, 2004). However, this information can be insufficient for customers who are seeking opinions on specific product features (aspects) such as design, battery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect</context>
<context position="8105" citStr="Turney, 2002" startWordPosition="1256" endWordPosition="1257">related words, e.g., “Monday”), nationality, word of reasoning (e.g., “decision”, “reason”), etc. • Semantic orientation (SO) score of token represents a low, mean or high SO score as separate feature values (the thresholds were determined experimentally). The SO of a word indicates the strength of its association with positive and negative reviews. We calculated SO of each word w using Pointwise Mutual Information (PMI) measures as SO (w) = PMI(w, pr) – PMI(w, nr), 310 where PMI is the amount of information that we acquire about the presence of the word in positive pr or negative reviews nr (Turney, 2002). For the calculation of SO score, we used rated reviews from Epinions.com, Amazon.com and TripAdvisor.com. To make corpus more precise, we included only 5-star reviews in our positive corpus, and 1-star reviews in our negative corpus. • Frequency of token occurrence is represented by five values ranging from very frequent to very rare words with an experimentally determined threshold. The frequency was obtained by dividing the number of reviews containing the token by the total number of reviews. The reason of using this as a feature is that people usually comment on the same product aspects </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of Annual Meeting of the Association for Computational Linguistics, ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP’05.</booktitle>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Bing Liu</author>
</authors>
<title>Aspect and Entity Extraction for Opinion Mining. Data Mining and Knowledge Discovery for Big Data.</title>
<date>2014</date>
<marker>Zhang, Liu, 2014</marker>
<rawString>Lei Zhang and Bing Liu. 2014. Aspect and Entity Extraction for Opinion Mining. Data Mining and Knowledge Discovery for Big Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACM International Conference on Information and Knowledge Management, CIKM’06.</booktitle>
<contexts>
<context position="2161" citStr="Zhuang et al., 2006" startWordPosition="325" endWordPosition="328"> are seeking opinions on specific product features (aspects) such as design, battery life, or screen. This fine-grained classification is a topic of asThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons. org/licenses/by/4.0/ pect-based sentiment analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etzioni, 2005; Blair-Goldensohn et al., 2008), exploiting opinions (Zhuang et al., 2006; Kobayashi, 2006), and supervised learning (Mukherjee and Liu, 2012). In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises (SemEval-2014) and focused on the phrase-level sentiment classification, namely aspect extraction (Pontiki et al., 2014). An aspect term means particular feature of a product or service used in opinion-bearing sentences (My phone has amazing screen), as well as in neutral sentences (The screen brightness automatically adjusts). The org</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie review mining and summarization. In Proceedings of ACM International Conference on Information and Knowledge Management, CIKM’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Haris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>SemEval-2014 Task 4: Aspect Based Sentiment Analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2544" citStr="Pontiki et al., 2014" startWordPosition="378" endWordPosition="381">ent analysis (Moghaddam and Ester, 2012). Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (Popescu and Etzioni, 2005; Blair-Goldensohn et al., 2008), exploiting opinions (Zhuang et al., 2006; Kobayashi, 2006), and supervised learning (Mukherjee and Liu, 2012). In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises (SemEval-2014) and focused on the phrase-level sentiment classification, namely aspect extraction (Pontiki et al., 2014). An aspect term means particular feature of a product or service used in opinion-bearing sentences (My phone has amazing screen), as well as in neutral sentences (The screen brightness automatically adjusts). The organizers of SemEval-2014 task have provided a dataset of customer reviews with annotated aspects of the target entities from two domains: restaurants (3041 sentences) and laptops (3045 sentences). The results were evaluated separately in each domain. Table 1 shows the distribution of the provided data for each domain dataset, training and testing set, with number of sentences and a</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 Task 4: Aspect Based Sentiment Analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), Dublin, Ireland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>