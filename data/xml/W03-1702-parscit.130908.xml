<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.980739">
Class Based Sense Definition Model for Word Sense Tagging and Disambiguation
</title>
<author confidence="0.999065">
Tracy Lin
</author>
<affiliation confidence="0.9946975">
Department of Communication Engineering
National Chiao Tung University,
</affiliation>
<address confidence="0.865162">
1001, Ta Hsueh Road, Hsinchu, 300, Taiwan, ROC
</address>
<email confidence="0.997213">
tracylin@cm.nctu.edu.tw
</email>
<sectionHeader confidence="0.993867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999414521739131">
We present an unsupervised learning
strategy for word sense disambiguation
(WSD) that exploits multiple linguistic
resources including a parallel corpus, a bi-
lingual machine readable dictionary, and a
thesaurus. The approach is based on Class
Based Sense Definition Model (CBSDM)
that generates the glosses and translations
for a class of word senses. The model can
be applied to resolve sense ambiguity for
words in a parallel corpus. That sense
tagging procedure, in effect, produces a
semantic bilingual concordance, which
can be used to train WSD systems for the
two languages involved. Experimental re-
sults show that CBSDM trained on
Longman Dictionary of Contemporary
English, English-Chinese Edition
(LDOCE E-C) and Longman Lexicon of
Contemporary English (LLOCE) is very
effectively in turning a Chinese-English
parallel corpus into sense tagged data for
development of WSD systems.
</bodyText>
<sectionHeader confidence="0.998266" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999636384615385">
Word sense disambiguation has been an important
research area for over 50 years. WSD is crucial for
many applications, including machine translation,
information retrieval, part of speech tagging, etc.
Ide and Veronis (1998) pointed out the two major
problems of WSD: sense tagging and data sparse-
ness. On one hand, tagged data are very difficult to
come by, since sense tagging is considerably more
difficult than other forms of linguistic annotation.
On the other hand, although the data sparseness is
a common problem, it is especially severe for
WSD. The problems were attacked in various ways.
Yarowsky (1992) showed a class-based approach
</bodyText>
<author confidence="0.82951">
Jason S. Chang
</author>
<affiliation confidence="0.998319">
Department of Computer Science
National Tsing Hua University
</affiliation>
<address confidence="0.72245">
101, Kuangfu Road, Hsinchu, 300, Taiwan, ROC
</address>
<email confidence="0.985512">
jschang@cs.nthu.edu.tw
</email>
<bodyText confidence="0.999317877192983">
under which a very large untagged corpus and the-
saurus can be used effectively for unsupervised
training for noun homograph disambiguation.
However, the method does not offer a method that
explicitly produces sense tagged data for any given
sense inventory. Li and Huang (1999) described a
similar unsupervised approach for Chinese text
based on a Chinese thesaurus. As noted in Meri-
aldo (1994), even minimal hand tagging improved
on the results of unsupervised methods. Yarowsky
(1995) showed that the learning strategy of boot-
strapping from small tagged data led to results ri-
valing supervised training methods. Li and Li
(2002) extended the approach by using corpora in
two languages to bootstrap the learning process.
They showed bilingual bootstrapping is even more
effective. The bootstrapping approach is limited by
lack of a systematic procedure of preparing seed
data for any word in a given sense inventory. The
approach also suffers from errors propagating from
one iteration into the next. Li and Huang
Another alternative involves using a parallel
corpus as a surrogate for tagged data. Gale, Church
and Yarowsky (1992) exploited the so-called one
sense per translation constraint for WSD. They
reported high precision rates of a WSD system for
two-way disambiguation of six English nouns
based on their translations in an English-French
Parallel corpus. However, when working with a
particular sense inventory, there is no obvious way
to know whether the one sense per translation con-
straint holds or how to determine the relevant
translations automatically.
Diab and Resnik (2002) extended the transla-
tion-based learning strategy with a weakened con-
straint that many instances of a word in a parallel
corpus often correspond to lexically varied but se-
mantically consistent translations. They proposed
to group those translations into a target set, which
can be automatically tagged with correct senses
based on the hypernym hierarchy of WordNet.
Diab and Resnik’s work represents a departure
from previous unsupervised approaches in that no
seed data is needed and explicit tagged data are
produced for a given sense inventory (WordNet in
their case). The system trained on the tagged data
was shown to be on a par with the best “supervised
training” systems in SENSEVAL-2 competition.
However, Diab and Resnik’s method is only appli-
cable to nominal WordNet senses. Moreover, the
method is seriously hampered by noise and seman-
tic inconsistency in a target set. Worse still, it is
not always possible to rely on the hypernym hier-
archy for tagging a target set. For instance, the
relevant senses of the target set of {serve, tee off}
for the Chinese counterpart [faqiu] do not
have a common hypernym:
</bodyText>
<figure confidence="0.990092857142857">
Sense 15
serve – (put the ball into play; as in games like tennis)
b move – (have a turn; make one’s move in a game)
Sense 1
Tee off – (strike a golf ball from a tee at the start of a game)
b play – (participating in game or sports)
b compete – (compete for something)
</figure>
<bodyText confidence="0.997992476190476">
This paper describes a new WSD approach to
simultaneously attack the problems of tagging and
data sparseness. The approach assumes the avail-
ability of a parallel corpus of text written in E (the
first language, L1+) and C (the second language,
L2), an L1 to L2 bilingual machine readable dic-
tionary M, and a L1 thesaurus T. A so-called Mu-
tually Assured Resolution of Sense Algorithm
(MARS) and Class Based Sense Definition Model
(CBSDM) are proposed to identify the word senses
in I for each word in a semantic class of words L in
T. Unlike Diab and Resnik, we do not apply the
MARS algorithm directly to target sets to avoid
the noisy words therein. The derived classes senses
and their relevant glosses in L1 and L2 make it
possible to build Class Based Sense Definition and
Translation Models (CBSDM and CBSTM), which
subsequently can be applied to assign sense tags to
words in a parallel corpus.
The main idea is to exploit the defining L1 and
L2 words in the glosses to resolve the sense ambi-
</bodyText>
<footnote confidence="0.9508984">
+ This has nothing to do with the direction of translation and is
not to be confused with the native and second language dis-
tinction made in the literature of Teaching English As a Sec-
ond Language (TESL) and Computer Assisted Language
Learning.
</footnote>
<bodyText confidence="0.978143538461539">
guity. For instance, for the class containing “serve”
and “tee off,” the approach exploits common defin-
ing words, including “ball” and “game” in two
relevant serve-15 and tee off-1 to assign the cor-
rect senses to “serve” and “tee off.” The character
bigram [faqiu] in an English-Chinese
MRD:
serve v 10 [I∅; T1] to begin play by striking (the
ball) to the opponent (LDOCE E-C p.
1300),
would make it possible to align and sense tag
“serve” or “tee off” in a parallel corpus such as the
bilingual citations in Example 1:
</bodyText>
<equation confidence="0.9143475">
(1C)
(1E) drink a capful before teeing off at each hole.
(Source: Sinorama, 1999, Nov. Issue, p.15, Who
Played the First Stroke?).
</equation>
<bodyText confidence="0.999406">
That effectively attaches semantic information to
bilingual citations and turns a parallel corpus into a
Bilingual Semantic Concordance (BSC). The BSC
enables us to simultaneously attack two critical
WSD problems of sense tagging difficulties and
data sparseness, thus provides an effective ap-
proach to WSD. BSC also embodies a projection
of the sense inventory from L1 onto L2, thus cre-
ates a new sense inventory and semantic concor-
dance for L2. If I is based on WordNet for English,
it is then possible to obtain an L2 WordNet. There
are many additional applications of BSC, including
bilingual lexicography, cross language information
retrieval, and computer assisted language learning.
The remainder of the paper is organized as fol-
lows: Sections 2 and 3 lay out the approach and
describe the MARS and SWAT algorithms. Sec-
tion 4 describes experiments and evaluation. Sec-
tion 5 contains discussion and we conclude in
Section 6.
</bodyText>
<sectionHeader confidence="0.594663" genericHeader="method">
2. Class Based Sense Definition Model
</sectionHeader>
<bodyText confidence="0.930791666666667">
We will first illustrate our approach with an exam-
ple. A formal treatment of the approach will follow
in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.99696">
2.1 An example
</subsectionHeader>
<bodyText confidence="0.999919416666667">
To make full use of existing machine readable dic-
tionaries and thesauri, some kind of linkage and
integration is necessary (Knight and Luk, 1994).
Therefore, we are interested in linking thesaurus
classes and MRD senses: Given a thesaurus class S,
it is important that the relevant senses for each
word w in S is determined in a MRD-based sense
inventory I. We will show such linkage is useful
for WSD and is feasible, based solely on the words
of the glosses in I. For instance, given the follow-
ing set of word (N060) in Longman Lexicon of
Contemporary English (McArthur 1992):
</bodyText>
<equation confidence="0.744023">
L = {difficult, hard, stiff, tough, arduous, awkward}.
</equation>
<bodyText confidence="0.992869">
Although those words are highly ambiguous,
the juxtaposition immediately brings to mind the
relevant senses. Specifically for the sense inven-
tory of LDOCE E-C, the relevant senses for L are
as follows:
Therefore, we have the intended senses, S
</bodyText>
<equation confidence="0.8344245">
S = {difficult-1, hard-2, stiff-6, tough-4, arduous-1, awk-
ward-2}.
</equation>
<bodyText confidence="0.997702">
It is reasonable to assume each sense in I is ac-
companied by a sense definition written in the
same language (L1). We use D(S) to denote the
glosses of S. Therefore we have
D(S) = “not easy; hard to do, make, understand, etc.; diffi-
cult to do or understand; difficult to do; difficult to do; not
easy; demanding effort; needing much effort; difficult; not
well made for use; difficult to use; causing difficulty;”
The intuition of bringing out the intended
senses of semantically related words can be for-
malized by Class Based Sense Definition Model
(CBSDM), which is a micro language model gen-
erating D(S), the glosses of S in I. For simplicity,
we assume an unigram language model P(d) that
generates the content words d in the glosses of S.
Therefore, we have
D(S) = “easy hard do make understand difficult do under-
stand difficult do difficult do easy demanding effort need-
ing much effort difficult well made use difficult use causing
difficulty”
If we have the relevant senses, it is a simple
matter of counting to estimate P(d). Conversely,
with P(d) available to us, we can pick the relevant
sense of S in I which is most likely generated by
P(d). The problem of learning the model P(d) lend
itself nicely to an iterative relaxation method such
as the Expectation and Maximization Algorithm
(Dempster, Laird, Rubin, 1977).
Initially, we assume all senses of S word in I is
equally likely and use all the defining words
therein to estimate P(d) regardless of whether they
are relevant. For LDOCE senses, initial estimate of
the relevant glosses is as follows:
D(S) = “easy hard do make understand people unfriendly
quarrelling pleased ... firm stiff broken pressed bent diffi-
cult do understand forceful needing using force body
mind ...bent painful moving moved ... strong weakened
suffer uncomfortable conditions cut worn bro-
ken ...needing effort difficult lacking skill moving body
parts body CLUMSY made use difficult use causing diffi-
culty”
</bodyText>
<tableCaption confidence="0.981824">
Table 1. The initial CBSDM for n-word list {difficult,
hard, stiff, tough, arduous, awkward} based on the rele-
vant and irrelevant LDOCE senses, n = 6.
</tableCaption>
<table confidence="0.999378533333333">
Defining word d Count, k P(d) = k/n
Difficult 5 0.83
Effort 3 0.50
Understand 2 0.33
Bad 2 0.33
Bent 2 0.33
Body 2 0.33
Broken 2 0.33
Difficulty 2 0.33
Easy 2 0.33
Firm 2 0.33
Hard 2 0.33
Moving 2 0.33
Needing 2 0.33
Water 2 0.33
</table>
<bodyText confidence="0.999395923076923">
As evident from Table 1, the initial estimates of
P(d) are quite close to the true probability distribu-
tion (based on the relevant senses only). The three
top ranking defining words “difficult,” “effort,” and
“understand” appear in glosses of relevant senses,
and not in irrelevant senses. Admittedly, there are
still some noisy, irrelevant words such as “bent”
and “broken.” But they do not figure prominently
in the model from the start and will fade out gradu-
ately with successive iterations of re-estimation.
We estimate the probability of a particular sense s
being in S by P(D(s)), the probability of its gloss
under P(d). For intance, we have
</bodyText>
<equation confidence="0.9999995">
P(hard-1) = P(D(hard-1)) = P(“firm and stiff; which ...”),
P(hard-2) = P(D(hard-2)) = P(“difficult to do or understand”).
</equation>
<bodyText confidence="0.999584928571429">
On the other hand, we re-estimate the probabil-
ity P(d) of a defining word d under CBSDM by
how often d appears in a sense s and P(s). P(d) is
positively prepositional to the frequency of d in
D(s) and to the value of P(s). Under that re-
estimation scheme, the defining words in relevant
senses will figure more prominently in CBSDM,
leading to more accurate estimation for probability
of s being in S. For instance, in the first round,
“difficult” in the gloss of hard-2 will weigh twice
more than “firm” in the gloss of irrelevant hard-1,
leading to relatively higher unigram probability for
“difficult.” That in turn makes hard-2 even more
probable than hard-1. See Table 2.
</bodyText>
<tableCaption confidence="0.824267">
Table 2. First round estimates for P(s), the probability of
sense s in S.
</tableCaption>
<table confidence="0.9963526">
Sense* Definition P(s)
hard-1 firm and stiff; which can- 0.2857
not easily be broken
hard-2 difficult to do or under- 0.7143
stand
stiff-1 not easily bent 0.2857
stiff-6 difficult to do 0.7143
* in LDOCE.
** AssumingP(s) ≈ max P(d)
(s)
</table>
<tableCaption confidence="0.897329">
Table 3. Classes Based Sense Translation Model for
hard-2, stiff-6, tough-4,
</tableCaption>
<figure confidence="0.424154333333333">
{difficult-1,
arduous-1, awk-
ward-2} in LDOCE*.
∈
d
D
</figure>
<bodyText confidence="0.93295715625">
in LDOCE E-C and their appearance counts an
d
probability are shown in Table 3.
Often the senses in I are accompanied with
glosses written in a second language (L2); exclu-
sively (as in a simple bilingual word list) or addi-
tionally (as in LDOCE E-C). Either way, the words
in L2 glosses can be incorporated into D(s) and
P(d). For instance, the character unigrams and/or
overlapping bigrams in the Mandarin glosses of S
We call the part of CBSDM that are involved
with words written in L2, Class Based Sense
Translation Model. CBSTM trained on a thesaurus
and a bilingual MRD can be exploited to align
words and translation counter part as well as to
assign word sense in a parallel corpus. For instance,
given a pair of al
igned sentences in a parallel cor-
pus:
(2E) A scholar close to Needham analyses the reasons
that he was able to achieve this huge work as
being due to a combination of factors that
would be hard to find in any other person.
(Source: 1990, Dec Issue Page 24, Giving Jus-
tice Back to China --Dr. Joseph Needham and
the History of Science and Civilisation in China)
It is possible to apply CBSTM to obtain the fol-
lowing pair of translation equivalent, ( [nan],
and, at the same time, determine the in-
tended sense. For instance, we can label the cita-
tion with
leading to the following
</bodyText>
<footnote confidence="0.561748714285714">
quadruple:
(3) (hard,
(2C, 2E))
After we have done this for all pairs of word and
translation counterpart, we would in effect estab-
lish
Semantic Concordan
</footnote>
<equation confidence="0.899433666666667">
“hard”)
hard-2LDOCE,
, hard-2LDOCE
</equation>
<bodyText confidence="0.7129305">
a Bilingual
ce (BSC).
</bodyText>
<subsectionHeader confidence="0.974898">
2.2 The Model
</subsectionHeader>
<bodyText confidence="0.99897125">
We assume that there is a Class Based Sense Defi-
nition Model, which can be viewed as a language
model that generates the glosses for a class of
senses S. Assume that we are given L, the words of
S but not explicitly the intended senses S. In addi-
tion, we are given a sense inventory I in the form
of an MRD with the regular glosses, which are
written in L1 and/or L2. We are concerned with
two problems: (1) Unsupervised training of M,
CBSDM for S; (2) Determining S by identifying a
relevant sense in I, if existing, for each word in L.
Those two problems can be solved based on
Maximum Likelihood Principle: Finding M and S
such that M generates the glosses of S with maxi-
mum probability. For that, we utilize the Expecta-
tion and Maximization Algorithm to derive M and
</bodyText>
<figure confidence="0.773969">
S through Mutually Assured Resolution of Sense
Algorithm (MARS) given below:
Mutual Assured Resolution of Sense Algorithm
Determine the intended sense for each of a set of seman-
tic related words.
Input: (1) Class of words L = {w1 w2 ...wj;
(2) Sense inventory I.
Output: (1) Senses S from I for words in L;
(2) CBSTM M from L1 to L2.
1. Initially, we assume that each of the senses wi,j, j =
1, mi in I is equally probable to be in S with prob-
</figure>
<figureCaption confidence="0.2844085">
the number of senses in I for the word wi.
2. Estimate CBSDM P(d  |L) for L ,
</figureCaption>
<equation confidence="0.933914">
k P(wi,j  |i,L)EQ(d,di,j,k )
P(d  |L) = i ,
n
where d is a unigram or overlapping bigram in L1
or L2, di,j,k = the kth word in D(wi,j), and EQ(x, y)
= 1, if x = y and 0 otherwise;
3. Re-estimate P(wi,j  |i,L) according to di,j,k , k = 1,n i,j :
P (  |, ) 0.5
1 i,j = max i,j,k + ∑ 1
w i L P(  |) 0.5
k i,j,k
d L P(d
k n i,j |),
L
P (  |, )
w i L
1 i,j
P(  |, )
w i L = ;
i,j
∑ P1(wi,j  |i,L)
∑ I(c ∈ti,j *)
P c L i = n
(  |) = 1 ,
n
</equation>
<bodyText confidence="0.995285444444445">
where c is a unigram or overlapping bigram in L2
and ti,j is the L2 gloss of wi,j.
Note that the purpose of Step 2 is to estimate how likely
a word will appear in the definition of S based on the
definining word for the senses, wi,j and relevant prob-
ability P(wi,j  |i,L). This likelihood of the word d being
used to define senses in questions is subsequently used
to re-estimate P(wi,j  |i,L), the likelihood of the jth sense,
wi,j of wi being in the intended senses of L.
</bodyText>
<sectionHeader confidence="0.650226" genericHeader="method">
3. Application to Word Sense Tagging
</sectionHeader>
<figureCaption confidence="0.929452285714286">
Armed with the Class Based Sense Translation
Model, we can attack the word alignment and
sense tagging problems simultaneously. Each word
in a pair of aligned sentences in a parallel corpus
will be considered and assigned a counterpart
translation and intended sense in the given context
through the proposed algorithm below:
</figureCaption>
<subsectionHeader confidence="0.992235">
Simutaneous Word Alignment and Tagging Algorithm (SWAT)
</subsectionHeader>
<bodyText confidence="0.849069666666667">
Align and sense tag words in a give sentence and trans-
lation.
Input: (1) Pair of sentences (E, C);
</bodyText>
<listItem confidence="0.9800603">
(2) Word w, POS p in question;
(3) Sense Inventory I;
(4) CBSTM, P(c|L).
Output: (1) Translation c of w in C;
(2) Intended sense s for w.
1. Perform part of speech tagging on E;
2. Proceed if w with part of speech p is found in the
results of tagging E;
3. For all classes L to which (w, p) belongs and all
words c in C:
</listItem>
<equation confidence="0.966633555555556">
* arg max max
 P c L
(  |)   ,
L =  LINK w c
( , )
L
c* arg
= max(P(c  |L*)) ,
c
</equation>
<bodyText confidence="0.987398333333333">
where LINK(x, y) means x and y are two word
aligned based on Competitive Linking Align-
ment
</bodyText>
<listItem confidence="0.999338">
4. Output c* as the translation;
5. Output the sense of w in L* as the intended sense.
</listItem>
<figure confidence="0.703433">
1
ability P(wi,j  |i, L) = , j = 1, mi; where mi is
mi
j,
max
∑
,
</figure>
<bodyText confidence="0.484211">
j m i
= 1,
</bodyText>
<listItem confidence="0.959243166666667">
4. Repeat Steps 2 and 3 until the values of P(d  |L) and
P(wi,j  |i, L) converge;
5. For each i, find the most probable sense wi,j* ,
j*=argmax j P(wi,j  |i, L) ;
6. Output S = { wi,j*  |j*=argmax j P(wi,j  |i, L)} ;
7. Estimate and output CBSTM for L,
</listItem>
<bodyText confidence="0.947340625">
To make sense tagging more precise, it is advisable
to place constraint on the translation counterpart c
of w. SWAT considers only those translations c
that has been linked with w based the Competitive
“star,” “interest,” “issue,” the adjective “hard,”
and the verb “serve.”
Linking Algorithm (Melamed 1997) and logarith-
mic likelihood ratio (Dunning 1993).
</bodyText>
<tableCaption confidence="0.957296">
Table 4. The experimental results of assigning LDOCE
senses to classes of LLOCE.
</tableCaption>
<sectionHeader confidence="0.914356" genericHeader="method">
4. Experiments and evaluation
</sectionHeader>
<bodyText confidence="0.999363307692308">
In order to assess the feasibility of the proposed
approach, we carried out experiments and evalua-
tion on an implementation of MARS and SWAT
based on LDOCE E-C, LLOCE, and Sinorama.
First experiment was involved with the train-
ability of CBSDM and CBSTM via MARS. The
second experiment was involved with the effec-
tiveness of using SWAT and CBSTM to annotate a
parallel corpus with sense information. Evaluation
was done on a set of 14 nouns, verbs, adjectives,
and adverbs studies in previous work. The set in-
cludes the nouns “bass,” “bow,” “cone,” “duty,”
“gallery,” “mole,” “sentence,” “slug,” “taste,”
</bodyText>
<tableCaption confidence="0.91136">
Table 5. Evaluation of the MARS Algorithm based on
12 nouns, 1 verb, 1 adjective in LDOCE.
</tableCaption>
<table confidence="0.992851444444445">
Word Pos #Senses #Done #Correct Prec Prec.
(LB*)
Bass N 4 1 1 0.25 1.00
Bow N 5 2 2 0.25 1.00
Cone N 3 3 2 0.33 0.67
Duty N 2 2 2 0.13 1.00
Galley N 3 3 2 0.33 0.67
Mole N 3 2 2 0.33 1.00
Sentence N 2 2 2 1.00 1.00
Slug N 2 2 2 0.20 1.00
Taste N 6 1 1 0.17 1.00
Star N 8 2 2 0.13 1.00
Interest N 6 4 4 0.17 1.00
Issue N 7 4 3 0.14 0.75
Serve V 13 4 2 0.08 0.50
Hard A 12 2 2 0.08 1.00
Avg. 4.14 1.36 1.29 0.26 0.90
* The lower bound of precision of picking one sense in random.
</table>
<tableCaption confidence="0.8023885">
Table 6. Experimental results of sense tagging the Sinorama
parallel Corpus.
</tableCaption>
<table confidence="0.991244333333333">
Word Instance #done #correct Precision
Star 173 86 82 0.95
Hard 325 37 33 0.89
</table>
<subsectionHeader confidence="0.929866">
4.1 Experiment 1: Training CBSDM
</subsectionHeader>
<bodyText confidence="0.998499166666667">
We applied MARS to assign LDOCE senses to
word classes in LLOCE. Some results related to
the test set are shown in Tables 4. The evaluation
in Tables indicates that MARS assigns LDOCE
senses to an LLOCE class with a high average pre-
cision rate of 90%.
</bodyText>
<subsectionHeader confidence="0.983478">
4.2 Experiment 2: Sense Tagging
</subsectionHeader>
<bodyText confidence="0.999453166666667">
We applied SWAT to sense tag English words in
some 50,000 reliably aligned sentence pairs in Si-
norama parallel Corpus based on LDOCE sense
inventory. The results are shown in Tables 6.
Evaluation indicates an average precision rate of
around 90%.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="method">
5. Discussion
</sectionHeader>
<bodyText confidence="0.9952025">
The proposed approach offers a new method for
automatic learning for the task of word sense dis-
ambiguation. The class based approach attacks the
problem of tagging and data sparseness in a way
similar to the Yarowsky approach (1992) based on
thesaurus categories. We differ from the
Yarowsky’s approach, in the following ways:
i. The WSD problem is solved for two languages in-
stead of one within a single sense inventory. Fur-
thermore, an explicit sense tagged corpus is
produced in the process.
ii. It is possible to work with any number of sense in-
ventories.
iii. The method is applicable not only to nouns but
also to adjectives and verbs, since it does not rely
on topical context, which is effective only for
nouns as pointed out by Towell and Voorhees
(1998).
The approach is very general and modular and
can work in conjunction with a number of learning
strategies for word sense disambiguation
(Yarowsky, 1995; Li and Li, 2002).
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999479444444445">
In this paper, we present the Mutual Assured Reso-
lution of Sense (MARS) Algorithm for assigning
relevant senses to word classes in a given sense
inventory (i.e. LDOCE or WordNet). We also de-
scribe the SWAT Algorithm for automatic sense
tagging of a parallel corpus.
We carried out experiments on an implementa-
tion of the MARS and SWAT Algorithms for all
the senses in LDOCE and LLOCE. Evaluation on a
set of 14 highly ambiguous words showed that
very high precision CBSDM and CBSTM can be
constructed. High applicability and precision rates
were achieved, when applying CBSTM to sense
tagging of a Chinese-English parallel corpus.
A number of interesting future directions pre-
sent themselves. First, it would be interesting to
see how effectively we can broaden the coverage
of CBSTM via backing off smoothing. Second, a
CBSTM trained directly on a parallel corpus would
be more effective in word alignment and sense
tagging. The approach of training CBSTM on the
L2 glosses in a bilingual MRD may lead to occa-
sional mismatch between MRD translations and in-
context translations. Third, there is a lack of re-
search for a more abstractive and modular repre-
sentation of sense differences and commonality.
There is potential of developing Sense Definition
Model to identify and represent semantic and sty-
listic differentiation reflected in the MRD glosses
pointed out in DiMarco, Hirst and Stede (1993).
Last but not the least, it would be interesting to
apply MARS to both LDOCE E-C and WordNet
and project WordNet’s sense inventory to a sen-
cond language via CBSDM and a parallel corpus,
thus creating a Chinese WordNet and semantic
concordance.
</bodyText>
<sectionHeader confidence="0.969159" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9877175">
We acknowledge the support for this study through
grants from National Science Council and Ministry
of Education, Taiwan (NSC 90-2411-H-007-033-
MC and MOE EX-91-E-FA06-4-4).
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889719512195">
Dagan, Ido; A. Itai, and U. Schwall (1991). Two lan-
guages are more informative than one. Proceedings
of the 29th Annual Meeting of the Association for
Computational Linguistics, 18-21 June 1991, Berke-
ley, California, 130-137.
Dempster, A., N. Laird, and D. Rubin (1977). Maxi-
mum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society,
Series B, 39(1):1–38.
Diab, M. and P. Resnik, (2002). An Unsupervised
Method for Word Sense Tagging using Parallel Cor-
pora, Proceedings of ACL, 255-262.
DiMarco, C., G. Hirst, M. Stede, (1993). &amp;quot;The semantic
and stylistic differentiation of synonyms and near-
synonyms.&amp;quot; In: Working notes of the AAAI Spring
Symposium on Building Lexicons for Machine Trans-
lation. Stanford University.
Dunning, T (1993) Accurate methods for the statistics
of surprise and coincidence, Computational Linguis-
tics 19:1, 61-75.
Gale, W., K. Church, and D. Yarowsky, (1992). Using
Bilingual Materials to Develop Word Sense Disam-
biguation Methods. In Proceedings, Fourth Interna-
tional Conference on Theoretical and
Methodological Issues in Machine Translation.
Montreal, 101-112, 1992.
Ide, N. and J. Véronis (1998). Word sense disambigua-
tion: The state of the art. Computational Linguistics,
24:1, 1-40.
Knight, K, and A. Luk, (1994). Building a Large-Scale
Knowledge Base for Machine Translation, Proc. of
the National Conference on Artificial Intelligence
(AAAI).
Knight, K., I. Chander, M. Haines, V. Hatzivassiloglou,
E. Hovy, M. Iida, S. Luk, A. Okumura, R. Whitney,
K. Yamada, (1994). &amp;quot;Integrating Knowledge Bases
and Statistics in MT, Proc. of the Conference of the
Association for Machine Translation in the Americas
(AMTA).
Leacock, C., G. Towell, and E. Voorhees (1993). Cor-
pus-based statistical sense resolution. Proceedings of
the ARPA Human Language Technology Worskshop,
San Francisco, Morgan Kaufman.
Li, C, and H. Li (2002). Word Translation Disambigua-
tion Using Bilingual Bootstrapping, Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Philadelphia, July 2002,
343-351.
Li, Juanzi and C. Huang (1999). A Model for Word
Sense Disambiguation. In Computational Linguistics
and Chinese Language Processing,4(2), August 1999,
pp.1-20
McArthur, T. (1992) Longman Lexicon of Contempo-
rary English, Longman Group (Far East) Ltd., Hong
Kong.
Mei, J. J., et al. (1984) Tongyici Cilin, Shanghai, Com-
mercial Press. (in Chinese)
Melamed, I.D. (1997). &amp;quot;A Word-to-Word Model of
Translational Equivalence&amp;quot;. In Procs. of the ACL97.
pp 490-497. Madrid Spain.
Merialdo, B, (1994). Tagging English Text with a
Probabilistic Model, Computational Linguistics,
20(2):155-171.
Miller, G., A, R.. Beckwith, C. Fellbaum, D. Gross and
K.J. Miller. (1990). WordNet: An on-line lexical da-
tabase. International Journal of Lexicography, 3(4),
235- 244.
Proctor, P. (1988) Longman English-Chinese Dictionary
of Contemporary English, Longman Group (Far East)
Ltd., Hong Kong.
Towell, G. and E. Voorhees. (1998) Disambiguating
Highly Ambiguous Words. Computational Linguis-
tics, vol. 24, no. 1, 125-146.
Yarowsky, D. (1992). Word sense disambiguation using
statistical models of Roget&apos;s categories trained on
large corpora. Proceedings of the 14th International
Conference on Computational Linguistics,
COLING&apos;92, 23-28 August, Nantes, France, 454-460.
Yarowsky, D. (1995). Unsupervised word sense disam-
biguation rivaling supervised methods. Proceedings
of the 33rd Annual Meeting of the Association for
Computational Linguistics, 189-196
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.785389">
<title confidence="0.99974">Class Based Sense Definition Model for Word Sense Tagging and Disambiguation</title>
<author confidence="0.999995">Tracy Lin</author>
<affiliation confidence="0.9999115">Department of Communication National Chiao Tung University,</affiliation>
<address confidence="0.999535">1001, Ta Hsueh Road, Hsinchu, 300, Taiwan, ROC</address>
<email confidence="0.960969">tracylin@cm.nctu.edu.tw</email>
<abstract confidence="0.983227041666667">We present an unsupervised learning strategy for word sense disambiguation (WSD) that exploits multiple linguistic resources including a parallel corpus, a bilingual machine readable dictionary, and a thesaurus. The approach is based on Class Based Sense Definition Model (CBSDM) that generates the glosses and translations for a class of word senses. The model can be applied to resolve sense ambiguity for words in a parallel corpus. That sense tagging procedure, in effect, produces a semantic bilingual concordance, which can be used to train WSD systems for the two languages involved. Experimental results show that CBSDM trained on Longman Dictionary of Contemporary English, English-Chinese (LDOCE E-C) and Longman Lexicon of Contemporary English (LLOCE) is very effectively in turning a Chinese-English parallel corpus into sense tagged data for development of WSD systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>A Itai</author>
<author>U Schwall</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>18--21</pages>
<location>Berkeley, California,</location>
<marker>Dagan, Itai, Schwall, 1991</marker>
<rawString>Dagan, Ido; A. Itai, and U. Schwall (1991). Two languages are more informative than one. Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, 18-21 June 1991, Berkeley, California, 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A., N. Laird, and D. Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>P Resnik</author>
</authors>
<title>An Unsupervised Method for Word Sense Tagging using Parallel Corpora,</title>
<date>2002</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="3530" citStr="Diab and Resnik (2002)" startWordPosition="533" endWordPosition="536">one iteration into the next. Li and Huang Another alternative involves using a parallel corpus as a surrogate for tagged data. Gale, Church and Yarowsky (1992) exploited the so-called one sense per translation constraint for WSD. They reported high precision rates of a WSD system for two-way disambiguation of six English nouns based on their translations in an English-French Parallel corpus. However, when working with a particular sense inventory, there is no obvious way to know whether the one sense per translation constraint holds or how to determine the relevant translations automatically. Diab and Resnik (2002) extended the translation-based learning strategy with a weakened constraint that many instances of a word in a parallel corpus often correspond to lexically varied but semantically consistent translations. They proposed to group those translations into a target set, which can be automatically tagged with correct senses based on the hypernym hierarchy of WordNet. Diab and Resnik’s work represents a departure from previous unsupervised approaches in that no seed data is needed and explicit tagged data are produced for a given sense inventory (WordNet in their case). The system trained on the ta</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Diab, M. and P. Resnik, (2002). An Unsupervised Method for Word Sense Tagging using Parallel Corpora, Proceedings of ACL, 255-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C DiMarco</author>
<author>G Hirst</author>
<author>M Stede</author>
</authors>
<title>The semantic and stylistic differentiation of synonyms and nearsynonyms.&amp;quot; In: Working notes of the AAAI Spring Symposium on Building Lexicons for Machine Translation.</title>
<date>1993</date>
<institution>Stanford University.</institution>
<marker>DiMarco, Hirst, Stede, 1993</marker>
<rawString>DiMarco, C., G. Hirst, M. Stede, (1993). &amp;quot;The semantic and stylistic differentiation of synonyms and nearsynonyms.&amp;quot; In: Working notes of the AAAI Spring Symposium on Building Lexicons for Machine Translation. Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>61--75</pages>
<contexts>
<context position="18575" citStr="Dunning 1993" startWordPosition="3257" endWordPosition="3258"> m i = 1, 4. Repeat Steps 2 and 3 until the values of P(d |L) and P(wi,j |i, L) converge; 5. For each i, find the most probable sense wi,j* , j*=argmax j P(wi,j |i, L) ; 6. Output S = { wi,j* |j*=argmax j P(wi,j |i, L)} ; 7. Estimate and output CBSTM for L, To make sense tagging more precise, it is advisable to place constraint on the translation counterpart c of w. SWAT considers only those translations c that has been linked with w based the Competitive “star,” “interest,” “issue,” the adjective “hard,” and the verb “serve.” Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). Table 4. The experimental results of assigning LDOCE senses to classes of LLOCE. 4. Experiments and evaluation In order to assess the feasibility of the proposed approach, we carried out experiments and evaluation on an implementation of MARS and SWAT based on LDOCE E-C, LLOCE, and Sinorama. First experiment was involved with the trainability of CBSDM and CBSTM via MARS. The second experiment was involved with the effectiveness of using SWAT and CBSTM to annotate a parallel corpus with sense information. Evaluation was done on a set of 14 nouns, verbs, adjectives, and adverbs studies in prev</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T (1993) Accurate methods for the statistics of surprise and coincidence, Computational Linguistics 19:1, 61-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Using Bilingual Materials to Develop Word Sense Disambiguation Methods. In</title>
<date>1992</date>
<booktitle>Proceedings, Fourth International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<location>Montreal,</location>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W., K. Church, and D. Yarowsky, (1992). Using Bilingual Materials to Develop Word Sense Disambiguation Methods. In Proceedings, Fourth International Conference on Theoretical and Methodological Issues in Machine Translation. Montreal, 101-112, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Véronis</author>
</authors>
<title>Word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>1--40</pages>
<marker>Ide, Véronis, 1998</marker>
<rawString>Ide, N. and J. Véronis (1998). Word sense disambiguation: The state of the art. Computational Linguistics, 24:1, 1-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>A Luk</author>
</authors>
<title>Building a Large-Scale Knowledge Base for</title>
<date>1994</date>
<booktitle>Machine Translation, Proc. of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="8060" citStr="Knight and Luk, 1994" startWordPosition="1314" endWordPosition="1317">information retrieval, and computer assisted language learning. The remainder of the paper is organized as follows: Sections 2 and 3 lay out the approach and describe the MARS and SWAT algorithms. Section 4 describes experiments and evaluation. Section 5 contains discussion and we conclude in Section 6. 2. Class Based Sense Definition Model We will first illustrate our approach with an example. A formal treatment of the approach will follow in Section 2.2. 2.1 An example To make full use of existing machine readable dictionaries and thesauri, some kind of linkage and integration is necessary (Knight and Luk, 1994). Therefore, we are interested in linking thesaurus classes and MRD senses: Given a thesaurus class S, it is important that the relevant senses for each word w in S is determined in a MRD-based sense inventory I. We will show such linkage is useful for WSD and is feasible, based solely on the words of the glosses in I. For instance, given the following set of word (N060) in Longman Lexicon of Contemporary English (McArthur 1992): L = {difficult, hard, stiff, tough, arduous, awkward}. Although those words are highly ambiguous, the juxtaposition immediately brings to mind the relevant senses. Sp</context>
</contexts>
<marker>Knight, Luk, 1994</marker>
<rawString>Knight, K, and A. Luk, (1994). Building a Large-Scale Knowledge Base for Machine Translation, Proc. of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
<author>M Haines</author>
<author>V Hatzivassiloglou</author>
<author>E Hovy</author>
<author>M Iida</author>
<author>S Luk</author>
<author>A Okumura</author>
<author>R Whitney</author>
<author>K Yamada</author>
</authors>
<title>Integrating Knowledge Bases and Statistics</title>
<date>1994</date>
<booktitle>in MT, Proc. of the Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<marker>Knight, Chander, Haines, Hatzivassiloglou, Hovy, Iida, Luk, Okumura, Whitney, Yamada, 1994</marker>
<rawString>Knight, K., I. Chander, M. Haines, V. Hatzivassiloglou, E. Hovy, M. Iida, S. Luk, A. Okumura, R. Whitney, K. Yamada, (1994). &amp;quot;Integrating Knowledge Bases and Statistics in MT, Proc. of the Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G Towell</author>
<author>E Voorhees</author>
</authors>
<title>Corpus-based statistical sense resolution.</title>
<date>1993</date>
<booktitle>Proceedings of the ARPA Human Language Technology Worskshop,</booktitle>
<location>San Francisco, Morgan Kaufman.</location>
<marker>Leacock, Towell, Voorhees, 1993</marker>
<rawString>Leacock, C., G. Towell, and E. Voorhees (1993). Corpus-based statistical sense resolution. Proceedings of the ARPA Human Language Technology Worskshop, San Francisco, Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>H Li</author>
</authors>
<title>Word Translation Disambiguation Using Bilingual Bootstrapping,</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>343--351</pages>
<location>Philadelphia,</location>
<contexts>
<context position="2565" citStr="Li and Li (2002)" startWordPosition="383" endWordPosition="386">very large untagged corpus and thesaurus can be used effectively for unsupervised training for noun homograph disambiguation. However, the method does not offer a method that explicitly produces sense tagged data for any given sense inventory. Li and Huang (1999) described a similar unsupervised approach for Chinese text based on a Chinese thesaurus. As noted in Merialdo (1994), even minimal hand tagging improved on the results of unsupervised methods. Yarowsky (1995) showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods. Li and Li (2002) extended the approach by using corpora in two languages to bootstrap the learning process. They showed bilingual bootstrapping is even more effective. The bootstrapping approach is limited by lack of a systematic procedure of preparing seed data for any word in a given sense inventory. The approach also suffers from errors propagating from one iteration into the next. Li and Huang Another alternative involves using a parallel corpus as a surrogate for tagged data. Gale, Church and Yarowsky (1992) exploited the so-called one sense per translation constraint for WSD. They reported high precisio</context>
<context position="21529" citStr="Li and Li, 2002" startWordPosition="3799" endWordPosition="3802">ng ways: i. The WSD problem is solved for two languages instead of one within a single sense inventory. Furthermore, an explicit sense tagged corpus is produced in the process. ii. It is possible to work with any number of sense inventories. iii. The method is applicable not only to nouns but also to adjectives and verbs, since it does not rely on topical context, which is effective only for nouns as pointed out by Towell and Voorhees (1998). The approach is very general and modular and can work in conjunction with a number of learning strategies for word sense disambiguation (Yarowsky, 1995; Li and Li, 2002). 6. Conclusion In this paper, we present the Mutual Assured Resolution of Sense (MARS) Algorithm for assigning relevant senses to word classes in a given sense inventory (i.e. LDOCE or WordNet). We also describe the SWAT Algorithm for automatic sense tagging of a parallel corpus. We carried out experiments on an implementation of the MARS and SWAT Algorithms for all the senses in LDOCE and LLOCE. Evaluation on a set of 14 highly ambiguous words showed that very high precision CBSDM and CBSTM can be constructed. High applicability and precision rates were achieved, when applying CBSTM to sense</context>
</contexts>
<marker>Li, Li, 2002</marker>
<rawString>Li, C, and H. Li (2002). Word Translation Disambiguation Using Bilingual Bootstrapping, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, 343-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juanzi Li</author>
<author>C Huang</author>
</authors>
<title>A Model for Word Sense Disambiguation.</title>
<date>1999</date>
<booktitle>In Computational Linguistics and Chinese Language Processing,4(2),</booktitle>
<pages>1--20</pages>
<contexts>
<context position="2212" citStr="Li and Huang (1999)" startWordPosition="327" endWordPosition="330">tion. On the other hand, although the data sparseness is a common problem, it is especially severe for WSD. The problems were attacked in various ways. Yarowsky (1992) showed a class-based approach Jason S. Chang Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan, ROC jschang@cs.nthu.edu.tw under which a very large untagged corpus and thesaurus can be used effectively for unsupervised training for noun homograph disambiguation. However, the method does not offer a method that explicitly produces sense tagged data for any given sense inventory. Li and Huang (1999) described a similar unsupervised approach for Chinese text based on a Chinese thesaurus. As noted in Merialdo (1994), even minimal hand tagging improved on the results of unsupervised methods. Yarowsky (1995) showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods. Li and Li (2002) extended the approach by using corpora in two languages to bootstrap the learning process. They showed bilingual bootstrapping is even more effective. The bootstrapping approach is limited by lack of a systematic procedure of preparing seed data </context>
</contexts>
<marker>Li, Huang, 1999</marker>
<rawString>Li, Juanzi and C. Huang (1999). A Model for Word Sense Disambiguation. In Computational Linguistics and Chinese Language Processing,4(2), August 1999, pp.1-20</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McArthur</author>
</authors>
<title>Longman Lexicon of Contemporary English,</title>
<date>1992</date>
<institution>Longman Group (Far East) Ltd., Hong Kong.</institution>
<contexts>
<context position="8492" citStr="McArthur 1992" startWordPosition="1393" endWordPosition="1394">ollow in Section 2.2. 2.1 An example To make full use of existing machine readable dictionaries and thesauri, some kind of linkage and integration is necessary (Knight and Luk, 1994). Therefore, we are interested in linking thesaurus classes and MRD senses: Given a thesaurus class S, it is important that the relevant senses for each word w in S is determined in a MRD-based sense inventory I. We will show such linkage is useful for WSD and is feasible, based solely on the words of the glosses in I. For instance, given the following set of word (N060) in Longman Lexicon of Contemporary English (McArthur 1992): L = {difficult, hard, stiff, tough, arduous, awkward}. Although those words are highly ambiguous, the juxtaposition immediately brings to mind the relevant senses. Specifically for the sense inventory of LDOCE E-C, the relevant senses for L are as follows: Therefore, we have the intended senses, S S = {difficult-1, hard-2, stiff-6, tough-4, arduous-1, awkward-2}. It is reasonable to assume each sense in I is accompanied by a sense definition written in the same language (L1). We use D(S) to denote the glosses of S. Therefore we have D(S) = “not easy; hard to do, make, understand, etc.; diffi</context>
</contexts>
<marker>McArthur, 1992</marker>
<rawString>McArthur, T. (1992) Longman Lexicon of Contemporary English, Longman Group (Far East) Ltd., Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Mei</author>
</authors>
<title>Tongyici Cilin, Shanghai, Commercial Press.</title>
<date>1984</date>
<note>(in Chinese)</note>
<marker>Mei, 1984</marker>
<rawString>Mei, J. J., et al. (1984) Tongyici Cilin, Shanghai, Commercial Press. (in Chinese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>A Word-to-Word Model of Translational Equivalence&amp;quot;.</title>
<date>1997</date>
<booktitle>In Procs. of the ACL97.</booktitle>
<pages>490--497</pages>
<location>Madrid</location>
<contexts>
<context position="18527" citStr="Melamed 1997" startWordPosition="3250" endWordPosition="3251">i, L) = , j = 1, mi; where mi is mi j, max ∑ , j m i = 1, 4. Repeat Steps 2 and 3 until the values of P(d |L) and P(wi,j |i, L) converge; 5. For each i, find the most probable sense wi,j* , j*=argmax j P(wi,j |i, L) ; 6. Output S = { wi,j* |j*=argmax j P(wi,j |i, L)} ; 7. Estimate and output CBSTM for L, To make sense tagging more precise, it is advisable to place constraint on the translation counterpart c of w. SWAT considers only those translations c that has been linked with w based the Competitive “star,” “interest,” “issue,” the adjective “hard,” and the verb “serve.” Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). Table 4. The experimental results of assigning LDOCE senses to classes of LLOCE. 4. Experiments and evaluation In order to assess the feasibility of the proposed approach, we carried out experiments and evaluation on an implementation of MARS and SWAT based on LDOCE E-C, LLOCE, and Sinorama. First experiment was involved with the trainability of CBSDM and CBSTM via MARS. The second experiment was involved with the effectiveness of using SWAT and CBSTM to annotate a parallel corpus with sense information. Evaluation was done on a set of 14 nouns</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>Melamed, I.D. (1997). &amp;quot;A Word-to-Word Model of Translational Equivalence&amp;quot;. In Procs. of the ACL97. pp 490-497. Madrid Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model,</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="2329" citStr="Merialdo (1994)" startWordPosition="347" endWordPosition="349">were attacked in various ways. Yarowsky (1992) showed a class-based approach Jason S. Chang Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan, ROC jschang@cs.nthu.edu.tw under which a very large untagged corpus and thesaurus can be used effectively for unsupervised training for noun homograph disambiguation. However, the method does not offer a method that explicitly produces sense tagged data for any given sense inventory. Li and Huang (1999) described a similar unsupervised approach for Chinese text based on a Chinese thesaurus. As noted in Merialdo (1994), even minimal hand tagging improved on the results of unsupervised methods. Yarowsky (1995) showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods. Li and Li (2002) extended the approach by using corpora in two languages to bootstrap the learning process. They showed bilingual bootstrapping is even more effective. The bootstrapping approach is limited by lack of a systematic procedure of preparing seed data for any word in a given sense inventory. The approach also suffers from errors propagating from one iteration into th</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, B, (1994). Tagging English Text with a Probabilistic Model, Computational Linguistics, 20(2):155-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith A</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>235--244</pages>
<marker>Miller, A, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G., A, R.. Beckwith, C. Fellbaum, D. Gross and K.J. Miller. (1990). WordNet: An on-line lexical database. International Journal of Lexicography, 3(4), 235- 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Proctor</author>
</authors>
<title>Longman English-Chinese Dictionary of Contemporary English,</title>
<date>1988</date>
<institution>Longman Group (Far East) Ltd., Hong Kong.</institution>
<marker>Proctor, 1988</marker>
<rawString>Proctor, P. (1988) Longman English-Chinese Dictionary of Contemporary English, Longman Group (Far East) Ltd., Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Towell</author>
<author>E Voorhees</author>
</authors>
<title>Disambiguating Highly Ambiguous Words.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>125--146</pages>
<contexts>
<context position="21358" citStr="Towell and Voorhees (1998)" startWordPosition="3771" endWordPosition="3774">cks the problem of tagging and data sparseness in a way similar to the Yarowsky approach (1992) based on thesaurus categories. We differ from the Yarowsky’s approach, in the following ways: i. The WSD problem is solved for two languages instead of one within a single sense inventory. Furthermore, an explicit sense tagged corpus is produced in the process. ii. It is possible to work with any number of sense inventories. iii. The method is applicable not only to nouns but also to adjectives and verbs, since it does not rely on topical context, which is effective only for nouns as pointed out by Towell and Voorhees (1998). The approach is very general and modular and can work in conjunction with a number of learning strategies for word sense disambiguation (Yarowsky, 1995; Li and Li, 2002). 6. Conclusion In this paper, we present the Mutual Assured Resolution of Sense (MARS) Algorithm for assigning relevant senses to word classes in a given sense inventory (i.e. LDOCE or WordNet). We also describe the SWAT Algorithm for automatic sense tagging of a parallel corpus. We carried out experiments on an implementation of the MARS and SWAT Algorithms for all the senses in LDOCE and LLOCE. Evaluation on a set of 14 hi</context>
</contexts>
<marker>Towell, Voorhees, 1998</marker>
<rawString>Towell, G. and E. Voorhees. (1998) Disambiguating Highly Ambiguous Words. Computational Linguistics, vol. 24, no. 1, 125-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>Proceedings of the 14th International Conference on Computational Linguistics, COLING&apos;92,</booktitle>
<pages>23--28</pages>
<location>Nantes, France,</location>
<contexts>
<context position="1760" citStr="Yarowsky (1992)" startWordPosition="262" endWordPosition="263"> Word sense disambiguation has been an important research area for over 50 years. WSD is crucial for many applications, including machine translation, information retrieval, part of speech tagging, etc. Ide and Veronis (1998) pointed out the two major problems of WSD: sense tagging and data sparseness. On one hand, tagged data are very difficult to come by, since sense tagging is considerably more difficult than other forms of linguistic annotation. On the other hand, although the data sparseness is a common problem, it is especially severe for WSD. The problems were attacked in various ways. Yarowsky (1992) showed a class-based approach Jason S. Chang Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan, ROC jschang@cs.nthu.edu.tw under which a very large untagged corpus and thesaurus can be used effectively for unsupervised training for noun homograph disambiguation. However, the method does not offer a method that explicitly produces sense tagged data for any given sense inventory. Li and Huang (1999) described a similar unsupervised approach for Chinese text based on a Chinese thesaurus. As noted in Merialdo (1994), even minimal hand tagging imp</context>
<context position="3067" citStr="Yarowsky (1992)" startWordPosition="464" endWordPosition="465">egy of bootstrapping from small tagged data led to results rivaling supervised training methods. Li and Li (2002) extended the approach by using corpora in two languages to bootstrap the learning process. They showed bilingual bootstrapping is even more effective. The bootstrapping approach is limited by lack of a systematic procedure of preparing seed data for any word in a given sense inventory. The approach also suffers from errors propagating from one iteration into the next. Li and Huang Another alternative involves using a parallel corpus as a surrogate for tagged data. Gale, Church and Yarowsky (1992) exploited the so-called one sense per translation constraint for WSD. They reported high precision rates of a WSD system for two-way disambiguation of six English nouns based on their translations in an English-French Parallel corpus. However, when working with a particular sense inventory, there is no obvious way to know whether the one sense per translation constraint holds or how to determine the relevant translations automatically. Diab and Resnik (2002) extended the translation-based learning strategy with a weakened constraint that many instances of a word in a parallel corpus often cor</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, D. (1992). Word sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. Proceedings of the 14th International Conference on Computational Linguistics, COLING&apos;92, 23-28 August, Nantes, France, 454-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2421" citStr="Yarowsky (1995)" startWordPosition="361" endWordPosition="362">Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan, ROC jschang@cs.nthu.edu.tw under which a very large untagged corpus and thesaurus can be used effectively for unsupervised training for noun homograph disambiguation. However, the method does not offer a method that explicitly produces sense tagged data for any given sense inventory. Li and Huang (1999) described a similar unsupervised approach for Chinese text based on a Chinese thesaurus. As noted in Merialdo (1994), even minimal hand tagging improved on the results of unsupervised methods. Yarowsky (1995) showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods. Li and Li (2002) extended the approach by using corpora in two languages to bootstrap the learning process. They showed bilingual bootstrapping is even more effective. The bootstrapping approach is limited by lack of a systematic procedure of preparing seed data for any word in a given sense inventory. The approach also suffers from errors propagating from one iteration into the next. Li and Huang Another alternative involves using a parallel corpus as a surrogate for</context>
<context position="21511" citStr="Yarowsky, 1995" startWordPosition="3797" endWordPosition="3798">, in the following ways: i. The WSD problem is solved for two languages instead of one within a single sense inventory. Furthermore, an explicit sense tagged corpus is produced in the process. ii. It is possible to work with any number of sense inventories. iii. The method is applicable not only to nouns but also to adjectives and verbs, since it does not rely on topical context, which is effective only for nouns as pointed out by Towell and Voorhees (1998). The approach is very general and modular and can work in conjunction with a number of learning strategies for word sense disambiguation (Yarowsky, 1995; Li and Li, 2002). 6. Conclusion In this paper, we present the Mutual Assured Resolution of Sense (MARS) Algorithm for assigning relevant senses to word classes in a given sense inventory (i.e. LDOCE or WordNet). We also describe the SWAT Algorithm for automatic sense tagging of a parallel corpus. We carried out experiments on an implementation of the MARS and SWAT Algorithms for all the senses in LDOCE and LLOCE. Evaluation on a set of 14 highly ambiguous words showed that very high precision CBSDM and CBSTM can be constructed. High applicability and precision rates were achieved, when apply</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, 189-196</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>