<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000575">
<title confidence="0.991375">
Content Models for Survey Generation: A Factoid-Based Evaluation
</title>
<author confidence="0.938482">
Rahul Jha*, Catherine Finegan-Dollak*, Reed Coke*, Ben King*, Dragomir Radev*†
</author>
<affiliation confidence="0.803321">
* Department of EECS, University of Michigan, USA
† School of Information, University of Michigan, USA
</affiliation>
<email confidence="0.997938">
{rahuljha,cfdollak,reedcoke,benking,radev}@umich.edu
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998596">
We present a new factoid-annotated
dataset for evaluating content models
for scientific survey article generation
containing 3,425 sentences from 7 topics
in natural language processing. We also
introduce a novel HITS-based content
model for automated survey article gen-
eration called HITSUM that exploits the
lexical network structure between sen-
tences from citing and cited papers. Using
the factoid-annotated data, we conduct a
pyramid evaluation and compare HITSUM
with two previous state-of-the-art content
models: C-Lexrank, a network based con-
tent model, and TOPICSUM, a Bayesian
content model. Our experiments show that
our new content model captures useful
survey-worthy information and outper-
forms C-Lexrank by 4% and TOPICSUM
by 7% in pyramid evaluation.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961722222222">
Survey article generation is the task of automat-
ically building informative surveys for scientific
topics. Given the rapid growth of publications in
scientific fields, the development of such systems
is crucial as human-written surveys exist for a lim-
ited number of topics and get outdated quickly.
In this paper, we investigate content models for
extracting survey-worthy information from scien-
tific papers. Such models are an essential com-
ponent of any system for automatic survey arti-
cle generation. Earlier work in the area of survey
article generation has investigated content mod-
els based on lexical networks (Mohammad et al.,
2009; Qazvinian and Radev, 2008). These mod-
els take as input citing sentences that describe
important papers on the topic and assign them a
salience score based on centrality in a lexical net-
work formed by the input citing sentences. In this
</bodyText>
<table confidence="0.99858175">
Factoid Weight
Question Answering
answer extraction 6
question classification 6
definition of question answering 5
TREC QA track 5
information retrieval 5
Dependency Parsing
non-projective dependency structures / 6
trees
projectivity / projective dependency trees 6
deterministic parsing approaches: Nivre’s 5
algorithm
terminology: head - dependent 4
grammar driven approaches for 4
dependency parsing
</table>
<tableCaption confidence="0.99424">
Table 1: Sample factoids from the topics of ques-
</tableCaption>
<bodyText confidence="0.991310576923077">
tion answering and dependency parsing along
with their factoid weights.
paper, we propose a new content model based on
network structure previously unexplored for this
task that exploits the lexical relationship between
citing sentences and the sentences from the origi-
nal papers that they cite. Our new formulation of
the lexical network structure fits nicely with the
hubs and authorities model for identifying impor-
tant nodes in a network (Kleinberg, 1999), leading
to a new content model called HITSUM. In addi-
tion to this new content model, we also describe
how Bayesian content models previously explored
in the news domain can be adapted for the content
modeling task for survey generation.
For the task of evaluating various content mod-
els discussed in this paper, we have annotated a
total of 3,425 sentences across 7 topics in the field
of natural language processing with factoids from
each of the topics. The factoids we use were ex-
tracted from existing survey articles and tutorials
on each topic (Jha et al., 2013), and thus repre-
sent information that must be captured by a survey
article on the corresponding topic. Each of the fac-
toids is assigned a weight based on its frequency in
the surveys/tutorials, which allows us to do pyra-
</bodyText>
<page confidence="0.98174">
441
</page>
<note confidence="0.987916">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 441–450,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.993973875">
Topic # Sentences
dependency parsing 487
named entity recognition 383
question answering 452
semantic role labeling 466
sentiment analysis 613
summarization 507
word sense disambiguation 425
</table>
<tableCaption confidence="0.980545">
Table 2: List of seven NLP topics used in our ex-
</tableCaption>
<bodyText confidence="0.991458684210527">
periments along with input size.
mid evaluation of our content models. Some sam-
ple factoids are shown in Table 1. Evaluation using
factoids extracted from existing survey articles can
help us understand the limits of automated survey
article generation and how well these systems can
be expected to perform. For example, if certain
kinds of factoids are missing consistently from our
input sentences, improvements in content models
are unlikely to get us closer to the goal of generat-
ing survey articles that match those generated by
humans, and effort must be directed to extracting
text from other sources that will contain the miss-
ing information. On the other hand, if most of the
factoids exist in the input sentences but important
factoids are not found by the content models, we
can think of strategies for improving these models
by doing error analysis.
The main contributions of this paper are:
</bodyText>
<listItem confidence="0.887641333333333">
• HITSUM, a new HITS-based content model
for automatic survey generation for scientific
topics.
• A new dataset of 3,425 factoid-annotated
sentences for scientific articles in 7 topics.
• Experimental results for pyramid evalua-
tion comparing three existing content models
(Lexrank, C-Lexrank, TOPICSUM) with HIT-
SUM.
</listItem>
<bodyText confidence="0.999521">
The rest of this paper is organized as follows.
Section 2 describes the dataset used in our exper-
iment and the factoid annotation process. Sec-
tion 3 describes each of the content models used
in our experiments including HITSUM. Section 4
describes our experiments and Section 5 summa-
rizes the results. We summarize the related work
in Section 6 and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.98865" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999942133333333">
Prior research in automatic survey generation has
explored using text from different parts of scien-
tific papers. Some of the recent work has treated
survey generation as a direct extension of sin-
gle paper summarization (Qazvinian and Radev,
2008) and used citing sentences to a set of relevant
papers as the input for the summarizer (Moham-
mad et al., 2009; Qazvinian et al., 2013). How-
ever, in our prior work, we have observed that it’s
difficult to generate coherent and readable sum-
maries using just citing sentences and have pro-
posed the use of sentences from introductory texts
of papers that cite a number of important papers
on a topic (Jha et al., 2015). The use of full text
allows for the use of discourse structure of these
documents in framing coherent and readable sur-
veys. Since the content models we explore are
meant to be part of a larger system that should be
able to generate coherent and readable survey ar-
ticles, we use the introduction sentences for our
experiments as well.
The corpus we used for extracting our experi-
mental data was the ACL Anthology Network, a
comprehensive bibliographic dataset that contains
full text and citations for papers in most of the
important venues in natural language processing
(Radev et al., 2013). An oracle method is used for
selecting the initial set of papers for each topic.
For each topic, the bibliographies of at least three
human-written surveys were extracted, and any
papers that appeared in more than one survey were
added to the target document set for the topic.
The text for summarization is extracted from in-
troductory sections of papers that cite papers in the
target document set. The intuition behind this is
that the introductory sections of papers that cite
these target document summarize the research in
papers from the target document set as well as the
relationships between these papers. Thus, these
introductions can be thought of as mini-surveys
for specific aspects of the topic; combining text
from these introductory sections should allow us
to generate good comprehensive survey articles
for the topic1. For our experiments, we sort the cit-
ing papers based on the number of papers they cite
</bodyText>
<footnote confidence="0.999528833333333">
1Other sections of papers might have such information,
e.g. related work. Initial data analysis showed, however, that
not all papers in our corpus had related work sections. Thus
for consistency, we decided to use introduction sections. The
perfect system for this task would be able to extract ”related
work style” text segments from an entire paper.
</footnote>
<page confidence="0.995847">
442
</page>
<bodyText confidence="0.997838533333333">
Input sentence Factoids
According to [1] , the corpus based supervised machine learning methods are supervised wsd, corpus based wsd
the most successful approaches to WSD where contextual features have been
used mainly to distinguish ambiguous words in these methods.
Compared with supervised methods, unsupervised methods do not require supervised wsd, unsupervised wsd
tagged corpus, but the precision is usually lower than that of the supervised
methods.
Word sense disambiguation (WSD) has been a hot topic in natural language definition of word sense disambiguation
processing, which is to determine the sense of an ambiguous word in a specific
context.
Improvement in the accuracy of identifying the correct word sense will result in wsd for machine translation, wsd for in-
better machine translation systems, information retrieval systems, etc. formation retrieval
The SENSEVAL evaluation framework ( Kilgarriff 1998 ) was a DARPA-style senseval
competition designed to bring some conformity to the field of WSD, although
it has yet to achieve that aim completely.
</bodyText>
<tableCaption confidence="0.996033">
Table 3: Sample input sentences from the topic of word sense disambiguation annotated with factoids.
</tableCaption>
<bodyText confidence="0.999961032258064">
in the target document set, pick the top 20 papers,
and extract sentences from their introductions to
form the input text for the summarizer. The seven
topics used in our experiments and input size for
each topic are shown in Table 2.
Once the input text for each topic has been ex-
tracted, we annotate the sentences in the input
text with factoids for that topic. Some annotated
sentences in the topic of word sense disambigua-
tion are shown in Table 3. Given this new an-
notated data, we can compare how the factoids
are distributed across different citing sentences (as
annotated by Jha et al. (2013)) and introduction
sentences that we have annotated. For this, we
divide the factoids into five categories: defini-
tions, venue, resources, methodology, and appli-
cations. The fractional distribution of factoids in
these categories is shown in Table 4. We can see
that the distribution of factoids relating to venues,
methodology and applications is similar for the
two datasets. However, factoids related to defini-
tional sentences are almost completely missing in
the citing sentences data. This lack of background
information in citing sentences is one of the moti-
vations for using introduction sentences for survey
article generation as opposed to previous work.
The complete set of factoids as well
as annotated sentences for all the top-
ics is available for download at http:
//clair.si.umich.edu/corpora/
Surveyor_CM_Data.tar.gz.
</bodyText>
<sectionHeader confidence="0.994117" genericHeader="method">
3 Content Models
</sectionHeader>
<bodyText confidence="0.999258">
We now describe each of the content models used
in our experiments.
</bodyText>
<construct confidence="0.5463375">
Factoid category % Citing % Intro
definitions 0 4
venue 6 6
resources 18 2
methodology 70 83
applications 6 5
</construct>
<tableCaption confidence="0.733190333333333">
Table 4: Fractional distribution of factoids across
various categories in citing sentences vs introduc-
tion sentences.
</tableCaption>
<subsectionHeader confidence="0.996646">
3.1 Lexrank
</subsectionHeader>
<bodyText confidence="0.999989090909091">
Lexrank is a network-based content selection al-
gorithm that serves as a baseline for our experi-
ments. Given an input set of sentences, it first cre-
ates a network using these sentences where each
node represents a sentence and each edge repre-
sents the tf-idf cosine similarity between the sen-
tences. Two methods for creating the network are
possible. First, we can remove all edges that are
lower than a certain threshold of similarity (gener-
ally set to 0.1). The Lexrank value for a node p(u)
in this case is calculated as:
</bodyText>
<equation confidence="0.994565">
1 − d+ d
N v∈adj[u]
</equation>
<bodyText confidence="0.999847">
Where N is the total number of sentences, d is
the damping factor that controls the probability of
a random jump (usually set to 0.85), deg(v) is the
degree of the node v, and adj[u] is the set of nodes
connected to the node u. A different way of creat-
ing the network is to treat the sentence similarities
as edge weights and use the adjacency matrix as
a transition matrix after normalizing the rows; the
formula then becomes:
</bodyText>
<equation confidence="0.9172315">
p(v)
deg(v)
</equation>
<page confidence="0.988589">
443
</page>
<bodyText confidence="0.9985424">
A dictionary such as the LDOCE has broad coverage of word senses, useful for WSD .
This paper describes a program that disambiguates English word senses in unrestricted text using
statistical models of the major Roget’s Thesaurus categories.
Our technique offers benefits both for online semantic processing and for the challenging task of
mapping word senses across multiple MRDs in creating a merged lexical database.
The words in the sentences may be any of the 28,000 headwords in Longman’s Dictionary of
Contemporary English (LDOCE) and are disambiguated relative to the senses given in LDOCE.
This paper describes a heuristic approach to automatically identifying which senses of a machine-
readable dictionary (MRD) headword are semantically related versus those which correspond to
fundamentally different senses of the word.
</bodyText>
<figureCaption confidence="0.996476">
Figure 1: A sentence from Pciting with a high hub score (bolded) and some of sentences from Pcited
that it links to (italicised). The sentence from Pciting obtain a high hub score by being connected to the
sentences with high authority scores.
</figureCaption>
<bodyText confidence="0.647864">
1 − d+ d cos(u,v) cluster is defined as its Lexrank value in the lexical
N v∈adj[u] p(v) network formed by sentences in the cluster.
T otalCosv 3.3 HITSUM
Where cos(u, v) gives the tf-idf cosine similar-
ity between sentence u and v and TotalCosv =
</bodyText>
<equation confidence="0.541781">
E
z∈adj[v] cos(z, v). In our experiments, we em-
</equation>
<bodyText confidence="0.999877444444444">
ploy this second formulation. The above equation
can be solved efficiently using the power method
(Newman, 2010) to obtain p(u) for each node,
which is then used as the score for ordering the
sentences. The final Lexrank values p(u) for a
node represent the stationary distribution of the
Markov chain represented by the transition matrix.
Lexrank has been shown to perform well in sum-
marization experiments (Erkan and Radev, 2004).
</bodyText>
<subsectionHeader confidence="0.999605">
3.2 C-Lexrank
</subsectionHeader>
<bodyText confidence="0.99995812">
C-Lexrank is a clustering-based summarization
system that was proposed by Qazvinian and Radev
(2008) to summarize different perspectives in cit-
ing sentences that reference a paper or a topic.
To create summaries, C-LexRank constructs a
fully connected network in which vertices are sen-
tences, and edges are cosine similarities calculated
using the tf-idf vectors of citation sentences. It
then employs a hierarchical agglomeration clus-
tering algorithm proposed by Clauset et al. (2004)
to find communities of sentences that discuss the
same scientific contributions. Once the graph is
clustered and communities are formed, the method
extracts sentences from different clusters to build
a summary. It iterates through the clusters from
largest to smallest, choosing the most salient sen-
tence of each cluster, until the summary length
limit is reached. The salience of a sentence in its
The input set of sentences in our data come from
introductory sections of papers that cite important
papers on a topic. We’ll refer to the set of cit-
ing papers that provide the input text for the sum-
marizer as Pciting and the set of important papers
that represent the research we are trying to sum-
marize as Pcited. Both Lexrank and C-Lexrank
work by finding central sentences in a network
formed by the input sentences and thus, only use
the lexical information present in Pciting, while ig-
noring additional lexical information from the pa-
pers in Pcited. We now present a formulation that
uses the network structure that exists between the
sentences in the two sets of papers to incorporate
additional lexical information into the summariza-
tion system. This system is based on the hubs and
authorities or the HITS model (Kleinberg, 1999)
and hence is called HITSUM.
HITSUM, in addition to the sentences from the
introductory sections of papers in Pciting, also
uses sentences from the abstracts of Pcited. It starts
by computing the tf-idf cosine similarity between
the sentences of each paper pi ∈ Pciting with the
sentences in the abstracts of each paper pj ∈ Pcited
that is directly cited by pi. A directed edge is cre-
ated between every sentence si in pi and sj in pj
if sim(si, sj) &gt; smin, where smin is a similarity
threshold (set to 0.1 for our experiments). Once
this process has been completed for all papers in
Pciting, we end up with a bipartite graph between
sentences from Pciting and Pcited.
In this bipartite graph, sentences in Pcited that
</bodyText>
<page confidence="0.990436">
444
</page>
<table confidence="0.954930363636364">
OB OC/QA OD/J07−1005 OC/NER OD/I08−1071
the 0.066 question 0.044 metathesaurus 0.00032 ne 0.028 wikipedia 0.0087
of 0.040 questions 0.038 umls 0.00032 entity 0.022 pages 0.0053
and 0.034 answer 0.028 biomedical 0.00024 named 0.022 million 0.0018
a 0.029 answering 0.022 relevance 0.00024 entities 0.017 extracting 0.0018
in 0.027 qa 0.021 citation 0.00024 ner 0.014 articles 0.0018
to 0.027 answers 0.017 wykoff 0.00024 names 0.009 contributors 0.0018
is 0.017 2001 0.016 bringing 0.00016 location 0.008 version 0.0009
for 0.014 system 0.011 appropriately 0.00016 tagging 0.007 dakka 0.0009
that 0.012 trec 0.008 organized 0.00016 recognition 0.007 service 0.0009
we 0.011 factoid 0.008 foundation 0.00016 classes 0.007 academic 0.0009
</table>
<figureCaption confidence="0.897199">
Figure 2: Top words from different word distributions learned by TOPICSUM on our input document set
</figureCaption>
<bodyText confidence="0.905814789473684">
of 15 topics. OB is the background word distribution that captures stop words. OC/QA and OC/NER are
the word distributions for the topics of question answering and named entity recognition respectively.
OD/J07−1005 is the document-specific word distribution for a single paper in question answering that
focuses on clinical question answering. OD/z08−1071 is the document-specific word distribution for a
single paper in named entity recognition that focuses on named entity recognition in Wikipedia articles.
have a lot of incoming edges represent sentences
that presented important contributions in the field.
Similarly, sentences in Pciting that have a lot of
outgoing edges represent sentences that summa-
rize a number of important contributions in the
field. This suggests using the HITS algorithm,
which, given a network, assigns hubs and author-
ities scores to each node in the network in a mu-
tually reinforcing way. Thus, nodes with high au-
thority scores are those that are pointed to by a
number of good hubs, and nodes with high hub
scores are those that point to a number of good
authorities. This can be formalized with the fol-
lowing equation for the hub score of a node:
</bodyText>
<equation confidence="0.993329">
h(v) _ � a(u)
uEsuccessors(v)
</equation>
<bodyText confidence="0.9869714">
Where h(v) is the hub score for node v,
successors(v) is the set of all nodes that v has an
edge to, and a(u) is the authority score for node
u. Similarly, the authority score for each node is
computed as:
</bodyText>
<equation confidence="0.993736">
�a(v) _ h(u)
uEpredecessors(v)
</equation>
<bodyText confidence="0.999793090909091">
Where predecessors(v) is the set of all nodes
that have an edge to v. The hub and authority score
for each node can be computed using the power
method that starts with an initial value and itera-
tively updates the scores for each node based on
the above equations until the hub and authority
scores for each node converge to within a toler-
ance value (set to 1E-08 for our experiments).
In our bipartite lexical network, we expect sen-
tences in Pcited receiving high authority scores to
be the ones reporting important contributions and
sentences in Pciting that receive high hub scores
to be sentences summarizing important contribu-
tions. Figure 1 shows an example of a sentence
with a high hub score from the topic of word sense
disambiguation, along with some of the sentences
that it points to. HITSUM computes the hub and
authority score for each sentence in the lexical net-
work and then uses the hub scores for sentences in
Pciting as their relevance score. Sentences from
Pcited are part of the lexical network, but are not
used in the output summary.
</bodyText>
<sectionHeader confidence="0.531305" genericHeader="method">
3.4 TOPICSUM
</sectionHeader>
<bodyText confidence="0.999864466666667">
TOPICSUM is a probabilistic content model pre-
sented in Haghighi and Vanderwende (2009)
and is very similar to an earlier model called
BayesSum proposed by Daum´e and Marcu (2006).
It is a hierarchical, LDA (Latent Dirichlet Alloca-
tion) style model that is based on the following
generative story:2 words in any sentence in the
corpus can come from one of three word distri-
butions: a background word distribution OB that
flexibly models stop words, a content word dis-
tribution OC for each document set that models
content relevant to the entire document set, and
a document-specific word distribution OD. The
word distributions are learned using Gibbs sam-
pling. Given n document sets each with k doc-
</bodyText>
<footnote confidence="0.9779804">
2To avoid confusion in use of the term “topic,” in this pa-
per we refer to topics in the LDA sense as “word distribu-
tions.” “Topics” in this paper refer to the natural language
processing topics such as question answering, word sense
disambiguation, etc.
</footnote>
<page confidence="0.98393">
445
</page>
<table confidence="0.999946333333333">
Topic Lexrank C-Lexrank TOPICSUM HITSUM
dependency parsing 0.47 0.76 0.62 1.00*
named entity recognition 0.80 0.89 0.90* 0.80
question answering 0.65 0.67 0.65 0.76*
sentiment analysis 0.64 0.62 0.75* 0.63
semantic role labeling 0.75* 0.67 0.65 0.69
summarization 0.52 0.75* 0.57 0.68
word sense disambiguation 0.78 0.66 0.67 0.79*
Average 0.66 0.72 0.69 0.76*
</table>
<tableCaption confidence="0.994774">
Table 5: Pyramid scores obtained by different content models for each topic along with average scores
</tableCaption>
<bodyText confidence="0.995735685714286">
for each model across all topics. For each topic as well as the average, the best performing method has
been highlighted with a *.
uments, we get n content word distributions and
n * k document-specific distributions leading to a
total of 1 + n + n * k word distributions.
To illustrate the kind of distributions TOPIC-
SUM learns in our dataset, Figure 2 shows the
top words along with their probabilities from the
background word distribution, two content distri-
butions and two document-specific word distribu-
tions. We see that the model effectively captures
general content words for each topic. OC/QA is the
word distribution for the topic of question answer-
ing, while OD/J07−1005 is the document-specific
word distribution for a specific paper in the docu-
ment set for question answering3 that focuses on
clinical question answering. The word distribu-
tion OD/J07−1005 contains words that are relevant
to the specific subtopic in the paper, while OC/QA
contains content words relevant to the general
topic of question answering. Similar results can
be seen in the word distributions for named entity
recognition OC/NER and the document-specific
word distribution for a specific paper in the topic
OD/z08−10714 that focuses on comparable entity
mining.
These topics, learned using Gibbs sampling, can
be used to select sentences for a summary in the
following way. To summarize a document set, we
greedily select sentences that minimize the KL-
divergence of our summary to the document-set-
specific topic. Thus, the score for each sentence s
is KL(OC||P3) where P3 is the sentence word dis-
tribution with add-one smoothing applied to both
distributions. Using this objective, sentences that
</bodyText>
<footnote confidence="0.9856812">
3Dina Demner-Fushman and Jimmy Lin. 2007. Answer-
ing Clinical Questions with Knowledge-Based and Statistical
Techniques. Computational Linguistics.
4Wisam Dakka and Silviu Cucerzan. 2008. Augmenting
wikipedia with named entity tags. In Proceedings of IJCNLP.
</footnote>
<bodyText confidence="0.964578555555556">
contain words from the content word distribution
with high probability are more likely to be selected
in the generated summary.
We implemented TOPICSUM in Python
using Numpy and then optimized it using
Scipy Weave. This code is available for use
at https://github.com/rahuljha/
content-models. The repository also
contains Python code for HITSUM.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999986192307692">
For evaluating our content models, we gener-
ated 2,000-character-long summaries using each
of the systems (Lexrank, C-Lexrank, HITSUM,
and TOPICSUM) for each of the topics. The sum-
maries are generated by ranking the input sen-
tences using each content model and picking the
top sentences till the budget of 2,000 characters is
reached. Each of these summaries is then given
a pyramid score (Nenkova and Passonneau, 2004)
computed using the factoids assigned to each sen-
tence.
For the pyramid evaluation, the factoids are or-
ganized in a pyramid of order n. The top tier in
this pyramid contains the highest weighted fac-
toids, the next tier contains the second highest
weighted factoids, and so on. The score assigned
to a summary is the ratio of the sum of the weights
of the factoids it contains to the sum of weights
of an optimal summary with the same number of
factoids. Pyramid evaluation allows us to capture
how each content model performs in terms of se-
lecting sentences with the most highly weighted
factoids. Since the factoids have been extracted
from human-written surveys and tutorials on each
of the topics, the pyramid score gives us an idea of
the survey-worthiness of the sentences selected by
</bodyText>
<page confidence="0.983555">
446
</page>
<bodyText confidence="0.923852777777778">
Question classification is a crucial component of modern question answering system.
A what-type question is defined as the one whose question word is ‘what’, ‘which’, ‘name’ or ‘list’.
This metaclassifier beats all published numbers on standard question classification benchmarks
[4.4].
Due to its challenge, this paper focuses on what-type question classification.
In this paper, we focus on fine-category classification.
The promise of a machine learning approach is that the QA system builder can now focus on de-
signing features and providing labeled data, rather than coding and maintaining complex heuristic
rule bases.
</bodyText>
<figureCaption confidence="0.998679">
Figure 3: Part of the summary generated by HITSUM for the topic of question answering.
</figureCaption>
<bodyText confidence="0.900469">
each content model.
</bodyText>
<sectionHeader confidence="0.999794" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999991134328359">
The results of pyramid evaluation are summarized
in Table 5. It shows the pyramid score obtained by
each system on each of the topics as well as the av-
erage score. The highest performing system on av-
erage is HITSUM with an average performance of
76%. HITSUM does especially well for the topics
of dependency parsing, question answering, and
word sense disambiguation. The second best per-
forming system is C-Lexrank, which is not sur-
prising because it was developed specifically for
the task of scientific paper summarization. How-
ever, HITSUM outperforms C-Lexrank on several
topics and by 4% on average.
Figure 3 shows part of the summary generated
by HITSUM for the topic of question answering.
The summary contains mostly informative sen-
tences about different aspects of question answer-
ing. One obvious drawback of this summary is
that it’s not very coherent and readable. How-
ever, previous work has shown how network based
content models can be combined with discourse
models to generate informative yet readable sum-
maries (Jha et al., 2015). We looked at some of the
network statistics of the lexical networks used by
HITSUM. One of the things we noticed is that the
lexical networks for topics where HITSUM per-
forms well seem to have higher degree assorta-
tivity compared to the topics for which it doesn’t
perform well. High degree assortativity in lexical
networks means sentences with high degree tend
to be linked to other sentences with high degree.
This suggests that HITS performs well for topics
where a set of important factoids are mentioned in
many citing and source sentences. A larger evalua-
tion dataset is needed for a more thorough analysis
of how the network properties of these lexical net-
works correlate with the performance of various
content models.
TOPICSUM does well on the topics of named
entity recognition and sentiment analysis, but does
not do well on average. This can be attributed to
the fact that it was developed as a content model
for the domain of news summarization and does
not translate well to our domain. All systems out-
perform Lexrank, which achieves the lowest aver-
age score. This result is also intuitive, because ev-
ery other system in our evaluation uses additional
information not used by Lexrank: C-Lexrank ex-
ploits the community structure in the input set of
sentences, HITSUM exploits the lexical informa-
tion from cited sentences, and TOPICSUM exploits
information about global word distribution across
all topics.
The different systems we tried in our evaluation
depend on using different lexical information and
seem to perform well for different topics. This
suggests that further gains can be made by com-
bining these systems. For example, C-Lexrank
and HITSUM can be combined by utilizing both
the network formed by citing sentences and the
network between the citing sentences and the cited
sentences into a larger lexical network. TOPIC-
SUM scores can be combined with these network-
based system by using the TOPICSUM scores as a
prior for each node, and then running either Pager-
ank or HITS on top of it. We leave exploration of
such hybrid systems to future work.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.998217833333333">
The goal of content models in the context of sum-
marization is to extract a representation from in-
put text that can help in identifying important sen-
tences that should be in the output summary. Our
work is related to two main classes of content
models: network-based methods and probabilis-
</bodyText>
<page confidence="0.997103">
447
</page>
<bodyText confidence="0.999214019801981">
tic methods. We summarize related work for each
of these classes of content models, followed by a
short summary of the related work in the domain
of scientific summarization.
Network-based content models: Network-
based content models (Erkan and Radev, 2004;
Mihalcea and Tarau, 2004) work by converting
the input sentences into a network. Each sentence
is represented by a node in the network, and
the edges between sentences are given weight
based on the similarities of sentences. They then
run Pagerank on this network, and sentences are
selected based on their Pagerank score in the
network. For computing Pagerank, the network
can either be pruned by removing edges that
have weights less than a certain threshold, or
a weighted version of Pagerank can be run on
the network. The method can also be modified
for query-focused summarization (Otterbacher
et al., 2009). C-Lexrank (Qazvinian and Radev,
2008) modifies Lexrank by first running a clus-
tering algorithm on the network to partition the
network into different communities and then
selecting sentences from each community by
running Lexrank on the sub-network within each
community. C-Lexrank was also used in the task
of automated survey generation with encouraging
results (Mohammad et al., 2009).
Probabilistic content models: One of the
first probabilistic content models seems to be
BAYESSUM (Daum´e and Marcu, 2006), designed
for query-focused summarization. BAYESSUM
models a set of document collections using a hi-
erarchical LDA style model. Each word in a sen-
tence can be generated using one of three language
models: 1) a general English language model that
captures English filler or background knowledge,
2) a document-specific language model, and 3) a
query language model. These language models are
inferred using expectation propagation, and then
sentences are ranked based on their likelihood of
being generated from the query language model.
A similar model for general multidocument sum-
marization called TOPICSUM was proposed by
Haghighi and Vanderwende (2009), where the
query language model is replaced by a document-
collection-specific language model; thus sentences
are selected based on how likely they are to con-
tain information that summarizes the entire doc-
ument collection instead of information pertain-
ing to individual documents or background knowl-
edge.
Barzilay and Lee (2004) present a Hidden
Markov Model (HMM) based content model
where the hidden states of the HMM represent
the topics in the text. The transition probabili-
ties are learned through Viterbi decoding. They
show that the HMM model can be used for both re-
ordering of sentences for coherence and discrimi-
native scoring of sentences for extractive summa-
rization. Fung and Ngai (2006) present a simi-
lar HMM-based model for multi-document sum-
marization. Jiang and Zhai (2005) proposed an
HMM-based model for the problem of extract-
ing coherent passages relevant to a query from
a relevant document. They learn an HMM with
two background states (B1 and B2) and a query-
relevant state (R), each associated with a language
model. The HMM starts in background state B1,
switches to relevant state R and then switches to
the next background state B2. The sentences that
the HMM emits while in R constitute the query-
relevant passage from the document.
Scientific summarization: Early work in scien-
tific summarization used abstracts of scientific ar-
ticles to produce summaries of specific scientific
papers (Kupiec et al., 1995). However, later work
(Elkiss et al., 2008) showed that citation sentences
are as important in understanding the main contri-
butions of a paper.
Nanba and Okumura (1999) explored using ref-
erence information to build a system for support-
ing writing survey articles. Their system extracts
citing sentences that describe a referred paper and
identify the type of reference relationships. The
type of references can be one of the three: 1) type
B that base on other researcher’s theory, 2) type
C that compare with related works, or 3) type O
representing relationships other than B or C. They
posit that type C sentences are the most important
for survey generation and can help show the simi-
larities and differences among cited papers.
Teufel and Moens (2002) propose a method for
summarizing scientific articles based on rhetorical
status of sentences in scientific articles. They an-
notate sentences in a corpus of 80 scientific arti-
cles with rhetorical status, where the rhetorical sta-
tus can be one of aim (specific research goal), tex-
tual (section structure), own (neutral description of
own work), background (generally accepted back-
ground), contrast (comparison with other work),
</bodyText>
<page confidence="0.99607">
448
</page>
<bodyText confidence="0.999971485714286">
basis (agreement with or continuation of other
work), and other (neutral description of other’s
work). They describe classifiers for tagging the
rhetorical status of sentences automatically and
present a method for using this to assign relevance
score to sentences.
In other work, Kan et al. (2002) use a corpus of
2000 annotated bibliographies for scientific papers
as a first step towards a supervised summariza-
tion system. They found that summaries in their
corpus were mostly single-document abstractive
summaries that were both indicative and informa-
tive and were organized around a “theme,” making
them ideal for query-based summarization. Mei
and Zhai (2008) presented an impact-based sum-
marization method for single-paper summariza-
tion that assigns relevance scores to sentences in
a paper based on their similarity to the set of cit-
ing sentences that reference the paper.
More recently, Hoang and Kan (2010) present
a method for automated related work generation.
Their system takes as input a set of keywords ar-
ranged in a hierarchical fashion that describes a
target paper’s topic. They hypothesize that sen-
tences in a related work provide either background
information or specific contributions. They use
two different models to extract these two kinds
of sentences using the input tree and combines
them to create the final output summary. Zhang
et al. (2013) explore methods for biomedical sum-
marization by identifying cliques in a network
of semantic predications extracted from citations.
These cliques are then clustered and labeled to
identify different points of view represented in the
summary.
</bodyText>
<sectionHeader confidence="0.996358" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999985578947369">
We have presented a new factoid-annotated dataset
for evaluating content models for scientific survey
article generation by annotating sentences from
seven topics in natural language processing. We
also introduce a new HITS-based content model
called HITSUM for survey article generation that
exploits the lexical information from cited papers
along with citing papers to rank input sentences
for survey-worthiness. We conduct pyramid
evaluation using our factoid dataset to compare
HITSUM with existing network-based methods
(Lexrank, C-Lexrank) as well as methods based
on Bayesian content modeling (TOPICSUM). On
average, HITSUM outperforms C-Lexrank by 4%
and TOPICSUM by 7%. Since the different con-
tent models use different kinds of lexical informa-
tion, further gains might be obtained by combining
some of these models into a joint model. We plan
to explore this in future work.
</bodyText>
<sectionHeader confidence="0.995905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999493042553192">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Main Proceedings, pages 113–120,
Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70(6):066111, Dec.
Hal Daum´e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 305–312, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51–62.
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Pascale Fung and Grace Ngai. 2006. One story, one
flow: Hidden markov story models for multilingual
multidocument summarization. ACM Trans. Speech
Lang. Process., 3(2):1–16, July.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL ’09, pages 362–370,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Cong Duy Vu Hoang and Min-Yen Kan. 2010. To-
wards automated related work summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ’10,
pages 427–435, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Rahul Jha, Amjad Abu-Jbara, and Dragomir R. Radev.
2013. A system for summarizing scientific topics
</reference>
<page confidence="0.990361">
449
</page>
<reference confidence="0.999608121951219">
starting from keywords. In Proceedings of The As-
sociation for Computational Linguistics (short pa-
per).
Rahul Jha, Reed Coke, and Dragomir R. Radev. 2015.
Surveyor: A system for generating coherent survey
articles for scientific topics. In Proceedings of the
Twenty-Ninth AAAI Conference.
Jing Jiang and ChengXiang Zhai. 2005. Accurately
extracting coherent relevant passages using hidden
Markov models. pages 289–290.
Min-Yen Kan, Judith L. Klavans, and Kathleen R.
McKeown. 2002. Using the Annotated Bibliog-
raphy as a Resource for Indicative Summarization.
In The International Conference on Language Re-
sources and Evaluation (LREC), Las Palmas, Spain.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. J. ACM, 46:604–632,
September.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR-95), pages 68–73.
Qiaozhu Mei and ChengXiang Zhai. 2008. Gener-
ating impact-based summaries for scientific litera-
ture. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-08), pages 816–824.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-04), July.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 584–592, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of the 16th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-99), pages 926–931.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics - Human Language Technologies (HLT-
NAACL ’04).
Mark E. J. Newman. 2010. Networks: An Introduc-
tion. Oxford University Press.
Jahna Otterbacher, Gunes Erkan, and Dragomir R.
Radev. 2009. Biased lexrank: Passage retrieval us-
ing random walks with question-based priors. Inf.
Process. Manage., 45(1):42–54, January.
Vahed Qazvinian and Dragomir R. Radev. 2008.
Scientific paper summarization using citation sum-
mary networks. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING-08), Manchester, UK.
Vahed Qazvinian, Dragomir R. Radev, Saif M. Mo-
hammad, Bonnie Dorr, David Zajic, Michael
Whidby, and Taesun Moon. 2013. Generating ex-
tractive summaries of scientific paradigms. J. Artif.
Int. Res., 46(1):165–201, January.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources
and Evaluation, pages 1–26.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.
Han Zhang, Marcelo Fiszman, Dongwook Shin, Bart-
lomiej Wilkowski, and Thomas C. Rindflesch. 2013.
Clustering cliques for graph-based summarization of
the biomedical research literature. BMC Bioinfor-
matics, 14:182.
</reference>
<page confidence="0.997853">
450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649254">
<title confidence="0.893951">Content Models for Survey Generation: A Factoid-Based Evaluation</title>
<affiliation confidence="0.805016">of EECS, University of Michigan,</affiliation>
<address confidence="0.973941">of Information, University of Michigan, USA</address>
<abstract confidence="0.993059285714286">We present a new factoid-annotated dataset for evaluating content models for scientific survey article generation containing 3,425 sentences from 7 topics language We also introduce a novel HITS-based content model for automated survey article gencalled exploits the lexical network structure between sentences from citing and cited papers. Using the factoid-annotated data, we conduct a evaluation and compare with two previous state-of-the-art content models: C-Lexrank, a network based conmodel, and a Bayesian content model. Our experiments show that our new content model captures useful survey-worthy information and outper- C-Lexrank by 4% and by 7% in pyramid evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>113--120</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="31324" citStr="Barzilay and Lee (2004)" startWordPosition="5050" endWordPosition="5053">del. These language models are inferred using expectation propagation, and then sentences are ranked based on their likelihood of being generated from the query language model. A similar model for general multidocument summarization called TOPICSUM was proposed by Haghighi and Vanderwende (2009), where the query language model is replaced by a documentcollection-specific language model; thus sentences are selected based on how likely they are to contain information that summarizes the entire document collection instead of information pertaining to individual documents or background knowledge. Barzilay and Lee (2004) present a Hidden Markov Model (HMM) based content model where the hidden states of the HMM represent the topics in the text. The transition probabilities are learned through Viterbi decoding. They show that the HMM model can be used for both reordering of sentences for coherence and discriminative scoring of sentences for extractive summarization. Fung and Ngai (2006) present a similar HMM-based model for multi-document summarization. Jiang and Zhai (2005) proposed an HMM-based model for the problem of extracting coherent passages relevant to a query from a relevant document. They learn an HM</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Main Proceedings, pages 113–120, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Mark E J Newman</author>
<author>Cristopher Moore</author>
</authors>
<title>Finding community structure in very large networks.</title>
<date>2004</date>
<journal>Phys. Rev. E,</journal>
<volume>70</volume>
<issue>6</issue>
<contexts>
<context position="14522" citStr="Clauset et al. (2004)" startWordPosition="2319" endWordPosition="2322">ed by the transition matrix. Lexrank has been shown to perform well in summarization experiments (Erkan and Radev, 2004). 3.2 C-Lexrank C-Lexrank is a clustering-based summarization system that was proposed by Qazvinian and Radev (2008) to summarize different perspectives in citing sentences that reference a paper or a topic. To create summaries, C-LexRank constructs a fully connected network in which vertices are sentences, and edges are cosine similarities calculated using the tf-idf vectors of citation sentences. It then employs a hierarchical agglomeration clustering algorithm proposed by Clauset et al. (2004) to find communities of sentences that discuss the same scientific contributions. Once the graph is clustered and communities are formed, the method extracts sentences from different clusters to build a summary. It iterates through the clusters from largest to smallest, choosing the most salient sentence of each cluster, until the summary length limit is reached. The salience of a sentence in its The input set of sentences in our data come from introductory sections of papers that cite important papers on a topic. We’ll refer to the set of citing papers that provide the input text for the summ</context>
</contexts>
<marker>Clauset, Newman, Moore, 2004</marker>
<rawString>Aaron Clauset, Mark E. J. Newman, and Cristopher Moore. 2004. Finding community structure in very large networks. Phys. Rev. E, 70(6):066111, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>305--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 305–312, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Elkiss</author>
<author>Siwei Shen</author>
<author>Anthony Fader</author>
<author>G¨unes¸ Erkan</author>
<author>David States</author>
<author>Dragomir R Radev</author>
</authors>
<title>Blind men and elephants: What do citation summaries tell us about a research article?</title>
<date>2008</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="32472" citStr="Elkiss et al., 2008" startWordPosition="5239" endWordPosition="5242"> passages relevant to a query from a relevant document. They learn an HMM with two background states (B1 and B2) and a queryrelevant state (R), each associated with a language model. The HMM starts in background state B1, switches to relevant state R and then switches to the next background state B2. The sentences that the HMM emits while in R constitute the queryrelevant passage from the document. Scientific summarization: Early work in scientific summarization used abstracts of scientific articles to produce summaries of specific scientific papers (Kupiec et al., 1995). However, later work (Elkiss et al., 2008) showed that citation sentences are as important in understanding the main contributions of a paper. Nanba and Okumura (1999) explored using reference information to build a system for supporting writing survey articles. Their system extracts citing sentences that describe a referred paper and identify the type of reference relationships. The type of references can be one of the three: 1) type B that base on other researcher’s theory, 2) type C that compare with related works, or 3) type O representing relationships other than B or C. They posit that type C sentences are the most important for</context>
</contexts>
<marker>Elkiss, Shen, Fader, Erkan, States, Radev, 2008</marker>
<rawString>Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸ Erkan, David States, and Dragomir R. Radev. 2008. Blind men and elephants: What do citation summaries tell us about a research article? Journal of the American Society for Information Science and Technology, 59(1):51–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR).</journal>
<contexts>
<context position="14021" citStr="Erkan and Radev, 2004" startWordPosition="2245" endWordPosition="2248"> by sentences in the cluster. T otalCosv 3.3 HITSUM Where cos(u, v) gives the tf-idf cosine similarity between sentence u and v and TotalCosv = E z∈adj[v] cos(z, v). In our experiments, we employ this second formulation. The above equation can be solved efficiently using the power method (Newman, 2010) to obtain p(u) for each node, which is then used as the score for ordering the sentences. The final Lexrank values p(u) for a node represent the stationary distribution of the Markov chain represented by the transition matrix. Lexrank has been shown to perform well in summarization experiments (Erkan and Radev, 2004). 3.2 C-Lexrank C-Lexrank is a clustering-based summarization system that was proposed by Qazvinian and Radev (2008) to summarize different perspectives in citing sentences that reference a paper or a topic. To create summaries, C-LexRank constructs a fully connected network in which vertices are sentences, and edges are cosine similarities calculated using the tf-idf vectors of citation sentences. It then employs a hierarchical agglomeration clustering algorithm proposed by Clauset et al. (2004) to find communities of sentences that discuss the same scientific contributions. Once the graph is</context>
<context position="29214" citStr="Erkan and Radev, 2004" startWordPosition="4724" endWordPosition="4727">e leave exploration of such hybrid systems to future work. 6 Related Work The goal of content models in the context of summarization is to extract a representation from input text that can help in identifying important sentences that should be in the output summary. Our work is related to two main classes of content models: network-based methods and probabilis447 tic methods. We summarize related work for each of these classes of content models, followed by a short summary of the related work in the domain of scientific summarization. Network-based content models: Networkbased content models (Erkan and Radev, 2004; Mihalcea and Tarau, 2004) work by converting the input sentences into a network. Each sentence is represented by a node in the network, and the edges between sentences are given weight based on the similarities of sentences. They then run Pagerank on this network, and sentences are selected based on their Pagerank score in the network. For computing Pagerank, the network can either be pruned by removing edges that have weights less than a certain threshold, or a weighted version of Pagerank can be run on the network. The method can also be modified for query-focused summarization (Otterbache</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Grace Ngai</author>
</authors>
<title>One story, one flow: Hidden markov story models for multilingual multidocument summarization.</title>
<date>2006</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="31695" citStr="Fung and Ngai (2006)" startWordPosition="5112" endWordPosition="5115">language model; thus sentences are selected based on how likely they are to contain information that summarizes the entire document collection instead of information pertaining to individual documents or background knowledge. Barzilay and Lee (2004) present a Hidden Markov Model (HMM) based content model where the hidden states of the HMM represent the topics in the text. The transition probabilities are learned through Viterbi decoding. They show that the HMM model can be used for both reordering of sentences for coherence and discriminative scoring of sentences for extractive summarization. Fung and Ngai (2006) present a similar HMM-based model for multi-document summarization. Jiang and Zhai (2005) proposed an HMM-based model for the problem of extracting coherent passages relevant to a query from a relevant document. They learn an HMM with two background states (B1 and B2) and a queryrelevant state (R), each associated with a language model. The HMM starts in background state B1, switches to relevant state R and then switches to the next background state B2. The sentences that the HMM emits while in R constitute the queryrelevant passage from the document. Scientific summarization: Early work in s</context>
</contexts>
<marker>Fung, Ngai, 2006</marker>
<rawString>Pascale Fung and Grace Ngai. 2006. One story, one flow: Hidden markov story models for multilingual multidocument summarization. ACM Trans. Speech Lang. Process., 3(2):1–16, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19922" citStr="Haghighi and Vanderwende (2009)" startWordPosition="3216" endWordPosition="3219">tions and sentences in Pciting that receive high hub scores to be sentences summarizing important contributions. Figure 1 shows an example of a sentence with a high hub score from the topic of word sense disambiguation, along with some of the sentences that it points to. HITSUM computes the hub and authority score for each sentence in the lexical network and then uses the hub scores for sentences in Pciting as their relevance score. Sentences from Pcited are part of the lexical network, but are not used in the output summary. 3.4 TOPICSUM TOPICSUM is a probabilistic content model presented in Haghighi and Vanderwende (2009) and is very similar to an earlier model called BayesSum proposed by Daum´e and Marcu (2006). It is a hierarchical, LDA (Latent Dirichlet Allocation) style model that is based on the following generative story:2 words in any sentence in the corpus can come from one of three word distributions: a background word distribution OB that flexibly models stop words, a content word distribution OC for each document set that models content relevant to the entire document set, and a document-specific word distribution OD. The word distributions are learned using Gibbs sampling. Given n document sets eac</context>
<context position="30997" citStr="Haghighi and Vanderwende (2009)" startWordPosition="5000" endWordPosition="5003">sed summarization. BAYESSUM models a set of document collections using a hierarchical LDA style model. Each word in a sentence can be generated using one of three language models: 1) a general English language model that captures English filler or background knowledge, 2) a document-specific language model, and 3) a query language model. These language models are inferred using expectation propagation, and then sentences are ranked based on their likelihood of being generated from the query language model. A similar model for general multidocument summarization called TOPICSUM was proposed by Haghighi and Vanderwende (2009), where the query language model is replaced by a documentcollection-specific language model; thus sentences are selected based on how likely they are to contain information that summarizes the entire document collection instead of information pertaining to individual documents or background knowledge. Barzilay and Lee (2004) present a Hidden Markov Model (HMM) based content model where the hidden states of the HMM represent the topics in the text. The transition probabilities are learned through Viterbi decoding. They show that the HMM model can be used for both reordering of sentences for co</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 362–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Duy Vu Hoang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Towards automated related work summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>427--435</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34532" citStr="Hoang and Kan (2010)" startWordPosition="5564" endWordPosition="5567">2) use a corpus of 2000 annotated bibliographies for scientific papers as a first step towards a supervised summarization system. They found that summaries in their corpus were mostly single-document abstractive summaries that were both indicative and informative and were organized around a “theme,” making them ideal for query-based summarization. Mei and Zhai (2008) presented an impact-based summarization method for single-paper summarization that assigns relevance scores to sentences in a paper based on their similarity to the set of citing sentences that reference the paper. More recently, Hoang and Kan (2010) present a method for automated related work generation. Their system takes as input a set of keywords arranged in a hierarchical fashion that describes a target paper’s topic. They hypothesize that sentences in a related work provide either background information or specific contributions. They use two different models to extract these two kinds of sentences using the input tree and combines them to create the final output summary. Zhang et al. (2013) explore methods for biomedical summarization by identifying cliques in a network of semantic predications extracted from citations. These cliqu</context>
</contexts>
<marker>Hoang, Kan, 2010</marker>
<rawString>Cong Duy Vu Hoang and Min-Yen Kan. 2010. Towards automated related work summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 427–435, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Jha</author>
<author>Amjad Abu-Jbara</author>
<author>Dragomir R Radev</author>
</authors>
<title>A system for summarizing scientific topics starting from keywords.</title>
<date>2013</date>
<booktitle>In Proceedings of The Association for Computational Linguistics</booktitle>
<contexts>
<context position="3444" citStr="Jha et al., 2013" startWordPosition="517" endWordPosition="520">g important nodes in a network (Kleinberg, 1999), leading to a new content model called HITSUM. In addition to this new content model, we also describe how Bayesian content models previously explored in the news domain can be adapted for the content modeling task for survey generation. For the task of evaluating various content models discussed in this paper, we have annotated a total of 3,425 sentences across 7 topics in the field of natural language processing with factoids from each of the topics. The factoids we use were extracted from existing survey articles and tutorials on each topic (Jha et al., 2013), and thus represent information that must be captured by a survey article on the corresponding topic. Each of the factoids is assigned a weight based on its frequency in the surveys/tutorials, which allows us to do pyra441 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 441–450, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Topic # Sentences dependency parsing 487 named entity recognition 383 question answering 452 semantic role labeling</context>
<context position="10074" citStr="Jha et al. (2013)" startWordPosition="1595" endWordPosition="1598">d with factoids. in the target document set, pick the top 20 papers, and extract sentences from their introductions to form the input text for the summarizer. The seven topics used in our experiments and input size for each topic are shown in Table 2. Once the input text for each topic has been extracted, we annotate the sentences in the input text with factoids for that topic. Some annotated sentences in the topic of word sense disambiguation are shown in Table 3. Given this new annotated data, we can compare how the factoids are distributed across different citing sentences (as annotated by Jha et al. (2013)) and introduction sentences that we have annotated. For this, we divide the factoids into five categories: definitions, venue, resources, methodology, and applications. The fractional distribution of factoids in these categories is shown in Table 4. We can see that the distribution of factoids relating to venues, methodology and applications is similar for the two datasets. However, factoids related to definitional sentences are almost completely missing in the citing sentences data. This lack of background information in citing sentences is one of the motivations for using introduction sente</context>
</contexts>
<marker>Jha, Abu-Jbara, Radev, 2013</marker>
<rawString>Rahul Jha, Amjad Abu-Jbara, and Dragomir R. Radev. 2013. A system for summarizing scientific topics starting from keywords. In Proceedings of The Association for Computational Linguistics (short paper).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Jha</author>
<author>Reed Coke</author>
<author>Dragomir R Radev</author>
</authors>
<title>Surveyor: A system for generating coherent survey articles for scientific topics.</title>
<date>2015</date>
<booktitle>In Proceedings of the Twenty-Ninth AAAI Conference.</booktitle>
<contexts>
<context position="6427" citStr="Jha et al., 2015" startWordPosition="1003" endWordPosition="1006">on has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization (Qazvinian and Radev, 2008) and used citing sentences to a set of relevant papers as the input for the summarizer (Mohammad et al., 2009; Qazvinian et al., 2013). However, in our prior work, we have observed that it’s difficult to generate coherent and readable summaries using just citing sentences and have proposed the use of sentences from introductory texts of papers that cite a number of important papers on a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of a larger system that should be able to generate coherent and readable survey articles, we use the introduction sentences for our experiments as well. The corpus we used for extracting our experimental data was the ACL Anthology Network, a comprehensive bibliographic dataset that contains full text and citations for papers in most of the important venues in natural language processing (Radev et al., 2013). An or</context>
<context position="26575" citStr="Jha et al., 2015" startWordPosition="4282" endWordPosition="4285">t surprising because it was developed specifically for the task of scientific paper summarization. However, HITSUM outperforms C-Lexrank on several topics and by 4% on average. Figure 3 shows part of the summary generated by HITSUM for the topic of question answering. The summary contains mostly informative sentences about different aspects of question answering. One obvious drawback of this summary is that it’s not very coherent and readable. However, previous work has shown how network based content models can be combined with discourse models to generate informative yet readable summaries (Jha et al., 2015). We looked at some of the network statistics of the lexical networks used by HITSUM. One of the things we noticed is that the lexical networks for topics where HITSUM performs well seem to have higher degree assortativity compared to the topics for which it doesn’t perform well. High degree assortativity in lexical networks means sentences with high degree tend to be linked to other sentences with high degree. This suggests that HITS performs well for topics where a set of important factoids are mentioned in many citing and source sentences. A larger evaluation dataset is needed for a more th</context>
</contexts>
<marker>Jha, Coke, Radev, 2015</marker>
<rawString>Rahul Jha, Reed Coke, and Dragomir R. Radev. 2015. Surveyor: A system for generating coherent survey articles for scientific topics. In Proceedings of the Twenty-Ninth AAAI Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Accurately extracting coherent relevant passages using hidden Markov models.</title>
<date>2005</date>
<pages>289--290</pages>
<contexts>
<context position="31785" citStr="Jiang and Zhai (2005)" startWordPosition="5126" endWordPosition="5129">mation that summarizes the entire document collection instead of information pertaining to individual documents or background knowledge. Barzilay and Lee (2004) present a Hidden Markov Model (HMM) based content model where the hidden states of the HMM represent the topics in the text. The transition probabilities are learned through Viterbi decoding. They show that the HMM model can be used for both reordering of sentences for coherence and discriminative scoring of sentences for extractive summarization. Fung and Ngai (2006) present a similar HMM-based model for multi-document summarization. Jiang and Zhai (2005) proposed an HMM-based model for the problem of extracting coherent passages relevant to a query from a relevant document. They learn an HMM with two background states (B1 and B2) and a queryrelevant state (R), each associated with a language model. The HMM starts in background state B1, switches to relevant state R and then switches to the next background state B2. The sentences that the HMM emits while in R constitute the queryrelevant passage from the document. Scientific summarization: Early work in scientific summarization used abstracts of scientific articles to produce summaries of spec</context>
</contexts>
<marker>Jiang, Zhai, 2005</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2005. Accurately extracting coherent relevant passages using hidden Markov models. pages 289–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Judith L Klavans</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Using the Annotated Bibliography as a Resource for Indicative Summarization.</title>
<date>2002</date>
<booktitle>In The International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="33914" citStr="Kan et al. (2002)" startWordPosition="5469" endWordPosition="5472">cles. They annotate sentences in a corpus of 80 scientific articles with rhetorical status, where the rhetorical status can be one of aim (specific research goal), textual (section structure), own (neutral description of own work), background (generally accepted background), contrast (comparison with other work), 448 basis (agreement with or continuation of other work), and other (neutral description of other’s work). They describe classifiers for tagging the rhetorical status of sentences automatically and present a method for using this to assign relevance score to sentences. In other work, Kan et al. (2002) use a corpus of 2000 annotated bibliographies for scientific papers as a first step towards a supervised summarization system. They found that summaries in their corpus were mostly single-document abstractive summaries that were both indicative and informative and were organized around a “theme,” making them ideal for query-based summarization. Mei and Zhai (2008) presented an impact-based summarization method for single-paper summarization that assigns relevance scores to sentences in a paper based on their similarity to the set of citing sentences that reference the paper. More recently, Ho</context>
</contexts>
<marker>Kan, Klavans, McKeown, 2002</marker>
<rawString>Min-Yen Kan, Judith L. Klavans, and Kathleen R. McKeown. 2002. Using the Annotated Bibliography as a Resource for Indicative Summarization. In The International Conference on Language Resources and Evaluation (LREC), Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>J. ACM,</journal>
<pages>46--604</pages>
<contexts>
<context position="2875" citStr="Kleinberg, 1999" startWordPosition="421" endWordPosition="422">ches: Nivre’s 5 algorithm terminology: head - dependent 4 grammar driven approaches for 4 dependency parsing Table 1: Sample factoids from the topics of question answering and dependency parsing along with their factoid weights. paper, we propose a new content model based on network structure previously unexplored for this task that exploits the lexical relationship between citing sentences and the sentences from the original papers that they cite. Our new formulation of the lexical network structure fits nicely with the hubs and authorities model for identifying important nodes in a network (Kleinberg, 1999), leading to a new content model called HITSUM. In addition to this new content model, we also describe how Bayesian content models previously explored in the news domain can be adapted for the content modeling task for survey generation. For the task of evaluating various content models discussed in this paper, we have annotated a total of 3,425 sentences across 7 topics in the field of natural language processing with factoids from each of the topics. The factoids we use were extracted from existing survey articles and tutorials on each topic (Jha et al., 2013), and thus represent informatio</context>
<context position="15760" citStr="Kleinberg, 1999" startWordPosition="2528" endWordPosition="2529">the set of important papers that represent the research we are trying to summarize as Pcited. Both Lexrank and C-Lexrank work by finding central sentences in a network formed by the input sentences and thus, only use the lexical information present in Pciting, while ignoring additional lexical information from the papers in Pcited. We now present a formulation that uses the network structure that exists between the sentences in the two sets of papers to incorporate additional lexical information into the summarization system. This system is based on the hubs and authorities or the HITS model (Kleinberg, 1999) and hence is called HITSUM. HITSUM, in addition to the sentences from the introductory sections of papers in Pciting, also uses sentences from the abstracts of Pcited. It starts by computing the tf-idf cosine similarity between the sentences of each paper pi ∈ Pciting with the sentences in the abstracts of each paper pj ∈ Pcited that is directly cited by pi. A directed edge is created between every sentence si in pi and sj in pj if sim(si, sj) &gt; smin, where smin is a similarity threshold (set to 0.1 for our experiments). Once this process has been completed for all papers in Pciting, we end u</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46:604–632, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-95),</booktitle>
<pages>68--73</pages>
<contexts>
<context position="32429" citStr="Kupiec et al., 1995" startWordPosition="5232" endWordPosition="5235">odel for the problem of extracting coherent passages relevant to a query from a relevant document. They learn an HMM with two background states (B1 and B2) and a queryrelevant state (R), each associated with a language model. The HMM starts in background state B1, switches to relevant state R and then switches to the next background state B2. The sentences that the HMM emits while in R constitute the queryrelevant passage from the document. Scientific summarization: Early work in scientific summarization used abstracts of scientific articles to produce summaries of specific scientific papers (Kupiec et al., 1995). However, later work (Elkiss et al., 2008) showed that citation sentences are as important in understanding the main contributions of a paper. Nanba and Okumura (1999) explored using reference information to build a system for supporting writing survey articles. Their system extracts citing sentences that describe a referred paper and identify the type of reference relationships. The type of references can be one of the three: 1) type B that base on other researcher’s theory, 2) type C that compare with related works, or 3) type O representing relationships other than B or C. They posit that </context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-95), pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Generating impact-based summaries for scientific literature.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>816--824</pages>
<contexts>
<context position="34281" citStr="Mei and Zhai (2008)" startWordPosition="5524" endWordPosition="5527"> work), and other (neutral description of other’s work). They describe classifiers for tagging the rhetorical status of sentences automatically and present a method for using this to assign relevance score to sentences. In other work, Kan et al. (2002) use a corpus of 2000 annotated bibliographies for scientific papers as a first step towards a supervised summarization system. They found that summaries in their corpus were mostly single-document abstractive summaries that were both indicative and informative and were organized around a “theme,” making them ideal for query-based summarization. Mei and Zhai (2008) presented an impact-based summarization method for single-paper summarization that assigns relevance scores to sentences in a paper based on their similarity to the set of citing sentences that reference the paper. More recently, Hoang and Kan (2010) present a method for automated related work generation. Their system takes as input a set of keywords arranged in a hierarchical fashion that describes a target paper’s topic. They hypothesize that sentences in a related work provide either background information or specific contributions. They use two different models to extract these two kinds </context>
</contexts>
<marker>Mei, Zhai, 2008</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2008. Generating impact-based summaries for scientific literature. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics (ACL-08), pages 816–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-04),</booktitle>
<contexts>
<context position="29241" citStr="Mihalcea and Tarau, 2004" startWordPosition="4728" endWordPosition="4731">such hybrid systems to future work. 6 Related Work The goal of content models in the context of summarization is to extract a representation from input text that can help in identifying important sentences that should be in the output summary. Our work is related to two main classes of content models: network-based methods and probabilis447 tic methods. We summarize related work for each of these classes of content models, followed by a short summary of the related work in the domain of scientific summarization. Network-based content models: Networkbased content models (Erkan and Radev, 2004; Mihalcea and Tarau, 2004) work by converting the input sentences into a network. Each sentence is represented by a node in the network, and the edges between sentences are given weight based on the similarities of sentences. They then run Pagerank on this network, and sentences are selected based on their Pagerank score in the network. For computing Pagerank, the network can either be pruned by removing edges that have weights less than a certain threshold, or a weighted version of Pagerank can be run on the network. The method can also be modified for query-focused summarization (Otterbacher et al., 2009). C-Lexrank </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-04), July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Melissa Egan</author>
<author>Ahmed Hassan</author>
<author>Pradeep Muthukrishan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
<author>David Zajic</author>
</authors>
<title>Using citations to generate surveys of scientific paradigms.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>584--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1727" citStr="Mohammad et al., 2009" startWordPosition="244" endWordPosition="247">eneration is the task of automatically building informative surveys for scientific topics. Given the rapid growth of publications in scientific fields, the development of such systems is crucial as human-written surveys exist for a limited number of topics and get outdated quickly. In this paper, we investigate content models for extracting survey-worthy information from scientific papers. Such models are an essential component of any system for automatic survey article generation. Earlier work in the area of survey article generation has investigated content models based on lexical networks (Mohammad et al., 2009; Qazvinian and Radev, 2008). These models take as input citing sentences that describe important papers on the topic and assign them a salience score based on centrality in a lexical network formed by the input citing sentences. In this Factoid Weight Question Answering answer extraction 6 question classification 6 definition of question answering 5 TREC QA track 5 information retrieval 5 Dependency Parsing non-projective dependency structures / 6 trees projectivity / projective dependency trees 6 deterministic parsing approaches: Nivre’s 5 algorithm terminology: head - dependent 4 grammar dr</context>
<context position="6122" citStr="Mohammad et al., 2009" startWordPosition="948" endWordPosition="952">ctoid annotation process. Section 3 describes each of the content models used in our experiments including HITSUM. Section 4 describes our experiments and Section 5 summarizes the results. We summarize the related work in Section 6 and conclude in Section 7. 2 Data Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization (Qazvinian and Radev, 2008) and used citing sentences to a set of relevant papers as the input for the summarizer (Mohammad et al., 2009; Qazvinian et al., 2013). However, in our prior work, we have observed that it’s difficult to generate coherent and readable summaries using just citing sentences and have proposed the use of sentences from introductory texts of papers that cite a number of important papers on a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of a larger system that should be able to generate coherent and readable survey articles, we use the introductio</context>
<context position="30218" citStr="Mohammad et al., 2009" startWordPosition="4883" endWordPosition="4886">ther be pruned by removing edges that have weights less than a certain threshold, or a weighted version of Pagerank can be run on the network. The method can also be modified for query-focused summarization (Otterbacher et al., 2009). C-Lexrank (Qazvinian and Radev, 2008) modifies Lexrank by first running a clustering algorithm on the network to partition the network into different communities and then selecting sentences from each community by running Lexrank on the sub-network within each community. C-Lexrank was also used in the task of automated survey generation with encouraging results (Mohammad et al., 2009). Probabilistic content models: One of the first probabilistic content models seems to be BAYESSUM (Daum´e and Marcu, 2006), designed for query-focused summarization. BAYESSUM models a set of document collections using a hierarchical LDA style model. Each word in a sentence can be generated using one of three language models: 1) a general English language model that captures English filler or background knowledge, 2) a document-specific language model, and 3) a query language model. These language models are inferred using expectation propagation, and then sentences are ranked based on their l</context>
</contexts>
<marker>Mohammad, Dorr, Egan, Hassan, Muthukrishan, Qazvinian, Radev, Zajic, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 584–592, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99),</booktitle>
<pages>926--931</pages>
<contexts>
<context position="32597" citStr="Nanba and Okumura (1999)" startWordPosition="5259" endWordPosition="5262">eryrelevant state (R), each associated with a language model. The HMM starts in background state B1, switches to relevant state R and then switches to the next background state B2. The sentences that the HMM emits while in R constitute the queryrelevant passage from the document. Scientific summarization: Early work in scientific summarization used abstracts of scientific articles to produce summaries of specific scientific papers (Kupiec et al., 1995). However, later work (Elkiss et al., 2008) showed that citation sentences are as important in understanding the main contributions of a paper. Nanba and Okumura (1999) explored using reference information to build a system for supporting writing survey articles. Their system extracts citing sentences that describe a referred paper and identify the type of reference relationships. The type of references can be one of the three: 1) type B that base on other researcher’s theory, 2) type C that compare with related works, or 3) type O representing relationships other than B or C. They posit that type C sentences are the most important for survey generation and can help show the similarities and differences among cited papers. Teufel and Moens (2002) propose a m</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Hidetsugu Nanba and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99), pages 926–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (HLTNAACL ’04).</booktitle>
<contexts>
<context position="23980" citStr="Nenkova and Passonneau, 2004" startWordPosition="3858" endWordPosition="3861">M in Python using Numpy and then optimized it using Scipy Weave. This code is available for use at https://github.com/rahuljha/ content-models. The repository also contains Python code for HITSUM. 4 Experiments For evaluating our content models, we generated 2,000-character-long summaries using each of the systems (Lexrank, C-Lexrank, HITSUM, and TOPICSUM) for each of the topics. The summaries are generated by ranking the input sentences using each content model and picking the top sentences till the budget of 2,000 characters is reached. Each of these summaries is then given a pyramid score (Nenkova and Passonneau, 2004) computed using the factoids assigned to each sentence. For the pyramid evaluation, the factoids are organized in a pyramid of order n. The top tier in this pyramid contains the highest weighted factoids, the next tier contains the second highest weighted factoids, and so on. The score assigned to a summary is the ratio of the sum of the weights of the factoids it contains to the sum of weights of an optimal summary with the same number of factoids. Pyramid evaluation allows us to capture how each content model performs in terms of selecting sentences with the most highly weighted factoids. Si</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (HLTNAACL ’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E J Newman</author>
</authors>
<title>Networks: An Introduction.</title>
<date>2010</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="13702" citStr="Newman, 2010" startWordPosition="2194" endWordPosition="2195">h a high hub score (bolded) and some of sentences from Pcited that it links to (italicised). The sentence from Pciting obtain a high hub score by being connected to the sentences with high authority scores. 1 − d+ d cos(u,v) cluster is defined as its Lexrank value in the lexical N v∈adj[u] p(v) network formed by sentences in the cluster. T otalCosv 3.3 HITSUM Where cos(u, v) gives the tf-idf cosine similarity between sentence u and v and TotalCosv = E z∈adj[v] cos(z, v). In our experiments, we employ this second formulation. The above equation can be solved efficiently using the power method (Newman, 2010) to obtain p(u) for each node, which is then used as the score for ordering the sentences. The final Lexrank values p(u) for a node represent the stationary distribution of the Markov chain represented by the transition matrix. Lexrank has been shown to perform well in summarization experiments (Erkan and Radev, 2004). 3.2 C-Lexrank C-Lexrank is a clustering-based summarization system that was proposed by Qazvinian and Radev (2008) to summarize different perspectives in citing sentences that reference a paper or a topic. To create summaries, C-LexRank constructs a fully connected network in wh</context>
</contexts>
<marker>Newman, 2010</marker>
<rawString>Mark E. J. Newman. 2010. Networks: An Introduction. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Biased lexrank: Passage retrieval using random walks with question-based priors.</title>
<date>2009</date>
<journal>Inf. Process. Manage.,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="29829" citStr="Otterbacher et al., 2009" startWordPosition="4825" endWordPosition="4828">adev, 2004; Mihalcea and Tarau, 2004) work by converting the input sentences into a network. Each sentence is represented by a node in the network, and the edges between sentences are given weight based on the similarities of sentences. They then run Pagerank on this network, and sentences are selected based on their Pagerank score in the network. For computing Pagerank, the network can either be pruned by removing edges that have weights less than a certain threshold, or a weighted version of Pagerank can be run on the network. The method can also be modified for query-focused summarization (Otterbacher et al., 2009). C-Lexrank (Qazvinian and Radev, 2008) modifies Lexrank by first running a clustering algorithm on the network to partition the network into different communities and then selecting sentences from each community by running Lexrank on the sub-network within each community. C-Lexrank was also used in the task of automated survey generation with encouraging results (Mohammad et al., 2009). Probabilistic content models: One of the first probabilistic content models seems to be BAYESSUM (Daum´e and Marcu, 2006), designed for query-focused summarization. BAYESSUM models a set of document collection</context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2009</marker>
<rawString>Jahna Otterbacher, Gunes Erkan, and Dragomir R. Radev. 2009. Biased lexrank: Passage retrieval using random walks with question-based priors. Inf. Process. Manage., 45(1):42–54, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08),</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="1755" citStr="Qazvinian and Radev, 2008" startWordPosition="248" endWordPosition="251">f automatically building informative surveys for scientific topics. Given the rapid growth of publications in scientific fields, the development of such systems is crucial as human-written surveys exist for a limited number of topics and get outdated quickly. In this paper, we investigate content models for extracting survey-worthy information from scientific papers. Such models are an essential component of any system for automatic survey article generation. Earlier work in the area of survey article generation has investigated content models based on lexical networks (Mohammad et al., 2009; Qazvinian and Radev, 2008). These models take as input citing sentences that describe important papers on the topic and assign them a salience score based on centrality in a lexical network formed by the input citing sentences. In this Factoid Weight Question Answering answer extraction 6 question classification 6 definition of question answering 5 TREC QA track 5 information retrieval 5 Dependency Parsing non-projective dependency structures / 6 trees projectivity / projective dependency trees 6 deterministic parsing approaches: Nivre’s 5 algorithm terminology: head - dependent 4 grammar driven approaches for 4 depend</context>
<context position="6013" citStr="Qazvinian and Radev, 2008" startWordPosition="928" endWordPosition="931"> The rest of this paper is organized as follows. Section 2 describes the dataset used in our experiment and the factoid annotation process. Section 3 describes each of the content models used in our experiments including HITSUM. Section 4 describes our experiments and Section 5 summarizes the results. We summarize the related work in Section 6 and conclude in Section 7. 2 Data Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization (Qazvinian and Radev, 2008) and used citing sentences to a set of relevant papers as the input for the summarizer (Mohammad et al., 2009; Qazvinian et al., 2013). However, in our prior work, we have observed that it’s difficult to generate coherent and readable summaries using just citing sentences and have proposed the use of sentences from introductory texts of papers that cite a number of important papers on a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of </context>
<context position="14137" citStr="Qazvinian and Radev (2008)" startWordPosition="2261" endWordPosition="2264">entence u and v and TotalCosv = E z∈adj[v] cos(z, v). In our experiments, we employ this second formulation. The above equation can be solved efficiently using the power method (Newman, 2010) to obtain p(u) for each node, which is then used as the score for ordering the sentences. The final Lexrank values p(u) for a node represent the stationary distribution of the Markov chain represented by the transition matrix. Lexrank has been shown to perform well in summarization experiments (Erkan and Radev, 2004). 3.2 C-Lexrank C-Lexrank is a clustering-based summarization system that was proposed by Qazvinian and Radev (2008) to summarize different perspectives in citing sentences that reference a paper or a topic. To create summaries, C-LexRank constructs a fully connected network in which vertices are sentences, and edges are cosine similarities calculated using the tf-idf vectors of citation sentences. It then employs a hierarchical agglomeration clustering algorithm proposed by Clauset et al. (2004) to find communities of sentences that discuss the same scientific contributions. Once the graph is clustered and communities are formed, the method extracts sentences from different clusters to build a summary. It </context>
<context position="29868" citStr="Qazvinian and Radev, 2008" startWordPosition="4830" endWordPosition="4833">work by converting the input sentences into a network. Each sentence is represented by a node in the network, and the edges between sentences are given weight based on the similarities of sentences. They then run Pagerank on this network, and sentences are selected based on their Pagerank score in the network. For computing Pagerank, the network can either be pruned by removing edges that have weights less than a certain threshold, or a weighted version of Pagerank can be run on the network. The method can also be modified for query-focused summarization (Otterbacher et al., 2009). C-Lexrank (Qazvinian and Radev, 2008) modifies Lexrank by first running a clustering algorithm on the network to partition the network into different communities and then selecting sentences from each community by running Lexrank on the sub-network within each community. C-Lexrank was also used in the task of automated survey generation with encouraging results (Mohammad et al., 2009). Probabilistic content models: One of the first probabilistic content models seems to be BAYESSUM (Daum´e and Marcu, 2006), designed for query-focused summarization. BAYESSUM models a set of document collections using a hierarchical LDA style model.</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08), Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
<author>Saif M Mohammad</author>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Michael Whidby</author>
<author>Taesun Moon</author>
</authors>
<title>Generating extractive summaries of scientific paradigms.</title>
<date>2013</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="6147" citStr="Qazvinian et al., 2013" startWordPosition="953" endWordPosition="956">s. Section 3 describes each of the content models used in our experiments including HITSUM. Section 4 describes our experiments and Section 5 summarizes the results. We summarize the related work in Section 6 and conclude in Section 7. 2 Data Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization (Qazvinian and Radev, 2008) and used citing sentences to a set of relevant papers as the input for the summarizer (Mohammad et al., 2009; Qazvinian et al., 2013). However, in our prior work, we have observed that it’s difficult to generate coherent and readable summaries using just citing sentences and have proposed the use of sentences from introductory texts of papers that cite a number of important papers on a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of a larger system that should be able to generate coherent and readable survey articles, we use the introduction sentences for our exper</context>
</contexts>
<marker>Qazvinian, Radev, Mohammad, Dorr, Zajic, Whidby, Moon, 2013</marker>
<rawString>Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic, Michael Whidby, and Taesun Moon. 2013. Generating extractive summaries of scientific paradigms. J. Artif. Int. Res., 46(1):165–201, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
<author>Amjad Abu-Jbara</author>
</authors>
<title>The acl anthology network corpus. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>1--26</pages>
<contexts>
<context position="7020" citStr="Radev et al., 2013" startWordPosition="1103" endWordPosition="1106">a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of a larger system that should be able to generate coherent and readable survey articles, we use the introduction sentences for our experiments as well. The corpus we used for extracting our experimental data was the ACL Anthology Network, a comprehensive bibliographic dataset that contains full text and citations for papers in most of the important venues in natural language processing (Radev et al., 2013). An oracle method is used for selecting the initial set of papers for each topic. For each topic, the bibliographies of at least three human-written surveys were extracted, and any papers that appeared in more than one survey were added to the target document set for the topic. The text for summarization is extracted from introductory sections of papers that cite papers in the target document set. The intuition behind this is that the introductory sections of papers that cite these target document summarize the research in papers from the target document set as well as the relationships betwe</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, Abu-Jbara, 2013</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The acl anthology network corpus. Language Resources and Evaluation, pages 1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="33185" citStr="Teufel and Moens (2002)" startWordPosition="5358" endWordPosition="5361">f a paper. Nanba and Okumura (1999) explored using reference information to build a system for supporting writing survey articles. Their system extracts citing sentences that describe a referred paper and identify the type of reference relationships. The type of references can be one of the three: 1) type B that base on other researcher’s theory, 2) type C that compare with related works, or 3) type O representing relationships other than B or C. They posit that type C sentences are the most important for survey generation and can help show the similarities and differences among cited papers. Teufel and Moens (2002) propose a method for summarizing scientific articles based on rhetorical status of sentences in scientific articles. They annotate sentences in a corpus of 80 scientific articles with rhetorical status, where the rhetorical status can be one of aim (specific research goal), textual (section structure), own (neutral description of own work), background (generally accepted background), contrast (comparison with other work), 448 basis (agreement with or continuation of other work), and other (neutral description of other’s work). They describe classifiers for tagging the rhetorical status of sen</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Zhang</author>
<author>Marcelo Fiszman</author>
<author>Dongwook Shin</author>
<author>Bartlomiej Wilkowski</author>
<author>Thomas C Rindflesch</author>
</authors>
<title>Clustering cliques for graph-based summarization of the biomedical research literature.</title>
<date>2013</date>
<journal>BMC Bioinformatics,</journal>
<pages>14--182</pages>
<contexts>
<context position="34988" citStr="Zhang et al. (2013)" startWordPosition="5638" endWordPosition="5641">gns relevance scores to sentences in a paper based on their similarity to the set of citing sentences that reference the paper. More recently, Hoang and Kan (2010) present a method for automated related work generation. Their system takes as input a set of keywords arranged in a hierarchical fashion that describes a target paper’s topic. They hypothesize that sentences in a related work provide either background information or specific contributions. They use two different models to extract these two kinds of sentences using the input tree and combines them to create the final output summary. Zhang et al. (2013) explore methods for biomedical summarization by identifying cliques in a network of semantic predications extracted from citations. These cliques are then clustered and labeled to identify different points of view represented in the summary. 7 Conclusion and Future Work We have presented a new factoid-annotated dataset for evaluating content models for scientific survey article generation by annotating sentences from seven topics in natural language processing. We also introduce a new HITS-based content model called HITSUM for survey article generation that exploits the lexical information fr</context>
</contexts>
<marker>Zhang, Fiszman, Shin, Wilkowski, Rindflesch, 2013</marker>
<rawString>Han Zhang, Marcelo Fiszman, Dongwook Shin, Bartlomiej Wilkowski, and Thomas C. Rindflesch. 2013. Clustering cliques for graph-based summarization of the biomedical research literature. BMC Bioinformatics, 14:182.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>