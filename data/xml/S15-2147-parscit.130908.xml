<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001607">
<title confidence="0.826485">
SemEval 2015, Task 7: Diachronic Text Evaluation
</title>
<author confidence="0.945035">
Octavian Popescu Carlo Strapparava
</author>
<affiliation confidence="0.837662">
IBM Research FBK
Yorktown, NY 10598, USA Povo, TN 38123, Italy
</affiliation>
<email confidence="0.996235">
o.popescu@us.ibm.com strappa@fbk.eu
</email>
<sectionHeader confidence="0.995811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9949844375">
In this paper we describe a novel task, namely
the Diachronic Text Evaluation task. A cor-
pus of snippets which contain relevant infor-
mation for the time when the text was created
is extracted from a large collection of newspa-
pers published between 1700 and 2010. The
task, subdivided in three subtasks, requires the
automatic system to identify the time inter-
val when the piece of news was written. The
subtasks concern specific type of information
that might be available in news. The intervals
come in three grades: fine, medium and coarse
according to their length. The systems par-
ticipating in the tasks have proved that this a
doable task with very interesting possible con-
tinuations.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999872421052632">
Language changes over the time, even over rela-
tively small periods. For example, as the main in-
tent of publishing newspapers is to disseminate in-
formation to the population of a whole country, there
is an objective pressure to impose a standard and
to smooth over the dialectical differences. How-
ever, since the late 1600s, each generation has read
pieces of news containing new words, borrowed or
invented, exhibiting new drifts in the meanings of
old words, printed with different spelling etc.
The examples (1), (2), (3) and (4) below exhibit
a series of features which are useful to pin point the
year when the respective piece of news was created.
Well known global events, sense superseding, spe-
cific spelling and new vocabulary entry words are
all time relevant features. At a deeper level of anal-
ysis, time is revealed also by the mentions of named
entities, such as Security Council, the topic and the
linguistic genre are also relevant features.
</bodyText>
<listItem confidence="0.981279214285714">
1. Dictator Saddam Hussein ordered his troops to march into
Kuwait. After the invasion is condemned by the UN Se-
curity Council, the US has forged a coalition with allies.
Today American troops are sent to Saudi Arabia in Opera-
tion Desert Shield, protecting Saudi Arabia from possible
attack. circa 1990
2. We have cabled the English house from which we get it
and expect a reply to-morrow. circa 1900
3. Occasional selfies are acceptable, but uploading a new
picture of yourself every day is not necessary. circa 2014
4. ... The House of Samuel Sandbroke was brokt and sev-
eral Pistols discharged ... Her Majesty, for the better Dis-
covery of the Offenders, is pleased to promise Her most
Gracious Pardon for the said Crime. circa 1705
</listItem>
<bodyText confidence="0.999794285714286">
While for humans it is relatively easy to notice the
language differences between two texts, and even to
be accurate in determining the period when a piece
of news was written, for computational systems this
task is challenging. On the other hand, with the
availability of large time-tagged corpora, a computa-
tional system can perform various analyses and ex-
tract correlations that are impossible for humans to
know beforehand or acquire through manual inspec-
tion of the information scattered over huge collec-
tions of texts.
We propose to tackle the task of automatically
identifying the time period when a piece of news was
written. We provide a corpus of fragments of pieces
</bodyText>
<page confidence="0.959784">
870
</page>
<note confidence="0.953224">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 870–878,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999642">
of news, for both training and testing. The interest-
ing question is whether it is possible to automati-
cally determine the period when a text was written.
To this end, we have devised a SemEval 2015 task,
called Diachronic Text Evaluation, hence DTE task.
For this task, all aspects of language change may be
taken into account and systems of various levels of
analysis can be developed. The systems could ben-
efit for a training corpus and are evaluated against a
gold standard.
Organizing a diachronic task has proven to be a
difficult one and we made decisions regarding what
type of pieces of news are selected, what type of in-
formation they contain and how the evaluation could
be carried out. In a nutshell, we have selected pieces
of news of variable length ranging from ten to a cou-
ple of hundred words, and we have made a differen-
tiation between pieces of news that mention famous
named entities and those which do not. Our defini-
tion of famous is associated with the possibility of
finding information about the respective named en-
tities in external resources, such as Wikipedia. Con-
sequently, we proposed two subtasks according to
the difference above. For both tasks, the system has
to guess the correct time interval in which the text
was created. The intervals come in three types: fine,
medium and coarse, according to their length. The
third and last subtasks regard the phrases that carry
time information and the systems only have to de-
cide if a certain phrase in a given context is time
relevant, and not to assign a precise time interval to
the text.
The systems could use any type of algorithm to
analyze the text and find the time relevant informa-
tion. In fact, the main goal of the task was to identify
fragments of text which by themselves, or in con-
junction with a publicly available external resource,
are time relevant. As such, the task is a systematic
investigation into the actual capacity of NLP to com-
bine both textual and meta-textual information in or-
der to place a piece of text into a larger, temporal,
context.
To the best of our knowledge, the present task
is one of the very first systematic investigation in
diachronic corpora with a focus on the textual and
meta-textual features that are time relevant. We be-
lieve that systems for finding diachronic information
for pieces of text are very interesting from both theo-
retical and practical point of view. Socio and histori-
cal linguistics are both based on the analyses of spe-
cific linguistics variability in a certain epoch, loca-
tion, social class etc. The statistical methods are able
to discover correlations and linguistic provable evi-
dence of language change at all levels: morphologi-
cal, syntactical, semantic and discourse. It would be
physically impossible for a human, or a team of hu-
mans for what it matters, to analyze and corroborate
the data from hundreds of gigabytes of data and find
all the relevant differences. Looking at the distribu-
tion of words across timeline, salient periods, with
statistically non-random behavior, can be automati-
cally inferred (Popescu and Strapparava, 2013). The
structure of such periods, or epochs, are by far more
complex than what it could be manually performed.
From a practical point of view, diachronic systems
have a wide range of applications from emergent
fields such as computational forensics, computa-
tional journalism to more traditional tasks, such as
discourse similarity, sense shifting, readability and
narrative frameworks, etc.
The paper is organized as follow: in the next sec-
tion we review the relevant literature. In Section 3
we present the main motivation for the DTE task
and the three subtasks with their specific corpora. In
Section 4 we present the data format and the evalua-
tion method together with a simple baseline. In Sec-
tion 5 we discuss the main properties of the submit-
ted systems and their results. The paper ends with
a substantial section on conclusion and main future
research direction in DTE.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999832428571429">
The availability of large time annotated corpora like
Google N-gram open the perspective of a new field
of the research which focuses on the distribution of
the linguistics elements in certain periods. (Popescu
and Strapparava, 2014; Popescu and Strapparava,
2013) showed how such corpora can be used to infer
transition periods between epoch with specific char-
acteristics. A ground breaking paper, (Niculae et
al., 2014) focuses on historical documents in three
languages, English, Portuguese and Romanian. The
paper shows how statistical method can be used to
predict the date when the documents have been cre-
ated. The similarity of the ideas in the present task
and their paper, although developed in completely
</bodyText>
<page confidence="0.996471">
871
</page>
<bodyText confidence="0.999939">
autonomy, prove that there is indeed a major interest
in building diachronic systems and that the time is
high for this task. We believe that there is a lot to do
in this emergent field.
</bodyText>
<sectionHeader confidence="0.988101" genericHeader="method">
3 Task Description
</sectionHeader>
<bodyText confidence="0.999934307692308">
In this section we present the main motivations for
a diachronic task and in particular, we focus on how
these motivations have influenced the choice in the
present task. Let us start from the example (1)-(4)
presented in Section 1.
We can observe that the choice of words, the mor-
phology and word particular meaning, are an impor-
tant part of time detection. Words like brokt, selfie,
spellings like to-morrow or a sense like the one of
the verb cable in (2) are used only within a certain
period. Also, the topics are time specific and the
reader may not even need to consult other sources
in order to realize that an American war in Saudi
Arabia and Her Majesty pardon for a domestic inci-
dent cannot possibly happen in the same period, as
much as telegraphing and uploading selfies cannot
either. Any of these clues seems to be a strong clue,
but it would have been difficult to consider them be-
fore seeing this particular set of sentences. Intu-
itively, if one would read another set of sentences,
some other clues, equally strong, are found. It makes
sense to ask ourselves: How many such clues exist?
Can such clues be systematically found and consis-
tently organized? A human investigation of large
corpora is hopeless, as billions of sentences must be
inspected.
</bodyText>
<subsectionHeader confidence="0.999431">
3.1 From News Corpora to Diachronic Data
and Tasks
</subsectionHeader>
<bodyText confidence="0.994752225">
To answer this question we may want to link the lin-
guistic information to the timeline. A big quantity of
data, chronologically ordered, allows accurate sta-
tistical statements regarding the covariance between
the frequencies of two or more terms over a certain
period of time. By discovering significant statisti-
cal changes in word usage behavior, it is possible
to define epoch boundaries. Inside these epochs the
news are written in a rather uniform way. However,
small changes as well as reference to famous histor-
ical events may lead to the formation of sub epochs.
Clearly, the mentioning of specific historical
events makes it much easier for a diachronic system.
The system must be able to consult an external
resource such as Wikipedia, in order to assign a
time stamp to the extracted entities. However, an
extra analysis is required in order to make sure that
the text does not refer to the respective historical
event as past experience. On the other hand, surface
features, such as spelling, reference to institutions
that are specific to a epoch, or the usage of words in
specific context, can be used to infer a time interval
within which the text was written. Generally, this
interval is much larger when compared to the time
stamp assigned to the historical events and unless
the system is provided with a crystal globe, no
more accurate predictions can be made. It becomes
clear that one needs to differentiate between the
two types of information discussed above. And,
also, that different precision is to be expected
between these two subtasks. Let us call subtask 1
the diachronic task which considers pieces of news
in which specific historical events, named entities
etc. are clearly mentioned and let us call subtask
2 the diachronic task in which such information
is missing, but in which there is enough surface
information to assign a time interval, at least for a
human. We present and discuss below a few typical
examples for each of the subtasks mentioned here.
Task 1
</bodyText>
<listItem confidence="0.993914416666667">
5. At the Court at St.James’s, the 29th Day of March,
and1744 Present, the King’s most excellent Majesty in
Council. His Majesty’s Declaration of War against the
French King.
6. The Troubles which broke out in Germany on Account
of the Succession of the late Emperor Charles the Sixth,
having been begun, and carried on, by the Instigation, As-
sistance, and Support of the French King
7. By 1971 about one-third of Edison’s electric output will
be generated with nuclear capacity,
8. 1935 Ford V-8 Tudor Sedan Only an year old. not a flaw
in it anywhere.
</listItem>
<bodyText confidence="0.999585428571429">
In example (5) the date is clearly indicated and
the phrase war against the French King anchors the
text very precisely in time. The mention of the late
Emperor Charles the Sixth in example (6) pinpoints
the time very precisely. The epoch is indicated in
example (7) as nuclear capacity cannot possible
happen before mid sixties. The last example, (8),
</bodyText>
<page confidence="0.986811">
872
</page>
<bodyText confidence="0.9654435">
requires a subtraction of the dates expressed via
temporal phrase, 1935 and one year old respec-
tively. To sum up, task 1 requires systems to work
with temporal expressions, name entity recognition
and external resources, such as Wikipedia.
Task 2
</bodyText>
<listItem confidence="0.990734833333333">
9. By Letters from the Frontiers there is Advice, that the
French Intendant has given Orders for tracing out a Camp
near Givet for 10000 Men;
10. Receipts at Chicago to-day. Wheats 206 cars; corn fill;
oats, 181 cars. Estimated receipts to-morrow. Wheat, 400
cars; corn, 85 cars; oats, 235 cars; hogs, 16,000 head.
</listItem>
<bodyText confidence="0.92928585">
11. There is a theory evolved by a French scientist to the
effect that tho human. race is diminishing In size and
will finally become microscopic and vanish into thin air.
He says that statistics from the days of the giants to the
present time prove that man is getting smaller and shorter
and more diminutive live in every way.
12. Red Blankets $1.98 a pair. White Blankets 69c a pair.Bed
Comforts 69c each. Heavy Knit Skirts 69c each.
Advice was used at the beginning of the 18th cen-
tury for military information. The fact that the event
takes place in Europe, Givet, and what is a small
amount of troops for modern times is mentioned,
plus the whole linguistics register of the text deter-
mines clearly the date of the text in Example (9).
As displayed in example (10), the spelling, to-day
and to-morrow is a characteristics of the period be-
tween 19th and 20th century, and the quantity in-
volved shows that indeed the time stamp is about
that time. The scientific language used in example
(11), especially the term statistics shows that the text
cannot be produced earlier than the second half of
the 19th century, yet the mentioning of days of gi-
ants shows clearly that the science was not yet fully
evolved and it was still tributary to an ecclesiasti-
cal view of the world. Thus, the text must have
been produced around the last quarter of 18th cen-
tury. The prices specified in example (12) are clearly
related to an epoch when the American dollar had a
very high value, but yet, it has to be close enough
to the modern times in order for an advertisement to
the bed comforts to be made.
The examples above, which are prototypical for
task 2, show that in order to identify correctly the
time interval a system must corroborate different
types of information, among which an important
role play the linguistics register and the details spe-
cific to each epoch. In fact, there are few NLP sys-
tems, if any, which are able to identify and cluster
accordingly to these features. This is why our main
effort was directed to provide a good coverage of
diachronic corpus especially for task 2, see next sec-
tion. As we worked on compiling the data for task 2
it becomes clear that a different accuracy is to be ex-
pected between task 1 and task 2, and consequently,
different types of intervals must be provided for the
two subtasks.
The focus of subtask number three is on individ-
ual phrases in context. There are certain phrases
that are time specific. In fact we can distinguish
two categories of such phrases: (i) phrases that have
been used preponderantly in a certain epoch and (ii)
phrases that have a specific meaning within a certain
epoch. For the first type, it is sufficient to recognize
them, while for the second, a deeper analysis is
necessary and the context in which they are used is
relevant. A system able to deal with the challenges
posed by task 1 and, especially task 2, must be able
to correctly make the distinction between phrases,
which carry temporal value and those which do not.
Task 3
</bodyText>
<listItem confidence="0.656137375">
13. According to Advices from Germany, a Rupture between
the Courts of Dresden and Berlin is at Hand
14. The Regiments of Guelderland, and another belonging to
this Republick, which were accused to not charging the
Enemy
15. corporal punishment
16. his artillery retreat so that he constantly marched under
the grapeshot
</listItem>
<bodyText confidence="0.9835519">
For the contiguous phrases marked with italic for-
mat in the examples (13)-(15), a system must be able
to decide whether, in the provided context, there is
temporal information attached to them. The con-
text is crucial, because, out of context, the temporal
value may be cancelled. In a sentence, more than
a phrase can be proposed. Roughly, all the features
discussed above for task 1 and task 2 are present in
the examples of task 3. From this point of view, task
3 can be viewed as a classical feature selection task.
</bodyText>
<page confidence="0.993951">
873
</page>
<subsectionHeader confidence="0.997875">
3.2 News Corpus and Data Statistics
</subsectionHeader>
<bodyText confidence="0.99518972">
Instead of considering whole pieces of news, we fo-
cused on individual parts of text that may carry rele-
vant time information. The data proposed for train-
ing and test is made out of snippets of text of variable
length. Typically a snippet will have between tens to
a couple of hundred of words.
We used a series of journals available in electronic
format from extracting the data. Most of the elec-
tronic archive do not make available newspapers that
are older than the beginning of 19th century. How-
ever, we wanted to cover the whole period between
1,700 to 2,010. A second detail to consider is the
diversity of the sources. Most of the archives are
linked to one journal, which restricts the scope of
the news to one location and one community. An-
other aspect that we want to consider for our data is
to be hard to find it by searching the web. That will
kind of prohibiting a simple system that only does
string match to correctly solve the task. A system
that find the whole piece of news and its publishing
date on the web , may produce good results for task
1 , but would fail to do so for task 2 and task 3.
In order to cope with this restriction we subscribed
to several web newspaper archives. The influence of
each of these sources in our data set is specified in
</bodyText>
<tableCaption confidence="0.945393">
Table 1.
</tableCaption>
<table confidence="0.9087758">
Source address Data task coverage
NPA newspaper.achive.com 75%
SPR archive.spectator.co.uk 12%
BDY www.bodley.ox.ac.uk/ilej/ 10%
OTHER 3%
</table>
<tableCaption confidence="0.998241">
Table 1: Data Sources.
</tableCaption>
<bodyText confidence="0.999797692307692">
The separation of the data into trial, training and
test is presented in table 2. The data for task1 is not
very rich. This is because the learning methodology
for task 1 is pretty clear, so we are mainly interested
in having a statistical sufficient pool for drawing ac-
curate conclusions after the evaluation of the task.
For task 2 the methodology is still a matter of re-
search we want to provide as much data as possible
in order for machine learning systems to be able to
learn both the surface and meta-textual features. For
task 3, there is no need for training. A phrase is or
it is not time relevant, and each case must be treated
separately.
</bodyText>
<table confidence="0.9993598">
data task 1 task 2 task 3
trial 17 87 7
training 167 5, 436 NA
test 267 1, 041 108
total 451 6, 568 115
</table>
<tableCaption confidence="0.995309">
Table 2: Data size.
</tableCaption>
<figureCaption confidence="0.976831">
Figure 1: Task 2 distribution.
</figureCaption>
<bodyText confidence="0.998442137931034">
The snippets cover the last three centuries. How-
ever, the number of snippets per year may vary. In
Figure 1 we plot the distribution of the number of
snippets for each time interval of 25 years for task
2. With the notable difference of the first 50 years
of the 18th century, each quarter of the century is
covered by a number between 200 to 400 snippets,
which men an average between 4 to 8 snippets per
year. The first two quarters of the 18th centuries are
substantially better covered: 1, 343 and 780 snippets
respectively. The explanation for this skewness is
two fold: (1) the data for the beginning of the 18th
century is much more difficult to acquire than the
rest of the data. Basically the text exists only as pdf
and the OCRss are not trained to work on this kind
of text. Therefore, it is really hard to get a good cor-
pus for the beginning of 18th century, but, as this is
in fact our goal, we pursued into acquiring the snip-
pets for this period with priority. (2) the data at the
beginning of the 18th century is the one which has
a rich variation of linguistic constructions, and the
present corpus can be used further for different anal-
yses. We note here, that from the point of view of
lexical variability, the 19th century is very rich and
there is a huge jump from the previous century in the
size of vocabulary.
In this section we have defined the broad scope
of the DTE task, we have reviewed the main char-
acteristics of the subtasks, and we have shown to
</bodyText>
<page confidence="0.996475">
874
</page>
<bodyText confidence="0.9999512">
what type of information must be extracted and man-
aged by a diachronic system. In the next section we
present the details of task organization - the format
of data, the input and expected output and the evalu-
ation procedure.
</bodyText>
<sectionHeader confidence="0.985476" genericHeader="method">
4 Task Organization
</sectionHeader>
<subsectionHeader confidence="0.994853">
4.1 Data Format
</subsectionHeader>
<bodyText confidence="0.999988041666667">
As this task is the first of its genre, it is hard to
know priorly how accurate a system can be in deter-
mining epochs and sub epochs from a news corpus.
On the basis of our previous experience (Popescu
and Strapparava, 2014; Popescu and Strapparava,
2013), we have reasons to believe that separation
into epochs is not linear: the epochs tend to change
much faster in modern times. However, the topics
seemed to be much better differentiate a couple of
hundred of years ago than in the modern times. All
in all, it seems that a 50 years time interval is some-
thing that could be inferred without carrying out a
special analysis for both tasks T1 and T2. Thus, in
order to be able to judge justly the contribution to
each system, a shorter time interval should be taken
into account. We have decided to consider an inter-
val centered around the year in which the news was
actually produced and to have three types of inter-
vals: fine, medium and coarse. The three intervals
are included one in another, and for all three there is
an equal number of years to the left and to the right
of the actual date. This condition creates intervals
with even number of years. We considered the inter-
vals for task T1 and T2 as presented in Table 3.
</bodyText>
<equation confidence="0.49388">
accuracy task1 task2
fine 2 6
medium 6 12
coarse 12 20
</equation>
<tableCaption confidence="0.997718">
Table 3: Time intervals.
</tableCaption>
<bodyText confidence="0.999977615384615">
The system has to choose the correct time period,
e.g. 1700-1720, ..., 1900-1920, ..., from the given
set of contiguous intervals which cover the whole
period considered, i.e. from 1700 to 2014. In both
subtasks 1 and 2 the explicit choice of intervals is
available. Only one interval is correct for each level
of accuracy. In the training data each snippet has an
unique ID, followed by three lines, one for each level
of precision and each containing the set of intervals
with the specific length. Only one interval is marked
with yes in training, while in test all are marked with
no. At the evaluation time, the system performances
are compared against the gold standard.
</bodyText>
<subsectionHeader confidence="0.996283">
4.2 Evaluation and baseline
</subsectionHeader>
<bodyText confidence="0.999992294117647">
The results on each snippet can be evaluated individ-
ually. The system has to specify the chosen interval,
and if this is the same as the one specified in the gold
standard, then the answer is correct, otherwise not.
However, the distance from the chosen interval and
the correct interval is relevant. Between two systems
that have exactly the same number of strictly correct
answers, it is preferable to work with the one that has
the minimal error average. Keeping in mind these
ideas we implemented an evaluation script, which
takes into account the distance between the chosen
and the gold standard interval. The score is normal-
ized to [0,1) interval. The correct answer is marked
with a zero loss and a ten or more interval difference
is marked with 0.99 loss. According to the number
of intervals off, a loss is computed between 0 and 1,
see Table 4. The final score is 1- loss. The evalua-
tion script also outputs the number of years by which
the system was off and their distributions, that is, the
distribution of loss function from 0 to 9.
We have considered a simple baseline, that is ran-
dom choice. Another candidate is to always choose
the median interval, like 1850, for example. How-
ever, both options are bad, and the number of 9 or
more intervals off is very large, these baselines tend
to have a very high loss function. Their behavior
is not actually very different one another. That is
why we choose officially to have just one baseline,
namely random choice. This choice is supported
by the following reason: the median produces ev-
ery time the same output, while the random choice
is different. Averaging over several runs of the ran-
dom choice we have a much better approximation of
what are the baseline performances.
</bodyText>
<footnote confidence="0.8751115">
intervals off loss
0 0
1 .1
2 .15
3 .2
4 .4
</footnote>
<tableCaption confidence="0.990977">
Table 4: Loss as function of off intervals.
</tableCaption>
<figure confidence="0.992562166666667">
intervals off loss
5 .5
6 .6
7 .8
8 .9
&gt; 9 .99
</figure>
<page confidence="0.994008">
875
</page>
<sectionHeader confidence="0.884849" genericHeader="method">
5 Submitted Runs and Results
</sectionHeader>
<bodyText confidence="0.999936866666667">
There were 7 teams that expressed their interest in
the task, but in the end there were only four teams
which successfully submitted the results. The num-
ber of submitted runs was less, though, as not all
the teams participated in all the tasks. In fact there
is only one team that participated in all three tasks,
namely IXA. As such, we are glad to acknowledge
them as the winner of the tasks, if the average over
the all three task is made.
We are going to present the team by including
their own description of their systems. More details
can be found in their system paper, submitted to the
SemEval 2015. Then we present their results and
discuss the performances of their system individu-
ally.
</bodyText>
<subsectionHeader confidence="0.931255">
5.1 Systems
</subsectionHeader>
<bodyText confidence="0.998215">
A short description of the system follows:
</bodyText>
<sectionHeader confidence="0.544065" genericHeader="method">
I AMBRA
</sectionHeader>
<bodyText confidence="0.9999297">
Our approach is based on the learning-to-rank
framework using pairwise comparisons, pre-
viously proposed for temporal text modelling
by (Niculae et al., 2014). We train a classi-
fier to learn which document out of a pair is
older and which is newer. If two documents
come from overlapping intervals, then their or-
der cannot be determined with certainty, so the
pair is not used in training. We use the prop-
erty of linear models to extend a set of pair-
wise decisions into a ranking of test documents
(Joachims, 1998). In light of this, our system
is named AMBRA (Anachronism Modelling by
Ranking). We used four types of features: doc-
ument length meta-features, stylistic, grammat-
ical, and lexical features. The four stylistic fea-
tures used were previously proposed by (Stajner
and Zampieri, 2013): Average Word Length
(AWL), Average Sentence Length (ASL), Lex-
ical Density (LD) and Lexical Richness (LR).
</bodyText>
<sectionHeader confidence="0.972886" genericHeader="method">
II IXA
</sectionHeader>
<bodyText confidence="0.999931285714286">
Four different approaches are undertaken in or-
der to automatically determine the period of
time in which a piece of news was written:
the first approach consists of searching for the
mentioned time period within the text. The
second approach, on the other hand, consists
of searching for named entities present in the
text and then establishing the period of time
by linking these to Wikipedia. The third ap-
proach uses Google NGrams and, to conclude,
the fourth approach consists of using linguis-
tic features that are significant with respect to
language change in combination with machine
learning.
</bodyText>
<sectionHeader confidence="0.90203" genericHeader="method">
III UCD
</sectionHeader>
<bodyText confidence="0.999980444444444">
We approach the task of dating a text (sub-
task 2) as a stylistic classification problem. For
each level of granularity (6-year, 12-year, and
20-year), we train a multi-class SVM classi-
fier using a set of stylistic features extracted
from the texts. These features include fre-
quency counts of character, word, and POS-tag
n-grams, and syntactic phrase-structure rule oc-
currences. We also incorporate date estimates
of syntactic nodes from the Google syntactic
n-grams database. Our submission is a clas-
sifier incorporating all of these features and
trained on the task training data. We find that
of the stylistic features, character n-grams are
the most informative. The Google syntactic n-
gram dates, while weak predictors on their own,
are also among the most informative features in
our combined classifier.
</bodyText>
<sectionHeader confidence="0.940733" genericHeader="method">
IV USAAR
</sectionHeader>
<bodyText confidence="0.999998285714286">
We built a crawler to crawl the text snippets in
the task and also we found that the webpages
retrieved were dated. We use those dates as an-
swers to the task evaluation. We then crawl the
two webpages fully and then clean the website
to produce a corpus of diachronic texts for fu-
ture use (in total 24,280 articles).
</bodyText>
<subsectionHeader confidence="0.983881">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999980142857143">
The results are presented in Table 5. The acc column
lists the score of the system, computed as described
in Section 4.2, and the P shows how many times the
system was perfectly accurate, that is, it found the
exact interval. The fine grade seems to be a prob-
lem for the big majority of the systems. The only
system which reports very high value, USAAR, is
</bodyText>
<page confidence="0.996479">
876
</page>
<table confidence="0.99621425">
System Task 1 Task 2 Task 3
F M C F M C
acc P acc P acc P acc P acc P acc P acc
AMBRA .167 .037 .367 .071 .554 .074 .605 .143 .767 .143 .868 .292 NA
IXA .187 .02 .375 .041 .557 .090 0.261 .037 .428 .067 0.622 .098 .573
UCD NA NA NA NA NA NA 0.759 .463 .846 .472 0.910 0.542 .551
USAAR .953 .910 .972 .928 .981 .943 NA NA NA NA NA NA NA
baseL .107 .112 .174 .187 .377 .037 .224 0 .391 0 .524 0 .237
</table>
<tableCaption confidence="0.999313">
Table 5: DTE results.
</tableCaption>
<bodyText confidence="0.999969461538462">
based on web crawling, thus is not a generalizable
method. In fact, the team participated only in task 1.
The medium grade seems to be doable, all systems
scoring better than the baseline. For the coarse grade
the systems outperform the baseline by several tens
of percent and obtain very good results, with accu-
racy between 0.868-0.91. These results confirm the
fact that the task is doable and a 20 years interval is
appropriate for DTE. We hope these results can be
further improved in the future.
The results for task 3 show that this task is indeed
difficult, and even if the baseline has been overcome
with a great margin, the results show that the system
could be improved further. We plot the distribution
of errors for the system which participated in task 2,
see Figure 2. Interestingly, AMBRA and UCD have
very similar distributional curves, with the exception
of perfect guess. The IXA system has a more regu-
lar shape and its errors seem to be evenly distributed
with a big exception for the maximum error cate-
gory. Maybe an interpolation between these three
methods could lead to a better overall result.
To conclude, we are glad we received different
systems which produce good and very good results.
These initial ideas represent a valuable pool from
which further work can be developed in the future.
</bodyText>
<figureCaption confidence="0.892718">
Figure 2: Task 2 medium error distribution.
</figureCaption>
<sectionHeader confidence="0.986229" genericHeader="conclusions">
6 Conclusion and Further Research
</sectionHeader>
<bodyText confidence="0.999803783783784">
In this paper we described the Diachronic Text Eval-
uation task. We explain the main motivation for this
task and we presented what the main issues behind
the diachronic task are and how these issues have
influenced our decisions. We presented the sources
and the distribution of snippets in task data. A short
paragraph description for each of the participating
systems is provided, and we carried out a global
evaluation. Finally we have provided an analysis of
errors for task 2.
We think that there are some very interesting di-
rections we would like to investigate further. The
first one is to consolidate the actual corpus. This a
necessary step in order to build a solid basis for fur-
ther experiments and developments. We would like
to improve the quality and quantity of training text
for allowing search of changes at all linguistics level.
We would like to work more in revealing the connec-
tion between diachronic evaluation and epoch dis-
covery.
Another direction of research is a systematic
study of the textual and meta-textual features that
are relevant for the DTE task and what their individ-
ual contributions to the over all accuracy is. Besides
the overt temporal features we need to identify, the
linguistics register, the topics and the discourse fea-
tures - from grammar to pragmatics must be taken
into account. We believe that DTE is a very good in-
dicator on the performance of machine learning sys-
tems for the meta-textual feature management.
Last, but not least, we would like to bridge the gap
between different old and emergent fields, such as
sociology, socio-historic linguistics and social com-
putational analysis, computational journalism and
forensic linguistics respectively. We think that NLP
systems are able to tackle the difficult issues posed
by this research.
</bodyText>
<page confidence="0.996005">
877
</page>
<sectionHeader confidence="0.995881" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9983852">
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning, pages 137–142.
Vlad Niculae, Marcos Zampieri, Liviu Dinu, and Alina
Ciobanu. 2014. Temporal text ranking and auto-
matic dating of texts. In Proceedings of EACL 2014,
Gothenburg, Sweden.
Octavian Popescu and Carlo Strapparava. 2013. Be-
hind the Times: Detecting epoch changes using large
corpora. In Proceedings of the 6th International
Joint Conference on Natural Language Processing
(IJCNLP-2013), Nagoya, Japan, October.
Octavian Popescu and Carlo Strapparava. 2014. Time
corpora: Epochs, opinions and changes. Knowledge-
Based Systems, 69:3—13, October.
Sanja Stajner and Marcos Zampieri. 2013. Stylistic
changes for temporal text classification. In Proceed-
ings of the 16th International Conference on Text,
Speech and Dialogue (TSD2013).
</reference>
<page confidence="0.997668">
878
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.856266">
<title confidence="0.995605">SemEval 2015, Task 7: Diachronic Text Evaluation</title>
<author confidence="0.99884">Octavian Popescu Carlo Strapparava</author>
<affiliation confidence="0.999903">IBM Research FBK</affiliation>
<address confidence="0.993807">Yorktown, NY 10598, USA Povo, TN 38123, Italy</address>
<email confidence="0.990162">o.popescu@us.ibm.comstrappa@fbk.eu</email>
<abstract confidence="0.992572647058824">In this paper we describe a novel task, namely the Diachronic Text Evaluation task. A corpus of snippets which contain relevant information for the time when the text was created is extracted from a large collection of newspapers published between 1700 and 2010. The task, subdivided in three subtasks, requires the automatic system to identify the time interval when the piece of news was written. The subtasks concern specific type of information that might be available in news. The intervals come in three grades: fine, medium and coarse according to their length. The systems participating in the tasks have proved that this a doable task with very interesting possible continuations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with Support Vector Machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="26224" citStr="Joachims, 1998" startWordPosition="4581" endWordPosition="4582">ormances of their system individually. 5.1 Systems A short description of the system follows: I AMBRA Our approach is based on the learning-to-rank framework using pairwise comparisons, previously proposed for temporal text modelling by (Niculae et al., 2014). We train a classifier to learn which document out of a pair is older and which is newer. If two documents come from overlapping intervals, then their order cannot be determined with certainty, so the pair is not used in training. We use the property of linear models to extend a set of pairwise decisions into a ranking of test documents (Joachims, 1998). In light of this, our system is named AMBRA (Anachronism Modelling by Ranking). We used four types of features: document length meta-features, stylistic, grammatical, and lexical features. The four stylistic features used were previously proposed by (Stajner and Zampieri, 2013): Average Word Length (AWL), Average Sentence Length (ASL), Lexical Density (LD) and Lexical Richness (LR). II IXA Four different approaches are undertaken in order to automatically determine the period of time in which a piece of news was written: the first approach consists of searching for the mentioned time period </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with Support Vector Machines: learning with many relevant features. In Proceedings of the European Conference on Machine Learning, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlad Niculae</author>
<author>Marcos Zampieri</author>
<author>Liviu Dinu</author>
<author>Alina Ciobanu</author>
</authors>
<title>Temporal text ranking and automatic dating of texts.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL 2014,</booktitle>
<location>Gothenburg,</location>
<contexts>
<context position="7853" citStr="Niculae et al., 2014" startWordPosition="1305" endWordPosition="1308"> Section 5 we discuss the main properties of the submitted systems and their results. The paper ends with a substantial section on conclusion and main future research direction in DTE. 2 Related Work The availability of large time annotated corpora like Google N-gram open the perspective of a new field of the research which focuses on the distribution of the linguistics elements in certain periods. (Popescu and Strapparava, 2014; Popescu and Strapparava, 2013) showed how such corpora can be used to infer transition periods between epoch with specific characteristics. A ground breaking paper, (Niculae et al., 2014) focuses on historical documents in three languages, English, Portuguese and Romanian. The paper shows how statistical method can be used to predict the date when the documents have been created. The similarity of the ideas in the present task and their paper, although developed in completely 871 autonomy, prove that there is indeed a major interest in building diachronic systems and that the time is high for this task. We believe that there is a lot to do in this emergent field. 3 Task Description In this section we present the main motivations for a diachronic task and in particular, we focu</context>
<context position="25868" citStr="Niculae et al., 2014" startWordPosition="4512" endWordPosition="4515">all three tasks, namely IXA. As such, we are glad to acknowledge them as the winner of the tasks, if the average over the all three task is made. We are going to present the team by including their own description of their systems. More details can be found in their system paper, submitted to the SemEval 2015. Then we present their results and discuss the performances of their system individually. 5.1 Systems A short description of the system follows: I AMBRA Our approach is based on the learning-to-rank framework using pairwise comparisons, previously proposed for temporal text modelling by (Niculae et al., 2014). We train a classifier to learn which document out of a pair is older and which is newer. If two documents come from overlapping intervals, then their order cannot be determined with certainty, so the pair is not used in training. We use the property of linear models to extend a set of pairwise decisions into a ranking of test documents (Joachims, 1998). In light of this, our system is named AMBRA (Anachronism Modelling by Ranking). We used four types of features: document length meta-features, stylistic, grammatical, and lexical features. The four stylistic features used were previously prop</context>
</contexts>
<marker>Niculae, Zampieri, Dinu, Ciobanu, 2014</marker>
<rawString>Vlad Niculae, Marcos Zampieri, Liviu Dinu, and Alina Ciobanu. 2014. Temporal text ranking and automatic dating of texts. In Proceedings of EACL 2014, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
<author>Carlo Strapparava</author>
</authors>
<title>Behind the Times: Detecting epoch changes using large corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP-2013),</booktitle>
<location>Nagoya, Japan,</location>
<contexts>
<context position="6539" citStr="Popescu and Strapparava, 2013" startWordPosition="1092" endWordPosition="1095">f specific linguistics variability in a certain epoch, location, social class etc. The statistical methods are able to discover correlations and linguistic provable evidence of language change at all levels: morphological, syntactical, semantic and discourse. It would be physically impossible for a human, or a team of humans for what it matters, to analyze and corroborate the data from hundreds of gigabytes of data and find all the relevant differences. Looking at the distribution of words across timeline, salient periods, with statistically non-random behavior, can be automatically inferred (Popescu and Strapparava, 2013). The structure of such periods, or epochs, are by far more complex than what it could be manually performed. From a practical point of view, diachronic systems have a wide range of applications from emergent fields such as computational forensics, computational journalism to more traditional tasks, such as discourse similarity, sense shifting, readability and narrative frameworks, etc. The paper is organized as follow: in the next section we review the relevant literature. In Section 3 we present the main motivation for the DTE task and the three subtasks with their specific corpora. In Secti</context>
<context position="21284" citStr="Popescu and Strapparava, 2013" startWordPosition="3689" endWordPosition="3692"> broad scope of the DTE task, we have reviewed the main characteristics of the subtasks, and we have shown to 874 what type of information must be extracted and managed by a diachronic system. In the next section we present the details of task organization - the format of data, the input and expected output and the evaluation procedure. 4 Task Organization 4.1 Data Format As this task is the first of its genre, it is hard to know priorly how accurate a system can be in determining epochs and sub epochs from a news corpus. On the basis of our previous experience (Popescu and Strapparava, 2014; Popescu and Strapparava, 2013), we have reasons to believe that separation into epochs is not linear: the epochs tend to change much faster in modern times. However, the topics seemed to be much better differentiate a couple of hundred of years ago than in the modern times. All in all, it seems that a 50 years time interval is something that could be inferred without carrying out a special analysis for both tasks T1 and T2. Thus, in order to be able to judge justly the contribution to each system, a shorter time interval should be taken into account. We have decided to consider an interval centered around the year in which</context>
</contexts>
<marker>Popescu, Strapparava, 2013</marker>
<rawString>Octavian Popescu and Carlo Strapparava. 2013. Behind the Times: Detecting epoch changes using large corpora. In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP-2013), Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
<author>Carlo Strapparava</author>
</authors>
<title>Time corpora: Epochs, opinions and changes. KnowledgeBased Systems,</title>
<date>2014</date>
<pages>69--3</pages>
<contexts>
<context position="7664" citStr="Popescu and Strapparava, 2014" startWordPosition="1276" endWordPosition="1279">resent the main motivation for the DTE task and the three subtasks with their specific corpora. In Section 4 we present the data format and the evaluation method together with a simple baseline. In Section 5 we discuss the main properties of the submitted systems and their results. The paper ends with a substantial section on conclusion and main future research direction in DTE. 2 Related Work The availability of large time annotated corpora like Google N-gram open the perspective of a new field of the research which focuses on the distribution of the linguistics elements in certain periods. (Popescu and Strapparava, 2014; Popescu and Strapparava, 2013) showed how such corpora can be used to infer transition periods between epoch with specific characteristics. A ground breaking paper, (Niculae et al., 2014) focuses on historical documents in three languages, English, Portuguese and Romanian. The paper shows how statistical method can be used to predict the date when the documents have been created. The similarity of the ideas in the present task and their paper, although developed in completely 871 autonomy, prove that there is indeed a major interest in building diachronic systems and that the time is high fo</context>
<context position="21252" citStr="Popescu and Strapparava, 2014" startWordPosition="3685" endWordPosition="3688">his section we have defined the broad scope of the DTE task, we have reviewed the main characteristics of the subtasks, and we have shown to 874 what type of information must be extracted and managed by a diachronic system. In the next section we present the details of task organization - the format of data, the input and expected output and the evaluation procedure. 4 Task Organization 4.1 Data Format As this task is the first of its genre, it is hard to know priorly how accurate a system can be in determining epochs and sub epochs from a news corpus. On the basis of our previous experience (Popescu and Strapparava, 2014; Popescu and Strapparava, 2013), we have reasons to believe that separation into epochs is not linear: the epochs tend to change much faster in modern times. However, the topics seemed to be much better differentiate a couple of hundred of years ago than in the modern times. All in all, it seems that a 50 years time interval is something that could be inferred without carrying out a special analysis for both tasks T1 and T2. Thus, in order to be able to judge justly the contribution to each system, a shorter time interval should be taken into account. We have decided to consider an interval c</context>
</contexts>
<marker>Popescu, Strapparava, 2014</marker>
<rawString>Octavian Popescu and Carlo Strapparava. 2014. Time corpora: Epochs, opinions and changes. KnowledgeBased Systems, 69:3—13, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanja Stajner</author>
<author>Marcos Zampieri</author>
</authors>
<title>Stylistic changes for temporal text classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the 16th International Conference on Text, Speech and Dialogue (TSD2013).</booktitle>
<contexts>
<context position="26504" citStr="Stajner and Zampieri, 2013" startWordPosition="4622" endWordPosition="4625">n a classifier to learn which document out of a pair is older and which is newer. If two documents come from overlapping intervals, then their order cannot be determined with certainty, so the pair is not used in training. We use the property of linear models to extend a set of pairwise decisions into a ranking of test documents (Joachims, 1998). In light of this, our system is named AMBRA (Anachronism Modelling by Ranking). We used four types of features: document length meta-features, stylistic, grammatical, and lexical features. The four stylistic features used were previously proposed by (Stajner and Zampieri, 2013): Average Word Length (AWL), Average Sentence Length (ASL), Lexical Density (LD) and Lexical Richness (LR). II IXA Four different approaches are undertaken in order to automatically determine the period of time in which a piece of news was written: the first approach consists of searching for the mentioned time period within the text. The second approach, on the other hand, consists of searching for named entities present in the text and then establishing the period of time by linking these to Wikipedia. The third approach uses Google NGrams and, to conclude, the fourth approach consists of us</context>
</contexts>
<marker>Stajner, Zampieri, 2013</marker>
<rawString>Sanja Stajner and Marcos Zampieri. 2013. Stylistic changes for temporal text classification. In Proceedings of the 16th International Conference on Text, Speech and Dialogue (TSD2013).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>