<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000854">
<title confidence="0.996087">
Improving Chinese Word Segmentation on Micro-blog Using Rich
Punctuations
</title>
<author confidence="0.974467">
Longkai Zhang Li Li Zhengyan He Houfeng Wang∗ Ni Sun
</author>
<affiliation confidence="0.854715">
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
</affiliation>
<email confidence="0.959141">
zhlongk@qq.com, li.l@pku.edu.cn, hezhengyan.hit@gmail.com,
wanghf@pku.edu.cn,sunny.forwork@gmail.com
</email>
<sectionHeader confidence="0.993836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987135">
Micro-blog is a new kind of medium
which is short and informal. While no
segmented corpus of micro-blogs is avail-
able to train Chinese word segmentation
model, existing Chinese word segmenta-
tion tools cannot perform equally well
as in ordinary news texts. In this pa-
per we present an effective yet simple ap-
proach to Chinese word segmentation of
micro-blog. In our approach, we incor-
porate punctuation information of unla-
beled micro-blog data by introducing char-
acters behind or ahead of punctuations,
for they indicate the beginning or end of
words. Meanwhile a self-training frame-
work to incorporate confident instances is
also used, which prove to be helpful. Ex-
periments on micro-blog data show that
our approach improves performance, espe-
cially in OOV-recall.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.98794080952381">
Micro-blog (also known as tweets in English) is
a new kind of broadcast medium in the form of
blogging. A micro-blog differs from a traditional
blog in that it is typically smaller in size. Further-
more, texts in micro-blogs tend to be informal and
new words occur more frequently. These new fea-
tures of micro-blogs make the Chinese Word Seg-
mentation (CWS) models trained on the source do-
main, such as news corpus, fail to perform equally
well when transferred to texts from micro-blogs.
For example, the most widely used Chinese seg-
menter ”ICTCLAS” yields 0.95 f-score in news
corpus, only gets 0.82 f-score on micro-blog data.
The poor segmentation results will hurt subse-
quent analysis on micro-blog text.
∗Corresponding author
Manually labeling the texts of micro-blog is
time consuming. Luckily, punctuations provide
useful information because they are used as indi-
cators of the end of previous sentence and the be-
ginning of the next one, which also indicate the
start and the end of a word. These ”natural bound-
aries” appear so frequently in micro-blog texts that
we can easily make good use of them. TABLE 1
shows some statistics of the news corpus vs. the
micro-blogs. Besides, English letters and digits
are also more than those in news corpus. They
all are natural delimiters of Chinese characters and
we treat them just the same as punctuations.
We propose a method to enlarge the training
corpus by using punctuation information. We
build a semi-supervised learning (SSL) framework
which can iteratively incorporate newly labeled in-
stances from unlabeled micro-blog data during the
training process. We test our method on micro-
blog texts and experiments show good results.
This paper is organized as follows. In section 1
we introduce the problem. Section 2 gives detailed
description of our approach. We show the experi-
ment and analyze the results in section 3. Section
4 gives the related works and in section 5 we con-
clude the whole work.
</bodyText>
<sectionHeader confidence="0.97559" genericHeader="method">
2 Our method
</sectionHeader>
<subsectionHeader confidence="0.974558">
2.1 Punctuations
</subsectionHeader>
<bodyText confidence="0.999891545454545">
Chinese word segmentation problem might be
treated as a character labeling problem which
gives each character a label indicating its position
in one word. To be simple, one can use label ’B’
to indicate a character is the beginning of a word,
and use ’N’ to indicate a character is not the be-
ginning of a word. We also use the 2-tag in our
work. Other tag sets like the ’BIES’ tag set are not
suiteable because the puctuation information can-
not decide whether a character after punctuation
should be labeled as ’B’ or ’S’(word with Single
</bodyText>
<page confidence="0.968432">
177
</page>
<note confidence="0.529991">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 177–182,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.998999">
Chinese English Number Punctuation
News 85.7% 0.6% 0.7% 13.0%
micro-blog 66.3% 11.8% 2.6% 19.3%
</table>
<tableCaption confidence="0.999927">
Table 1: Percentage of Chinese, English, number, punctuation in the news corpus vs. the micro-blogs.
</tableCaption>
<bodyText confidence="0.998005571428571">
character).
Punctuations can serve as implicit labels for the
characters before and after them. The character
right after punctuations must be the first character
of a word, meanwhile the character right before
punctuations must be the last character of a word.
An example is given in TABLE 2.
</bodyText>
<subsectionHeader confidence="0.995683">
2.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.999985555555555">
Our algorithm “ADD-N” is shown in TABLE 3.
The initially selected character instances are those
right after punctuations. By definition they are all
labeled with ’B’. In this case, the number of train-
ing instances with label ’B’ is increased while the
number with label ’N’ remains unchanged. Be-
cause of this, the model trained on this unbal-
anced corpus tends to be biased. This problem can
become even worse when there is inexhaustible
supply of texts from the target domain. We as-
sume that labeled corpus of the source domain can
be treated as a balanced reflection of different la-
bels. Therefore we choose to estimate the bal-
anced point by counting characters labeling ’B’
and ’N’ and calculate the ratio which we denote
as η. We assume the enlarged corpus is also bal-
anced if and only if the ratio of ’B’ to ’N’ is just
the same to η of the source domain.
Our algorithm uses data from source domain to
make the labels balanced. When enlarging corpus
using characters behind punctuations from texts
in target domain, only characters labeling ’B’ are
added. We randomly reuse some characters label-
ing ’N’ from labeled data until ratio η is reached.
We do not use characters ahead of punctuations,
because the single-character words ahead of punc-
tuations take the label of ’B’ instead of ’N’. In
summary our algorithm tackles the problem by du-
plicating labeled data in source domain. We de-
note our algorithm as ”ADD-N”.
We also use baseline feature templates include
the features described in previous works (Sun and
Xu, 2011; Sun et al., 2012). Our algorithm is not
necessarily limited to a specific tagger. For sim-
plicity and reliability, we use a simple Maximum-
Entropy tagger.
</bodyText>
<sectionHeader confidence="0.995384" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.99946">
3.1 Data set
</subsectionHeader>
<bodyText confidence="0.999927868421052">
We evaluate our method using the data from
weibo.com, which is the biggest micro-blog ser-
vice in China. We use the API provided by
weibo.com1 to crawl 500,000 micro-blog texts of
weibo.com, which contains 24,243,772 charac-
ters. To keep the experiment tractable, we first ran-
domly choose 50,000 of all the texts as unlabeled
data, which contain 2,420,037 characters. We
manually segment 2038 randomly selected micro-
blogs.We follow the segmentation standard as the
PKU corpus.
In micro-blog texts, the user names and URLs
have fixed format. User names start with ’@’, fol-
lowed by Chinese characters, English letters, num-
bers and ’ ’, and terminated when meeting punc-
tuations or blanks. URLs also match fixed pat-
terns, which are shortened using ”http://t.
cn/” plus six random English letters or numbers.
Thus user names and URLs can be pre-processed
separately. We follow this principle in following
experiments.
We use the benchmark datasets provided by the
second International Chinese Word Segmentation
Bakeoff2 as the labeled data. We choose the PKU
data in our experiment because our baseline meth-
ods use the same segmentation standard.
We compare our method with three baseline
methods. The first two are both famous Chinese
word segmentation tools: ICTCLAS3 and Stan-
ford Chinese word segmenter4, which are widely
used in NLP related to word segmentation. Stan-
ford Chinese word segmenter is a CRF-based seg-
mentation tool and its segmentation standard is
chosen as the PKU standard, which is the same
to ours. ICTCLAS, on the other hand, is a HMM-
based Chinese word segmenter. Another baseline
is Li and Sun (2009), which also uses punctua-
tion in their semi-supervised framework. F-score
</bodyText>
<footnote confidence="0.9662504">
1http://open.weibo.com/wiki
2http://www.sighan.org/bakeoff2005/
3http://ictclas.org/
4http://nlp.stanford.edu/projects/
chinese-nlp.shtml\#cws
</footnote>
<page confidence="0.933735">
178
</page>
<equation confidence="0.708300333333333">
W il � A FXL W il � A 4I )
B - - - - - B - - - - -
B N B B N B B N B B N B
</equation>
<tableCaption confidence="0.963468">
Table 2: The first line represents the original text. The second line indicates whether each character is
the Beginning of sentence. The third line is the tag sequence using ”BN” tag set.
</tableCaption>
<sectionHeader confidence="0.797353" genericHeader="method">
ADD-N algorithm
</sectionHeader>
<bodyText confidence="0.787333">
Input: labeled data {(xi, yi)li_1}, unlabeled data {xj}l+u
j=l+1.
</bodyText>
<listItem confidence="0.9540103">
1. Initially, let L = {(xi, yi)li_1} and U = {xj}l+u
j=l+1.
2. Label instances behind punctuations in U as ’B’ and add them into
L.
3. Calculate ’B’, ’N’ ratio η in labeled data.
4. Randomly duplicate characters whose labels are ’N’ in L to make
’B’/’N’= η
5. Repeat:
5.1 Train a classifier f from L using supervised learning.
5.2 Apply f to tag the unlabeled instances in U.
</listItem>
<subsectionHeader confidence="0.36179">
5.3 Add confident instances from U to L.
</subsectionHeader>
<tableCaption confidence="0.996711">
Table 3: ADD-N algorithm.
</tableCaption>
<bodyText confidence="0.9927465">
is used as the accuracy measure. The recall of
out-of-vocabulary is also taken into consideration,
which measures the ability of the model to cor-
rectly segment out of vocabulary words.
</bodyText>
<subsectionHeader confidence="0.994714">
3.2 Main results
</subsectionHeader>
<table confidence="0.9999055">
Method P R F OOV-R
Stanford 0.861 0.853 0.857 0.639
ICTCLAS 0.812 0.861 0.836 0.602
Li-Sun 0.707 0.820 0.760 0.734
Maxent 0.868 0.844 0.856 0.760
No-punc 0.865 0.829 0.846 0.760
No-balance 0.869 0.877 0.873 0.757
Our method 0.875 0.875 0.875 0.773
</table>
<tableCaption confidence="0.965786">
Table 4: Segmentation performance with different
methods on the development data.
</tableCaption>
<bodyText confidence="0.999344023809524">
TABLE 4 summarizes the segmentation results.
In TABLE 4, Li-Sun is the method in Li and
Sun (2009). Maxent only uses the PKU data for
training, with neither punctuation information nor
self-training framework incorporated. The next 4
methods all require a 100 iteration of self-training.
No-punc is the method that only uses self-training
while no punctuation information is added. No-
balance is similar to ADD N. The only difference
between No-balance and ADD-N is that the for-
mer does not balance label ’B’ and label ’N’.
The comparison of Maxent and No-punctuation
shows that naively adding confident unlabeled in-
stances does not guarantee to improve perfor-
mance. The writing style and word formation of
the source domain is different from target domain.
When segmenting texts of the target domain using
models trained on source domain, the performance
will be hurt with more false segmented instances
added into the training set.
The comparison of Maxent, No-balance and
ADD-N shows that considering punctuation as
well as self-training does improve performance.
Both the f-score and OOV-recall increase. By
comparing No-balance and ADD-N alone we can
find that we achieve relatively high f-score if we
ignore tag balance issue, while slightly hurt the
OOV-Recall. However, considering it will im-
prove OOV-Recall by about +1.6% and the f-
score +0.2%.
We also experimented on different size of un-
labeled data to evaluate the performance when
adding unlabeled target domain data. TABLE 5
shows different f-scores and OOV-Recalls on dif-
ferent unlabeled data set.
We note that when the number of texts changes
from 0 to 50,000, the f-score and OOV both are
improved. However, when unlabeled data changes
to 200,000, the performance is a bit decreased,
while still better than not using unlabeled data.
This result comes from the fact that the method
’ADD-N’ only uses characters behind punctua-
</bodyText>
<page confidence="0.998014">
179
</page>
<table confidence="0.9997685">
Size P R F OOV-R
0 0.864 0.846 0.855 0.754
10000 0.872 0.869 0.871 0.765
50000 0.875 0.875 0.875 0.773
100000 0.874 0.879 0.876 0.772
200000 0.865 0.865 0.865 0.759
</table>
<tableCaption confidence="0.9706245">
Table 5: Segmentation performance with different
size of unlabeled data
</tableCaption>
<bodyText confidence="0.858694">
tions from target domain. Taking more texts into
consideration means selecting more characters la-
beling ’N’ from source domain to simulate those
in target domain. If too many ’N’s are introduced,
the training data will be biased against the true dis-
tribution of target domain.
</bodyText>
<subsectionHeader confidence="0.999218">
3.3 Characters ahead of punctuations
</subsectionHeader>
<bodyText confidence="0.9995869">
In the ”BN” tagging method mentioned above,
we incorporate characters after punctuations from
texts in micro-blog to enlarge training set.We also
try an opposite approach, ”EN” tag, which uses
’E’ to represent ”End of word”, and ’N’ to rep-
resent ”Not the end of word”. In this contrasting
method, we only use characters just ahead of punc-
tuations. We find that the two methods show sim-
ilar results. Experiment results with ADD-N are
shown in TABLE 6 .
</bodyText>
<table confidence="0.996952">
Unlabeled ”BN” tag ”EN” tag
Data size
F OOV-R F OOV-R
50000 0.875 0.773 0.870 0.763
</table>
<tableCaption confidence="0.999708">
Table 6: Comparison of BN and EN.
</tableCaption>
<sectionHeader confidence="0.999547" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999974444444444">
Recent studies show that character sequence la-
beling is an effective formulation of Chinese
word segmentation (Low et al., 2005; Zhao et al.,
2006a,b; Chen et al., 2006; Xue, 2003). These
supervised methods show good results, however,
are unable to incorporate information from new
domain, where OOV problem is a big challenge
for the research community. On the other hand
unsupervised word segmentation Peng and Schu-
urmans (2001); Goldwater et al. (2006); Jin and
Tanaka-Ishii (2006); Feng et al. (2004); Maosong
et al. (1998) takes advantage of the huge amount
of raw text to solve Chinese word segmentation
problems. However, they usually are less accurate
and more complicated than supervised ones.
Meanwhile semi-supervised methods have been
applied into NLP applications. Bickel et al. (2007)
learns a scaling factor from data of source domain
and use the distribution to resemble target do-
main distribution. Wu et al. (2009) uses a Domain
adaptive bootstrapping (DAB) framework, which
shows good results on Named Entity Recognition.
Similar semi-supervised applications include Shen
et al. (2004); Daum´e III and Marcu (2006); Jiang
and Zhai (2007); Weinberger et al. (2006). Be-
sides, Sun and Xu (2011) uses a sequence labeling
framework, while unsupervised statistics are used
as discrete features in their model, which prove to
be effective in Chinese word segmentation.
There are previous works using punctuations as
implicit annotations. Riley (1989) uses it in sen-
tence boundary detection. Li and Sun (2009) pro-
posed a compromising solution to by using a clas-
sifier to select the most confident characters. We
do not follow this approach because the initial er-
rors will dramatically harm the performance. In-
stead, we only add the characters after punctua-
tions which are sure to be the beginning of words
(which means labeling ’B’) into our training set.
Sun and Xu (2011) uses punctuation information
as discrete feature in a sequence labeling frame-
work, which shows improvement compared to the
pure sequence labeling approach. Our method
is different from theirs. We use characters after
punctuations directly.
</bodyText>
<sectionHeader confidence="0.994581" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9987325">
In this paper we have presented an effective yet
simple approach to Chinese word segmentation on
micro-blog texts. In our approach, punctuation in-
formation of unlabeled micro-blog data is used,
as well as a self-training framework to incorpo-
rate confident instances. Experiments show that
our approach improves performance, especially in
OOV-recall. Both the punctuation information and
the self-training phase contribute to this improve-
ment.
</bodyText>
<sectionHeader confidence="0.989819" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990079571428572">
This research was partly supported by Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101), National Natural Science Foun-
dation of China (No.91024009) and Major
National Social Science Fund of China(No.
12&amp;ZD227).
</bodyText>
<page confidence="0.996174">
180
</page>
<sectionHeader confidence="0.990166" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.975730105263158">
Bickel, S., Br¨uckner, M., and Scheffer, T. (2007).
Discriminative learning for differing training
and test distributions. In Proceedings of the 24th
international conference on Machine learning,
pages 81–88. ACM.
Chen, W., Zhang, Y., and Isahara, H. (2006). Chi-
nese named entity recognition with conditional
random fields. In 5th SIGHAN Workshop on
Chinese Language Processing, Australia.
Daum´e III, H. and Marcu, D. (2006). Domain
adaptation for statistical classifiers. Journal of
Artificial Intelligence Research, 26(1):101–126.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75–93.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
</reference>
<bodyText confidence="0.945818809523809">
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, pages 673–680. Association for Computa-
tional Linguistics.
Jiang, J. and Zhai, C. (2007). Instance weight-
ing for domain adaptation in nlp. In Annual
Meeting-Association For Computational Lin-
guistics, volume 45, page 264.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsuper-
vised segmentation of chinese text by use of
branching entropy. In Proceedings of the COL-
ING/ACL on Main conference poster sessions,
pages 428–435. Association for Computational
Linguistics.
Li, Z. and Sun, M. (2009). Punctuation as im-
plicit annotations for chinese word segmenta-
tion. Computational Linguistics, 35(4):505–
512.
Low, J., Ng, H., and Guo, W. (2005). A maximum
entropy approach to chinese word segmenta-
</bodyText>
<reference confidence="0.954261473684211">
tion. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing,
volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265–1271. Association for Computa-
tional Linguistics.
Pan, S. and Yang, Q. (2010). A survey on trans-
fer learning. Knowledge and Data Engineering,
IEEE Transactions on, 22(10):1345–1359.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
vances in Intelligent Data Analysis, pages 238–
247.
Riley, M. (1989). Some applications of tree-based
modelling to speech and language. In Pro-
ceedings of the workshop on Speech and Nat-
ural Language, pages 339–352. Association for
Computational Linguistics.
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan,
C. (2004). Multi-criteria-based active learning
for named entity recognition. In Proceedings
of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589. Associa-
tion for Computational Linguistics.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970–979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 253–262, Jeju Island, Korea. Association
for Computational Linguistics.
Weinberger, K., Blitzer, J., and Saul, L. (2006).
Distance metric learning for large margin near-
est neighbor classification. In In NIPS. Citeseer.
Wu, D., Lee, W., Ye, N., and Chieu, H. (2009).
Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009
Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume
3, pages 1523–1532. Association for Computa-
tional Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29–48.
Zhao, H., Huang, C., and Li, M. (2006a). An im-
proved chinese word segmentation system with
</reference>
<page confidence="0.979924">
181
</page>
<reference confidence="0.998867125">
conditional random field. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language
Processing, volume 117. Sydney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field model-
ing. In Proceedings of PACLIC, volume 20,
pages 87–94.
</reference>
<page confidence="0.99801">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.708428">
<title confidence="0.996252">Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations</title>
<author confidence="0.99922">Zhang Li Li Zhengyan He Houfeng Ni</author>
<affiliation confidence="0.997058">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,</affiliation>
<email confidence="0.875202">zhlongk@qq.com,li.l@pku.edu.cn,wanghf@pku.edu.cn,sunny.forwork@gmail.com</email>
<abstract confidence="0.997787">Micro-blog is a new kind of medium which is short and informal. While no segmented corpus of micro-blogs is available to train Chinese word segmentation model, existing Chinese word segmentation tools cannot perform equally well as in ordinary news texts. In this paper we present an effective yet simple approach to Chinese word segmentation of micro-blog. In our approach, we incorporate punctuation information of unlabeled micro-blog data by introducing characters behind or ahead of punctuations, for they indicate the beginning or end of words. Meanwhile a self-training framework to incorporate confident instances is also used, which prove to be helpful. Experiments on micro-blog data show that our approach improves performance, especially in OOV-recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bickel</author>
<author>M Br¨uckner</author>
<author>T Scheffer</author>
</authors>
<title>Discriminative learning for differing training and test distributions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>81--88</pages>
<publisher>ACM.</publisher>
<marker>Bickel, Br¨uckner, Scheffer, 2007</marker>
<rawString>Bickel, S., Br¨uckner, M., and Scheffer, T. (2007). Discriminative learning for differing training and test distributions. In Proceedings of the 24th international conference on Machine learning, pages 81–88. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chen</author>
<author>Y Zhang</author>
<author>H Isahara</author>
</authors>
<title>Chinese named entity recognition with conditional random fields.</title>
<date>2006</date>
<booktitle>In 5th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<publisher>Australia.</publisher>
<contexts>
<context position="12391" citStr="Chen et al., 2006" startWordPosition="2036" endWordPosition="2039">t.We also try an opposite approach, ”EN” tag, which uses ’E’ to represent ”End of word”, and ’N’ to represent ”Not the end of word”. In this contrasting method, we only use characters just ahead of punctuations. We find that the two methods show similar results. Experiment results with ADD-N are shown in TABLE 6 . Unlabeled ”BN” tag ”EN” tag Data size F OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP appl</context>
</contexts>
<marker>Chen, Zhang, Isahara, 2006</marker>
<rawString>Chen, W., Zhang, Y., and Isahara, H. (2006). Chinese named entity recognition with conditional random fields. In 5th SIGHAN Workshop on Chinese Language Processing, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>26</volume>
<issue>1</issue>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Daum´e III, H. and Marcu, D. (2006). Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26(1):101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Feng</author>
<author>K Chen</author>
<author>X Deng</author>
<author>W Zheng</author>
</authors>
<title>Accessor variety criteria for chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="12727" citStr="Feng et al. (2004)" startWordPosition="2088" endWordPosition="2091">”EN” tag Data size F OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004);</context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Feng, H., Chen, K., Deng, X., and Zheng, W. (2004). Accessor variety criteria for chinese word extraction. Computational Linguistics, 30(1):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of tion. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>164</volume>
<location>Island,</location>
<contexts>
<context position="12678" citStr="Goldwater et al. (2006)" startWordPosition="2080" endWordPosition="2083"> with ADD-N are shown in TABLE 6 . Unlabeled ”BN” tag ”EN” tag Data size F OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-su</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Goldwater, S., Griffiths, T., and Johnson, M. (2006). Contextual dependencies in unsupervised word segmentation. In Proceedings of tion. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 164. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maosong</author>
<author>S Dayang</author>
<author>B Tsou</author>
</authors>
<title>Chinese word segmentation without using lexicon and hand-crafted training data.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 2,</booktitle>
<pages>1265--1271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12750" citStr="Maosong et al. (1998)" startWordPosition="2092" endWordPosition="2095"> OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (</context>
</contexts>
<marker>Maosong, Dayang, Tsou, 1998</marker>
<rawString>Maosong, S., Dayang, S., and Tsou, B. (1998). Chinese word segmentation without using lexicon and hand-crafted training data. In Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 1265–1271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>Q Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>22</volume>
<issue>10</issue>
<marker>Pan, Yang, 2010</marker>
<rawString>Pan, S. and Yang, Q. (2010). A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Schuurmans</author>
</authors>
<title>Selfsupervised chinese word segmentation.</title>
<date>2001</date>
<booktitle>Advances in Intelligent Data Analysis,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="12653" citStr="Peng and Schuurmans (2001)" startWordPosition="2075" endWordPosition="2079"> results. Experiment results with ADD-N are shown in TABLE 6 . Unlabeled ”BN” tag ”EN” tag Data size F OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Rec</context>
</contexts>
<marker>Peng, Schuurmans, 2001</marker>
<rawString>Peng, F. and Schuurmans, D. (2001). Selfsupervised chinese word segmentation. Advances in Intelligent Data Analysis, pages 238– 247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Riley</author>
</authors>
<title>Some applications of tree-based modelling to speech and language.</title>
<date>1989</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>339--352</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13684" citStr="Riley (1989)" startWordPosition="2235" endWordPosition="2236"> and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a compromising solution to by using a classifier to select the most confident characters. We do not follow this approach because the initial errors will dramatically harm the performance. Instead, we only add the characters after punctuations which are sure to be the beginning of words (which means labeling ’B’) into our training set. Sun and Xu (2011) uses punctuation information as discrete feature in a sequence labeling framework, which shows improvement compared to the pure sequence labeling approach. Our method is differe</context>
</contexts>
<marker>Riley, 1989</marker>
<rawString>Riley, M. (1989). Some applications of tree-based modelling to speech and language. In Proceedings of the workshop on Speech and Natural Language, pages 339–352. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J Zhang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>589</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13326" citStr="Shen et al. (2004)" startWordPosition="2178" endWordPosition="2181"> Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a compromising solution to by using a classifier to select the most confident characters. We do not follow this approach because the initial errors will dramatically harm the</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Shen, D., Zhang, J., Su, J., Zhou, G., and Tan, C. (2004). Multi-criteria-based active learning for named entity recognition. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 589. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sun</author>
<author>J Xu</author>
</authors>
<title>Enhancing chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5854" citStr="Sun and Xu, 2011" startWordPosition="953" endWordPosition="956"> the labels balanced. When enlarging corpus using characters behind punctuations from texts in target domain, only characters labeling ’B’ are added. We randomly reuse some characters labeling ’N’ from labeled data until ratio η is reached. We do not use characters ahead of punctuations, because the single-character words ahead of punctuations take the label of ’B’ instead of ’N’. In summary our algorithm tackles the problem by duplicating labeled data in source domain. We denote our algorithm as ”ADD-N”. We also use baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). Our algorithm is not necessarily limited to a specific tagger. For simplicity and reliability, we use a simple MaximumEntropy tagger. 3 Experiment 3.1 Data set We evaluate our method using the data from weibo.com, which is the biggest micro-blog service in China. We use the API provided by weibo.com1 to crawl 500,000 micro-blog texts of weibo.com, which contains 24,243,772 characters. To keep the experiment tractable, we first randomly choose 50,000 of all the texts as unlabeled data, which contain 2,420,037 characters. We manually segment 2038 randomly selected microblogs</context>
<context position="13432" citStr="Sun and Xu (2011)" startWordPosition="2197" endWordPosition="2200">word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a compromising solution to by using a classifier to select the most confident characters. We do not follow this approach because the initial errors will dramatically harm the performance. Instead, we only add the characters after punctuations which are sure to be the beginning of</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Sun, W. and Xu, J. (2011). Enhancing chinese word segmentation using unlabeled data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>H Wang</author>
<author>W Li</author>
</authors>
<title>Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>253--262</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="5873" citStr="Sun et al., 2012" startWordPosition="957" endWordPosition="960">ed. When enlarging corpus using characters behind punctuations from texts in target domain, only characters labeling ’B’ are added. We randomly reuse some characters labeling ’N’ from labeled data until ratio η is reached. We do not use characters ahead of punctuations, because the single-character words ahead of punctuations take the label of ’B’ instead of ’N’. In summary our algorithm tackles the problem by duplicating labeled data in source domain. We denote our algorithm as ”ADD-N”. We also use baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). Our algorithm is not necessarily limited to a specific tagger. For simplicity and reliability, we use a simple MaximumEntropy tagger. 3 Experiment 3.1 Data set We evaluate our method using the data from weibo.com, which is the biggest micro-blog service in China. We use the API provided by weibo.com1 to crawl 500,000 micro-blog texts of weibo.com, which contains 24,243,772 characters. To keep the experiment tractable, we first randomly choose 50,000 of all the texts as unlabeled data, which contain 2,420,037 characters. We manually segment 2038 randomly selected microblogs.We follow the segm</context>
</contexts>
<marker>Sun, Wang, Li, 2012</marker>
<rawString>Sun, X., Wang, H., and Li, W. (2012). Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 253–262, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Weinberger</author>
<author>J Blitzer</author>
<author>L Saul</author>
</authors>
<title>Distance metric learning for large margin nearest neighbor classification.</title>
<date>2006</date>
<booktitle>In In NIPS. Citeseer.</booktitle>
<contexts>
<context position="13404" citStr="Weinberger et al. (2006)" startWordPosition="2191" endWordPosition="2194">mount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a compromising solution to by using a classifier to select the most confident characters. We do not follow this approach because the initial errors will dramatically harm the performance. Instead, we only add the characters after punctuations which are</context>
</contexts>
<marker>Weinberger, Blitzer, Saul, 2006</marker>
<rawString>Weinberger, K., Blitzer, J., and Saul, L. (2006). Distance metric learning for large margin nearest neighbor classification. In In NIPS. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>W Lee</author>
<author>N Ye</author>
<author>H Chieu</author>
</authors>
<title>Domain adaptive bootstrapping for named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1523--1532</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13154" citStr="Wu et al. (2009)" startWordPosition="2155" endWordPosition="2158">g challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a </context>
</contexts>
<marker>Wu, Lee, Ye, Chieu, 2009</marker>
<rawString>Wu, D., Lee, W., Ye, N., and Chieu, H. (2009). Domain adaptive bootstrapping for named entity recognition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1523–1532. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="12403" citStr="Xue, 2003" startWordPosition="2040" endWordPosition="2041">posite approach, ”EN” tag, which uses ’E’ to represent ”End of word”, and ’N’ to represent ”Not the end of word”. In this contrasting method, we only use characters just ahead of punctuations. We find that the two methods show similar results. Experiment results with ADD-N are shown in TABLE 6 . Unlabeled ”BN” tag ”EN” tag Data size F OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been applied into NLP applications. Bi</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Xue, N. (2003). Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
<author>C Huang</author>
<author>M Li</author>
</authors>
<title>An improved chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>117</volume>
<location>Sydney:</location>
<contexts>
<context position="12369" citStr="Zhao et al., 2006" startWordPosition="2032" endWordPosition="2035">to enlarge training set.We also try an opposite approach, ”EN” tag, which uses ’E’ to represent ”End of word”, and ’N’ to represent ”Not the end of word”. In this contrasting method, we only use characters just ahead of punctuations. We find that the two methods show similar results. Experiment results with ADD-N are shown in TABLE 6 . Unlabeled ”BN” tag ”EN” tag Data size F OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Zhao, H., Huang, C., and Li, M. (2006a). An improved chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, volume 117. Sydney: July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
<author>C Huang</author>
<author>M Li</author>
<author>B Lu</author>
</authors>
<title>Effective tag set selection in chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC,</booktitle>
<volume>20</volume>
<pages>87--94</pages>
<contexts>
<context position="12369" citStr="Zhao et al., 2006" startWordPosition="2032" endWordPosition="2035">to enlarge training set.We also try an opposite approach, ”EN” tag, which uses ’E’ to represent ”End of word”, and ’N’ to represent ”Not the end of word”. In this contrasting method, we only use characters just ahead of punctuations. We find that the two methods show similar results. Experiment results with ADD-N are shown in TABLE 6 . Unlabeled ”BN” tag ”EN” tag Data size F OOV-R F OOV-R 50000 0.875 0.773 0.870 0.763 Table 6: Comparison of BN and EN. 4 Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Meanwhile semi-supervised methods have been</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Zhao, H., Huang, C., Li, M., and Lu, B. (2006b). Effective tag set selection in chinese word segmentation via conditional random field modeling. In Proceedings of PACLIC, volume 20, pages 87–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>