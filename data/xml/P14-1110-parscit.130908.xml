<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000214">
<title confidence="0.996704">
Polylingual Tree-Based Topic Models for Translation Domain Adaptation
</title>
<author confidence="0.999588">
Yuening Hu† Ke Zhai† Vladimir Eidelman Jordan Boyd-Graber
</author>
<affiliation confidence="0.9997975">
Computer Science Computer Science FiscalNote Inc. iSchool and UMIACS
University of Maryland University of Maryland Washington DC University of Maryland
</affiliation>
<email confidence="0.994561">
ynhu@cs.umd.edu zhaike@cs.umd.edu vlad@fiscalnote.com jbg@umiacs.umd.edu
</email>
<sectionHeader confidence="0.993782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999622">
Topic models, an unsupervised technique
for inferring translation domains improve
machine translation quality. However, pre-
vious work uses only the source language
and completely ignores the target language,
which can disambiguate domains. We pro-
pose new polylingual tree-based topic mod-
els to extract domain knowledge that con-
siders both source and target languages and
derive three different inference schemes.
We evaluate our model on a Chinese to En-
glish translation task and obtain up to 1.2
BLEU improvement over strong baselines.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999465">
Probabilistic topic models (Blei and Lafferty,
2009), exemplified by latent Dirichlet alloca-
tion (Blei et al., 2003, LDA), are one of the most
popular statistical frameworks for navigating large
unannotated document collections. Topic models
discover—without any supervision—the primary
themes presented in a dataset: the namesake topics.
Topic models have two primary applications: to
aid human exploration of corpora (Chang et al.,
2009) or serve as a low-dimensional representa-
tion for downstream applications. We focus on
the second application, which has been fruitful for
computer vision (Li Fei-Fei and Perona, 2005),
computational biology (Perina et al., 2010), and
information retrieval (Kataria et al., 2011).
In particular, we use topic models to aid statisti-
cal machine translation (Koehn, 2009, SMT). Mod-
ern machine translation systems use millions of
examples of translations to learn translation rules.
These systems work best when the training corpus
has consistent genre, register, and topic. Systems
that are robust to systematic variation in the train-
ing set are said to exhibit domain adaptation.
† indicates equal contributions.
As we review in Section 2, topic models are
a promising solution for automatically discover-
ing domains in machine translation corpora. How-
ever, past work either relies solely on monolingual
source-side models (Eidelman et al., 2012; Hasler
et al., 2012; Su et al., 2012), or limited modeling
of the target side (Xiao et al., 2012). In contrast,
machine translation uses inherently multilingual
data: an SMT system must translate a phrase or sen-
tence from a source language to a different target
language, so existing applications of topic mod-
els (Eidelman et al., 2012) are wilfully ignoring
available information on the target side that could
aid domain discovery.
This is not for a lack of multilingual topic mod-
els. Topic models bridge the chasm between lan-
guages using document connections (Mimno et
al., 2009), dictionaries (Boyd-Graber and Resnik,
2010), and word alignments (Zhao and Xing, 2006).
In Section 2, we review these models for discover-
ing topics in multilingual datasets and discuss how
they can improve SMT.
However, no models combine multiple bridges
between languages. In Section 3, we create a
model—the polylingual tree-based topic models
(ptLDA)—that uses information from both external
dictionaries and document alignments simultane-
ously. In Section 4, we derive both MCMC and
variational inference for this new topic model.
In Section 5, we evaluate our model on the task
of SMT using aligned datasets. We show that ptLDA
offers better domain adaptation than other topic
models for machine translation. Finally, in Sec-
tion 6, we show how these topic models improve
SMT with detailed examples.
</bodyText>
<sectionHeader confidence="0.953845" genericHeader="method">
2 Topic Models for Machine Translation
</sectionHeader>
<bodyText confidence="0.998559666666667">
Before considering past approaches using topic
models to improve SMT, we briefly review lexical
weighting and domain adaptation for SMT.
</bodyText>
<page confidence="0.942114">
1166
</page>
<note confidence="0.8646205">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166–1176,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.933976">
2.1 Statistical Machine Translation
</subsectionHeader>
<bodyText confidence="0.999733347826087">
Statistical machine translation casts machine trans-
lation as a probabilistic process (Koehn, 2009). For
a parallel corpus of aligned source and target sen-
tences (F, £), a phrase f E F is translated to a
phrase e¯ E £ according to a distribution pw(¯e |f).
One popular method to estimate the probability
pw(¯e|¯f) is via lexical weighting features.
Lexical Weighting In phrase-based SMT, lexi-
cal weighting features estimate the phrase pair
quality by combining lexical translation probabil-
ities of words in a phrase (Koehn et al., 2003).
Lexical conditional probabilities p(e|f) are maxi-
mum likelihood estimates from relative lexical fre-
quencies c(f, e)/Ee c(f, e), where c(f, e) is the
count of observing lexical pair (f, e) in the train-
ing dataset. The phrase pair probabilities pw(¯e |f)
are the normalized product of lexical probabili-
ties of the aligned word pairs within that phrase
pair (Koehn et al., 2003). In Section 2.2, we create
topic-specific lexical weighting features.
Cross-Domain SMT A SMT system is usu-
ally trained on documents with the same genre
(e.g., sports, business) from a similar style (e.g.,
newswire, blog-posts). These are called domains.
Translations within one domain are better than
translations across domains since they vary dra-
matically in their word choices and style. A correct
translation in one domain may be inappropriate in
another domain. For example, “�7K” in a newspa-
per usually means “underwater diving”. On social
media, it means a non-contributing “lurker”.
Domain Adaptation for SMT Training a SMT
system using diverse data requires domain adap-
tation. Early efforts focus on building separate
models (Foster and Kuhn, 2007) and adding fea-
tures (Matsoukas et al., 2009) to model domain
information. Chiang et al. (2011) combine these
approaches by directly optimizing genre and col-
lection features by computing separate translation
tables for each domain.
However, these approaches treat domains as
hand-labeled, constant, and known a priori. This
setup is at best expensive and at worst infeasible for
large data. Topic models provide a solution where
domains can be automatically induced from raw
data: treat each topic as a domain.1
</bodyText>
<footnote confidence="0.765388333333333">
1Henceforth we will use the term “topic” and “domain”
interchangeably: “topic” to refer to the concept in topic models
and “domain” to refer to SMT corpora.
</footnote>
<subsectionHeader confidence="0.993714">
2.2 Inducing Domains with Topic Models
</subsectionHeader>
<bodyText confidence="0.999918277777778">
Topic models take the number of topics K and a
collection of documents as input, where each docu-
ment is a bag of words. They output two distribu-
tions: a distribution over topics for each document
d; and a distribution over words for each topic. If
each topic defines a SMT domain, the document’s
topic distribution is a soft domain assignment for
that document.
Given the soft domain assignments, Eidelman et
al. (2012) extract lexical weighting features condi-
tioned on the topics, optimizing feature weights us-
ing the Margin Infused Relaxed Algorithm (Cram-
mer et al., 2006, MIRA). The topics come from
source documents only and create topic-specific
lexical weights from the per-document topic distri-
bution p(k  |d). The lexical probability conditioned
on the topic is expected count ek(e, f) of a word
translation pair under topic k,
</bodyText>
<equation confidence="0.991949">
ˆck(e,f) _ Ed p(k|d)cd(e,f), (1)
</equation>
<bodyText confidence="0.9990095">
where cd(0) is the number of occurrences of the
word pair in document d. The lexical probability
conditioned on topic k is the unsmoothed probabil-
ity estimate of those expected counts
</bodyText>
<equation confidence="0.999741">
pw(e|f; k) _ &amp; ck(e,f)
ˆck (e,f) (2)
</equation>
<bodyText confidence="0.998422285714286">
from which we can compute the phrase pair proba-
bilities pw(¯e |¯f; k) by multiplying the lexical prob-
abilities and normalizing as in Koehn et al. (2003).
For a test document d, the document topic dis-
tribution p(k  |d) is inferred based on the topics
learned from training data. The feature value of a
phrase pair (¯e, ¯f) is
</bodyText>
<equation confidence="0.936795">
fk(¯e |¯f) _ −log {pw(¯e|¯f;k) - p(k|d)I, (3)
</equation>
<bodyText confidence="0.999965923076923">
a combination of the topic dependent lexical weight
and the topic distribution of the document, from
which we extract the phrase. Eidelman et al. (2012)
compute the resulting model score by combining
these features in a linear model with other standard
SMT features and optimizing the weights.
Conceptually, this approach is just reweighting
examples. The probability of a topic given a docu-
ment is never zero. Every translation observed in
the training set will contribute to pk(e|f); many of
the expected counts, however, will be less than one.
This obviates the explicit smoothing used in other
domain adaptation systems (Chiang et al., 2011).
</bodyText>
<page confidence="0.991075">
1167
</page>
<bodyText confidence="0.999884">
We adopt this framework in its entirety. Our
contribution are topics that capture multilingual
information and thus better capture the domains in
the parallel corpus.
</bodyText>
<subsectionHeader confidence="0.998755">
2.3 Beyond Vanilla Topic Models
</subsectionHeader>
<bodyText confidence="0.999940882352941">
Eidelman et al. (2012) ignore a wealth of infor-
mation that could improve topic models and help
machine translation. Namely, they only use mono-
lingual data from the source language, ignoring all
target-language data and available lexical semantic
resources between source and target languages.
Different complement each other to reduce ambi-
guity. For example, “��” in a Chinese document
can be either “hobbyhorse” in a children’s topic,
or “Trojan virus” in a technology topic. A short
Chinese context obscures the true topic. However,
these terms are unambiguous in English, revealing
the true topic.
While vanilla topic models (LDA) can only be
applied to monolingual data, there are a number
of topic models for parallel corpora: Zhao and
Xing (2006) assume aligned word pairs share same
topics; Mimno et al. (2009) connect different lan-
guages through comparable documents. These
models take advantage of word or document align-
ment information and infer more robust topics from
the aligned dataset.
On the other hand, lexical information can in-
duce topics from multilingual corpora. For in-
stance, orthographic similarity connects words with
the same meaning in related languages (Boyd-
Graber and Blei, 2009), and dictionaries are a
more general source of information on which words
share meaning (Boyd-Graber and Resnik, 2010).
These two approaches are not mutually exclu-
sive, however; they reveal different connections
across languages. In the next section, we combine
these two approaches into a polylingual tree-based
topic model.
</bodyText>
<sectionHeader confidence="0.97874" genericHeader="method">
3 Polylingual Tree-based Topic Models
</sectionHeader>
<bodyText confidence="0.999894440677966">
In this section, we bring existing tree-based topic
models (Boyd-Graber et al., 2007, tLDA) and
polylingual topic models (Mimno et al., 2009,
pLDA) together and create the polylingual tree-
based topic model (ptLDA) that incorporates both
word-level correlations and document-level align-
ment information.
Word-level Correlations Tree-based topic mod-
els incorporate the correlations between words by
encouraging words that appear together in a con-
cept to have similar probabilities given a topic.
These concepts can come from WordNet (Boyd-
Graber and Resnik, 2010), domain experts (An-
drzejewski et al., 2009), or user constrains (Hu et
al., 2013). When we gather concepts from bilin-
gual resources, these concepts can connect different
languages. For example, if a bilingual dictionary
defines “U#p–q” as “computer”, we combine these
words in a concept.
We organize the vocabulary in a tree structure
based on these concepts (Figure 1): words in the
same concept share a common parent node, and
then that concept becomes one of many children of
the root node. Words that are not in any concept—
uncorrelated words—are directly connected to
the root node. We call this structure the tree prior.
When this tree serves as a prior for topic models,
words in the same concept are correlated in topics.
For example, if “q#p–q” has high probability in a
topic, so will “computer”, since they share the same
parent node. With the tree priors, each topic is no
longer a distribution over word types, instead, it is a
distribution over paths, and each path is associated
with a word type. The same word could appear in
multiple paths, and each path represents a unique
sense of this word.
Document-level Alignments Lexical resources
connect languages and help guide the topics. How-
ever, these resources are sometimes brittle and may
not cover the whole vocabulary. Aligned document
pairs provide a more corpus-specific, flexible asso-
ciation across languages.
Polylingual topic models (Mimno et al., 2009)
assume that the aligned documents in different lan-
guages share the same topic distribution and each
language has a unique topic distribution over its
word types. This level of connection between lan-
guages is flexible: instead of requiring the exact
matching on words and sentences, only a coarse
document alignment is necessary, as long as the
documents discuss the same topics.
Combine Words and Documents We propose
polylingual tree-based topic models (ptLDA),
which connect information across different lan-
guages by incorporating both word correlation (as
in tLDA) and document alignment information (as
in pLDA). We initially assume a given tree struc-
ture, deferring the tree’s provenance to the end of
this section.
</bodyText>
<page confidence="0.96402">
1168
</page>
<bodyText confidence="0.999825766666667">
Generative Process As in LDA, each word to-
ken is associated with a topic. However, tree-based
topic models introduce an additional step of select-
ing a concept in a topic responsible for generating
each word token. This is represented by a path yd,n
through the topic’s tree.
The probability of a path in a topic depends on
the transition probabilities in a topic. Each concept
i in topic k has a distribution over its children nodes
is governed by a Dirichlet prior: πk,i ∼ Dir(βi).
Each path ends in a word (i.e., a leaf node) and
the probability of a path is the product of all of
the transitions between topics it traverses. Topics
have correlations over words because the Dirichlet
parameters can encode positive or negative correla-
tions (Andrzejewski et al., 2009).
With these correlated in topics in hand, the gen-
eration of documents are very similar to LDA. For
every document d, we first sample a distribution
over topics θd from a Dirichlet prior Dir(α). For
every token in the documents, we first sample a
topic zdn from the multinomial distribution θd, and
then sample a path ydn along the tree according to
the transition distributions specified by topic zdn.
Because every path ydn leads to a word wdn in lan-
guage ldn, we append the sampled word wdn to
document dldn. Aligned documents have words in
both languages; monolingual documents only have
words in a single language.
The full generative process is:
</bodyText>
<listItem confidence="0.941677833333333">
1: for topic k ∈ 1, · · · , K do
2: for each internal node ni do
3: draw a distribution πki ∼ Dir(βi)
4: for document set d ∈ 1, · · · , D do
5: draw a distribution θd ∼ Dir(α)
6: for each word in documents d do
7: choose a topic zdn ∼ Mult(θd)
8: sample a path ydn with probability
H
(i,j)∈ydn πzdn,i,j
9: ydn leads to word wdn in language ldn
10: append token wdn to document dldn
</listItem>
<bodyText confidence="0.981436111111111">
If we use a flat symmetric Dirichlet prior instead
of the tree prior, we recover pLDA; and if all docu-
ments are monolingual (i.e., with distinct distribu-
tions over topics θ), we recover tLDA. ptLDA con-
nects different languages on both the word level (us-
ing the word correlations) and the document level
(using the document alignments). We compare
these models’ machine translation performance in
Section 5.
</bodyText>
<figure confidence="0.9525542">
Dictionary: Vocabulary: English (0), Chinese (1)
0 computer 0 scientific 1 政府
0 market 0 policy 1 科学
0 government 1 *10
0 science 1 市JJjj 1 天气
</figure>
<figureCaption confidence="0.999472">
Figure 1: An example of constructing a prior tree
</figureCaption>
<bodyText confidence="0.9790165">
from a bilingual dictionary: word pairs with the
same meaning but in different languages are con-
cepts; we create a common parent node to group
words in a concept, and then connect to the root; un-
correlated words are connected to the root directly.
Each topic uses this tree structure as a prior.
Build Prior Tree Structures One remaining
question is the source of the word-level connections
across languages for the tree prior. We consider
two resources to build trees that correlate words
across languages. The first are a multilingual dic-
tionaries (dict), which match words with the same
meaning in different languages together. These re-
lations between words are used as the concepts in
the prior tree (Figure 1).
In addition, we extract the word alignments from
aligned sentences in a parallel corpus. The word
pairs define concepts for the prior tree (align). We
use both resources for our models (denoted as
ptLDA-dict and ptLDA-align) in our experiments
(Section 5) and show that they yield comparable
performance in SMT.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.9999088">
Inference of probabilistic models discovers the pos-
terior distribution over latent variables. For a col-
lection of D documents, each of which contains
Nd number of words, the latent variables of ptLDA
are: transition distributions πki for every topic k
and internal node i in the prior tree structure; multi-
nomial distributions over topics θd for every docu-
ment d; topic assignments zdn and path ydn for the
nth word wdn in document d. The joint distribution
of polylingual tree-based topic models is
</bodyText>
<equation confidence="0.670854">
p(w, z7, -y, θ, π; α, β)- lIHk Hi p(πki |βi) (4)
Hd (θd |α) · lld np(zdn |θd)
· H H (p(ydn|zdn, π)p(wdn|ydn)).
d n
Exact inference is intractable, so we turn to ap-
computer FRO market 市1, government 政府 science 科学
Prior Tree: 0 1
scientific policy 天气
computer, *,Ji
market, 市.*
government, 政府
science, 科学
</equation>
<page confidence="0.965495">
1169
</page>
<bodyText confidence="0.999949470588235">
proximate posterior inference to discover the latent
variables that best explain our data. Two widely
used approximation approaches are Markov chain
Monte Carlo (Neal, 2000, MCMC) and variational
Bayesian inference (Blei et al., 2003, VB). Both
frameworks produce good approximations of the
posterior mode (Asuncion et al., 2009). In addition,
Mimno et al. (2012) propose hybrid inference that
takes advantage of parallelizable variational infer-
ence for global variables (Wolfe et al., 2008) while
enjoying the sparse, efficient updates for local vari-
ables (Neal, 1993). In the rest of this section, we
discuss all three methods in turn.
We explore multiple inference schemes because
while all of these methods optimize likelihood be-
cause they might give different results on the trans-
lation task.
</bodyText>
<subsectionHeader confidence="0.861315">
4.1 Markov Chain Monte Carlo Inference
</subsectionHeader>
<bodyText confidence="0.999719">
We use a collapsed Gibbs sampler for tree-based
topic models to sample the path ydn and topic as-
signment zdn for word wdn,
</bodyText>
<equation confidence="0.9920785">
p(zdn = k, ydn = s|¬zdn, ¬ydn, w; α,β)
∝ I [Ω(s) = wdn] ·Nk|d+α
r
k0(Nk0|d+α)
· Ni→j  |k+βi→j
Hi→j∈s rj0(Ni→j0|k+βi→j0)
</equation>
<bodyText confidence="0.9994245">
where Ω(s) represents the word that path s leads
to, Nk|d is the number of tokens assigned to topic k
in document d and Ni→j|k is the number of times
edge i → j in the tree assigned to topic k, exclud-
ing the topic assignment zdn and its path ydn of
current token wdn. In practice, we sample the la-
tent variables using efficient sparse updates (Yao et
al., 2009; Hu and Boyd-Graber, 2012).
</bodyText>
<subsectionHeader confidence="0.992615">
4.2 Variational Bayesian Inference
</subsectionHeader>
<bodyText confidence="0.999636684210526">
Variational Bayesian inference approximates the
posterior distribution with a simplified variational
distribution q over the latent variables: document
topic proportions θ, transition probabilities π, topic
assignments z, and path assignments y.
Variational distributions typically assume a
mean-field distribution over these latent variables,
removing all dependencies between the latent vari-
ables. We follow this assumption for the transi-
tion probabilities q(π  |λ) and the document topic
proportions q(θ  |γ); both are variational Dirichlet
distributions. However, due to the tight coupling
between the path and topic variables, we must
model this joint distribution as one multinomial,
q(z, y  |φ). If word token wdn has K topics and
S paths, it has a K ∗ S length variational multino-
mial φdnks, which represents the probability that
the word takes path s in topic k. The complete
variational distribution is
</bodyText>
<equation confidence="0.8248265">
q(θ, π, z, y|γ, λ, φ) = Hd� g7(θd  |γd)· (5)
Hk Hi q(πki |λki) · l ld l In q(zdn, ydn |φdn).
</equation>
<bodyText confidence="0.999395166666667">
Our goal is to find the variational distribution q
that is closest to the true posterior, as measured by
the Kullback-Leibler (KL) divergence between the
true posterior p and variational distribution q. This
induces an “evidence lower bound” (ELBO, L) as a
function of a variational distribution q: L =
</bodyText>
<equation confidence="0.739778833333333">
Eq[log p(w, z, y, θ,π)] − Eq[log q(θ, π, z, y)]
= Ek Ei Eq [log p(πki |βi)]
+ Ed Eq[log p(θd|α)]
+ E En Eq[log p(zdn, ydn|θd, π)p(wdn|ydn)]
d
+ H[q(θ)] + H[q(π)] + H[q(z, y)], (6)
</equation>
<bodyText confidence="0.999045666666667">
where H[•] represents the entropy of a distribution.
Optimizing L using coordinate descent provides
the following updates:
</bodyText>
<equation confidence="0.990829142857143">
φdnkt ∝ exp{Ψ(γdk) − Ψ(Ek γdk) (7)
+ E (Ψ(λk,i→j) − Ψ(Ej0 λk,i→j0))};
i→j∈s
γdk = αk + E Es∈Q−1(wdn) φdnkt; (8)
n
λk,i→j = βi→j (9)
+ E E Es∈Q0(wdn) φdnktI [i → j ∈ s] ;
</equation>
<bodyText confidence="0.8999064">
d n
where Ω0(wdn) is the set of all paths that lead to
word wdn in the tree, and t represents one particular
path in this set. I [i → j ∈ s] is the indicator of
whether path s contains an edge from node i to j.
</bodyText>
<subsectionHeader confidence="0.99951">
4.3 Hybrid Stochastic Inference
</subsectionHeader>
<bodyText confidence="0.999980333333333">
Given the complementary strengths of MCMC and
VB, and following hybrid inference proposed by
Mimno et al. (2012), we also derive hybrid infer-
ence for ptLDA.
The transition distributions π are treated identi-
cally as in variational inference. We posit a varia-
tional Dirichlet distribution λ and choose the one
that minimizes the KL divergence between the true
posterior and the variational distribution.
For topic z and path y, instead of variational
updates, we use a Gibbs sampler within a document.
We sample zdn and ydn conditioned on the topic
</bodyText>
<page confidence="0.960916">
1170
</page>
<bodyText confidence="0.9987515">
and path assignments of all other document tokens,
based on the variational expectation of π,
</bodyText>
<equation confidence="0.962124666666667">
q(zdn = k, ydn = s|¬zdn, ¬ydn; w) ∝ (10)
(α + E m6=n I [zdm = k])
· exp{Eq[log p(ydn|zdn,π)p(wdn|ydn)]}.
</equation>
<bodyText confidence="0.999248833333333">
This equation embodies how this is a hybrid algo-
rithm: the first term resembles the Gibbs sampling
term encoding how much a document prefers a
topic, while the second term encodes the expecta-
tion under the variational distribution of how much
a path is preferred by this topic,
</bodyText>
<equation confidence="0.7937145">
Eq[log p(ydn|zdn, π)p(wdn|ydn)] = I[Q(ydn)=wdn]
· Ei→j∈ydn Eq[log Azdn,i→j].
</equation>
<bodyText confidence="0.999912">
For every document, we sweep over all its to-
kens and resample their topic zdn and path ydn
conditioned on all the other tokens’ topic and path
assignments ¬zdn and ¬ydn. To avoid bias, we
discard the first B burn-in sweeps and take the
following M samples. We then use the empirical
average of these samples update the global varia-
tional parameter q(π|λ) based on how many times
we sampled these paths
</bodyText>
<equation confidence="0.954803">
Ak,i→j = M Ed En Es∈Q−1(wdn) (I [i j ∈ s]
· I[zdn = k, ydn = s]) + Qi→j. (11)
</equation>
<bodyText confidence="0.999189">
For our experiments, we use the recommended set-
tings B = 5 and M = 5 from Mimno et al. (2012).
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99874720338983">
We evaluate our new topic model, ptLDA, and exist-
ing topic models—LDA, pLDA, and tLDA—on their
ability to induce domains for machine translation
and the resulting performance of the translations
on standard machine translation metrics.
Dataset and SMT Pipeline We use the NIST MT
Chinese-English parallel corpus (NIST), excluding
non-UN and non-HK Hansards portions as our train-
ing dataset. It contains 1.6M sentence pairs, with
40.4M Chinese tokens and 44.4M English tokens.
We replicate the SMT pipeline of Eidelman et al.
(2012): word segmentation (Tseng et al., 2005),
align (Och and Ney, 2003), and symmetrize (Koehn
et al., 2003) the data. We train a modified Kneser-
Ney trigram language model on English (Chen and
Goodman, 1996). We use CDEC (Dyer et al., 2010)
for decoding, and MIRA (Crammer et al., 2006)
for parameter training. To optimize SMT system,
we tune the parameters on NIST MT06, and report
results on three test sets: MT02, MT03 and MT05.2
Topic Models Configuration We compare our
polylingual tree-based topic model (ptLDA) against
tree-based topic models (tLDA), polylingual topic
models (pLDA) and vanilla topic models (LDA).3
We also examine different inference algorithms—
Gibbs sampling (gibbs), variational inference
(variational) and hybrid approach (variational-
hybrid)—on the effects of SMT performance. In
all experiments, we set the per-document Dirichlet
parameter α = 0.01 and the number of topics to
10, as used in Eidelman et al. (2012).
Resources for Prior Tree To build the tree for
tLDA and ptLDA, we extract the word correla-
tions from a Chinese-English bilingual dictio-
nary (Denisowski, 1997).4 We filter the dictionary
using the NIST vocabulary, and keep entries map-
ping single Chinese and single English words. The
prior tree has about 1000 word pairs (dict).
We also extract the bidirectional word align-
ments between Chinese and English using
GIZA++ (Och and Ney, 2003). We then remove
the word pairs appearing more than 50K times or
fewer than 500 times and construct a second prior
tree with about 2500 word pairs (align).
We apply both trees to tLDA and ptLDA, denoted
as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA-
align. However, tLDA-align and ptLDA-align do
worse than tLDA-dict and ptLDA-dict, so we omit
tLDA-align in the results.
Domain Adaptation using Topic Models We
examine the effectiveness of using topic models
for domain adaptation on standard SMT evalua-
tion metrics—BLEU (Papineni et al., 2002) and
TER (Snover et al., 2006). We report the results
on three different test sets (Figure 2), and all SMT
results are averaged over five runs.
We refer to the SMT model without domain adap-
tation as baseline.5 LDA marginally improves ma-
chine translation (less than half a BLEU point).
</bodyText>
<footnote confidence="0.9954879">
2The NIST datasets contain 878, 919, 1082 and 1664 sen-
tences for MT02, MT03, MT05 and MT06 respectively.
3For Gibbs sampling, we use implementations available in
Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum,
2002) for LDA and pLDA.
4This is a two-level tree structure. However, one could
build a more sophisticated tree prior with a hierarchical dictio-
nary such as multilingual WordNet.
5Our replication of Eidelman et al. (2012) yields slightly
higher baseline performance, but the trend is consistent.
</footnote>
<page confidence="0.992674">
1171
</page>
<figure confidence="0.99685825">
model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict
BLEU Score
37
36
35
34
33
32
1
37
36
35
34
33
32
1
37
36
35
34
33
32
31
34.8 +0.3 +0.6 +0.4
35.1
+0.1 +0.3 +0.2 +0.7 +0.4
31.4 +0.4 +0.7 +0.4 +1 +0.4
gibbs
model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict
+1.2
+0.5 34.8
+0.4 +0.5 +0.4 +0.8 +0.5
35.1 −0.1 +0.2 −0.1 +0.2 +0.2
31.4 +0.3 +0.5 +0.3 +0.8 +0.4
variational
34.8 +0.2 +0.4 +0.2 +0.7 +0.4
35.1 −0.1 −0.1 −0.1 +0.2 +0.2
31.4 +0.3 +0.3 +0.1 +0.6 +0.3
variational−hybrid
mt02
mt03
mt05
TER Score
66
64
62
60
66
64
62
60
66
64
62
60
58
56
58
56
58
56
61.9 −0.1
60.1 −0.3 −0.9 −0.8 −1.9 −0.9
63.3
−0.9 −1.3 −1.2
−1 −1.2
gibbs
−2.5
−2.6
−1.1
−1.1
61.9 −0.4 −1 −0.6
−1.6 −1.3
60.1 −0.2 −0.5 −0.1 −1 −0.7
63.3 −0.5 −1 −0.4 −1.5 −1.2
variational
61.9 −0.3 −0.7 −0.1
60.1 0 −0.2 +0.2 −1.1 −0.5
63.3 −0.1
0.4 −0.7
variational−hybrid
−1.6
−1.6
−0.9
−0.8
mt02
mt03
mt05
</figure>
<figureCaption confidence="0.8229748">
Figure 2: Machine translation performance for different models and inference algorithms against the
baseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores. Our proposed ptLDA
performs best. Results are averaged over 5 random runs. For model ptLDA-dict with different inference
schemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the results
on MT03 using variational and variational-hybrid inferences.
</figureCaption>
<bodyText confidence="0.998523526315789">
Polylingual topic models pLDA and tree-based
topic models tLDA-dict are consistently better than
LDA, suggesting that incorporating additional bilin-
gual knowledge improves topic models. These im-
provements are not redundant: our new ptLDA-dict
model, which has aspects of both models yields the
best performance among these approaches—up to a
1.2 BLEU point gain (higher is better), and -2.6 TER
improvement (lower is better). The BLEU improve-
ment is significant (Koehn, 2004) at p = 0.01,6
except on MT03 with variational and variational-
hybrid inference.
While ptLDA-align performs better than base-
line SMT and LDA, it is worse than ptLDA-dict,
possibly because of errors in the word alignments,
making the tree priors less effective.
Scalability While gibbs has better translation
scores than variational and variational-hybrid, it
is less scalable to larger datasets. With 1.6M NIST
</bodyText>
<footnote confidence="0.909223">
6Because we have multiple runs of each topic model (and
thus different translation models), we select the run closest to
the average BLEU for the translation significance test.
</footnote>
<bodyText confidence="0.999767666666667">
training sentences, gibbs takes nearly a week to
run 1000 iterations. In contrast, the parallelized
variational and variational-hybrid approaches,
which we implement in MapReduce (Dean and
Ghemawat, 2004; Wolfe et al., 2008; Zhai et al.,
2012), take less than a day to converge.
</bodyText>
<sectionHeader confidence="0.998487" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9998022">
In this section, we qualitatively analyze the trans-
lation results and investigate how ptLDA and its
cousins improve SMT. We also discuss other ap-
proaches to improve unsupervised domain adapta-
tion for SMT.
</bodyText>
<subsectionHeader confidence="0.998479">
6.1 How do Topic Models Help SMT?
</subsectionHeader>
<bodyText confidence="0.999836428571429">
We present two examples of how topic models can
improve SMT. The first example shows both LDA
and ptLDA improve the baseline. The second exam-
ple shows how LDA introduce biases that mislead
SMT and how ptLDA’s bilingual constraints correct
these mistakes.
Figure 3 shows a sentence about a company
</bodyText>
<page confidence="0.986245">
1172
</page>
<figure confidence="0.87790975">
source hoz在北美地Cx售出大�7�1tFi � � � , ta套售价n LDA-Topic 0 (business)
baseline
LDA
ptLDA
</figure>
<construct confidence="0.933112266666667">
公司(company), 中国(China), 服*(service), 市i;3i
(market), 技�(technology), 企A(industry), 提供
(provide), 1UR(develop), 年(year), Y�Ipp(product),
上, 合作(coorporate), 中, 管理(manage), 投Sf
(invest), ik�b�(economy), 国�(international), 系*
(system), fi&apos;T(bank)
ptLDA-Topic 0 (business)
公司(company), 服*(service), 市�(market), 技
~(technology), china, 企�(industry), ��
(product), market, company, technology, services,
系�(system), year, industry, products, business, �
(economy), information, 管理(manage), 投5f
(invest), percent, 网�(internet), companies, world,
system, 信息(information), 增*(increase), A*
(device), service, Ak*(service)
</construct>
<figure confidence="0.87394225">
�����, 相容游�������
reference sony has already sold about 570,000 units of narrowband connection
kits in north america at the price of about 39 us dollars and some 20
compatible games .
source ... PgJ$1JM-3_L11q- ... ... 相容游An;q__ * �
reference ... connection kits ... ... some 20 compatible games .
... internet links set ...
... internet links kit ...
... internet links kit ...
... with about 20 of the game .
... , there are about 20 compatible games .
... , there are about 20 compatible games .
</figure>
<figureCaption confidence="0.9961112">
Figure 3: Better SMT result using topic models for domain adaptation. Top row: the source sentence and
its reference translation. Middle row: the highlighted translations from different approaches. Bottom row:
the change of relevant translation probabilities after incorporating the domain knowledge from LDA and
ptLDA. Right: most-probable words of the topic the source sentence is assigned to under LDA (top) and
ptLDA (bottom). The Chinese translations are in parenthesis.
</figureCaption>
<bodyText confidence="0.999975486486486">
introducing new technology gadgets where both
LDA and ptLDA improve translations. The base-
line translates “4#” to “set” (red), and “��” to
“with” (blue), which do not capture the reference
meaning of a add-on device that works with com-
patible games. Both LDA and ptLDA assign this
sentence to a business domain, which makes the
translations probabilities shift toward correct trans-
lations: the probability of translating “�V” to
“compatible” and the probability of translating “4
#” to “kit” in the business domain are both signif-
icantly larger than without the domain knowledge;
and the probabilities of translating “��” to “with”
and the probability of translating “set” to “4#”
in the business domain decrease.
The second example (Figure 4) illustrates how
ptLDA offers further improvements over LDA. The
source sentence discusses foreign affairs. The
baseline correctly translates the word “WPA” to
“affect”. However, LDA—which only takes mono-
lingual information from the source language—
assigns this sentence to economic development.
This misleads SMT to lower the probability for
the correct translation “affect”; it chooses “impact”
instead. In contrast, ptLDA—which incorporates
bilingual constraints—successfully labels this sen-
tence as foreign affairs and produces a softer, more
nuanced translation that better matches the refer-
ence. The translation of “•%” is very similar,
except in this case, both the baseline and LDA
produce the incorrect translation “the commitment
of”. This is possible because the probabilities of
translating “•%” to “promised to” and translat-
ing “promised to” to “•%” (the correct transla-
tion, in both directions) increase when conditioned
on ptLDA’s correct topic but decrease when condi-
tioned on LDA’s incorrect topic.
</bodyText>
<subsectionHeader confidence="0.999168">
6.2 Other Approaches
</subsectionHeader>
<bodyText confidence="0.999994235294118">
Other approaches have used topic models for ma-
chine translation. Xiao et al. (2012) present a topic
similarity model based on LDA that produces a fea-
ture that weights grammar rules based on topic
compatibility. They also model the source and tar-
get side of rules and compare the target similarity
during decoding by projecting the target distribu-
tion into the source space. Hasler et al. (2012)
use the source-side topic assignments from hidden
topic Markov models (Gruber et al., 2007, HTMM)
which models documents as a Markov chain and
assign one topic to the whole sentence, instead of
a mixture of topics. Su et al. (2012) also apply
HTMM to monolingual data and apply the results to
machine translation. To our knowledge, however,
this is the first work to use multilingual topic mod-
els for domain adaptation in machine translation.
</bodyText>
<subsectionHeader confidence="0.997958">
6.3 Improving Language Models
</subsectionHeader>
<bodyText confidence="0.999794333333333">
Topic models capture document-level properties
of language, but a critical component of machine
translation systems is the language model, which
provides local constraints and preferences. Do-
main adaptation for language models (Bellegarda,
2004; Wood and Teh, 2009) is an important avenue
for improving machine translation. Models that si-
multaneously discover global document themes as
well as local, contextual domain-specific informa-
</bodyText>
<page confidence="0.968383">
1173
</page>
<table confidence="0.950017266666667">
source 消息指出, �国使kft人j%向中方官j%表示, �国方面并没有支持朝�人以��方法前往�国, �国并不希望�类事件再次
�生, 以免�中国和朝�半�双方�的�系�来影响, �国方面并向中国方面承�, 愿意�助中国管理好在京的�国居民
reference sources said rok embassy personnel told chinese officials that rok has not backed any dpr koreans to get to rok in such a manner
and rok would not like such things happen again to affect relationship between china and the two sides of the korean peninsula .
rok also promised to assist china in the administration of koreans in beijing .
source É 不希望 ... ... 以免n...�系;R来影响... ... �国方面并向中国方面承TT...
reference ... would not like ... ... to affect the relationship ... ... rok also promised to the chinese side ...
baseline ... does not want ... ... so as not to affect the relations... ... south korea and the commitment of the chinese side ...
LDA ... does not hope that ... ... so as to avoid impact the relations... ... the rok side , and the commitment of the chinese side ...
ptLDA ... does not hope that ... ... so as not to affect the relations... ... south korea has promised to the chinese side ...
LDA-Topic 5 (economic development) ptLDA-Topic 2 (foreign affairs)
It%develop), 国(country), �(two), 中国(China), �系(relation), china, NIK(issue), military, united, president, 国家(country), 地区(area), minister, 伊
中, 合作(cooperate), ik�IVF(economy), 人民(people), 友好(friendly), 拉克(Iraq), 和平(peace), nuclear, people, .VfA;(president), peace, security, �aN
国家(country), 新(new), Ip7ftroblem), 上, 加强(emphasize), 重要 (UN), W*(military), 以色列(Israel), iraq, foreign, international, 部K(army), beijing,
(important), 和平(peace), 共同(together), 建A(build), 世界(world) world, defense, south, 安 全(security), war, t1(agreement), 会*(conference)
</table>
<figureCaption confidence="0.8045896">
Figure 4: Better SMT result using ptLDA compared to LDA and the baseline. Top row: the source sentence
and a reference translation. Second row: the highlighted translations from different models. Third row:
the change of relevant translation probabilities after incorporating domain knowledge from LDA and
ptLDA. Bottom row: most-probable words for the topics the source sentence is assigned to under LDA
(left) and ptLDA (right). The meanings of Chinese words are in parenthesis.
</figureCaption>
<bodyText confidence="0.820186">
tion (Wallach, 2006; Boyd-Graber and Blei, 2008)
may offer further improvements.
</bodyText>
<subsectionHeader confidence="0.986156">
6.4 External Data
</subsectionHeader>
<bodyText confidence="0.999927333333333">
The topic models presented here only require weak
alignment between documents at the document
level. Extending to larger datasets for learning
topics is straightforward in principle. For exam-
ple, ptLDA could learn domains from a much larger
corpus like Wikipedia and then apply the extracted
domains to machine translation data. However,
this presents further challenges, as Wikipedia’s do-
mains are not representative of newswire machine
translation datasets; a flexible hierarchical topic
model (Teh et al., 2006) would better distinguish
useful domains from extraneous ones.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999969882352941">
Topic models generate great interest, but their use
in “real world” applications still lags; this is par-
ticularly true for multilingual topic models. As
topic models become more integrated in common-
place applications, their adoption, understanding,
and robustness will improve.
This paper contributes to the deeper integration
of topic models into critical applications by present-
ing a new multilingual topic model, ptLDA, com-
paring it with other multilingual topic models on
a machine translation task, and showing that these
topic models improve machine translation. ptLDA
models both source and target data to induce do-
mains from both dictionaries and alignments. Fur-
ther improvement is possible by incorporating topic
models deeper in the decoding process and adding
domain knowledge to the language model.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999983888888889">
We would like to thank the anonymous reviewers,
Doug Oard, and John Morgan for their helpful com-
ments, and thank Junhui Li and Ke Wu for insight-
ful discussions. This work was supported by NSF
Grant IIS-1320538. Boyd-Graber is also supported
by NSF Grant CCF-1018625. Any opinions, find-
ings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.
</bodyText>
<sectionHeader confidence="0.998611" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996052416666667">
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings
of the International Conference of Machine Learn-
ing.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of Uncertainty in
Artificial Intelligence.
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. vol-
ume 42, pages 93–108.
</reference>
<page confidence="0.961916">
1174
</page>
<reference confidence="0.998365834951456">
David M. Blei and John D. Lafferty. 2009. Visualizing
topics with Multi-Word expressions. arXiv.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3.
Jordan Boyd-Graber and David M. Blei. 2008. Syn-
tactic topic models. In Proceedings of Advances in
Neural Information Processing Systems.
Jordan Boyd-Graber and David M. Blei. 2009. Multi-
lingual topic models for unaligned text. In Proceed-
ings of Uncertainty in Artificial Intelligence.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual
supervised latent Dirichlet allocation. In Proceed-
ings of Emperical Methods in Natural Language
Processing.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proceedings of Emperical Methods in Natu-
ral Language Processing.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Proceedings ofAdvances in Neural Information Pro-
cessing Systems.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the Association for
Computational Linguistics.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In
Proceedings of the Human Language Technology
Conference.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified data processing on large clusters.
In Symposium on Operating System Design and Im-
plementation.
Paul Denisowski. 1997. CEDICT.
http://www.mdbg.net/chindict/.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the Association
for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.
2007. Hidden topic Markov models. In Artificial
Intelligence and Statistics.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse lexicalised features and topic adaptation for
SMT. In Proceedings of IWSLT.
Yuening Hu and Jordan Boyd-Graber. 2012. Efficient
tree-based topic modeling. In Proceedings of the As-
sociation for Computational Linguistics.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2013. Interactive topic modeling.
Machine Learning Journal.
Saurabh S. Kataria, Krishnan S. Kumar, Rajeev R. Ras-
togi, Prithviraj Sen, and Srinivasan H. Sengamedu.
2011. Entity disambiguation with hierarchical topic
models. In Knowledge Discovery and Data Mining.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Emperical Methods in Natural Language Process-
ing.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.
Li Fei-Fei and Pietro Perona. 2005. A Bayesian hier-
archical model for learning natural scene categories.
In Computer Vision and Pattern Recognition.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of Em-
perical Methods in Natural Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of Emper-
ical Methods in Natural Language Processing.
David Mimno, Matthew Hoffman, and David Blei.
2012. Sparse stochastic inference for latent Dirich-
let allocation. In Proceedings of the International
Conference of Machine Learning.
Radford M. Neal. 1993. Probabilistic inference using
Markov chain Monte Carlo methods. Technical Re-
port CRG-TR-93-1, University of Toronto.
</reference>
<page confidence="0.840799">
1175
</page>
<reference confidence="0.999692656716418">
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9(2):249–
265.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics,
pages 311–318.
Alessandro Perina, Pietro Lovato, Vittorio Murino, and
Manuele Bicego. 2010. Biologically-aware latent
Dirichlet allocation (balda) for the classification of
expression microarray. In Proceedings of the 5th
IAPR international conference on Pattern recogni-
tion in bioinformatics, PRIB’10.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the Association for Computational
Linguistics.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In SIGHAN
Workshop on Chinese Language Processing.
Hanna M. Wallach. 2006. Topic modeling: Beyond
bag-of-words. In Proceedings of the International
Conference of Machine Learning.
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008.
Fully distributed EM for very large datasets. In Pro-
ceedings of the International Conference of Machine
Learning, pages 1184–1191.
Frank Wood and Yee Whye Teh. 2009. A hierarchi-
cal nonparametric Bayesian approach to statistical
language model domain adaptation. In Proceedings
of the International Conference on Artificial Intelli-
gence and Statistics, volume 12.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proceedings
of the Association for Computational Linguistics.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Knowledge
Discovery and Data Mining.
Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-
hamad Alkhouja. 2012. Mr. LDA: A flexible large
scale topic modeling package using variational infer-
ence in mapreduce. In Proceedings of World Wide
Web Conference.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the Association for Computational Lin-
guistics.
</reference>
<page confidence="0.99434">
1176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987106">
<title confidence="0.99993">Polylingual Tree-Based Topic Models for Translation Domain Adaptation</title>
<author confidence="0.999925">Eidelman Jordan Boyd-Graber</author>
<affiliation confidence="0.999379">Computer Science Computer Science FiscalNote Inc. iSchool and University of Maryland University of Maryland Washington DC University of Maryland</affiliation>
<email confidence="0.99942">ynhu@cs.umd.eduzhaike@cs.umd.eduvlad@fiscalnote.comjbg@umiacs.umd.edu</email>
<abstract confidence="0.998798857142857">Topic models, an unsupervised technique for inferring translation domains improve machine translation quality. However, previous work uses only the source language and completely ignores the target language, which can disambiguate domains. We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes. We evaluate our model on a Chinese to English translation task and obtain up to 1.2 over strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via Dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="10963" citStr="Andrzejewski et al., 2009" startWordPosition="1693" endWordPosition="1697">ee-based Topic Models In this section, we bring existing tree-based topic models (Boyd-Graber et al., 2007, tLDA) and polylingual topic models (Mimno et al., 2009, pLDA) together and create the polylingual treebased topic model (ptLDA) that incorporates both word-level correlations and document-level alignment information. Word-level Correlations Tree-based topic models incorporate the correlations between words by encouraging words that appear together in a concept to have similar probabilities given a topic. These concepts can come from WordNet (BoydGraber and Resnik, 2010), domain experts (Andrzejewski et al., 2009), or user constrains (Hu et al., 2013). When we gather concepts from bilingual resources, these concepts can connect different languages. For example, if a bilingual dictionary defines “U#p–q” as “computer”, we combine these words in a concept. We organize the vocabulary in a tree structure based on these concepts (Figure 1): words in the same concept share a common parent node, and then that concept becomes one of many children of the root node. Words that are not in any concept— uncorrelated words—are directly connected to the root node. We call this structure the tree prior. When this tree </context>
<context position="13844" citStr="Andrzejewski et al., 2009" startWordPosition="2168" endWordPosition="2171">cting a concept in a topic responsible for generating each word token. This is represented by a path yd,n through the topic’s tree. The probability of a path in a topic depends on the transition probabilities in a topic. Each concept i in topic k has a distribution over its children nodes is governed by a Dirichlet prior: πk,i ∼ Dir(βi). Each path ends in a word (i.e., a leaf node) and the probability of a path is the product of all of the transitions between topics it traverses. Topics have correlations over words because the Dirichlet parameters can encode positive or negative correlations (Andrzejewski et al., 2009). With these correlated in topics in hand, the generation of documents are very similar to LDA. For every document d, we first sample a distribution over topics θd from a Dirichlet prior Dir(α). For every token in the documents, we first sample a topic zdn from the multinomial distribution θd, and then sample a path ydn along the tree according to the transition distributions specified by topic zdn. Because every path ydn leads to a word wdn in language ldn, we append the sampled word wdn to document dldn. Aligned documents have words in both languages; monolingual documents only have words in</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating domain knowledge into topic modeling via Dirichlet forest priors. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Max Welling</author>
<author>Padhraic Smyth</author>
<author>Yee Whye Teh</author>
</authors>
<title>On smoothing and inference for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="17656" citStr="Asuncion et al., 2009" startWordPosition="2833" endWordPosition="2836"> π; α, β)- lIHk Hi p(πki |βi) (4) Hd (θd |α) · lld np(zdn |θd) · H H (p(ydn|zdn, π)p(wdn|ydn)). d n Exact inference is intractable, so we turn to apcomputer FRO market 市1, government 政府 science 科学 Prior Tree: 0 1 scientific policy 天气 computer, *,Ji market, 市.* government, 政府 science, 科学 1169 proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo (Neal, 2000, MCMC) and variational Bayesian inference (Blei et al., 2003, VB). Both frameworks produce good approximations of the posterior mode (Asuncion et al., 2009). In addition, Mimno et al. (2012) propose hybrid inference that takes advantage of parallelizable variational inference for global variables (Wolfe et al., 2008) while enjoying the sparse, efficient updates for local variables (Neal, 1993). In the rest of this section, we discuss all three methods in turn. We explore multiple inference schemes because while all of these methods optimize likelihood because they might give different results on the translation task. 4.1 Markov Chain Monte Carlo Inference We use a collapsed Gibbs sampler for tree-based topic models to sample the path ydn and topi</context>
</contexts>
<marker>Asuncion, Welling, Smyth, Teh, 2009</marker>
<rawString>Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. 2009. On smoothing and inference for topic models. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<volume>42</volume>
<pages>93--108</pages>
<contexts>
<context position="33619" citStr="Bellegarda, 2004" startWordPosition="5438" endWordPosition="5439">ch models documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. Su et al. (2012) also apply HTMM to monolingual data and apply the results to machine translation. To our knowledge, however, this is the first work to use multilingual topic models for domain adaptation in machine translation. 6.3 Improving Language Models Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Domain adaptation for language models (Bellegarda, 2004; Wood and Teh, 2009) is an important avenue for improving machine translation. Models that simultaneously discover global document themes as well as local, contextual domain-specific informa1173 source 消息指出, �国使kft人j%向中方官j%表示, �国方面并没有支持朝�人以��方法前往�国, �国并不希望�类事件再次 �生, 以免�中国和朝�半�双方�的�系�来影响, �国方面并向中国方面承�, 愿意�助中国管理好在京的�国居民 reference sources said rok embassy personnel told chinese officials that rok has not backed any dpr koreans to get to rok in such a manner and rok would not like such things happen again to affect relationship between china and the two sides of the korean peninsula . rok also pr</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: review and perspectives. volume 42, pages 93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Visualizing topics with Multi-Word expressions.</title>
<date>2009</date>
<tech>arXiv.</tech>
<contexts>
<context position="966" citStr="Blei and Lafferty, 2009" startWordPosition="127" endWordPosition="130">cs.umd.edu Abstract Topic models, an unsupervised technique for inferring translation domains improve machine translation quality. However, previous work uses only the source language and completely ignores the target language, which can disambiguate domains. We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes. We evaluate our model on a Chinese to English translation task and obtain up to 1.2 BLEU improvement over strong baselines. 1 Introduction Probabilistic topic models (Blei and Lafferty, 2009), exemplified by latent Dirichlet allocation (Blei et al., 2003, LDA), are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Peri</context>
</contexts>
<marker>Blei, Lafferty, 2009</marker>
<rawString>David M. Blei and John D. Lafferty. 2009. Visualizing topics with Multi-Word expressions. arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="1029" citStr="Blei et al., 2003" startWordPosition="137" endWordPosition="140">ing translation domains improve machine translation quality. However, previous work uses only the source language and completely ignores the target language, which can disambiguate domains. We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes. We evaluate our model on a Chinese to English translation task and obtain up to 1.2 BLEU improvement over strong baselines. 1 Introduction Probabilistic topic models (Blei and Lafferty, 2009), exemplified by latent Dirichlet allocation (Blei et al., 2003, LDA), are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 20</context>
<context position="17560" citStr="Blei et al., 2003" startWordPosition="2819" endWordPosition="2822">ocument d. The joint distribution of polylingual tree-based topic models is p(w, z7, -y, θ, π; α, β)- lIHk Hi p(πki |βi) (4) Hd (θd |α) · lld np(zdn |θd) · H H (p(ydn|zdn, π)p(wdn|ydn)). d n Exact inference is intractable, so we turn to apcomputer FRO market 市1, government 政府 science 科学 Prior Tree: 0 1 scientific policy 天气 computer, *,Ji market, 市.* government, 政府 science, 科学 1169 proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo (Neal, 2000, MCMC) and variational Bayesian inference (Blei et al., 2003, VB). Both frameworks produce good approximations of the posterior mode (Asuncion et al., 2009). In addition, Mimno et al. (2012) propose hybrid inference that takes advantage of parallelizable variational inference for global variables (Wolfe et al., 2008) while enjoying the sparse, efficient updates for local variables (Neal, 1993). In the rest of this section, we discuss all three methods in turn. We explore multiple inference schemes because while all of these methods optimize likelihood because they might give different results on the translation task. 4.1 Markov Chain Monte Carlo Infere</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Syntactic topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="36000" citStr="Boyd-Graber and Blei, 2008" startWordPosition="5784" endWordPosition="5787">r), 建A(build), 世界(world) world, defense, south, 安 全(security), war, t1(agreement), 会*(conference) Figure 4: Better SMT result using ptLDA compared to LDA and the baseline. Top row: the source sentence and a reference translation. Second row: the highlighted translations from different models. Third row: the change of relevant translation probabilities after incorporating domain knowledge from LDA and ptLDA. Bottom row: most-probable words for the topics the source sentence is assigned to under LDA (left) and ptLDA (right). The meanings of Chinese words are in parenthesis. tion (Wallach, 2006; Boyd-Graber and Blei, 2008) may offer further improvements. 6.4 External Data The topic models presented here only require weak alignment between documents at the document level. Extending to larger datasets for learning topics is straightforward in principle. For example, ptLDA could learn domains from a much larger corpus like Wikipedia and then apply the extracted domains to machine translation data. However, this presents further challenges, as Wikipedia’s domains are not representative of newswire machine translation datasets; a flexible hierarchical topic model (Teh et al., 2006) would better distinguish useful do</context>
</contexts>
<marker>Boyd-Graber, Blei, 2008</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2008. Syntactic topic models. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Multilingual topic models for unaligned text.</title>
<date>2009</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2009. Multilingual topic models for unaligned text. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2923" citStr="Boyd-Graber and Resnik, 2010" startWordPosition="427" endWordPosition="430">2; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review these models for discovering topics in multilingual datasets and discuss how they can improve SMT. However, no models combine multiple bridges between languages. In Section 3, we create a model—the polylingual tree-based topic models (ptLDA)—that uses information from both external dictionaries and document alignments simultaneously. In Section 4, we derive both MCMC and variational inference for this new topic model. In Section 5, we evaluate our model on the task of SMT using aligned datasets. We show that ptLDA offers bett</context>
<context position="10113" citStr="Boyd-Graber and Resnik, 2010" startWordPosition="1570" endWordPosition="1573">opic models for parallel corpora: Zhao and Xing (2006) assume aligned word pairs share same topics; Mimno et al. (2009) connect different languages through comparable documents. These models take advantage of word or document alignment information and infer more robust topics from the aligned dataset. On the other hand, lexical information can induce topics from multilingual corpora. For instance, orthographic similarity connects words with the same meaning in related languages (BoydGraber and Blei, 2009), and dictionaries are a more general source of information on which words share meaning (Boyd-Graber and Resnik, 2010). These two approaches are not mutually exclusive, however; they reveal different connections across languages. In the next section, we combine these two approaches into a polylingual tree-based topic model. 3 Polylingual Tree-based Topic Models In this section, we bring existing tree-based topic models (Boyd-Graber et al., 2007, tLDA) and polylingual topic models (Mimno et al., 2009, pLDA) together and create the polylingual treebased topic model (ptLDA) that incorporates both word-level correlations and document-level alignment information. Word-level Correlations Tree-based topic models inc</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10443" citStr="Boyd-Graber et al., 2007" startWordPosition="1618" endWordPosition="1621">mation can induce topics from multilingual corpora. For instance, orthographic similarity connects words with the same meaning in related languages (BoydGraber and Blei, 2009), and dictionaries are a more general source of information on which words share meaning (Boyd-Graber and Resnik, 2010). These two approaches are not mutually exclusive, however; they reveal different connections across languages. In the next section, we combine these two approaches into a polylingual tree-based topic model. 3 Polylingual Tree-based Topic Models In this section, we bring existing tree-based topic models (Boyd-Graber et al., 2007, tLDA) and polylingual topic models (Mimno et al., 2009, pLDA) together and create the polylingual treebased topic model (ptLDA) that incorporates both word-level correlations and document-level alignment information. Word-level Correlations Tree-based topic models incorporate the correlations between words by encouraging words that appear together in a concept to have similar probabilities given a topic. These concepts can come from WordNet (BoydGraber and Resnik, 2010), domain experts (Andrzejewski et al., 2009), or user constrains (Hu et al., 2013). When we gather concepts from bilingual r</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Chong Wang</author>
<author>Sean Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1353" citStr="Chang et al., 2009" startWordPosition="182" endWordPosition="185">erive three different inference schemes. We evaluate our model on a Chinese to English translation task and obtain up to 1.2 BLEU improvement over strong baselines. 1 Introduction Probabilistic topic models (Blei and Lafferty, 2009), exemplified by latent Dirichlet allocation (Blei et al., 2003, LDA), are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 2011). In particular, we use topic models to aid statistical machine translation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to sy</context>
</contexts>
<marker>Chang, Boyd-Graber, Wang, Gerrish, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23280" citStr="Chen and Goodman, 1996" startWordPosition="3807" endWordPosition="3810">e domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— Gibbs sampling (gibbs), variational inference (variational) and hybrid approach (variationalhybrid)—on the effects of SMT performance. I</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Michael Pust</author>
</authors>
<title>Two easy improvements to lexical weighting.</title>
<date>2011</date>
<booktitle>In Proceedings of the Human Language Technology Conference.</booktitle>
<contexts>
<context position="5863" citStr="Chiang et al. (2011)" startWordPosition="884" endWordPosition="887">e are called domains. Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example, “�7K” in a newspaper usually means “underwater diving”. On social media, it means a non-contributing “lurker”. Domain Adaptation for SMT Training a SMT system using diverse data requires domain adaptation. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al., 2009) to model domain information. Chiang et al. (2011) combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain. However, these approaches treat domains as hand-labeled, constant, and known a priori. This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain.1 1Henceforth we will use the term “topic” and “domain” interchangeably: “topic” to refer to the concept in topic models and “domain” to refer to SMT corpora. 2.2 Inducing Domains with </context>
<context position="8580" citStr="Chiang et al., 2011" startWordPosition="1332" endWordPosition="1335">c dependent lexical weight and the topic distribution of the document, from which we extract the phrase. Eidelman et al. (2012) compute the resulting model score by combining these features in a linear model with other standard SMT features and optimizing the weights. Conceptually, this approach is just reweighting examples. The probability of a topic given a document is never zero. Every translation observed in the training set will contribute to pk(e|f); many of the expected counts, however, will be less than one. This obviates the explicit smoothing used in other domain adaptation systems (Chiang et al., 2011). 1167 We adopt this framework in its entirety. Our contribution are topics that capture multilingual information and thus better capture the domains in the parallel corpus. 2.3 Beyond Vanilla Topic Models Eidelman et al. (2012) ignore a wealth of information that could improve topic models and help machine translation. Namely, they only use monolingual data from the source language, ignoring all target-language data and available lexical semantic resources between source and target languages. Different complement each other to reduce ambiguity. For example, “��” in a Chinese document can be e</context>
</contexts>
<marker>Chiang, DeNeefe, Pust, 2011</marker>
<rawString>David Chiang, Steve DeNeefe, and Michael Pust. 2011. Two easy improvements to lexical weighting. In Proceedings of the Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="7049" citStr="Crammer et al., 2006" startWordPosition="1076" endWordPosition="1080">orpora. 2.2 Inducing Domains with Topic Models Topic models take the number of topics K and a collection of documents as input, where each document is a bag of words. They output two distributions: a distribution over topics for each document d; and a distribution over words for each topic. If each topic defines a SMT domain, the document’s topic distribution is a soft domain assignment for that document. Given the soft domain assignments, Eidelman et al. (2012) extract lexical weighting features conditioned on the topics, optimizing feature weights using the Margin Infused Relaxed Algorithm (Crammer et al., 2006, MIRA). The topics come from source documents only and create topic-specific lexical weights from the per-document topic distribution p(k |d). The lexical probability conditioned on the topic is expected count ek(e, f) of a word translation pair under topic k, ˆck(e,f) _ Ed p(k|d)cd(e,f), (1) where cd(0) is the number of occurrences of the word pair in document d. The lexical probability conditioned on topic k is the unsmoothed probability estimate of those expected counts pw(e|f; k) _ &amp; ck(e,f) ˆck (e,f) (2) from which we can compute the phrase pair probabilities pw(¯e |¯f; k) by multiplying</context>
<context position="23359" citStr="Crammer et al., 2006" startWordPosition="3822" endWordPosition="3825">ns on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— Gibbs sampling (gibbs), variational inference (variational) and hybrid approach (variationalhybrid)—on the effects of SMT performance. In all experiments, we set the per-document Dirichlet parameter α = 0.01 and the</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters. In</title>
<date>2004</date>
<booktitle>Symposium on Operating System Design and Implementation.</booktitle>
<contexts>
<context position="28425" citStr="Dean and Ghemawat, 2004" startWordPosition="4648" endWordPosition="4651">t, possibly because of errors in the word alignments, making the tree priors less effective. Scalability While gibbs has better translation scores than variational and variational-hybrid, it is less scalable to larger datasets. With 1.6M NIST 6Because we have multiple runs of each topic model (and thus different translation models), we select the run closest to the average BLEU for the translation significance test. training sentences, gibbs takes nearly a week to run 1000 iterations. In contrast, the parallelized variational and variational-hybrid approaches, which we implement in MapReduce (Dean and Ghemawat, 2004; Wolfe et al., 2008; Zhai et al., 2012), take less than a day to converge. 6 Discussion In this section, we qualitatively analyze the translation results and investigate how ptLDA and its cousins improve SMT. We also discuss other approaches to improve unsupervised domain adaptation for SMT. 6.1 How do Topic Models Help SMT? We present two examples of how topic models can improve SMT. The first example shows both LDA and ptLDA improve the baseline. The second example shows how LDA introduce biases that mislead SMT and how ptLDA’s bilingual constraints correct these mistakes. Figure 3 shows a </context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In Symposium on Operating System Design and Implementation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Denisowski</author>
</authors>
<date>1997</date>
<note>CEDICT. http://www.mdbg.net/chindict/.</note>
<contexts>
<context position="24177" citStr="Denisowski, 1997" startWordPosition="3948" endWordPosition="3949">tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— Gibbs sampling (gibbs), variational inference (variational) and hybrid approach (variationalhybrid)—on the effects of SMT performance. In all experiments, we set the per-document Dirichlet parameter α = 0.01 and the number of topics to 10, as used in Eidelman et al. (2012). Resources for Prior Tree To build the tree for tLDA and ptLDA, we extract the word correlations from a Chinese-English bilingual dictionary (Denisowski, 1997).4 We filter the dictionary using the NIST vocabulary, and keep entries mapping single Chinese and single English words. The prior tree has about 1000 word pairs (dict). We also extract the bidirectional word alignments between Chinese and English using GIZA++ (Och and Ney, 2003). We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align). We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDAalign. However, tLDA-align and ptLDA-align do worse than tLDA-dict and</context>
</contexts>
<marker>Denisowski, 1997</marker>
<rawString>Paul Denisowski. 1997. CEDICT. http://www.mdbg.net/chindict/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL System Demonstrations.</booktitle>
<contexts>
<context position="23313" citStr="Dyer et al., 2010" startWordPosition="3814" endWordPosition="3817">the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— Gibbs sampling (gibbs), variational inference (variational) and hybrid approach (variationalhybrid)—on the effects of SMT performance. In all experiments, we set the per</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2295" citStr="Eidelman et al., 2012" startWordPosition="325" endWordPosition="328">d statistical machine translation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (B</context>
<context position="6895" citStr="Eidelman et al. (2012)" startWordPosition="1053" endWordPosition="1056">main.1 1Henceforth we will use the term “topic” and “domain” interchangeably: “topic” to refer to the concept in topic models and “domain” to refer to SMT corpora. 2.2 Inducing Domains with Topic Models Topic models take the number of topics K and a collection of documents as input, where each document is a bag of words. They output two distributions: a distribution over topics for each document d; and a distribution over words for each topic. If each topic defines a SMT domain, the document’s topic distribution is a soft domain assignment for that document. Given the soft domain assignments, Eidelman et al. (2012) extract lexical weighting features conditioned on the topics, optimizing feature weights using the Margin Infused Relaxed Algorithm (Crammer et al., 2006, MIRA). The topics come from source documents only and create topic-specific lexical weights from the per-document topic distribution p(k |d). The lexical probability conditioned on the topic is expected count ek(e, f) of a word translation pair under topic k, ˆck(e,f) _ Ed p(k|d)cd(e,f), (1) where cd(0) is the number of occurrences of the word pair in document d. The lexical probability conditioned on topic k is the unsmoothed probability e</context>
<context position="8808" citStr="Eidelman et al. (2012)" startWordPosition="1367" endWordPosition="1370">d SMT features and optimizing the weights. Conceptually, this approach is just reweighting examples. The probability of a topic given a document is never zero. Every translation observed in the training set will contribute to pk(e|f); many of the expected counts, however, will be less than one. This obviates the explicit smoothing used in other domain adaptation systems (Chiang et al., 2011). 1167 We adopt this framework in its entirety. Our contribution are topics that capture multilingual information and thus better capture the domains in the parallel corpus. 2.3 Beyond Vanilla Topic Models Eidelman et al. (2012) ignore a wealth of information that could improve topic models and help machine translation. Namely, they only use monolingual data from the source language, ignoring all target-language data and available lexical semantic resources between source and target languages. Different complement each other to reduce ambiguity. For example, “��” in a Chinese document can be either “hobbyhorse” in a children’s topic, or “Trojan virus” in a technology topic. A short Chinese context obscures the true topic. However, these terms are unambiguous in English, revealing the true topic. While vanilla topic m</context>
<context position="23077" citStr="Eidelman et al. (2012)" startWordPosition="3773" endWordPosition="3776">s, we use the recommended settings B = 5 and M = 5 from Mimno et al. (2012). 5 Experiments We evaluate our new topic model, ptLDA, and existing topic models—LDA, pLDA, and tLDA—on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla to</context>
<context position="25727" citStr="Eidelman et al. (2012)" startWordPosition="4200" endWordPosition="4203">MT results are averaged over five runs. We refer to the SMT model without domain adaptation as baseline.5 LDA marginally improves machine translation (less than half a BLEU point). 2The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4This is a two-level tree structure. However, one could build a more sophisticated tree prior with a hierarchical dictionary such as multilingual WordNet. 5Our replication of Eidelman et al. (2012) yields slightly higher baseline performance, but the trend is consistent. 1171 model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict BLEU Score 37 36 35 34 33 32 1 37 36 35 34 33 32 1 37 36 35 34 33 32 31 34.8 +0.3 +0.6 +0.4 35.1 +0.1 +0.3 +0.2 +0.7 +0.4 31.4 +0.4 +0.7 +0.4 +1 +0.4 gibbs model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict +1.2 +0.5 34.8 +0.4 +0.5 +0.4 +0.8 +0.5 35.1 −0.1 +0.2 −0.1 +0.2 +0.2 31.4 +0.3 +0.5 +0.3 +0.8 +0.4 variational 34.8 +0.2 +0.4 +0.2 +0.7 +0.4 35.1 −0.1 −0.1 −0.1 +0.2 +0.2 31.4 +0.3 +0.3 +0.1 +0.6 +0.3 variational−hybrid mt02 mt03 mt05 TER Score 66 6</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="5768" citStr="Foster and Kuhn, 2007" startWordPosition="868" endWordPosition="871">h the same genre (e.g., sports, business) from a similar style (e.g., newswire, blog-posts). These are called domains. Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example, “�7K” in a newspaper usually means “underwater diving”. On social media, it means a non-contributing “lurker”. Domain Adaptation for SMT Training a SMT system using diverse data requires domain adaptation. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al., 2009) to model domain information. Chiang et al. (2011) combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain. However, these approaches treat domains as hand-labeled, constant, and known a priori. This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain.1 1Henceforth we will use the term “topic” and “domain” interchangeably: “topic” to refer </context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for smt. In Proceedings of the Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michael Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden topic Markov models.</title>
<date>2007</date>
<booktitle>In Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="32992" citStr="Gruber et al., 2007" startWordPosition="5339" endWordPosition="5342">ions) increase when conditioned on ptLDA’s correct topic but decrease when conditioned on LDA’s incorrect topic. 6.2 Other Approaches Other approaches have used topic models for machine translation. Xiao et al. (2012) present a topic similarity model based on LDA that produces a feature that weights grammar rules based on topic compatibility. They also model the source and target side of rules and compare the target similarity during decoding by projecting the target distribution into the source space. Hasler et al. (2012) use the source-side topic assignments from hidden topic Markov models (Gruber et al., 2007, HTMM) which models documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. Su et al. (2012) also apply HTMM to monolingual data and apply the results to machine translation. To our knowledge, however, this is the first work to use multilingual topic models for domain adaptation in machine translation. 6.3 Improving Language Models Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Domain adaptation for langua</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007. Hidden topic Markov models. In Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Sparse lexicalised features and topic adaptation for SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="2316" citStr="Hasler et al., 2012" startWordPosition="329" endWordPosition="332">ranslation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik</context>
<context position="32901" citStr="Hasler et al. (2012)" startWordPosition="5325" endWordPosition="5328">“promised to” and translating “promised to” to “•%” (the correct translation, in both directions) increase when conditioned on ptLDA’s correct topic but decrease when conditioned on LDA’s incorrect topic. 6.2 Other Approaches Other approaches have used topic models for machine translation. Xiao et al. (2012) present a topic similarity model based on LDA that produces a feature that weights grammar rules based on topic compatibility. They also model the source and target side of rules and compare the target similarity during decoding by projecting the target distribution into the source space. Hasler et al. (2012) use the source-side topic assignments from hidden topic Markov models (Gruber et al., 2007, HTMM) which models documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. Su et al. (2012) also apply HTMM to monolingual data and apply the results to machine translation. To our knowledge, however, this is the first work to use multilingual topic models for domain adaptation in machine translation. 6.3 Improving Language Models Topic models capture document-level properties of language, but a critical component of machine translation systems is the lan</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2012</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse lexicalised features and topic adaptation for SMT. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Jordan Boyd-Graber</author>
</authors>
<title>Efficient tree-based topic modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18793" citStr="Hu and Boyd-Graber, 2012" startWordPosition="3033" endWordPosition="3036">se a collapsed Gibbs sampler for tree-based topic models to sample the path ydn and topic assignment zdn for word wdn, p(zdn = k, ydn = s|¬zdn, ¬ydn, w; α,β) ∝ I [Ω(s) = wdn] ·Nk|d+α r k0(Nk0|d+α) · Ni→j |k+βi→j Hi→j∈s rj0(Ni→j0|k+βi→j0) where Ω(s) represents the word that path s leads to, Nk|d is the number of tokens assigned to topic k in document d and Ni→j|k is the number of times edge i → j in the tree assigned to topic k, excluding the topic assignment zdn and its path ydn of current token wdn. In practice, we sample the latent variables using efficient sparse updates (Yao et al., 2009; Hu and Boyd-Graber, 2012). 4.2 Variational Bayesian Inference Variational Bayesian inference approximates the posterior distribution with a simplified variational distribution q over the latent variables: document topic proportions θ, transition probabilities π, topic assignments z, and path assignments y. Variational distributions typically assume a mean-field distribution over these latent variables, removing all dependencies between the latent variables. We follow this assumption for the transition probabilities q(π |λ) and the document topic proportions q(θ |γ); both are variational Dirichlet distributions. Howeve</context>
<context position="25473" citStr="Hu and Boyd-Graber (2012)" startWordPosition="4159" endWordPosition="4162">ing Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics—BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on three different test sets (Figure 2), and all SMT results are averaged over five runs. We refer to the SMT model without domain adaptation as baseline.5 LDA marginally improves machine translation (less than half a BLEU point). 2The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4This is a two-level tree structure. However, one could build a more sophisticated tree prior with a hierarchical dictionary such as multilingual WordNet. 5Our replication of Eidelman et al. (2012) yields slightly higher baseline performance, but the trend is consistent. 1171 model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict BLEU Score 37 36 35 34 33 32 1 37 36 35 34 33 32 1 37 36 35 34 33 32 31 34.8 +0.3 +0.6 +0.4 35.1 +0.1 +0.3 +0.2 +0.7 +0.4 31.4 +0.4 +0.7 +0.4 +1 +0.4 gibbs model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−d</context>
</contexts>
<marker>Hu, Boyd-Graber, 2012</marker>
<rawString>Yuening Hu and Jordan Boyd-Graber. 2012. Efficient tree-based topic modeling. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Jordan Boyd-Graber</author>
<author>Brianna Satinoff</author>
<author>Alison Smith</author>
</authors>
<title>Interactive topic modeling.</title>
<date>2013</date>
<journal>Machine Learning Journal.</journal>
<contexts>
<context position="11001" citStr="Hu et al., 2013" startWordPosition="1701" endWordPosition="1704">existing tree-based topic models (Boyd-Graber et al., 2007, tLDA) and polylingual topic models (Mimno et al., 2009, pLDA) together and create the polylingual treebased topic model (ptLDA) that incorporates both word-level correlations and document-level alignment information. Word-level Correlations Tree-based topic models incorporate the correlations between words by encouraging words that appear together in a concept to have similar probabilities given a topic. These concepts can come from WordNet (BoydGraber and Resnik, 2010), domain experts (Andrzejewski et al., 2009), or user constrains (Hu et al., 2013). When we gather concepts from bilingual resources, these concepts can connect different languages. For example, if a bilingual dictionary defines “U#p–q” as “computer”, we combine these words in a concept. We organize the vocabulary in a tree structure based on these concepts (Figure 1): words in the same concept share a common parent node, and then that concept becomes one of many children of the root node. Words that are not in any concept— uncorrelated words—are directly connected to the root node. We call this structure the tree prior. When this tree serves as a prior for topic models, wo</context>
</contexts>
<marker>Hu, Boyd-Graber, Satinoff, Smith, 2013</marker>
<rawString>Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2013. Interactive topic modeling. Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saurabh S Kataria</author>
<author>Krishnan S Kumar</author>
<author>Rajeev R Rastogi</author>
<author>Prithviraj Sen</author>
<author>Srinivasan H Sengamedu</author>
</authors>
<title>Entity disambiguation with hierarchical topic models.</title>
<date>2011</date>
<booktitle>In Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="1632" citStr="Kataria et al., 2011" startWordPosition="223" endWordPosition="226">(Blei et al., 2003, LDA), are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 2011). In particular, we use topic models to aid statistical machine translation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies </context>
</contexts>
<marker>Kataria, Kumar, Rastogi, Sen, Sengamedu, 2011</marker>
<rawString>Saurabh S. Kataria, Krishnan S. Kumar, Rajeev R. Rastogi, Prithviraj Sen, and Srinivasan H. Sengamedu. 2011. Entity disambiguation with hierarchical topic models. In Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4633" citStr="Koehn et al., 2003" startWordPosition="690" endWordPosition="693">�2014 Association for Computational Linguistics 2.1 Statistical Machine Translation Statistical machine translation casts machine translation as a probabilistic process (Koehn, 2009). For a parallel corpus of aligned source and target sentences (F, £), a phrase f E F is translated to a phrase e¯ E £ according to a distribution pw(¯e |f). One popular method to estimate the probability pw(¯e|¯f) is via lexical weighting features. Lexical Weighting In phrase-based SMT, lexical weighting features estimate the phrase pair quality by combining lexical translation probabilities of words in a phrase (Koehn et al., 2003). Lexical conditional probabilities p(e|f) are maximum likelihood estimates from relative lexical frequencies c(f, e)/Ee c(f, e), where c(f, e) is the count of observing lexical pair (f, e) in the training dataset. The phrase pair probabilities pw(¯e |f) are the normalized product of lexical probabilities of the aligned word pairs within that phrase pair (Koehn et al., 2003). In Section 2.2, we create topic-specific lexical weighting features. Cross-Domain SMT A SMT system is usually trained on documents with the same genre (e.g., sports, business) from a similar style (e.g., newswire, blog-po</context>
<context position="7717" citStr="Koehn et al. (2003)" startWordPosition="1189" endWordPosition="1192">ly and create topic-specific lexical weights from the per-document topic distribution p(k |d). The lexical probability conditioned on the topic is expected count ek(e, f) of a word translation pair under topic k, ˆck(e,f) _ Ed p(k|d)cd(e,f), (1) where cd(0) is the number of occurrences of the word pair in document d. The lexical probability conditioned on topic k is the unsmoothed probability estimate of those expected counts pw(e|f; k) _ &amp; ck(e,f) ˆck (e,f) (2) from which we can compute the phrase pair probabilities pw(¯e |¯f; k) by multiplying the lexical probabilities and normalizing as in Koehn et al. (2003). For a test document d, the document topic distribution p(k |d) is inferred based on the topics learned from training data. The feature value of a phrase pair (¯e, ¯f) is fk(¯e |¯f) _ −log {pw(¯e|¯f;k) - p(k|d)I, (3) a combination of the topic dependent lexical weight and the topic distribution of the document, from which we extract the phrase. Eidelman et al. (2012) compute the resulting model score by combining these features in a linear model with other standard SMT features and optimizing the weights. Conceptually, this approach is just reweighting examples. The probability of a topic giv</context>
<context position="23181" citStr="Koehn et al., 2003" startWordPosition="3790" endWordPosition="3793">new topic model, ptLDA, and existing topic models—LDA, pLDA, and tLDA—on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— Gibbs sampling (gibbs), variational i</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="27635" citStr="Koehn, 2004" startWordPosition="4531" endWordPosition="4532"> on three test sets is mostly significant with p = 0.01, except the results on MT03 using variational and variational-hybrid inferences. Polylingual topic models pLDA and tree-based topic models tLDA-dict are consistently better than LDA, suggesting that incorporating additional bilingual knowledge improves topic models. These improvements are not redundant: our new ptLDA-dict model, which has aspects of both models yields the best performance among these approaches—up to a 1.2 BLEU point gain (higher is better), and -2.6 TER improvement (lower is better). The BLEU improvement is significant (Koehn, 2004) at p = 0.01,6 except on MT03 with variational and variationalhybrid inference. While ptLDA-align performs better than baseline SMT and LDA, it is worse than ptLDA-dict, possibly because of errors in the word alignments, making the tree priors less effective. Scalability While gibbs has better translation scores than variational and variational-hybrid, it is less scalable to larger datasets. With 1.6M NIST 6Because we have multiple runs of each topic model (and thus different translation models), we select the run closest to the average BLEU for the translation significance test. training sent</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2009</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1720" citStr="Koehn, 2009" startWordPosition="239" endWordPosition="240">unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 2011). In particular, we use topic models to aid statistical machine translation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su</context>
<context position="4196" citStr="Koehn, 2009" startWordPosition="618" endWordPosition="619">slation. Finally, in Section 6, we show how these topic models improve SMT with detailed examples. 2 Topic Models for Machine Translation Before considering past approaches using topic models to improve SMT, we briefly review lexical weighting and domain adaptation for SMT. 1166 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166–1176, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Statistical Machine Translation Statistical machine translation casts machine translation as a probabilistic process (Koehn, 2009). For a parallel corpus of aligned source and target sentences (F, £), a phrase f E F is translated to a phrase e¯ E £ according to a distribution pw(¯e |f). One popular method to estimate the probability pw(¯e|¯f) is via lexical weighting features. Lexical Weighting In phrase-based SMT, lexical weighting features estimate the phrase pair quality by combining lexical translation probabilities of words in a phrase (Koehn et al., 2003). Lexical conditional probabilities p(e|f) are maximum likelihood estimates from relative lexical frequencies c(f, e)/Ee c(f, e), where c(f, e) is the count of obs</context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Philipp Koehn. 2009. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Fei-Fei</author>
<author>Pietro Perona</author>
</authors>
<title>A Bayesian hierarchical model for learning natural scene categories.</title>
<date>2005</date>
<booktitle>In Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="1537" citStr="Fei-Fei and Perona, 2005" startWordPosition="210" endWordPosition="213">n Probabilistic topic models (Blei and Lafferty, 2009), exemplified by latent Dirichlet allocation (Blei et al., 2003, LDA), are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 2011). In particular, we use topic models to aid statistical machine translation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for auto</context>
</contexts>
<marker>Fei-Fei, Perona, 2005</marker>
<rawString>Li Fei-Fei and Pietro Perona. 2005. A Bayesian hierarchical model for learning natural scene categories. In Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5813" citStr="Matsoukas et al., 2009" startWordPosition="876" endWordPosition="879">om a similar style (e.g., newswire, blog-posts). These are called domains. Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example, “�7K” in a newspaper usually means “underwater diving”. On social media, it means a non-contributing “lurker”. Domain Adaptation for SMT Training a SMT system using diverse data requires domain adaptation. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al., 2009) to model domain information. Chiang et al. (2011) combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain. However, these approaches treat domains as hand-labeled, constant, and known a priori. This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain.1 1Henceforth we will use the term “topic” and “domain” interchangeably: “topic” to refer to the concept in topic models and “domain” t</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://www.cs.umass.edu/ mccallum/mallet.</note>
<contexts>
<context position="25511" citStr="McCallum, 2002" startWordPosition="4167" endWordPosition="4168"> using topic models for domain adaptation on standard SMT evaluation metrics—BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on three different test sets (Figure 2), and all SMT results are averaged over five runs. We refer to the SMT model without domain adaptation as baseline.5 LDA marginally improves machine translation (less than half a BLEU point). 2The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4This is a two-level tree structure. However, one could build a more sophisticated tree prior with a hierarchical dictionary such as multilingual WordNet. 5Our replication of Eidelman et al. (2012) yields slightly higher baseline performance, but the trend is consistent. 1171 model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict BLEU Score 37 36 35 34 33 32 1 37 36 35 34 33 32 1 37 36 35 34 33 32 31 34.8 +0.3 +0.6 +0.4 35.1 +0.1 +0.3 +0.2 +0.7 +0.4 31.4 +0.4 +0.7 +0.4 +1 +0.4 gibbs model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict +1.2 +0.5 34.8 +0.4 +0.5 +0.4 +0.8</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://www.cs.umass.edu/ mccallum/mallet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Jason Naradowsky</author>
<author>David Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2878" citStr="Mimno et al., 2009" startWordPosition="422" endWordPosition="425">e-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review these models for discovering topics in multilingual datasets and discuss how they can improve SMT. However, no models combine multiple bridges between languages. In Section 3, we create a model—the polylingual tree-based topic models (ptLDA)—that uses information from both external dictionaries and document alignments simultaneously. In Section 4, we derive both MCMC and variational inference for this new topic model. In Section 5, we evaluate our model on the task of SMT using ali</context>
<context position="9603" citStr="Mimno et al. (2009)" startWordPosition="1493" endWordPosition="1496">et-language data and available lexical semantic resources between source and target languages. Different complement each other to reduce ambiguity. For example, “��” in a Chinese document can be either “hobbyhorse” in a children’s topic, or “Trojan virus” in a technology topic. A short Chinese context obscures the true topic. However, these terms are unambiguous in English, revealing the true topic. While vanilla topic models (LDA) can only be applied to monolingual data, there are a number of topic models for parallel corpora: Zhao and Xing (2006) assume aligned word pairs share same topics; Mimno et al. (2009) connect different languages through comparable documents. These models take advantage of word or document alignment information and infer more robust topics from the aligned dataset. On the other hand, lexical information can induce topics from multilingual corpora. For instance, orthographic similarity connects words with the same meaning in related languages (BoydGraber and Blei, 2009), and dictionaries are a more general source of information on which words share meaning (Boyd-Graber and Resnik, 2010). These two approaches are not mutually exclusive, however; they reveal different connecti</context>
<context position="12348" citStr="Mimno et al., 2009" startWordPosition="1922" endWordPosition="1925">ince they share the same parent node. With the tree priors, each topic is no longer a distribution over word types, instead, it is a distribution over paths, and each path is associated with a word type. The same word could appear in multiple paths, and each path represents a unique sense of this word. Document-level Alignments Lexical resources connect languages and help guide the topics. However, these resources are sometimes brittle and may not cover the whole vocabulary. Aligned document pairs provide a more corpus-specific, flexible association across languages. Polylingual topic models (Mimno et al., 2009) assume that the aligned documents in different languages share the same topic distribution and each language has a unique topic distribution over its word types. This level of connection between languages is flexible: instead of requiring the exact matching on words and sentences, only a coarse document alignment is necessary, as long as the documents discuss the same topics. Combine Words and Documents We propose polylingual tree-based topic models (ptLDA), which connect information across different languages by incorporating both word correlation (as in tLDA) and document alignment informat</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna Wallach, Jason Naradowsky, David Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Matthew Hoffman</author>
<author>David Blei</author>
</authors>
<title>Sparse stochastic inference for latent Dirichlet allocation.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="17690" citStr="Mimno et al. (2012)" startWordPosition="2839" endWordPosition="2842">(θd |α) · lld np(zdn |θd) · H H (p(ydn|zdn, π)p(wdn|ydn)). d n Exact inference is intractable, so we turn to apcomputer FRO market 市1, government 政府 science 科学 Prior Tree: 0 1 scientific policy 天气 computer, *,Ji market, 市.* government, 政府 science, 科学 1169 proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo (Neal, 2000, MCMC) and variational Bayesian inference (Blei et al., 2003, VB). Both frameworks produce good approximations of the posterior mode (Asuncion et al., 2009). In addition, Mimno et al. (2012) propose hybrid inference that takes advantage of parallelizable variational inference for global variables (Wolfe et al., 2008) while enjoying the sparse, efficient updates for local variables (Neal, 1993). In the rest of this section, we discuss all three methods in turn. We explore multiple inference schemes because while all of these methods optimize likelihood because they might give different results on the translation task. 4.1 Markov Chain Monte Carlo Inference We use a collapsed Gibbs sampler for tree-based topic models to sample the path ydn and topic assignment zdn for word wdn, p(z</context>
<context position="20963" citStr="Mimno et al. (2012)" startWordPosition="3409" endWordPosition="3412">ropy of a distribution. Optimizing L using coordinate descent provides the following updates: φdnkt ∝ exp{Ψ(γdk) − Ψ(Ek γdk) (7) + E (Ψ(λk,i→j) − Ψ(Ej0 λk,i→j0))}; i→j∈s γdk = αk + E Es∈Q−1(wdn) φdnkt; (8) n λk,i→j = βi→j (9) + E E Es∈Q0(wdn) φdnktI [i → j ∈ s] ; d n where Ω0(wdn) is the set of all paths that lead to word wdn in the tree, and t represents one particular path in this set. I [i → j ∈ s] is the indicator of whether path s contains an edge from node i to j. 4.3 Hybrid Stochastic Inference Given the complementary strengths of MCMC and VB, and following hybrid inference proposed by Mimno et al. (2012), we also derive hybrid inference for ptLDA. The transition distributions π are treated identically as in variational inference. We posit a variational Dirichlet distribution λ and choose the one that minimizes the KL divergence between the true posterior and the variational distribution. For topic z and path y, instead of variational updates, we use a Gibbs sampler within a document. We sample zdn and ydn conditioned on the topic 1170 and path assignments of all other document tokens, based on the variational expectation of π, q(zdn = k, ydn = s|¬zdn, ¬ydn; w) ∝ (10) (α + E m6=n I [zdm = k]) </context>
<context position="22530" citStr="Mimno et al. (2012)" startWordPosition="3689" endWordPosition="3692"> = I[Q(ydn)=wdn] · Ei→j∈ydn Eq[log Azdn,i→j]. For every document, we sweep over all its tokens and resample their topic zdn and path ydn conditioned on all the other tokens’ topic and path assignments ¬zdn and ¬ydn. To avoid bias, we discard the first B burn-in sweeps and take the following M samples. We then use the empirical average of these samples update the global variational parameter q(π|λ) based on how many times we sampled these paths Ak,i→j = M Ed En Es∈Q−1(wdn) (I [i j ∈ s] · I[zdn = k, ydn = s]) + Qi→j. (11) For our experiments, we use the recommended settings B = 5 and M = 5 from Mimno et al. (2012). 5 Experiments We evaluate our new topic model, ptLDA, and existing topic models—LDA, pLDA, and tLDA—on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och </context>
</contexts>
<marker>Mimno, Hoffman, Blei, 2012</marker>
<rawString>David Mimno, Matthew Hoffman, and David Blei. 2012. Sparse stochastic inference for latent Dirichlet allocation. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Probabilistic inference using Markov chain Monte Carlo methods.</title>
<date>1993</date>
<tech>Technical Report CRG-TR-93-1,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="17896" citStr="Neal, 1993" startWordPosition="2871" endWordPosition="2872">rket, 市.* government, 政府 science, 科学 1169 proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo (Neal, 2000, MCMC) and variational Bayesian inference (Blei et al., 2003, VB). Both frameworks produce good approximations of the posterior mode (Asuncion et al., 2009). In addition, Mimno et al. (2012) propose hybrid inference that takes advantage of parallelizable variational inference for global variables (Wolfe et al., 2008) while enjoying the sparse, efficient updates for local variables (Neal, 1993). In the rest of this section, we discuss all three methods in turn. We explore multiple inference schemes because while all of these methods optimize likelihood because they might give different results on the translation task. 4.1 Markov Chain Monte Carlo Inference We use a collapsed Gibbs sampler for tree-based topic models to sample the path ydn and topic assignment zdn for word wdn, p(zdn = k, ydn = s|¬zdn, ¬ydn, w; α,β) ∝ I [Ω(s) = wdn] ·Nk|d+α r k0(Nk0|d+α) · Ni→j |k+βi→j Hi→j∈s rj0(Ni→j0|k+βi→j0) where Ω(s) represents the word that path s leads to, Nk|d is the number of tokens assigned</context>
</contexts>
<marker>Neal, 1993</marker>
<rawString>Radford M. Neal. 1993. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR-93-1, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Markov chain sampling methods for Dirichlet process mixture models.</title>
<date>2000</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>9</volume>
<issue>2</issue>
<pages>265</pages>
<contexts>
<context position="17499" citStr="Neal, 2000" startWordPosition="2812" endWordPosition="2813">assignments zdn and path ydn for the nth word wdn in document d. The joint distribution of polylingual tree-based topic models is p(w, z7, -y, θ, π; α, β)- lIHk Hi p(πki |βi) (4) Hd (θd |α) · lld np(zdn |θd) · H H (p(ydn|zdn, π)p(wdn|ydn)). d n Exact inference is intractable, so we turn to apcomputer FRO market 市1, government 政府 science 科学 Prior Tree: 0 1 scientific policy 天气 computer, *,Ji market, 市.* government, 政府 science, 科学 1169 proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo (Neal, 2000, MCMC) and variational Bayesian inference (Blei et al., 2003, VB). Both frameworks produce good approximations of the posterior mode (Asuncion et al., 2009). In addition, Mimno et al. (2012) propose hybrid inference that takes advantage of parallelizable variational inference for global variables (Wolfe et al., 2008) while enjoying the sparse, efficient updates for local variables (Neal, 1993). In the rest of this section, we discuss all three methods in turn. We explore multiple inference schemes because while all of these methods optimize likelihood because they might give different results</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Radford M. Neal. 2000. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2):249– 265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>29</volume>
<issue>21</issue>
<pages>pages</pages>
<contexts>
<context position="23144" citStr="Och and Ney, 2003" startWordPosition="3784" endWordPosition="3787">012). 5 Experiments We evaluate our new topic model, ptLDA, and existing topic models—LDA, pLDA, and tLDA—on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— </context>
<context position="24457" citStr="Och and Ney, 2003" startWordPosition="3992" endWordPosition="3995">rid)—on the effects of SMT performance. In all experiments, we set the per-document Dirichlet parameter α = 0.01 and the number of topics to 10, as used in Eidelman et al. (2012). Resources for Prior Tree To build the tree for tLDA and ptLDA, we extract the word correlations from a Chinese-English bilingual dictionary (Denisowski, 1997).4 We filter the dictionary using the NIST vocabulary, and keep entries mapping single Chinese and single English words. The prior tree has about 1000 word pairs (dict). We also extract the bidirectional word alignments between Chinese and English using GIZA++ (Och and Ney, 2003). We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align). We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDAalign. However, tLDA-align and ptLDA-align do worse than tLDA-dict and ptLDA-dict, so we omit tLDA-align in the results. Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics—BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 29(21), pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25001" citStr="Papineni et al., 2002" startWordPosition="4079" endWordPosition="4082">l word alignments between Chinese and English using GIZA++ (Och and Ney, 2003). We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align). We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDAalign. However, tLDA-align and ptLDA-align do worse than tLDA-dict and ptLDA-dict, so we omit tLDA-align in the results. Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics—BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on three different test sets (Figure 2), and all SMT results are averaged over five runs. We refer to the SMT model without domain adaptation as baseline.5 LDA marginally improves machine translation (less than half a BLEU point). 2The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4This is a two-level tree structure. However, one could build a more so</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Perina</author>
<author>Pietro Lovato</author>
<author>Vittorio Murino</author>
<author>Manuele Bicego</author>
</authors>
<title>Biologically-aware latent Dirichlet allocation (balda) for the classification of expression microarray.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th IAPR international conference on Pattern recognition in bioinformatics,</booktitle>
<pages>10</pages>
<contexts>
<context position="1582" citStr="Perina et al., 2010" startWordPosition="216" endWordPosition="219">009), exemplified by latent Dirichlet allocation (Blei et al., 2003, LDA), are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 2011). In particular, we use topic models to aid statistical machine translation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine tran</context>
</contexts>
<marker>Perina, Lovato, Murino, Bicego, 2010</marker>
<rawString>Alessandro Perina, Pietro Lovato, Vittorio Murino, and Manuele Bicego. 2010. Biologically-aware latent Dirichlet allocation (balda) for the classification of expression microarray. In Proceedings of the 5th IAPR international conference on Pattern recognition in bioinformatics, PRIB’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="25031" citStr="Snover et al., 2006" startWordPosition="4085" endWordPosition="4088">e and English using GIZA++ (Och and Ney, 2003). We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align). We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDAalign. However, tLDA-align and ptLDA-align do worse than tLDA-dict and ptLDA-dict, so we omit tLDA-align in the results. Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics—BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on three different test sets (Figure 2), and all SMT results are averaged over five runs. We refer to the SMT model without domain adaptation as baseline.5 LDA marginally improves machine translation (less than half a BLEU point). 2The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4This is a two-level tree structure. However, one could build a more sophisticated tree prior with a </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinsong Su</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Yidong Chen</author>
<author>Xiaodong Shi</author>
<author>Huailin Dong</author>
<author>Qun Liu</author>
</authors>
<title>Translation model adaptation for statistical machine translation with monolingual topic information.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2334" citStr="Su et al., 2012" startWordPosition="333" endWordPosition="336">09, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word </context>
<context position="33133" citStr="Su et al. (2012)" startWordPosition="5365" endWordPosition="5368">oaches have used topic models for machine translation. Xiao et al. (2012) present a topic similarity model based on LDA that produces a feature that weights grammar rules based on topic compatibility. They also model the source and target side of rules and compare the target similarity during decoding by projecting the target distribution into the source space. Hasler et al. (2012) use the source-side topic assignments from hidden topic Markov models (Gruber et al., 2007, HTMM) which models documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. Su et al. (2012) also apply HTMM to monolingual data and apply the results to machine translation. To our knowledge, however, this is the first work to use multilingual topic models for domain adaptation in machine translation. 6.3 Improving Language Models Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Domain adaptation for language models (Bellegarda, 2004; Wood and Teh, 2009) is an important avenue for improving machine translation. Models that simultaneously discove</context>
</contexts>
<marker>Su, Wu, Wang, Chen, Shi, Dong, Liu, 2012</marker>
<rawString>Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, and Qun Liu. 2012. Translation model adaptation for statistical machine translation with monolingual topic information. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="36565" citStr="Teh et al., 2006" startWordPosition="5867" endWordPosition="5870">is. tion (Wallach, 2006; Boyd-Graber and Blei, 2008) may offer further improvements. 6.4 External Data The topic models presented here only require weak alignment between documents at the document level. Extending to larger datasets for learning topics is straightforward in principle. For example, ptLDA could learn domains from a much larger corpus like Wikipedia and then apply the extracted domains to machine translation data. However, this presents further challenges, as Wikipedia’s domains are not representative of newswire machine translation datasets; a flexible hierarchical topic model (Teh et al., 2006) would better distinguish useful domains from extraneous ones. 7 Conclusion Topic models generate great interest, but their use in “real world” applications still lags; this is particularly true for multilingual topic models. As topic models become more integrated in commonplace applications, their adoption, understanding, and robustness will improve. This paper contributes to the deeper integration of topic models into critical applications by presenting a new multilingual topic model, ptLDA, comparing it with other multilingual topic models on a machine translation task, and showing that the</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="23117" citStr="Tseng et al., 2005" startWordPosition="3779" endWordPosition="3782">d M = 5 from Mimno et al. (2012). 5 Experiments We evaluate our new topic model, ptLDA, and existing topic models—LDA, pLDA, and tLDA—on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine diffe</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: Beyond bag-of-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="35971" citStr="Wallach, 2006" startWordPosition="5782" endWordPosition="5783">ce), 共同(together), 建A(build), 世界(world) world, defense, south, 安 全(security), war, t1(agreement), 会*(conference) Figure 4: Better SMT result using ptLDA compared to LDA and the baseline. Top row: the source sentence and a reference translation. Second row: the highlighted translations from different models. Third row: the change of relevant translation probabilities after incorporating domain knowledge from LDA and ptLDA. Bottom row: most-probable words for the topics the source sentence is assigned to under LDA (left) and ptLDA (right). The meanings of Chinese words are in parenthesis. tion (Wallach, 2006; Boyd-Graber and Blei, 2008) may offer further improvements. 6.4 External Data The topic models presented here only require weak alignment between documents at the document level. Extending to larger datasets for learning topics is straightforward in principle. For example, ptLDA could learn domains from a much larger corpus like Wikipedia and then apply the extracted domains to machine translation data. However, this presents further challenges, as Wikipedia’s domains are not representative of newswire machine translation datasets; a flexible hierarchical topic model (Teh et al., 2006) would</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling: Beyond bag-of-words. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Wolfe</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Fully distributed EM for very large datasets.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference of Machine Learning,</booktitle>
<pages>1184--1191</pages>
<contexts>
<context position="17818" citStr="Wolfe et al., 2008" startWordPosition="2857" endWordPosition="2860">et 市1, government 政府 science 科学 Prior Tree: 0 1 scientific policy 天气 computer, *,Ji market, 市.* government, 政府 science, 科学 1169 proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo (Neal, 2000, MCMC) and variational Bayesian inference (Blei et al., 2003, VB). Both frameworks produce good approximations of the posterior mode (Asuncion et al., 2009). In addition, Mimno et al. (2012) propose hybrid inference that takes advantage of parallelizable variational inference for global variables (Wolfe et al., 2008) while enjoying the sparse, efficient updates for local variables (Neal, 1993). In the rest of this section, we discuss all three methods in turn. We explore multiple inference schemes because while all of these methods optimize likelihood because they might give different results on the translation task. 4.1 Markov Chain Monte Carlo Inference We use a collapsed Gibbs sampler for tree-based topic models to sample the path ydn and topic assignment zdn for word wdn, p(zdn = k, ydn = s|¬zdn, ¬ydn, w; α,β) ∝ I [Ω(s) = wdn] ·Nk|d+α r k0(Nk0|d+α) · Ni→j |k+βi→j Hi→j∈s rj0(Ni→j0|k+βi→j0) where Ω(s) r</context>
<context position="28445" citStr="Wolfe et al., 2008" startWordPosition="4652" endWordPosition="4655">rors in the word alignments, making the tree priors less effective. Scalability While gibbs has better translation scores than variational and variational-hybrid, it is less scalable to larger datasets. With 1.6M NIST 6Because we have multiple runs of each topic model (and thus different translation models), we select the run closest to the average BLEU for the translation significance test. training sentences, gibbs takes nearly a week to run 1000 iterations. In contrast, the parallelized variational and variational-hybrid approaches, which we implement in MapReduce (Dean and Ghemawat, 2004; Wolfe et al., 2008; Zhai et al., 2012), take less than a day to converge. 6 Discussion In this section, we qualitatively analyze the translation results and investigate how ptLDA and its cousins improve SMT. We also discuss other approaches to improve unsupervised domain adaptation for SMT. 6.1 How do Topic Models Help SMT? We present two examples of how topic models can improve SMT. The first example shows both LDA and ptLDA improve the baseline. The second example shows how LDA introduce biases that mislead SMT and how ptLDA’s bilingual constraints correct these mistakes. Figure 3 shows a sentence about a com</context>
</contexts>
<marker>Wolfe, Haghighi, Klein, 2008</marker>
<rawString>Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully distributed EM for very large datasets. In Proceedings of the International Conference of Machine Learning, pages 1184–1191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wood</author>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence and Statistics,</booktitle>
<volume>12</volume>
<contexts>
<context position="33640" citStr="Wood and Teh, 2009" startWordPosition="5440" endWordPosition="5443">s as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. Su et al. (2012) also apply HTMM to monolingual data and apply the results to machine translation. To our knowledge, however, this is the first work to use multilingual topic models for domain adaptation in machine translation. 6.3 Improving Language Models Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Domain adaptation for language models (Bellegarda, 2004; Wood and Teh, 2009) is an important avenue for improving machine translation. Models that simultaneously discover global document themes as well as local, contextual domain-specific informa1173 source 消息指出, �国使kft人j%向中方官j%表示, �国方面并没有支持朝�人以��方法前往�国, �国并不希望�类事件再次 �生, 以免�中国和朝�半�双方�的�系�来影响, �国方面并向中国方面承�, 愿意�助中国管理好在京的�国居民 reference sources said rok embassy personnel told chinese officials that rok has not backed any dpr koreans to get to rok in such a manner and rok would not like such things happen again to affect relationship between china and the two sides of the korean peninsula . rok also promised to assist chin</context>
</contexts>
<marker>Wood, Teh, 2009</marker>
<rawString>Frank Wood and Yee Whye Teh. 2009. A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A topic similarity model for hierarchical phrase-based translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2394" citStr="Xiao et al., 2012" startWordPosition="344" endWordPosition="347">of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review th</context>
<context position="32590" citStr="Xiao et al. (2012)" startWordPosition="5272" endWordPosition="5275"> foreign affairs and produces a softer, more nuanced translation that better matches the reference. The translation of “•%” is very similar, except in this case, both the baseline and LDA produce the incorrect translation “the commitment of”. This is possible because the probabilities of translating “•%” to “promised to” and translating “promised to” to “•%” (the correct translation, in both directions) increase when conditioned on ptLDA’s correct topic but decrease when conditioned on LDA’s incorrect topic. 6.2 Other Approaches Other approaches have used topic models for machine translation. Xiao et al. (2012) present a topic similarity model based on LDA that produces a feature that weights grammar rules based on topic compatibility. They also model the source and target side of rules and compare the target similarity during decoding by projecting the target distribution into the source space. Hasler et al. (2012) use the source-side topic assignments from hidden topic Markov models (Gruber et al., 2007, HTMM) which models documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. Su et al. (2012) also apply HTMM to monolingual data and apply the result</context>
</contexts>
<marker>Xiao, Xiong, Zhang, Liu, Lin, 2012</marker>
<rawString>Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and Shouxun Lin. 2012. A topic similarity model for hierarchical phrase-based translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<booktitle>In Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="18766" citStr="Yao et al., 2009" startWordPosition="3029" endWordPosition="3032">rlo Inference We use a collapsed Gibbs sampler for tree-based topic models to sample the path ydn and topic assignment zdn for word wdn, p(zdn = k, ydn = s|¬zdn, ¬ydn, w; α,β) ∝ I [Ω(s) = wdn] ·Nk|d+α r k0(Nk0|d+α) · Ni→j |k+βi→j Hi→j∈s rj0(Ni→j0|k+βi→j0) where Ω(s) represents the word that path s leads to, Nk|d is the number of tokens assigned to topic k in document d and Ni→j|k is the number of times edge i → j in the tree assigned to topic k, excluding the topic assignment zdn and its path ydn of current token wdn. In practice, we sample the latent variables using efficient sparse updates (Yao et al., 2009; Hu and Boyd-Graber, 2012). 4.2 Variational Bayesian Inference Variational Bayesian inference approximates the posterior distribution with a simplified variational distribution q over the latent variables: document topic proportions θ, transition probabilities π, topic assignments z, and path assignments y. Variational distributions typically assume a mean-field distribution over these latent variables, removing all dependencies between the latent variables. We follow this assumption for the transition probabilities q(π |λ) and the document topic proportions q(θ |γ); both are variational Diri</context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections. In Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ke Zhai</author>
<author>Jordan Boyd-Graber</author>
<author>Nima Asadi</author>
<author>Mohamad Alkhouja</author>
</authors>
<title>Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce.</title>
<date>2012</date>
<booktitle>In Proceedings of World Wide Web Conference.</booktitle>
<contexts>
<context position="28465" citStr="Zhai et al., 2012" startWordPosition="4656" endWordPosition="4659">gnments, making the tree priors less effective. Scalability While gibbs has better translation scores than variational and variational-hybrid, it is less scalable to larger datasets. With 1.6M NIST 6Because we have multiple runs of each topic model (and thus different translation models), we select the run closest to the average BLEU for the translation significance test. training sentences, gibbs takes nearly a week to run 1000 iterations. In contrast, the parallelized variational and variational-hybrid approaches, which we implement in MapReduce (Dean and Ghemawat, 2004; Wolfe et al., 2008; Zhai et al., 2012), take less than a day to converge. 6 Discussion In this section, we qualitatively analyze the translation results and investigate how ptLDA and its cousins improve SMT. We also discuss other approaches to improve unsupervised domain adaptation for SMT. 6.1 How do Topic Models Help SMT? We present two examples of how topic models can improve SMT. The first example shows both LDA and ptLDA improve the baseline. The second example shows how LDA introduce biases that mislead SMT and how ptLDA’s bilingual constraints correct these mistakes. Figure 3 shows a sentence about a company 1172 source hoz</context>
</contexts>
<marker>Zhai, Boyd-Graber, Asadi, Alkhouja, 2012</marker>
<rawString>Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mohamad Alkhouja. 2012. Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce. In Proceedings of World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2966" citStr="Zhao and Xing, 2006" startWordPosition="434" endWordPosition="437"> modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review these models for discovering topics in multilingual datasets and discuss how they can improve SMT. However, no models combine multiple bridges between languages. In Section 3, we create a model—the polylingual tree-based topic models (ptLDA)—that uses information from both external dictionaries and document alignments simultaneously. In Section 4, we derive both MCMC and variational inference for this new topic model. In Section 5, we evaluate our model on the task of SMT using aligned datasets. We show that ptLDA offers better domain adaptation than other topic model</context>
<context position="9538" citStr="Zhao and Xing (2006)" startWordPosition="1482" endWordPosition="1485">y use monolingual data from the source language, ignoring all target-language data and available lexical semantic resources between source and target languages. Different complement each other to reduce ambiguity. For example, “��” in a Chinese document can be either “hobbyhorse” in a children’s topic, or “Trojan virus” in a technology topic. A short Chinese context obscures the true topic. However, these terms are unambiguous in English, revealing the true topic. While vanilla topic models (LDA) can only be applied to monolingual data, there are a number of topic models for parallel corpora: Zhao and Xing (2006) assume aligned word pairs share same topics; Mimno et al. (2009) connect different languages through comparable documents. These models take advantage of word or document alignment information and infer more robust topics from the aligned dataset. On the other hand, lexical information can induce topics from multilingual corpora. For instance, orthographic similarity connects words with the same meaning in related languages (BoydGraber and Blei, 2009), and dictionaries are a more general source of information on which words share meaning (Boyd-Graber and Resnik, 2010). These two approaches ar</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>