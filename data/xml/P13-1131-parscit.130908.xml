<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000085">
<title confidence="0.974853">
A Two Level Model for Context Sensitive Inference Rules
</title>
<author confidence="0.848741">
Oren Melamud§, Jonathan Berant†, Ido Dagan§, Jacob Goldberger♦, Idan Szpektor‡
</author>
<note confidence="0.6860224">
§ Computer Science Department, Bar-Ilan University
t Computer Science Department, Stanford University
Q Faculty of Engineering, Bar-Ilan University
t Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
</note>
<email confidence="0.776824">
joberant@stanford.edu
idan@yahoo-inc.com
</email>
<sectionHeader confidence="0.98441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885529411765">
Automatic acquisition of inference rules
for predicates has been commonly ad-
dressed by computing distributional simi-
larity between vectors of argument words,
operating at the word space level. A re-
cent line of work, which addresses context
sensitivity of rules, represented contexts in
a latent topic space and computed similar-
ity over topic vectors. We propose a novel
two-level model, which computes simi-
larities between word-level vectors that
are biased by topic-level context repre-
sentations. Evaluations on a naturally-
distributed dataset show that our model
significantly outperforms prior word-level
and topic-level models. We also release a
first context-sensitive inference rule set.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998490196721312">
Inference rules for predicates have been identi-
fied as an important component in semantic ap-
plications, such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ‘X treat Y -+ X relieve
Y’ can be useful to extract pairs of drugs and the
illnesses which they relieve, or to answer a ques-
tion like “Which drugs relieve headache?”. Along
this vein, such inference rules constitute a crucial
component in generic modeling of textual infer-
ence, under the Textual Entailment paradigm (Da-
gan et al., 2006; Dinu and Wang, 2009).
Motivated by these needs, substantial research
was devoted to automatic learning of inference
rules from corpora, mostly in an unsupervised dis-
tributional setting. This research line was mainly
initiated by the highly-cited DIRT algorithm (Lin
and Pantel, 2001), which learns inference for bi-
nary predicates with two argument slots (like the
rule in the example above). DIRT represents a
predicate by two vectors, one for each of the ar-
gument slots, where the vector entries correspond
to the argument words that occurred with the pred-
icate in the corpus. Inference rules between pairs
of predicates are then identified by measuring the
similarity between their corresponding argument
vectors. This general scheme was further en-
hanced in several directions, e.g. directional sim-
ilarity (Bhagat et al., 2007; Szpektor and Dagan,
2008) and meta-classification over similarity val-
ues (Berant et al., 2011). Consequently, several
knowledge resources of inference rules were re-
leased, containing the top scoring rules for each
predicate (Schoenmackers et al., 2010; Berant et
al., 2011; Nakashole et al., 2012).
The above mentioned methods provide a sin-
gle confidence score for each rule, which is based
on the obtained degree of argument-vector sim-
ilarities. Thus, a system that applies an infer-
ence rule to a text may estimate the validity of
the rule application based on the pre-specified rule
score. However, the validity of an inference rule
may depend on the context in which it is applied,
such as the context specified by the given predi-
cate’s arguments. For example, ‘AT&amp;T acquire T-
Mobile -+ AT&amp;T purchase T-Mobile’, is a valid
application of the rule ‘X acquire Y -+ X pur-
chase Y’, while ‘Children acquire skills -+ Chil-
dren purchase skills’ is not. To address this issue, a
line of works emerged which computes a context-
sensitive reliability score for each rule application,
based on the given context.
The major trend in context-sensitive inference
models utilizes latent or class-based methods for
context modeling (Pantel et al., 2007; Szpektor et
al., 2008; Ritter et al., 2010; Dinu and Lapata,
2010b). In particular, the more recent methods
(Ritter et al., 2010; Dinu and Lapata, 2010b) mod-
eled predicates in context as a probability distribu-
tion over topics learned by a Latent Dirichlet Allo-
</bodyText>
<page confidence="0.934469">
1331
</page>
<note confidence="0.913217">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940644444444">
cation (LDA) model. Then, similarity is measured
between the two topic distribution vectors corre-
sponding to the two sides of the rule in the given
context, yielding a context-sensitive score for each
particular rule application.
We notice at this point that while context-
insensitive methods represent predicates by ar-
gument vectors in the original fine-grained word
space, context-sensitive methods represent them
as vectors at the level of latent topics. This raises
the question of whether such coarse-grained topic
vectors might be less informative in determining
the semantic similarity between the two predi-
cates.
To address this hypothesized caveat of prior
context-sensitive rule scoring methods, we pro-
pose a novel generic scheme that integrates word-
level and topic-level representations. Our scheme
can be applied on top of any context-insensitive
“base” similarity measure for rule learning, which
operates at the word level, such as Cosine or
Lin (Lin, 1998). Rather than computing a single
context-insensitive rule score, we compute a dis-
tinct word-level similarity score for each topic in
an LDA model. Then, when applying a rule in a
given context, these different scores are weighed
together based on the specific topic distribution
under the given context. This way, we calculate
similarity over vectors in the original word space,
while biasing them towards the given context via
a topic model.
In order to promote replicability and equal-term
comparison with our results, we based our experi-
ments on publicly available datasets, both for un-
supervised learning of the evaluated models and
for testing them over a random sample of rule ap-
plications. We apply our two-level scheme over
three state-of-the-art context-insensitive similar-
ity measures. The evaluation compares perfor-
mances both with the original context-insensitive
measures and with recent LDA-based context-
sensitive methods, showing consistent and robust
advantages of our scheme. Finally, we release
a context-sensitive rule resource comprising over
2,000 frequent verbs and one million rules.
</bodyText>
<sectionHeader confidence="0.945406" genericHeader="introduction">
2 Background and Model Setting
</sectionHeader>
<bodyText confidence="0.999808555555556">
This section presents components of prior work
which are included in our model and experiments,
setting the technical preliminaries for the rest of
the paper. We first present context-insensitive rule
learning, based on distributional similarity at the
word level, and then context-sensitive scoring for
rule applications, based on topic-level similarity.
Some further discussion of related work appears
in Section 6.
</bodyText>
<subsectionHeader confidence="0.99751">
2.1 Context-insensitive Rule Learning
</subsectionHeader>
<bodyText confidence="0.983458891891892">
A predicate inference rule ‘LHS → RHS’, such
as ‘X acquire Y → X purchase Y’, specifies a
directional inference relation between two predi-
cates. Each rule side consists of a lexical pred-
icate and (two) variable slots for its arguments.1
Different representations have been used to spec-
ify predicates and their argument slots, such as
word lemma sequences, regular expressions and
dependency parse fragments. A rule can be ap-
plied when its LHS matches a predicate with a
pair of arguments in a text, allowing us to infer its
RHS, with the corresponding instantiations for the
argument variables. For example, given the text
“AT&amp;T acquires T-Mobile”, the above rule infers
“AT&amp;T purchases T-Mobile”.
The DIRT algorithm (Lin and Pantel, 2001)
follows the distributional similarity paradigm to
learn predicate inference rules. For each predi-
cate, DIRT represents each of its argument slots
by an argument vector. We denote the two vectors
of the X and Y slots of a predicate pred by vx
pred
and vypred, respectively. Each entry of a vector v
corresponds to a particular word (or term) w that
instantiated the argument slot in a learning corpus,
with a value v(w) = PMI(pred,w) (with PMI
standing for point-wise mutual information).
To learn inference rules, DIRT considers (in
principle) each pair of binary predicates that
occurred in the corpus for a candidate rule,
‘LHS → RHS’. Then, DIRT computes a reliabil-
ity score for the rule by combining the measured
similarities between the corresponding argument
vectors of the two rule sides. Concretely, denot-
ing by l and r the predicates appearing in the two
rule sides, DIRT’s reliability score is defined as
follows:
</bodyText>
<equation confidence="0.94452">
scoreDIRT(LHS → RHS)
�= sim(vi , vxr) · sim(vyl , vyr)
</equation>
<bodyText confidence="0.9996305">
where sim(v, v&apos;) is a vector similarity measure.
Specifically, DIRT employs the Lin similarity
</bodyText>
<footnote confidence="0.989678">
1We follow most of the inference-rule learning literature,
which focused on binary predicates. However, our context-
sensitive scheme can be applied to any arity.
</footnote>
<equation confidence="0.59227">
(1)
</equation>
<page confidence="0.802766">
1332
</page>
<bodyText confidence="0.799828">
measure from (Lin, 1998), defined as follows:
</bodyText>
<equation confidence="0.9997435">
Lin (v, v&apos;) = EwEvnv&apos; [v (w) + v&apos; (w)] (2)
EwEvUv&apos; [v(w) + v&apos;(w)]
</equation>
<bodyText confidence="0.999965677419355">
We note that the general DIRT scheme may be
used while employing other “base” vector similar-
ity measures. For example, the Lin measure is
symmetric, and thus using it would yield the same
reliability score when swapping the two sides of
a rule. This issue has been addressed in a sepa-
rate line of research which introduced directional
similarity measures suitable for inference rela-
tions (Bhagat et al., 2007; Szpektor and Dagan,
2008; Kotlerman et al., 2010). In our experiments
we apply our proposed context-sensitive similarity
scheme over three different base similarity mea-
sures.
DIRT and similar context-insensitive inference
methods provide a single reliability score for a
learned inference rule, which aims to predict the
validity of the rule’s applications. However, as
exemplified in the Introduction, an inference rule
may be valid in some contexts but invalid in oth-
ers (e.g. acquiring entails purchasing for goods,
but not for skills). Since vector similarity in DIRT
is computed over the single aggregate argument
vector, the obtained reliability score tends to be
biased towards the dominant contexts of the in-
volved predicates. For example, we may expect
a higher score for ‘acquire —� purchase’ than for
‘acquire —� learn’, since the former matches a
more frequent sense of acquire in a typical corpus.
Following this observation, it is desired to obtain
a context-sensitive reliability score for each rule
application in a given context, as described next.
</bodyText>
<subsectionHeader confidence="0.999487">
2.2 Context-sensitive Rule Applications
</subsectionHeader>
<bodyText confidence="0.99999175">
To assess the reliability of applying an inference
rule in a given context we need some model for
context representation, that should affect the rule
reliability score. A major trend in past work is
to represent contexts in a reduced-dimensionality
latent or class-based model. A couple of earlier
works utilized a cluster-based model (Pantel et al.,
2007) and an LSA-based model (Szpektor et al.,
2008), in a selectional-preferences style approach.
Several more recent works utilize a Latent Dirich-
let Allocation (LDA) (Blei et al., 2003) frame-
work. We now present an underlying unified view
of the topic-level models in (Ritter et al., 2010;
Dinu and Lapata, 2010b), which we follow in our
own model and in comparative model evaluations.
We note that a similar LDA model construction
was employed also in (S´eaghdha, 2010), for esti-
mating predicate-argument likelihood.
First, an LDA model is constructed, as follows.
Similar to the construction of argument vectors
in the distributional model (described above in
subsection 2.1), all arguments instantiating each
predicate slot are extracted from a large learning
corpus. Then, for each slot of each predicate, a
pseudo-document is constructed containing the set
of all argument words that instantiated this slot in
the corpus. We denote the two documents con-
structed for the X and Y slots of a predicate pred
by dxpred and dy pred, respectively. In comparison to
the distributional model, these two documents cor-
respond to the analogous argument vectors vxpred
and vypred, both containing exactly the same set of
words.
Next, an LDA model is learned from the set
of all pseudo-documents, extracted for all predi-
cates.2 The learning process results in the con-
struction of K latent topics, where each topic t
specifies a distribution over all words, denoted by
p(w|t), and a topic distribution for each pseudo-
document d, denoted by p(t|d).
Within the LDA model we can derive the
a-posteriori topic distribution conditioned on a
particular word within a document, denoted by
p(t|d, w) a p(w|t) · p(t|d). In the topic-level
model, d corresponds to a predicate slot and w to
a particular argument word instantiating this slot.
Hence, p(t|d, w) is viewed as specifying the rele-
vance (or likelihood) of the topic t for the predi-
cate slot in the context of the given argument in-
stantiation. For example, for the predicate slot ‘ac-
quire Y’ in the context of the argument ‘IBM’, we
expect high relevance for a topic about companies,
while in the context of the argument ‘knowledge’
we expect high relevance for a topic about abstract
concepts. Accordingly, the distribution p(t|d, w)
over all topics provides a topic-level representa-
tion for a predicate slot in the context of a particu-
lar argument w. This representation is used by the
topic-level model to compute a context-sensitive
score for inference rule applications, as follows.
</bodyText>
<footnote confidence="0.543360333333333">
2We note that there are variants in the type of LDA model
and the way the pseudo-documents are constructed in the
referenced prior work. In order to focus on the inference
methods rather than on the underlying LDA model, we use
the LDA framework described in this paper for all compared
methods.
</footnote>
<page confidence="0.957295">
1333
</page>
<bodyText confidence="0.898448333333333">
Consider the application of an inference rule
‘LHS -+ RHS’ in the context of a particular pair
of arguments for the X and Y slots, denoted by
wx and wy, respectively. Denoting by l and r the
predicates appearing in the two rule sides, the reli-
ability score of the topic-level model is defined as
follows (we present a geometric mean formulation
for consistency with DIRT):
scoreTopic(LHS -+ RHS, wx, wy) (3)
_
�sim(dxl , dxr, wx) · sim(di , dyr, wy)
where sim(d, d&apos;, w) is a topic-distribution similar-
ity measure conditioned on a given context word.
Specifically, Ritter et al. (2010) utilized the dot
product form for their similarity measure:
</bodyText>
<equation confidence="0.521952">
simDC(d, d&apos;, w) _ Et[p(t|d, w) · p(t|d&apos;, w)] (4)
</equation>
<bodyText confidence="0.990911885714286">
(the subscript DC stands for double-conditioning,
as both distributions are conditioned on the argu-
ment word, unlike the measure below).
Dinu and Lapata (2010b) presented a slightly
different similarity measure for topic distributions
that performed better in their setting as well as in a
related later paper on context-sensitive scoring of
lexical similarity (Dinu and Lapata, 2010a). In this
measure, the topic distribution for the right hand
side of the rule is not conditioned on w:
simSC(d, d&apos;, w) _ Et[p(t|d, w) · p(t|d&apos;)] (5)
(the subscript SC stands for single-conditioning,
as only the left distribution is conditioned on the
argument word). They also experimented with a
few variants for the structure of the similarity mea-
sure and assessed that best results are obtained
with the dot product form. In our experiments,
we employ these two similarity measures for topic
distributions as baselines representing topic-level
models.
Comparing the context-insensitive and context-
sensitive models, we see that both of them mea-
sure similarity between vector representations of
corresponding predicate slots. However, while
DIRT computes sim(v, v&apos;) over vectors in the
original word-level space, topic-level models com-
pute sim(d, d&apos;, w) by measuring similarity of vec-
tors in a reduced-dimensionality latent space. As
conjectured in the introduction, such coarse-grain
representation might lead to loss of information.
Hence, in the next section we propose a com-
bined two-level model, which represents predicate
slots in the original word-level space while biasing
the similarity measure through topic-level context
models.
</bodyText>
<sectionHeader confidence="0.991855" genericHeader="method">
3 Two-level Context-sensitive Inference
</sectionHeader>
<bodyText confidence="0.999980571428572">
Our model follows the general DIRT scheme
while extending it to handle context-sensitive scor-
ing of rule applications, addressing the scenario
dealt by the context-sensitive topic models. In
particular, we define the context-sensitive score
scoreWT, where WT stands for the combination
of the Word/Topic levels:
</bodyText>
<equation confidence="0.988312333333333">
scoreWT(LHS -+ RHS, wx, wy)
_
�sim(vlx , vxr, wx) · sim(vi , vyr, wy)
</equation>
<bodyText confidence="0.999959925925926">
Thus, our model computes similarity over word-
level (rather than topic-level) argument vectors,
while biasing it according to the specific argu-
ment words in the given rule application con-
text. The core of our contribution is thus defining
the context-sensitive word-level vector similarity
measure sim(v, v&apos;, w), as described in the remain-
der of this section.
Following the methods in Section 2, for each
predicate pred we construct, from the learning
corpus, its argument vectors vxpred and vypred as
well as its argument pseudo-documents dxpred and
dypred. For convenience, when referring to an ar-
gument vector v, we will denote the correspond-
ing pseudo-document by dv. Based on all pseudo-
documents we learn an LDA model and obtain its
associated probability distributions.
The calculation of sim(v, v&apos;, w) is composed of
two steps. At learning time, we compute for each
candidate rule a separate, topic-biased, similarity
score per each of the topics in the LDA model.
Then, at rule application time, we compute an
overall reliability score for the rule by combining
the per-topic similarity scores, while biasing the
score combination according to the given context
of w. These two steps are described in the follow-
ing two subsections.
</bodyText>
<subsectionHeader confidence="0.998096">
3.1 Topic-biased Word-vector Similarities
</subsectionHeader>
<bodyText confidence="0.9997494">
Given a pair of word vectors v and v&apos;, and
any desired “base” vector similarity measure sim
(e.g. simLZn), we compute a topic-biased sim-
ilarity score for each LDA topic t, denoted by
simt(v, v&apos;). simt(v, v&apos;) is computed by applying
</bodyText>
<equation confidence="0.480325">
(6)
</equation>
<page confidence="0.931959">
1334
</page>
<bodyText confidence="0.9970955">
the original similarity measure over topic-biased
versions of v and v0, denoted by vt and v0t:
</bodyText>
<equation confidence="0.742639">
simt(v, v0) = sim(vt, v0t)
</equation>
<bodyText confidence="0.777188">
where
</bodyText>
<equation confidence="0.996975">
vt(w) = v(w) · p(t|dv, w)
</equation>
<bodyText confidence="0.999928392857143">
That is, each value in the biased vector, vt(w),
is obtained by weighing the original value v(w)
by the relevance of the topic t to the argument
word w within dv. This way, rather than replac-
ing altogether the word-level values v(w) by the
topic probabilities p(t|dv, w), as done in the topic-
level models, we use the latter to only bias the for-
mer while preserving fine-grained word-level rep-
resentations. The notation Lint denotes the simt
measure when applied using Lin as the base simi-
larity measure sim.
This learning process results in K different
topic-biased similarity scores for each candidate
rule, where K is the number of LDA topics. Ta-
ble 1 illustrates topic-biased similarities for the Y
slot of two rules involving the predicate ‘acquire’.
As can be seen, the topic-biased score Lint for ‘ac-
quire -+ learn’ for t2 is higher than the Lin score,
since this topic is characterized by arguments that
commonly appear with both predicates of the rule.
Consequently, the two predicates are found to be
distributionally similar when biased for this topic.
On the other hand, the topic-biased similarity for
t1 is substantially lower, since prominent words
in this topic are likely to occur with ‘acquire’ but
not with ‘learn’, yielding low distributional simi-
larity. Opposite behavior is exhibited for the rule
‘acquire -+ purchase’.
</bodyText>
<subsectionHeader confidence="0.997408">
3.2 Context-sensitive Similarity
</subsectionHeader>
<bodyText confidence="0.999953454545454">
When applying an inference rule, we compute
for each slot its context-sensitive similarity score
simWT(v, v0, w), where v and v0 are the slot’s ar-
gument vectors for the two rule sides and w is the
word instantiating the slot in the given rule appli-
cation. This score is computed as a weighted aver-
age of the rule’s K topic-biased similarity scores
simt. In this average, each topic is weighed by
its “relevance” for the context in which the rule is
applied, which consists of the left-hand-side pred-
icate v and the argument w. This relevance is cap-
</bodyText>
<table confidence="0.999428416666667">
Topic t1 t2
Top 5 calbiochem rights
words corel syndrome
networks majority
viacom knowledge
financially skill
acquire → learn
Lint(v, v&apos;) 0.040 0.334
Lin(v, v&apos;) 0.165
acquire → purchase
Lint(v, v&apos;) 0.427 0.241
Lin(v, v&apos;) 0.267
</table>
<tableCaption confidence="0.997148">
Table 1: Two characteristic topics for the Y slot of
</tableCaption>
<bodyText confidence="0.994687">
‘acquire’, along with their topic-biased Lin sim-
ilarities scores Lint, compared with the original
Lin similarity, for two rules. The relevance of each
topic to different arguments of ‘acquire’ is illus-
trated by showing the top 5 words in the argument
vector vyacquire for which the illustrated topic is the
most likely one.
</bodyText>
<equation confidence="0.99230375">
tured by p(t|dv, w):
�simWT(v, v0, w) = [p(t|dv, w) · simt(v, v0)]
t
(7)
</equation>
<bodyText confidence="0.999990529411765">
This way, a rule application would obtain a high
score only if the current context fits those topics
for which the rule is indeed likely to be valid, as
captured by a high topic-biased similarity. The no-
tation LinWT denotes the simWT measure, when
using Lint as the topic-biased similarity measure.
Table 2 illustrates the calculation of context-
sensitive similarity scores in four rule applica-
tions, involving the Y slot of the predicate ‘ac-
quire’. We observe that relative to the fixed
context-insensitive Lin score, the score of ‘ac-
quire -+ learn’ is substantially promoted for
the argument ‘skill’ while being demoted for
‘Skype’. The opposite behavior is observed for
‘acquire -+ purchase’, altogether demonstrating
how our model successfully biases the similarity
score according to rule validity in context.
</bodyText>
<sectionHeader confidence="0.999" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999729428571429">
To evaluate our model, we compare it both to
context-insensitive similarity measures as well as
to prior context-sensitive methods. Furthermore,
to better understand its applicability in typical
NLP tasks, we focus on an evaluation setting that
corresponds to a natural distribution of examples
from a large corpus.
</bodyText>
<page confidence="0.977584">
1335
</page>
<table confidence="0.999839384615385">
Topic t1 t2
Top 5 calbiochem rights
words corel syndrome
networks majority
viacom knowledge
financially skill
‘acquire Skype -+ learn Skype’
p(t|dv, w) 0.974 0.000
Lint(v, v&apos;) 0.040 0.334
LinWT(v, v&apos;, w) 0.039
Lin(v, v&apos;) 0.165
‘acquire Skype -+ purchase Skype’
p(t|dv, w) 0.974 0.000
Lint(v, v&apos;) 0.427 0.241
LinWT(v, v&apos;, w) 0.417
Lin(v, v&apos;) 0.267
‘acquire skill -+ learn skill’
p(t|dv, w) 0.000 0.380
Lint(v, v&apos;) 0.040 0.334
LinWT(v, v&apos;, w) 0.251
Lin(v, v&apos;) 0.165
‘acquire skill -+ purchase skill’
p(t|dv, w) 0.000 0.380
Lint(v, v&apos;) 0.427 0.241
LinWT(v, v&apos;, w) 0.181
Lin(v, v&apos;) 0.267
</table>
<tableCaption confidence="0.991443">
Table 2: Context-sensitive similarity scores (in
</tableCaption>
<bodyText confidence="0.990832111111111">
bold) for the Y slots of four rule applications. The
components of the score calculation are shown for
the topics of Table 1. For each rule application,
the table shows a couple of the topic-biased scores
Lint of the rule (as in Table 1), along with the topic
relevance for the given context p(t|dv, w), which
weighs the topic-biased scores in the LinWT cal-
culation. The context-insensitive Lin score is
shown for comparison.
</bodyText>
<subsectionHeader confidence="0.966672">
4.1 Evaluated Rule Application Methods
</subsectionHeader>
<bodyText confidence="0.999848417910448">
We evaluated the following rule application meth-
ods: the original context-insensitive word model,
following DIRT (Lin and Pantel, 2001), as de-
scribed in Equation 1, denoted by CI; our own
topic-word context-sensitive model, as described
in Equation 6, denoted by WT. In addition, we
evaluated two variants of the topic-level context-
sensitive model, denoted DC and SC. DC follows
the double conditioned contextualized similarity
measure according to Equation 4, as implemented
by (Ritter et al., 2010), while SC follows the sin-
gle conditioned one at Equation 5, as implemented
by (Dinu and Lapata, 2010b; Dinu and Lapata,
2010a).
Since our model can contextualize various dis-
tributional similarity measures, we evaluated the
performance of all the above methods on several
base similarity measures and their learned rule-
sets, namely Lin (Lin, 1998), BInc (Szpektor and
Dagan, 2008) and vector Cosine similarity. The
Lin similarity measure is described in Equation 2.
Binc (Szpektor and Dagan, 2008) is a directional
similarity measure between word vectors, which
outperformed Lin for predicate inference (Szpek-
tor and Dagan, 2008).
To build the rule-sets and models for the tested
approaches we utilized the ReVerb corpus (Fader
et al., 2011), a large scale publicly available web-
based open extractions data set, containing about
15 million unique template extractions.3 ReVerb
template extractions/instantiations are in the form
of a tuple (x, pred, y), containing pred, a verb
predicate, x, the argument instantiation of the tem-
plate’s slot X, and y, the instantiation of the tem-
plate’s slot Y .
ReVerb includes over 600,000 different tem-
plates that comprise a verb but may also include
other words, for example ‘X can accommodate up
to Y’. Yet, many of these templates share a similar
meaning, e.g. ‘X accommodate up to Y’, ‘X can
accommodate up to Y’, ‘X will accommodate up
to Y’, etc. Following Sekine (2005), we clustered
templates that share their main verb predicate in
order to scale down the number of different pred-
icates in the corpus and collect richer word co-
occurrence statistics per predicate.
Next, we applied some clean-up preprocessing
to the ReVerb extractions. This includes discard-
ing stop words, rare words and non-alphabetical
words instantiating either the X or the Y argu-
ments. In addition, we discarded all predicates
that co-occur with less than 100 unique argument
words in each slot. The remaining corpus consists
of 7 million unique extractions and 2,155 verb
predicates.
Finally, we trained an LDA model, as described
in Section 2, using Mallet (McCallum, 2002).
Then, for each original context-insensitive simi-
larity measure, we learned from ReVerb a rule-set
comprised of the top 500 rules for every identi-
fied predicate. To complete the learning, we cal-
culated the topic-biased similarity score for each
learned rule under each LDA topic, as specified
in our context-sensitive model. We release a rule
set comprising the top 500 context-sensitive rules
that we learned for each of the verb predicates in
our learning corpus, along with our trained LDA
</bodyText>
<footnote confidence="0.9930375">
3ReVerb is available at http://reverb.cs.
washington.edu/
</footnote>
<page confidence="0.958275">
1336
</page>
<table confidence="0.99939125">
Method Lin BInc Cosine
Valid 266 254 272
Invalid 545 523 539
Total 811 777 811
</table>
<tableCaption confidence="0.9853405">
Table 3: Sizes of rule application test set for each
learned rule-set.
</tableCaption>
<bodyText confidence="0.54826">
model.4
</bodyText>
<subsectionHeader confidence="0.985087">
4.2 Evaluation Task
</subsectionHeader>
<bodyText confidence="0.999982333333333">
To evaluate the performance of the different meth-
ods we chose the dataset constructed by Zeich-
ner et al. (2012). 5 This publicly available dataset
contains about 6,500 manually annotated predi-
cate template rule applications, each one labeled
as correct or incorrect. For example, ‘Jack agree
with Jill --/+ Jack feel sorry for Jill’ is a rule ap-
plication in this dataset, labeled as incorrect, and
‘Registration open this month → Registration be-
gin this month’ is another rule application, labeled
as correct. Rule applications were generated by
randomly sampling extractions from ReVerb, such
as (‘Jack’,‘agree with’,‘Jill’) and then sampling
possible rules for each, such as ‘agree with → feel
sorry for’. Hence, this dataset provides naturally
distributed rule inferences with respect to ReVerb.
Whenever we evaluated a distributional similar-
ity measure (namely Lin, BInc, or Cosine), we
discarded instances from Zeichner et al.’s dataset
in which the assessed rule is not in the context-
insensitive rule-set learned for this measure or the
argument instantiation of the rule is not in the LDA
lexicon. We refer to the remaining instances as the
test set per measure, e.g. Lin’s test set. Table 3
details the size of each such test set in our experi-
ment.
Finally, the task under which we assessed the
tested models is to rank all rule applications in
each test set, aiming to rank the valid rule appli-
cations above the invalid ones.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999759">
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al., 2008) of the rule appli-
cation ranking computed by this method. In order
</bodyText>
<footnote confidence="0.9953718">
4Ourresource is available at: http://www.cs.biu.
ac.il/˜nlp/downloads/wt-rules.html
5The dataset is available at: http://
www.cs.biu.ac.il/˜nlp/downloads/
annotation-rule-application.htm
</footnote>
<table confidence="0.9989308">
Method Lin BInc Cosine
CI 0.503 0.513 0.513
DC 0.451 (1200) 0.455 (1200) 0.455 (1200)
SC 0.443 (1200) 0.458 (1200) 0.452 (1200)
WT 0.562 (100) 0.584 (50) 0.565 (25)
</table>
<tableCaption confidence="0.986722">
Table 4: MAP values on corresponding test set ob-
</tableCaption>
<bodyText confidence="0.986664340909091">
tained by each method. Figures in parentheses in-
dicate optimal number of LDA topics.
to compute MAP values and corresponding statis-
tical significance, we randomly split each test set
into 30 subsets. For each method we computed
Average Precision on every subset and then took
the average over all subsets as the MAP value.
Since all tested context-sensitive approaches are
based on LDA topics, we varied for each method
the number of LDA topics K that optimizes its
performance, ranging from 25 to 1600 topics. We
used LDA hyperparameters β = 0.01 and α = 0.1
for K &lt; 600 and α = K for K &gt;= 600.
Table 4 presents the optimal MAP performance
of each tested measure. Our main result is that
our model outperforms all other methods, both
context-insensitive and context-sensitive, by a rel-
ative increase of more than 10% for all three sim-
ilarity measures that we tested. This improvement
is statistically significant at p &lt; 0.01 for BInc and
Lin, and p &lt; 0.015 for Cosine, using paired t-
test. This shows that our model indeed success-
fully leverages contextual information beyond the
basic context-agnostic rule scores and is robust
across measures.
Surprisingly, both baseline topic-level context-
sensitive methods, namely DC and SC, underper-
formed compared to their context-insensitive base-
lines. While Dinu and Lapata (Dinu and Lap-
ata, 2010b) did show improvement over context-
insensitive DIRT, this result was obtained on the
verbs of the Lexical Substitution Task in SemEval
(McCarthy and Navigli, 2007), which was manu-
ally created with a bias for context-sensitive sub-
stitutions. However, our result suggests that topic-
level models might not be robust enough when ap-
plied to a random sample of inferences.
An interesting indication of the differences be-
tween our word-topic model, WT, and topic-only
models, DC and SC, lies in the optimal number of
LDA topics required for each method. The num-
ber of topics in the range 25-100 performed almost
equally well under the WT model for all base mea-
sures, with a moderate decline for higher numbers.
</bodyText>
<page confidence="0.987004">
1337
</page>
<bodyText confidence="0.999831235294118">
The need for this rather small number of topics is
due to the nature of utilization of topics in WT.
Specifically, topics are leveraged for high-level
domain disambiguation, while fine grained word-
level distributional similarity is computed for each
rule under each such domain. This works best for
a relatively low number of topics. However, in
higher numbers, topics relate to narrower domains
and then topic biased word level similarity may
become less effective due to potential sparseness.
On the other hand, DC and SC rely on topics as
a surrogate to predicate-argument co-occurrence
features, and thus require a relatively large num-
ber of them to be effective.
Delving deeper into our test-set, Zeichner et al.
provided a more detailed annotation for each in-
valid rule application. Specifically, they annotated
whether the context under which the rule is ap-
plied is valid. For example, in ‘John bought my
car --/+ John sold my car’ the inference is invalid
due to an inherently incorrect rule, but the con-
text is valid. On the other hand in ‘my boss raised
my salary --/+ my boss constructed my salary’ the
context {‘my boss’, ‘my salary’} for applying
‘raise → construct’ is invalid. Following, we split
the test-set for the base Lin measure into two test-
sets: (a) test-setvc, which includes all correct rule
applications and incorrect ones only under valid
contexts, and (b) test-setivc, which includes again
all correct rule applications but incorrect ones only
under invalid contexts.
Table 5 presents the performance of each com-
pared method on the two test sets. On test-
setivc, where context mismatches are abundant,
our model outperformed all other baselines (sta-
tistically significant at p &lt; 0.01). In addition,
this time DC slightly outperformed CI. This re-
sult more explicitly shows the advantages of in-
tegrating word-level and context-sensitive topic-
level similarities for differentiating valid and in-
valid contexts for rule applications. Yet, many in-
valid rule applications occur under valid contexts
due to inherently incorrect rules, and we want to
make sure that also in this scenario our model
does not fall behind the context-insensitive mea-
sure. Indeed, on test-setvc, in which context mis-
matches are rare, our algorithm is still better than
the original measure, indicating that WT can be
safely applied to distributional similarity measures
without concerns of reduced performance in dif-
ferent context scenarios.
</bodyText>
<table confidence="0.995797857142857">
test-seti„c test-set„c
Size 432 645
(valid:invalid) (266:166) (266:379)
CI 0.780 0.587
DC 0.796 0.498
SC 0.779 0.512
WT 0.854 0.621
</table>
<tableCaption confidence="0.9698275">
Table 5: MAP results for the two split Lin test-
sets.
</tableCaption>
<sectionHeader confidence="0.996611" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999991375">
This paper addressed the problem of computing
context-sensitive reliability scores for predicate in-
ference rules. In particular, we proposed a novel
scheme that applies over any base distributional
similarity measure which operates at the word
level, and computes a single context-insensitive
score for a rule. Based on such a measure, our
scheme constructs a context-sensitive similarity
measure that computes a reliability score for pred-
icate inference rules applications in the context of
given arguments.
The contextualization of the base similarity
score was obtained using a topic-level LDA
model, which was used in a novel way. First,
it provides a topic bias for learning separate per-
topic word-level similarity scores between predi-
cates. Then, given a specific candidate rule ap-
plication, the LDA model is used to infer the
topic distribution relevant to the context speci-
fied by the given arguments. Finally, the context-
sensitive rule application score is computed as a
weighted average of the per-topic word-level sim-
ilarity scores, which are weighed according to the
inferred topic distribution.
While most works on context-insensitive pred-
icate inference rules, such as DIRT (Lin and Pan-
tel, 2001), are based on word-level similarity mea-
sures, almost all prior models addressing context-
sensitive predicate inference rules are based on
topic models (except for (Pantel et al., 2007),
which was outperformed by later models). We
therefore focused on comparing the performance
of our two-level scheme with state-of-the-art prior
topic-level and word-level models of distributional
similarity, over a random sample of inference rule
applications. Under this natural setting, the two-
level scheme consistently outperformed both types
of models when tested with three different base
similarity measures. Notably, our model shows
stable performance over a large subset of the data
</bodyText>
<page confidence="0.976783">
1338
</page>
<bodyText confidence="0.999992">
where context sensitivity is rare, while topic-level
models tend to underperform in such cases com-
pared to the base context-insensitive methods.
Our work is closely related to another research
line that addresses lexical similarity and substi-
tution scenarios in context. While we focus on
lexical-syntactic predicate templates and instanti-
ations of their argument slots as context, lexical
similarity methods consider various lexical units
that are not necessarily predicates, with their con-
text typically being the collection of words in a
window around them.
Various approaches have been proposed to ad-
dress lexical similarity. A number of works are
based on a compositional semantics approach,
where a prior representation of a target lexical unit
is composed with the representations of words in
its given context (Mitchell and Lapata, 2008; Erk
and Pad´o, 2008; Thater et al., 2010). Other works
(Erk and Pad´o, 2010; Reisinger and Mooney,
2010) use a rather large word window around tar-
get words and compute similarities between clus-
ters comprising instances of word windows. In ad-
dition, (Dinu and Lapata, 2010a) adapted the pred-
icate inference topic model from (Dinu and Lap-
ata, 2010b) to compute lexical similarity in con-
text.
A natural extension of our work would be to ex-
tend our two level model to accommodate context-
sensitive lexical similarity. For this purpose we
will need to redefine the scope of context in our
model, and adapt our method to compute context-
biased lexical similarities accordingly. Then we
will also be able to evaluate our model on the
Lexical Substitution Task (McCarthy and Navigli,
2007), which has been commonly used in recent
years as a benchmark for context-sensitive lexical
similarity models.
In a different NLP task, Eidelman et al. (2012)
utilize a similar approach to ours for improving
the performance of statistical machine translation
(SMT). They learn an LDA model on the source
language side of the training corpus with the pur-
pose of identifying implicit sub-domains. Then
they utilize the distribution over topics inferred for
each document in their corpus to compute sepa-
rate per-topic translation probability tables. Fi-
nally, they train a classifier to translate a given
target word based on these tables and the inferred
topic distribution of the given document in which
the target word appears. A notable difference be-
tween our approach and theirs is that we use predi-
cate pseudo-documents consisting of argument in-
stantiations to learn our LDA model, while Eidel-
man et al. use the real documents in a corpus.
We believe that combining these two approaches
may improve performance for both textual infer-
ence and SMT and plan to experiment with this
direction in future work.
</bodyText>
<sectionHeader confidence="0.998335" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.954612333333333">
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community’s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
</bodyText>
<sectionHeader confidence="0.99599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999078787878788">
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL.
Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorithm
for learning directionality of inference rules. In Pro-
ceedings of EMNLP-CoNLL.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177–190.
Georgiana Dinu and Mirella Lapata. 2010a. Measur-
ing distributional similarity in context. In Proceed-
ings of EMNLP.
Georgiana Dinu and Mirella Lapata. 2010b. Topic
models for meaning similarity in context. In Pro-
ceedings of COLING: Posters.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings EACL.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the ACL
conference short papers.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP.
Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL conference short papers.
</reference>
<page confidence="0.886564">
1339
</page>
<reference confidence="0.999723753424658">
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Dekang Lin and Patrick Pantel. 2001. DIRT – discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining 2001.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SemEval.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
Proceedings of ACL.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Diarmuid O S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings ofACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings ofACL (short papers).
</reference>
<page confidence="0.989741">
1340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.152908">
<title confidence="0.999936">A Two Level Model for Context Sensitive Inference Rules</title>
<author confidence="0.999727">Jonathan Ido Jacob Idan</author>
<affiliation confidence="0.653853">Science Department, Bar-Ilan</affiliation>
<address confidence="0.452313333333333">Science Department, Stanford of Engineering, Bar-Ilan Research Israel</address>
<email confidence="0.999771">idan@yahoo-inc.com</email>
<abstract confidence="0.999406055555556">Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2632" citStr="Berant et al., 2011" startWordPosition="384" endWordPosition="387">nce for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as t</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
<author>Marina Rey</author>
</authors>
<title>Ledir: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2536" citStr="Bhagat et al., 2007" startWordPosition="370" endWordPosition="373">mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. Howeve</context>
<context position="9298" citStr="Bhagat et al., 2007" startWordPosition="1440" endWordPosition="1443">d on binary predicates. However, our contextsensitive scheme can be applied to any arity. (1) 1332 measure from (Lin, 1998), defined as follows: Lin (v, v&apos;) = EwEvnv&apos; [v (w) + v&apos; (w)] (2) EwEvUv&apos; [v(w) + v&apos;(w)] We note that the general DIRT scheme may be used while employing other “base” vector similarity measures. For example, the Lin measure is symmetric, and thus using it would yield the same reliability score when swapping the two sides of a rule. This issue has been addressed in a separate line of research which introduced directional similarity measures suitable for inference relations (Bhagat et al., 2007; Szpektor and Dagan, 2008; Kotlerman et al., 2010). In our experiments we apply our proposed context-sensitive similarity scheme over three different base similarity measures. DIRT and similar context-insensitive inference methods provide a single reliability score for a learned inference rule, which aims to predict the validity of the rule’s applications. However, as exemplified in the Introduction, an inference rule may be valid in some contexts but invalid in others (e.g. acquiring entails purchasing for goods, but not for skills). Since vector similarity in DIRT is computed over the singl</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, Rey, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Marina Rey. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="10945" citStr="Blei et al., 2003" startWordPosition="1694" endWordPosition="1697">in a given context, as described next. 2.2 Context-sensitive Rule Applications To assess the reliability of applying an inference rule in a given context we need some model for context representation, that should affect the rule reliability score. A major trend in past work is to represent contexts in a reduced-dimensionality latent or class-based model. A couple of earlier works utilized a cluster-based model (Pantel et al., 2007) and an LSA-based model (Szpektor et al., 2008), in a selectional-preferences style approach. Several more recent works utilize a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) framework. We now present an underlying unified view of the topic-level models in (Ritter et al., 2010; Dinu and Lapata, 2010b), which we follow in our own model and in comparative model evaluations. We note that a similar LDA model construction was employed also in (S´eaghdha, 2010), for estimating predicate-argument likelihood. First, an LDA model is constructed, as follows. Similar to the construction of argument vectors in the distributional model (described above in subsection 2.1), all arguments instantiating each predicate slot are extracted from a large learning corpus. Then, for each</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<contexts>
<context position="1706" citStr="Dagan et al., 2006" startWordPosition="240" endWordPosition="244">ule set. 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y -+ X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules betw</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Lecture Notes in Computer Science, volume 3944, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3841" citStr="Dinu and Lapata, 2010" startWordPosition="584" endWordPosition="587">d, such as the context specified by the given predicate’s arguments. For example, ‘AT&amp;T acquire TMobile -+ AT&amp;T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y -+ X purchase Y’, while ‘Children acquire skills -+ Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular</context>
<context position="11071" citStr="Dinu and Lapata, 2010" startWordPosition="1716" endWordPosition="1719">erence rule in a given context we need some model for context representation, that should affect the rule reliability score. A major trend in past work is to represent contexts in a reduced-dimensionality latent or class-based model. A couple of earlier works utilized a cluster-based model (Pantel et al., 2007) and an LSA-based model (Szpektor et al., 2008), in a selectional-preferences style approach. Several more recent works utilize a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) framework. We now present an underlying unified view of the topic-level models in (Ritter et al., 2010; Dinu and Lapata, 2010b), which we follow in our own model and in comparative model evaluations. We note that a similar LDA model construction was employed also in (S´eaghdha, 2010), for estimating predicate-argument likelihood. First, an LDA model is constructed, as follows. Similar to the construction of argument vectors in the distributional model (described above in subsection 2.1), all arguments instantiating each predicate slot are extracted from a large learning corpus. Then, for each slot of each predicate, a pseudo-document is constructed containing the set of all argument words that instantiated this slot</context>
<context position="14440" citStr="Dinu and Lapata (2010" startWordPosition="2277" endWordPosition="2280">es, the reliability score of the topic-level model is defined as follows (we present a geometric mean formulation for consistency with DIRT): scoreTopic(LHS -+ RHS, wx, wy) (3) _ �sim(dxl , dxr, wx) · sim(di , dyr, wy) where sim(d, d&apos;, w) is a topic-distribution similarity measure conditioned on a given context word. Specifically, Ritter et al. (2010) utilized the dot product form for their similarity measure: simDC(d, d&apos;, w) _ Et[p(t|d, w) · p(t|d&apos;, w)] (4) (the subscript DC stands for double-conditioning, as both distributions are conditioned on the argument word, unlike the measure below). Dinu and Lapata (2010b) presented a slightly different similarity measure for topic distributions that performed better in their setting as well as in a related later paper on context-sensitive scoring of lexical similarity (Dinu and Lapata, 2010a). In this measure, the topic distribution for the right hand side of the rule is not conditioned on w: simSC(d, d&apos;, w) _ Et[p(t|d, w) · p(t|d&apos;)] (5) (the subscript SC stands for single-conditioning, as only the left distribution is conditioned on the argument word). They also experimented with a few variants for the structure of the similarity measure and assessed that b</context>
<context position="23463" citStr="Dinu and Lapata, 2010" startWordPosition="3727" endWordPosition="3730">Application Methods We evaluated the following rule application methods: the original context-insensitive word model, following DIRT (Lin and Pantel, 2001), as described in Equation 1, denoted by CI; our own topic-word context-sensitive model, as described in Equation 6, denoted by WT. In addition, we evaluated two variants of the topic-level contextsensitive model, denoted DC and SC. DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al., 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rulesets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. The Lin similarity measure is described in Equation 2. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpektor and Dagan, 2008). To build the rule-sets and models for the tested approaches we utilize</context>
<context position="29555" citStr="Dinu and Lapata, 2010" startWordPosition="4708" endWordPosition="4712">her methods, both context-insensitive and context-sensitive, by a relative increase of more than 10% for all three similarity measures that we tested. This improvement is statistically significant at p &lt; 0.01 for BInc and Lin, and p &lt; 0.015 for Cosine, using paired ttest. This shows that our model indeed successfully leverages contextual information beyond the basic context-agnostic rule scores and is robust across measures. Surprisingly, both baseline topic-level contextsensitive methods, namely DC and SC, underperformed compared to their context-insensitive baselines. While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over contextinsensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. However, our result suggests that topiclevel models might not be robust enough when applied to a random sample of inferences. An interesting indication of the differences between our word-topic model, WT, and topic-only models, DC and SC, lies in the optimal number of LDA topics required for each method. The number of topics in the range 25-100 performed a</context>
<context position="35926" citStr="Dinu and Lapata, 2010" startWordPosition="5715" endWordPosition="5718">typically being the collection of words in a window around them. Various approaches have been proposed to address lexical similarity. A number of works are based on a compositional semantics approach, where a prior representation of a target lexical unit is composed with the representations of words in its given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010). Other works (Erk and Pad´o, 2010; Reisinger and Mooney, 2010) use a rather large word window around target words and compute similarities between clusters comprising instances of word windows. In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute contextbiased lexical similarities accordingly. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical </context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010a. Measuring distributional similarity in context. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for meaning similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING:</booktitle>
<publisher>Posters.</publisher>
<contexts>
<context position="3841" citStr="Dinu and Lapata, 2010" startWordPosition="584" endWordPosition="587">d, such as the context specified by the given predicate’s arguments. For example, ‘AT&amp;T acquire TMobile -+ AT&amp;T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y -+ X purchase Y’, while ‘Children acquire skills -+ Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular</context>
<context position="11071" citStr="Dinu and Lapata, 2010" startWordPosition="1716" endWordPosition="1719">erence rule in a given context we need some model for context representation, that should affect the rule reliability score. A major trend in past work is to represent contexts in a reduced-dimensionality latent or class-based model. A couple of earlier works utilized a cluster-based model (Pantel et al., 2007) and an LSA-based model (Szpektor et al., 2008), in a selectional-preferences style approach. Several more recent works utilize a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) framework. We now present an underlying unified view of the topic-level models in (Ritter et al., 2010; Dinu and Lapata, 2010b), which we follow in our own model and in comparative model evaluations. We note that a similar LDA model construction was employed also in (S´eaghdha, 2010), for estimating predicate-argument likelihood. First, an LDA model is constructed, as follows. Similar to the construction of argument vectors in the distributional model (described above in subsection 2.1), all arguments instantiating each predicate slot are extracted from a large learning corpus. Then, for each slot of each predicate, a pseudo-document is constructed containing the set of all argument words that instantiated this slot</context>
<context position="14440" citStr="Dinu and Lapata (2010" startWordPosition="2277" endWordPosition="2280">es, the reliability score of the topic-level model is defined as follows (we present a geometric mean formulation for consistency with DIRT): scoreTopic(LHS -+ RHS, wx, wy) (3) _ �sim(dxl , dxr, wx) · sim(di , dyr, wy) where sim(d, d&apos;, w) is a topic-distribution similarity measure conditioned on a given context word. Specifically, Ritter et al. (2010) utilized the dot product form for their similarity measure: simDC(d, d&apos;, w) _ Et[p(t|d, w) · p(t|d&apos;, w)] (4) (the subscript DC stands for double-conditioning, as both distributions are conditioned on the argument word, unlike the measure below). Dinu and Lapata (2010b) presented a slightly different similarity measure for topic distributions that performed better in their setting as well as in a related later paper on context-sensitive scoring of lexical similarity (Dinu and Lapata, 2010a). In this measure, the topic distribution for the right hand side of the rule is not conditioned on w: simSC(d, d&apos;, w) _ Et[p(t|d, w) · p(t|d&apos;)] (5) (the subscript SC stands for single-conditioning, as only the left distribution is conditioned on the argument word). They also experimented with a few variants for the structure of the similarity measure and assessed that b</context>
<context position="23463" citStr="Dinu and Lapata, 2010" startWordPosition="3727" endWordPosition="3730">Application Methods We evaluated the following rule application methods: the original context-insensitive word model, following DIRT (Lin and Pantel, 2001), as described in Equation 1, denoted by CI; our own topic-word context-sensitive model, as described in Equation 6, denoted by WT. In addition, we evaluated two variants of the topic-level contextsensitive model, denoted DC and SC. DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al., 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rulesets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. The Lin similarity measure is described in Equation 2. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpektor and Dagan, 2008). To build the rule-sets and models for the tested approaches we utilize</context>
<context position="29555" citStr="Dinu and Lapata, 2010" startWordPosition="4708" endWordPosition="4712">her methods, both context-insensitive and context-sensitive, by a relative increase of more than 10% for all three similarity measures that we tested. This improvement is statistically significant at p &lt; 0.01 for BInc and Lin, and p &lt; 0.015 for Cosine, using paired ttest. This shows that our model indeed successfully leverages contextual information beyond the basic context-agnostic rule scores and is robust across measures. Surprisingly, both baseline topic-level contextsensitive methods, namely DC and SC, underperformed compared to their context-insensitive baselines. While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over contextinsensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. However, our result suggests that topiclevel models might not be robust enough when applied to a random sample of inferences. An interesting indication of the differences between our word-topic model, WT, and topic-only models, DC and SC, lies in the optimal number of LDA topics required for each method. The number of topics in the range 25-100 performed a</context>
<context position="35926" citStr="Dinu and Lapata, 2010" startWordPosition="5715" endWordPosition="5718">typically being the collection of words in a window around them. Various approaches have been proposed to address lexical similarity. A number of works are based on a compositional semantics approach, where a prior representation of a target lexical unit is composed with the representations of words in its given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010). Other works (Erk and Pad´o, 2010; Reisinger and Mooney, 2010) use a rather large word window around target words and compute similarities between clusters comprising instances of word windows. In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute contextbiased lexical similarities accordingly. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical </context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010b. Topic models for meaning similarity in context. In Proceedings of COLING: Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Rui Wang</author>
</authors>
<title>Inference rules and their application to recognizing textual entailment.</title>
<date>2009</date>
<booktitle>In Proceedings EACL.</booktitle>
<contexts>
<context position="1728" citStr="Dinu and Wang, 2009" startWordPosition="245" endWordPosition="248">ion Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y -+ X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicate</context>
</contexts>
<marker>Dinu, Wang, 2009</marker>
<rawString>Georgiana Dinu and Rui Wang. 2009. Inference rules and their application to recognizing textual entailment. In Proceedings EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL conference short</booktitle>
<pages>papers.</pages>
<contexts>
<context position="36592" citStr="Eidelman et al. (2012)" startWordPosition="5826" endWordPosition="5829">el from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute contextbiased lexical similarities accordingly. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. In a different NLP task, Eidelman et al. (2012) utilize a similar approach to ours for improving the performance of statistical machine translation (SMT). They learn an LDA model on the source language side of the training corpus with the purpose of identifying implicit sub-domains. Then they utilize the distribution over topics inferred for each document in their corpus to compute separate per-topic translation probability tables. Finally, they train a classifier to translate a given target word based on these tables and the inferred topic distribution of the given document in which the target word appears. A notable difference between ou</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of the ACL conference short papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL conference short</booktitle>
<pages>papers.</pages>
<marker>Erk, Pad´o, 2010</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based models for word meaning in context. In Proceedings of the ACL conference short papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="24103" citStr="Fader et al., 2011" startWordPosition="3825" endWordPosition="3828">010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rulesets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. The Lin similarity measure is described in Equation 2. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpektor and Dagan, 2008). To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al., 2011), a large scale publicly available webbased open extractions data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X, and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y’. Yet, many of these templates share a similar meaning, e.g. ‘X accommodate up to Y’, ‘X can accommod</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="9349" citStr="Kotlerman et al., 2010" startWordPosition="1448" endWordPosition="1451">sitive scheme can be applied to any arity. (1) 1332 measure from (Lin, 1998), defined as follows: Lin (v, v&apos;) = EwEvnv&apos; [v (w) + v&apos; (w)] (2) EwEvUv&apos; [v(w) + v&apos;(w)] We note that the general DIRT scheme may be used while employing other “base” vector similarity measures. For example, the Lin measure is symmetric, and thus using it would yield the same reliability score when swapping the two sides of a rule. This issue has been addressed in a separate line of research which introduced directional similarity measures suitable for inference relations (Bhagat et al., 2007; Szpektor and Dagan, 2008; Kotlerman et al., 2010). In our experiments we apply our proposed context-sensitive similarity scheme over three different base similarity measures. DIRT and similar context-insensitive inference methods provide a single reliability score for a learned inference rule, which aims to predict the validity of the rule’s applications. However, as exemplified in the Introduction, an inference rule may be valid in some contexts but invalid in others (e.g. acquiring entails purchasing for goods, but not for skills). Since vector similarity in DIRT is computed over the single aggregate argument vector, the obtained reliabili</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT – discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining</booktitle>
<contexts>
<context position="1991" citStr="Lin and Pantel, 2001" startWordPosition="283" endWordPosition="286"> ‘X treat Y -+ X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification ove</context>
<context position="7533" citStr="Lin and Pantel, 2001" startWordPosition="1144" endWordPosition="1147">on between two predicates. Each rule side consists of a lexical predicate and (two) variable slots for its arguments.1 Different representations have been used to specify predicates and their argument slots, such as word lemma sequences, regular expressions and dependency parse fragments. A rule can be applied when its LHS matches a predicate with a pair of arguments in a text, allowing us to infer its RHS, with the corresponding instantiations for the argument variables. For example, given the text “AT&amp;T acquires T-Mobile”, the above rule infers “AT&amp;T purchases T-Mobile”. The DIRT algorithm (Lin and Pantel, 2001) follows the distributional similarity paradigm to learn predicate inference rules. For each predicate, DIRT represents each of its argument slots by an argument vector. We denote the two vectors of the X and Y slots of a predicate pred by vx pred and vypred, respectively. Each entry of a vector v corresponds to a particular word (or term) w that instantiated the argument slot in a learning corpus, with a value v(w) = PMI(pred,w) (with PMI standing for point-wise mutual information). To learn inference rules, DIRT considers (in principle) each pair of binary predicates that occurred in the cor</context>
<context position="22997" citStr="Lin and Pantel, 2001" startWordPosition="3652" endWordPosition="3655">imilarity scores (in bold) for the Y slots of four rule applications. The components of the score calculation are shown for the topics of Table 1. For each rule application, the table shows a couple of the topic-biased scores Lint of the rule (as in Table 1), along with the topic relevance for the given context p(t|dv, w), which weighs the topic-biased scores in the LinWT calculation. The context-insensitive Lin score is shown for comparison. 4.1 Evaluated Rule Application Methods We evaluated the following rule application methods: the original context-insensitive word model, following DIRT (Lin and Pantel, 2001), as described in Equation 1, denoted by CI; our own topic-word context-sensitive model, as described in Equation 6, denoted by WT. In addition, we evaluated two variants of the topic-level contextsensitive model, denoted DC and SC. DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al., 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance</context>
<context position="34128" citStr="Lin and Pantel, 2001" startWordPosition="5441" endWordPosition="5445">vel LDA model, which was used in a novel way. First, it provides a topic bias for learning separate pertopic word-level similarity scores between predicates. Then, given a specific candidate rule application, the LDA model is used to infer the topic distribution relevant to the context specified by the given arguments. Finally, the contextsensitive rule application score is computed as a weighted average of the per-topic word-level similarity scores, which are weighed according to the inferred topic distribution. While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing contextsensitive predicate inference rules are based on topic models (except for (Pantel et al., 2007), which was outperformed by later models). We therefore focused on comparing the performance of our two-level scheme with state-of-the-art prior topic-level and word-level models of distributional similarity, over a random sample of inference rule applications. Under this natural setting, the twolevel scheme consistently outperformed both types of models when tested with three different base similarity measures. </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT – discovery of inference rules from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="5200" citStr="Lin, 1998" startWordPosition="790" endWordPosition="791">d word space, context-sensitive methods represent them as vectors at the level of latent topics. This raises the question of whether such coarse-grained topic vectors might be less informative in determining the semantic similarity between the two predicates. To address this hypothesized caveat of prior context-sensitive rule scoring methods, we propose a novel generic scheme that integrates wordlevel and topic-level representations. Our scheme can be applied on top of any context-insensitive “base” similarity measure for rule learning, which operates at the word level, such as Cosine or Lin (Lin, 1998). Rather than computing a single context-insensitive rule score, we compute a distinct word-level similarity score for each topic in an LDA model. Then, when applying a rule in a given context, these different scores are weighed together based on the specific topic distribution under the given context. This way, we calculate similarity over vectors in the original word space, while biasing them towards the given context via a topic model. In order to promote replicability and equal-term comparison with our results, we based our experiments on publicly available datasets, both for unsupervised </context>
<context position="8802" citStr="Lin, 1998" startWordPosition="1355" endWordPosition="1356">a reliability score for the rule by combining the measured similarities between the corresponding argument vectors of the two rule sides. Concretely, denoting by l and r the predicates appearing in the two rule sides, DIRT’s reliability score is defined as follows: scoreDIRT(LHS → RHS) �= sim(vi , vxr) · sim(vyl , vyr) where sim(v, v&apos;) is a vector similarity measure. Specifically, DIRT employs the Lin similarity 1We follow most of the inference-rule learning literature, which focused on binary predicates. However, our contextsensitive scheme can be applied to any arity. (1) 1332 measure from (Lin, 1998), defined as follows: Lin (v, v&apos;) = EwEvnv&apos; [v (w) + v&apos; (w)] (2) EwEvUv&apos; [v(w) + v&apos;(w)] We note that the general DIRT scheme may be used while employing other “base” vector similarity measures. For example, the Lin measure is symmetric, and thus using it would yield the same reliability score when swapping the two sides of a rule. This issue has been addressed in a separate line of research which introduced directional similarity measures suitable for inference relations (Bhagat et al., 2007; Szpektor and Dagan, 2008; Kotlerman et al., 2010). In our experiments we apply our proposed context-se</context>
<context position="23709" citStr="Lin, 1998" startWordPosition="3766" endWordPosition="3767">in Equation 6, denoted by WT. In addition, we evaluated two variants of the topic-level contextsensitive model, denoted DC and SC. DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al., 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rulesets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. The Lin similarity measure is described in Equation 2. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpektor and Dagan, 2008). To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al., 2011), a large scale publicly available webbased open extractions data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pre</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="25455" citStr="McCallum, 2002" startWordPosition="4048" endWordPosition="4049">order to scale down the number of different predicates in the corpus and collect richer word cooccurrence statistics per predicate. Next, we applied some clean-up preprocessing to the ReVerb extractions. This includes discarding stop words, rare words and non-alphabetical words instantiating either the X or the Y arguments. In addition, we discarded all predicates that co-occur with less than 100 unique argument words in each slot. The remaining corpus consists of 7 million unique extractions and 2,155 verb predicates. Finally, we trained an LDA model, as described in Section 2, using Mallet (McCallum, 2002). Then, for each original context-insensitive similarity measure, we learned from ReVerb a rule-set comprised of the top 500 rules for every identified predicate. To complete the learning, we calculated the topic-biased similarity score for each learned rule under each LDA topic, as specified in our context-sensitive model. We release a rule set comprising the top 500 context-sensitive rules that we learned for each of the verb predicates in our learning corpus, along with our trained LDA 3ReVerb is available at http://reverb.cs. washington.edu/ 1336 Method Lin BInc Cosine Valid 266 254 272 In</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="29719" citStr="McCarthy and Navigli, 2007" startWordPosition="4734" endWordPosition="4737">provement is statistically significant at p &lt; 0.01 for BInc and Lin, and p &lt; 0.015 for Cosine, using paired ttest. This shows that our model indeed successfully leverages contextual information beyond the basic context-agnostic rule scores and is robust across measures. Surprisingly, both baseline topic-level contextsensitive methods, namely DC and SC, underperformed compared to their context-insensitive baselines. While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over contextinsensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. However, our result suggests that topiclevel models might not be robust enough when applied to a random sample of inferences. An interesting indication of the differences between our word-topic model, WT, and topic-only models, DC and SC, lies in the optimal number of LDA topics required for each method. The number of topics in the range 25-100 performed almost equally well under the WT model for all base measures, with a moderate decline for higher numbers. 1337 The need for this rather small number of topics is due</context>
<context position="36434" citStr="McCarthy and Navigli, 2007" startWordPosition="5801" endWordPosition="5804">rds and compute similarities between clusters comprising instances of word windows. In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute contextbiased lexical similarities accordingly. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. In a different NLP task, Eidelman et al. (2012) utilize a similar approach to ours for improving the performance of statistical machine translation (SMT). They learn an LDA model on the source language side of the training corpus with the purpose of identifying implicit sub-domains. Then they utilize the distribution over topics inferred for each document in their corpus to compute separate per-topic translation probability tables. Finally, they train a classifier to translate a given</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="35653" citStr="Mitchell and Lapata, 2008" startWordPosition="5669" endWordPosition="5672">l similarity and substitution scenarios in context. While we focus on lexical-syntactic predicate templates and instantiations of their argument slots as context, lexical similarity methods consider various lexical units that are not necessarily predicates, with their context typically being the collection of words in a window around them. Various approaches have been proposed to address lexical similarity. A number of works are based on a compositional semantics approach, where a prior representation of a target lexical unit is composed with the representations of words in its given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010). Other works (Erk and Pad´o, 2010; Reisinger and Mooney, 2010) use a rather large word window around target words and compute similarities between clusters comprising instances of word windows. In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adap</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>Patty: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<pages>12</pages>
<contexts>
<context position="2835" citStr="Nakashole et al., 2012" startWordPosition="414" endWordPosition="417">ond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&amp;T acquire TMobile -+ AT&amp;T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y -+ X purchase Y’, while ‘Children a</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns with semantic types. EMNLP12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3774" citStr="Pantel et al., 2007" startWordPosition="572" endWordPosition="575">an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&amp;T acquire TMobile -+ AT&amp;T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y -+ X purchase Y’, while ‘Children acquire skills -+ Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the gi</context>
<context position="10762" citStr="Pantel et al., 2007" startWordPosition="1666" endWordPosition="1669">former matches a more frequent sense of acquire in a typical corpus. Following this observation, it is desired to obtain a context-sensitive reliability score for each rule application in a given context, as described next. 2.2 Context-sensitive Rule Applications To assess the reliability of applying an inference rule in a given context we need some model for context representation, that should affect the rule reliability score. A major trend in past work is to represent contexts in a reduced-dimensionality latent or class-based model. A couple of earlier works utilized a cluster-based model (Pantel et al., 2007) and an LSA-based model (Szpektor et al., 2008), in a selectional-preferences style approach. Several more recent works utilize a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) framework. We now present an underlying unified view of the topic-level models in (Ritter et al., 2010; Dinu and Lapata, 2010b), which we follow in our own model and in comparative model evaluations. We note that a similar LDA model construction was employed also in (S´eaghdha, 2010), for estimating predicate-argument likelihood. First, an LDA model is constructed, as follows. Similar to the construction of argum</context>
<context position="34312" citStr="Pantel et al., 2007" startWordPosition="5470" endWordPosition="5473">idate rule application, the LDA model is used to infer the topic distribution relevant to the context specified by the given arguments. Finally, the contextsensitive rule application score is computed as a weighted average of the per-topic word-level similarity scores, which are weighed according to the inferred topic distribution. While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing contextsensitive predicate inference rules are based on topic models (except for (Pantel et al., 2007), which was outperformed by later models). We therefore focused on comparing the performance of our two-level scheme with state-of-the-art prior topic-level and word-level models of distributional similarity, over a random sample of inference rule applications. Under this natural setting, the twolevel scheme consistently outperformed both types of models when tested with three different base similarity measures. Notably, our model shows stable performance over a large subset of the data 1338 where context sensitivity is rare, while topic-level models tend to underperform in such cases compared</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1277" citStr="Ravichandran and Hovy, 2002" startWordPosition="169" endWordPosition="172">tivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y -+ X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distrib</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="35759" citStr="Reisinger and Mooney, 2010" startWordPosition="5687" endWordPosition="5690">tes and instantiations of their argument slots as context, lexical similarity methods consider various lexical units that are not necessarily predicates, with their context typically being the collection of words in a window around them. Various approaches have been proposed to address lexical similarity. A number of works are based on a compositional semantics approach, where a prior representation of a target lexical unit is composed with the representations of words in its given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010). Other works (Erk and Pad´o, 2010; Reisinger and Mooney, 2010) use a rather large word window around target words and compute similarities between clusters comprising instances of word windows. In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute contextbiased lexical similarities accordingly. Then we will also be able to evalu</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Ritter, Etzioni, 2010</marker>
<rawString>Alan Ritter, Oren Etzioni, et al. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Jesse Davis</author>
<author>Oren Etzioni</author>
<author>Daniel Weld</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2789" citStr="Schoenmackers et al., 2010" startWordPosition="406" endWordPosition="409"> argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&amp;T acquire TMobile -+ AT&amp;T purchase T-Mobile’, is a valid application of the rule ‘X</context>
</contexts>
<marker>Schoenmackers, Davis, Etzioni, Weld, 2010</marker>
<rawString>Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and Daniel Weld. 2010. Learning first-order horn clauses from web text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between ne pairs.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</booktitle>
<contexts>
<context position="24775" citStr="Sekine (2005)" startWordPosition="3940" endWordPosition="3941">s data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X, and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y’. Yet, many of these templates share a similar meaning, e.g. ‘X accommodate up to Y’, ‘X can accommodate up to Y’, ‘X will accommodate up to Y’, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word cooccurrence statistics per predicate. Next, we applied some clean-up preprocessing to the ReVerb extractions. This includes discarding stop words, rare words and non-alphabetical words instantiating either the X or the Y arguments. In addition, we discarded all predicates that co-occur with less than 100 unique argument words in each slot. The remaining corpus consists of 7 million unique extractions and 2,155 verb predicates. Finally, w</context>
</contexts>
<marker>Sekine, 2005</marker>
<rawString>Satoshi Sekine. 2005. Automatic paraphrase discovery based on context and keywords between ne pairs. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</booktitle>
<contexts>
<context position="1337" citStr="Shinyama and Sekine, 2006" startWordPosition="177" endWordPosition="180">and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y -+ X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by </context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2563" citStr="Szpektor and Dagan, 2008" startWordPosition="374" endWordPosition="377">he highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an infer</context>
<context position="9324" citStr="Szpektor and Dagan, 2008" startWordPosition="1444" endWordPosition="1447">s. However, our contextsensitive scheme can be applied to any arity. (1) 1332 measure from (Lin, 1998), defined as follows: Lin (v, v&apos;) = EwEvnv&apos; [v (w) + v&apos; (w)] (2) EwEvUv&apos; [v(w) + v&apos;(w)] We note that the general DIRT scheme may be used while employing other “base” vector similarity measures. For example, the Lin measure is symmetric, and thus using it would yield the same reliability score when swapping the two sides of a rule. This issue has been addressed in a separate line of research which introduced directional similarity measures suitable for inference relations (Bhagat et al., 2007; Szpektor and Dagan, 2008; Kotlerman et al., 2010). In our experiments we apply our proposed context-sensitive similarity scheme over three different base similarity measures. DIRT and similar context-insensitive inference methods provide a single reliability score for a learned inference rule, which aims to predict the validity of the rule’s applications. However, as exemplified in the Introduction, an inference rule may be valid in some contexts but invalid in others (e.g. acquiring entails purchasing for goods, but not for skills). Since vector similarity in DIRT is computed over the single aggregate argument vecto</context>
<context position="23742" citStr="Szpektor and Dagan, 2008" startWordPosition="3769" endWordPosition="3772">oted by WT. In addition, we evaluated two variants of the topic-level contextsensitive model, denoted DC and SC. DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al., 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rulesets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. The Lin similarity measure is described in Equation 2. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpektor and Dagan, 2008). To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al., 2011), a large scale publicly available webbased open extractions data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb pr</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
<author>Roy Bar-Haim</author>
<author>Jacob Goldberger</author>
</authors>
<title>Contextual preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="3797" citStr="Szpektor et al., 2008" startWordPosition="576" endWordPosition="579"> depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&amp;T acquire TMobile -+ AT&amp;T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y -+ X purchase Y’, while ‘Children acquire skills -+ Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a</context>
<context position="10809" citStr="Szpektor et al., 2008" startWordPosition="1674" endWordPosition="1677">re in a typical corpus. Following this observation, it is desired to obtain a context-sensitive reliability score for each rule application in a given context, as described next. 2.2 Context-sensitive Rule Applications To assess the reliability of applying an inference rule in a given context we need some model for context representation, that should affect the rule reliability score. A major trend in past work is to represent contexts in a reduced-dimensionality latent or class-based model. A couple of earlier works utilized a cluster-based model (Pantel et al., 2007) and an LSA-based model (Szpektor et al., 2008), in a selectional-preferences style approach. Several more recent works utilize a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) framework. We now present an underlying unified view of the topic-level models in (Ritter et al., 2010; Dinu and Lapata, 2010b), which we follow in our own model and in comparative model evaluations. We note that a similar LDA model construction was employed also in (S´eaghdha, 2010), for estimating predicate-argument likelihood. First, an LDA model is constructed, as follows. Similar to the construction of argument vectors in the distributional model (descri</context>
</contexts>
<marker>Szpektor, Dagan, Bar-Haim, Goldberger, 2008</marker>
<rawString>Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Goldberger. 2008. Contextual preferences. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Zeichner</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>Crowdsourcing inference-rule evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="26301" citStr="Zeichner et al. (2012)" startWordPosition="4183" endWordPosition="4187">larity score for each learned rule under each LDA topic, as specified in our context-sensitive model. We release a rule set comprising the top 500 context-sensitive rules that we learned for each of the verb predicates in our learning corpus, along with our trained LDA 3ReVerb is available at http://reverb.cs. washington.edu/ 1336 Method Lin BInc Cosine Valid 266 254 272 Invalid 545 523 539 Total 811 777 811 Table 3: Sizes of rule application test set for each learned rule-set. model.4 4.2 Evaluation Task To evaluate the performance of the different methods we chose the dataset constructed by Zeichner et al. (2012). 5 This publicly available dataset contains about 6,500 manually annotated predicate template rule applications, each one labeled as correct or incorrect. For example, ‘Jack agree with Jill --/+ Jack feel sorry for Jill’ is a rule application in this dataset, labeled as incorrect, and ‘Registration open this month → Registration begin this month’ is another rule application, labeled as correct. Rule applications were generated by randomly sampling extractions from ReVerb, such as (‘Jack’,‘agree with’,‘Jill’) and then sampling possible rules for each, such as ‘agree with → feel sorry for’. Hen</context>
</contexts>
<marker>Zeichner, Berant, Dagan, 2012</marker>
<rawString>Naomi Zeichner, Jonathan Berant, and Ido Dagan. 2012. Crowdsourcing inference-rule evaluation. In Proceedings ofACL (short papers).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>