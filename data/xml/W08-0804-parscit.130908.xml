<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.130791">
<title confidence="0.978727">
Small Statistical Models by Random Feature Mixing
</title>
<author confidence="0.989215">
Kuzman Ganchev and Mark Dredze
</author>
<affiliation confidence="0.9994075">
Department of Computer and Information Science
University of Pennsylvania, Philadelphia, PA
</affiliation>
<email confidence="0.997593">
{kuzman,mdredze}@cis.upenn.edu
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989885">
The application of statistical NLP systems to
resource constrained devices is limited by the
need to maintain parameters for a large num-
ber of features and an alphabet mapping fea-
tures to parameters. We introduce random
feature mixing to eliminate alphabet storage
and reduce the number of parameters without
severely impacting model performance.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998383">
Statistical NLP learning systems are used for many
applications but have large memory requirements, a
serious problem for mobile platforms. Since NLP
applications use high dimensional models, a large
alphabet is required to map between features and
model parameters. Practically, this means storing
every observed feature string in memory, a pro-
hibitive cost for systems with constrained resources.
Offline feature selection is a possible solution, but
still requires an alphabet and eliminates the poten-
tial for learning new features after deployment, an
important property for adaptive e-mail or SMS pre-
diction and personalization tasks.
We propose a simple and effective approach to
eliminate the alphabet and reduce the problem of di-
mensionality through random feature mixing. We
explore this method on a variety of popular datasets
and classification algorithms. In addition to alpha-
bet elimination, this reduces model size by a factor
of 5–10 without a significant loss in performance.
</bodyText>
<sectionHeader confidence="0.970649" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.9999626">
Linear models learn a weight vector over features
constructed from the input. Features are constructed
as strings (e.g. “w=apple” interpreted as “contains
the word apple”) and converted to feature indices
maintained by an alphabet, a map from strings to
integers. Instances are efficiently represented as a
sparse vector and the model as a dense weight vec-
tor. Since the alphabet stores a string for each fea-
ture, potentially each unigram or bigram it encoun-
ters, it is much larger than the weight vector.
Our idea is to replace the alphabet with a random
function from strings to integers between 0 and an
intended size. This size controls the number of pa-
rameters in our model. While features are now eas-
ily mapped to model parameters, multiple features
can collide and confuse learning. The collision rate
is controlled by the intended size. Excessive colli-
sions can make the learning problem more difficult,
but we show significant reductions are still possible
without harming learning. We emphasize that even
when using an extremely large feature space to avoid
collisions, alphabet storage is eliminated. For the
experiments in this paper we use Java’s hashCode
function modulo the intended size rather than a ran-
dom function.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="acknowledgments">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9997734">
We evaluated the effect of random feature mix-
ing on four popular learning methods: Perceptron,
MIRA (Crammer et al., 2006), SVM and Maximum
entropy; with 4 NLP datasets: 20 Newsgroups1,
Reuters (Lewis et al., 2004), Sentiment (Blitzer
et al., 2007) and Spam (Bickel, 2006). For each
dataset we extracted binary unigram features and
sentiment was prepared according to Blitzer et al.
(2007). From 20 Newsgroups we created 3 binary
decision tasks to differentiate between two similar
</bodyText>
<footnote confidence="0.991718">
1http://people.csail.mit.edu/jrennie/20Newsgroups/
</footnote>
<page confidence="0.97174">
19
</page>
<note confidence="0.7216985">
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19–20,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999513666666667">
Figure 1: Kitchen appliance reviews. Left: Maximum en-
tropy. Right: Perceptron. Shaded area and vertical lines
extend one standard deviation from the mean.
</figureCaption>
<bodyText confidence="0.999787481481481">
labels from computers, science and talk. We cre-
ated 3 similar problems from Reuters from insur-
ance, business services and retail distribution. Senti-
ment used 4 Amazon domains (book, dvd, electron-
ics, kitchen). Spam used the three users from task
A data. Each problem had 2000 instances except for
20 Newsgroups, which used between 1850 and 1971
instances. This created 13 binary classification prob-
lems across four tasks. Each model was evaluated
on all problems using 10-fold cross validation and
parameter optimization. Experiments varied model
size to observe the effect of feature collisions on per-
formance.
Results for sentiment classification of kitchen ap-
pliance reviews (figure 1) are typical. The original
model has roughly 93.6k features and its alphabet
requires 1.3MB of storage. Assuming 4-byte float-
ing point numbers the weight vector needs under
0.37MB. Consequently our method reduces storage
by over 78% when we keep the number of param-
eters constant. A further reduction by a factor of 2
decreases accuracy by only 2%.
Figure 2 shows the results of all experiments
for SVM and MIRA. Each curve shows normalized
dataset performance relative to the full model as the
percentage of original features decrease. The shaded
rectangle extends one standard deviation above and
</bodyText>
<figure confidence="0.928947142857143">
1.02
1
0.98
0.96
0.94
0 0.5 1 1.5 2 0 0.5 1 1.5 2
Relative # features Relative # features
</figure>
<figureCaption confidence="0.9974365">
Figure 2: Relative performance on all datasets for SVM
(left) and MIRA (right).
Figure 3: The anomalous Reuters dataset from figure 2
for Perceptron (left) and MIRA (right).
</figureCaption>
<bodyText confidence="0.99998288">
below full model performance. Almost all datasets
perform within one standard deviation of the full
model when using feature mixing set to the total
number of features for the problem, indicating that
alphabet elimination is possible without hurting per-
formance. One dataset (Reuters retail distribution) is
a notable exception and is illustrated in detail in fig-
ure 3. We believe the small total number of features
used for this problem is the source of this behavior.
On the vast majority of datasets, our method can re-
duce the size of the weight vector and eliminate the
alphabet without any feature selection or changes to
the learning algorithm. When reducing weight vec-
tor size by a factor of 10, we still obtain between
96.7% and 97.4% of the performance of the original
model, depending on the learning algorithm. If we
eliminate the alphabet but keep the same size weight
vector, model the performance is between 99.3%
of the original for MIRA and a slight improvement
for Perceptron. The batch learning methods are be-
tween those two extremes at 99.4 and 99.5 for max-
imum entropy and SVM respectively. Feature mix-
ing yields substantial reductions in memory require-
ments with a minimal performance loss, a promising
result for resource constrained devices.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756818181818">
S. Bickel. 2006. Ecml-pkdd discovery challenge
overview. In The Discovery Challenge Workshop.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Ressearch, 7.
D. D. Lewis, Y. Yand, T. Rose, and F. Li. 2004. Rcv1:
A new benchmark collection for text categorization re-
search. JMLR, 5:361–397.
</reference>
<figure confidence="0.9601196">
1.02
1
0.98
0.96
0.94
</figure>
<page confidence="0.571497">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875236">
<title confidence="0.997976">Small Statistical Models by Random Feature Mixing</title>
<author confidence="0.899239">Ganchev</author>
<affiliation confidence="0.9903965">Department of Computer and Information University of Pennsylvania, Philadelphia,</affiliation>
<abstract confidence="0.999197333333333">The application of statistical NLP systems to resource constrained devices is limited by the need to maintain parameters for a large number of features and an alphabet mapping features to parameters. We introduce random feature mixing to eliminate alphabet storage and reduce the number of parameters without severely impacting model performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bickel</author>
</authors>
<title>Ecml-pkdd discovery challenge overview.</title>
<date>2006</date>
<booktitle>In The Discovery Challenge Workshop.</booktitle>
<contexts>
<context position="3096" citStr="Bickel, 2006" startWordPosition="477" endWordPosition="478">ult, but we show significant reductions are still possible without harming learning. We emphasize that even when using an extremely large feature space to avoid collisions, alphabet storage is eliminated. For the experiments in this paper we use Java’s hashCode function modulo the intended size rather than a random function. 3 Experiments We evaluated the effect of random feature mixing on four popular learning methods: Perceptron, MIRA (Crammer et al., 2006), SVM and Maximum entropy; with 4 NLP datasets: 20 Newsgroups1, Reuters (Lewis et al., 2004), Sentiment (Blitzer et al., 2007) and Spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007). From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar 1http://people.csail.mit.edu/jrennie/20Newsgroups/ 19 Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19–20, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Kitchen appliance reviews. Left: Maximum entropy. Right: Perceptron. Shaded area and vertical lines extend one standard deviation from the mean. labels from comp</context>
</contexts>
<marker>Bickel, 2006</marker>
<rawString>S. Bickel. 2006. Ecml-pkdd discovery challenge overview. In The Discovery Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3072" citStr="Blitzer et al., 2007" startWordPosition="471" endWordPosition="474">the learning problem more difficult, but we show significant reductions are still possible without harming learning. We emphasize that even when using an extremely large feature space to avoid collisions, alphabet storage is eliminated. For the experiments in this paper we use Java’s hashCode function modulo the intended size rather than a random function. 3 Experiments We evaluated the effect of random feature mixing on four popular learning methods: Perceptron, MIRA (Crammer et al., 2006), SVM and Maximum entropy; with 4 NLP datasets: 20 Newsgroups1, Reuters (Lewis et al., 2004), Sentiment (Blitzer et al., 2007) and Spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007). From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar 1http://people.csail.mit.edu/jrennie/20Newsgroups/ 19 Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19–20, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Kitchen appliance reviews. Left: Maximum entropy. Right: Perceptron. Shaded area and vertical lines extend one standard deviation from th</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Ressearch,</journal>
<volume>7</volume>
<contexts>
<context position="2946" citStr="Crammer et al., 2006" startWordPosition="451" endWordPosition="454">atures can collide and confuse learning. The collision rate is controlled by the intended size. Excessive collisions can make the learning problem more difficult, but we show significant reductions are still possible without harming learning. We emphasize that even when using an extremely large feature space to avoid collisions, alphabet storage is eliminated. For the experiments in this paper we use Java’s hashCode function modulo the intended size rather than a random function. 3 Experiments We evaluated the effect of random feature mixing on four popular learning methods: Perceptron, MIRA (Crammer et al., 2006), SVM and Maximum entropy; with 4 NLP datasets: 20 Newsgroups1, Reuters (Lewis et al., 2004), Sentiment (Blitzer et al., 2007) and Spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007). From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar 1http://people.csail.mit.edu/jrennie/20Newsgroups/ 19 Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19–20, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Kitchen app</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Ressearch, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yand</author>
<author>T Rose</author>
<author>F Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>JMLR,</journal>
<pages>5--361</pages>
<contexts>
<context position="3038" citStr="Lewis et al., 2004" startWordPosition="466" endWordPosition="469">. Excessive collisions can make the learning problem more difficult, but we show significant reductions are still possible without harming learning. We emphasize that even when using an extremely large feature space to avoid collisions, alphabet storage is eliminated. For the experiments in this paper we use Java’s hashCode function modulo the intended size rather than a random function. 3 Experiments We evaluated the effect of random feature mixing on four popular learning methods: Perceptron, MIRA (Crammer et al., 2006), SVM and Maximum entropy; with 4 NLP datasets: 20 Newsgroups1, Reuters (Lewis et al., 2004), Sentiment (Blitzer et al., 2007) and Spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007). From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar 1http://people.csail.mit.edu/jrennie/20Newsgroups/ 19 Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19–20, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Kitchen appliance reviews. Left: Maximum entropy. Right: Perceptron. Shaded area and vertical lines ext</context>
</contexts>
<marker>Lewis, Yand, Rose, Li, 2004</marker>
<rawString>D. D. Lewis, Y. Yand, T. Rose, and F. Li. 2004. Rcv1: A new benchmark collection for text categorization research. JMLR, 5:361–397.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>