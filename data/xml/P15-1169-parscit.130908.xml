<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.994236">
An analysis of the user occupational class through Twitter content
</title>
<author confidence="0.993722">
Daniel Preot¸iuc-Pietro1, Vasileios Lampos2 and Nikolaos Aletras2
</author>
<affiliation confidence="0.954195">
1 Computer &amp; Information Science, University of Pennsylvania
2 Department of Computer Science, University College London
</affiliation>
<email confidence="0.993717">
danielpr@sas.upenn.edu, {v.lampos,n.aletras}@ucl.ac.uk
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862590909091">
Social media content can be used as a
complementary source to the traditional
methods for extracting and studying col-
lective social attributes. This study focuses
on the prediction of the occupational class
for a public user profile. Our analysis is
conducted on a new annotated corpus of
Twitter users, their respective job titles,
posted textual content and platform-related
attributes. We frame our task as classifi-
cation using latent feature representations
such as word clusters and embeddings. The
employed linear and, especially, non-linear
methods can predict a user’s occupational
class with strong accuracy for the coars-
est level of a standard occupation taxon-
omy which includes nine classes. Com-
bined with a qualitative assessment, the
derived results confirm the feasibility of
our approach in inferring a new user at-
tribute that can be embedded in a multitude
of downstream applications.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940642857143">
The growth of online social networks provides the
opportunity to analyse user text in a broader context
(Tumasjan et al., 2010; Bollen et al., 2011; Lam-
pos and Cristianini, 2012). This includes the social
network (Sadilek et al., 2012), spatio-temporal in-
formation (Lampos and Cristianini, 2010) and per-
sonal attributes (Al Zamal et al., 2012). Previous
research has analysed language differences in user
attributes like location (Cheng et al., 2010), gender
(Burger et al., 2011), impact (Lampos et al., 2014)
and age (Rao et al., 2010), showing that language
use is influenced by them. Therefore, user text al-
lows us to infer these properties. This user profiling
is important not only for sociolinguistic studies, but
also for other applications: recommender systems
to provide targeted advertising, analysts who study
different opinions in each social class or integra-
tion in text regression tasks such as voting intention
(Lampos et al., 2013).
Social status reflected through a person’s occu-
pation is a factor which influences language use
(Bernstein, 1960; Bernstein, 2003; Labov, 2006).
Therefore, our hypothesis is that language use in
social media can be indicative of a user’s occu-
pational class. For example, executives may write
more frequently about business or financial news,
while people in manufacturing positions could re-
fer more to their personal interests and less to job
related activities. Similarly, we expect some cate-
gories of people, like those working in sales and
customer services, to be more social or to use more
informal language.
Focusing on the microblogging platform of Twit-
ter, we explore our hypothesis by studying the
task of predicting a user’s occupational class given
platform-related attributes and generated content,
i.e. tweets. That has direct applicability in a broad
range of areas from sociological studies, which
analyse the behaviour of different occupations, to
recruiting companies that target people for new job
opportunities. For this study, we created a publicly
available data set of users, including their profile
information and historical text content as well as
a label to an occupational class from the “Stan-
dard Occupational Classification” taxonomy (see
Section 2).
We frame our task as classification, aiming to
identify the most likely job class for a given user
based on profile and a variety of textual features:
general word embeddings and clusters (or ‘topics’).
Both linear and non-linear classification methods
are applied with a focus on those that can assist in-
terpretation and offer qualitative insights. We find
that text features, especially word clusters, lead
to good predictive performance. Accuracy for our
best model is well above 50% for 9-way classifi-
</bodyText>
<page confidence="0.946744">
1754
</page>
<note confidence="0.976584666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1754–1764,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999686444444444">
cation, outperforming competitive methods. The
best results are obtained using the Bayesian non-
parametric framework of Gaussian Processes (Ras-
mussen and Williams, 2006), which also accom-
modates feature interpretation via the Automatic
Relevance Determination. This allows us to get in-
sight into differences in language use across job
classes and, finally, assess our original hypothesis
about the thematic divergence across them.
</bodyText>
<sectionHeader confidence="0.927611" genericHeader="introduction">
2 Standard Occupational Classification
</sectionHeader>
<bodyText confidence="0.99999004">
To enable the user occupation study, we adopt a
standardised job classification taxonomy for map-
ping Twitter users to occupations. The Standard Oc-
cupational Classification (SOC)1 is a UK govern-
ment system developed by the Office of National
Statistics for classifying occupations. Jobs are cate-
gorised hierarchically based on skill requirements
and content. The SOC scheme includes nine major
groups coded with a digit from 1 to 9. Each ma-
jor group is divided into sub-major groups coded
with 2 digits, where the first digit indicates the ma-
jor group. Each sub-major group is further divided
into minor groups coded with 3 digits and finally,
minor groups are divided into unit groups, coded
with 4 digits. The unit groups are the leaves of the
hierarchy and represent specific jobs related to the
group.
Table 1 shows a part of the SOC hierarchy. In to-
tal, there are 9 major groups, 25 sub-major groups,
90 minor groups and 369 unit groups. Although
other hierarchies exist, we use the SOC because
it has been published recently (in 2010), includes
newly introduced jobs, has a balanced hierarchy
and offers a wide variety of job titles that were
crucial in our data set creation.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999937">
To the best of our knowledge there are no pub-
licly available data sets suitable for the task we
aim to investigate. Thus, we have created a new
one consisting of Twitter users mapped to their oc-
cupation, together with their profile information
and historical tweets. We use the account’s profile
information to capture users with self-disclosed
occupations. The potential self-selection bias is ac-
knowledged, but filtering content via self disclosure
</bodyText>
<footnote confidence="0.98538475">
1http://www.ons.gov.uk/ons/
guide-method/classifications/
current-standard-classifications/
soc2010/index.html; accessed on 24/02/2015.
</footnote>
<table confidence="0.972548266666667">
Major Group 1 (C1): Managers, Directors and Senior Officials
Sub-major Group 11: Corporate Managers and Directors
Minor Group 111: Chief Executives and Senior Officials
Unit Group 1115: Chief Executives and Senior Officials
•Job: chief executive, bank manager
Unit Group 1116: Elected Officers and Representatives
Minor Group 112: Production Managers and Directors
Minor Group 113: Functional Managers and Directors
Minor Group 115: Financial Institution Managers and Directors
Minor Group 116: Managers and Directors in Transport and Logistics
Minor Group 117: Senior Officers in Protective Services
Minor Group 118: Health and Social Services Managers and Directors
Minor Group 119: Managers and Directors in Retail and Wholesale
Sub-major Group 12: Other Managers and Proprietors
Major Group (C2): Professional Occupations
•Job: mechanical engineer, pediatrist
Major Group (C3): Associate Professional and Technical Occupations
•Job: system administrator, dispensing optician
Major Group (C4): Administrative and Secretarial Occupations
•Job: legal clerk, company secretary
Major Group (C5): Skilled Trades Occupations
•Job: electrical fitter, tailor
Major Group (C6): Caring, Leisure and Other Service Occupations
•Job: nursery assistant, hairdresser
Major Group (C7): Sales and Customer Service Occupations
•Job: sales assistant, telephonist
Major Group (C8): Process, Plant and Machine Operatives
•Job: factory worker, van driver
Major Group (C9): Elementary Occupations
•Job: shelf stacker, bartender
</table>
<tableCaption confidence="0.999791">
Table 1: Subset of the SOC classification hierarchy.
</tableCaption>
<bodyText confidence="0.999065551724138">
is widespread when extracting large-scale data for
user attribute inference (Pennacchiotti and Popescu,
2011; Coppersmith et al., 2014).
Similarly to Hecht et al. (2011), we first assess
the proportion of Twitter accounts with a clear men-
tion to their occupation by annotating the user de-
scription field of a random set of 500 users. There
were chosen from the random 1% sample, having at
least 200 tweets in their history and with a majority
of English tweets. There, we can identify the fol-
lowing categories: no description (12.2%), random
information (22%), user information but not occu-
pation related (45.8%), and job related information
(20%).
To create our data set, we thus use the user de-
scription field to search for self-disclosed job titles
provided by the 4-digit SOC unit groups, since
they contain specific job titles. We queried Twit-
ter’s Search API to retrieve for each job title a max-
imum of 200 accounts which best matched occupa-
tion keywords. Then, we aggregated the accounts
into the 3-digit (minor) categories. To remove po-
tential ambiguity in the retrieved set, we manually
inspected accounts in each minor category and fil-
tered out those that belong to companies, contain
no description or the description provided does not
indicate that the user has a job corresponding to
the minor category. In total, around 50% of the
accounts were removed by manual inspection per-
</bodyText>
<page confidence="0.978533">
1755
</page>
<bodyText confidence="0.999957105263158">
formed by the authors. We also removed users in
multiple categories and or users that have tweeted
less than 50 times in their history. Finally, we elim-
inated all 3-digit categories that contained less than
45 user accounts after this filtering. This process
produced a total number of 5,191 users from 55 mi-
nor groups (22 sub-major groups), spread across all
nine major SOC groups. The distribution of users
across these nine groups is: 9.7%, 34.5%, 20.6%,
3.8%, 16.7%, 6.1%, 1.4%, 4.2%, and 3% (follow-
ing the ordering of Table 1). In our data set the
most well represented minor occupational groups
are ‘Functional Managers and Directors’ (184 users
– code 113), ‘Therapy Professionals’ (159 users –
code 222) and ‘Quality and Regulatory Profession-
als’ (158 users – code 246), whereas the least rep-
resented ones are ‘Textile and Garment Trades’ (45
users – code 541), ‘Elementary Security Occupa-
tions’ (46 users – code 924), ‘Elementary Cleaning
Occupations’ (47 users – code 923). The mean num-
ber of users in the minor classes is equal to 94.4
with a standard deviation of 35.6. For these users,
we have collected all their tweets, going as far back
as the latest 3,200, and their profile information.
The final data set consists of 10,796,836 tweets col-
lected around 5 August 2014 and is openly avail-
able.2
A separate Twitter data set is used as a reference
corpus in order to build the feature representations
detailed in Section 4. This data set is an extract
from the Twitter Gardenhose stream (a 10% repre-
sentative sample of the entire Twitter stream) from
2 January to 28 February 2011. Based on this con-
tent, we also build the vocabulary for the text fea-
tures, containing the most frequent 71,555 words.
We tokenise and filter for English using the Trend-
miner preprocessing pipeline (Preot¸iuc-Pietro et al.,
2012).
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.99975425">
In this section, we overview the features used in
the occupational class prediction task. They are
divided into two types: (1) user level features, (2)
textual features.
</bodyText>
<subsectionHeader confidence="0.995056">
4.1 User Level Features (UserLevel)
</subsectionHeader>
<bodyText confidence="0.999817333333333">
The user level features are based on the general
user information or aggregated statistics about the
tweets. Table 2 introduces the 18 features in this
</bodyText>
<footnote confidence="0.9109795">
2http://www.sas.upenn.edu/˜danielpr/
jobs.tar.gz
</footnote>
<bodyText confidence="0.981869666666667">
number of followers
number of friends
number of times listed
follower/friend ratio
proportion of non-duplicate tweets
proportion of retweeted tweets
average no. of retweets/tweet
proportion of retweets done
proportion of hashtags
proportion of tweets with hashtags
proportion of tweets with @-mentions
proportion of @-replies
no. of unique @-mentions in tweets
proportion of tweets with links
no. of favourites the account made
avg. number of tweets/day
total number of tweets
proportion of tweets in English
</bodyText>
<tableCaption confidence="0.810106">
Table 2: User level attributes for a Twitter user.
category.
</tableCaption>
<subsectionHeader confidence="0.985836">
4.2 Textual Features
</subsectionHeader>
<bodyText confidence="0.999976444444445">
The textual features are derived from the aggre-
gated set of user’s tweets. We use our reference
corpus to represent each user as a distribution over
these features. We ignore the bio field from build-
ing textual features to avoid introducing biases
from our data collection method. While this is a re-
striction, our analysis showed that in less than 20%
of the cases the information in the bio is directly
relevant to the occupation.
</bodyText>
<subsectionHeader confidence="0.684385">
4.2.1 SVD Word Embeddings (SVD-E)
</subsectionHeader>
<bodyText confidence="0.9998996">
We use a more abstract representation of words
than simple unigram counts in order to aid inter-
pretability of our analysis. We compute a word
to word similarity matrix from our reference cor-
pus. Normalised Pointwise Mutual Information
(NPMI) (Bouma, 2009) is used to compute word to
word similarity. NPMI is an information theoretic
measure indicating which words co-occur in the
same context, where the context is represented by
a whole tweet:
</bodyText>
<equation confidence="0.977476333333333">
P(x,y)
NPMI(x, y) = − log P(x, y) · log P(x) · P(y).
(1)
</equation>
<bodyText confidence="0.9991265">
We then perform singular value decomposition
(SVD) on the word to word similarity matrix and
obtain an embedding of words into a low dimen-
sional space. In our experiments we tried the fol-
lowing dimensionalities: 30, 50, 100 and 200. The
feature representation for each user is obtained
summing over each of the embedding dimensions
across all words.
</bodyText>
<figure confidence="0.997138833333333">
u1
u2
u3
u4
u5
u6
u7
u8
uy
u10
u11
u12
u13
u14
u15
u16
u17
u18
</figure>
<page confidence="0.890868">
1756
</page>
<subsubsectionHeader confidence="0.76877">
4.2.2 NPMI Clusters (SVD-C)
</subsubsectionHeader>
<bodyText confidence="0.99999175">
We use the NPMI matrix described in the previous
paragraph to create hard clusters of words. These
clusters can be thought as ‘topics’, i.e. words that
are semantically similar. From a variety of cluster-
ing techniques we choose spectral clustering (Shi
and Malik, 2000; Ng et al., 2002), a hard-clustering
approach which deals well with high-dimensional
and non-convex data (von Luxburg, 2007). Spectral
clustering is based on applying SVD to the graph
Laplacian and aims to perform an optimal graph
partitioning on the NPMI similarity matrix. The
number of clusters needs to be pre-specified. We
use 30, 50, 100 and 200 clusters – numbers were
chosen a priori based on previous work (Lampos
et al., 2014). The feature representation is the stan-
dardised number of words from each cluster.
Although there is a loss of information compared
to the original representation, the clusters are very
useful in the model analysis step. Embeddings are
hard to interpret because each dimension is an ab-
stract notion, while the clusters can be interpreted
by presenting a list of the most frequent or repre-
sentative words. The latter are identified using the
following centrality metric:
</bodyText>
<equation confidence="0.9989885">
C,,, = ExEc NPMI(w, x) ,(2)
JcJ — 1
</equation>
<bodyText confidence="0.99973">
where c denotes the cluster and w the target word.
</bodyText>
<subsubsectionHeader confidence="0.483127">
4.2.3 Neural Embeddings (W2V-E)
</subsubsectionHeader>
<bodyText confidence="0.998702470588235">
Recently, there has been a growing interest in neu-
ral language models, where the words are projected
into a lower dimensional dense vector space via a
hidden layer (Mikolov et al., 2013b). These models
showed they can provide a better representation
of words compared to traditional language models
(Mikolov et al., 2013c) because they capture syntac-
tic information rather than just bag-of-context, han-
dling non-linear transformations. In this low dimen-
sional vector space, words with a small distance are
considered semantically similar. We use the skip-
gram model with negative sampling (Mikolov et al.,
2013a) to learn word embeddings on the Twitter
reference corpus. In that case, the skip-gram model
is factorising a word-context PMI matrix (Levy and
Goldberg, 2014). We use a layer size of 50 and the
Gensim implementation.3
</bodyText>
<footnote confidence="0.9825715">
3http://radimrehurek.com/gensim/
models/word2vec.html
</footnote>
<subsubsectionHeader confidence="0.811429">
4.2.4 Neural Clusters (W2V-C)
</subsubsectionHeader>
<bodyText confidence="0.999949833333333">
Similar to the NPMI cluster, we use the neural
embeddings in order to obtain clusters of related
words, i.e. ‘topics’. We derive a word to word simi-
larity matrix using cosine similarity on the neural
embeddings. We apply spectral clustering on this
matrix to obtain 30, 50, 100 and 200 word clusters.
</bodyText>
<sectionHeader confidence="0.855914" genericHeader="method">
5 Classification with Gaussian Processes
</sectionHeader>
<bodyText confidence="0.998714307692308">
In this section, we briefly overview Gaussian Pro-
cess (GP) for classification, highlighting our mo-
tivation for using this method. GPs formulate a
Bayesian non-parametric machine learning frame-
work which defines a prior on functions (Ras-
mussen and Williams, 2006). The properties of
the functions are given by a kernel which models
the covariance in the response values as a function
of its inputs. Although GPs form a powerful learn-
ing tool, they have only recently been used in NLP
research (Cohn and Specia, 2013; Preoiiuc-Pietro
and Cohn, 2013) with classification applications
limited to (Polajnar et al., 2011).
</bodyText>
<equation confidence="0.7482745">
Formally, GP methods aim to learn a function
f : Rd —* R drawn from a GP prior given the
inputs x E Rd:
f(x) — 9P(m(x), k(x,x&apos;)) , (3)
</equation>
<bodyText confidence="0.9999456">
where m(·) is the mean function (here 0) and k(·, ·)
is the covariance kernel. Usually, the Squared Ex-
ponential (SE) kernel (a.k.a. RBF or Gaussian) is
used to encourage smooth functions. For the multi-
dimensional pair of inputs (x,x&apos;), this is:
</bodyText>
<equation confidence="0.946635">
� d
_(xz — xi)2
2l?
z
</equation>
<bodyText confidence="0.949160785714286">
where lz are lengthscale parameters learnt only
using training data by performing gradient as-
cent on the type-II marginal likelihood. Intuitively,
the lengthscale parameter lz controls the variation
along the i input dimension, i.e. a low value makes
the output very sensitive to input data, thus mak-
ing that input more useful for the prediction. If the
lengthscales are learnt separately for each input
dimension the kernel is named SE with Automatic
Relevance Determination (ARD) (Neal, 1996).
Binary classification using GPs
the
real valued latent function
output through a
</bodyText>
<equation confidence="0.707046666666667">
logistic function:
°= P(y =
=
</equation>
<bodyText confidence="0.9995605">
in a similar way to logistic regression classification.
The object of the GP inference is the distri
</bodyText>
<equation confidence="0.976015375">
‘squashes’
f(x)
π(x)
1Jx)
σ(f(x))
bution
kard(x,x,) = σ2 exp
, (4)
</equation>
<page confidence="0.514092">
1757
</page>
<bodyText confidence="0.5336735">
of the latent variable corresponding to a test case
x*:
</bodyText>
<equation confidence="0.98712">
�P(f*|x,y,x*) = P(f*|x, x*,f)P(f|x,y)df ,
</equation>
<bodyText confidence="0.93625475">
(5)
where P(f|x,y) = P(y|f)P(f|x)/P(y|x) is the
posterior over the latent variables. If the likelihood
P(y|f) is Gaussian, the combination with a GP
prior P(f|x) gives a posterior GP over functions.
In binary classification, the distribution over the
latent f* is combined with the logistic function to
produce the prediction:
</bodyText>
<equation confidence="0.997449">
�¯π* = σ(f*)P(f*|x,y,x*)df*. (6)
</equation>
<bodyText confidence="0.99818875">
This results in a non-Gaussian likelihood in the
posterior formulation and therefore, exact infer-
ence is infeasible for classification models. Multi-
ple approximations exist that make the computa-
tion tractable (Gibbs and Mackay, 1997; Williams
and Barber, 1998; Neal, 1999). In our experiments
we opt to use the Expectation Propagation (EP)
method (Minka, 2001) which approximates the non-
Gaussian joint posterior with a Gaussian one. EP
offers very good empirical results for many differ-
ent likelihoods, although it has no proof of con-
vergence. The complexity for the inference step is
O(n3). Given that our data set is very large and the
number of features is high, we conduct inference
using the fully independent training conditional
(FITC) approximation (Snelson and Ghahramani,
2006) with 500 random inducing points. We refer
the interested reader to Rasmussen and Williams
(2006) for further information on GP classification.
Although we could use multi-class classification
methods, in order to provide insight, we perform a
separate one-vs-all classification for each class and
then determine a label through the occupational
class that has the highest likelihood.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999915142857143">
This section presents the experimental results for
our task. We first compare the accuracy of our clas-
sification methods on held out data using each fea-
ture set and conduct a standard error analysis. We
then use the interpretability of the ARD length-
scales from the GP classifier to further analyse the
relevant features.
</bodyText>
<subsectionHeader confidence="0.992075">
6.1 Predictive Accuracy
</subsectionHeader>
<bodyText confidence="0.978041">
We assign users to one of nine possible classes (see
the ‘Major Groups’ on Table 1) using one set of
</bodyText>
<table confidence="0.999921625">
Feature LR SVM GP
Most frequent class 34.4% 34.4% 34.4%
UserLevel 34.0% 31.5% 34.2%
SVD-E-30 36.3% 35.0% 39.8%
SVD-E-50 36.7% 36.9% 38.6%
SVD-E-100 40.8% 41.9% 40.9%
SVD-E-200 40.0% 43.1% 43.8%
SVD-C-30 36.9% 36.5% 38.2%
SVD-C-50 37.7% 38.3% 40.5%
SVD-C-100 40.4% 42.1% 44.6%
SVD-C-200 44.2% 47.9% 48.2%
W2V-E-50 42.5% 49.0% 48.4%
W2V-C-30 40.0% 46.0% 47.1%
W2V-C-50 42.3% 48.5% 47.9%
W2V-C-100 44.4% 48.7% 51.3%
W2V-C-200 46.9% 51.7% 52.7%
</table>
<tableCaption confidence="0.996524">
Table 3: 9-way classification accuracy on held-out
</tableCaption>
<bodyText confidence="0.997726333333333">
data for our 3 methods. Textual features are ob-
tained using SVD or Word2Vec (W2V). E repre-
sents embeddings, C clusters. The final number
denotes the amount of clusters or the size of the
embedding.
features at a time. Experiments combining features
yielded only minor improvements. We apply com-
mon linear and non-linear methods together with
our proposed GP classifier. The linear method is
logistic regression (LR) with Elastic Net regulari-
sation (Freedman, 2009) and the non-linear one is
formulated by a Support Vector Machine (SVM)
with an RBF kernel (Vapnik, 1998). The accuracy
of our classifiers is measured on held-out data. Our
data set is divided into stratified training (80%),
validation (10%) and testing (10%) sets. The val-
idation set was used to learn the LR and SVM
hyperparameters, while the GP did not use this set
at all. We report results using all three methods and
all feature sets in Table 3.
We first observe that user level features (User-
Level; see Section 4.1) are not useful for predicting
the job class. This finding indicates that general so-
cial behaviour or user impact are likely to be spread
evenly across classes. It also highlights the diffi-
culty of the task and motivates the use of deeper
textual features.
The textual features (see Section 4.2) improve
performance as compared to the most frequent class
baseline. We also notice that the embeddings (SVD-
E and W2V-E) have lower performance than the
clusters (SVD-C and W2V-C) in most of the cases.
This is expected, as adding word vectors to rep-
resent a user’s text may overemphasise common
words. The size of the embedding also increases
performance. The W2V features show better ac-
</bodyText>
<page confidence="0.902672">
1758
</page>
<table confidence="0.9996166">
Rank Topic # Label Topic (most central words; most frequent words) MRR µ(l)
1 116 Arts archival, stencil, canvas, minimalist, illustration, paintings, abstract, designs, .43 1.35
lettering, steampunk; art, design, print, collection, poster, painting, custom, logo,
printing, drawing
2 105 Health chemotherapy, diagnosis, disease, inflammation, diseases, arthritis, symptoms, .20 2.76
patients, mrsa, colitis; risk, cancer, mental, stress, patients, treatment, surgery,
disease, drugs, doctor
3 153 Beauty Care exfoliating, cleanser, hydrating, moisturizer, moisturiser, shampoo, lotions, .19 3.69
serum, moisture, clarins; beauty, natural, dry, skin, massage, plastic, spray,
facial, treatments, soap
4 21 Higher undergraduate, doctoral, academic, students, curriculum, postgraduate, enrolled, .18 3.21
Education master’s, admissions, literacy; students, research, board, student, college,
education, library, schools, teaching, teachers
5 158 Software integrated, data, implementation, integration, enterprise, configuration, .17 3.10
Engineering open-source, cisco, proprietary, avaya; service, data, system, services, access,
security, development, software, testing, standard
7 186 Football bardsley, etherington, gallas, heitinga, assou-ekotto, lescott, pienaar, warnock, .16 3.11
ridgewell, jenas; van, foster, cole, winger, terry, reckons, youngster, rooney,
fielding, kenny
8 124 Corporate consortium, institutional, firm’s, acquisition, enterprises, subsidiary, corp, .15 2.44
telecommunications, infrastructure, partnership; patent, industry, reports, global,
survey, leading, firm, 2015, innovation, financial
9 96 Cooking parmesan, curried, marinated, zucchini, roasted, coleslaw, salad, tomato, spinach, .15 3.00
lentils; recipe, meat, salad, egg, soup, sauce, beef, served, pork, rice
164 Elongated 12 .11 3.47
Words yaaayy, wooooo, woooo, yayyyyy, yaaaaay, yayayaya, yayy, yaaaaaaay,
wooohooo, yaayyy; wait, till, til, yay, ahhh, hoo, woo, woot, whoop, woohoo
16 176 Politics religious, colonialism, christianity, judaism, persecution, fascism, marxism, .08 3.09
nationalism, communism, apartheid; human, culture, justice, religion, democracy,
religious, humanity, tradition, ancient, racism
</table>
<tableCaption confidence="0.987407">
Table 4: Topics, represented by their most central and most frequent 10 words, sorted by their ARD
</tableCaption>
<figureCaption confidence="0.7180252">
lengthscale MRR across the nine GP-based occupation classifiers. µ(l) denotes the average lengthscale
for a topic across these classifiers. Topic labels are manually created.
Figure 1: Confusion matrix of the prediction results.
Rows represent the actual occupational class (C 1–
9) and columns the predicted class.
</figureCaption>
<bodyText confidence="0.999111684210526">
curacy than the SVD on the NPMI matrix. This
is consistent with previous work that showed the
efficiency of word2vec and the ability of those em-
beddings to capture non-linear relationships and
syntactic features (Mikolov et al., 2013a; Mikolov
et al., 2013b; Mikolov et al., 2013c).
LR has a lower performance than the non-linear
methods, especially when using clusters as features.
GPs usually outperform SVMs by a small margin.
However, these offer the advantages of not using
the validation set and the interpretability properties
we highlight in the next section. Although we only
draw our focus on major occupational classes, the
data set allows the study of finer granularities of oc-
cupation classes in future work. For example, pre-
diction performance for sub-major groups reaches
33.9% accuracy (15.6% majority class, 22 classes)
and 29.2% accuracy for minor groups (3.4% major-
ity class, 55 classes).
</bodyText>
<subsectionHeader confidence="0.988248">
6.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999899555555555">
To illustrate the errors made by our classifiers, Fig-
ure 1 shows the confusion matrix of the classi-
fication results. First, we observe that class 4 is
many times classified as class 2 or 3. This can be
explained by the fact that classes 2, 3 and 4 con-
tain similar types of occupations, e.g. doctors and
nurses or accountants and assistant accountants.
However, with very few exceptions, we notice that
only adjacent classes get misclassified, suggesting
</bodyText>
<figure confidence="0.989516285714286">
1 2 3 4 5 6 7 8 9
1
2
3
4
5
6
7
8
9
0.6
0.4
0.2
0.0
</figure>
<page confidence="0.943159">
1759
</page>
<bodyText confidence="0.84628">
that our model captures the general user skill level.
</bodyText>
<subsectionHeader confidence="0.997752">
6.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999947675324676">
The word clusters that were built from a reference
corpus and then used as features in the GP classi-
fier, give us the opportunity to extract some qual-
itative derivations from our predictive task. For
the rest of the section we use the best performing
model of this type (W2V-C-200) in order to anal-
yse the results. Our main assumption is that there
might be a divergence of language and topic us-
age across occupational classes following previous
studies in sociology (Bernstein, 1960; Bernstein,
2003). Knowing that the inferred GP lengthscale
hyperparameters are inversely proportional to fea-
ture (i.e. topic) relevance (see Section 5), we can
use them to rank the topic importance and give
answers to our hypothesis.
Table 4 shows 10 of the most informative top-
ics (represented by the top 10 most central and
frequent words) sorted by their ARD lengthscale
Mean Reciprocal Rank (MRR) (Manning et al.,
2008) across the nine classifiers. Evidently, they
cover a broad range of thematic subjects, includ-
ing potentially work specific topics in different do-
mains such as ‘Corporate’ (Topic #124), ‘Software
Engineering’ (#158), ‘Health’ (#105), ‘Higher Ed-
ucation’ (#21) and ‘Arts’ (#116), as well as topics
covering recreational interests such as ‘Football’
(#186), ‘Cooking’ (#96) and ‘Beauty Care’ (#153).
The highest ranked MRR GP lengthscales only
highlight the topics that are the most discrimina-
tive of the particular learning task, i.e. which topic
used alone would have had the best performance.
To examine the difference in topic usage across
occupations, we illustrate how six topics are cov-
ered by the users of each class. Figure 2 shows the
Cumulative Distribution Functions (CDFs) across
the nine different occupational classes for these six
topics. CDFs indicate the fraction of users having
at least a certain topic proportion in their tweets. A
topic is more prevalent in a class, if the CDF line
leans towards the bottom-right corner of the plot.
‘Higher Education’ (#21) is more prevalent in
classes 1 and 2, but is also discriminative for classes
3 and 4 compared to the rest. This is expected be-
cause the vast majority of jobs in these classes
require a university degree (holds for all of the jobs
in classes 2 and 3) or are actually jobs in higher
education. On the other hand, classes 5 to 9 have a
similar behaviour, tweeting less on this topic. We
also observe that words in ‘Corporate’ (#124) are
used more as the skill required for a job gets higher.
This topic is mainly used by people in classes 1
and 2 and with less extent in classes 3 and 4, in-
dicating that people in these occupational classes
are more likely to use social media for discussions
about corporate business.
There is a clear trend of people with more skilled
jobs to talk about ‘Politics’ (#176). Indeed, highly
ranked politicians and political philosophers are
parts of classes 1 and 2 respectively. Neverthe-
less, this pattern expands to the entire spectrum
of the investigated occupational classes, providing
further proof-of-concept for our methodology, un-
der the assumption that the theme of politics is
more attractive to the higher skilled classes rather
than the lower skilled occupations. By examining
‘Arts’ (#116), we see that it clearly separates class
5, which includes artists, from all others. This topic
appears to be relevant to most of the classifica-
tion tasks and it is ranked first according to the
MRR metric. Moreover, we observe that people
with higher skilled jobs and education (classes 1–3)
post more content about arts. Finally, we examine
two topics containing words that can be used in
more informal occasions, i.e. ‘Elongated Words’
(#164) and ‘Beauty Care’ (#153). We observe a
similar pattern in both topics by which users with
lower skilled jobs tweet more often.
</bodyText>
<equation confidence="0.508129">
1 2 3 4 5 6 7 8 9
</equation>
<figureCaption confidence="0.993246666666667">
Figure 3: Jensen-Shannon divergence in the topic
distributions between the different occupational
classes (C 1–9).
</figureCaption>
<bodyText confidence="0.999725333333333">
The main conclusion we draw from Figure 2 is
that there exists a topic divergence between users in
the lower vs. higher skilled occupational classes. To
examine this distinction better, we use the Jensen-
Shannon divergence (JSD) to quantify the differ-
ence between the topic distributions across every
</bodyText>
<figure confidence="0.99847932231405">
1
2
3
4
5
6
7
8
9
0.03
0.02
0.01
0.00
1760
Higher Education (#21)
C1
C2
C3
C4
C5
C6
C7
C8
C9
0.001 0.01 0.05
Beauty Care (#153)
Corporate (#124)
1
0.8
0.6
0.4
0.2
0.001 0.01 0.05
Elongated Words (#164)
0
0.001 0.01 0.05
Politics (#176)
1
User probability
0.2
C1
C2
C3
C4
C5
C6
C7
C8
C9
0.8
0.6
0.4
0
0.001 0.01 0.05
Arts (#116)
1
0.8
0.6
0.4
0.2
C1
C2
C3
C4
C5
C6
C7
C8
C9
1
User probability
0.8
0.6
0.4
0.2
0
0
C1
C2
C3
C4
C5
C6
C7
C8
C9
0
0.8
0.6
0.4
0.2
0
1
C1
C2
C3
C4
C5
C6
C7
C8
C9
0.001 0.01 0.05
Topic proportion
C1
C2
C3
C4
C5
C6
C7
C8
C9
0.001 0.01 0.05
Topic proportion
1
User probability
0.8
0.6
0.4
0.2
</figure>
<figureCaption confidence="0.992306">
Figure 2: CDFs for six of the most important topics; the x-axis is on the log-scale for display purposes. A
</figureCaption>
<bodyText confidence="0.95774696">
point on a CDF line indicates the fraction of users (y-axis point) with a topic proportion in their tweets
lower or equal to the corresponding x-axis point. The topic is more prevalent in a class, if the CDF line
leans closer to the bottom-right corner of the plot.
class pair. Figure 3 visualises these differences.
There, we confirm that adjacent classes use simi-
lar topics of discussion. We also notice that JSD
increases as the classes are further apart. Two main
groups of related classes, with a clear separation
from the rest, are identified: classes 1–2 and 6–9.
For the users belonging to these two groups, we
compute their topic usage distribution (for the top
topics listed in Table 4). Then, we assess whether
the topic usage distributions of those super-classes
of occupations have a statistically significant dif-
ference by performing a two-sample Kolmogorov-
Smirnov test. We enumerate the group topic usage
means in Table 5; all differences were indeed sta-
tistically significant (p &lt; 10−5). From this compar-
ison, we conclude that users in the higher skilled
classes have a higher representation in all top topics
but ‘Beauty Care’ and ‘Elongated Words’. Hence,
the original hypothesis about the difference in the
usage of language between upper and lower occu-
pational classes is reconfirmed in this more generic
testing. A very noticeable difference occurs for the
</bodyText>
<page confidence="0.958947">
1761
</page>
<table confidence="0.999628636363636">
Topics C 1–2 C 6–9
Arts 4.95 2.79
Health 4.45 2.13
Beauty Care 1.40 2.24
Higher Education 6.04 2.56
Software Engineering 6.31 2.54
Football 0.54 0.52
Corporate 5.15 1.41
Cooking 2.81 2.49
Elongated Words 1.90 3.78
Politics 2.14 1.06
</table>
<tableCaption confidence="0.996744">
Table 5: Comparison of mean topic usage for
</tableCaption>
<bodyText confidence="0.890733">
super-sets (classes 1–2 vs. 6–9) of the occupational
classes; all values were multiplied by 103. The dif-
ference between the topic usage distributions was
statistically significant (p &lt; 10−5).
‘Corporate’ topic, whereas ‘Football’ registers the
lowest distance.
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999984673076923">
Occupational class prediction has been studied in
the past in the areas of psychology and economics.
French (1959) investigated the relation between var-
ious measures on 232 undergraduate students and
their future occupations. This study concluded that
occupational membership can be predicted from
variables such as the ability of subjects in using
mathematical and verbal symbols, their family eco-
nomic status, body-build and personality compo-
nents. Schmidt and Strauss (1975) also studied the
relationship between job types (five classes) and
certain demographic attributes (gender, race, expe-
rience, education, location). Their analysis identi-
fied biases or discrimination which possibly exist
in different types of jobs. Sociolinguistic and so-
ciology studies deduct that social status is an im-
portant factor in determining the use of language
(Bernstein, 1960; Bernstein, 2003; Labov, 2006).
Differences arise either due to language use or due
to the topics people discuss as parts of various so-
cial domains. However, a large scale investigation
of this hypothesis has never been attempted.
Relevant to our task is a relation extraction ap-
proach proposed by Li et al. (2014) aiming to ex-
tract user profile information on Twitter. They used
a weakly supervised approach to obtain informa-
tion for job, education and spouse. Nonetheless,
the information relevant to the job attribute re-
gards the employer of a user (i.e. the name of a
company) rather than the type of occupation. In
addition, Huang et al. (2014) proposed a method
to classify Sina Weibo users to twelve predefined
occupations using content based and network fea-
tures. However, there exist significant differences
from our task since this inference is based on a dis-
tinct platform, with an ambiguous distribution over
occupations (e.g. more than 25% related to me-
dia), while the occupational classes are not generic
(e.g. media, welfare and electronic are three of the
twelve categories). Most importantly, the applied
model did not allow for a qualitative interpreta-
tion. Filho et al. (2014) inferred the social class of
social media users by combining geolocation infor-
mation derived from Foursquare and Twitter posts.
Recently, Sloan et al. (2015) introduced tools for
the automated extraction of demographic data (age,
occupation and social class) from the profile de-
scriptions of Twitter users using a similar method
to our data set extraction approach. They showed
that it is feasible to build a data set that matches
the real-world UK occupation distribution as given
by the SOC.
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999980181818182">
Our paper presents the first large-scale systematic
study on language use on social media as a factor
for inferring a user’s occupational class. To address
this problem, we have also introduced an exten-
sive labelled data set extracted from Twitter. We
have framed prediction as a classification task and,
to this end, we used the powerful, non-linear GP
framework that combines strong predictive perfor-
mance with feature interpretability. Results show
that we can achieve a good predictive accuracy,
highlighting that the occupation of a user influences
text use. Through a qualitative analysis, we have
shown that the derived topics capture both occupa-
tion specific interests as well as general class-based
behaviours. We acknowledge that the derivations
of this study, similarly to other studies in the field,
are reflecting the Twitter population and may expe-
rience a bias introduced by users self-mentioning
their occupations. However, the magnitude, occupa-
tional diversity and face validity of our conclusions
suggest that the presented approach is useful for
future downstream applications.
</bodyText>
<page confidence="0.993322">
1762
</page>
<sectionHeader confidence="0.996497" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996694">
DP-P acknowledges the support from Templeton
Religion Trust, grant TRT-0048. VL and NA ac-
knowledge the support from EPSRC (UK) project
EP/K031953/1. We thank Mark Stevenson for his
critical comments on early drafts of this paper.
</bodyText>
<sectionHeader confidence="0.998449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998442061111112">
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and Latent Attribute Inference: Inferring
Latent Attributes of Twitter Users from Neighbors.
In Proc. of 6th International Conference on Weblogs
and Social Media, pages 387–390.
Basil Bernstein. 1960. Language and social class.
British Journal of Sociology, pages 271–276.
Basil Bernstein. 2003. Class, codes and control: Ap-
plied studies towards a sociology of language, vol-
ume 2. Psychology Press.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1–8.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Biennial
GSCL Conference, pages 31–40.
D. John Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating Gender on
Twitter. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP, pages 1301–1309.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based ap-
proach to geo-locating twitter users. In Proceedings
of the 19th ACM Conference on Information and
Knowledge Management, CIKM, pages 759–768.
Trevor Cohn and Lucia Specia. 2013. Modelling an-
notator bias with multi-task gaussian processes: An
application to machine translation quality estimation.
In 51st Annual Meeting of the Association for Com-
putational Linguistics, ACL, pages 32–42.
Glen Coppersmith, Craig Harman, and Mark Dredze.
2014. Measuring post traumatic stress disorder in
twitter. In International Conference on Weblogs and
Social Media, ICWSM.
Renato Miranda Filho, Guilherme R. Borges, Jus-
sara M. Almeida, and Gisele L. Pappa. 2014. Infer-
ring user social class in online social networks. In
Proceedings of the 8th Workshop on Social Network
Mining and Analysis, SNAKDD’14, pages 10:1–
10:5.
David Freedman. 2009. Statistical models: theory and
practice. Cambridge University Press.
Wendell L French. 1959. Can a man’s occupation
be predicted? Journal of Counseling Psychology,
6(2):95.
Mark Gibbs and David J. C. Mackay. 1997. Variational
gaussian process classifiers. IEEE Transactions on
Neural Networks, 11:1458–1464.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H.
Chi. 2011. Tweets from justin bieber’s heart: The
dynamics of the location field in user profiles. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI.
Yanxiang Huang, Lele Yu, Xiang Wang, and Bin Cui.
2014. A multi-source integration framework for
user occupation inference in social media systems.
World Wide Web, pages 1–21.
William Labov. 2006. The Social Stratification of En-
glish in New York City. Cambridge University Press,
second edition.
Vasileios Lampos and Nello Cristianini. 2010. Track-
ing the flu pandemic by monitoring the Social Web.
In Proc. of the 2nd International Workshop on Cog-
nitive Information Processing, pages 411–416.
Vasileios Lampos and Nello Cristianini. 2012. Now-
casting Events from the Social Web with Statistical
Learning. ACM Transactions on Intelligent Systems
and Technology, 3(4):72:1–72:22.
Vasileios Lampos, Daniel Preot¸iuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting in-
tention from Social Media. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL, pages 993–1003.
Vasileios Lampos, Nikolaos Aletras, Daniel Preot¸iuc-
Pietro, and Trevor Cohn. 2014. Predicting and char-
acterising user impact on Twitter. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, EACL,
pages 405–413.
Omer Levy and Yoav Goldberg. 2014. Neural word
embeddings as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems,
NIPS, pages 2177–2185.
Jiwei Li, Alan Ritter, and Eduard H. Hovy. 2014.
Weakly supervised user profile extraction from twit-
ter. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, ACL,
pages 165–174.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at the International Conference on Learning Repre-
sentations, ICLR.
1763
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems, NIPS, pages 3111–3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2010
annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL, pages 746–751.
Thomas P. Minka. 2001. Expectation propagation for
approximate bayesian inference. In Proceedings of
the 17th Conference in Uncertainty in Artificial In-
telligence, UAI ’01.
Radford M. Neal. 1996. Bayesian Learning for Neural
Networks. Springer-Verlag New York, Inc.
Radford M. Neal. 1999. Regression and classification
using gaussian process priors. Bayesian Statistics 6,
pages 475–501.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.
2002. On spectral clustering: Analysis and an algo-
rithm. In Advances in Neural Information Process-
ing Systems, NIPS, pages 849–856.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to twitter user classifi-
cation. ICWSM, pages 281–288.
Tamara Polajnar, Simon Rogers, and Mark Girolami.
2011. Protein interaction detection in sentences via
gaussian processes; a preliminary evaluation. Inter-
national Journal ofData Mining and Bioinformatics,
5(1):52–72.
Daniel Preot¸iuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. EMNLP.
Daniel Preot¸iuc-Pietro, Sina Samangooei, Trevor Cohn,
Nicholas Gibbins, and Mahesan Niranjan. 2012.
Trendminer: An architecture for real time analysis
of social media text. In Workshop on Real-Time
Analysis and Mining of Social Streams (RAMSS),
ICWSM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying Latent User At-
tributes in Twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents, SMUC, pages 37–44.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian Processes for Machine
Learning. The MIT Press.
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012. Modeling Spread of Disease from Social In-
teractions. In Proc. of 6th International Conference
on Weblogs and Social Media, pages 322–329.
Peter Schmidt and Robert P Strauss. 1975. The predic-
tion of occupation using multiple logit models. In-
ternational Economic Review, 16(2):471–86.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. Transactions on Pattern
Analysis and Machine Intelligence, 22(8):888–905.
Luke Sloan, Jeffrey Morgan, Pete Burnap, and Matthew
Williams. 2015. Who tweets? Deriving the demo-
graphic characteristics of age, occupation and so-
cial class from twitter user meta-data. PloS one,
10(3):e0115545.
Edward Snelson and Zoubin Ghahramani. 2006.
Sparse gaussian processes using pseudo-inputs. In
Advances in Neural Information Processing Systems,
NIPS, pages 1257–1264.
Andranik Tumasjan, Timm Oliver Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting
Elections with Twitter: What 140 Characters Reveal
about Political Sentiment. In Proc. of 4th Inter-
national Conference on Weblogs and Social Media,
pages 178–185.
Vladimir N Vapnik. 1998. Statistical learning theory.
Wiley, New York.
Ulrike von Luxburg. 2007. A tutorial on spectral clus-
tering. Statistics and computing, 17(4):395–416.
Christopher K.I Williams and David Barber. 1998.
Bayesian classification with gaussian processes.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20 (12):1342–1351.
</reference>
<page confidence="0.994944">
1764
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.360503">
<title confidence="0.945976">An analysis of the user occupational class through Twitter content</title>
<author confidence="0.470434">Vasileios</author>
<affiliation confidence="0.62077">amp; Information Science, University of Pennsylvania of Computer Science, University College London</affiliation>
<abstract confidence="0.999527608695652">Social media content can be used as a complementary source to the traditional methods for extracting and studying collective social attributes. This study focuses on the prediction of the occupational class for a public user profile. Our analysis is conducted on a new annotated corpus of Twitter users, their respective job titles, posted textual content and platform-related attributes. We frame our task as classification using latent feature representations such as word clusters and embeddings. The employed linear and, especially, non-linear methods can predict a user’s occupational class with strong accuracy for the coarsest level of a standard occupation taxonomy which includes nine classes. Combined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Faiyaz Al Zamal</author>
<author>Wendy Liu</author>
<author>Derek Ruths</author>
</authors>
<title>Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors.</title>
<date>2012</date>
<booktitle>In Proc. of 6th International Conference on Weblogs and Social Media,</booktitle>
<pages>387--390</pages>
<contexts>
<context position="1577" citStr="Zamal et al., 2012" startWordPosition="228" endWordPosition="231">of a standard occupation taxonomy which includes nine classes. Combined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression tasks such as voting intention (Lampos et al., </context>
</contexts>
<marker>Zamal, Liu, Ruths, 2012</marker>
<rawString>Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012. Homophily and Latent Attribute Inference: Inferring Latent Attributes of Twitter Users from Neighbors. In Proc. of 6th International Conference on Weblogs and Social Media, pages 387–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Basil Bernstein</author>
</authors>
<title>Language and social class.</title>
<date>1960</date>
<journal>British Journal of Sociology,</journal>
<pages>271--276</pages>
<contexts>
<context position="2296" citStr="Bernstein, 1960" startWordPosition="341" endWordPosition="342">10), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression tasks such as voting intention (Lampos et al., 2013). Social status reflected through a person’s occupation is a factor which influences language use (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Therefore, our hypothesis is that language use in social media can be indicative of a user’s occupational class. For example, executives may write more frequently about business or financial news, while people in manufacturing positions could refer more to their personal interests and less to job related activities. Similarly, we expect some categories of people, like those working in sales and customer services, to be more social or to use more informal language. Focusing on the microblogging platform of Twitter, we explore our hypothesis by studying the task </context>
<context position="26917" citStr="Bernstein, 1960" startWordPosition="4190" endWordPosition="4191">3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 0.6 0.4 0.2 0.0 1759 that our model captures the general user skill level. 6.3 Qualitative Analysis The word clusters that were built from a reference corpus and then used as features in the GP classifier, give us the opportunity to extract some qualitative derivations from our predictive task. For the rest of the section we use the best performing model of this type (W2V-C-200) in order to analyse the results. Our main assumption is that there might be a divergence of language and topic usage across occupational classes following previous studies in sociology (Bernstein, 1960; Bernstein, 2003). Knowing that the inferred GP lengthscale hyperparameters are inversely proportional to feature (i.e. topic) relevance (see Section 5), we can use them to rank the topic importance and give answers to our hypothesis. Table 4 shows 10 of the most informative topics (represented by the top 10 most central and frequent words) sorted by their ARD lengthscale Mean Reciprocal Rank (MRR) (Manning et al., 2008) across the nine classifiers. Evidently, they cover a broad range of thematic subjects, including potentially work specific topics in different domains such as ‘Corporate’ (To</context>
<context position="34180" citStr="Bernstein, 1960" startWordPosition="5415" endWordPosition="5416">hat occupational membership can be predicted from variables such as the ability of subjects in using mathematical and verbal symbols, their family economic status, body-build and personality components. Schmidt and Strauss (1975) also studied the relationship between job types (five classes) and certain demographic attributes (gender, race, experience, education, location). Their analysis identified biases or discrimination which possibly exist in different types of jobs. Sociolinguistic and sociology studies deduct that social status is an important factor in determining the use of language (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Differences arise either due to language use or due to the topics people discuss as parts of various social domains. However, a large scale investigation of this hypothesis has never been attempted. Relevant to our task is a relation extraction approach proposed by Li et al. (2014) aiming to extract user profile information on Twitter. They used a weakly supervised approach to obtain information for job, education and spouse. Nonetheless, the information relevant to the job attribute regards the employer of a user (i.e. the name of a company) rather than the ty</context>
</contexts>
<marker>Bernstein, 1960</marker>
<rawString>Basil Bernstein. 1960. Language and social class. British Journal of Sociology, pages 271–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Basil Bernstein</author>
</authors>
<title>Class, codes and control: Applied studies towards a sociology of language,</title>
<date>2003</date>
<volume>2</volume>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="2313" citStr="Bernstein, 2003" startWordPosition="343" endWordPosition="344">er et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression tasks such as voting intention (Lampos et al., 2013). Social status reflected through a person’s occupation is a factor which influences language use (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Therefore, our hypothesis is that language use in social media can be indicative of a user’s occupational class. For example, executives may write more frequently about business or financial news, while people in manufacturing positions could refer more to their personal interests and less to job related activities. Similarly, we expect some categories of people, like those working in sales and customer services, to be more social or to use more informal language. Focusing on the microblogging platform of Twitter, we explore our hypothesis by studying the task of predicting a u</context>
<context position="26935" citStr="Bernstein, 2003" startWordPosition="4192" endWordPosition="4193"> 3 4 5 6 7 8 9 0.6 0.4 0.2 0.0 1759 that our model captures the general user skill level. 6.3 Qualitative Analysis The word clusters that were built from a reference corpus and then used as features in the GP classifier, give us the opportunity to extract some qualitative derivations from our predictive task. For the rest of the section we use the best performing model of this type (W2V-C-200) in order to analyse the results. Our main assumption is that there might be a divergence of language and topic usage across occupational classes following previous studies in sociology (Bernstein, 1960; Bernstein, 2003). Knowing that the inferred GP lengthscale hyperparameters are inversely proportional to feature (i.e. topic) relevance (see Section 5), we can use them to rank the topic importance and give answers to our hypothesis. Table 4 shows 10 of the most informative topics (represented by the top 10 most central and frequent words) sorted by their ARD lengthscale Mean Reciprocal Rank (MRR) (Manning et al., 2008) across the nine classifiers. Evidently, they cover a broad range of thematic subjects, including potentially work specific topics in different domains such as ‘Corporate’ (Topic #124), ‘Softwa</context>
<context position="34197" citStr="Bernstein, 2003" startWordPosition="5417" endWordPosition="5418">membership can be predicted from variables such as the ability of subjects in using mathematical and verbal symbols, their family economic status, body-build and personality components. Schmidt and Strauss (1975) also studied the relationship between job types (five classes) and certain demographic attributes (gender, race, experience, education, location). Their analysis identified biases or discrimination which possibly exist in different types of jobs. Sociolinguistic and sociology studies deduct that social status is an important factor in determining the use of language (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Differences arise either due to language use or due to the topics people discuss as parts of various social domains. However, a large scale investigation of this hypothesis has never been attempted. Relevant to our task is a relation extraction approach proposed by Li et al. (2014) aiming to extract user profile information on Twitter. They used a weakly supervised approach to obtain information for job, education and spouse. Nonetheless, the information relevant to the job attribute regards the employer of a user (i.e. the name of a company) rather than the type of occupation.</context>
</contexts>
<marker>Bernstein, 2003</marker>
<rawString>Basil Bernstein. 2003. Class, codes and control: Applied studies towards a sociology of language, volume 2. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1381" citStr="Bollen et al., 2011" startWordPosition="198" endWordPosition="201">e representations such as word clusters and embeddings. The employed linear and, especially, non-linear methods can predict a user’s occupational class with strong accuracy for the coarsest level of a standard occupation taxonomy which includes nine classes. Combined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other application</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>In Biennial GSCL Conference,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="12995" citStr="Bouma, 2009" startWordPosition="2001" endWordPosition="2002">resent each user as a distribution over these features. We ignore the bio field from building textual features to avoid introducing biases from our data collection method. While this is a restriction, our analysis showed that in less than 20% of the cases the information in the bio is directly relevant to the occupation. 4.2.1 SVD Word Embeddings (SVD-E) We use a more abstract representation of words than simple unigram counts in order to aid interpretability of our analysis. We compute a word to word similarity matrix from our reference corpus. Normalised Pointwise Mutual Information (NPMI) (Bouma, 2009) is used to compute word to word similarity. NPMI is an information theoretic measure indicating which words co-occur in the same context, where the context is represented by a whole tweet: P(x,y) NPMI(x, y) = − log P(x, y) · log P(x) · P(y). (1) We then perform singular value decomposition (SVD) on the word to word similarity matrix and obtain an embedding of words into a low dimensional space. In our experiments we tried the following dimensionalities: 30, 50, 100 and 200. The feature representation for each user is obtained summing over each of the embedding dimensions across all words. u1 </context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. In Biennial GSCL Conference, pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D John Burger</author>
<author>John Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating Gender on Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP,</booktitle>
<pages>1301--1309</pages>
<contexts>
<context position="1714" citStr="Burger et al., 2011" startWordPosition="248" endWordPosition="251">easibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression tasks such as voting intention (Lampos et al., 2013). Social status reflected through a person’s occupation is a factor which influences language use (Bernstein, 1960; Bernstein, 2003;</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>D. John Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating Gender on Twitter. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1301–1309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Cheng</author>
<author>James Caverlee</author>
<author>Kyumin Lee</author>
</authors>
<title>You are where you tweet: a content-based approach to geo-locating twitter users.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM,</booktitle>
<pages>759--768</pages>
<contexts>
<context position="1684" citStr="Cheng et al., 2010" startWordPosition="243" endWordPosition="246">derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression tasks such as voting intention (Lampos et al., 2013). Social status reflected through a person’s occupation is a factor which influences language use (Ber</context>
</contexts>
<marker>Cheng, Caverlee, Lee, 2010</marker>
<rawString>Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010. You are where you tweet: a content-based approach to geo-locating twitter users. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM, pages 759–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics, ACL,</booktitle>
<pages>32--42</pages>
<contexts>
<context position="16753" citStr="Cohn and Specia, 2013" startWordPosition="2618" endWordPosition="2621">ering on this matrix to obtain 30, 50, 100 and 200 word clusters. 5 Classification with Gaussian Processes In this section, we briefly overview Gaussian Process (GP) for classification, highlighting our motivation for using this method. GPs formulate a Bayesian non-parametric machine learning framework which defines a prior on functions (Rasmussen and Williams, 2006). The properties of the functions are given by a kernel which models the covariance in the response values as a function of its inputs. Although GPs form a powerful learning tool, they have only recently been used in NLP research (Cohn and Specia, 2013; Preoiiuc-Pietro and Cohn, 2013) with classification applications limited to (Polajnar et al., 2011). Formally, GP methods aim to learn a function f : Rd —* R drawn from a GP prior given the inputs x E Rd: f(x) — 9P(m(x), k(x,x&apos;)) , (3) where m(·) is the mean function (here 0) and k(·, ·) is the covariance kernel. Usually, the Squared Exponential (SE) kernel (a.k.a. RBF or Gaussian) is used to encourage smooth functions. For the multidimensional pair of inputs (x,x&apos;), this is: � d _(xz — xi)2 2l? z where lz are lengthscale parameters learnt only using training data by performing gradient asce</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation. In 51st Annual Meeting of the Association for Computational Linguistics, ACL, pages 32–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Coppersmith</author>
<author>Craig Harman</author>
<author>Mark Dredze</author>
</authors>
<title>Measuring post traumatic stress disorder in twitter.</title>
<date>2014</date>
<booktitle>In International Conference on Weblogs and Social Media, ICWSM.</booktitle>
<contexts>
<context position="8178" citStr="Coppersmith et al., 2014" startWordPosition="1207" endWordPosition="1210">etary Major Group (C5): Skilled Trades Occupations •Job: electrical fitter, tailor Major Group (C6): Caring, Leisure and Other Service Occupations •Job: nursery assistant, hairdresser Major Group (C7): Sales and Customer Service Occupations •Job: sales assistant, telephonist Major Group (C8): Process, Plant and Machine Operatives •Job: factory worker, van driver Major Group (C9): Elementary Occupations •Job: shelf stacker, bartender Table 1: Subset of the SOC classification hierarchy. is widespread when extracting large-scale data for user attribute inference (Pennacchiotti and Popescu, 2011; Coppersmith et al., 2014). Similarly to Hecht et al. (2011), we first assess the proportion of Twitter accounts with a clear mention to their occupation by annotating the user description field of a random set of 500 users. There were chosen from the random 1% sample, having at least 200 tweets in their history and with a majority of English tweets. There, we can identify the following categories: no description (12.2%), random information (22%), user information but not occupation related (45.8%), and job related information (20%). To create our data set, we thus use the user description field to search for self-disc</context>
</contexts>
<marker>Coppersmith, Harman, Dredze, 2014</marker>
<rawString>Glen Coppersmith, Craig Harman, and Mark Dredze. 2014. Measuring post traumatic stress disorder in twitter. In International Conference on Weblogs and Social Media, ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renato Miranda Filho</author>
<author>Guilherme R Borges</author>
<author>Jussara M Almeida</author>
<author>Gisele L Pappa</author>
</authors>
<title>Inferring user social class in online social networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th Workshop on Social Network Mining and Analysis, SNAKDD’14,</booktitle>
<pages>10--1</pages>
<contexts>
<context position="35376" citStr="Filho et al. (2014)" startWordPosition="5609" endWordPosition="5612">any) rather than the type of occupation. In addition, Huang et al. (2014) proposed a method to classify Sina Weibo users to twelve predefined occupations using content based and network features. However, there exist significant differences from our task since this inference is based on a distinct platform, with an ambiguous distribution over occupations (e.g. more than 25% related to media), while the occupational classes are not generic (e.g. media, welfare and electronic are three of the twelve categories). Most importantly, the applied model did not allow for a qualitative interpretation. Filho et al. (2014) inferred the social class of social media users by combining geolocation information derived from Foursquare and Twitter posts. Recently, Sloan et al. (2015) introduced tools for the automated extraction of demographic data (age, occupation and social class) from the profile descriptions of Twitter users using a similar method to our data set extraction approach. They showed that it is feasible to build a data set that matches the real-world UK occupation distribution as given by the SOC. 8 Conclusions Our paper presents the first large-scale systematic study on language use on social media a</context>
</contexts>
<marker>Filho, Borges, Almeida, Pappa, 2014</marker>
<rawString>Renato Miranda Filho, Guilherme R. Borges, Jussara M. Almeida, and Gisele L. Pappa. 2014. Inferring user social class in online social networks. In Proceedings of the 8th Workshop on Social Network Mining and Analysis, SNAKDD’14, pages 10:1– 10:5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Freedman</author>
</authors>
<title>Statistical models: theory and practice.</title>
<date>2009</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="21085" citStr="Freedman, 2009" startWordPosition="3310" endWordPosition="3311">0.0% 46.0% 47.1% W2V-C-50 42.3% 48.5% 47.9% W2V-C-100 44.4% 48.7% 51.3% W2V-C-200 46.9% 51.7% 52.7% Table 3: 9-way classification accuracy on held-out data for our 3 methods. Textual features are obtained using SVD or Word2Vec (W2V). E represents embeddings, C clusters. The final number denotes the amount of clusters or the size of the embedding. features at a time. Experiments combining features yielded only minor improvements. We apply common linear and non-linear methods together with our proposed GP classifier. The linear method is logistic regression (LR) with Elastic Net regularisation (Freedman, 2009) and the non-linear one is formulated by a Support Vector Machine (SVM) with an RBF kernel (Vapnik, 1998). The accuracy of our classifiers is measured on held-out data. Our data set is divided into stratified training (80%), validation (10%) and testing (10%) sets. The validation set was used to learn the LR and SVM hyperparameters, while the GP did not use this set at all. We report results using all three methods and all feature sets in Table 3. We first observe that user level features (UserLevel; see Section 4.1) are not useful for predicting the job class. This finding indicates that gene</context>
</contexts>
<marker>Freedman, 2009</marker>
<rawString>David Freedman. 2009. Statistical models: theory and practice. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendell L French</author>
</authors>
<title>Can a man’s occupation be predicted?</title>
<date>1959</date>
<journal>Journal of Counseling Psychology,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="33431" citStr="French (1959)" startWordPosition="5307" endWordPosition="5308">uty Care 1.40 2.24 Higher Education 6.04 2.56 Software Engineering 6.31 2.54 Football 0.54 0.52 Corporate 5.15 1.41 Cooking 2.81 2.49 Elongated Words 1.90 3.78 Politics 2.14 1.06 Table 5: Comparison of mean topic usage for super-sets (classes 1–2 vs. 6–9) of the occupational classes; all values were multiplied by 103. The difference between the topic usage distributions was statistically significant (p &lt; 10−5). ‘Corporate’ topic, whereas ‘Football’ registers the lowest distance. 7 Related Work Occupational class prediction has been studied in the past in the areas of psychology and economics. French (1959) investigated the relation between various measures on 232 undergraduate students and their future occupations. This study concluded that occupational membership can be predicted from variables such as the ability of subjects in using mathematical and verbal symbols, their family economic status, body-build and personality components. Schmidt and Strauss (1975) also studied the relationship between job types (five classes) and certain demographic attributes (gender, race, experience, education, location). Their analysis identified biases or discrimination which possibly exist in different type</context>
</contexts>
<marker>French, 1959</marker>
<rawString>Wendell L French. 1959. Can a man’s occupation be predicted? Journal of Counseling Psychology, 6(2):95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Gibbs</author>
<author>David J C Mackay</author>
</authors>
<title>Variational gaussian process classifiers.</title>
<date>1997</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<pages>11--1458</pages>
<contexts>
<context position="18728" citStr="Gibbs and Mackay, 1997" startWordPosition="2938" endWordPosition="2941">) = P(f*|x, x*,f)P(f|x,y)df , (5) where P(f|x,y) = P(y|f)P(f|x)/P(y|x) is the posterior over the latent variables. If the likelihood P(y|f) is Gaussian, the combination with a GP prior P(f|x) gives a posterior GP over functions. In binary classification, the distribution over the latent f* is combined with the logistic function to produce the prediction: �¯π* = σ(f*)P(f*|x,y,x*)df*. (6) This results in a non-Gaussian likelihood in the posterior formulation and therefore, exact inference is infeasible for classification models. Multiple approximations exist that make the computation tractable (Gibbs and Mackay, 1997; Williams and Barber, 1998; Neal, 1999). In our experiments we opt to use the Expectation Propagation (EP) method (Minka, 2001) which approximates the nonGaussian joint posterior with a Gaussian one. EP offers very good empirical results for many different likelihoods, although it has no proof of convergence. The complexity for the inference step is O(n3). Given that our data set is very large and the number of features is high, we conduct inference using the fully independent training conditional (FITC) approximation (Snelson and Ghahramani, 2006) with 500 random inducing points. We refer th</context>
</contexts>
<marker>Gibbs, Mackay, 1997</marker>
<rawString>Mark Gibbs and David J. C. Mackay. 1997. Variational gaussian process classifiers. IEEE Transactions on Neural Networks, 11:1458–1464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brent Hecht</author>
<author>Lichan Hong</author>
<author>Bongwon Suh</author>
<author>Ed H Chi</author>
</authors>
<title>Tweets from justin bieber’s heart: The dynamics of the location field in user profiles.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<location>CHI.</location>
<contexts>
<context position="8212" citStr="Hecht et al. (2011)" startWordPosition="1213" endWordPosition="1216">ccupations •Job: electrical fitter, tailor Major Group (C6): Caring, Leisure and Other Service Occupations •Job: nursery assistant, hairdresser Major Group (C7): Sales and Customer Service Occupations •Job: sales assistant, telephonist Major Group (C8): Process, Plant and Machine Operatives •Job: factory worker, van driver Major Group (C9): Elementary Occupations •Job: shelf stacker, bartender Table 1: Subset of the SOC classification hierarchy. is widespread when extracting large-scale data for user attribute inference (Pennacchiotti and Popescu, 2011; Coppersmith et al., 2014). Similarly to Hecht et al. (2011), we first assess the proportion of Twitter accounts with a clear mention to their occupation by annotating the user description field of a random set of 500 users. There were chosen from the random 1% sample, having at least 200 tweets in their history and with a majority of English tweets. There, we can identify the following categories: no description (12.2%), random information (22%), user information but not occupation related (45.8%), and job related information (20%). To create our data set, we thus use the user description field to search for self-disclosed job titles provided by the 4</context>
</contexts>
<marker>Hecht, Hong, Suh, Chi, 2011</marker>
<rawString>Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi. 2011. Tweets from justin bieber’s heart: The dynamics of the location field in user profiles. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxiang Huang</author>
<author>Lele Yu</author>
<author>Xiang Wang</author>
<author>Bin Cui</author>
</authors>
<title>A multi-source integration framework for user occupation inference in social media systems. World Wide Web,</title>
<date>2014</date>
<pages>1--21</pages>
<contexts>
<context position="34830" citStr="Huang et al. (2014)" startWordPosition="5523" endWordPosition="5526">. Differences arise either due to language use or due to the topics people discuss as parts of various social domains. However, a large scale investigation of this hypothesis has never been attempted. Relevant to our task is a relation extraction approach proposed by Li et al. (2014) aiming to extract user profile information on Twitter. They used a weakly supervised approach to obtain information for job, education and spouse. Nonetheless, the information relevant to the job attribute regards the employer of a user (i.e. the name of a company) rather than the type of occupation. In addition, Huang et al. (2014) proposed a method to classify Sina Weibo users to twelve predefined occupations using content based and network features. However, there exist significant differences from our task since this inference is based on a distinct platform, with an ambiguous distribution over occupations (e.g. more than 25% related to media), while the occupational classes are not generic (e.g. media, welfare and electronic are three of the twelve categories). Most importantly, the applied model did not allow for a qualitative interpretation. Filho et al. (2014) inferred the social class of social media users by co</context>
</contexts>
<marker>Huang, Yu, Wang, Cui, 2014</marker>
<rawString>Yanxiang Huang, Lele Yu, Xiang Wang, and Bin Cui. 2014. A multi-source integration framework for user occupation inference in social media systems. World Wide Web, pages 1–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>The Social Stratification of English in New York City.</title>
<date>2006</date>
<publisher>Cambridge University Press,</publisher>
<note>second edition.</note>
<contexts>
<context position="2327" citStr="Labov, 2006" startWordPosition="345" endWordPosition="346"> impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression tasks such as voting intention (Lampos et al., 2013). Social status reflected through a person’s occupation is a factor which influences language use (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Therefore, our hypothesis is that language use in social media can be indicative of a user’s occupational class. For example, executives may write more frequently about business or financial news, while people in manufacturing positions could refer more to their personal interests and less to job related activities. Similarly, we expect some categories of people, like those working in sales and customer services, to be more social or to use more informal language. Focusing on the microblogging platform of Twitter, we explore our hypothesis by studying the task of predicting a user’s occupati</context>
<context position="34211" citStr="Labov, 2006" startWordPosition="5419" endWordPosition="5420"> predicted from variables such as the ability of subjects in using mathematical and verbal symbols, their family economic status, body-build and personality components. Schmidt and Strauss (1975) also studied the relationship between job types (five classes) and certain demographic attributes (gender, race, experience, education, location). Their analysis identified biases or discrimination which possibly exist in different types of jobs. Sociolinguistic and sociology studies deduct that social status is an important factor in determining the use of language (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Differences arise either due to language use or due to the topics people discuss as parts of various social domains. However, a large scale investigation of this hypothesis has never been attempted. Relevant to our task is a relation extraction approach proposed by Li et al. (2014) aiming to extract user profile information on Twitter. They used a weakly supervised approach to obtain information for job, education and spouse. Nonetheless, the information relevant to the job attribute regards the employer of a user (i.e. the name of a company) rather than the type of occupation. In addition, </context>
</contexts>
<marker>Labov, 2006</marker>
<rawString>William Labov. 2006. The Social Stratification of English in New York City. Cambridge University Press, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nello Cristianini</author>
</authors>
<title>Tracking the flu pandemic by monitoring the Social Web.</title>
<date>2010</date>
<booktitle>In Proc. of the 2nd International Workshop on Cognitive Information Processing,</booktitle>
<pages>411--416</pages>
<contexts>
<context position="1529" citStr="Lampos and Cristianini, 2010" startWordPosition="219" endWordPosition="222">ational class with strong accuracy for the coarsest level of a standard occupation taxonomy which includes nine classes. Combined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression</context>
</contexts>
<marker>Lampos, Cristianini, 2010</marker>
<rawString>Vasileios Lampos and Nello Cristianini. 2010. Tracking the flu pandemic by monitoring the Social Web. In Proc. of the 2nd International Workshop on Cognitive Information Processing, pages 411–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nello Cristianini</author>
</authors>
<title>Nowcasting Events from the Social Web with Statistical Learning.</title>
<date>2012</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1412" citStr="Lampos and Cristianini, 2012" startWordPosition="202" endWordPosition="206">h as word clusters and embeddings. The employed linear and, especially, non-linear methods can predict a user’s occupational class with strong accuracy for the coarsest level of a standard occupation taxonomy which includes nine classes. Combined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provi</context>
</contexts>
<marker>Lampos, Cristianini, 2012</marker>
<rawString>Vasileios Lampos and Nello Cristianini. 2012. Nowcasting Events from the Social Web with Statistical Learning. ACM Transactions on Intelligent Systems and Technology, 3(4):72:1–72:22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A user-centric model of voting intention from Social Media.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL,</booktitle>
<pages>993--1003</pages>
<marker>Lampos, Preot¸iuc-Pietro, Cohn, 2013</marker>
<rawString>Vasileios Lampos, Daniel Preot¸iuc-Pietro, and Trevor Cohn. 2013. A user-centric model of voting intention from Social Media. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL, pages 993–1003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nikolaos Aletras</author>
<author>Daniel Preot¸iucPietro</author>
<author>Trevor Cohn</author>
</authors>
<title>Predicting and characterising user impact on Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL,</booktitle>
<pages>405--413</pages>
<marker>Lampos, Aletras, Preot¸iucPietro, Cohn, 2014</marker>
<rawString>Vasileios Lampos, Nikolaos Aletras, Daniel Preot¸iucPietro, and Trevor Cohn. 2014. Predicting and characterising user impact on Twitter. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL, pages 405–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embeddings as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="15753" citStr="Levy and Goldberg, 2014" startWordPosition="2459" endWordPosition="2462">ia a hidden layer (Mikolov et al., 2013b). These models showed they can provide a better representation of words compared to traditional language models (Mikolov et al., 2013c) because they capture syntactic information rather than just bag-of-context, handling non-linear transformations. In this low dimensional vector space, words with a small distance are considered semantically similar. We use the skipgram model with negative sampling (Mikolov et al., 2013a) to learn word embeddings on the Twitter reference corpus. In that case, the skip-gram model is factorising a word-context PMI matrix (Levy and Goldberg, 2014). We use a layer size of 50 and the Gensim implementation.3 3http://radimrehurek.com/gensim/ models/word2vec.html 4.2.4 Neural Clusters (W2V-C) Similar to the NPMI cluster, we use the neural embeddings in order to obtain clusters of related words, i.e. ‘topics’. We derive a word to word similarity matrix using cosine similarity on the neural embeddings. We apply spectral clustering on this matrix to obtain 30, 50, 100 and 200 word clusters. 5 Classification with Gaussian Processes In this section, we briefly overview Gaussian Process (GP) for classification, highlighting our motivation for usi</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embeddings as implicit matrix factorization. In Advances in Neural Information Processing Systems, NIPS, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Alan Ritter</author>
<author>Eduard H Hovy</author>
</authors>
<title>Weakly supervised user profile extraction from twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL,</booktitle>
<pages>165--174</pages>
<contexts>
<context position="34495" citStr="Li et al. (2014)" startWordPosition="5466" endWordPosition="5469">ic attributes (gender, race, experience, education, location). Their analysis identified biases or discrimination which possibly exist in different types of jobs. Sociolinguistic and sociology studies deduct that social status is an important factor in determining the use of language (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Differences arise either due to language use or due to the topics people discuss as parts of various social domains. However, a large scale investigation of this hypothesis has never been attempted. Relevant to our task is a relation extraction approach proposed by Li et al. (2014) aiming to extract user profile information on Twitter. They used a weakly supervised approach to obtain information for job, education and spouse. Nonetheless, the information relevant to the job attribute regards the employer of a user (i.e. the name of a company) rather than the type of occupation. In addition, Huang et al. (2014) proposed a method to classify Sina Weibo users to twelve predefined occupations using content based and network features. However, there exist significant differences from our task since this inference is based on a distinct platform, with an ambiguous distributio</context>
</contexts>
<marker>Li, Ritter, Hovy, 2014</marker>
<rawString>Jiwei Li, Alan Ritter, and Eduard H. Hovy. 2014. Weakly supervised user profile extraction from twitter. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL, pages 165–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at the International Conference on Learning Representations, ICLR.</booktitle>
<contexts>
<context position="15168" citStr="Mikolov et al., 2013" startWordPosition="2371" endWordPosition="2374">entation, the clusters are very useful in the model analysis step. Embeddings are hard to interpret because each dimension is an abstract notion, while the clusters can be interpreted by presenting a list of the most frequent or representative words. The latter are identified using the following centrality metric: C,,, = ExEc NPMI(w, x) ,(2) JcJ — 1 where c denotes the cluster and w the target word. 4.2.3 Neural Embeddings (W2V-E) Recently, there has been a growing interest in neural language models, where the words are projected into a lower dimensional dense vector space via a hidden layer (Mikolov et al., 2013b). These models showed they can provide a better representation of words compared to traditional language models (Mikolov et al., 2013c) because they capture syntactic information rather than just bag-of-context, handling non-linear transformations. In this low dimensional vector space, words with a small distance are considered semantically similar. We use the skipgram model with negative sampling (Mikolov et al., 2013a) to learn word embeddings on the Twitter reference corpus. In that case, the skip-gram model is factorising a word-context PMI matrix (Levy and Goldberg, 2014). We use a laye</context>
<context position="25150" citStr="Mikolov et al., 2013" startWordPosition="3884" endWordPosition="3887"> by their most central and most frequent 10 words, sorted by their ARD lengthscale MRR across the nine GP-based occupation classifiers. µ(l) denotes the average lengthscale for a topic across these classifiers. Topic labels are manually created. Figure 1: Confusion matrix of the prediction results. Rows represent the actual occupational class (C 1– 9) and columns the predicted class. curacy than the SVD on the NPMI matrix. This is consistent with previous work that showed the efficiency of word2vec and the ability of those embeddings to capture non-linear relationships and syntactic features (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). LR has a lower performance than the non-linear methods, especially when using clusters as features. GPs usually outperform SVMs by a small margin. However, these offer the advantages of not using the validation set and the interpretability properties we highlight in the next section. Although we only draw our focus on major occupational classes, the data set allows the study of finer granularities of occupation classes in future work. For example, prediction performance for sub-major groups reaches 33.9% accuracy (15.6% majority class, 22 class</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at the International Conference on Learning Representations, ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="15168" citStr="Mikolov et al., 2013" startWordPosition="2371" endWordPosition="2374">entation, the clusters are very useful in the model analysis step. Embeddings are hard to interpret because each dimension is an abstract notion, while the clusters can be interpreted by presenting a list of the most frequent or representative words. The latter are identified using the following centrality metric: C,,, = ExEc NPMI(w, x) ,(2) JcJ — 1 where c denotes the cluster and w the target word. 4.2.3 Neural Embeddings (W2V-E) Recently, there has been a growing interest in neural language models, where the words are projected into a lower dimensional dense vector space via a hidden layer (Mikolov et al., 2013b). These models showed they can provide a better representation of words compared to traditional language models (Mikolov et al., 2013c) because they capture syntactic information rather than just bag-of-context, handling non-linear transformations. In this low dimensional vector space, words with a small distance are considered semantically similar. We use the skipgram model with negative sampling (Mikolov et al., 2013a) to learn word embeddings on the Twitter reference corpus. In that case, the skip-gram model is factorising a word-context PMI matrix (Levy and Goldberg, 2014). We use a laye</context>
<context position="25150" citStr="Mikolov et al., 2013" startWordPosition="3884" endWordPosition="3887"> by their most central and most frequent 10 words, sorted by their ARD lengthscale MRR across the nine GP-based occupation classifiers. µ(l) denotes the average lengthscale for a topic across these classifiers. Topic labels are manually created. Figure 1: Confusion matrix of the prediction results. Rows represent the actual occupational class (C 1– 9) and columns the predicted class. curacy than the SVD on the NPMI matrix. This is consistent with previous work that showed the efficiency of word2vec and the ability of those embeddings to capture non-linear relationships and syntactic features (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). LR has a lower performance than the non-linear methods, especially when using clusters as features. GPs usually outperform SVMs by a small margin. However, these offer the advantages of not using the validation set and the interpretability properties we highlight in the next section. Although we only draw our focus on major occupational classes, the data set allows the study of finer granularities of occupation classes in future work. For example, prediction performance for sub-major groups reaches 33.9% accuracy (15.6% majority class, 22 class</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2010 annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="15168" citStr="Mikolov et al., 2013" startWordPosition="2371" endWordPosition="2374">entation, the clusters are very useful in the model analysis step. Embeddings are hard to interpret because each dimension is an abstract notion, while the clusters can be interpreted by presenting a list of the most frequent or representative words. The latter are identified using the following centrality metric: C,,, = ExEc NPMI(w, x) ,(2) JcJ — 1 where c denotes the cluster and w the target word. 4.2.3 Neural Embeddings (W2V-E) Recently, there has been a growing interest in neural language models, where the words are projected into a lower dimensional dense vector space via a hidden layer (Mikolov et al., 2013b). These models showed they can provide a better representation of words compared to traditional language models (Mikolov et al., 2013c) because they capture syntactic information rather than just bag-of-context, handling non-linear transformations. In this low dimensional vector space, words with a small distance are considered semantically similar. We use the skipgram model with negative sampling (Mikolov et al., 2013a) to learn word embeddings on the Twitter reference corpus. In that case, the skip-gram model is factorising a word-context PMI matrix (Levy and Goldberg, 2014). We use a laye</context>
<context position="25150" citStr="Mikolov et al., 2013" startWordPosition="3884" endWordPosition="3887"> by their most central and most frequent 10 words, sorted by their ARD lengthscale MRR across the nine GP-based occupation classifiers. µ(l) denotes the average lengthscale for a topic across these classifiers. Topic labels are manually created. Figure 1: Confusion matrix of the prediction results. Rows represent the actual occupational class (C 1– 9) and columns the predicted class. curacy than the SVD on the NPMI matrix. This is consistent with previous work that showed the efficiency of word2vec and the ability of those embeddings to capture non-linear relationships and syntactic features (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). LR has a lower performance than the non-linear methods, especially when using clusters as features. GPs usually outperform SVMs by a small margin. However, these offer the advantages of not using the validation set and the interpretability properties we highlight in the next section. Although we only draw our focus on major occupational classes, the data set allows the study of finer granularities of occupation classes in future work. For example, prediction performance for sub-major groups reaches 33.9% accuracy (15.6% majority class, 22 class</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of the 2010 annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Expectation propagation for approximate bayesian inference.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, UAI ’01.</booktitle>
<contexts>
<context position="18856" citStr="Minka, 2001" startWordPosition="2960" endWordPosition="2961">) is Gaussian, the combination with a GP prior P(f|x) gives a posterior GP over functions. In binary classification, the distribution over the latent f* is combined with the logistic function to produce the prediction: �¯π* = σ(f*)P(f*|x,y,x*)df*. (6) This results in a non-Gaussian likelihood in the posterior formulation and therefore, exact inference is infeasible for classification models. Multiple approximations exist that make the computation tractable (Gibbs and Mackay, 1997; Williams and Barber, 1998; Neal, 1999). In our experiments we opt to use the Expectation Propagation (EP) method (Minka, 2001) which approximates the nonGaussian joint posterior with a Gaussian one. EP offers very good empirical results for many different likelihoods, although it has no proof of convergence. The complexity for the inference step is O(n3). Given that our data set is very large and the number of features is high, we conduct inference using the fully independent training conditional (FITC) approximation (Snelson and Ghahramani, 2006) with 500 random inducing points. We refer the interested reader to Rasmussen and Williams (2006) for further information on GP classification. Although we could use multi-c</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>Thomas P. Minka. 2001. Expectation propagation for approximate bayesian inference. In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, UAI ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Bayesian Learning for Neural Networks.</title>
<date>1996</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="17752" citStr="Neal, 1996" startWordPosition="2789" endWordPosition="2790">s used to encourage smooth functions. For the multidimensional pair of inputs (x,x&apos;), this is: � d _(xz — xi)2 2l? z where lz are lengthscale parameters learnt only using training data by performing gradient ascent on the type-II marginal likelihood. Intuitively, the lengthscale parameter lz controls the variation along the i input dimension, i.e. a low value makes the output very sensitive to input data, thus making that input more useful for the prediction. If the lengthscales are learnt separately for each input dimension the kernel is named SE with Automatic Relevance Determination (ARD) (Neal, 1996). Binary classification using GPs the real valued latent function output through a logistic function: °= P(y = = in a similar way to logistic regression classification. The object of the GP inference is the distri ‘squashes’ f(x) π(x) 1Jx) σ(f(x)) bution kard(x,x,) = σ2 exp , (4) 1757 of the latent variable corresponding to a test case x*: �P(f*|x,y,x*) = P(f*|x, x*,f)P(f|x,y)df , (5) where P(f|x,y) = P(y|f)P(f|x)/P(y|x) is the posterior over the latent variables. If the likelihood P(y|f) is Gaussian, the combination with a GP prior P(f|x) gives a posterior GP over functions. In binary classif</context>
</contexts>
<marker>Neal, 1996</marker>
<rawString>Radford M. Neal. 1996. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Regression and classification using gaussian process priors.</title>
<date>1999</date>
<journal>Bayesian Statistics</journal>
<volume>6</volume>
<pages>475--501</pages>
<contexts>
<context position="18768" citStr="Neal, 1999" startWordPosition="2946" endWordPosition="2947">P(y|f)P(f|x)/P(y|x) is the posterior over the latent variables. If the likelihood P(y|f) is Gaussian, the combination with a GP prior P(f|x) gives a posterior GP over functions. In binary classification, the distribution over the latent f* is combined with the logistic function to produce the prediction: �¯π* = σ(f*)P(f*|x,y,x*)df*. (6) This results in a non-Gaussian likelihood in the posterior formulation and therefore, exact inference is infeasible for classification models. Multiple approximations exist that make the computation tractable (Gibbs and Mackay, 1997; Williams and Barber, 1998; Neal, 1999). In our experiments we opt to use the Expectation Propagation (EP) method (Minka, 2001) which approximates the nonGaussian joint posterior with a Gaussian one. EP offers very good empirical results for many different likelihoods, although it has no proof of convergence. The complexity for the inference step is O(n3). Given that our data set is very large and the number of features is high, we conduct inference using the fully independent training conditional (FITC) approximation (Snelson and Ghahramani, 2006) with 500 random inducing points. We refer the interested reader to Rasmussen and Wil</context>
</contexts>
<marker>Neal, 1999</marker>
<rawString>Radford M. Neal. 1999. Regression and classification using gaussian process priors. Bayesian Statistics 6, pages 475–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On spectral clustering: Analysis and an algorithm.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="13974" citStr="Ng et al., 2002" startWordPosition="2173" endWordPosition="2176">words into a low dimensional space. In our experiments we tried the following dimensionalities: 30, 50, 100 and 200. The feature representation for each user is obtained summing over each of the embedding dimensions across all words. u1 u2 u3 u4 u5 u6 u7 u8 uy u10 u11 u12 u13 u14 u15 u16 u17 u18 1756 4.2.2 NPMI Clusters (SVD-C) We use the NPMI matrix described in the previous paragraph to create hard clusters of words. These clusters can be thought as ‘topics’, i.e. words that are semantically similar. From a variety of clustering techniques we choose spectral clustering (Shi and Malik, 2000; Ng et al., 2002), a hard-clustering approach which deals well with high-dimensional and non-convex data (von Luxburg, 2007). Spectral clustering is based on applying SVD to the graph Laplacian and aims to perform an optimal graph partitioning on the NPMI similarity matrix. The number of clusters needs to be pre-specified. We use 30, 50, 100 and 200 clusters – numbers were chosen a priori based on previous work (Lampos et al., 2014). The feature representation is the standardised number of words from each cluster. Although there is a loss of information compared to the original representation, the clusters are</context>
</contexts>
<marker>Ng, Jordan, Weiss, 2002</marker>
<rawString>Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2002. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems, NIPS, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana-Maria Popescu</author>
</authors>
<title>A machine learning approach to twitter user classification. ICWSM,</title>
<date>2011</date>
<pages>281--288</pages>
<contexts>
<context position="8151" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="1203" endWordPosition="1206">s •Job: legal clerk, company secretary Major Group (C5): Skilled Trades Occupations •Job: electrical fitter, tailor Major Group (C6): Caring, Leisure and Other Service Occupations •Job: nursery assistant, hairdresser Major Group (C7): Sales and Customer Service Occupations •Job: sales assistant, telephonist Major Group (C8): Process, Plant and Machine Operatives •Job: factory worker, van driver Major Group (C9): Elementary Occupations •Job: shelf stacker, bartender Table 1: Subset of the SOC classification hierarchy. is widespread when extracting large-scale data for user attribute inference (Pennacchiotti and Popescu, 2011; Coppersmith et al., 2014). Similarly to Hecht et al. (2011), we first assess the proportion of Twitter accounts with a clear mention to their occupation by annotating the user description field of a random set of 500 users. There were chosen from the random 1% sample, having at least 200 tweets in their history and with a majority of English tweets. There, we can identify the following categories: no description (12.2%), random information (22%), user information but not occupation related (45.8%), and job related information (20%). To create our data set, we thus use the user description fi</context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana-Maria Popescu. 2011. A machine learning approach to twitter user classification. ICWSM, pages 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Simon Rogers</author>
<author>Mark Girolami</author>
</authors>
<title>Protein interaction detection in sentences via gaussian processes; a preliminary evaluation.</title>
<date>2011</date>
<journal>International Journal ofData Mining and Bioinformatics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="16854" citStr="Polajnar et al., 2011" startWordPosition="2631" endWordPosition="2634">cesses In this section, we briefly overview Gaussian Process (GP) for classification, highlighting our motivation for using this method. GPs formulate a Bayesian non-parametric machine learning framework which defines a prior on functions (Rasmussen and Williams, 2006). The properties of the functions are given by a kernel which models the covariance in the response values as a function of its inputs. Although GPs form a powerful learning tool, they have only recently been used in NLP research (Cohn and Specia, 2013; Preoiiuc-Pietro and Cohn, 2013) with classification applications limited to (Polajnar et al., 2011). Formally, GP methods aim to learn a function f : Rd —* R drawn from a GP prior given the inputs x E Rd: f(x) — 9P(m(x), k(x,x&apos;)) , (3) where m(·) is the mean function (here 0) and k(·, ·) is the covariance kernel. Usually, the Squared Exponential (SE) kernel (a.k.a. RBF or Gaussian) is used to encourage smooth functions. For the multidimensional pair of inputs (x,x&apos;), this is: � d _(xz — xi)2 2l? z where lz are lengthscale parameters learnt only using training data by performing gradient ascent on the type-II marginal likelihood. Intuitively, the lengthscale parameter lz controls the variati</context>
</contexts>
<marker>Polajnar, Rogers, Girolami, 2011</marker>
<rawString>Tamara Polajnar, Simon Rogers, and Mark Girolami. 2011. Protein interaction detection in sentences via gaussian processes; a preliminary evaluation. International Journal ofData Mining and Bioinformatics, 5(1):52–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A temporal model of text periodicities using Gaussian Processes.</title>
<date>2013</date>
<publisher>EMNLP.</publisher>
<marker>Preot¸iuc-Pietro, Cohn, 2013</marker>
<rawString>Daniel Preot¸iuc-Pietro and Trevor Cohn. 2013. A temporal model of text periodicities using Gaussian Processes. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Sina Samangooei</author>
<author>Trevor Cohn</author>
<author>Nicholas Gibbins</author>
<author>Mahesan Niranjan</author>
</authors>
<title>Trendminer: An architecture for real time analysis of social media text.</title>
<date>2012</date>
<booktitle>In Workshop on Real-Time Analysis and Mining of Social Streams (RAMSS), ICWSM.</booktitle>
<marker>Preot¸iuc-Pietro, Samangooei, Cohn, Gibbins, Niranjan, 2012</marker>
<rawString>Daniel Preot¸iuc-Pietro, Sina Samangooei, Trevor Cohn, Nicholas Gibbins, and Mahesan Niranjan. 2012. Trendminer: An architecture for real time analysis of social media text. In Workshop on Real-Time Analysis and Mining of Social Streams (RAMSS), ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying Latent User Attributes in Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd International Workshop on Search and Mining Usergenerated Contents, SMUC,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="1771" citStr="Rao et al., 2010" startWordPosition="259" endWordPosition="262"> that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opinions in each social class or integration in text regression tasks such as voting intention (Lampos et al., 2013). Social status reflected through a person’s occupation is a factor which influences language use (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Therefore, our hypothesis is that language</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying Latent User Attributes in Twitter. In Proceedings of the 2nd International Workshop on Search and Mining Usergenerated Contents, SMUC, pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<date>2006</date>
<booktitle>Gaussian Processes for Machine Learning.</booktitle>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="4405" citStr="Rasmussen and Williams, 2006" startWordPosition="656" endWordPosition="660">ation and offer qualitative insights. We find that text features, especially word clusters, lead to good predictive performance. Accuracy for our best model is well above 50% for 9-way classifi1754 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1754–1764, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation, outperforming competitive methods. The best results are obtained using the Bayesian nonparametric framework of Gaussian Processes (Rasmussen and Williams, 2006), which also accommodates feature interpretation via the Automatic Relevance Determination. This allows us to get insight into differences in language use across job classes and, finally, assess our original hypothesis about the thematic divergence across them. 2 Standard Occupational Classification To enable the user occupation study, we adopt a standardised job classification taxonomy for mapping Twitter users to occupations. The Standard Occupational Classification (SOC)1 is a UK government system developed by the Office of National Statistics for classifying occupations. Jobs are categoris</context>
<context position="16501" citStr="Rasmussen and Williams, 2006" startWordPosition="2572" endWordPosition="2576"> Neural Clusters (W2V-C) Similar to the NPMI cluster, we use the neural embeddings in order to obtain clusters of related words, i.e. ‘topics’. We derive a word to word similarity matrix using cosine similarity on the neural embeddings. We apply spectral clustering on this matrix to obtain 30, 50, 100 and 200 word clusters. 5 Classification with Gaussian Processes In this section, we briefly overview Gaussian Process (GP) for classification, highlighting our motivation for using this method. GPs formulate a Bayesian non-parametric machine learning framework which defines a prior on functions (Rasmussen and Williams, 2006). The properties of the functions are given by a kernel which models the covariance in the response values as a function of its inputs. Although GPs form a powerful learning tool, they have only recently been used in NLP research (Cohn and Specia, 2013; Preoiiuc-Pietro and Cohn, 2013) with classification applications limited to (Polajnar et al., 2011). Formally, GP methods aim to learn a function f : Rd —* R drawn from a GP prior given the inputs x E Rd: f(x) — 9P(m(x), k(x,x&apos;)) , (3) where m(·) is the mean function (here 0) and k(·, ·) is the covariance kernel. Usually, the Squared Exponentia</context>
<context position="19380" citStr="Rasmussen and Williams (2006)" startWordPosition="3041" endWordPosition="3044">1998; Neal, 1999). In our experiments we opt to use the Expectation Propagation (EP) method (Minka, 2001) which approximates the nonGaussian joint posterior with a Gaussian one. EP offers very good empirical results for many different likelihoods, although it has no proof of convergence. The complexity for the inference step is O(n3). Given that our data set is very large and the number of features is high, we conduct inference using the fully independent training conditional (FITC) approximation (Snelson and Ghahramani, 2006) with 500 random inducing points. We refer the interested reader to Rasmussen and Williams (2006) for further information on GP classification. Although we could use multi-class classification methods, in order to provide insight, we perform a separate one-vs-all classification for each class and then determine a label through the occupational class that has the highest likelihood. 6 Experiments This section presents the experimental results for our task. We first compare the accuracy of our classification methods on held out data using each feature set and conduct a standard error analysis. We then use the interpretability of the ARD lengthscales from the GP classifier to further analyse</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Henry Kautz</author>
<author>Vincent Silenzio</author>
</authors>
<title>Modeling Spread of Disease from Social Interactions.</title>
<date>2012</date>
<booktitle>In Proc. of 6th International Conference on Weblogs and Social Media,</booktitle>
<pages>322--329</pages>
<contexts>
<context position="1469" citStr="Sadilek et al., 2012" startWordPosition="212" endWordPosition="215">ially, non-linear methods can predict a user’s occupational class with strong accuracy for the coarsest level of a standard occupation taxonomy which includes nine classes. Combined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also for other applications: recommender systems to provide targeted advertising, analysts who study different opi</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>Adam Sadilek, Henry Kautz, and Vincent Silenzio. 2012. Modeling Spread of Disease from Social Interactions. In Proc. of 6th International Conference on Weblogs and Social Media, pages 322–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Schmidt</author>
<author>Robert P Strauss</author>
</authors>
<title>The prediction of occupation using multiple logit models.</title>
<date>1975</date>
<journal>International Economic Review,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="33794" citStr="Schmidt and Strauss (1975)" startWordPosition="5357" endWordPosition="5360">ic usage distributions was statistically significant (p &lt; 10−5). ‘Corporate’ topic, whereas ‘Football’ registers the lowest distance. 7 Related Work Occupational class prediction has been studied in the past in the areas of psychology and economics. French (1959) investigated the relation between various measures on 232 undergraduate students and their future occupations. This study concluded that occupational membership can be predicted from variables such as the ability of subjects in using mathematical and verbal symbols, their family economic status, body-build and personality components. Schmidt and Strauss (1975) also studied the relationship between job types (five classes) and certain demographic attributes (gender, race, experience, education, location). Their analysis identified biases or discrimination which possibly exist in different types of jobs. Sociolinguistic and sociology studies deduct that social status is an important factor in determining the use of language (Bernstein, 1960; Bernstein, 2003; Labov, 2006). Differences arise either due to language use or due to the topics people discuss as parts of various social domains. However, a large scale investigation of this hypothesis has neve</context>
</contexts>
<marker>Schmidt, Strauss, 1975</marker>
<rawString>Peter Schmidt and Robert P Strauss. 1975. The prediction of occupation using multiple logit models. International Economic Review, 16(2):471–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianbo Shi</author>
<author>Jitendra Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>2000</date>
<journal>Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>22</volume>
<issue>8</issue>
<contexts>
<context position="13956" citStr="Shi and Malik, 2000" startWordPosition="2169" endWordPosition="2172">tain an embedding of words into a low dimensional space. In our experiments we tried the following dimensionalities: 30, 50, 100 and 200. The feature representation for each user is obtained summing over each of the embedding dimensions across all words. u1 u2 u3 u4 u5 u6 u7 u8 uy u10 u11 u12 u13 u14 u15 u16 u17 u18 1756 4.2.2 NPMI Clusters (SVD-C) We use the NPMI matrix described in the previous paragraph to create hard clusters of words. These clusters can be thought as ‘topics’, i.e. words that are semantically similar. From a variety of clustering techniques we choose spectral clustering (Shi and Malik, 2000; Ng et al., 2002), a hard-clustering approach which deals well with high-dimensional and non-convex data (von Luxburg, 2007). Spectral clustering is based on applying SVD to the graph Laplacian and aims to perform an optimal graph partitioning on the NPMI similarity matrix. The number of clusters needs to be pre-specified. We use 30, 50, 100 and 200 clusters – numbers were chosen a priori based on previous work (Lampos et al., 2014). The feature representation is the standardised number of words from each cluster. Although there is a loss of information compared to the original representation</context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation. Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Sloan</author>
<author>Jeffrey Morgan</author>
<author>Pete Burnap</author>
<author>Matthew Williams</author>
</authors>
<title>Who tweets? Deriving the demographic characteristics of age, occupation and social class from twitter user meta-data.</title>
<date>2015</date>
<journal>PloS one,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="35534" citStr="Sloan et al. (2015)" startWordPosition="5633" endWordPosition="5636">g content based and network features. However, there exist significant differences from our task since this inference is based on a distinct platform, with an ambiguous distribution over occupations (e.g. more than 25% related to media), while the occupational classes are not generic (e.g. media, welfare and electronic are three of the twelve categories). Most importantly, the applied model did not allow for a qualitative interpretation. Filho et al. (2014) inferred the social class of social media users by combining geolocation information derived from Foursquare and Twitter posts. Recently, Sloan et al. (2015) introduced tools for the automated extraction of demographic data (age, occupation and social class) from the profile descriptions of Twitter users using a similar method to our data set extraction approach. They showed that it is feasible to build a data set that matches the real-world UK occupation distribution as given by the SOC. 8 Conclusions Our paper presents the first large-scale systematic study on language use on social media as a factor for inferring a user’s occupational class. To address this problem, we have also introduced an extensive labelled data set extracted from Twitter. </context>
</contexts>
<marker>Sloan, Morgan, Burnap, Williams, 2015</marker>
<rawString>Luke Sloan, Jeffrey Morgan, Pete Burnap, and Matthew Williams. 2015. Who tweets? Deriving the demographic characteristics of age, occupation and social class from twitter user meta-data. PloS one, 10(3):e0115545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Snelson</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Sparse gaussian processes using pseudo-inputs.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS,</booktitle>
<pages>1257--1264</pages>
<contexts>
<context position="19283" citStr="Snelson and Ghahramani, 2006" startWordPosition="3026" endWordPosition="3029">imations exist that make the computation tractable (Gibbs and Mackay, 1997; Williams and Barber, 1998; Neal, 1999). In our experiments we opt to use the Expectation Propagation (EP) method (Minka, 2001) which approximates the nonGaussian joint posterior with a Gaussian one. EP offers very good empirical results for many different likelihoods, although it has no proof of convergence. The complexity for the inference step is O(n3). Given that our data set is very large and the number of features is high, we conduct inference using the fully independent training conditional (FITC) approximation (Snelson and Ghahramani, 2006) with 500 random inducing points. We refer the interested reader to Rasmussen and Williams (2006) for further information on GP classification. Although we could use multi-class classification methods, in order to provide insight, we perform a separate one-vs-all classification for each class and then determine a label through the occupational class that has the highest likelihood. 6 Experiments This section presents the experimental results for our task. We first compare the accuracy of our classification methods on held out data using each feature set and conduct a standard error analysis. W</context>
</contexts>
<marker>Snelson, Ghahramani, 2006</marker>
<rawString>Edward Snelson and Zoubin Ghahramani. 2006. Sparse gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems, NIPS, pages 1257–1264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm Oliver Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment.</title>
<date>2010</date>
<booktitle>In Proc. of 4th International Conference on Weblogs and Social Media,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="1360" citStr="Tumasjan et al., 2010" startWordPosition="194" endWordPosition="197">ion using latent feature representations such as word clusters and embeddings. The employed linear and, especially, non-linear methods can predict a user’s occupational class with strong accuracy for the coarsest level of a standard occupation taxonomy which includes nine classes. Combined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user attribute that can be embedded in a multitude of downstream applications. 1 Introduction The growth of online social networks provides the opportunity to analyse user text in a broader context (Tumasjan et al., 2010; Bollen et al., 2011; Lampos and Cristianini, 2012). This includes the social network (Sadilek et al., 2012), spatio-temporal information (Lampos and Cristianini, 2010) and personal attributes (Al Zamal et al., 2012). Previous research has analysed language differences in user attributes like location (Cheng et al., 2010), gender (Burger et al., 2011), impact (Lampos et al., 2014) and age (Rao et al., 2010), showing that language use is influenced by them. Therefore, user text allows us to infer these properties. This user profiling is important not only for sociolinguistic studies, but also </context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm Oliver Sprenger, Philipp G Sandner, and Isabell M Welpe. 2010. Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment. In Proc. of 4th International Conference on Weblogs and Social Media, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="21190" citStr="Vapnik, 1998" startWordPosition="3328" endWordPosition="3329">: 9-way classification accuracy on held-out data for our 3 methods. Textual features are obtained using SVD or Word2Vec (W2V). E represents embeddings, C clusters. The final number denotes the amount of clusters or the size of the embedding. features at a time. Experiments combining features yielded only minor improvements. We apply common linear and non-linear methods together with our proposed GP classifier. The linear method is logistic regression (LR) with Elastic Net regularisation (Freedman, 2009) and the non-linear one is formulated by a Support Vector Machine (SVM) with an RBF kernel (Vapnik, 1998). The accuracy of our classifiers is measured on held-out data. Our data set is divided into stratified training (80%), validation (10%) and testing (10%) sets. The validation set was used to learn the LR and SVM hyperparameters, while the GP did not use this set at all. We report results using all three methods and all feature sets in Table 3. We first observe that user level features (UserLevel; see Section 4.1) are not useful for predicting the job class. This finding indicates that general social behaviour or user impact are likely to be spread evenly across classes. It also highlights the</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N Vapnik. 1998. Statistical learning theory. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike von Luxburg</author>
</authors>
<title>A tutorial on spectral clustering.</title>
<date>2007</date>
<booktitle>Statistics and computing,</booktitle>
<pages>17--4</pages>
<marker>von Luxburg, 2007</marker>
<rawString>Ulrike von Luxburg. 2007. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher K I Williams</author>
<author>David Barber</author>
</authors>
<title>Bayesian classification with gaussian processes.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<pages>12--1342</pages>
<contexts>
<context position="18755" citStr="Williams and Barber, 1998" startWordPosition="2942" endWordPosition="2945">)df , (5) where P(f|x,y) = P(y|f)P(f|x)/P(y|x) is the posterior over the latent variables. If the likelihood P(y|f) is Gaussian, the combination with a GP prior P(f|x) gives a posterior GP over functions. In binary classification, the distribution over the latent f* is combined with the logistic function to produce the prediction: �¯π* = σ(f*)P(f*|x,y,x*)df*. (6) This results in a non-Gaussian likelihood in the posterior formulation and therefore, exact inference is infeasible for classification models. Multiple approximations exist that make the computation tractable (Gibbs and Mackay, 1997; Williams and Barber, 1998; Neal, 1999). In our experiments we opt to use the Expectation Propagation (EP) method (Minka, 2001) which approximates the nonGaussian joint posterior with a Gaussian one. EP offers very good empirical results for many different likelihoods, although it has no proof of convergence. The complexity for the inference step is O(n3). Given that our data set is very large and the number of features is high, we conduct inference using the fully independent training conditional (FITC) approximation (Snelson and Ghahramani, 2006) with 500 random inducing points. We refer the interested reader to Rasm</context>
</contexts>
<marker>Williams, Barber, 1998</marker>
<rawString>Christopher K.I Williams and David Barber. 1998. Bayesian classification with gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20 (12):1342–1351.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>