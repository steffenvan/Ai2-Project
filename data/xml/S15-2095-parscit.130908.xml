<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005433">
<title confidence="0.9383675">
Lsislif: Feature Extraction and Label Weighting for Sentiment Analysis in
Twitter
</title>
<author confidence="0.99769">
Hussam Hamdan Patrice Bellot Frederic Bechet
</author>
<affiliation confidence="0.999426">
Aix-Marseille University Aix-Marseille University Aix-Marseille University
</affiliation>
<email confidence="0.995235">
hussam.hamdan@lsis.org patrice.bellot@lsis.org frederic.bechet@lif.univ-mrs.fr
</email>
<sectionHeader confidence="0.995602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987164">
This paper describes our sentiment analysis
systems which have been built for SemEval-
2015 Task 10 Subtask B and E. For sub-
task B, a Logistic Regression classifier has
been trained after extracting several groups of
features including lexical, syntactic, lexicon-
based, Z score and semantic features. A
weighting schema has been adapted for pos-
itive and negative labels in order to take into
account the unbalanced distribution of tweets
between the positive and negative classes.
This system is ranked third over 40 partici-
pants, it achieves average F1 64.27 on Twit-
ter data set 2015 just 0.57% less than the first
system. We also present our participation in
Subtask E in which our system has got the sec-
ond rank with Kendall metric but the first one
with Spearman for ranking twitter terms ac-
cording to their association with the positive
sentiment.
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944347826087">
Twitter is one of the most social media platforms
which allows the users to express their opinions and
feelings towards different issues. The users have
become an important source of content. This con-
tent may be interesting to analyze for those who are
interested in understanding user’s interests such as
buyers, sellers and producers.
Sentiment Analysis can be done in different lev-
els; Document level; Sentence level; Clause level or
Aspect-Based level. SA in Twitter can be seen as
sentence level task, but some limitations should be
considered in such sentences. The size of tweet is
limited to 140 characters, informal language, emo-
tion icons and non-standard expressions are very
used, and many spelling errors can be find due to
the absence of correctness verification.
Three different approaches can be identified in the
literature of Sentiment Analysis in Twitter, the first
approach is a lexicon based which uses specific
types of lexicons to derive the polarity of a text,
this approach suffers from the limited size of lexi-
con and requires human expertise to build manual
lexicons, in the other hand the automatic lexicons
needs labeled data. The second approach is ma-
chine learning one which uses annotated texts with
given labels to learn a classifying model. Both lex-
icon and machine learning approaches can be com-
bined to achieve a better performance. These two
approaches are used for SA task but the third one is
specific for Twitter or social content, the social ap-
proach exploits social network properties and data
for enhancing the accuracy of the classification.
In this paper, we present our supervised system
which adapts a logistic regression classifier with
several groups of features and weighting schema
for positive and negative labels. The features are
grouped into 5 groups: word n-gram, lexicon-based,
negation, Z score and semantic features. We also
describe our system used for ranking terms accord-
ing to their positivity, in which we derive the term
polarity score from different lexicons.
The rest of this paper is organized as follows. Sec-
tion 2 outlines existing work of sentiment analysis in
Twitter. Section 3 describes the data and resources.
The features we used for training the classifier pre-
sented in Section 4. Our experiments are described
</bodyText>
<page confidence="0.988794">
568
</page>
<bodyText confidence="0.847472">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 568–573,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
in section 5, our participation in subtask E is de-
scribed in section 6 and future work is presented in
section 7.
used; they reported that social theories such as Sen-
timent Consistency and Emotional Contagion could
be helpful for sentiment analysis.
</bodyText>
<sectionHeader confidence="0.999328" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997951261904762">
Three main approaches for sentiment analysis can
be identified in Twitter. The lexicon based ap-
proach which depends on sentiment lexicons con-
taining positive, negative and neutral words or ex-
pressions; the polarity is computed according to the
number of common opinionated words between the
lexicons and the text. Many dictionaries have been
created manually such as MPQA Lexicon (Wilson
et al., 2005) or automatically such as SentiWordNet
(Baccianella et al., 2010).
Machine learning approach adapts different clas-
sifiers and features. Naive Bayes, Maximum En-
tropy MaxEnt and Support Vector Machines (SVM)
were adapted in (Go et al., 2009) in which the au-
thors reported that SVM outperforms other classi-
fiers. They tried a unigram and a bi-gram model
in conjunction with parts-of-speech (POS) features;
they noted that the unigram model outperforms all
other models when using SVM and that POS fea-
tures decrease the results. Authors in (Hamdan et al.,
4 29) used the concepts extracted from DBPedia
and the adjectives from WordNet, they reported that
the DBpedia concepts are useful with Nave-Bayes
classifier but less useful with SVM. Many features
were used with SVM including the lexicon-based
features in (Mohammad et al., 2013) which seem to
get the most gain in performance. Another work has
also proved the importance of lexicon-based features
with logistic regression classifier (Miura et al., 4 08;
Hamdan et al., 2015a; Hamdan et al., 2015b).
The third main approach takes into account the
influence of users on their followers and the relation
between the users and the tweets they wrote. It as-
sumes that using the Twitter follower graph might
improve the polarity classification. In (Speriosu
et al., 2011) authors demonstrated that using label
propagation with Twitter follower graph improves
the polarity classification. In (Tan et al., 2011) au-
thors employed social relation for user-level senti-
ment analysis. In (Hu et al., 2013) a Sociologi-
cal Approach to handling the Noisy and short Text
(SANT) for supervised sentiment classification is
</bodyText>
<sectionHeader confidence="0.920502" genericHeader="method">
3 Data and Resources
</sectionHeader>
<subsectionHeader confidence="0.999685">
3.1 Labeled Data
</subsectionHeader>
<bodyText confidence="0.997679444444444">
We used the data set provided in SemEval 2013 for
subtask B of sentiment analysis in Twitter (Nakov
et al., 2013). The participants have been provided
with training tweets annotated positive, negative
or neutral. We downloaded these tweets using the
given script. We obtained 9646 tweets, the whole
training data set is used for training, the provided
development set containing 1654 tweets is used for
tuning the machine learner. The test data set 2015
contains about 2390 tweets (Rosenthal et al., 5 06).
Table 1 shows the distribution of each label in each
data set.
Twitter all neg. pos. neut.
train 9684 1458 3640 4586
dev 1654 340 739 575
test-2015 2390 365 1038 987
Table1. Sentiment labels distribution in the training
and development, test data sets.
</bodyText>
<subsectionHeader confidence="0.999918">
3.2 Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.999782894736842">
The system exploits two types of sentiment lexi-
cons: manual constructed lexicons and automatic
ones. The manual ones are the Bing Lius Opin-
ion Lexicon which is created in (Hu and Liu,
2004) and augmented in many research papers; and
MPQA subjectivity lexicons (Wilson et al., 2005).
Both lexicons contain English words annotated pos-
itive and negative. While the automatic lexicons
are NRC Hashtag Sentiment Lexicon (Mohammad,
6 07), Sentiment140 Lexicon (Mohammad et al.,
2013), and SentiWordNet (Baccianella et al., 2010).
NRC Hashtag Sentiment Lexicon and Sentiment140
Lexicon contain tweet terms with scores, positive
score indicates association with positive sentiment,
whereas negative score indicates association with
negative sentiment. NRC has entries for 54,129 un-
igrams and 316,531 bigrams; Sentiment140 has en-
tries for 62,468 unigrams, 677,698 bigrams. Sen-
tiWordNet is the result of automatically annotating
</bodyText>
<page confidence="0.99075">
569
</page>
<bodyText confidence="0.9976015">
all WORDNET synsets according to their degrees
of positivity, negativity, and neutrality.
</bodyText>
<subsectionHeader confidence="0.999135">
3.3 Twitter Dictionary
</subsectionHeader>
<bodyText confidence="0.999974428571429">
We constructed a dictionary for the abbreviations
and the slang words used in Twitter in order to over-
come the ambiguity of the these terms. This dictio-
nary maps certain twitter expressions and emotion
icons by their meaning or their corresponding sen-
timent (e.g. gr8 replaced by great, :) replaced by
very-happy).
</bodyText>
<sectionHeader confidence="0.979729" genericHeader="method">
4 Feature Extraction
</sectionHeader>
<subsectionHeader confidence="0.998034">
4.1 Word ngrams
</subsectionHeader>
<bodyText confidence="0.9995855">
unigram and bigram are extracted for each word in
text without any stemming or stop-word removing,
all terms with occurrence less than 3 are removed
from the feature space.
</bodyText>
<subsectionHeader confidence="0.99285">
4.2 Negation Features
</subsectionHeader>
<bodyText confidence="0.999949285714286">
The rule-based algorithm presented in Christo-
pher Potts Sentiment Symposium Tutorial is imple-
mented. This algorithm appends a negation suffix to
all words that appear within a negation scope which
is determined by the negation key and a punctua-
tion. All these words have been added to the feature
space.
</bodyText>
<subsectionHeader confidence="0.998954">
4.3 Twitter dictionary
</subsectionHeader>
<bodyText confidence="0.9999965">
All terms presented in a text and in the twitter dic-
tionary presented in 3.3 are mapped to their corre-
sponding terms in the dictionary and added to the
feature space.
</bodyText>
<subsectionHeader confidence="0.999796">
4.4 Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.999995666666667">
The system extracts four features from the manual
constructed lexicons and six features from the auto-
matic ones. For each sentence the number of posi-
tive words, the number of negative ones, the number
of positive words divided by the number of negative
ones and the polarity of the last word are extracted
from manual constructed lexicons. In addition to the
sum of the positive scores and the sum of the nega-
tive scores from the automatic constructed lexicons.
</bodyText>
<subsectionHeader confidence="0.981386">
4.5 Z score
</subsectionHeader>
<bodyText confidence="0.999928">
Z score can distinguish the importance of each
term in each class, their performances have been
proved in (Hamdan et al., 2014). We assume as
in the mentioned work that the term frequencies
are following a multi-nomial distribution. Thus,
Z score can be seen as a standardization of the
term frequency using multi-nomial distribution.
We compute the Z score for each term ti in a class
Cj (tij) by calculating its term relative frequency
tfrij in a particular class Cj, as well as the mean
(meani) which is the term probability over the
whole corpus multiplied by the number of terms
in the class Cj, and standard deviation (sdi) of
term ti according to the underlying corpus. Like in
(Hamdan et al., 4 29) we tested different thresholds
for choosing the words which have higher Z score.
</bodyText>
<equation confidence="0.968339666666667">
tfrij − meani
Zscore(ti) = (1)
sdi
</equation>
<bodyText confidence="0.999916285714286">
Thus, we added the number of words having Z
score higher than the threshold in each class pos-
itive,negative and neutral, the two classes which
have the maximum number and minimum number
of words having Z score higher than the thresh-
old. These 5 features have been added to the feature
space.
</bodyText>
<subsectionHeader confidence="0.993776">
4.6 Semantic Features
</subsectionHeader>
<bodyText confidence="0.99995725">
The semantic representation of a text may bring
some important hidden information, which may re-
sult in a better text representation and a better clas-
sification system.
</bodyText>
<subsectionHeader confidence="0.759974">
4.6.1 Brown Dictionary Features
</subsectionHeader>
<bodyText confidence="0.999943571428571">
Each word in the text is mapped to its cluster
in Brown, 1000 features are added to feature space
where each feature represents the number of words
in the text mapped to each cluster. The 1000 clusters
is provided in Twitter Word Clusters of CMU ARK
group. 1000 clusters were constructed from approx-
imately 56 million tweets.
</bodyText>
<subsubsectionHeader confidence="0.715297">
4.6.2 Topic features
</subsubsectionHeader>
<bodyText confidence="0.999930333333333">
Latent dirichlet association or topic modeling is
used to extract 10 features. Lda-c is configured with
10 topics and the training data is used for training
the model, then for each sentence in the test set,
the trained model estimates the number of words as-
signed to each topic.
</bodyText>
<page confidence="0.97338">
570
</page>
<subsubsectionHeader confidence="0.564268">
4.6.3 Semantic Role Labeling Features
</subsubsectionHeader>
<bodyText confidence="0.998505416666667">
Authors in (Ruppenhofer and Rehbein, 2012) en-
code semantic role labeling features in SVM classi-
fier. Our system also extract two types of features,
the names: the whole term which represents an argu-
ment of the predicate and the tags: the type of each
argument in the text (A0 represents the subject of
predicate, A1 the object, AM-TMP the time, AM-
ADV the situation, AM-loc the location). These
encodings are defined by the tool which we used
(Senna). We think that the predicate arguments can
constitute a multi-word expression which may be
helpful in Sentiment Classification.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997751">
5.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999967">
We trained the L1-regularized Logistic regression
classifier implemented in LIBLINEAR (Fan et al.,
2008). The classifier is trained on the training data
set using the features of Section 4 with the three
polarities (positive, negative, and neutral) as labels.
A weighting schema is adapted for each class, we
use the weighting option −wi which enables a use
of different cost parameter C for different classes.
Since the training data is unbalanced, this weighting
schema adjusts the probability of each label. Thus,
we tuned the classifier in adjusting the cost param-
eter C of Logistic Regression, weight wpos of pos-
itive class and weight Wneg of negative class. We
used the development set for tuning the three param-
eters, all combinations of C in range 0.1 to to 4 by
step 0.1, wpos in range 1 to 8 by step 0.1, wneg in
range 1 to 8 by step 0.1 are tested. The combination
C=0.2, wpos=5.2, wneg=4.2 have given the best F1
score for the development set and therefore it was
selected for our submission.
</bodyText>
<subsectionHeader confidence="0.882528">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999155264705882">
The evaluation score used by the task organizers was
the averaged F1-score of the positive and negative
classes. In the SemEval-2015 competition, our sub-
mission is ranked third (64.27) over 40 submissions,
just 0.57% less than the first system.
Table 2 shows the results of our experiments after
removing a feature group at each run for the three
test sets 2013, 2014, and 2015. For the test set 2015,
we note that using Z score feature provides a gain
of 0.45%, n-gram provides a gain of 0.28%, lexi-
con features gain is about 3.31%, LDA gain is 0.8%,
Brown clusters 0.44%, semantic role labeling de-
creases the F1 score by 0.83%. The most influen-
tial features is the sentiment lexicon features; they
provided gains of 3.31%.
Because of negative effect of semantic role
labeling features, we have done another analysis
in order to estimate if these features are useful or
not, the fact that the combination of features makes
some of them not influential are not sufficient to
consider the features not useful. Thus, we repeat
the same classification process but add one feature
group at a time (Tabel 3). Z score seems to give
gain of 1.91%, LDA topics gain is 0.66%, semantic
role labeling 0.64%, brown clusters 3.38% and
sentiment lexicons 6.58%. The most influential
features is also the sentiment lexicon features.
Brown cluster features obtains an interesting gain
of 3.38%. From the previous two analysis, we
find that sentiment lexicon features are the most
influential ones as concluded by (S. M. Mohammad
et al., 2013). Some features have improved the
performance in test set 2015 but not in the other test
sets such as Z score, Semantic Role Labeling.
</bodyText>
<table confidence="0.829188">
Run Test-2015 Test-2014 Test-2013
All features 64.27 71.54 71.34
all-zscore 63.82 73.05 69.99
all-lexicons 60.96 67.6 66.63
all-ngram 63.99 69.06 69.67
all-srl 65.1 71.81 70.41
all-topics 63.47 71.49 71
all-brown 63.82 70.74 69.9
</table>
<bodyText confidence="0.89204175">
Table2. The F1 score for each run, All features
run exploits all features while the others remove a
feature group at each run Zscore, lexicons, n-gram,
srl, topics and brown cluster, respectively.
</bodyText>
<table confidence="0.957741714285714">
Run Test-2015 Test-2014 Test-2013
bl 57.47 66.71 66.25
bl+lexicon 64.05 70.57 69.31
bl+zscore 59.38 63.47 65.28
bl+brown 60.85 66.71 66.25
bl+topics 58.13 - -
bl+srl 58.13 66.69 63.35
</table>
<tableCaption confidence="0.3078255">
Table3. The F1 score for each run, bl run exploits
the n-gram, negation, twitter dictionary features
</tableCaption>
<page confidence="0.996514">
571
</page>
<bodyText confidence="0.999225666666667">
while the other runs add to bl one feature group at
each run, lexicon, Zscore, brown, topics, slr features
have been respectively added.
</bodyText>
<sectionHeader confidence="0.823661" genericHeader="method">
6 SubTask E: determining strength of
</sectionHeader>
<subsectionHeader confidence="0.9818015">
association of Twitter terms with positive
sentiment
</subsectionHeader>
<bodyText confidence="0.999978692307692">
This subtask is new in SemEval-2015, the objective
is to provide for each Twitter term a score between
0 and 1 that is indicative of its strength of associa-
tion with positive sentiment. If a word is more pos-
itive than another, then it should have a higher score
than the other. Participants are provided with 200
terms with their scores as a trail data. The test data
includes 1315 terms to rank. The organizers have
chosen Kendall’s Tau correlation coefficient to com-
pare the ranked lists, they have also provided the
scores of Spearman’s Rank Correlation, but partic-
ipating teams will be ranked according to Kendall’s
Tau.
To rank these terms, we have used six different
sentiment lexicons for computing the score for each
twitter term. Four of them are described in sec-
tion 3.2 (manaul lexicons: Bing Liu and MPQA
Subjectivity Lexicon , automatic constructed lexi-
cons: NRC Hashtag and Sentiment140 ) and we
have built two other automatic construction lexi-
cons: the first named PMi-Sem from the training
tweets provided by SemEval-2013 sub-task B Ta-
ble 1, the second named PMI-sentiment140 from the
sentiment140 corpus (Go et al., 2009), we calculated
PMI from the labeled tweets for the two corpus us-
ing the following equation:
</bodyText>
<equation confidence="0.972861">
PMI(word,positive) = log p(positive).p(word)
(2)
</equation>
<bodyText confidence="0.99910075">
where p(positive,word): The joint probability of the
positive class and the word. p(positive): the proba-
bility of positive class. p(word): the probability of
the word in whole corpus.
</bodyText>
<subsectionHeader confidence="0.999857">
6.1 Score computing
</subsectionHeader>
<bodyText confidence="0.999989222222222">
If the word exists in a manual constructed lexicon
(two lexicons), a score of 1 is assigned if the word
is positive else -1 if negative. If the word exists in
an automatic constructed lexicon (four lexicons),
the lexicon score of the word is used. For each
lexicon which does not have the word a default
score is assigned, this default score is chosen to be
1/(number of the words in the test set). the final
score is the average score of the previous six scores.
</bodyText>
<table confidence="0.98723875">
Run Kendall Spearman
all 0.621 0.820
all-BingLiu 0.616 0.816
all-MPQA 0.616 0.815
all-NRC Hashtag 0.510 0.689
all-Sentiment140 0.617 0.813
all-PMI-Sem 0.620 0.822
all-PMI-sentiment140 0.621 0.821
</table>
<bodyText confidence="0.9971785">
Table4. The results of Twitter term ranking, the
first run all exploits all six lexicons, one lexicon is
removed in the following runs.
The test data set contains 1315 twitter terms.
Our system is ranked second with Kendall 0.004%
less than the first ranked system, but first with
Spearman. Table 4 shows our results with the two
evaluation metrics. We repeat the experiment after
removing one lexicon at each run, we can note that
NRC Hashtag is the most influential lexicon.
</bodyText>
<sectionHeader confidence="0.987885" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.989430058823529">
In this paper, we tested the impact of combining sev-
eral groups of features on the sentiment classifica-
tion of tweets. A logistic regression classifier with
weighting schema is used, the sentiment lexicon-
based features seem to get the most influential effect
with the combination.
We have also exploited four existing lexicons and
constructed two other lexicons using PMI metric in
order to rank the twitter terms according to their as-
sociation with positive sentiment.
As the sentiment lexicon-based features have proved
their performance, future work will focus on the au-
tomatic lexicon construction on testing several met-
rics like Z score which we think promising in mea-
suring the association between each term and senti-
ment labels.
p(positive, word)
</bodyText>
<page confidence="0.976994">
572
</page>
<sectionHeader confidence="0.989839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999407875">
Baccianella, S., Esuli, A., and Sebastiani, F. (2010). Sen-
tiWordNet 3.0: An enhanced lexical resource for sen-
timent analysis and opinion mining. In in Proc. of
LREC.
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and
Lin, C.-J. (2008). LIBLINEAR: A library for large
linear classification. 9:1871–1874.
Go, A., Bhayani, R., and Huang, L. (2009). Twitter sen-
timent classification using distant supervision. pages
1–6.
Hamdan, H., Bechet, F., and Bellot, P. (2013-04-29). Ex-
periments with DBpedia, WordNet and SentiWordNet
as resources for sentiment analysis in micro-blogging.
In International Workshop on Semantic Evaluation
SemEval-2013 (NAACL Workshop).
Hamdan, H., Bellot, P., and Bechet, F. (2014). The im-
pact of z score on twitter sentiment analysis. In In
Proceedings of the Eighth International Workshop on
Semantic Evaluation (SemEval 2014), page 636.
Hamdan, H., Bellot, P., and Bechet, F. (2015a). lsislif:
Feature extraction and label weighting for sentiment
analysis in twitter. In In Proceedings of the 9th Inter-
national Workshop on Semantic Evaluation (SemEval
2015).
Hamdan, H., Bellot, P., and Bechet, F. (2015b). Senti-
ment lexicon-based features for sentiment analysis in
short text. In In Proceeding of the 16th International
Conference on Intelligent Text Processing and Compu-
tational Linguistics.
Hu, M. and Liu, B. (2004). Mining and summariz-
ing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177. ACM.
Hu, X., Tang, L., Tang, J., and Liu, H. (2013). Exploiting
social relations for sentiment analysis in microblog-
ging. In Proceedings of the Sixth ACM International
Conference on Web Search and Data Mining, WSDM
’13, pages 537–546. ACM.
Miura, Y., Sakaki, S., Hattori, K., and Ohkuma, T. (2014-
08). TeamX: A sentiment analyzer with enhanced lex-
icon mapping and weighting scheme for unbalanced
data. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
628–632. Association for Computational Linguistics
and Dublin City University.
Mohammad, S. (2012-06-07). #emotional tweets. In
*SEM 2012: The First Joint Conference on Lexical
and Computational Semantics Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
246–255. Association for Computational Linguistics.
Mohammad, S. M., Kiritchenko, S., and Zhu, X. (2013).
NRCCanada: Building the state-of-the-art in senti-
ment analysis of tweets. In In Proceedings of the Inter-
national Workshop on Semantic Evaluation, SemEval
13.
Nakov, P., Rosenthal, S., Kozareva, Z., Stoyanov, V., Rit-
ter, A., and Wilson, T. (2013). SemEval-2013 task
2: Sentiment analysis in twitter. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (SemEval
2013), pages 312–320. Association for Computational
Linguistics.
Rosenthal, S., Nakov, P., Kiritchenko, S., Moham-
mad, S. M., Ritter, A., and Stoyanov, V. (2015-06).
SemEval-2015 task 10: Sentiment analysis in twitter.
In Proceedings of the 9th International Workshop on
Semantic Evaluation, SemEval ’2015. Association for
Computational Linguistics.
Ruppenhofer, J. and Rehbein, I. (2012). Semantic frames
as an anchor representation for sentiment analysis. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
pages 104–109. Association for Computational Lin-
guistics.
Speriosu, M., Sudan, N., Upadhyay, S., and Baldridge, J.
(2011). Twitter polarity classification with label prop-
agation over lexical links and the follower graph. In
Proceedings of the First Workshop on Unsupervised
Learning in NLP, EMNLP ’11, pages 53–63. Associa-
tion for Computational Linguistics.
Tan, C., Lee, L., Tang, J., Jiang, L., Zhou, M., and
Li, P. (2011). User-level sentiment analysis incorpo-
rating social networks. In Proceedings of the 17th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’11, pages
1397–1405. ACM.
Wilson, T., Hoffmann, P., Somasundaran, S., Kessler, J.,
Wiebe, J., Choi, Y., Cardie, C., Riloff, E., and Patward-
han, S. (2005). OpinionFinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on In-
teractive Demonstrations, HLT-Demo ’05, pages 34–
35. Association for Computational Linguistics.
</reference>
<page confidence="0.998924">
573
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.747098">
<title confidence="0.9799965">Lsislif: Feature Extraction and Label Weighting for Sentiment Analysis in Twitter</title>
<author confidence="0.994323">Hussam Hamdan Patrice Bellot Frederic Bechet</author>
<affiliation confidence="0.999238">Aix-Marseille University Aix-Marseille University Aix-Marseille University</affiliation>
<email confidence="0.845235">hussam.hamdan@lsis.orgpatrice.bellot@lsis.orgfrederic.bechet@lif.univ-mrs.fr</email>
<abstract confidence="0.996014190476191">This paper describes our sentiment analysis systems which have been built for SemEval- 2015 Task 10 Subtask B and E. For subtask B, a Logistic Regression classifier has been trained after extracting several groups of features including lexical, syntactic, lexiconbased, Z score and semantic features. A weighting schema has been adapted for positive and negative labels in order to take into account the unbalanced distribution of tweets between the positive and negative classes. This system is ranked third over 40 participants, it achieves average F1 64.27 on Twitter data set 2015 just 0.57% less than the first system. We also present our participation in Subtask E in which our system has got the second rank with Kendall metric but the first one with Spearman for ranking twitter terms according to their association with the positive sentiment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Baccianella</author>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In</title>
<date>2010</date>
<booktitle>in Proc. of LREC.</booktitle>
<contexts>
<context position="4362" citStr="Baccianella et al., 2010" startWordPosition="680" endWordPosition="683">tion 7. used; they reported that social theories such as Sentiment Consistency and Emotional Contagion could be helpful for sentiment analysis. 2 Related Work Three main approaches for sentiment analysis can be identified in Twitter. The lexicon based approach which depends on sentiment lexicons containing positive, negative and neutral words or expressions; the polarity is computed according to the number of common opinionated words between the lexicons and the text. Many dictionaries have been created manually such as MPQA Lexicon (Wilson et al., 2005) or automatically such as SentiWordNet (Baccianella et al., 2010). Machine learning approach adapts different classifiers and features. Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) were adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia co</context>
<context position="7297" citStr="Baccianella et al., 2010" startWordPosition="1152" endWordPosition="1155"> Sentiment labels distribution in the training and development, test data sets. 3.2 Sentiment Lexicons The system exploits two types of sentiment lexicons: manual constructed lexicons and automatic ones. The manual ones are the Bing Lius Opinion Lexicon which is created in (Hu and Liu, 2004) and augmented in many research papers; and MPQA subjectivity lexicons (Wilson et al., 2005). Both lexicons contain English words annotated positive and negative. While the automatic lexicons are NRC Hashtag Sentiment Lexicon (Mohammad, 6 07), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon contain tweet terms with scores, positive score indicates association with positive sentiment, whereas negative score indicates association with negative sentiment. NRC has entries for 54,129 unigrams and 316,531 bigrams; Sentiment140 has entries for 62,468 unigrams, 677,698 bigrams. SentiWordNet is the result of automatically annotating 569 all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. 3.3 Twitter Dictionary We constructed a dictionary for the abbreviations and the slang words used in Twitter </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Baccianella, S., Esuli, A., and Sebastiani, F. (2010). SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In in Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<pages>9--1871</pages>
<contexts>
<context position="12072" citStr="Fan et al., 2008" startWordPosition="1941" endWordPosition="1944"> Our system also extract two types of features, the names: the whole term which represents an argument of the predicate and the tags: the type of each argument in the text (A0 represents the subject of predicate, A1 the object, AM-TMP the time, AMADV the situation, AM-loc the location). These encodings are defined by the tool which we used (Senna). We think that the predicate arguments can constitute a multi-word expression which may be helpful in Sentiment Classification. 5 Experiments 5.1 Experiment Setup We trained the L1-regularized Logistic regression classifier implemented in LIBLINEAR (Fan et al., 2008). The classifier is trained on the training data set using the features of Section 4 with the three polarities (positive, negative, and neutral) as labels. A weighting schema is adapted for each class, we use the weighting option −wi which enables a use of different cost parameter C for different classes. Since the training data is unbalanced, this weighting schema adjusts the probability of each label. Thus, we tuned the classifier in adjusting the cost parameter C of Logistic Regression, weight wpos of positive class and weight Wneg of negative class. We used the development set for tuning t</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Go</author>
<author>R Bhayani</author>
<author>L Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<pages>1--6</pages>
<contexts>
<context position="4536" citStr="Go et al., 2009" startWordPosition="707" endWordPosition="710">r sentiment analysis can be identified in Twitter. The lexicon based approach which depends on sentiment lexicons containing positive, negative and neutral words or expressions; the polarity is computed according to the number of common opinionated words between the lexicons and the text. Many dictionaries have been created manually such as MPQA Lexicon (Wilson et al., 2005) or automatically such as SentiWordNet (Baccianella et al., 2010). Machine learning approach adapts different classifiers and features. Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) were adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which </context>
<context position="16727" citStr="Go et al., 2009" startWordPosition="2723" endWordPosition="2726">res of Spearman’s Rank Correlation, but participating teams will be ranked according to Kendall’s Tau. To rank these terms, we have used six different sentiment lexicons for computing the score for each twitter term. Four of them are described in section 3.2 (manaul lexicons: Bing Liu and MPQA Subjectivity Lexicon , automatic constructed lexicons: NRC Hashtag and Sentiment140 ) and we have built two other automatic construction lexicons: the first named PMi-Sem from the training tweets provided by SemEval-2013 sub-task B Table 1, the second named PMI-sentiment140 from the sentiment140 corpus (Go et al., 2009), we calculated PMI from the labeled tweets for the two corpus using the following equation: PMI(word,positive) = log p(positive).p(word) (2) where p(positive,word): The joint probability of the positive class and the word. p(positive): the probability of positive class. p(word): the probability of the word in whole corpus. 6.1 Score computing If the word exists in a manual constructed lexicon (two lexicons), a score of 1 is assigned if the word is positive else -1 if negative. If the word exists in an automatic constructed lexicon (four lexicons), the lexicon score of the word is used. For ea</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Go, A., Bhayani, R., and Huang, L. (2009). Twitter sentiment classification using distant supervision. pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hamdan</author>
<author>F Bechet</author>
<author>P Bellot</author>
</authors>
<title>Experiments with DBpedia, WordNet and SentiWordNet as resources for sentiment analysis in micro-blogging.</title>
<date>2013</date>
<booktitle>In International Workshop on Semantic Evaluation SemEval-2013 (NAACL Workshop).</booktitle>
<marker>Hamdan, Bechet, Bellot, 2013</marker>
<rawString>Hamdan, H., Bechet, F., and Bellot, P. (2013-04-29). Experiments with DBpedia, WordNet and SentiWordNet as resources for sentiment analysis in micro-blogging. In International Workshop on Semantic Evaluation SemEval-2013 (NAACL Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hamdan</author>
<author>P Bellot</author>
<author>F Bechet</author>
</authors>
<title>The impact of z score on twitter sentiment analysis.</title>
<date>2014</date>
<booktitle>In In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval 2014),</booktitle>
<pages>636</pages>
<contexts>
<context position="9476" citStr="Hamdan et al., 2014" startWordPosition="1503" endWordPosition="1506">Sentiment Lexicons The system extracts four features from the manual constructed lexicons and six features from the automatic ones. For each sentence the number of positive words, the number of negative ones, the number of positive words divided by the number of negative ones and the polarity of the last word are extracted from manual constructed lexicons. In addition to the sum of the positive scores and the sum of the negative scores from the automatic constructed lexicons. 4.5 Z score Z score can distinguish the importance of each term in each class, their performances have been proved in (Hamdan et al., 2014). We assume as in the mentioned work that the term frequencies are following a multi-nomial distribution. Thus, Z score can be seen as a standardization of the term frequency using multi-nomial distribution. We compute the Z score for each term ti in a class Cj (tij) by calculating its term relative frequency tfrij in a particular class Cj, as well as the mean (meani) which is the term probability over the whole corpus multiplied by the number of terms in the class Cj, and standard deviation (sdi) of term ti according to the underlying corpus. Like in (Hamdan et al., 4 29) we tested different </context>
</contexts>
<marker>Hamdan, Bellot, Bechet, 2014</marker>
<rawString>Hamdan, H., Bellot, P., and Bechet, F. (2014). The impact of z score on twitter sentiment analysis. In In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval 2014), page 636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hamdan</author>
<author>P Bellot</author>
<author>F Bechet</author>
</authors>
<title>lsislif: Feature extraction and label weighting for sentiment analysis in twitter. In</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="5324" citStr="Hamdan et al., 2015" startWordPosition="835" endWordPosition="838">noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervise</context>
</contexts>
<marker>Hamdan, Bellot, Bechet, 2015</marker>
<rawString>Hamdan, H., Bellot, P., and Bechet, F. (2015a). lsislif: Feature extraction and label weighting for sentiment analysis in twitter. In In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hamdan</author>
<author>P Bellot</author>
<author>F Bechet</author>
</authors>
<title>Sentiment lexicon-based features for sentiment analysis in short text.</title>
<date>2015</date>
<booktitle>In In Proceeding of the 16th International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="5324" citStr="Hamdan et al., 2015" startWordPosition="835" endWordPosition="838">noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervise</context>
</contexts>
<marker>Hamdan, Bellot, Bechet, 2015</marker>
<rawString>Hamdan, H., Bellot, P., and Bechet, F. (2015b). Sentiment lexicon-based features for sentiment analysis in short text. In In Proceeding of the 16th International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6964" citStr="Hu and Liu, 2004" startWordPosition="1104" endWordPosition="1107">elopment set containing 1654 tweets is used for tuning the machine learner. The test data set 2015 contains about 2390 tweets (Rosenthal et al., 5 06). Table 1 shows the distribution of each label in each data set. Twitter all neg. pos. neut. train 9684 1458 3640 4586 dev 1654 340 739 575 test-2015 2390 365 1038 987 Table1. Sentiment labels distribution in the training and development, test data sets. 3.2 Sentiment Lexicons The system exploits two types of sentiment lexicons: manual constructed lexicons and automatic ones. The manual ones are the Bing Lius Opinion Lexicon which is created in (Hu and Liu, 2004) and augmented in many research papers; and MPQA subjectivity lexicons (Wilson et al., 2005). Both lexicons contain English words annotated positive and negative. While the automatic lexicons are NRC Hashtag Sentiment Lexicon (Mohammad, 6 07), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon contain tweet terms with scores, positive score indicates association with positive sentiment, whereas negative score indicates association with negative sentiment. NRC has entries for 54,129 unigrams and 316,5</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, M. and Liu, B. (2004). Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Hu</author>
<author>L Tang</author>
<author>J Tang</author>
<author>H Liu</author>
</authors>
<title>Exploiting social relations for sentiment analysis in microblogging.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth ACM International Conference on Web Search and Data Mining, WSDM ’13,</booktitle>
<pages>537--546</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5842" citStr="Hu et al., 2013" startWordPosition="917" endWordPosition="920">exicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3 Data and Resources 3.1 Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downloaded these tweets using the given script. We obtained 9646 tweets, the whole training data set is used for training, the provided development set containing 1654 tweets is used for tuning the machine learner. The test data set 2</context>
</contexts>
<marker>Hu, Tang, Tang, Liu, 2013</marker>
<rawString>Hu, X., Tang, L., Tang, J., and Liu, H. (2013). Exploiting social relations for sentiment analysis in microblogging. In Proceedings of the Sixth ACM International Conference on Web Search and Data Mining, WSDM ’13, pages 537–546. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miura</author>
<author>S Sakaki</author>
<author>K Hattori</author>
<author>T Ohkuma</author>
</authors>
<title>TeamX: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>628--632</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<marker>Miura, Sakaki, Hattori, Ohkuma, 2014</marker>
<rawString>Miura, Y., Sakaki, S., Hattori, K., and Ohkuma, T. (2014-08). TeamX: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628–632. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
</authors>
<title>emotional tweets.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>246--255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Mohammad, 2012</marker>
<rawString>Mohammad, S. (2012-06-07). #emotional tweets. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 246–255. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Mohammad</author>
<author>S Kiritchenko</author>
<author>X Zhu</author>
</authors>
<title>NRCCanada: Building the state-of-the-art in sentiment analysis of tweets. In</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval 13.</booktitle>
<contexts>
<context position="5129" citStr="Mohammad et al., 2013" startWordPosition="803" endWordPosition="806"> adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classificatio</context>
<context position="7252" citStr="Mohammad et al., 2013" startWordPosition="1146" endWordPosition="1149">39 575 test-2015 2390 365 1038 987 Table1. Sentiment labels distribution in the training and development, test data sets. 3.2 Sentiment Lexicons The system exploits two types of sentiment lexicons: manual constructed lexicons and automatic ones. The manual ones are the Bing Lius Opinion Lexicon which is created in (Hu and Liu, 2004) and augmented in many research papers; and MPQA subjectivity lexicons (Wilson et al., 2005). Both lexicons contain English words annotated positive and negative. While the automatic lexicons are NRC Hashtag Sentiment Lexicon (Mohammad, 6 07), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon contain tweet terms with scores, positive score indicates association with positive sentiment, whereas negative score indicates association with negative sentiment. NRC has entries for 54,129 unigrams and 316,531 bigrams; Sentiment140 has entries for 62,468 unigrams, 677,698 bigrams. SentiWordNet is the result of automatically annotating 569 all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. 3.3 Twitter Dictionary We constructed a dictionary for the abbre</context>
<context position="14511" citStr="Mohammad et al., 2013" startWordPosition="2361" endWordPosition="2364">n of features makes some of them not influential are not sufficient to consider the features not useful. Thus, we repeat the same classification process but add one feature group at a time (Tabel 3). Z score seems to give gain of 1.91%, LDA topics gain is 0.66%, semantic role labeling 0.64%, brown clusters 3.38% and sentiment lexicons 6.58%. The most influential features is also the sentiment lexicon features. Brown cluster features obtains an interesting gain of 3.38%. From the previous two analysis, we find that sentiment lexicon features are the most influential ones as concluded by (S. M. Mohammad et al., 2013). Some features have improved the performance in test set 2015 but not in the other test sets such as Z score, Semantic Role Labeling. Run Test-2015 Test-2014 Test-2013 All features 64.27 71.54 71.34 all-zscore 63.82 73.05 69.99 all-lexicons 60.96 67.6 66.63 all-ngram 63.99 69.06 69.67 all-srl 65.1 71.81 70.41 all-topics 63.47 71.49 71 all-brown 63.82 70.74 69.9 Table2. The F1 score for each run, All features run exploits all features while the others remove a feature group at each run Zscore, lexicons, n-gram, srl, topics and brown cluster, respectively. Run Test-2015 Test-2014 Test-2013 bl 5</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Mohammad, S. M., Kiritchenko, S., and Zhu, X. (2013). NRCCanada: Building the state-of-the-art in sentiment analysis of tweets. In In Proceedings of the International Workshop on Semantic Evaluation, SemEval 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>S Rosenthal</author>
<author>Z Kozareva</author>
<author>V Stoyanov</author>
<author>A Ritter</author>
<author>T Wilson</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6105" citStr="Nakov et al., 2013" startWordPosition="962" endWordPosition="965">hey wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3 Data and Resources 3.1 Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downloaded these tweets using the given script. We obtained 9646 tweets, the whole training data set is used for training, the provided development set containing 1654 tweets is used for tuning the machine learner. The test data set 2015 contains about 2390 tweets (Rosenthal et al., 5 06). Table 1 shows the distribution of each label in each data set. Twitter all neg. pos. neut. train 9684 1458 3640 4586 dev 1654 340 739 575 test-2015 2390 365 1038 987 Table1. Sentiment labels distribution in</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Nakov, P., Rosenthal, S., Kozareva, Z., Stoyanov, V., Ritter, A., and Wilson, T. (2013). SemEval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rosenthal</author>
<author>P Nakov</author>
<author>S Kiritchenko</author>
<author>S M Mohammad</author>
<author>A Ritter</author>
<author>V Stoyanov</author>
</authors>
<title>SemEval-2015 task 10: Sentiment analysis in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015. Association for Computational Linguistics.</booktitle>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Rosenthal, S., Nakov, P., Kiritchenko, S., Mohammad, S. M., Ritter, A., and Stoyanov, V. (2015-06). SemEval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>I Rehbein</author>
</authors>
<title>Semantic frames as an anchor representation for sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<pages>104--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11397" citStr="Ruppenhofer and Rehbein, 2012" startWordPosition="1832" endWordPosition="1835">to feature space where each feature represents the number of words in the text mapped to each cluster. The 1000 clusters is provided in Twitter Word Clusters of CMU ARK group. 1000 clusters were constructed from approximately 56 million tweets. 4.6.2 Topic features Latent dirichlet association or topic modeling is used to extract 10 features. Lda-c is configured with 10 topics and the training data is used for training the model, then for each sentence in the test set, the trained model estimates the number of words assigned to each topic. 570 4.6.3 Semantic Role Labeling Features Authors in (Ruppenhofer and Rehbein, 2012) encode semantic role labeling features in SVM classifier. Our system also extract two types of features, the names: the whole term which represents an argument of the predicate and the tags: the type of each argument in the text (A0 represents the subject of predicate, A1 the object, AM-TMP the time, AMADV the situation, AM-loc the location). These encodings are defined by the tool which we used (Senna). We think that the predicate arguments can constitute a multi-word expression which may be helpful in Sentiment Classification. 5 Experiments 5.1 Experiment Setup We trained the L1-regularized</context>
</contexts>
<marker>Ruppenhofer, Rehbein, 2012</marker>
<rawString>Ruppenhofer, J. and Rehbein, I. (2012). Semantic frames as an anchor representation for sentiment analysis. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, pages 104–109. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Speriosu</author>
<author>N Sudan</author>
<author>S Upadhyay</author>
<author>J Baldridge</author>
</authors>
<title>Twitter polarity classification with label propagation over lexical links and the follower graph.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11,</booktitle>
<pages>53--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5615" citStr="Speriosu et al., 2011" startWordPosition="883" endWordPosition="886">es classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3 Data and Resources 3.1 Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downloa</context>
</contexts>
<marker>Speriosu, Sudan, Upadhyay, Baldridge, 2011</marker>
<rawString>Speriosu, M., Sudan, N., Upadhyay, S., and Baldridge, J. (2011). Twitter polarity classification with label propagation over lexical links and the follower graph. In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11, pages 53–63. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tan</author>
<author>L Lee</author>
<author>J Tang</author>
<author>L Jiang</author>
<author>M Zhou</author>
<author>P Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11,</booktitle>
<pages>1397--1405</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5753" citStr="Tan et al., 2011" startWordPosition="902" endWordPosition="905">seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3 Data and Resources 3.1 Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downloaded these tweets using the given script. We obtained 9646 tweets, the whole training data set is used for training, the provided developme</context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Tan, C., Lee, L., Tang, J., Jiang, L., Zhou, M., and Li, P. (2011). User-level sentiment analysis incorporating social networks. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, pages 1397–1405. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>P Hoffmann</author>
<author>S Somasundaran</author>
<author>J Kessler</author>
<author>J Wiebe</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>E Riloff</author>
<author>S Patwardhan</author>
</authors>
<title>OpinionFinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations, HLT-Demo ’05,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4297" citStr="Wilson et al., 2005" startWordPosition="671" endWordPosition="674">s described in section 6 and future work is presented in section 7. used; they reported that social theories such as Sentiment Consistency and Emotional Contagion could be helpful for sentiment analysis. 2 Related Work Three main approaches for sentiment analysis can be identified in Twitter. The lexicon based approach which depends on sentiment lexicons containing positive, negative and neutral words or expressions; the polarity is computed according to the number of common opinionated words between the lexicons and the text. Many dictionaries have been created manually such as MPQA Lexicon (Wilson et al., 2005) or automatically such as SentiWordNet (Baccianella et al., 2010). Machine learning approach adapts different classifiers and features. Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) were adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia a</context>
<context position="7056" citStr="Wilson et al., 2005" startWordPosition="1118" endWordPosition="1121"> set 2015 contains about 2390 tweets (Rosenthal et al., 5 06). Table 1 shows the distribution of each label in each data set. Twitter all neg. pos. neut. train 9684 1458 3640 4586 dev 1654 340 739 575 test-2015 2390 365 1038 987 Table1. Sentiment labels distribution in the training and development, test data sets. 3.2 Sentiment Lexicons The system exploits two types of sentiment lexicons: manual constructed lexicons and automatic ones. The manual ones are the Bing Lius Opinion Lexicon which is created in (Hu and Liu, 2004) and augmented in many research papers; and MPQA subjectivity lexicons (Wilson et al., 2005). Both lexicons contain English words annotated positive and negative. While the automatic lexicons are NRC Hashtag Sentiment Lexicon (Mohammad, 6 07), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon contain tweet terms with scores, positive score indicates association with positive sentiment, whereas negative score indicates association with negative sentiment. NRC has entries for 54,129 unigrams and 316,531 bigrams; Sentiment140 has entries for 62,468 unigrams, 677,698 bigrams. SentiWordNet is t</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Wilson, T., Hoffmann, P., Somasundaran, S., Kessler, J., Wiebe, J., Choi, Y., Cardie, C., Riloff, E., and Patwardhan, S. (2005). OpinionFinder: A system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, HLT-Demo ’05, pages 34– 35. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>