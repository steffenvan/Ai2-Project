<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000163">
<title confidence="0.999363">
An Empirical Study of Vietnamese Noun Phrase Chunking with
Discriminative Sequence Models
</title>
<author confidence="0.991049">
Le Minh Nguyen Huong Thao Nguyen and Phuong Thai Nguyen
</author>
<affiliation confidence="0.99407">
School of Information Science, JAIST College of Technology, VNU
</affiliation>
<email confidence="0.974488">
nguyenml@jaist.ac.jp {thaonth, thainp}@vnu.edu.vn
</email>
<author confidence="0.724991">
Tu Bao Ho and Akira Shimazu
</author>
<affiliation confidence="0.896379">
Japan Advanced Institute of Science and Technology
</affiliation>
<email confidence="0.997535">
{bao,shimazu}@jaist.ac.jp
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887538461538">
This paper presents an empirical work
for Vietnamese NP chunking task. We
show how to build an annotation corpus of
NP chunking and how discriminative se-
quence models are trained using the cor-
pus. Experiment results using 5 fold cross
validation test show that discriminative se-
quence learning are well suitable for Viet-
namese chunking. In addition, by em-
pirical experiments we show that the part
of speech information contribute signifi-
cantly to the performance of there learning
models.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999665979166667">
Many Natural Language Processing applications
(i.e machine translation) require syntactic infor-
mation and tools for syntactic analysis. However,
these linguistic resources are only available for
some languages(i.e English, Japanese, Chines). In
the case of Vietnamese, currently most researchers
have focused on word segmentation and part of
speech tagging. For example, Nghiem et al
(Nghiem, Dinh, Nguyen, 2008) has developed a
Vietnamese POS tagging. Tu (Tu, Phan, Nguyen,
Ha, 2006) (Nguyen, Romary, Rossignol, Vu,
2006)(Dien, Thuy, 2006) have developed Viet-
namese word segmentation.
The processing of building tools and annotated
data for other fundamental tasks such as chunk-
ing and syntactic parsing are currently developed.
This can be viewed as a bottleneck for develop-
ing NLP applications that require a deeper under-
standing of the language. The requirement of de-
veloping such tools motives us to develop a Viet-
namese chunking tool. For this goal, we have
been looking for an annotation corpus for conduct-
ing a Vietnamese chunking using machine learn-
ing methods. Unfortunately, at the moment, there
is still no common standard annotated corpus for
evaluation and comparison regarding Vietnamese
chunking.
In this paper, we aim at discussing on how
we can build annotated data for Vietnamese text
chunking and how to apply discriminative se-
quence learning for Vietnamese text chunking. We
choose discriminative sequence models for Viet-
namese text chunking because they have shown
very suitable methods for several languages(i.e
English, Japanese, Chinese) (Sha and Pereira,
2005)(Chen, Zhang, and Ishihara, 2006) (Kudo
and Matsumoto, 2001). These presentative dis-
criminative models which we choose for conduct-
ing empirical experiments including: Conditional
Random Fields (Lafferty, McCallum, and Pereira,
2001), Support Vector Machine (Vapnik, 1995)
and Online Prediction (Crammer et al, 2006). In
other words, because Noun Phrase chunks appear
most frequently in sentences. So, in this paper
we focus mainly on empirical experiments for the
tasks of Vietnamese NP chunking.
We plan to answer several major questions by
using empirical experiments as follows.
</bodyText>
<listItem confidence="0.9927333">
• Whether or not the discriminative learning
models are suitable for Vietnamese chunking
problem?
• We want to know the difference of SVM,
Online Learning, and Conditional Random
Fields for Vietnamese chunking task.
• Which features are suitable for discriminative
learning models and how they contribute to
the performance of Vietnamese text chunk-
ing?
</listItem>
<bodyText confidence="0.9918785">
The rest of this paper is organized as follows:
Section 2 describes Vietnamese text chunking with
discriminative sequence learning models. Section
3 shows experimental results and Section 4 dis-
</bodyText>
<page confidence="0.970043">
9
</page>
<note confidence="0.940101">
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 9–16,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9860055">
cusses the advantage of our method and describes
future work.
</bodyText>
<sectionHeader confidence="0.902085" genericHeader="method">
2 Vietnamese NP Chunking with
Discriminative Sequence Learning
</sectionHeader>
<bodyText confidence="0.9998505">
Noun Phrase chunking is considered as the task
of grouping a consecutive sequence of words into
a NP chunk lablel. For example: “[NP Anh Ay
(He)] [VP thich(likes)] [NP mot chiec oto(a car)]
Before describing NP chunking tasks, we
summarize the characteristic of Vietnamese lan-
guage and the background of Conditional Ran-
dom Fields, Support Vector Machine, and Online
Learning. Then, we present how to build the an-
notated corpus for the NP chunking task.
</bodyText>
<subsectionHeader confidence="0.988853">
2.1 The characteristic of Vietnamese Words
</subsectionHeader>
<bodyText confidence="0.99996496875">
Vietnamese syllables are elementary units that
have one way of pronunciation. In documents,
they are usually delimited by white-space. Be-
ing the elementary units, Vietnamese syllables are
not undivided elements but a structure. Generally,
each Vietnamese syllable has all five parts: first
consonant, secondary vowel, main vowel, last con-
sonant and a tone mark. For instance, the sylla-
ble tu.n (week) has a tone mark (grave accent), a
first consonant (t), a secondary vowel (u), a main
vowel () and a last consonant (n). However, except
for main vowel that is required for all syllables,
the other parts may be not present in some cases.
For example, the syllable anh (brother) has no tone
mark, no secondary vowel and no first consonant.
In other case, the syllable hoa (flower) has a sec-
ondary vowel (o) but no last consonant.
Words in Vietnamese are made of one or more
syllables which are combined in different ways.
Based on the way of constructing words from syl-
lables, we can classify them into three categories:
single words, complex words and reduplicative
words (Mai,Vu, Hoang, 1997).
The past of speechs (Pos) of each word in Viet-
namese are mainly sketched as follows.
A Noun Phrase (NP) in Vietnamese consists of
three main parts as follows: the noun center, the
prefix part, and the post fix part. The prefix and
postfix are used to support the meaning of the NP.
For example in the NP ”ba sinh vien nay”, the
noun center is ”sinh vien”, and the prefix is ”ba
(three)”, the postfix is ”nay”.
</bodyText>
<table confidence="0.999384">
Vietnamese Tag Equivalent to English Tag
CC Coordinating conjunction)
CD Cardinal number)
DT Determiner)
V Verb
P Preposition
A Adjective
LS List item marker
MD Modal
N Noun
</table>
<tableCaption confidence="0.99988">
Table 1: Part of Speeches in Vietnamese
</tableCaption>
<subsectionHeader confidence="0.997289">
2.2 The Corpus
</subsectionHeader>
<bodyText confidence="0.999590214285714">
We have collected more than 9,000 sentences from
several web-sites through the internet. After that,
we then applied the segmentation tool (Tu, Phan,
Nguyen, Ha, 2006) to segment each sentences
into a sequence of tokens. Each sequence of
tokens are then represented using the format of
CONLL 2000. The details are sketched as follows.
Each line in the annotated data consists of
three columns: the token (a word or a punc-
tuation mark), the part-of-speech tag of the to-
ken, and the phrase type label (label for short)
of the token. The label of each token indicates
whether the token is outside a phrase (O), starts
a phrase (B-(PhraseType)), or continues a phrase
(I-(PhraseType)).
In order to save time for building annotated
data, we made a set of simple rules for automat-
ically generating the chunking data as follows. If
a word is not a ”noun”, ”adjective”, or ”article” it
should be assigned the label ”O”. The consecu-
tive words are NP if they is one of type as follows:
”noun noun”; ”article noun”, ”article noun adjec-
tive”. After generating such as data, we ask an
expert about Vietnamese linguistic to correct the
data. Finally, we got more than 9,000 sentences
which are annotated with NP chunking labels.
Figure 1 shows an example of the Vietnamese
chunking corpus.
</bodyText>
<subsectionHeader confidence="0.997996">
2.3 Discriminative Sequence Learning
</subsectionHeader>
<bodyText confidence="0.999954333333333">
In this section, we briefly introduce three dis-
criminative sequence learning models for chunk-
ing problems.
</bodyText>
<subsectionHeader confidence="0.876997">
2.3.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.966875666666667">
Conditional Random Fields (CRFs) (Lafferty,
McCallum, and Pereira, 2001) are undirected
graphical models used to calculate the conditional
</bodyText>
<page confidence="0.991252">
10
</page>
<figureCaption confidence="0.966741">
Figure 1: An Example of the Vietnamese chunk-
ing corpus
</figureCaption>
<bodyText confidence="0.999932666666667">
probability of values on designated output nodes,
given values assigned to other designated input
nodes for data sequences. CRFs make a first-order
Markov independence assumption among output
nodes, and thus correspond to finite state machine
(FSMs).
Let o = (o1, o2, ... , oT) be some observed in-
put data sequence, such as a sequence of words in
a text (values on T input nodes of the graphical
model). Let S be a finite set of FSM states, each is
associated with a label l such as a clause start po-
sition. Let s = (s1, s2, ... , sT) be some sequences
of states (values on T output nodes). CRFs de-
fine the conditional probability of a state sequence
given an input sequence to be
</bodyText>
<equation confidence="0.99908075">
Ã T
PA(s|o) = ZoexpXF(s,o, t) (1)
t=1
3PT ´
</equation>
<bodyText confidence="0.99848228">
where Zo = Ps exp t=1 F(s, o, t) is a nor-
malization factor over all state sequences. We de-
note δ to be the Kronecker-δ. Let F(s, o, t) be the
sum of CRFs features at time position t:
current state st, such as the previous label l, =
AV (adverb) and the current label l = JJ (adjec-
tive). gi(o, st, t) = δ(st, l)xk(o, t) is a per-state
feature function which combines the label l of cur-
rent state st and a context predicate, i.e., the binary
function xk(o, t) that captures a particular prop-
erty of the observation sequence o at time position
t. For instance, the current label is JJ and the cur-
rent word is “conditional“.
Training CRFs is commonly performed by max-
imizing the likelihood function with respect to
the training data using advanced convex optimiza-
tion techniques like L-BFGS. Recently, there are
several works apply Stochastic Gradient Descent
(SGD) for training CRFs models. SGD has been
historically associated with back-propagation al-
gorithms in multilayer neural networks.
And inference in CRFs, i.e., searching the most
likely output label sequence of an input observa-
tion sequence, can be done using Viterbi algo-
rithm.
</bodyText>
<subsectionHeader confidence="0.945808">
2.3.2 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999919642857143">
Support vector machine (SVM)(Vapnik, 1995)
is a technique of machine learning based on sta-
tistical learning theory. The main idea behind
this method can be summarized as follows. Sup-
pose that we are given l training examples (xi, yi),
(1 &lt; i &lt; l), where xi is a feature vector in n di-
mensional feature space, and yi is the class label
1-1, +11 of xi.
SVM finds a hyperplane w.x+b = 0 which cor-
rectly separates training examples and has maxi-
mum margin which is the distance between two
hyperplanes w · x + b &gt; 1 and w · x + b &lt; −1.
Finally, the optimal hyperplane is formulated as
follows:
</bodyText>
<equation confidence="0.996598333333333">
Ã Xl !
f(x) = sign αiyiK(xi, x) + b (3)
1
</equation>
<bodyText confidence="0.9993982">
where αi is the Lagrange multiple, and K(x&apos;, x&apos;&apos;)
is called a kernel function, which calculates sim-
ilarity between two arguments x&apos; and x&apos;&apos;. For in-
stance, the Polynomial kernel function is formu-
lated as follows:
</bodyText>
<equation confidence="0.570803">
X X Aj9j(o, st, t) (2) K(x&apos;, x&apos;&apos;) = (x&apos; · x&apos;&apos;)T&apos; (4)
Adfd(st−1, st, t) +
d j
</equation>
<bodyText confidence="0.998742125">
where fi(st−1, st, t) = δ(st−1, l,)δ(st, l) is a
transition feature function which represents se-
quential dependencies by combining the label l,
of the previous state st−1 and the label l of the
SVMs estimate the label of an unknown example
x whether the sign of f(x) is positive or not.
Basically, SVMs are binary classifier, thus we
must extend SVMs to multi-class classifier in or-
</bodyText>
<page confidence="0.996234">
11
</page>
<bodyText confidence="0.9999102">
der to classify three or more classes. The pair-
wise classifier is one of the most popular meth-
ods to extend the binary classification task to that
of K classes. Though, we leave the details to
(Kudo and Matsumoto, 2001), the idea of pairwise
classification is to build K.(K-1)/2 classifiers con-
sidering all pairs of classes, and final decision is
given by their weighted voting. The implementa-
tion of Vietnamese text chunking is based on Yam-
cha (V0.33)1.
</bodyText>
<subsectionHeader confidence="0.533916">
2.3.3 Online Passive-Aggressive Learning
</subsectionHeader>
<bodyText confidence="0.994680891891892">
Online Passive-Aggressive Learning (PA) was
proposed by Crammer (Crammer et al, 2006) as
an alternative learning algorithm to the maximize
margin algorithm. The Perceptron style for nat-
ural language processing problems as initially pro-
posed by (Collins, 2002) can provide to state of
the art results on various domains including text
segmentation, syntactic parsing, and dependency
parsing. The main drawback of the Perceptron
style algorithm is that it does not have a mech-
anism for attaining the maximize margin of the
training data. It may be difficult to obtain high
accuracy in dealing with hard learning data. The
online algorithm for chunking parsing in which
we can attain the maximize margin of the training
data without using an optimization technique. It
is thus much faster and easier to implement. The
details of PA algorithm for chunking parsing are
presented as follows.
Assume that we are given a set of sentences
xi and their chunks yi where i = 1, ..., n. Let
the feature mapping between a sentence x and
a sequence of chunk labels y be: 4b(x, y) =
4b1(x, y), 4b2(x, y), ..., 4bd(x, y) where each fea-
ture mapping 4b1 maps (x, y) to a real value. We
assume that each feature 4b(x, y) is associated with
a weight value. The goal of PA learning for chunk-
ing parsing is to obtain a parameter w that min-
imizes the hinge-loss function and the margin of
learning data.
Algorithm 1 shows briefly the Online Learning
for chunking problem. The detail about this al-
gorithm can be referred to the work of (Crammer
et al, 2006). In Line 7, the argmax value is com-
puted by using the Viterbi algorithm which is sim-
ilar to the one described in (Collins, 2002). Algo-
rithm 1 is terminated after T round.
</bodyText>
<footnote confidence="0.9890915">
1Yamcha is available at
http://chasen.org/ taku/software/yamcha/
</footnote>
<listItem confidence="0.681612">
1 Input: S = (xi7 yi), i = 1, 2,..., n in which
xi is the sentence and yi is a sequence of
chunks
2 Aggressive parameter C
3 Output: the model
</listItem>
<figure confidence="0.251969416666667">
4 Initialize: w1 = (0, 0, ..., 0)
5 for t=1, 2... do
6 Receive an sentence xt
7 Predict y∗t = arg maxy∈Y (wt.4b(xt, yt))
Suffer loss: lt =
wt.4b(xt, y∗t ) − wt.4b(xt, yt) + �/ρ(yt, y∗t )
lt
Set:τt =
8 ||D(xt,yt )−D(xt,yt)||2
9 Update:
wt+1 = wt + τt(4b(xt, yt) − 4b(xt, y∗t ))
10 end
</figure>
<figureCaption confidence="0.563344">
Algorithm 1: The Passive-Aggressive algo-
rithm for NP chunking.
</figureCaption>
<subsectionHeader confidence="0.551686">
2.3.4 Feature Set
</subsectionHeader>
<bodyText confidence="0.999766517241379">
Feature set is designed through features template
which is shown in Table 2. All edge features obey
the first-order Markov dependency that the label
(l) of the current state depends on the label (l0)
of the previous state (e.g., “l = I-NP” and “l0 =
B-NP”). Each observation feature expresses how
much influence a statistic (x(o, i)) observed sur-
rounding the current position i has on the label
(l) of the current state. A statistic captures a par-
ticular property of the observation sequence. For
instance, the observation feature “l = I-NP” and
“word−1 is the” indicates that the label of the cur-
rent state should be I-NP (i.e., continue a noun
phrase) if the previous word is the. Table 2 de-
scribes both edge and observation feature tem-
plates. Statistics for observation features are iden-
tities of words, POS tags surrounding the current
position, such as words and POS tags at −2, −1,
1, 2.
We also employ 2-order conjunctions of the cur-
rent word with the previous (w−1w0) or the next
word (w0w1), and 2-order and 3-order conjunc-
tions of two or three consecutive POS tags within
the current window to make use of the mutual de-
pendencies among singleton properties. With the
feature templates shown in Table 2 and the feature
rare threshold of 1 (i.e., only features with occur-
rence frequency larger than 1 are included into the
discriminative models)
</bodyText>
<page confidence="0.997106">
12
</page>
<table confidence="0.991644222222222">
Edge feature templates
Current state: sz Previous state: sz−1
l l0
Observation feature templates
Current state: sz Statistic (or context predicate) templates: x(o, i)
l w−2; w−1; w0; w1; w2; w−1w0; w0w1;
t−2; t−1; t0; t1; t2;
t−2t−1; t−1t0; t0t1; t1t2; t−2t−1t0;
t−1t0t1; t0t1t2
</table>
<tableCaption confidence="0.994593">
Table 2: Feature templates for phrase chunking
</tableCaption>
<sectionHeader confidence="0.986653" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999985142857143">
We evaluate the performance of using several se-
quence learning models for the Vietnamese NP
chunking problem. The data of more than 9,000
sentences is evaluated using an empirical experi-
ment with 5 fold cross validation test. It means
we used 1,800 and 7,200 sentences for testing
and training the discriminative sequence learning
models, respectively. Note that the evaluation
method is used the same as CONLL2000 did. We
used Precision, Recall, and F-Measure in which
Precision measures how many chunks found by
the algorithm are correct and the recall is per-
centage of chunks defined in the corpus that were
found by the chunking program.
</bodyText>
<equation confidence="0.974913857142857">
Precision = #correct−chunk
#numberofchunks
#correct−chunks
Recall =
#numerofchunksinthecorpus
2 � Precision � Recall
F − measure =Precision + Recall
</equation>
<bodyText confidence="0.976089690476191">
To compute the scores in our experiments, we
utilized the evaluation tool (conlleval.pl) which is
available in CONLL 2000 (Sang and Buchholz,
2000, ).
Figure 2 shows the precision scores of three
methods using 5 Folds cross validation test. It
reports that the CRF-LBFGS attain the highest
score. The SVMs and CRF-SGD are comparable
to CRF-LBFGS. The Online Learning achieved
the lowest score.
Figure 3 shows the recall scores of three CRFs-
LBFGS, CRFs-SGD, SVM, and Online Learning.
The results show that CRFs-SGD achieved the
highest score while the Online Learning obtained
the lowest score in comparison with others.
Figure 4 and Figure 5 show the F-measure and
accuracy scores using 5 Folds Cross-validation
Figure 2: Precision results in 5 Fold cross valida-
tion test
Test. Similar to these results of Precision and Re-
call, CRFs-LBFGS was superior to the other ones
while the Online Learning method obtained the
lowest result.
Table 3 shows the comparison of three discrim-
inative learning methods for Vietnamese Noun
Phrase chunking. We compared the three se-
quence learning methods including: CRFs using
the LBFGS method, CRFs with SGD, and On-
line Learning. Experiment results show that the
CRFs-LBFGS is the best in comparison with oth-
ers. However, the computational times when train-
ing the data is slower than either SGD or Online
Learning. The SGD is faster than CRF-LBFS ap-
proximately 6 times. The SVM model obtained a
comparable results with CRFs models and it was
superior to Online Learning. It yields results that
were 0.712% than Online Learning. However, the
SVM’s training process take slower than CRFs
and Online Learning. According to our empirical
investigation, it takes approximately slower than
CRF-SGF, CRF-LBFGS as well as Online Learn-
ing.
</bodyText>
<page confidence="0.999126">
13
</page>
<figureCaption confidence="0.99444825">
Figure 3: Recall result in 5 Fold cross validation
test
Figure 4: The F-measure results of 5 Folds Cross-
validation Test
</figureCaption>
<bodyText confidence="0.9951348">
Note that we used FlexCRFs (Phan, Nguyen,
Tu , 2005) for Conditional Random Fields us-
ing LBFGS, and for Stochastic Gradient Descent
(SGD) we used SGD1.3 which is developed by
Leon Bottou 2.
</bodyText>
<table confidence="0.999473">
Methods Precision Recall F1
CRF-LBGS 80.85 81.034 80.86
CRF-SGD 80.74 80.66 80.58
Online-PA 80.034 80.13 79.89
SVM 80.412 80.982 80.638
</table>
<tableCaption confidence="0.995297">
Table 3: Vietnamese Noun Phrase chunking per-
</tableCaption>
<bodyText confidence="0.862758166666667">
formance using Discriminative Sequence Learn-
ing (CRFs, SVM, Online-PA)
In order to investigate which features are ma-
jor effect on the discriminative learning models for
Vietnamese Chunking problems, we conduct three
experiments as follows.
</bodyText>
<footnote confidence="0.942523">
2http://leon.bottou.org/projects/sgd
</footnote>
<figureCaption confidence="0.973373">
Figure 5: The accuracy scores of four methods
with 5 Folds Cross-validation Test
</figureCaption>
<listItem confidence="0.997569875">
• Cross validation test for three modes without
considering the edge features
• Cross validation test for three models without
using POS features
• Cross validation test for three models without
using lexical features
• Cross validation test for three models without
using ”edge features template” features
</listItem>
<bodyText confidence="0.999185">
Note that the computational time of training
SVMs model is slow, so we skip considering fea-
ture selection for SVMs. We only consider feature
selection for CRFs and Online Learning.
</bodyText>
<table confidence="0.8442292">
Feature Set LBFGS SGD Online
Full-Features 80.86 80.58 79.89
Without-Edge 80.91 78.66 80.13
Without-Pos 62.264 62.626 59.572
Without-Lex 77.204 77.712 75.576
</table>
<tableCaption confidence="0.75042725">
Table 4: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, Online-PA)
Table 4 shows that the Edge features have an
</tableCaption>
<bodyText confidence="0.998725">
impact to the CRF-SGD model while it do not
affect to the performance of CRFs-LBFGS and
Online-PA learning. Table 4 also indicates that
the POS features are severed as important features
regarding to the performance of all discrimina-
tive sequence learning models. As we can see,
if one do not use POS features the F1-score of
each model is decreased more than 20%. We also
remark that the lexical features contribute an im-
portant role to the performance of Vietnamese text
</bodyText>
<page confidence="0.998002">
14
</page>
<figureCaption confidence="0.999706">
Figure 6: F-measures of three methods with different feature set
</figureCaption>
<bodyText confidence="0.999797571428571">
chunking. If we do not use lexical features the
F1-score of each model is decreased till approxi-
mately 3%. In conclusion, the POS features signif-
icantly effect on the performance of the discrimi-
native sequence models. This is similar to the note
of (Chen, Zhang, and Ishihara, 2006).
Figure 6 reports the F-Measures of using dif-
ferent feature set for each discriminative models.
Note that WPos, WLex, and WEdge mean without
using Pos features, without using lexical features,
and without using edge features, respectively. As
we can see, the CRF-LBFGs always achieved the
best scores in comparison with the other ones and
the Online Learning achieved the lowest scores.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999972565217391">
In this paper, we report an investigation of devel-
oping a Vietnamese Chunking tool. We have con-
structed an annotation corpus of more than 9,000
sentences and exploiting discriminative learning
models for the NP chunking task. Experimen-
tal results using 5 Folds cross-validation test have
showed that the discriminative models are well
suitable for Vietnamese phrase chunking. Con-
ditional random fields show a better performance
in comparison with other methods. The part of
speech features are known as the most influence
features regarding to the performances of discrim-
inative models on Vietnamese phrases chunking.
What our contribution is expected to be useful
for the development of Vietnamese Natural Lan-
guage Processing. Our results and corpus can be
severed as a very good baseline for Natural Lan-
guage Processing community to develop the Viet-
namese chunking task.
There are still room for improving the perfor-
mance of Vietnamese chunking models. For ex-
ample, more attention on features selection is nec-
essary. We would like to solve this in future work.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999155285714286">
The constructive comments and helpful sugges-
tions from three anonymous reviewers are greatly
appreciated. This paper is supported by JAIST
Grant for Research Associates and a part from a
national project named Building Basic Resources
and Tools for Vietnamese Language and Speech
Processing, KC01.01/06-10.
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997996555555555">
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002.
K. Crammer et al. 2006. Online Passive-Aggressive
Algorithm. Journal of Machine Learning Research,
2006
W. Chen, Y. Zhang, and H. Ishihara 2006. An em-
pirical study of Chinese chunking. In Proceedings
COLING/ACL 2006
Dinh Dien, Vu Thuy 2006. A maximum entropy
approach for vietnamese word segmentation. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2006: 248-253
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In the proceed-
</reference>
<page confidence="0.959831">
15
</page>
<reference confidence="0.999697138888889">
ings of International Conference on Machine Learn-
ing (ICML), pp.282-289, 2001
N.C. Mai, D.N. Vu, T.P. Hoang. 1997. Foundations
of linguistics and Vietnamese. Education Publisher
(1997) 142. 152
Thi Minh Huyen Nguyen, Laurent Romary, Mathias
Rossignol, Xuan Luong Vu. 2006. A lexicon
for Vietnamese language processing. Language Re-
seourse Evaluation (2006) 40:291-309.
Minh Nghiem, Dien Dinh, Mai Nguyen. 2008. Im-
proving Vietnamese POS tagging by integrating a
rich feature set and Support Vector Machines. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2008: 128–133.
X.H. Phan, M.L. Nguyen, C.T. Nguyen. Flex-
CRFs: Flexible Conditional Random Field Toolkit.
http://flexcrfs.sourceforge.net, 2005
T. Kudo and Y. Matsumoto. 2001. Chunking with
Support Vector Machines. The Second Meeting of
the North American Chapter of the Association for
Computational Linguistics (2001)
F. Sha and F. Pereira. 2005. Shallow Parsing with
Conditional Random Fields. Proceedings of HLT-
NAACL 2003 213-220 (2003)
C.T. Nguyen, T.K. Nguyen, X.H. Phan, L.M. Viet-
namese Word Segmentation with CRFs and SVMs:
An Investigation. 2006. The 20th Pacific Asia Con-
ference on Language, Information, and Computation
(PACLIC), 1-3 November, 2006, Wuhan, China
Tjong Kim Sang and Sabine Buchholz. 2000. Intro-
duction to the CoNLL-2000 Shared Task: Chunk-
ing. Proceedings of CoNLL-2000 , Lisbon, Portugal,
2000.
V. Vapnik. 1995. The Natural of Statistical Learning
Theory. New York: Springer-Verlag, 1995.
</reference>
<page confidence="0.9987">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388830">
<title confidence="0.9988375">An Empirical Study of Vietnamese Noun Phrase Chunking with Discriminative Sequence Models</title>
<author confidence="0.999817">Minh Nguyen Huong Thao Nguyen Thai Nguyen</author>
<affiliation confidence="0.750121333333333">School of Information Science, JAIST College of Technology, VNU Bao Ho Shimazu Japan Advanced Institute of Science and</affiliation>
<abstract confidence="0.998089071428571">This paper presents an empirical work for Vietnamese NP chunking task. We show how to build an annotation corpus of NP chunking and how discriminative sequence models are trained using the corpus. Experiment results using 5 fold cross validation test show that discriminative sequence learning are well suitable for Vietnamese chunking. In addition, by empirical experiments we show that the part of speech information contribute significantly to the performance of there learning models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="11697" citStr="Collins, 2002" startWordPosition="1933" endWordPosition="1934"> that of K classes. Though, we leave the details to (Kudo and Matsumoto, 2001), the idea of pairwise classification is to build K.(K-1)/2 classifiers considering all pairs of classes, and final decision is given by their weighted voting. The implementation of Vietnamese text chunking is based on Yamcha (V0.33)1. 2.3.3 Online Passive-Aggressive Learning Online Passive-Aggressive Learning (PA) was proposed by Crammer (Crammer et al, 2006) as an alternative learning algorithm to the maximize margin algorithm. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide to state of the art results on various domains including text segmentation, syntactic parsing, and dependency parsing. The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data. It may be difficult to obtain high accuracy in dealing with hard learning data. The online algorithm for chunking parsing in which we can attain the maximize margin of the training data without using an optimization technique. It is thus much faster and easier to implement. The details of PA algorithm for chunking parsing</context>
<context position="13099" citStr="Collins, 2002" startWordPosition="2187" endWordPosition="2188">ls y be: 4b(x, y) = 4b1(x, y), 4b2(x, y), ..., 4bd(x, y) where each feature mapping 4b1 maps (x, y) to a real value. We assume that each feature 4b(x, y) is associated with a weight value. The goal of PA learning for chunking parsing is to obtain a parameter w that minimizes the hinge-loss function and the margin of learning data. Algorithm 1 shows briefly the Online Learning for chunking problem. The detail about this algorithm can be referred to the work of (Crammer et al, 2006). In Line 7, the argmax value is computed by using the Viterbi algorithm which is similar to the one described in (Collins, 2002). Algorithm 1 is terminated after T round. 1Yamcha is available at http://chasen.org/ taku/software/yamcha/ 1 Input: S = (xi7 yi), i = 1, 2,..., n in which xi is the sentence and yi is a sequence of chunks 2 Aggressive parameter C 3 Output: the model 4 Initialize: w1 = (0, 0, ..., 0) 5 for t=1, 2... do 6 Receive an sentence xt 7 Predict y∗t = arg maxy∈Y (wt.4b(xt, yt)) Suffer loss: lt = wt.4b(xt, y∗t ) − wt.4b(xt, yt) + �/ρ(yt, y∗t ) lt Set:τt = 8 ||D(xt,yt )−D(xt,yt)||2 9 Update: wt+1 = wt + τt(4b(xt, yt) − 4b(xt, y∗t )) 10 end Algorithm 1: The Passive-Aggressive algorithm for NP chunking. 2.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
</authors>
<title>Online Passive-Aggressive Algorithm.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<marker>Crammer, 2006</marker>
<rawString>K. Crammer et al. 2006. Online Passive-Aggressive Algorithm. Journal of Machine Learning Research, 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chen</author>
<author>Y Zhang</author>
<author>H Ishihara</author>
</authors>
<title>An empirical study of Chinese chunking.</title>
<date>2006</date>
<booktitle>In Proceedings COLING/ACL</booktitle>
<contexts>
<context position="2496" citStr="Chen, Zhang, and Ishihara, 2006" startWordPosition="368" endWordPosition="372">otation corpus for conducting a Vietnamese chunking using machine learning methods. Unfortunately, at the moment, there is still no common standard annotated corpus for evaluation and comparison regarding Vietnamese chunking. In this paper, we aim at discussing on how we can build annotated data for Vietnamese text chunking and how to apply discriminative sequence learning for Vietnamese text chunking. We choose discriminative sequence models for Vietnamese text chunking because they have shown very suitable methods for several languages(i.e English, Japanese, Chinese) (Sha and Pereira, 2005)(Chen, Zhang, and Ishihara, 2006) (Kudo and Matsumoto, 2001). These presentative discriminative models which we choose for conducting empirical experiments including: Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001), Support Vector Machine (Vapnik, 1995) and Online Prediction (Crammer et al, 2006). In other words, because Noun Phrase chunks appear most frequently in sentences. So, in this paper we focus mainly on empirical experiments for the tasks of Vietnamese NP chunking. We plan to answer several major questions by using empirical experiments as follows. • Whether or not the discriminative learning model</context>
<context position="20449" citStr="Chen, Zhang, and Ishihara, 2006" startWordPosition="3392" endWordPosition="3396">rding to the performance of all discriminative sequence learning models. As we can see, if one do not use POS features the F1-score of each model is decreased more than 20%. We also remark that the lexical features contribute an important role to the performance of Vietnamese text 14 Figure 6: F-measures of three methods with different feature set chunking. If we do not use lexical features the F1-score of each model is decreased till approximately 3%. In conclusion, the POS features significantly effect on the performance of the discriminative sequence models. This is similar to the note of (Chen, Zhang, and Ishihara, 2006). Figure 6 reports the F-Measures of using different feature set for each discriminative models. Note that WPos, WLex, and WEdge mean without using Pos features, without using lexical features, and without using edge features, respectively. As we can see, the CRF-LBFGs always achieved the best scores in comparison with the other ones and the Online Learning achieved the lowest scores. 4 Conclusions In this paper, we report an investigation of developing a Vietnamese Chunking tool. We have constructed an annotation corpus of more than 9,000 sentences and exploiting discriminative learning mode</context>
</contexts>
<marker>Chen, Zhang, Ishihara, 2006</marker>
<rawString>W. Chen, Y. Zhang, and H. Ishihara 2006. An empirical study of Chinese chunking. In Proceedings COLING/ACL 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dinh Dien</author>
</authors>
<title>Vu Thuy</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE - International Conference on Computing and Telecommunication Technologies RIVF</booktitle>
<pages>248--253</pages>
<marker>Dien, 2006</marker>
<rawString>Dinh Dien, Vu Thuy 2006. A maximum entropy approach for vietnamese word segmentation. In Proceedings of the IEEE - International Conference on Computing and Telecommunication Technologies RIVF 2006: 248-253</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In the proceedings of International Conference on Machine Learning (ICML),</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2695" citStr="Lafferty, McCallum, and Pereira, 2001" startWordPosition="394" endWordPosition="398">n regarding Vietnamese chunking. In this paper, we aim at discussing on how we can build annotated data for Vietnamese text chunking and how to apply discriminative sequence learning for Vietnamese text chunking. We choose discriminative sequence models for Vietnamese text chunking because they have shown very suitable methods for several languages(i.e English, Japanese, Chinese) (Sha and Pereira, 2005)(Chen, Zhang, and Ishihara, 2006) (Kudo and Matsumoto, 2001). These presentative discriminative models which we choose for conducting empirical experiments including: Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001), Support Vector Machine (Vapnik, 1995) and Online Prediction (Crammer et al, 2006). In other words, because Noun Phrase chunks appear most frequently in sentences. So, in this paper we focus mainly on empirical experiments for the tasks of Vietnamese NP chunking. We plan to answer several major questions by using empirical experiments as follows. • Whether or not the discriminative learning models are suitable for Vietnamese chunking problem? • We want to know the difference of SVM, Online Learning, and Conditional Random Fields for Vietnamese chunking task. • Which features are suitable for</context>
<context position="7619" citStr="Lafferty, McCallum, and Pereira, 2001" startWordPosition="1202" endWordPosition="1206">e assigned the label ”O”. The consecutive words are NP if they is one of type as follows: ”noun noun”; ”article noun”, ”article noun adjective”. After generating such as data, we ask an expert about Vietnamese linguistic to correct the data. Finally, we got more than 9,000 sentences which are annotated with NP chunking labels. Figure 1 shows an example of the Vietnamese chunking corpus. 2.3 Discriminative Sequence Learning In this section, we briefly introduce three discriminative sequence learning models for chunking problems. 2.3.1 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty, McCallum, and Pereira, 2001) are undirected graphical models used to calculate the conditional 10 Figure 1: An Example of the Vietnamese chunking corpus probability of values on designated output nodes, given values assigned to other designated input nodes for data sequences. CRFs make a first-order Markov independence assumption among output nodes, and thus correspond to finite state machine (FSMs). Let o = (o1, o2, ... , oT) be some observed input data sequence, such as a sequence of words in a text (values on T input nodes of the graphical model). Let S be a finite set of FSM states, each is associated with a label l</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In the proceedings of International Conference on Machine Learning (ICML), pp.282-289, 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>N C Mai</author>
<author>D N Vu</author>
<author>T P Hoang</author>
</authors>
<date>1997</date>
<booktitle>Foundations of linguistics and Vietnamese. Education Publisher</booktitle>
<pages>142--152</pages>
<marker>Mai, Vu, Hoang, 1997</marker>
<rawString>N.C. Mai, D.N. Vu, T.P. Hoang. 1997. Foundations of linguistics and Vietnamese. Education Publisher (1997) 142. 152</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thi Minh Huyen Nguyen</author>
<author>Laurent Romary</author>
<author>Mathias Rossignol</author>
<author>Xuan Luong Vu</author>
</authors>
<title>A lexicon for Vietnamese language processing. Language Reseourse Evaluation</title>
<date>2006</date>
<pages>40--291</pages>
<marker>Nguyen, Romary, Rossignol, Vu, 2006</marker>
<rawString>Thi Minh Huyen Nguyen, Laurent Romary, Mathias Rossignol, Xuan Luong Vu. 2006. A lexicon for Vietnamese language processing. Language Reseourse Evaluation (2006) 40:291-309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh Nghiem</author>
<author>Dien Dinh</author>
<author>Mai Nguyen</author>
</authors>
<title>Improving Vietnamese POS tagging by integrating a rich feature set and Support Vector Machines.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE - International Conference on Computing and Telecommunication Technologies RIVF 2008:</booktitle>
<pages>128--133</pages>
<marker>Nghiem, Dinh, Nguyen, 2008</marker>
<rawString>Minh Nghiem, Dien Dinh, Mai Nguyen. 2008. Improving Vietnamese POS tagging by integrating a rich feature set and Support Vector Machines. In Proceedings of the IEEE - International Conference on Computing and Telecommunication Technologies RIVF 2008: 128–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X H Phan</author>
<author>M L Nguyen</author>
<author>C T Nguyen</author>
</authors>
<title>FlexCRFs: Flexible Conditional Random Field Toolkit.</title>
<date>2005</date>
<location>http://flexcrfs.sourceforge.net,</location>
<marker>Phan, Nguyen, Nguyen, 2005</marker>
<rawString>X.H. Phan, M.L. Nguyen, C.T. Nguyen. FlexCRFs: Flexible Conditional Random Field Toolkit. http://flexcrfs.sourceforge.net, 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with Support Vector Machines. The Second Meeting of the North American Chapter of the Association for Computational Linguistics</title>
<date>2001</date>
<contexts>
<context position="2524" citStr="Kudo and Matsumoto, 2001" startWordPosition="373" endWordPosition="376">etnamese chunking using machine learning methods. Unfortunately, at the moment, there is still no common standard annotated corpus for evaluation and comparison regarding Vietnamese chunking. In this paper, we aim at discussing on how we can build annotated data for Vietnamese text chunking and how to apply discriminative sequence learning for Vietnamese text chunking. We choose discriminative sequence models for Vietnamese text chunking because they have shown very suitable methods for several languages(i.e English, Japanese, Chinese) (Sha and Pereira, 2005)(Chen, Zhang, and Ishihara, 2006) (Kudo and Matsumoto, 2001). These presentative discriminative models which we choose for conducting empirical experiments including: Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001), Support Vector Machine (Vapnik, 1995) and Online Prediction (Crammer et al, 2006). In other words, because Noun Phrase chunks appear most frequently in sentences. So, in this paper we focus mainly on empirical experiments for the tasks of Vietnamese NP chunking. We plan to answer several major questions by using empirical experiments as follows. • Whether or not the discriminative learning models are suitable for Vietnames</context>
<context position="11161" citStr="Kudo and Matsumoto, 2001" startWordPosition="1851" endWordPosition="1854">d(st−1, st, t) + d j where fi(st−1, st, t) = δ(st−1, l,)δ(st, l) is a transition feature function which represents sequential dependencies by combining the label l, of the previous state st−1 and the label l of the SVMs estimate the label of an unknown example x whether the sign of f(x) is positive or not. Basically, SVMs are binary classifier, thus we must extend SVMs to multi-class classifier in or11 der to classify three or more classes. The pairwise classifier is one of the most popular methods to extend the binary classification task to that of K classes. Though, we leave the details to (Kudo and Matsumoto, 2001), the idea of pairwise classification is to build K.(K-1)/2 classifiers considering all pairs of classes, and final decision is given by their weighted voting. The implementation of Vietnamese text chunking is based on Yamcha (V0.33)1. 2.3.3 Online Passive-Aggressive Learning Online Passive-Aggressive Learning (PA) was proposed by Crammer (Crammer et al, 2006) as an alternative learning algorithm to the maximize margin algorithm. The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide to state of the art results on various domains incl</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with Support Vector Machines. The Second Meeting of the North American Chapter of the Association for Computational Linguistics (2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2005</date>
<booktitle>Proceedings of HLTNAACL</booktitle>
<pages>213--220</pages>
<contexts>
<context position="2464" citStr="Sha and Pereira, 2005" startWordPosition="365" endWordPosition="368">been looking for an annotation corpus for conducting a Vietnamese chunking using machine learning methods. Unfortunately, at the moment, there is still no common standard annotated corpus for evaluation and comparison regarding Vietnamese chunking. In this paper, we aim at discussing on how we can build annotated data for Vietnamese text chunking and how to apply discriminative sequence learning for Vietnamese text chunking. We choose discriminative sequence models for Vietnamese text chunking because they have shown very suitable methods for several languages(i.e English, Japanese, Chinese) (Sha and Pereira, 2005)(Chen, Zhang, and Ishihara, 2006) (Kudo and Matsumoto, 2001). These presentative discriminative models which we choose for conducting empirical experiments including: Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001), Support Vector Machine (Vapnik, 1995) and Online Prediction (Crammer et al, 2006). In other words, because Noun Phrase chunks appear most frequently in sentences. So, in this paper we focus mainly on empirical experiments for the tasks of Vietnamese NP chunking. We plan to answer several major questions by using empirical experiments as follows. • Whether or not t</context>
</contexts>
<marker>Sha, Pereira, 2005</marker>
<rawString>F. Sha and F. Pereira. 2005. Shallow Parsing with Conditional Random Fields. Proceedings of HLTNAACL 2003 213-220 (2003)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Nguyen</author>
<author>T K Nguyen</author>
<author>X H Phan</author>
<author>L M</author>
</authors>
<title>Vietnamese Word Segmentation with CRFs and SVMs: An Investigation.</title>
<date>2006</date>
<booktitle>The 20th Pacific Asia Conference on Language, Information, and Computation (PACLIC),</booktitle>
<pages>1--3</pages>
<location>Wuhan, China</location>
<marker>Nguyen, Nguyen, Phan, M, 2006</marker>
<rawString>C.T. Nguyen, T.K. Nguyen, X.H. Phan, L.M. Vietnamese Word Segmentation with CRFs and SVMs: An Investigation. 2006. The 20th Pacific Asia Conference on Language, Information, and Computation (PACLIC), 1-3 November, 2006, Wuhan, China</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>Proceedings of CoNLL-2000 ,</booktitle>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="16358" citStr="Sang and Buchholz, 2000" startWordPosition="2739" endWordPosition="2742">equence learning models, respectively. Note that the evaluation method is used the same as CONLL2000 did. We used Precision, Recall, and F-Measure in which Precision measures how many chunks found by the algorithm are correct and the recall is percentage of chunks defined in the corpus that were found by the chunking program. Precision = #correct−chunk #numberofchunks #correct−chunks Recall = #numerofchunksinthecorpus 2 � Precision � Recall F − measure =Precision + Recall To compute the scores in our experiments, we utilized the evaluation tool (conlleval.pl) which is available in CONLL 2000 (Sang and Buchholz, 2000, ). Figure 2 shows the precision scores of three methods using 5 Folds cross validation test. It reports that the CRF-LBFGS attain the highest score. The SVMs and CRF-SGD are comparable to CRF-LBFGS. The Online Learning achieved the lowest score. Figure 3 shows the recall scores of three CRFsLBFGS, CRFs-SGD, SVM, and Online Learning. The results show that CRFs-SGD achieved the highest score while the Online Learning obtained the lowest score in comparison with others. Figure 4 and Figure 5 show the F-measure and accuracy scores using 5 Folds Cross-validation Figure 2: Precision results in 5 F</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. Proceedings of CoNLL-2000 , Lisbon, Portugal, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Natural of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York:</location>
<contexts>
<context position="2735" citStr="Vapnik, 1995" startWordPosition="402" endWordPosition="403">sing on how we can build annotated data for Vietnamese text chunking and how to apply discriminative sequence learning for Vietnamese text chunking. We choose discriminative sequence models for Vietnamese text chunking because they have shown very suitable methods for several languages(i.e English, Japanese, Chinese) (Sha and Pereira, 2005)(Chen, Zhang, and Ishihara, 2006) (Kudo and Matsumoto, 2001). These presentative discriminative models which we choose for conducting empirical experiments including: Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001), Support Vector Machine (Vapnik, 1995) and Online Prediction (Crammer et al, 2006). In other words, because Noun Phrase chunks appear most frequently in sentences. So, in this paper we focus mainly on empirical experiments for the tasks of Vietnamese NP chunking. We plan to answer several major questions by using empirical experiments as follows. • Whether or not the discriminative learning models are suitable for Vietnamese chunking problem? • We want to know the difference of SVM, Online Learning, and Conditional Random Fields for Vietnamese chunking task. • Which features are suitable for discriminative learning models and how </context>
<context position="9679" citStr="Vapnik, 1995" startWordPosition="1569" endWordPosition="1570">t word is “conditional“. Training CRFs is commonly performed by maximizing the likelihood function with respect to the training data using advanced convex optimization techniques like L-BFGS. Recently, there are several works apply Stochastic Gradient Descent (SGD) for training CRFs models. SGD has been historically associated with back-propagation algorithms in multilayer neural networks. And inference in CRFs, i.e., searching the most likely output label sequence of an input observation sequence, can be done using Viterbi algorithm. 2.3.2 Support Vector Machines Support vector machine (SVM)(Vapnik, 1995) is a technique of machine learning based on statistical learning theory. The main idea behind this method can be summarized as follows. Suppose that we are given l training examples (xi, yi), (1 &lt; i &lt; l), where xi is a feature vector in n dimensional feature space, and yi is the class label 1-1, +11 of xi. SVM finds a hyperplane w.x+b = 0 which correctly separates training examples and has maximum margin which is the distance between two hyperplanes w · x + b &gt; 1 and w · x + b &lt; −1. Finally, the optimal hyperplane is formulated as follows: Ã Xl ! f(x) = sign αiyiK(xi, x) + b (3) 1 where αi is</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Natural of Statistical Learning Theory. New York: Springer-Verlag, 1995.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>