<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.518147">
<title confidence="0.990319">
GENEVAL: A Proposal for Shared-task Evaluation in NLG
</title>
<author confidence="0.990635">
Ehud Reiter
</author>
<affiliation confidence="0.995065">
University of Aberdeen, UK
</affiliation>
<email confidence="0.986657">
ereiter@csd.abdn.ac.uk
</email>
<author confidence="0.984529">
Anja Belz
</author>
<affiliation confidence="0.99201">
University of Brighton, UK
</affiliation>
<email confidence="0.990917">
a.s.belz@brighton.ac.uk
</email>
<sectionHeader confidence="0.993745" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999286">
We propose to organise a series of shared-
task NLG events, where participants are
asked to build systems with similar in-
put/output functionalities, and these sys-
tems are evaluated with a range of differ-
ent evaluation techniques. The main pur-
pose of these events is to allow us to com-
pare different evaluation techniques, by
correlating the results of different evalua-
tions on the systems entered in the events.
</bodyText>
<sectionHeader confidence="0.988236" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.99998023255814">
Evaluation is becoming increasingly important in
Natural Language Generation (NLG), as in most
other areas of Natural Language Processing (NLP).
NLG systems can be evaluated in many differ-
ent ways, with different associated resource re-
quirements. For example, a large-scale task-
effectiveness study with human subjects could last
over a year and cost more than US$100,000 (Re-
iter et al., 2003); on the other hand, a small-scale
comparison of generated texts to human-written
reference texts can be done in a manner of days.
However, while the latter kind of study is very
appealing in terms of cost and time, and cheap
and reliable evaluation techniques would be very
useful for people developing and testing new NLG
techniques, it is only worth doing if we have rea-
son to believe that its results tell us something
about how useful the generated texts are to real
human users. It is not obvious that this is the case
(Reiter and Sripada, 2002).
Perhaps the best way to study the reliability of
different evaluation techniques, and more gener-
ally to develop a better empirical understanding of
the strengths and problems of different evaluation
techniques, is to perform studies where a range of
different evaluation techniques are used to evalu-
ate a set of NLG systems with similar functional-
ities. Correlating the results of the different eval-
uation techniques will give us empirical insight as
to how well these techniques work in practice.
Unfortunately, few such studies have been car-
ried out, perhaps because (to date) few NLG sys-
tems have been built with comparable functional-
ity (our own work in this area is discussed below).
We hope to surmount this problem, by organising
‘shared task’ events to which NLG researchers can
submit systems based on a supplied data set of in-
puts and (human-written) text outputs. We will
then carry out our evaluation experiments on the
submitted systems. We hope that such shared-task
events will also make it easier for new researchers
to get involved in NLG, by providing data sets and
an evaluation framework.
</bodyText>
<sectionHeader confidence="0.92674" genericHeader="introduction">
2 Comparative Evaluations in NLG
</sectionHeader>
<bodyText confidence="0.999989807692308">
There is a long history of shared task initiatives
in NLP, of which the best known is perhaps MUC
(Hirschman, 1998); others include TREC, PARSE-
VAL, SENSEVAL, and the range of shared tasks or-
ganised by CoNLL. Such exercises are now com-
mon in most areas of NLP, and have had a major
impact on many areas, including machine transla-
tion and information extraction (see discussion of
history of shared-task initiatives and their impact
in Belz and Kilgarriff (2006)).
One of the best-known comparative studies
of evaluation techniques was by Papineni et al.
(2002) who proposed the BLEU metric for machine
translation and showed that BLEU correlated well
with human judgements when comparing several
machine translation systems. Several other studies
of this type have been carried out in the MT and
Summarisation communities.
The first comparison of NLG evaluation tech-
niques which we are aware of is by Bangalore et al.
(2000). The authors manually created several
variants of sentences from the Wall Street Jour-
nal, and evaluated these sentences using both hu-
man judgements and several corpus-based metrics.
They used linear regression to suggest a combina-
tion of the corpus-based metrics which they be-
</bodyText>
<page confidence="0.951505">
136
</page>
<bodyText confidence="0.986561916666667">
Proceedings of the Fourth International Natural Language Generation Conference, pages 136–138,
Sydney, July 2006. c�2006 Association for Computational Linguistics
lieve is a better predictor of human judgements
than any of the individual metrics.
In our work (Belz and Reiter, 2006), we used
several different evaluation techniques (human
and corpus-based) to evaluate the output of five
NLG systems which generated wind descriptions
for weather forecasts. We then analysed how well
the corpus-based evaluations correlated with the
human-based evaluations. Amongst other things,
we concluded that BLEU-type metrics work rea-
sonably well when comparing statistical NLG sys-
tems, but less well when comparing statistical NLG
systems to knowledge-based NLG systems.
We worked in this domain because of the avail-
ability of the SumTime corpus (Sripada et al.,
2003), which contains both numerical weather
prediction data (i.e., inputs to NLG) and human
written forecast texts (i.e., target outputs from
NLG). We are not aware of any other NLG-related
corpora which contain a large number of texts and
corresponding input data sets, and are freely avail-
able to the research community.
</bodyText>
<sectionHeader confidence="0.997003" genericHeader="method">
3 Our Proposal
</sectionHeader>
<bodyText confidence="0.999941">
We intend to apply for funding for a three-year
project to create more shared input/output data sets
(we are focusing on data-to-text tasks for the rea-
sons discussed in Belz and Kilgarriff (2006)), or-
ganise shared task workshops, and create and test
a range of methods for evaluating submitted sys-
tems.
</bodyText>
<subsectionHeader confidence="0.997986">
3.1 Step 1: Create data sets
</subsectionHeader>
<bodyText confidence="0.9999145">
We intend to create input/output data sets that con-
tain the following types of representations:
</bodyText>
<listItem confidence="0.99966925">
• raw non-linguistic input data;
• structured content representations, roughly
corresponding to document plans (Reiter and
Dale, 2000);
• semantic-level representations, roughly cor-
responding to text specifications (Reiter and
Dale, 2000);
• actual human-authored corpus texts.
</listItem>
<bodyText confidence="0.999944193548387">
The presence of intermediate representations in
our data sets means that researchers who are just
interested in document planning, microplanning,
or surface realisation do not need to build com-
plete NLG systems in order to participate.
We will create the semantic-level representa-
tions by parsing the corpus texts, probably us-
ing a LinGO parser1. We will create the content
representations using application-specific analysis
tools, similar to a tool we have already created for
SumTime wind statements. The actual data sets
we currently intend to create are as follows (see
also summary in Table 1).
SumTime weather statements: These are brief
statements which describe predicted precipitation
and cloud over a forecast period. We will extract
the texts (and the corresponding input data) from
the existing SumTime corpus.
Statistics summaries: We will ask people (prob-
ably students) to write paragraph-length textual
summaries of statistical data. The actual data will
come from opinion polls or national statistics of-
fices. The corpus will also include data about the
authors (e.g., age, sex, domain expertise).
Nurses’ reports: As part of a new project at Ab-
erdeen, Babytalk2, we will be acquiring a corpus
of texts written by nurses to summarise the status
of a baby in a neonatal intensive care unit, along
with the raw data this is based on (sensor read-
ings, records of actions taken such as giving med-
ication).
</bodyText>
<subsectionHeader confidence="0.998924">
3.2 Step 2: Organise workshops
</subsectionHeader>
<bodyText confidence="0.999833388888889">
The second step is to organise workshops. We
intend to use a fairly standard organisation (Belz
and Kilgarriff, 2006). We will release the data
sets (but not the reference texts), give people six
months to develop systems, and invite people who
submit systems to a workshop. Participants can
submit either complete data-to-text NLG systems,
or components which just do document planning,
microplanning, or realisation.
We are planning to increase the number and
complexity of tasks from one round to the next,
as this has been useful in other NLP evaluations
(Belz and Kilgarriff, 2006); for example, we will
add surface realisation as a separate task in round
2 and layout/structuring task in round 3.
We will carry out all evaluation activities (see
below) ourselves, workshop participants will not
be involved in this.
</bodyText>
<subsectionHeader confidence="0.999779">
3.3 Step 3: Evaluation
</subsectionHeader>
<bodyText confidence="0.999514">
The final step is to evaluate the systems and com-
ponents submitted to the workshop. As the main
</bodyText>
<footnote confidence="0.999411">
1http://lingo.stanford.edu/
2http://www.csd.abdn.ac.uk/research/babytalk/
</footnote>
<page confidence="0.985797">
137
</page>
<table confidence="0.958721">
Corpus num texts num ref (*) text size main NLG challenges
Weather statements 3000 300 1-2 sentences content det, lex choice, aggregation
Statistical summaries 1000 100 paragraph above plus surface realisation
Nurses’ reports 200 50 several paras above plus text structuring/layout
(*) In addition to the main corpus, we will also gather texts which will be used as reference texts for
corpus-based evaluations; ‘num ref’ is the number of such texts. These texts will not be released.
</table>
<tableCaption confidence="0.999914">
Table 1: Planned GENEVAL data sets.
</tableCaption>
<bodyText confidence="0.999890166666667">
purpose of this whole exercise is to see how well
different evaluation techniques correlate with each
other, we plan to carry out a range of different
evaluations, including the following.
Corpus-based evaluations: We will develop
new, linguistically grounded evaluation metrics,
and compare these to existing metrics including
BLEU, NIST, and string-edit distance. We will also
investigate how sensitive different metrics are to
size and make-up of the reference corpus.
Human-based preference judgements: We will
investigate different experimental designs and
methods for overcoming respondent bias (e.g.
what is known as ‘central tendency bias’, where
some respondents avoid judgements at either end
of a scale). As we showed previously (Belz and
Reiter, 2006) that there are significant inter-subject
differences in ratings, one thing we want to deter-
mine is how many subjects are needed to get reli-
able and reproducible results.
Task performance. This depends on the do-
main, but e.g. in the nurse-report domain we
could use the methodology of (Law et al., 2005),
who showed medical professionals the texts, asked
them to make a treatment decision, and then rated
the correctness of the suggested treatments.
As well as recommendations about the appro-
priateness of existing evaluation techniques, we
hope the above experiments will allow us to sug-
gest new evaluation techniques for NLG.
</bodyText>
<sectionHeader confidence="0.996857" genericHeader="method">
4 Next Steps
</sectionHeader>
<bodyText confidence="0.999968923076923">
At this point, we encourage NLG researchers to
give us their views regarding our plans for the or-
ganisation of GENEVAL, the data and evaluation
methods we are planning to use, to suggest addi-
tional data sets or evaluation techniques, and espe-
cially to let us know whether they would be inter-
ested in participating.
If our proposal is successful, we hope that the
project will start in summer 2007, with the first
data set released in late 2007 and the first work-
shop in summer 2008. ELRA/ELDA have also al-
ready agreed to help us with this work, contribut-
ing human and data resources.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999795472222222">
Srinavas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation. In
Proceedings of INLG-2000, pages 1–8.
Anja Belz and Adam Kilgarriff. 2006. Shared-task
evaluations in HLT: Lessons for NLG. In Proceed-
ings of INLG-2006.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of EACL-2006, pages 313–320.
Lynette Hirschman. 1998. The evolution of evaluation:
Lessons from the Message Understanding Confer-
ences. Computer Speech and Language, 12:283–
285.
Anna Law, Yvonne Freer, Jim Hunter, Robert Logie,
Neil McIntosh, and John Quinn. 2005. Generat-
ing textual summaries of graphical time series data
to support medical decision making in the neonatal
intensive care unit. Journal of Clinical Monitoring
and Computing, 19:183–194.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
ofACL-2002, pages 311–318.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter and Somayajulu Sripada. 2002. Should
corpora texts be gold standards for NLG? In Pro-
ceedings of INLG-2002, pages 97–104.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144:41–58.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Exploiting a parallel text-data corpus. In
Proceedings of Corpus Linguistics 2003, pages 734–
743.
</reference>
<page confidence="0.997336">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.046558">
<title confidence="0.999901">GENEVAL: A Proposal for Shared-task Evaluation in NLG</title>
<author confidence="0.993572">Ehud Reiter</author>
<affiliation confidence="0.999957">University of Aberdeen,</affiliation>
<email confidence="0.998056">ereiter@csd.abdn.ac.uk</email>
<author confidence="0.988324">Anja Belz</author>
<affiliation confidence="0.999969">University of Brighton,</affiliation>
<email confidence="0.998031">a.s.belz@brighton.ac.uk</email>
<abstract confidence="0.986385757575757">We propose to organise a series of sharedwhere participants are asked to build systems with similar input/output functionalities, and these systems are evaluated with a range of different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events. 1 Background Evaluation is becoming increasingly important in Language Generation as in most areas of Natural Language Processing can be evaluated in many different ways, with different associated resource requirements. For example, a large-scale taskeffectiveness study with human subjects could last over a year and cost more than US$100,000 (Reiter et al., 2003); on the other hand, a small-scale comparison of generated texts to human-written reference texts can be done in a manner of days. However, while the latter kind of study is very appealing in terms of cost and time, and cheap and reliable evaluation techniques would be very for people developing and testing new techniques, it is only worth doing if we have reason to believe that its results tell us something about how useful the generated texts are to real human users. It is not obvious that this is the case (Reiter and Sripada, 2002). Perhaps the best way to study the reliability of different evaluation techniques, and more generally to develop a better empirical understanding of the strengths and problems of different evaluation techniques, is to perform studies where a range of different evaluation techniques are used to evalua set of with similar functionalities. Correlating the results of the different evaluation techniques will give us empirical insight as to how well these techniques work in practice. Unfortunately, few such studies have been carout, perhaps because (to date) few systems have been built with comparable functionality (our own work in this area is discussed below). We hope to surmount this problem, by organising task’ events to which can submit systems based on a supplied data set of inputs and (human-written) text outputs. We will then carry out our evaluation experiments on the submitted systems. We hope that such shared-task events will also make it easier for new researchers get involved in by providing data sets and an evaluation framework. 2 Comparative Evaluations in NLG There is a long history of shared task initiatives of which the best known is perhaps 1998); others include and the range of shared tasks orby Such exercises are now comin most areas of and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. who proposed the for machine and showed that well with human judgements when comparing several machine translation systems. Several other studies this type have been carried out in the Summarisation communities. first comparison of techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combinaof the corpus-based metrics which they be- 136 of the Fourth International Natural Language Generation pages July 2006. Association for Computational Linguistics lieve is a better predictor of human judgements than any of the individual metrics. In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five which generated wind descriptions for weather forecasts. We then analysed how well the corpus-based evaluations correlated with the human-based evaluations. Amongst other things, concluded that metrics work reawell when comparing statistical sysbut less well when comparing statistical to knowledge-based We worked in this domain because of the availability of the SumTime corpus (Sripada et al., 2003), which contains both numerical weather data (i.e., inputs to and human written forecast texts (i.e., target outputs from We are not aware of any other corpora which contain a large number of texts and corresponding input data sets, and are freely available to the research community. 3 Our Proposal We intend to apply for funding for a three-year project to create more shared input/output data sets (we are focusing on data-to-text tasks for the reasons discussed in Belz and Kilgarriff (2006)), organise shared task workshops, and create and test a range of methods for evaluating submitted systems. 3.1 Step 1: Create data sets We intend to create input/output data sets that contain the following types of representations: • raw non-linguistic input data; • structured content representations, roughly corresponding to document plans (Reiter and Dale, 2000); • semantic-level representations, roughly corresponding to text specifications (Reiter and Dale, 2000); • actual human-authored corpus texts. The presence of intermediate representations in our data sets means that researchers who are just interested in document planning, microplanning, or surface realisation do not need to build comin order to participate. We will create the semantic-level representations by parsing the corpus texts, probably usa LinGO We will create the content representations using application-specific analysis tools, similar to a tool we have already created for SumTime wind statements. The actual data sets we currently intend to create are as follows (see also summary in Table 1). weather statements: are brief statements which describe predicted precipitation and cloud over a forecast period. We will extract the texts (and the corresponding input data) from the existing SumTime corpus. summaries: will ask people (probably students) to write paragraph-length textual summaries of statistical data. The actual data will come from opinion polls or national statistics offices. The corpus will also include data about the authors (e.g., age, sex, domain expertise). reports: part of a new project at Abwe will be acquiring a corpus of texts written by nurses to summarise the status of a baby in a neonatal intensive care unit, along with the raw data this is based on (sensor readings, records of actions taken such as giving medication). 3.2 Step 2: Organise workshops The second step is to organise workshops. We intend to use a fairly standard organisation (Belz and Kilgarriff, 2006). We will release the data sets (but not the reference texts), give people six months to develop systems, and invite people who submit systems to a workshop. Participants can either complete data-to-text or components which just do document planning, microplanning, or realisation. We are planning to increase the number and complexity of tasks from one round to the next, this has been useful in other (Belz and Kilgarriff, 2006); for example, we will add surface realisation as a separate task in round 2 and layout/structuring task in round 3. We will carry out all evaluation activities (see below) ourselves, workshop participants will not be involved in this. 3.3 Step 3: Evaluation The final step is to evaluate the systems and components submitted to the workshop. As the main 137 Corpus num texts num ref (*) text size Weather statements 3000 300 1-2 sentences content det, lex choice, aggregation Statistical summaries 1000 100 paragraph above plus surface realisation Nurses’ reports 200 50 several paras above plus text structuring/layout (*) In addition to the main corpus, we will also gather texts which will be used as reference texts for corpus-based evaluations; ‘num ref’ is the number of such texts. These texts will not be released. 1: Planned sets. purpose of this whole exercise is to see how well different evaluation techniques correlate with each other, we plan to carry out a range of different evaluations, including the following. evaluations: will develop new, linguistically grounded evaluation metrics, and compare these to existing metrics including and string-edit distance. We will also investigate how sensitive different metrics are to size and make-up of the reference corpus. preference judgements: will investigate different experimental designs and methods for overcoming respondent bias (e.g. what is known as ‘central tendency bias’, where some respondents avoid judgements at either end of a scale). As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. This depends on the domain, but e.g. in the nurse-report domain we could use the methodology of (Law et al., 2005), who showed medical professionals the texts, asked them to make a treatment decision, and then rated the correctness of the suggested treatments. As well as recommendations about the appropriateness of existing evaluation techniques, we hope the above experiments will allow us to sugnew evaluation techniques for 4 Next Steps this point, we encourage to give us their views regarding our plans for the orof the data and evaluation methods we are planning to use, to suggest additional data sets or evaluation techniques, and especially to let us know whether they would be interested in participating. If our proposal is successful, we hope that the project will start in summer 2007, with the first data set released in late 2007 and the first workin summer 2008. also already agreed to help us with this work, contributing human and data resources.</abstract>
<note confidence="0.953421142857143">References Srinavas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation. In of pages 1–8. Anja Belz and Adam Kilgarriff. 2006. Shared-task in HLT: Lessons for NLG. In Proceedof Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In of pages 313–320. Lynette Hirschman. 1998. The evolution of evaluation: Lessons from the Message Understanding Confer- Speech and 12:283– 285.</note>
<author confidence="0.736338">Generat-</author>
<abstract confidence="0.785086">ing textual summaries of graphical time series data to support medical decision making in the neonatal care unit. of Clinical Monitoring 19:183–194. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: A method for automatic of machine translation. In pages 311–318.</abstract>
<note confidence="0.959028857142857">Reiter and Robert Dale. 2000. Natural Generation Cambridge University Press. Ehud Reiter and Somayajulu Sripada. 2002. Should texts be gold standards for NLG? In Proof pages 97–104. Ehud Reiter, Roma Robertson, and Liesl Osman. 2003. Lessons from a failure: Generating tailored smoking letters. 144:41–58. Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2003. Exploiting a parallel text-data corpus. In of Corpus Linguistics pages 734– 743. 138</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinavas Bangalore</author>
<author>Owen Rambow</author>
<author>Steve Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of INLG-2000,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3619" citStr="Bangalore et al. (2000)" startWordPosition="588" endWordPosition="591">s, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combination of the corpus-based metrics which they be136 Proceedings of the Fourth International Natural Language Generation Conference, pages 136–138, Sydney, July 2006. c�2006 Association for Computational Linguistics lieve is a better predictor of human judgements than any of the individual metrics. In our work (Belz and Reiter, 2006), we used several different evaluatio</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>Srinavas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation. In Proceedings of INLG-2000, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Shared-task evaluations in HLT: Lessons for NLG.</title>
<date>2006</date>
<booktitle>In Proceedings of INLG-2006.</booktitle>
<contexts>
<context position="3156" citStr="Belz and Kilgarriff (2006)" startWordPosition="515" endWordPosition="518">task events will also make it easier for new researchers to get involved in NLG, by providing data sets and an evaluation framework. 2 Comparative Evaluations in NLG There is a long history of shared task initiatives in NLP, of which the best known is perhaps MUC (Hirschman, 1998); others include TREC, PARSEVAL, SENSEVAL, and the range of shared tasks organised by CoNLL. Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human</context>
<context position="5287" citStr="Belz and Kilgarriff (2006)" startWordPosition="844" endWordPosition="847">We worked in this domain because of the availability of the SumTime corpus (Sripada et al., 2003), which contains both numerical weather prediction data (i.e., inputs to NLG) and human written forecast texts (i.e., target outputs from NLG). We are not aware of any other NLG-related corpora which contain a large number of texts and corresponding input data sets, and are freely available to the research community. 3 Our Proposal We intend to apply for funding for a three-year project to create more shared input/output data sets (we are focusing on data-to-text tasks for the reasons discussed in Belz and Kilgarriff (2006)), organise shared task workshops, and create and test a range of methods for evaluating submitted systems. 3.1 Step 1: Create data sets We intend to create input/output data sets that contain the following types of representations: • raw non-linguistic input data; • structured content representations, roughly corresponding to document plans (Reiter and Dale, 2000); • semantic-level representations, roughly corresponding to text specifications (Reiter and Dale, 2000); • actual human-authored corpus texts. The presence of intermediate representations in our data sets means that researchers who </context>
<context position="7365" citStr="Belz and Kilgarriff, 2006" startWordPosition="1170" endWordPosition="1173">al data. The actual data will come from opinion polls or national statistics offices. The corpus will also include data about the authors (e.g., age, sex, domain expertise). Nurses’ reports: As part of a new project at Aberdeen, Babytalk2, we will be acquiring a corpus of texts written by nurses to summarise the status of a baby in a neonatal intensive care unit, along with the raw data this is based on (sensor readings, records of actions taken such as giving medication). 3.2 Step 2: Organise workshops The second step is to organise workshops. We intend to use a fairly standard organisation (Belz and Kilgarriff, 2006). We will release the data sets (but not the reference texts), give people six months to develop systems, and invite people who submit systems to a workshop. Participants can submit either complete data-to-text NLG systems, or components which just do document planning, microplanning, or realisation. We are planning to increase the number and complexity of tasks from one round to the next, as this has been useful in other NLP evaluations (Belz and Kilgarriff, 2006); for example, we will add surface realisation as a separate task in round 2 and layout/structuring task in round 3. We will carry </context>
</contexts>
<marker>Belz, Kilgarriff, 2006</marker>
<rawString>Anja Belz and Adam Kilgarriff. 2006. Shared-task evaluations in HLT: Lessons for NLG. In Proceedings of INLG-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-2006,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="4182" citStr="Belz and Reiter, 2006" startWordPosition="672" endWordPosition="675">hniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combination of the corpus-based metrics which they be136 Proceedings of the Fourth International Natural Language Generation Conference, pages 136–138, Sydney, July 2006. c�2006 Association for Computational Linguistics lieve is a better predictor of human judgements than any of the individual metrics. In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. We then analysed how well the corpus-based evaluations correlated with the human-based evaluations. Amongst other things, we concluded that BLEU-type metrics work reasonably well when comparing statistical NLG systems, but less well when comparing statistical NLG systems to knowledge-based NLG systems. We worked in this domain because of the availability of the SumTime corpus (Sripada et al., 2003), which contains both nu</context>
<context position="9551" citStr="Belz and Reiter, 2006" startWordPosition="1502" endWordPosition="1505">different evaluations, including the following. Corpus-based evaluations: We will develop new, linguistically grounded evaluation metrics, and compare these to existing metrics including BLEU, NIST, and string-edit distance. We will also investigate how sensitive different metrics are to size and make-up of the reference corpus. Human-based preference judgements: We will investigate different experimental designs and methods for overcoming respondent bias (e.g. what is known as ‘central tendency bias’, where some respondents avoid judgements at either end of a scale). As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. Task performance. This depends on the domain, but e.g. in the nurse-report domain we could use the methodology of (Law et al., 2005), who showed medical professionals the texts, asked them to make a treatment decision, and then rated the correctness of the suggested treatments. As well as recommendations about the appropriateness of existing evaluation techniques, we hope the above experiments will allow us to suggest new eva</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of EACL-2006, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>The evolution of evaluation: Lessons from the Message Understanding Conferences. Computer Speech and Language,</title>
<date>1998</date>
<pages>12--283</pages>
<contexts>
<context position="2811" citStr="Hirschman, 1998" startWordPosition="459" endWordPosition="460">(our own work in this area is discussed below). We hope to surmount this problem, by organising ‘shared task’ events to which NLG researchers can submit systems based on a supplied data set of inputs and (human-written) text outputs. We will then carry out our evaluation experiments on the submitted systems. We hope that such shared-task events will also make it easier for new researchers to get involved in NLG, by providing data sets and an evaluation framework. 2 Comparative Evaluations in NLG There is a long history of shared task initiatives in NLP, of which the best known is perhaps MUC (Hirschman, 1998); others include TREC, PARSEVAL, SENSEVAL, and the range of shared tasks organised by CoNLL. Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation </context>
</contexts>
<marker>Hirschman, 1998</marker>
<rawString>Lynette Hirschman. 1998. The evolution of evaluation: Lessons from the Message Understanding Conferences. Computer Speech and Language, 12:283– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Law</author>
<author>Yvonne Freer</author>
<author>Jim Hunter</author>
<author>Robert Logie</author>
<author>Neil McIntosh</author>
<author>John Quinn</author>
</authors>
<title>Generating textual summaries of graphical time series data to support medical decision making in the neonatal intensive care unit.</title>
<date>2005</date>
<journal>Journal of Clinical Monitoring and Computing,</journal>
<pages>19--183</pages>
<contexts>
<context position="9854" citStr="Law et al., 2005" startWordPosition="1554" endWordPosition="1557"> the reference corpus. Human-based preference judgements: We will investigate different experimental designs and methods for overcoming respondent bias (e.g. what is known as ‘central tendency bias’, where some respondents avoid judgements at either end of a scale). As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. Task performance. This depends on the domain, but e.g. in the nurse-report domain we could use the methodology of (Law et al., 2005), who showed medical professionals the texts, asked them to make a treatment decision, and then rated the correctness of the suggested treatments. As well as recommendations about the appropriateness of existing evaluation techniques, we hope the above experiments will allow us to suggest new evaluation techniques for NLG. 4 Next Steps At this point, we encourage NLG researchers to give us their views regarding our plans for the organisation of GENEVAL, the data and evaluation methods we are planning to use, to suggest additional data sets or evaluation techniques, and especially to let us kno</context>
</contexts>
<marker>Law, Freer, Hunter, Logie, McIntosh, Quinn, 2005</marker>
<rawString>Anna Law, Yvonne Freer, Jim Hunter, Robert Logie, Neil McIntosh, and John Quinn. 2005. Generating textual summaries of graphical time series data to support medical decision making in the neonatal intensive care unit. Journal of Clinical Monitoring and Computing, 19:183–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL-2002,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="3255" citStr="Papineni et al. (2002)" startWordPosition="530" endWordPosition="533"> and an evaluation framework. 2 Comparative Evaluations in NLG There is a long history of shared task initiatives in NLP, of which the best known is perhaps MUC (Hirschman, 1998); others include TREC, PARSEVAL, SENSEVAL, and the range of shared tasks organised by CoNLL. Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combination </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings ofACL-2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5654" citStr="Reiter and Dale, 2000" startWordPosition="901" endWordPosition="904"> freely available to the research community. 3 Our Proposal We intend to apply for funding for a three-year project to create more shared input/output data sets (we are focusing on data-to-text tasks for the reasons discussed in Belz and Kilgarriff (2006)), organise shared task workshops, and create and test a range of methods for evaluating submitted systems. 3.1 Step 1: Create data sets We intend to create input/output data sets that contain the following types of representations: • raw non-linguistic input data; • structured content representations, roughly corresponding to document plans (Reiter and Dale, 2000); • semantic-level representations, roughly corresponding to text specifications (Reiter and Dale, 2000); • actual human-authored corpus texts. The presence of intermediate representations in our data sets means that researchers who are just interested in document planning, microplanning, or surface realisation do not need to build complete NLG systems in order to participate. We will create the semantic-level representations by parsing the corpus texts, probably using a LinGO parser1. We will create the content representations using application-specific analysis tools, similar to a tool we ha</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Should corpora texts be gold standards for NLG?</title>
<date>2002</date>
<booktitle>In Proceedings of INLG-2002,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="1552" citStr="Reiter and Sripada, 2002" startWordPosition="249" endWordPosition="252">could last over a year and cost more than US$100,000 (Reiter et al., 2003); on the other hand, a small-scale comparison of generated texts to human-written reference texts can be done in a manner of days. However, while the latter kind of study is very appealing in terms of cost and time, and cheap and reliable evaluation techniques would be very useful for people developing and testing new NLG techniques, it is only worth doing if we have reason to believe that its results tell us something about how useful the generated texts are to real human users. It is not obvious that this is the case (Reiter and Sripada, 2002). Perhaps the best way to study the reliability of different evaluation techniques, and more generally to develop a better empirical understanding of the strengths and problems of different evaluation techniques, is to perform studies where a range of different evaluation techniques are used to evaluate a set of NLG systems with similar functionalities. Correlating the results of the different evaluation techniques will give us empirical insight as to how well these techniques work in practice. Unfortunately, few such studies have been carried out, perhaps because (to date) few NLG systems hav</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>Ehud Reiter and Somayajulu Sripada. 2002. Should corpora texts be gold standards for NLG? In Proceedings of INLG-2002, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Roma Robertson</author>
<author>Liesl Osman</author>
</authors>
<title>Lessons from a failure: Generating tailored smoking cessation letters.</title>
<date>2003</date>
<journal>Artificial Intelligence,</journal>
<pages>144--41</pages>
<contexts>
<context position="1001" citStr="Reiter et al., 2003" startWordPosition="150" endWordPosition="154">different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events. 1 Background Evaluation is becoming increasingly important in Natural Language Generation (NLG), as in most other areas of Natural Language Processing (NLP). NLG systems can be evaluated in many different ways, with different associated resource requirements. For example, a large-scale taskeffectiveness study with human subjects could last over a year and cost more than US$100,000 (Reiter et al., 2003); on the other hand, a small-scale comparison of generated texts to human-written reference texts can be done in a manner of days. However, while the latter kind of study is very appealing in terms of cost and time, and cheap and reliable evaluation techniques would be very useful for people developing and testing new NLG techniques, it is only worth doing if we have reason to believe that its results tell us something about how useful the generated texts are to real human users. It is not obvious that this is the case (Reiter and Sripada, 2002). Perhaps the best way to study the reliability o</context>
</contexts>
<marker>Reiter, Robertson, Osman, 2003</marker>
<rawString>Ehud Reiter, Roma Robertson, and Liesl Osman. 2003. Lessons from a failure: Generating tailored smoking cessation letters. Artificial Intelligence, 144:41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Exploiting a parallel text-data corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>734--743</pages>
<contexts>
<context position="4758" citStr="Sripada et al., 2003" startWordPosition="757" endWordPosition="760"> metrics. In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. We then analysed how well the corpus-based evaluations correlated with the human-based evaluations. Amongst other things, we concluded that BLEU-type metrics work reasonably well when comparing statistical NLG systems, but less well when comparing statistical NLG systems to knowledge-based NLG systems. We worked in this domain because of the availability of the SumTime corpus (Sripada et al., 2003), which contains both numerical weather prediction data (i.e., inputs to NLG) and human written forecast texts (i.e., target outputs from NLG). We are not aware of any other NLG-related corpora which contain a large number of texts and corresponding input data sets, and are freely available to the research community. 3 Our Proposal We intend to apply for funding for a three-year project to create more shared input/output data sets (we are focusing on data-to-text tasks for the reasons discussed in Belz and Kilgarriff (2006)), organise shared task workshops, and create and test a range of metho</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2003</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2003. Exploiting a parallel text-data corpus. In Proceedings of Corpus Linguistics 2003, pages 734– 743.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>