<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032823">
<title confidence="0.972282">
SU-Sentilab : A Classification System for Sentiment Analysis in Twitter
</title>
<author confidence="0.994392">
Gizem Gezici, Rahim Dehkharghani, Berrin Yanikoglu, Dilek Tapucu, Yucel Saygin
</author>
<affiliation confidence="0.817665">
Sabanci University
Istanbul, Turkey 34956
</affiliation>
<email confidence="0.999489">
{gizemgezici,rdehkharghani,berrin,dilektapucu,ysaygin}@sabanciuniv.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999332818181818">
Sentiment analysis refers to automatically ex-
tracting the sentiment present in a given natu-
ral language text. We present our participation
to the SemEval2013 competition, in the senti-
ment analysis of Twitter and SMS messages.
Our approach for this task is the combination
of two sentiment analysis subsystems which
are combined together to build the final sys-
tem. Both subsystems use supervised learning
using features based on various polarity lexi-
cons.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945710526316">
Business owners are interested in the feedback of
their customers about the products and services pro-
vided by businesses. Social media networks and
micro-blogs such as Facebook and Twitter play an
important role in this area. Micro-blogs allow users
share their ideas with others in terms of small sen-
tences; while Facebook updates may indicate an
opinion inside a longer text. Automatic sentiment
analysis of text collected from social media makes it
possible to quantitatively analyze this feedback.
In this paper we describe our sentiment analy-
sis system identified as SU-Sentilab in the SemEval
2013 competition, Task 2: Sentiment analysis in
Twitter. One or the problems in this competition was
to label a given tweet or sms message with the cor-
rect sentiment orientation, as positive, negative or
neutral. In the second task of the same competition,
the polarity of a given word or word sequence in the
message was asked. Details are described in (Man-
andhar and Yuret, 2013).
We participated in both of these tasks using a
classifier combination consisting of two sub-systems
that are based on (Dehkharghani et al., 2012)(Gezici
et al., 2012) and adapted to the tweet domain. Both
sub-systems use supervised learning in which the
system is trained using tweets with known polari-
ties and used to predict the label (polarity) of tweets
in the test set. Both systems use features that
are based on well-known polarity resources namely
SentiWordNet (Baccianella et al., 2010), SenticNet
(Cambria et al., 2012) and NRC Emotion Lexicon
(Mohammad, 2012). Also a set of positive and neg-
ative seed words (Liu et al., 2005) is used in feature
extraction.
The remainder of paper is organized as follows:
Related works are presented in Section 2; the pro-
posed approach is described in Section 3 and exper-
imental evaluation is presented in Section 4.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999865714285714">
There has been much work on sentiment analysis in
the last ten years (Riloff and Wiebe, 2003) (Wilson
et al., 2009) (Taboada et al., 2011) (Pang and Lee,
2008). The two fundamental methods for sentiment
analysis are lexicon-based and supervised methods.
The lexicon-based technique adopts the idea of de-
termining the review sentiment by obtaining word
polarities from a lexicon, such as the SentiWordNet
(Baccianella et al., 2010), SenticNet (Cambria et al.,
2012). This lexicon can be domain-independent or
domain-specific. One can use a domain-specific lex-
icon whenever available, to get a better performance
by obtaining the correct word polarities in the given
domain (e.g., the word ’small’ has a positive mean-
</bodyText>
<page confidence="0.983328">
471
</page>
<bodyText confidence="0.986063119402985">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 471–477, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
ing in cell phone domain, while it has a negative
meaning in hotel domain). On the other hand, estab-
lishing a domain-specific lexicon is costly, so many
systems use a domain-independent lexicon, such as
the SentiWordNet, shortly SWN, (Baccianella et al.,
2010) and SenticNet (Cambria et al., 2012). Part
of Speech (POS) information is commonly indicated
in polarity lexicons, partly to overcome word-sense
disambiguity and therefore help achieve a better sen-
timent classification performance.
Alternatively, supervised methods use machine
learning techniques to build models or discrimina-
tors for the different classes (e.g. positive reviews),
using a large corpus. For example, in (Pang et al.,
2002) (Yu and Hatzivassiloglou, 2003), the Naive
Bayes algorithm is used to separate positive reviews
from negative ones. Note that supervised learning
techniques can also use a lexicon in the feature ex-
traction stage. They also generally perform bet-
ter compared to lexicon-based approaches; however
collecting a large training data may be an issue.
In estimating the sentiment of a given natural lan-
guage text, many issues are considered. For instance
one important problem is determining the subjectiv-
ity of a given sentence. In an early study, the ef-
fects of adjective orientation and gradability on sen-
tence subjectivity was studied (Hatzivassiloglou and
Wiebe, 2000). Wiebe et al. (Wiebe et al., 2004)
presents a broad survey on subjectivity recognition
and the key elements that may have an impact on it.
In estimating the sentiment polarity, the use of
higher-order n-grams is also studied. Pang et. al
report results where unigrams work better than bi-
grams for sentiment classification on a movie dataset
(Pang et al., 2002). Similarly, occurrence of rare
words (Yang et al., 2006) or the position of words in
a text are examined for usefulness (Kim and Hovy,
2006)(Pang et al., 2002). In connection with the oc-
currences of rare words, different variations of delta
tf*idf scores of words, indicating the difference in
occurrences of words in different classes (positive or
negative reviews), have been suggested (Paltoglou
and Thelwall, 2010).
In addition to sentiment classification, obtaining
the opinion strength is another issue which may be
of interest. Wilson et al. (Wilson et al., 2004) for
instance, attempts to determine clause-level opinion
strength. Since this is a difficult task, one of the re-
cent studies also investigated the relations between
word disambiguation and subjectivity, in order to
obtain sufficient information for a better sentiment
classification (Wiebe and Mihalcea, 2006). A recent
survey describing the fundamental approaches can
be found in (Liu, 2012).
Two sub-systems combined to form the SU-
Sentilab submission are slightly modified from our
previous work (Gezici et al., 2012) (Dehkharghani
et al., 2012) (Demiroz et al., 2012). For subsys-
tem SU1, we presented some new features in addi-
tion to the ones suggested in (Dehkharghani et al.,
2012). For subsystem SU2, we combined two sys-
tems (Demiroz et al., 2012) (Gezici et al., 2012).
The detailed descriptions for our subsystems SU1
and SU2 as well as our combined system can be
found in the following sections.
</bodyText>
<sectionHeader confidence="0.994864" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999984571428571">
We built two sentiment analysis systems using su-
pervised learning techniques with labelled tweets for
training. Then, another classifier was trained for
combining the two systems, which is what is sub-
mitted to SemEval-2013 Task 2. The subsystems,
SU1 and SU2, and also the combination method are
explained in the following subsections.
</bodyText>
<subsectionHeader confidence="0.9962">
3.1 Subsystem SU1
</subsectionHeader>
<bodyText confidence="0.956265">
Subsystem SU1 uses subjectivity based features that
are listed in Table 1. These features are divided into
two groups:
</bodyText>
<listItem confidence="0.989043285714286">
• F1 through F8, using domain independent
resources SenticNet (SN) (Cambria et al.,
2012), SentiWordNet (SWN) (Baccianella et
al., 2010) and the NRC Emotion lexicons
(NRC) (Mohammad, 2012),
• F9 through F13 using the seed word list (called
SubjWords).
</listItem>
<bodyText confidence="0.999305166666667">
In the remainder of this subsection, we describe
the features which are grouped according to the lex-
ical resource used.
SentiWordNet In SentiWordNet (Baccianella et
al., 2010), three scores are assigned to each connota-
tion of a word: positivity, negativity and objectivity.
</bodyText>
<page confidence="0.993101">
472
</page>
<bodyText confidence="0.993356">
The summation of these three scores equals to one:
</bodyText>
<equation confidence="0.99805">
Pos(w) + Neg(w) + Obj(w) = 1 (1)
</equation>
<bodyText confidence="0.99998575">
where w stands for a given word; and the three
scores stand for its positivity, negativity and objec-
tivity scores, respectively. Furthermore, we define
the the polarity of a word w as:
</bodyText>
<equation confidence="0.999173">
Pol(w) = Pos(w) − Neg(w) (2)
</equation>
<bodyText confidence="0.9998576">
We also do not do word sense disambiguation
(WSD) because it is an ongoing problem that has not
been completely solved. The average polarity of all
words in a review, r, denoted by AP(r) is computed
as in (3).
</bodyText>
<subsubsectionHeader confidence="0.412834">
wiEr
</subsubsectionHeader>
<bodyText confidence="0.9943355">
where |r |is the number of words in tweet r and
Pol(wz) is the polarity of the word wz as defined
above.
Feature name
</bodyText>
<listItem confidence="0.995803230769231">
F1: Avg. polarity of all words using SWN
F2: Avg. polarity of negative words using SWN
F3: Avg. polarity of positive words using SWN
F4: Avg. polarity of negative words using SN
F5: Avg. polarity of positive words using SN
F6: term frequency of negative words using NRC
F7: term frequency of positive words using NRC
F8: term frequency of swear words
F9: Cumulative frequency of positive SubjWords
F10: Cumulative frequency of negative SubjWords
F11: Proportion of positive to negative SubjWords
F12: Weighted probability of positive SubjWords
F13: Weighted probability of negative SubjWords
</listItem>
<tableCaption confidence="0.9604685">
Table 1: Features extracted for each tweet in subsystem
SU1
</tableCaption>
<bodyText confidence="0.999928529411765">
The first three features (F1, F2, F3) are based on
the average polarity concept (AP). A word w is de-
cided as positive if Pol(w) &gt; 0, and decided as neg-
ative if Pol(w) &lt; 0.
SenticNet SenticNet (Cambria et al., 2012) is a
polarity lexicon that assigns numerical values be-
tween -1 and +1 to a phrase.
Unlike SentiWordNet (Baccianella et al., 2010),
we did not need to do word sense disambiguation
for SenticNet. Two features, F4 and F5 use the aver-
age polarity of negative and positive words extracted
from SenticNet. A term is considered as positive if
its overall polarity score is greater than 0 and is con-
sidered as negative if this score is lower than 0.
NRC Emotion Lexicon The NRC Emotion Lex-
icon (Mohammad, 2012) is similar to SenticNet
in terms of considering different emotions such as
anger and happiness; but it is different from Sentic-
Net because it only assigns a binary value (0 or 1)
to words. Features F6 and F7 use the number of
negative and positive words seen according to this
lexicon.
Feature F8 is an isolated feature from other
groups which is the list of English swear words col-
lected from the Internet. As an indication to negative
sentiment, we counted the number of appearances of
those swear words in tweets and used it as a feature.
Subjective Words (SubjWords) We also use a set
of seed words which is a subset of the seed word list
proposed in (Liu et al., 2005), which we called Sub-
jWords. The filtering of the original set of subjec-
tive words, for a particular domain, is done through
a supervised learning process, where words that are
not seen in any tweet in the training set are elimi-
nated. Specifically, we add a positive seed word to
the positive subset of SubjWords if it has been seen
in at least one positive tweet; and similarly a nega-
tive seed word is added to negative subset if it has
been seen in a negative tweet.
The number of positive and negative words in the
initial set before filtering is 2005 and 4783 respec-
tively. Those numbers decrease to 387 positive and
558 negative words after filtering. Note that this fil-
tering helps us to make the seed word sets domain-
specific, which in turn helps increase the accuracy
of sentiment classification.
The mentioned filtered seed words are used in fea-
tures F9 through F13 in different ways. For F9 and
F10, we compute the cumulative term frequency of
positive and negative seed words for each tweet in
the training set, respectively.
</bodyText>
<equation confidence="0.968516857142857">
F9(r) = � tf(tz7 r) (4)
4EPS
AP(r) = 1 |E Pol(wz) (3)
 |r
473
F10(r) = 1] tf(tZ7 r) (5)
tiENS
</equation>
<bodyText confidence="0.999457666666667">
The feature F11 is the proportion of positive seed
words (the number of occurrences) to the negative
ones in a review (tweet):
</bodyText>
<equation confidence="0.988739">
F11(r) = p � 1n 1 (6)
+
</equation>
<bodyText confidence="0.9997798">
where p and n are the number of positive and nega-
tive seed words, respectively.
Finally features F12 and F13 are the weighted
probabilities of positive and negative words in a re-
view, calculated as follows:
</bodyText>
<equation confidence="0.9999945">
F12(r) = p * (1 − P+(p)) (7)
F13(r) = n * (1 − P_(n)) (8)
</equation>
<bodyText confidence="0.9971465">
where p is the number of positive seed words
in review r and P+(p) is the probability of seeing
p positive words in a review. Similarly, F13(r) is
the weighted probability of negative words in a re-
view r; n is the number of negative seed words in
the review, and P_(n) is the probability of seeing
n negative words in a review. Probabilities P+(p)
and P_(n) are calculated from training set. Table 2
presents the values of P+(p) for n = 1... 5. For
instance, the probability of seeing at least one posi-
tive subjective word in a positive tweet is 0.87, while
seeing three positive words is only 0.47.
</bodyText>
<table confidence="0.992119">
p 1 2 3 4 5
P+(p) 0.87 0.69 0.47 0.17 0.06
</table>
<tableCaption confidence="0.9921095">
Table 2: The probability of seeing p positive words in a
positive tweet.
</tableCaption>
<bodyText confidence="0.9998866">
Classifier The extracted features are fed into a lo-
gistic regression classifier, chosen for its simplicity
and successful use in many problems. We have used
WEKA 3.6 (Hall et al., 2009) implementation for
this classifier, all with default parameters.
</bodyText>
<subsectionHeader confidence="0.995605">
3.2 Subsystem SU2
</subsectionHeader>
<bodyText confidence="0.999592833333333">
Subsystem SU2 uses word-based and sentence-
based features proposed in (Gezici et al., 2012) and
summarized in Table 3. For adapting to the tweet
domain, we also added some new features regarding
smileys.
The features consist of an extensive set of 24 fea-
tures that can be grouped in five categories: (1) basic
features, (2) features based on subjective word oc-
currence statistics, (3) delta-tf-idf weighting of word
polarities, (4) punctuation based features, and (5)
sentence-based features. They are as follows:
Basic Features In this group of features, we ex-
ploit word-based features and compute straightfor-
ward features which were proposed several times be-
fore in the literature (e.g. avg. review polarity and
review purity). Moreover, smileys which are crucial
symbols in Twitter are also included here.
Seed Words Features In the second group of fea-
tures, we have two seed sets as positive and negative
seed words. These seed words are the words that are
obviously positive or negative irrelevant of the con-
text. As seed words features, we make calculations
related to their occurrences in a review to capture
several clues for sentiment determination.
Atf-idf Features This group consists of features
based on the Atf-idf score of a word-sense pair,
indicating the relative occurrence of a word-sense
among positive and negative classes (Demiroz et al.,
2012).
Punctuation-based Features This group contains
the number of question and exclamation marks in the
message, as they may give some information about
the sentiment of a review, especially for the Twitter
domain.
Sentence-based Features In this last group of fea-
tures, we extract features based on sentence type
(e.g. subjective, pure, and non-irrealis) (Taboada et
al., 2011) and sentence position (e.g. first line and
last line) (Zhao et al., 2008). Features include sev-
eral basic ones such as the average polarity of the
first sentence and the average polarity of subjective
or pure sentences. We also compute Atf-idf scores
on sentence level.
Finally, we consider the number of sentences
which may be significant in SMS messages and the
estimated review subjectivity as a feature derived
from sentence-level processing. The review is con-
sidered subjective if it contains at least one subjec-
</bodyText>
<page confidence="0.983818">
474
</page>
<bodyText confidence="0.990808111111111">
tive sentence. In turn, a sentence is subjective if and Pol(w) = { 0 if max(p=, p+, p−) = p=
only if it contains at least one subjective word-sense p+ else if p+ &gt; p−
pair or contains at least one smiley. A word-sense −p− otherwise
pair is subjective if and only if the sum of its posi-
tive and negative polarity taken from SentiWordNet
(Baccianella et al., 2010) is bigger than 0.5 (Zhang
and Zhang, 2006). These features are described in
detail in (Gezici et al., 2012).
Feature name
</bodyText>
<listItem confidence="0.998727083333333">
F1: Average review polarity
F2: Review purity
F3: # of positive smileys
F4: # of negative smileys
F5: Freq. of seed words
F6: Avg. polarity of seed words
F7: Std. of polarities of seed words
F8: Otf-idf weighted avg. polarity of words
F9: Otf-idf scores of words
F10: # of Exclamation marks
F11: # of Question marks
F12: Avg. First Line Polarity
F13: Avg. Last Line Polarity
F14: First Line Purity
F15: Last Line Purity
F16: Avg. pol. of subj. sentences
F17: Avg. pol. of pure sentences
F18: Avg. pol. of non-irrealis sentences
F19: Otf-idf weighted polarity of first line
F20: Otf-idf scores of words in the first line
F21: Otf-idf weighted polarity of last line
F22: Otf-idf scores of words in the last line
F23: Review subjectivity (0 or 1)
F24: Number of sentences in review
</listItem>
<tableCaption confidence="0.970133">
Table 3: Features extracted for each tweet in subsystem
SU2
</tableCaption>
<bodyText confidence="0.999694178571429">
Obtaining Polarities from SentiWordNet For all
the features in subsystem SU2, we use SentiWord-
Net (Baccianella et al., 2010) as a lexicon. Al-
though, we use the same lexicon for our two subsys-
tems, the way we include the lexicon to our subsys-
tems differs. In this subsystem, we obtain the domi-
nant polarity of the word-sense pair from the lexicon
and use the sign for the indication of polarity direc-
tion. The dominant polarity of a word w, denoted by
Pol(w), is calculated as:
where p+, p= and p− are the positive, objective and
negative polarities of a word w, respectively.
After obtaining the dominant polarities of words
from SentiWordNet (Baccianella et al., 2010), we
update these polarities using our domain adaptation
technique (Demiroz et al., 2012). The Otf − idf
scores of words are computed and if there is a dis-
agreement between the Otf − idf and the domi-
nant polarity of a word indicated by the lexicon, then
the polarity of the word is updated. This adaptation
is described in detail in one of our previous works
(Demiroz et al., 2012).
Classifier The extracted features are fed into a
Naive Bayes classifier, also chosen for its simplic-
ity and successful use in many problems. We have
used WEKA 3.6 (Hall et al., 2009) implementation
for this classifier, where the Kernel estimator param-
eter was set to true.
</bodyText>
<subsectionHeader confidence="0.99995">
3.3 Combination of Subsystems
</subsectionHeader>
<bodyText confidence="0.999990388888889">
As we had two independently developed systems
that were only slightly adapted for this competition,
we wanted to apply a sophisticated classifier combi-
nation technique. Rather than averaging the outputs
of the two classifiers, we used the development set
to train a new classifier, to learn how to best combine
the two systems. Note that in this way the combiner
takes into account the different score scales and ac-
curacies of the two sub-systems automatically.
The new classifier takes as features the probabil-
ities assigned by the systems to the three possible
classes (positive, objective, negative) and another
feature which is an estimate of subjectivity of the
tweet or SMS messages. We trained the system us-
ing these 7 features obtained from the development
data for which we had the groundtruth, with the goal
of predicting the actual class label based on the esti-
mates of the two subsystems.
</bodyText>
<page confidence="0.999187">
475
</page>
<sectionHeader confidence="0.998637" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999864">
4.1 Competition Tasks
</subsectionHeader>
<bodyText confidence="0.999935">
There were two tasks in this competition: 1) Task A
where the aim was to determine the sentiment of a
phrase within the message and 2) Task B where the
aim was to obtain the overall sentiment of a mes-
sage. In each task, the classification involves the as-
signment of one of the three sentiment classes, posi-
tive, negative and objective/neutral. There were two
different datasets for each task, namely tweet and
SMS datasets (Manandhar and Yuret, 2013). Due to
the different nature of tweets and SMS and the two
tasks (A and B), we in fact considered this as four
different tasks.
</bodyText>
<subsectionHeader confidence="0.99556">
4.2 Submitted Systems
</subsectionHeader>
<bodyText confidence="0.999963956521739">
Due to time constraints, we mainly worked on
TaskB where we had some prior experience, and
only submitted participated in TaskA for complete-
ness.
As we did not use any outside labelled data
(tweets or SMS), we trained our systems on the
available training data which consisted only of
tweets and submitted them on both tweets and SMS
sets. In fact, we separated part of the training data
as validation set and comparison of the two subsys-
tems.
Since only one system is allowed for each task,
we selected the submitted system from our 3 sys-
tems (SU1, SU2, combined) based on their perfor-
mance on the validation set. The performances of
these systems are summarized in Table 4.
Finally, we re-trained the selected system with the
full training data, to use all available data.
For the implementation, we used C# for subsys-
tem SU1 and Java &amp; Stanford NLP Parser (De Marn-
effe and Manning, 2008) for subsystem SU2 and
WEKA (Hall et al., 2009) for the classification part
for both of the systems.
</bodyText>
<subsectionHeader confidence="0.921111">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.998342222222222">
In order to evaluate and compare the performances
of our two systems, we separated a portion of the
training data as validation set, and kept it separate.
Then we trained each system on the training set and
tested it on the validation set. These test results are
given in Table 4.
We obtained 75.60% accuracy on the validation
set with subsystem SU1 on TaskA twitter using lo-
gistic regression. For the same dataset, we obtained
70.74% accuracy on the validation set with subsys-
tem SU2 using a Naive Bayes classifier.
For TaskB Twitter dataset on the other hand, we
benefited from our combined system in order to get
better results. With this combined system using lo-
gistic regression as a classifier, we achieved 64%
accuracy on the validation set. The accuracies ob-
tained by the individual subsystems on this task was
63.10% by SU1 and 62.92% by SU2.
</bodyText>
<figure confidence="0.919126333333333">
Dataset System Accuracy
TaskA Twitter SU1
SU2 70.74%
SU1
TaskB Twitter SU2
Combined 64.00%
</figure>
<tableCaption confidence="0.991586">
Table 4: Performance of Our Systems on Validation Data
</tableCaption>
<subsectionHeader confidence="0.908688">
4.4 Discussion &amp; Future Work
</subsectionHeader>
<bodyText confidence="0.99992975">
The accuracy of our submitted systems for different
tasks are not very high due to many factors. First of
all, both domains (tweets and SMSs) were new to us
as we had only worked on review polarity estimation
on hotel and movie domains before.
For tweets, the problem is quite difficult due to
especially short message length; misspelled words;
and lack of domain knowledge (e.g. ’Good Girl, Bad
Girl’ does not convey a sentiment, rather it is a stage
play’s name). As for the SMS data, there were no
training data for SMSs, so we could not tune or re-
train our existing systems, either. Finally, for Task
A, we had some difficulty with the phrase index, due
to some ambiguity in the documentation. Nonethe-
less, we thank the organizers for a chance to evaluate
ourselves among others.
This was our first experience with this competi-
tion and with the Twitter and SMS domains. Given
the nature of tweets, we used simple features ex-
tracted from term polarities obtained from domain-
independent lexicons. In the future, we intend to use
more sophisticated algorithms, both in the natural
language processing stage, as well as the machine
learning algorithms.
</bodyText>
<figure confidence="0.924267">
75.60%
63.10%
62.92%
</figure>
<page confidence="0.997882">
476
</page>
<sectionHeader confidence="0.989704" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999570798076923">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proc. of LREC.
Erik Cambria, Catherine Havasi, and Amir Hussain.
2012. Senticnet 2: A semantic and affective re-
source for opinion mining and sentiment analysis. In
G. Michael Youngblood and Philip M. McCarthy, edi-
tors, FLAIRS Conf. AAAI Press.
Marie-Catherine De Marneffe and Christopher D
Manning. 2008. Stanford typed depen-
dencies manual. URL http://nlp. stanford.
edu/software/dependencies manual. pdf.
Rahim Dehkharghani, Berrin Yanikoglu, Dilek Tapucu,
and Yucel Saygin. 2012. Adaptation and use of
subjectivity lexicons for domain dependent sentiment
classification. In Data Mining Workshops (ICDMW),
2012 IEEE 12th International Conf., pages 669–673.
Gulsen Demiroz, Berrin Yanikoglu, Dilek Tapucu, and
Yucel Saygin. 2012. Learning domain-specific polar-
ity lexicons. In Data Mining Workshops (ICDMW),
2012 IEEE 12th International Conf. on, pages 674–
679.
Gizem Gezici, Berrin Yanikoglu, Dilek Tapucu, and
Y¨ucel Saygın. 2012. New features for sentiment anal-
ysis: Do sentences matter? In SDAD 2012 The 1st
International Workshop on Sentiment Discovery from
Affective Data, page 5.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten. 2009.
The weka data mining software: an update. ACM
SIGKDD Explorations Newsletter, 11(1):10–18.
Vasileios Hatzivassiloglou and Janyce M Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proc. of the 18th Conf. on Comp.
Ling.-Volume 1, pages 299–305.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews.
In Proc. of the COLING/ACL Main Conf. Poster Ses-
sions, pages 483–490.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In WWW ’05: Proc. of the 14th Interna-
tional Conf. on World Wide Web.
Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
5(1):1–167.
Suresh Manandhar and Deniz Yuret. 2013. Semeval
tweet competition. In Proc. of the 7th International
Workshop on Semantic Evaluation (SemEval 2013) in
conjunction with the Second Joint Conference on Lex-
ical and Comp.Semantics (*SEM 2013).
Saif Mohammad. 2012. #emotional tweets. In *SEM
2012: The First Joint Conf. on Lexical and Comp. Se-
mantics, pages 246–255. Association for Comp. Ling.
Georgios Paltoglou and Mike Thelwall. 2010. A study of
information retrieval weighting schemes for sentiment
analysis. In Proc. of the 48th Annual Meeting of the
Association for Comp. Ling., pages 1386–1395.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proc. of EMNLP, pages
79–86.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proc. of
the 2003 Conf. on Empirical methods in natural lan-
guage processing, pages 105–112, Morristown, NJ,
USA. Association for Comp. Ling.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-based
methods for sentiment analysis. Comput. Linguist.,
37(2):267–307.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense and
subjectivity. In ACL.
Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce,
Matthew Bell, and Melanie Martin. 2004. Learning
subjective language. Comp. Ling., 30(3):277–308.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opin-
ion clauses. In Proc. of the 19th national Conf. on
Artifical intelligence, AAAI’04, pages 761–767.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Comp. Ling., pages 399–433.
Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang.
2006. Widit in trec-2006 blog track. In Proc. of TREC,
pages 27–31.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. of the 2003 Conf. on Empirical meth-
ods in natural language processing, pages 129–136.
Association for Comp. Ling.
Ethan Zhang and Yi Zhang. 2006. Ucsc on trec 2006
blog opinion mining. In Text Retrieval Conference.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding
redundant features for crfs-based sentence sentiment
classification. In Proc. of the conference on empirical
methods in natural language processing, pages 117–
126. Association for Comp. Ling.
</reference>
<page confidence="0.998413">
477
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.326153">
<title confidence="0.989085">SU-Sentilab : A Classification System for Sentiment Analysis in Twitter</title>
<author confidence="0.534151">Gizem Gezici</author>
<author confidence="0.534151">Rahim Dehkharghani</author>
<author confidence="0.534151">Berrin Yanikoglu</author>
<author confidence="0.534151">Dilek Tapucu</author>
<author confidence="0.534151">Yucel</author>
<affiliation confidence="0.413847">Sabanci</affiliation>
<address confidence="0.909922">Istanbul, Turkey 34956</address>
<abstract confidence="0.994446666666667">Sentiment analysis refers to automatically extracting the sentiment present in a given natural language text. We present our participation to the SemEval2013 competition, in the sentiment analysis of Twitter and SMS messages. Our approach for this task is the combination of two sentiment analysis subsystems which are combined together to build the final system. Both subsystems use supervised learning using features based on various polarity lexicons.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="2220" citStr="Baccianella et al., 2010" startWordPosition="335" endWordPosition="338">on, the polarity of a given word or word sequence in the message was asked. Details are described in (Manandhar and Yuret, 2013). We participated in both of these tasks using a classifier combination consisting of two sub-systems that are based on (Dehkharghani et al., 2012)(Gezici et al., 2012) and adapted to the tweet domain. Both sub-systems use supervised learning in which the system is trained using tweets with known polarities and used to predict the label (polarity) of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are l</context>
<context position="3833" citStr="Baccianella et al., 2010" startWordPosition="587" endWordPosition="590">mance by obtaining the correct word polarities in the given domain (e.g., the word ’small’ has a positive mean471 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 471–477, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics ing in cell phone domain, while it has a negative meaning in hotel domain). On the other hand, establishing a domain-specific lexicon is costly, so many systems use a domain-independent lexicon, such as the SentiWordNet, shortly SWN, (Baccianella et al., 2010) and SenticNet (Cambria et al., 2012). Part of Speech (POS) information is commonly indicated in polarity lexicons, partly to overcome word-sense disambiguity and therefore help achieve a better sentiment classification performance. Alternatively, supervised methods use machine learning techniques to build models or discriminators for the different classes (e.g. positive reviews), using a large corpus. For example, in (Pang et al., 2002) (Yu and Hatzivassiloglou, 2003), the Naive Bayes algorithm is used to separate positive reviews from negative ones. Note that supervised learning techniques c</context>
<context position="7408" citStr="Baccianella et al., 2010" startWordPosition="1146" endWordPosition="1149">tem Description We built two sentiment analysis systems using supervised learning techniques with labelled tweets for training. Then, another classifier was trained for combining the two systems, which is what is submitted to SemEval-2013 Task 2. The subsystems, SU1 and SU2, and also the combination method are explained in the following subsections. 3.1 Subsystem SU1 Subsystem SU1 uses subjectivity based features that are listed in Table 1. These features are divided into two groups: • F1 through F8, using domain independent resources SenticNet (SN) (Cambria et al., 2012), SentiWordNet (SWN) (Baccianella et al., 2010) and the NRC Emotion lexicons (NRC) (Mohammad, 2012), • F9 through F13 using the seed word list (called SubjWords). In the remainder of this subsection, we describe the features which are grouped according to the lexical resource used. SentiWordNet In SentiWordNet (Baccianella et al., 2010), three scores are assigned to each connotation of a word: positivity, negativity and objectivity. 472 The summation of these three scores equals to one: Pos(w) + Neg(w) + Obj(w) = 1 (1) where w stands for a given word; and the three scores stand for its positivity, negativity and objectivity scores, respect</context>
<context position="9429" citStr="Baccianella et al., 2010" startWordPosition="1494" endWordPosition="1497">ency of positive SubjWords F10: Cumulative frequency of negative SubjWords F11: Proportion of positive to negative SubjWords F12: Weighted probability of positive SubjWords F13: Weighted probability of negative SubjWords Table 1: Features extracted for each tweet in subsystem SU1 The first three features (F1, F2, F3) are based on the average polarity concept (AP). A word w is decided as positive if Pol(w) &gt; 0, and decided as negative if Pol(w) &lt; 0. SenticNet SenticNet (Cambria et al., 2012) is a polarity lexicon that assigns numerical values between -1 and +1 to a phrase. Unlike SentiWordNet (Baccianella et al., 2010), we did not need to do word sense disambiguation for SenticNet. Two features, F4 and F5 use the average polarity of negative and positive words extracted from SenticNet. A term is considered as positive if its overall polarity score is greater than 0 and is considered as negative if this score is lower than 0. NRC Emotion Lexicon The NRC Emotion Lexicon (Mohammad, 2012) is similar to SenticNet in terms of considering different emotions such as anger and happiness; but it is different from SenticNet because it only assigns a binary value (0 or 1) to words. Features F6 and F7 use the number of </context>
<context position="15631" citStr="Baccianella et al., 2010" startWordPosition="2578" endWordPosition="2581">ence level. Finally, we consider the number of sentences which may be significant in SMS messages and the estimated review subjectivity as a feature derived from sentence-level processing. The review is considered subjective if it contains at least one subjec474 tive sentence. In turn, a sentence is subjective if and Pol(w) = { 0 if max(p=, p+, p−) = p= only if it contains at least one subjective word-sense p+ else if p+ &gt; p− pair or contains at least one smiley. A word-sense −p− otherwise pair is subjective if and only if the sum of its positive and negative polarity taken from SentiWordNet (Baccianella et al., 2010) is bigger than 0.5 (Zhang and Zhang, 2006). These features are described in detail in (Gezici et al., 2012). Feature name F1: Average review polarity F2: Review purity F3: # of positive smileys F4: # of negative smileys F5: Freq. of seed words F6: Avg. polarity of seed words F7: Std. of polarities of seed words F8: Otf-idf weighted avg. polarity of words F9: Otf-idf scores of words F10: # of Exclamation marks F11: # of Question marks F12: Avg. First Line Polarity F13: Avg. Last Line Polarity F14: First Line Purity F15: Last Line Purity F16: Avg. pol. of subj. sentences F17: Avg. pol. of pure </context>
<context position="17262" citStr="Baccianella et al., 2010" startWordPosition="2861" endWordPosition="2864">ll the features in subsystem SU2, we use SentiWordNet (Baccianella et al., 2010) as a lexicon. Although, we use the same lexicon for our two subsystems, the way we include the lexicon to our subsystems differs. In this subsystem, we obtain the dominant polarity of the word-sense pair from the lexicon and use the sign for the indication of polarity direction. The dominant polarity of a word w, denoted by Pol(w), is calculated as: where p+, p= and p− are the positive, objective and negative polarities of a word w, respectively. After obtaining the dominant polarities of words from SentiWordNet (Baccianella et al., 2010), we update these polarities using our domain adaptation technique (Demiroz et al., 2012). The Otf − idf scores of words are computed and if there is a disagreement between the Otf − idf and the dominant polarity of a word indicated by the lexicon, then the polarity of the word is updated. This adaptation is described in detail in one of our previous works (Demiroz et al., 2012). Classifier The extracted features are fed into a Naive Bayes classifier, also chosen for its simplicity and successful use in many problems. We have used WEKA 3.6 (Hall et al., 2009) implementation for this classifier</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Catherine Havasi</author>
<author>Amir Hussain</author>
</authors>
<title>Senticnet 2: A semantic and affective resource for opinion mining and sentiment analysis.</title>
<date>2012</date>
<editor>In G. Michael Youngblood and Philip M. McCarthy, editors, FLAIRS Conf.</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2254" citStr="Cambria et al., 2012" startWordPosition="340" endWordPosition="343">rd sequence in the message was asked. Details are described in (Manandhar and Yuret, 2013). We participated in both of these tasks using a classifier combination consisting of two sub-systems that are based on (Dehkharghani et al., 2012)(Gezici et al., 2012) and adapted to the tweet domain. Both sub-systems use supervised learning in which the system is trained using tweets with known polarities and used to predict the label (polarity) of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are lexicon-based and supervised method</context>
<context position="3870" citStr="Cambria et al., 2012" startWordPosition="593" endWordPosition="596">ities in the given domain (e.g., the word ’small’ has a positive mean471 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 471–477, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics ing in cell phone domain, while it has a negative meaning in hotel domain). On the other hand, establishing a domain-specific lexicon is costly, so many systems use a domain-independent lexicon, such as the SentiWordNet, shortly SWN, (Baccianella et al., 2010) and SenticNet (Cambria et al., 2012). Part of Speech (POS) information is commonly indicated in polarity lexicons, partly to overcome word-sense disambiguity and therefore help achieve a better sentiment classification performance. Alternatively, supervised methods use machine learning techniques to build models or discriminators for the different classes (e.g. positive reviews), using a large corpus. For example, in (Pang et al., 2002) (Yu and Hatzivassiloglou, 2003), the Naive Bayes algorithm is used to separate positive reviews from negative ones. Note that supervised learning techniques can also use a lexicon in the feature </context>
<context position="7361" citStr="Cambria et al., 2012" startWordPosition="1140" endWordPosition="1143">n be found in the following sections. 3 System Description We built two sentiment analysis systems using supervised learning techniques with labelled tweets for training. Then, another classifier was trained for combining the two systems, which is what is submitted to SemEval-2013 Task 2. The subsystems, SU1 and SU2, and also the combination method are explained in the following subsections. 3.1 Subsystem SU1 Subsystem SU1 uses subjectivity based features that are listed in Table 1. These features are divided into two groups: • F1 through F8, using domain independent resources SenticNet (SN) (Cambria et al., 2012), SentiWordNet (SWN) (Baccianella et al., 2010) and the NRC Emotion lexicons (NRC) (Mohammad, 2012), • F9 through F13 using the seed word list (called SubjWords). In the remainder of this subsection, we describe the features which are grouped according to the lexical resource used. SentiWordNet In SentiWordNet (Baccianella et al., 2010), three scores are assigned to each connotation of a word: positivity, negativity and objectivity. 472 The summation of these three scores equals to one: Pos(w) + Neg(w) + Obj(w) = 1 (1) where w stands for a given word; and the three scores stand for its positiv</context>
<context position="9299" citStr="Cambria et al., 2012" startWordPosition="1472" endWordPosition="1475">negative words using NRC F7: term frequency of positive words using NRC F8: term frequency of swear words F9: Cumulative frequency of positive SubjWords F10: Cumulative frequency of negative SubjWords F11: Proportion of positive to negative SubjWords F12: Weighted probability of positive SubjWords F13: Weighted probability of negative SubjWords Table 1: Features extracted for each tweet in subsystem SU1 The first three features (F1, F2, F3) are based on the average polarity concept (AP). A word w is decided as positive if Pol(w) &gt; 0, and decided as negative if Pol(w) &lt; 0. SenticNet SenticNet (Cambria et al., 2012) is a polarity lexicon that assigns numerical values between -1 and +1 to a phrase. Unlike SentiWordNet (Baccianella et al., 2010), we did not need to do word sense disambiguation for SenticNet. Two features, F4 and F5 use the average polarity of negative and positive words extracted from SenticNet. A term is considered as positive if its overall polarity score is greater than 0 and is considered as negative if this score is lower than 0. NRC Emotion Lexicon The NRC Emotion Lexicon (Mohammad, 2012) is similar to SenticNet in terms of considering different emotions such as anger and happiness; </context>
</contexts>
<marker>Cambria, Havasi, Hussain, 2012</marker>
<rawString>Erik Cambria, Catherine Havasi, and Amir Hussain. 2012. Senticnet 2: A semantic and affective resource for opinion mining and sentiment analysis. In G. Michael Youngblood and Philip M. McCarthy, editors, FLAIRS Conf. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford typed dependencies manual. URL http://nlp. stanford. edu/software/dependencies manual. pdf.</note>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008. Stanford typed dependencies manual. URL http://nlp. stanford. edu/software/dependencies manual. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahim Dehkharghani</author>
<author>Berrin Yanikoglu</author>
<author>Dilek Tapucu</author>
<author>Yucel Saygin</author>
</authors>
<title>Adaptation and use of subjectivity lexicons for domain dependent sentiment classification.</title>
<date>2012</date>
<booktitle>In Data Mining Workshops (ICDMW), 2012 IEEE 12th International Conf.,</booktitle>
<pages>669--673</pages>
<contexts>
<context position="1870" citStr="Dehkharghani et al., 2012" startWordPosition="279" endWordPosition="282"> In this paper we describe our sentiment analysis system identified as SU-Sentilab in the SemEval 2013 competition, Task 2: Sentiment analysis in Twitter. One or the problems in this competition was to label a given tweet or sms message with the correct sentiment orientation, as positive, negative or neutral. In the second task of the same competition, the polarity of a given word or word sequence in the message was asked. Details are described in (Manandhar and Yuret, 2013). We participated in both of these tasks using a classifier combination consisting of two sub-systems that are based on (Dehkharghani et al., 2012)(Gezici et al., 2012) and adapted to the tweet domain. Both sub-systems use supervised learning in which the system is trained using tweets with known polarities and used to predict the label (polarity) of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented </context>
<context position="6420" citStr="Dehkharghani et al., 2012" startWordPosition="986" endWordPosition="989">is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word disambiguation and subjectivity, in order to obtain sufficient information for a better sentiment classification (Wiebe and Mihalcea, 2006). A recent survey describing the fundamental approaches can be found in (Liu, 2012). Two sub-systems combined to form the SUSentilab submission are slightly modified from our previous work (Gezici et al., 2012) (Dehkharghani et al., 2012) (Demiroz et al., 2012). For subsystem SU1, we presented some new features in addition to the ones suggested in (Dehkharghani et al., 2012). For subsystem SU2, we combined two systems (Demiroz et al., 2012) (Gezici et al., 2012). The detailed descriptions for our subsystems SU1 and SU2 as well as our combined system can be found in the following sections. 3 System Description We built two sentiment analysis systems using supervised learning techniques with labelled tweets for training. Then, another classifier was trained for combining the two systems, which is what is submitted to SemEval-201</context>
</contexts>
<marker>Dehkharghani, Yanikoglu, Tapucu, Saygin, 2012</marker>
<rawString>Rahim Dehkharghani, Berrin Yanikoglu, Dilek Tapucu, and Yucel Saygin. 2012. Adaptation and use of subjectivity lexicons for domain dependent sentiment classification. In Data Mining Workshops (ICDMW), 2012 IEEE 12th International Conf., pages 669–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gulsen Demiroz</author>
<author>Berrin Yanikoglu</author>
<author>Dilek Tapucu</author>
<author>Yucel Saygin</author>
</authors>
<title>Learning domain-specific polarity lexicons.</title>
<date>2012</date>
<booktitle>In Data Mining Workshops (ICDMW), 2012 IEEE 12th International Conf. on,</booktitle>
<pages>674--679</pages>
<contexts>
<context position="6443" citStr="Demiroz et al., 2012" startWordPosition="990" endWordPosition="993">e of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word disambiguation and subjectivity, in order to obtain sufficient information for a better sentiment classification (Wiebe and Mihalcea, 2006). A recent survey describing the fundamental approaches can be found in (Liu, 2012). Two sub-systems combined to form the SUSentilab submission are slightly modified from our previous work (Gezici et al., 2012) (Dehkharghani et al., 2012) (Demiroz et al., 2012). For subsystem SU1, we presented some new features in addition to the ones suggested in (Dehkharghani et al., 2012). For subsystem SU2, we combined two systems (Demiroz et al., 2012) (Gezici et al., 2012). The detailed descriptions for our subsystems SU1 and SU2 as well as our combined system can be found in the following sections. 3 System Description We built two sentiment analysis systems using supervised learning techniques with labelled tweets for training. Then, another classifier was trained for combining the two systems, which is what is submitted to SemEval-2013 Task 2. The subsystem</context>
<context position="14372" citStr="Demiroz et al., 2012" startWordPosition="2367" endWordPosition="2370">ys which are crucial symbols in Twitter are also included here. Seed Words Features In the second group of features, we have two seed sets as positive and negative seed words. These seed words are the words that are obviously positive or negative irrelevant of the context. As seed words features, we make calculations related to their occurrences in a review to capture several clues for sentiment determination. Atf-idf Features This group consists of features based on the Atf-idf score of a word-sense pair, indicating the relative occurrence of a word-sense among positive and negative classes (Demiroz et al., 2012). Punctuation-based Features This group contains the number of question and exclamation marks in the message, as they may give some information about the sentiment of a review, especially for the Twitter domain. Sentence-based Features In this last group of features, we extract features based on sentence type (e.g. subjective, pure, and non-irrealis) (Taboada et al., 2011) and sentence position (e.g. first line and last line) (Zhao et al., 2008). Features include several basic ones such as the average polarity of the first sentence and the average polarity of subjective or pure sentences. We a</context>
<context position="17351" citStr="Demiroz et al., 2012" startWordPosition="2874" endWordPosition="2877">. Although, we use the same lexicon for our two subsystems, the way we include the lexicon to our subsystems differs. In this subsystem, we obtain the dominant polarity of the word-sense pair from the lexicon and use the sign for the indication of polarity direction. The dominant polarity of a word w, denoted by Pol(w), is calculated as: where p+, p= and p− are the positive, objective and negative polarities of a word w, respectively. After obtaining the dominant polarities of words from SentiWordNet (Baccianella et al., 2010), we update these polarities using our domain adaptation technique (Demiroz et al., 2012). The Otf − idf scores of words are computed and if there is a disagreement between the Otf − idf and the dominant polarity of a word indicated by the lexicon, then the polarity of the word is updated. This adaptation is described in detail in one of our previous works (Demiroz et al., 2012). Classifier The extracted features are fed into a Naive Bayes classifier, also chosen for its simplicity and successful use in many problems. We have used WEKA 3.6 (Hall et al., 2009) implementation for this classifier, where the Kernel estimator parameter was set to true. 3.3 Combination of Subsystems As </context>
</contexts>
<marker>Demiroz, Yanikoglu, Tapucu, Saygin, 2012</marker>
<rawString>Gulsen Demiroz, Berrin Yanikoglu, Dilek Tapucu, and Yucel Saygin. 2012. Learning domain-specific polarity lexicons. In Data Mining Workshops (ICDMW), 2012 IEEE 12th International Conf. on, pages 674– 679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gizem Gezici</author>
<author>Berrin Yanikoglu</author>
<author>Dilek Tapucu</author>
<author>Y¨ucel Saygın</author>
</authors>
<title>New features for sentiment analysis: Do sentences matter?</title>
<date>2012</date>
<booktitle>In SDAD 2012 The 1st International Workshop on Sentiment Discovery from Affective Data,</booktitle>
<pages>5</pages>
<contexts>
<context position="1891" citStr="Gezici et al., 2012" startWordPosition="282" endWordPosition="285">our sentiment analysis system identified as SU-Sentilab in the SemEval 2013 competition, Task 2: Sentiment analysis in Twitter. One or the problems in this competition was to label a given tweet or sms message with the correct sentiment orientation, as positive, negative or neutral. In the second task of the same competition, the polarity of a given word or word sequence in the message was asked. Details are described in (Manandhar and Yuret, 2013). We participated in both of these tasks using a classifier combination consisting of two sub-systems that are based on (Dehkharghani et al., 2012)(Gezici et al., 2012) and adapted to the tweet domain. Both sub-systems use supervised learning in which the system is trained using tweets with known polarities and used to predict the label (polarity) of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the pro</context>
<context position="6392" citStr="Gezici et al., 2012" startWordPosition="982" endWordPosition="985"> the opinion strength is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word disambiguation and subjectivity, in order to obtain sufficient information for a better sentiment classification (Wiebe and Mihalcea, 2006). A recent survey describing the fundamental approaches can be found in (Liu, 2012). Two sub-systems combined to form the SUSentilab submission are slightly modified from our previous work (Gezici et al., 2012) (Dehkharghani et al., 2012) (Demiroz et al., 2012). For subsystem SU1, we presented some new features in addition to the ones suggested in (Dehkharghani et al., 2012). For subsystem SU2, we combined two systems (Demiroz et al., 2012) (Gezici et al., 2012). The detailed descriptions for our subsystems SU1 and SU2 as well as our combined system can be found in the following sections. 3 System Description We built two sentiment analysis systems using supervised learning techniques with labelled tweets for training. Then, another classifier was trained for combining the two systems, which is what</context>
<context position="13097" citStr="Gezici et al., 2012" startWordPosition="2163" endWordPosition="2166">ce, the probability of seeing at least one positive subjective word in a positive tweet is 0.87, while seeing three positive words is only 0.47. p 1 2 3 4 5 P+(p) 0.87 0.69 0.47 0.17 0.06 Table 2: The probability of seeing p positive words in a positive tweet. Classifier The extracted features are fed into a logistic regression classifier, chosen for its simplicity and successful use in many problems. We have used WEKA 3.6 (Hall et al., 2009) implementation for this classifier, all with default parameters. 3.2 Subsystem SU2 Subsystem SU2 uses word-based and sentencebased features proposed in (Gezici et al., 2012) and summarized in Table 3. For adapting to the tweet domain, we also added some new features regarding smileys. The features consist of an extensive set of 24 features that can be grouped in five categories: (1) basic features, (2) features based on subjective word occurrence statistics, (3) delta-tf-idf weighting of word polarities, (4) punctuation based features, and (5) sentence-based features. They are as follows: Basic Features In this group of features, we exploit word-based features and compute straightforward features which were proposed several times before in the literature (e.g. av</context>
<context position="15739" citStr="Gezici et al., 2012" startWordPosition="2597" endWordPosition="2600">ed review subjectivity as a feature derived from sentence-level processing. The review is considered subjective if it contains at least one subjec474 tive sentence. In turn, a sentence is subjective if and Pol(w) = { 0 if max(p=, p+, p−) = p= only if it contains at least one subjective word-sense p+ else if p+ &gt; p− pair or contains at least one smiley. A word-sense −p− otherwise pair is subjective if and only if the sum of its positive and negative polarity taken from SentiWordNet (Baccianella et al., 2010) is bigger than 0.5 (Zhang and Zhang, 2006). These features are described in detail in (Gezici et al., 2012). Feature name F1: Average review polarity F2: Review purity F3: # of positive smileys F4: # of negative smileys F5: Freq. of seed words F6: Avg. polarity of seed words F7: Std. of polarities of seed words F8: Otf-idf weighted avg. polarity of words F9: Otf-idf scores of words F10: # of Exclamation marks F11: # of Question marks F12: Avg. First Line Polarity F13: Avg. Last Line Polarity F14: First Line Purity F15: Last Line Purity F16: Avg. pol. of subj. sentences F17: Avg. pol. of pure sentences F18: Avg. pol. of non-irrealis sentences F19: Otf-idf weighted polarity of first line F20: Otf-idf</context>
</contexts>
<marker>Gezici, Yanikoglu, Tapucu, Saygın, 2012</marker>
<rawString>Gizem Gezici, Berrin Yanikoglu, Dilek Tapucu, and Y¨ucel Saygın. 2012. New features for sentiment analysis: Do sentences matter? In SDAD 2012 The 1st International Workshop on Sentiment Discovery from Affective Data, page 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12923" citStr="Hall et al., 2009" startWordPosition="2138" endWordPosition="2141">lity of seeing n negative words in a review. Probabilities P+(p) and P_(n) are calculated from training set. Table 2 presents the values of P+(p) for n = 1... 5. For instance, the probability of seeing at least one positive subjective word in a positive tweet is 0.87, while seeing three positive words is only 0.47. p 1 2 3 4 5 P+(p) 0.87 0.69 0.47 0.17 0.06 Table 2: The probability of seeing p positive words in a positive tweet. Classifier The extracted features are fed into a logistic regression classifier, chosen for its simplicity and successful use in many problems. We have used WEKA 3.6 (Hall et al., 2009) implementation for this classifier, all with default parameters. 3.2 Subsystem SU2 Subsystem SU2 uses word-based and sentencebased features proposed in (Gezici et al., 2012) and summarized in Table 3. For adapting to the tweet domain, we also added some new features regarding smileys. The features consist of an extensive set of 24 features that can be grouped in five categories: (1) basic features, (2) features based on subjective word occurrence statistics, (3) delta-tf-idf weighting of word polarities, (4) punctuation based features, and (5) sentence-based features. They are as follows: Bas</context>
<context position="17827" citStr="Hall et al., 2009" startWordPosition="2963" endWordPosition="2966">of words from SentiWordNet (Baccianella et al., 2010), we update these polarities using our domain adaptation technique (Demiroz et al., 2012). The Otf − idf scores of words are computed and if there is a disagreement between the Otf − idf and the dominant polarity of a word indicated by the lexicon, then the polarity of the word is updated. This adaptation is described in detail in one of our previous works (Demiroz et al., 2012). Classifier The extracted features are fed into a Naive Bayes classifier, also chosen for its simplicity and successful use in many problems. We have used WEKA 3.6 (Hall et al., 2009) implementation for this classifier, where the Kernel estimator parameter was set to true. 3.3 Combination of Subsystems As we had two independently developed systems that were only slightly adapted for this competition, we wanted to apply a sophisticated classifier combination technique. Rather than averaging the outputs of the two classifiers, we used the development set to train a new classifier, to learn how to best combine the two systems. Note that in this way the combiner takes into account the different score scales and accuracies of the two sub-systems automatically. The new classifie</context>
<context position="20426" citStr="Hall et al., 2009" startWordPosition="3409" endWordPosition="3412">tweets and SMS sets. In fact, we separated part of the training data as validation set and comparison of the two subsystems. Since only one system is allowed for each task, we selected the submitted system from our 3 systems (SU1, SU2, combined) based on their performance on the validation set. The performances of these systems are summarized in Table 4. Finally, we re-trained the selected system with the full training data, to use all available data. For the implementation, we used C# for subsystem SU1 and Java &amp; Stanford NLP Parser (De Marneffe and Manning, 2008) for subsystem SU2 and WEKA (Hall et al., 2009) for the classification part for both of the systems. 4.3 Results In order to evaluate and compare the performances of our two systems, we separated a portion of the training data as validation set, and kept it separate. Then we trained each system on the training set and tested it on the validation set. These test results are given in Table 4. We obtained 75.60% accuracy on the validation set with subsystem SU1 on TaskA twitter using logistic regression. For the same dataset, we obtained 70.74% accuracy on the validation set with subsystem SU2 using a Naive Bayes classifier. For TaskB Twitter</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th Conf. on Comp. Ling.-Volume 1,</booktitle>
<pages>299--305</pages>
<contexts>
<context position="4940" citStr="Hatzivassiloglou and Wiebe, 2000" startWordPosition="754" endWordPosition="757"> the Naive Bayes algorithm is used to separate positive reviews from negative ones. Note that supervised learning techniques can also use a lexicon in the feature extraction stage. They also generally perform better compared to lexicon-based approaches; however collecting a large training data may be an issue. In estimating the sentiment of a given natural language text, many issues are considered. For instance one important problem is determining the subjectivity of a given sentence. In an early study, the effects of adjective orientation and gradability on sentence subjectivity was studied (Hatzivassiloglou and Wiebe, 2000). Wiebe et al. (Wiebe et al., 2004) presents a broad survey on subjectivity recognition and the key elements that may have an impact on it. In estimating the sentiment polarity, the use of higher-order n-grams is also studied. Pang et. al report results where unigrams work better than bigrams for sentiment classification on a movie dataset (Pang et al., 2002). Similarly, occurrence of rare words (Yang et al., 2006) or the position of words in a text are examined for usefulness (Kim and Hovy, 2006)(Pang et al., 2002). In connection with the occurrences of rare words, different variations of del</context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Vasileios Hatzivassiloglou and Janyce M Wiebe. 2000. Effects of adjective orientation and gradability on sentence subjectivity. In Proc. of the 18th Conf. on Comp. Ling.-Volume 1, pages 299–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic identification of pro and con reasons in online reviews.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING/ACL Main Conf. Poster Sessions,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="5442" citStr="Kim and Hovy, 2006" startWordPosition="840" endWordPosition="843">ects of adjective orientation and gradability on sentence subjectivity was studied (Hatzivassiloglou and Wiebe, 2000). Wiebe et al. (Wiebe et al., 2004) presents a broad survey on subjectivity recognition and the key elements that may have an impact on it. In estimating the sentiment polarity, the use of higher-order n-grams is also studied. Pang et. al report results where unigrams work better than bigrams for sentiment classification on a movie dataset (Pang et al., 2002). Similarly, occurrence of rare words (Yang et al., 2006) or the position of words in a text are examined for usefulness (Kim and Hovy, 2006)(Pang et al., 2002). In connection with the occurrences of rare words, different variations of delta tf*idf scores of words, indicating the difference in occurrences of words in different classes (positive or negative reviews), have been suggested (Paltoglou and Thelwall, 2010). In addition to sentiment classification, obtaining the opinion strength is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Automatic identification of pro and con reasons in online reviews. In Proc. of the COLING/ACL Main Conf. Poster Sessions, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In WWW ’05: Proc. of the 14th International Conf. on World Wide Web.</booktitle>
<contexts>
<context position="2362" citStr="Liu et al., 2005" startWordPosition="360" endWordPosition="363">oth of these tasks using a classifier combination consisting of two sub-systems that are based on (Dehkharghani et al., 2012)(Gezici et al., 2012) and adapted to the tweet domain. Both sub-systems use supervised learning in which the system is trained using tweets with known polarities and used to predict the label (polarity) of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are lexicon-based and supervised methods. The lexicon-based technique adopts the idea of determining the review sentiment by obtaining word polarit</context>
<context position="10476" citStr="Liu et al., 2005" startWordPosition="1684" endWordPosition="1687">t emotions such as anger and happiness; but it is different from SenticNet because it only assigns a binary value (0 or 1) to words. Features F6 and F7 use the number of negative and positive words seen according to this lexicon. Feature F8 is an isolated feature from other groups which is the list of English swear words collected from the Internet. As an indication to negative sentiment, we counted the number of appearances of those swear words in tweets and used it as a feature. Subjective Words (SubjWords) We also use a set of seed words which is a subset of the seed word list proposed in (Liu et al., 2005), which we called SubjWords. The filtering of the original set of subjective words, for a particular domain, is done through a supervised learning process, where words that are not seen in any tweet in the training set are eliminated. Specifically, we add a positive seed word to the positive subset of SubjWords if it has been seen in at least one positive tweet; and similarly a negative seed word is added to negative subset if it has been seen in a negative tweet. The number of positive and negative words in the initial set before filtering is 2005 and 4783 respectively. Those numbers decrease</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In WWW ’05: Proc. of the 14th International Conf. on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="6265" citStr="Liu, 2012" startWordPosition="964" endWordPosition="965">tive reviews), have been suggested (Paltoglou and Thelwall, 2010). In addition to sentiment classification, obtaining the opinion strength is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word disambiguation and subjectivity, in order to obtain sufficient information for a better sentiment classification (Wiebe and Mihalcea, 2006). A recent survey describing the fundamental approaches can be found in (Liu, 2012). Two sub-systems combined to form the SUSentilab submission are slightly modified from our previous work (Gezici et al., 2012) (Dehkharghani et al., 2012) (Demiroz et al., 2012). For subsystem SU1, we presented some new features in addition to the ones suggested in (Dehkharghani et al., 2012). For subsystem SU2, we combined two systems (Demiroz et al., 2012) (Gezici et al., 2012). The detailed descriptions for our subsystems SU1 and SU2 as well as our combined system can be found in the following sections. 3 System Description We built two sentiment analysis systems using supervised learning </context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval tweet competition.</title>
<date>2013</date>
<booktitle>In Proc. of the 7th International Workshop on Semantic Evaluation (SemEval 2013) in conjunction with the Second Joint Conference on Lexical and Comp.Semantics (*SEM</booktitle>
<contexts>
<context position="1723" citStr="Manandhar and Yuret, 2013" startWordPosition="255" endWordPosition="259">n inside a longer text. Automatic sentiment analysis of text collected from social media makes it possible to quantitatively analyze this feedback. In this paper we describe our sentiment analysis system identified as SU-Sentilab in the SemEval 2013 competition, Task 2: Sentiment analysis in Twitter. One or the problems in this competition was to label a given tweet or sms message with the correct sentiment orientation, as positive, negative or neutral. In the second task of the same competition, the polarity of a given word or word sequence in the message was asked. Details are described in (Manandhar and Yuret, 2013). We participated in both of these tasks using a classifier combination consisting of two sub-systems that are based on (Dehkharghani et al., 2012)(Gezici et al., 2012) and adapted to the tweet domain. Both sub-systems use supervised learning in which the system is trained using tweets with known polarities and used to predict the label (polarity) of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and</context>
<context position="19338" citStr="Manandhar and Yuret, 2013" startWordPosition="3215" endWordPosition="3218"> data for which we had the groundtruth, with the goal of predicting the actual class label based on the estimates of the two subsystems. 475 4 Evaluation 4.1 Competition Tasks There were two tasks in this competition: 1) Task A where the aim was to determine the sentiment of a phrase within the message and 2) Task B where the aim was to obtain the overall sentiment of a message. In each task, the classification involves the assignment of one of the three sentiment classes, positive, negative and objective/neutral. There were two different datasets for each task, namely tweet and SMS datasets (Manandhar and Yuret, 2013). Due to the different nature of tweets and SMS and the two tasks (A and B), we in fact considered this as four different tasks. 4.2 Submitted Systems Due to time constraints, we mainly worked on TaskB where we had some prior experience, and only submitted participated in TaskA for completeness. As we did not use any outside labelled data (tweets or SMS), we trained our systems on the available training data which consisted only of tweets and submitted them on both tweets and SMS sets. In fact, we separated part of the training data as validation set and comparison of the two subsystems. Since</context>
</contexts>
<marker>Manandhar, Yuret, 2013</marker>
<rawString>Suresh Manandhar and Deniz Yuret. 2013. Semeval tweet competition. In Proc. of the 7th International Workshop on Semantic Evaluation (SemEval 2013) in conjunction with the Second Joint Conference on Lexical and Comp.Semantics (*SEM 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>emotional tweets.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conf. on Lexical and Comp. Semantics,</booktitle>
<pages>246--255</pages>
<publisher>Association for Comp. Ling.</publisher>
<contexts>
<context position="2295" citStr="Mohammad, 2012" startWordPosition="348" endWordPosition="349">re described in (Manandhar and Yuret, 2013). We participated in both of these tasks using a classifier combination consisting of two sub-systems that are based on (Dehkharghani et al., 2012)(Gezici et al., 2012) and adapted to the tweet domain. Both sub-systems use supervised learning in which the system is trained using tweets with known polarities and used to predict the label (polarity) of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are lexicon-based and supervised methods. The lexicon-based technique adopts the</context>
<context position="7460" citStr="Mohammad, 2012" startWordPosition="1156" endWordPosition="1157">supervised learning techniques with labelled tweets for training. Then, another classifier was trained for combining the two systems, which is what is submitted to SemEval-2013 Task 2. The subsystems, SU1 and SU2, and also the combination method are explained in the following subsections. 3.1 Subsystem SU1 Subsystem SU1 uses subjectivity based features that are listed in Table 1. These features are divided into two groups: • F1 through F8, using domain independent resources SenticNet (SN) (Cambria et al., 2012), SentiWordNet (SWN) (Baccianella et al., 2010) and the NRC Emotion lexicons (NRC) (Mohammad, 2012), • F9 through F13 using the seed word list (called SubjWords). In the remainder of this subsection, we describe the features which are grouped according to the lexical resource used. SentiWordNet In SentiWordNet (Baccianella et al., 2010), three scores are assigned to each connotation of a word: positivity, negativity and objectivity. 472 The summation of these three scores equals to one: Pos(w) + Neg(w) + Obj(w) = 1 (1) where w stands for a given word; and the three scores stand for its positivity, negativity and objectivity scores, respectively. Furthermore, we define the the polarity of a </context>
<context position="9802" citStr="Mohammad, 2012" startWordPosition="1563" endWordPosition="1564">ded as positive if Pol(w) &gt; 0, and decided as negative if Pol(w) &lt; 0. SenticNet SenticNet (Cambria et al., 2012) is a polarity lexicon that assigns numerical values between -1 and +1 to a phrase. Unlike SentiWordNet (Baccianella et al., 2010), we did not need to do word sense disambiguation for SenticNet. Two features, F4 and F5 use the average polarity of negative and positive words extracted from SenticNet. A term is considered as positive if its overall polarity score is greater than 0 and is considered as negative if this score is lower than 0. NRC Emotion Lexicon The NRC Emotion Lexicon (Mohammad, 2012) is similar to SenticNet in terms of considering different emotions such as anger and happiness; but it is different from SenticNet because it only assigns a binary value (0 or 1) to words. Features F6 and F7 use the number of negative and positive words seen according to this lexicon. Feature F8 is an isolated feature from other groups which is the list of English swear words collected from the Internet. As an indication to negative sentiment, we counted the number of appearances of those swear words in tweets and used it as a feature. Subjective Words (SubjWords) We also use a set of seed wo</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>Saif Mohammad. 2012. #emotional tweets. In *SEM 2012: The First Joint Conf. on Lexical and Comp. Semantics, pages 246–255. Association for Comp. Ling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Paltoglou</author>
<author>Mike Thelwall</author>
</authors>
<title>A study of information retrieval weighting schemes for sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proc. of the 48th Annual Meeting of the Association for Comp. Ling.,</booktitle>
<pages>1386--1395</pages>
<contexts>
<context position="5720" citStr="Paltoglou and Thelwall, 2010" startWordPosition="881" endWordPosition="884">ng the sentiment polarity, the use of higher-order n-grams is also studied. Pang et. al report results where unigrams work better than bigrams for sentiment classification on a movie dataset (Pang et al., 2002). Similarly, occurrence of rare words (Yang et al., 2006) or the position of words in a text are examined for usefulness (Kim and Hovy, 2006)(Pang et al., 2002). In connection with the occurrences of rare words, different variations of delta tf*idf scores of words, indicating the difference in occurrences of words in different classes (positive or negative reviews), have been suggested (Paltoglou and Thelwall, 2010). In addition to sentiment classification, obtaining the opinion strength is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word disambiguation and subjectivity, in order to obtain sufficient information for a better sentiment classification (Wiebe and Mihalcea, 2006). A recent survey describing the fundamental approaches can be found in (Liu, 2012). Two sub-systems combined to form the SUSentilab submi</context>
</contexts>
<marker>Paltoglou, Thelwall, 2010</marker>
<rawString>Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment analysis. In Proc. of the 48th Annual Meeting of the Association for Comp. Ling., pages 1386–1395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="2762" citStr="Pang and Lee, 2008" startWordPosition="430" endWordPosition="433"> well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are lexicon-based and supervised methods. The lexicon-based technique adopts the idea of determining the review sentiment by obtaining word polarities from a lexicon, such as the SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012). This lexicon can be domain-independent or domain-specific. One can use a domain-specific lexicon whenever available, to get a better performance by obtaining the correct word polarities in the given domain (e.g., the word ’small’ has a positive mean471 Second Joint Conference on Lexical and C</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="4274" citStr="Pang et al., 2002" startWordPosition="650" endWordPosition="653">other hand, establishing a domain-specific lexicon is costly, so many systems use a domain-independent lexicon, such as the SentiWordNet, shortly SWN, (Baccianella et al., 2010) and SenticNet (Cambria et al., 2012). Part of Speech (POS) information is commonly indicated in polarity lexicons, partly to overcome word-sense disambiguity and therefore help achieve a better sentiment classification performance. Alternatively, supervised methods use machine learning techniques to build models or discriminators for the different classes (e.g. positive reviews), using a large corpus. For example, in (Pang et al., 2002) (Yu and Hatzivassiloglou, 2003), the Naive Bayes algorithm is used to separate positive reviews from negative ones. Note that supervised learning techniques can also use a lexicon in the feature extraction stage. They also generally perform better compared to lexicon-based approaches; however collecting a large training data may be an issue. In estimating the sentiment of a given natural language text, many issues are considered. For instance one important problem is determining the subjectivity of a given sentence. In an early study, the effects of adjective orientation and gradability on se</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proc. of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proc. of the 2003 Conf. on Empirical methods in natural language processing,</booktitle>
<pages>105--112</pages>
<publisher>Association for Comp. Ling.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2696" citStr="Riloff and Wiebe, 2003" startWordPosition="418" endWordPosition="421">of tweets in the test set. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are lexicon-based and supervised methods. The lexicon-based technique adopts the idea of determining the review sentiment by obtaining word polarities from a lexicon, such as the SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012). This lexicon can be domain-independent or domain-specific. One can use a domain-specific lexicon whenever available, to get a better performance by obtaining the correct word polarities in the given domain (e.g., the word ’smal</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proc. of the 2003 Conf. on Empirical methods in natural language processing, pages 105–112, Morristown, NJ, USA. Association for Comp. Ling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="2741" citStr="Taboada et al., 2011" startWordPosition="426" endWordPosition="429">tures that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are lexicon-based and supervised methods. The lexicon-based technique adopts the idea of determining the review sentiment by obtaining word polarities from a lexicon, such as the SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012). This lexicon can be domain-independent or domain-specific. One can use a domain-specific lexicon whenever available, to get a better performance by obtaining the correct word polarities in the given domain (e.g., the word ’small’ has a positive mean471 Second Joint Confer</context>
<context position="14747" citStr="Taboada et al., 2011" startWordPosition="2424" endWordPosition="2427">ral clues for sentiment determination. Atf-idf Features This group consists of features based on the Atf-idf score of a word-sense pair, indicating the relative occurrence of a word-sense among positive and negative classes (Demiroz et al., 2012). Punctuation-based Features This group contains the number of question and exclamation marks in the message, as they may give some information about the sentiment of a review, especially for the Twitter domain. Sentence-based Features In this last group of features, we extract features based on sentence type (e.g. subjective, pure, and non-irrealis) (Taboada et al., 2011) and sentence position (e.g. first line and last line) (Zhao et al., 2008). Features include several basic ones such as the average polarity of the first sentence and the average polarity of subjective or pure sentences. We also compute Atf-idf scores on sentence level. Finally, we consider the number of sentences which may be significant in SMS messages and the estimated review subjectivity as a feature derived from sentence-level processing. The review is considered subjective if it contains at least one subjec474 tive sentence. In turn, a sentence is subjective if and Pol(w) = { 0 if max(p=</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Comput. Linguist., 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6182" citStr="Wiebe and Mihalcea, 2006" startWordPosition="949" endWordPosition="952">of words, indicating the difference in occurrences of words in different classes (positive or negative reviews), have been suggested (Paltoglou and Thelwall, 2010). In addition to sentiment classification, obtaining the opinion strength is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word disambiguation and subjectivity, in order to obtain sufficient information for a better sentiment classification (Wiebe and Mihalcea, 2006). A recent survey describing the fundamental approaches can be found in (Liu, 2012). Two sub-systems combined to form the SUSentilab submission are slightly modified from our previous work (Gezici et al., 2012) (Dehkharghani et al., 2012) (Demiroz et al., 2012). For subsystem SU1, we presented some new features in addition to the ones suggested in (Dehkharghani et al., 2012). For subsystem SU2, we combined two systems (Demiroz et al., 2012) (Gezici et al., 2012). The detailed descriptions for our subsystems SU1 and SU2 as well as our combined system can be found in the following sections. 3 Sy</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Janyce Wiebe and Rada Mihalcea. 2006. Word sense and subjectivity. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca F Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Comp. Ling.,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="4975" citStr="Wiebe et al., 2004" startWordPosition="761" endWordPosition="764">sitive reviews from negative ones. Note that supervised learning techniques can also use a lexicon in the feature extraction stage. They also generally perform better compared to lexicon-based approaches; however collecting a large training data may be an issue. In estimating the sentiment of a given natural language text, many issues are considered. For instance one important problem is determining the subjectivity of a given sentence. In an early study, the effects of adjective orientation and gradability on sentence subjectivity was studied (Hatzivassiloglou and Wiebe, 2000). Wiebe et al. (Wiebe et al., 2004) presents a broad survey on subjectivity recognition and the key elements that may have an impact on it. In estimating the sentiment polarity, the use of higher-order n-grams is also studied. Pang et. al report results where unigrams work better than bigrams for sentiment classification on a movie dataset (Pang et al., 2002). Similarly, occurrence of rare words (Yang et al., 2006) or the position of words in a text are examined for usefulness (Kim and Hovy, 2006)(Pang et al., 2002). In connection with the occurrences of rare words, different variations of delta tf*idf scores of words, indicati</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Comp. Ling., 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Just how mad are you? finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proc. of the 19th national Conf. on Artifical intelligence, AAAI’04,</booktitle>
<pages>761--767</pages>
<contexts>
<context position="5872" citStr="Wilson et al., 2004" startWordPosition="905" endWordPosition="908">sification on a movie dataset (Pang et al., 2002). Similarly, occurrence of rare words (Yang et al., 2006) or the position of words in a text are examined for usefulness (Kim and Hovy, 2006)(Pang et al., 2002). In connection with the occurrences of rare words, different variations of delta tf*idf scores of words, indicating the difference in occurrences of words in different classes (positive or negative reviews), have been suggested (Paltoglou and Thelwall, 2010). In addition to sentiment classification, obtaining the opinion strength is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a difficult task, one of the recent studies also investigated the relations between word disambiguation and subjectivity, in order to obtain sufficient information for a better sentiment classification (Wiebe and Mihalcea, 2006). A recent survey describing the fundamental approaches can be found in (Liu, 2012). Two sub-systems combined to form the SUSentilab submission are slightly modified from our previous work (Gezici et al., 2012) (Dehkharghani et al., 2012) (Demiroz et al., 2012). For subsystem SU1, we prese</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? finding strong and weak opinion clauses. In Proc. of the 19th national Conf. on Artifical intelligence, AAAI’04, pages 761–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Comp. Ling.,</journal>
<pages>399--433</pages>
<contexts>
<context position="2718" citStr="Wilson et al., 2009" startWordPosition="422" endWordPosition="425">. Both systems use features that are based on well-known polarity resources namely SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012) and NRC Emotion Lexicon (Mohammad, 2012). Also a set of positive and negative seed words (Liu et al., 2005) is used in feature extraction. The remainder of paper is organized as follows: Related works are presented in Section 2; the proposed approach is described in Section 3 and experimental evaluation is presented in Section 4. 2 Related Works There has been much work on sentiment analysis in the last ten years (Riloff and Wiebe, 2003) (Wilson et al., 2009) (Taboada et al., 2011) (Pang and Lee, 2008). The two fundamental methods for sentiment analysis are lexicon-based and supervised methods. The lexicon-based technique adopts the idea of determining the review sentiment by obtaining word polarities from a lexicon, such as the SentiWordNet (Baccianella et al., 2010), SenticNet (Cambria et al., 2012). This lexicon can be domain-independent or domain-specific. One can use a domain-specific lexicon whenever available, to get a better performance by obtaining the correct word polarities in the given domain (e.g., the word ’small’ has a positive mean</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Comp. Ling., pages 399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiduk Yang</author>
<author>Ning Yu</author>
<author>Alejandro Valerio</author>
<author>Hui Zhang</author>
</authors>
<title>Widit in trec-2006 blog track.</title>
<date>2006</date>
<booktitle>In Proc. of TREC,</booktitle>
<pages>27--31</pages>
<contexts>
<context position="5358" citStr="Yang et al., 2006" startWordPosition="824" endWordPosition="827">lem is determining the subjectivity of a given sentence. In an early study, the effects of adjective orientation and gradability on sentence subjectivity was studied (Hatzivassiloglou and Wiebe, 2000). Wiebe et al. (Wiebe et al., 2004) presents a broad survey on subjectivity recognition and the key elements that may have an impact on it. In estimating the sentiment polarity, the use of higher-order n-grams is also studied. Pang et. al report results where unigrams work better than bigrams for sentiment classification on a movie dataset (Pang et al., 2002). Similarly, occurrence of rare words (Yang et al., 2006) or the position of words in a text are examined for usefulness (Kim and Hovy, 2006)(Pang et al., 2002). In connection with the occurrences of rare words, different variations of delta tf*idf scores of words, indicating the difference in occurrences of words in different classes (positive or negative reviews), have been suggested (Paltoglou and Thelwall, 2010). In addition to sentiment classification, obtaining the opinion strength is another issue which may be of interest. Wilson et al. (Wilson et al., 2004) for instance, attempts to determine clause-level opinion strength. Since this is a di</context>
</contexts>
<marker>Yang, Yu, Valerio, Zhang, 2006</marker>
<rawString>Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang. 2006. Widit in trec-2006 blog track. In Proc. of TREC, pages 27–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proc. of the 2003 Conf. on Empirical methods in natural language processing,</booktitle>
<pages>129--136</pages>
<publisher>Association for Comp. Ling.</publisher>
<contexts>
<context position="4306" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="654" endWordPosition="657">hing a domain-specific lexicon is costly, so many systems use a domain-independent lexicon, such as the SentiWordNet, shortly SWN, (Baccianella et al., 2010) and SenticNet (Cambria et al., 2012). Part of Speech (POS) information is commonly indicated in polarity lexicons, partly to overcome word-sense disambiguity and therefore help achieve a better sentiment classification performance. Alternatively, supervised methods use machine learning techniques to build models or discriminators for the different classes (e.g. positive reviews), using a large corpus. For example, in (Pang et al., 2002) (Yu and Hatzivassiloglou, 2003), the Naive Bayes algorithm is used to separate positive reviews from negative ones. Note that supervised learning techniques can also use a lexicon in the feature extraction stage. They also generally perform better compared to lexicon-based approaches; however collecting a large training data may be an issue. In estimating the sentiment of a given natural language text, many issues are considered. For instance one important problem is determining the subjectivity of a given sentence. In an early study, the effects of adjective orientation and gradability on sentence subjectivity was studied </context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proc. of the 2003 Conf. on Empirical methods in natural language processing, pages 129–136. Association for Comp. Ling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ethan Zhang</author>
<author>Yi Zhang</author>
</authors>
<title>Ucsc on trec</title>
<date>2006</date>
<booktitle>In Text Retrieval Conference.</booktitle>
<contexts>
<context position="15674" citStr="Zhang and Zhang, 2006" startWordPosition="2586" endWordPosition="2589"> sentences which may be significant in SMS messages and the estimated review subjectivity as a feature derived from sentence-level processing. The review is considered subjective if it contains at least one subjec474 tive sentence. In turn, a sentence is subjective if and Pol(w) = { 0 if max(p=, p+, p−) = p= only if it contains at least one subjective word-sense p+ else if p+ &gt; p− pair or contains at least one smiley. A word-sense −p− otherwise pair is subjective if and only if the sum of its positive and negative polarity taken from SentiWordNet (Baccianella et al., 2010) is bigger than 0.5 (Zhang and Zhang, 2006). These features are described in detail in (Gezici et al., 2012). Feature name F1: Average review polarity F2: Review purity F3: # of positive smileys F4: # of negative smileys F5: Freq. of seed words F6: Avg. polarity of seed words F7: Std. of polarities of seed words F8: Otf-idf weighted avg. polarity of words F9: Otf-idf scores of words F10: # of Exclamation marks F11: # of Question marks F12: Avg. First Line Polarity F13: Avg. Last Line Polarity F14: First Line Purity F15: Last Line Purity F16: Avg. pol. of subj. sentences F17: Avg. pol. of pure sentences F18: Avg. pol. of non-irrealis se</context>
</contexts>
<marker>Zhang, Zhang, 2006</marker>
<rawString>Ethan Zhang and Yi Zhang. 2006. Ucsc on trec 2006 blog opinion mining. In Text Retrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Gen Wang</author>
</authors>
<title>Adding redundant features for crfs-based sentence sentiment classification.</title>
<date>2008</date>
<booktitle>In Proc. of the conference on empirical methods in natural language processing,</booktitle>
<pages>117--126</pages>
<publisher>Association for Comp. Ling.</publisher>
<contexts>
<context position="14821" citStr="Zhao et al., 2008" startWordPosition="2437" endWordPosition="2440">f features based on the Atf-idf score of a word-sense pair, indicating the relative occurrence of a word-sense among positive and negative classes (Demiroz et al., 2012). Punctuation-based Features This group contains the number of question and exclamation marks in the message, as they may give some information about the sentiment of a review, especially for the Twitter domain. Sentence-based Features In this last group of features, we extract features based on sentence type (e.g. subjective, pure, and non-irrealis) (Taboada et al., 2011) and sentence position (e.g. first line and last line) (Zhao et al., 2008). Features include several basic ones such as the average polarity of the first sentence and the average polarity of subjective or pure sentences. We also compute Atf-idf scores on sentence level. Finally, we consider the number of sentences which may be significant in SMS messages and the estimated review subjectivity as a feature derived from sentence-level processing. The review is considered subjective if it contains at least one subjec474 tive sentence. In turn, a sentence is subjective if and Pol(w) = { 0 if max(p=, p+, p−) = p= only if it contains at least one subjective word-sense p+ e</context>
</contexts>
<marker>Zhao, Liu, Wang, 2008</marker>
<rawString>Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding redundant features for crfs-based sentence sentiment classification. In Proc. of the conference on empirical methods in natural language processing, pages 117– 126. Association for Comp. Ling.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>