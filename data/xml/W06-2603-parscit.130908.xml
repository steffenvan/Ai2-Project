<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000577">
<title confidence="0.8245118">
Decomposition Kernels for Natural Language Processing
Fabrizio Costa Sauro Menchetti Alessio Ceroni Andrea Passerini Paolo Frasconi
Dipartimento di Sistemi e Informatica,
Universit`a degli Studi di Firenze,
via di S. Marta 3, 50139 Firenze, Italy
</title>
<email confidence="0.337619">
{costa,menchett,passerini,aceroni,p-f} AT dsi.unifi.it
</email>
<sectionHeader confidence="0.985794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914857142857">
We propose a simple solution to the se-
quence labeling problem based on an ex-
tension of weighted decomposition ker-
nels. We additionally introduce a multi-
instance kernel approach for representing
lexical word sense information. These
new ideas have been preliminarily tested
on named entity recognition and PP at-
tachment disambiguation. We finally sug-
gest how these techniques could be poten-
tially merged using a declarative formal-
ism that may provide a basis for the inte-
gration of multiple sources of information
when using kernel-based learning in NLP.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994035086206897">
Many tasks related to the analysis of natural lan-
guage are best solved today by machine learning
and other data driven approaches. In particular,
several subproblems related to information extrac-
tion can be formulated in the supervised learning
framework, where statistical learning has rapidly
become one of the preferred methods of choice.
A common characteristic of many NLP problems
is the relational and structured nature of the rep-
resentations that describe data and that are inter-
nally used by various algorithms. Hence, in or-
der to develop effective learning algorithms, it is
necessary to cope with the inherent structure that
characterize linguistic entities. Kernel methods
(see e.g. Shawe-Taylor and Cristianini, 2004) are
well suited to handle learning tasks in structured
domains as the statistical side of a learning algo-
rithm can be naturally decoupled from any rep-
resentational details that are handled by the ker-
nel function. As a matter of facts, kernel-based
statistical learning has gained substantial impor-
tance in the NLP field. Applications are numerous
and diverse and include for example refinement
of statistical parsers (Collins and Duffy, 2002),
tagging named entities (Cumby and Roth, 2003;
Tsochantaridis et al., 2004), syntactic chunking
(Daum´e III and Marcu, 2005), extraction of rela-
tions between entities (Zelenko et al., 2003; Cu-
lotta and Sorensen, 2004), semantic role label-
ing (Moschitti, 2004). The literature is rich with
examples of kernels on discrete data structures
such as sequences (Lodhi et al., 2002; Leslie et
al., 2002; Cortes et al., 2004), trees (Collins and
Duffy, 2002; Kashima and Koyanagi, 2002), and
annotated graphs (G¨artner, 2003; Smola and Kon-
dor, 2003; Kashima et al., 2003; Horv´ath et al.,
2004). Kernels of this kind can be almost in-
variably described as special cases of convolu-
tion and other decomposition kernels (Haussler,
1999). Thanks to its generality, decomposition
is an attractive and flexible approach for defining
the similarity between structured objects starting
from the similarity between smaller parts. How-
ever, excessively large feature spaces may result
from the combinatorial growth of the number of
distinct subparts with their size. When too many
dimensions in the feature space are irrelevant, the
Gram matrix will be nearly diagonal (Sch¨olkopf
et al., 2002), adversely affecting generalization in
spite of using large margin classifiers (Ben-David
et al., 2002). Possible cures include extensive use
of prior knowledge to guide the choice of rele-
vant parts (Cumby and Roth, 2003; Frasconi et al.,
2004), the use of feature selection (Suzuki et al.,
2004), and soft matches (Saunders et al., 2002). In
(Menchetti et al., 2005) we have shown that better
generalization can indeed be achieved by avoid-
ing hard comparisons between large parts. In a
</bodyText>
<page confidence="0.998308">
17
</page>
<bodyText confidence="0.999921916666667">
weighted decomposition kernel (WDK) only small
parts are matched, whereas the importance of the
match is determined by comparing the sufficient
statistics of elementary probabilistic models fit-
ted on larger contextual substructures. Here we
introduce a position-dependent version of WDK
that can solve sequence labeling problems without
searching the output space, as required by other re-
cently proposed kernel-based solutions (Tsochan-
taridis et al., 2004; Daum´e III and Marcu, 2005).
The paper is organized as follows. In the next
two sections we briefly review decomposition ker-
nels and its weighted variant. In Section 4 we in-
troduce a version of WDK for solving supervised
sequence labeling tasks and report a preliminary
evaluation on a named entity recognition problem.
In Section 5 we suggest a novel multi-instance ap-
proach for representing WordNet information and
present an application to the PP attachment am-
biguity resolution problem. In Section 6 we dis-
cuss how these ideas could be merged using a
declarative formalism in order to integrate mul-
tiple sources of information when using kernel-
based learning in NLP.
</bodyText>
<sectionHeader confidence="0.979457" genericHeader="introduction">
2 Decomposition Kernels
</sectionHeader>
<bodyText confidence="0.989177">
An R-decomposition structure (Haussler, 1999;
Shawe-Taylor and Cristianini, 2004) on a set X is
a triple R = ( X~, R,~k) where X~ = (X1, ... , XD)
is a D–tuple of non–empty subsets of X, R is
a finite relation on X1 x · · · x XD x X, and
k~ = (k1, ... , kD) is a D–tuple of positive defi-
nite kernel functions kd : Xd x Xd H IR. R(~x, x)
is true iff x~ is a tuple of “parts” for x — i.e. x~
is a decomposition of x. Note that this defini-
tion of “parts” is very general and does not re-
quire the parthood relation to obey any specific
mereological axioms, such as those that will be
introduced in Section 6. For any x E X, let
</bodyText>
<equation confidence="0.58856">
R−1(x) = {(x1, ... , xD) E X~ : R(~x, x)} de-
</equation>
<bodyText confidence="0.963367">
note the multiset of all possible decompositions1
of x. A decomposition kernel is then defined as
the multiset kernel between the decompositions:
</bodyText>
<equation confidence="0.9818995">
D
�Kiz(x, x&apos;) = H κd(xd,x&apos;d) (1)
x� ∈ R−1(x) d=1
V ∈ R−1(x&apos;)
</equation>
<footnote confidence="0.89119575">
1Decomposition examples in the string domain include
taking all the contiguous fixed-length substrings or all the
possible ways of dividing a string into two contiguous sub-
strings.
</footnote>
<bodyText confidence="0.99995025">
where, as an alternative way of combining the ker-
nels, we can use the product instead of a summa-
tion: intuitively this increases the feature space di-
mension and makes the similarity measure more
selective. Since decomposition kernels form a
rather vast class, the relation R needs to be care-
fully tuned to different applications in order to
characterize a suitable kernel. As discussed in
the Introduction, however, taking all possible sub-
parts into account may lead to poor predictivity be-
cause of the combinatorial explosion of the feature
space.
</bodyText>
<sectionHeader confidence="0.995878" genericHeader="method">
3 Weighted Decomposition Kernels
</sectionHeader>
<bodyText confidence="0.998374">
A weighted decomposition kernel (WDK) is char-
acterized by the following decomposition struc-
ture:
</bodyText>
<equation confidence="0.999042">
R = ( X~,R,(δ,κ1,...,κD))
</equation>
<bodyText confidence="0.99992004">
where X~ = (S, Z1, ... , ZD), R(s, z1, ... , zD, x)
is true iff s E S is a subpart of x called the selector
and z~ = (z1, ... , zD) E Z1x· · ·xZD is a tuple of
subparts of x called the contexts of s in x. Precise
definitions of s and z~ are domain-dependent. For
example in (Menchetti et al., 2005) we present two
formulations, one for comparing whole sequences
(where both the selector and the context are subse-
quences), and one for comparing attributed graphs
(where the selector is a single vertex and the con-
text is the subgraph reachable from the selector
within a short path). The definition is completed
by introducing a kernel on selectors and a kernel
on contexts. The former can be chosen to be the
exact matching kernel, δ, on S x S, defined as
δ(s, s&apos;) = 1 if s = s&apos; and δ(s, s&apos;) = 0 otherwise.
The latter, κd, is a kernel on Zd x Zd and pro-
vides a soft similarity measure based on attribute
frequencies. Several options are available for con-
text kernels, including the discrete version of prob-
ability product kernels (PPK) (Jebara et al., 2004)
and histogram intersection kernels (HIK) (Odone
et al., 2005). Assuming there are n categorical
attributes, each taking on mi distinct values, the
context kernel can be defined as:
</bodyText>
<equation confidence="0.964912666666667">
n
κd(z, z&apos;) = ki(z, z&apos;) (2)
i=1
</equation>
<bodyText confidence="0.987664">
where ki is a kernel on the i-th attribute. Denote by
pi(j) the observed frequency of value j in z. Then
</bodyText>
<page confidence="0.978719">
18
</page>
<bodyText confidence="0.60357">
ki can be defined as a HIK or a PPK respectively:
</bodyText>
<equation confidence="0.9984805">
ki(z, z&apos;) = Emi min{pi(j), p&apos;i(j)} (3)
j=1
ki(z, z&apos;) = Emi � pi(j) - p&apos;i(j) (4)
j=1
</equation>
<bodyText confidence="0.9823135">
This setting results in the following general form
of the kernel:
</bodyText>
<equation confidence="0.999703">
EK(x, x&apos;) =
(s, ~z) ∈ R−1(x)
(s&apos;, ~z&apos;) ∈ R−1(x&apos;)
</equation>
<bodyText confidence="0.985873">
where fve can replace the summation of kernels
</bodyText>
<equation confidence="0.522184">
with Hd=1 1 + nd(zd, z&apos;d).
</equation>
<bodyText confidence="0.999697428571429">
Compared to kernels that simply count the num-
ber of substructures, the above function weights
different matches between selectors according to
contextual information. The kernel can be after-
wards normalized in [-1, 1] to prevent similarity
to be boosted by the mere size of the structures
being compared.
</bodyText>
<sectionHeader confidence="0.9812625" genericHeader="method">
4 WDK for sequence labeling and
applications to NER
</sectionHeader>
<bodyText confidence="0.99973408">
In a sequence labeling task we want to map input
sequences to output sequences, or, more precisely,
we want to map each element of an input sequence
that takes label from a source alphabet to an ele-
ment with label in a destination alphabet.
Here we cast the sequence labeling task into
position specific classification, where different se-
quence positions give independent examples. This
is different from previous approaches in the lit-
erature where the sequence labeling problem is
solved by searching in the output space (Tsochan-
taridis et al., 2004; Daum´e III and Marcu, 2005).
Although the method lacks the potential for col-
lectively labeling all positions simultaneously, it
results in a much more efficient algorithm.
In the remainder of the section we introduce
a specialized version of the weighted decompo-
sition kernel suitable for a sequence transduction
task originating in the natural language process-
ing domain: the named entity recognition (NER)
problem, where we map sentences to sequences of
a reduced number of named entities (see Sec.4.1).
More formally, given a finite dictionary E of
words and an input sentence x E E*, our input ob-
jects are pairs of sentences and indices r = (x, t)
</bodyText>
<figureCaption confidence="0.998966">
Figure 1: Sentence decomposition.
</figureCaption>
<bodyText confidence="0.999329054054054">
where r E E* x IN. Given a sentence x, two in-
tegers b &gt; 1 and b &lt; e &lt; Ix1, let x[b] denote the
word at position b and x[b..e] the sub-sequence of
x spanning positions from b to e. Finally we will
denote by ξ(x[b]) a word attribute such as a mor-
phological trait (is a number or has capital initial,
see 4.1) for the word in sentence x at position b.
We introduce two versions of WDK: one with
four context types (D = 4) and one with in-
creased contextual information (D = 6) (see
Eq. 5). The relation R depends on two integers
t and i E {1, ... , Ix1}, where t indicates the po-
sition of the word we want to classify and i the
position of a generic word in the sentence. The
relation for the first kernel version is defined as:
R = {(s, zLL, zLR, zRL, zRR, r)} such that the
selector s = x[i] is the word at position i, the zLL
(LeftLeft) part is a sequence defined as x[1..i] if
i &lt; t or the null sequence E otherwise and the
zLR (LeftRight) part is the sequence x[i + 1..t] if
i &lt; t or E otherwise. Informally, zLL is the initial
portion of the sentence up to word of position i,
and zLR is the portion of the sentence from word
at position i + 1 up to t (see Fig. 1). Note that
when we are dealing with a word that lies to the
left of the target word t, its zRL and zRR parts are
empty. Symmetrical definitions hold for zRL and
zRR when i &gt; t. We define the weighted decom-
position kernel for sequences as
where Jξ(s, s&apos;) = 1 if ξ(s) = ξ(s&apos;) and 0 oth-
erwise (that is Jξ checks whether the two selector
words have the same morphological trait) and n
is Eq. 2 with only one attribute which then boils
down to Eq. 3 or Eq. 4, that is a kernel over the his-
togram for word occurrences over a specific part.
Intuitively, when applied to word sequences,
this kernel considers separately words to the left
</bodyText>
<equation confidence="0.99456925">
K(r, r&apos;)=
E
Jξ(s, s&apos;)
n(zd, z&apos;d) (6)
dE{LL,LR,RL,RR}
E |x|
t=1
E |x&apos;|
t&apos;=1
J(s, s&apos;)
ED nd(zd,z&apos;d) (5)
d=1
</equation>
<page confidence="0.984322">
19
</page>
<bodyText confidence="0.995891277777778">
of the entry we want to transduce and those to
its right. The kernel computes the similarity for
each sub-sequence by matching the corresponding
bag of enriched words: each word is matched only
with words that have the same trait as extracted by
ξ and the match is then weighted proportionally to
the frequency count of identical words preceding
and following it.
The kernel version with D=6 adds two parts
called zLO (LeftOther) and zRO (RightOther) de-
fined as x[t+ 1..|r|] and x[1..t] respectively; these
represent the remaining sequence parts so that x =
zLL ◦ zLR ◦ zLO and x = zRL ◦ zRR ◦ zRO.
Note that the WDK transforms the sentence
in a bag of enriched words computed in a pre-
processing phase thus achieving a significant re-
duction in computational complexity (compared to
the recursive procedure in (Lodhi et al., 2002)).
</bodyText>
<sectionHeader confidence="0.854702" genericHeader="method">
4.1 Named Entity Recognition Experimental
Results
</sectionHeader>
<bodyText confidence="0.999555733333333">
Named entities are phrases that contain the names
of persons, organizations, locations, times and
quantities. For example in the following sentence:
[PER Wolff ] , currently a journalist in [LOC
Argentina ] , played with [PER Del Bosque ] in the
final years of the seventies in [ORG Real Madrid].
we are interested in predicting that Wolff and Del
Bosque are people’s names, that Argentina is a
name of a location and that Real Madrid is a name
of an organization.
The chosen dataset is provided by the shared
task of CoNLL–2002 (Saunders et al., 2002)
which concerns language–independent named en-
tity recognition. There are four types of phrases:
person names (PER), organizations (ORG), loca-
tions (LOC) and miscellaneous names (MISC),
combined with two tags, B to denote the first item
of a phrase and I for any non–initial word; all other
phrases are classified as (OTHER). Of the two
available languages (Spanish and Dutch), we run
experiments only on the Spanish dataset which is a
collection of news wire articles made available by
the Spanish EFE News Agency. We select a sub-
set of 300 sentences for training and we evaluate
the performance on test set. For each category, we
evaluate the Fβ=1 measure of 4 versions of WDK:
word histograms are matched using HIK (Eq. 3)
and the kernels on various parts (zLL, zLR,etc) are
combined with a summation SUMHIK or product
PROHIK; alternatively the histograms are com-
</bodyText>
<tableCaption confidence="0.998815">
Table 1: NER experiment D=4
</tableCaption>
<table confidence="0.9999086">
CLASS SUMHIS PROHIS SUMPRO PROPRO
B-LOC 74.33 68.68 72.12 66.47
I-LOC 58.18 52.76 59.24 52.62
B-MISC 52.77 43.31 46.86 39.00
I-MISC 79.98 80.15 77.85 79.65
B-ORG 69.00 66.87 68.42 67.52
I-ORG 76.25 75.30 75.12 74.76
B-PER 60.11 56.60 59.33 54.80
I-PER 65.71 63.39 65.67 60.98
MICRO Fβ=1 69.28 66.33 68.03 65.30
</table>
<tableCaption confidence="0.9371">
Table 2: NER experiment with D=6
</tableCaption>
<table confidence="0.9998547">
CLASS SUMHIS PROHIS SUMPRO PROPRO
B-LOC 74.81 73.30 73.65 73.69
I-LOC 57.28 58.87 57.76 59.44
B-MISC 56.54 64.11 57.72 62.11
I-MISC 78.74 84.23 79.27 83.04
B-ORG 70.80 73.02 70.48 73.10
I-ORG 76.17 78.70 74.26 77.51
B-PER 66.25 66.84 66.04 67.46
I-PER 68.06 71.81 69.55 69.55
MICRO Fβ=1 70.69 72.90 70.32 72.38
</table>
<bodyText confidence="0.994943277777778">
bined with a PPK (Eq. 4) obtaining SUMPPK,
PROPPK.
The word attribute considered for the selector
is a word morphologic trait that classifies a word
in one of five possible categories: normal word,
number, all capital letters, only capital initial and
contains non alphabetic characters, while the con-
text histograms are computed counting the exact
word frequencies.
Results reported in Tab. 1 and Tab. 2 show that
performance is mildly affected by the different
choices on how to combine information on the var-
ious contexts, though it seems clear that increasing
contextual information has a positive influence.
Note that interesting preliminary results can be
obtained even without the use of any refined lan-
guage knowledge, such as part of speech tagging
or shallow/deep parsing.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="method">
5 Kernels for word semantic ambiguity
</sectionHeader>
<bodyText confidence="0.997756428571429">
Parsing a natural language sentence often involves
the choice between different syntax structures that
are equally admissible in the given grammar. One
of the most studied ambiguity arise when deciding
between attaching a prepositional phrase either to
the noun phrase or to the verb phrase. An example
could be:
</bodyText>
<listItem confidence="0.9954265">
1. eat salad with forks (attach to verb)
2. eat salad with tomatoes (attach to noun)
</listItem>
<page confidence="0.980557">
20
</page>
<bodyText confidence="0.999661846153846">
The resolution of such ambiguities is usually per-
formed by the human reader using its past expe-
riences and the knowledge of the words mean-
ing. Machine learning can simulate human experi-
ence by using corpora of disambiguated phrases to
compute a decision on new cases. However, given
the number of different words that are currently
used in texts, there would never be a sufficient
dataset from which to learn. Adding semantic in-
formation on the possible word meanings would
permit the learning of rules that apply to entire cat-
egories and can be generalized to all the member
words.
</bodyText>
<subsectionHeader confidence="0.995192">
5.1 Adding Semantic with WordNet
</subsectionHeader>
<bodyText confidence="0.999967473684211">
WordNet (Fellbaum, 1998) is an electronic lexi-
cal database of English words built and annotated
by linguistic researchers. WordNet is an exten-
sive and reliable source of semantic information
that can be used to enrich the representation of a
word. Each word is represented in the database by
a group of synonym sets (synset), with each synset
corresponding to an individual linguistic concept.
All the synsets contained in WordNet are linked by
relations of various types. An important relation
connects a synset to its hypernyms, that are its im-
mediately broader concepts. The hypernym (and
its opposite hyponym) relation defines a semantic
hierarchy of synsets that can be represented as a
directed acyclic graph. The different lexical cat-
egories (verbs, nouns, adjectives and adverbs) are
contained in distinct hierarchies and each one is
rooted by many synsets.
Several metrics have been devised to compute
a similarity score between two words using Word-
Net. In the following we resort to a multiset ver-
sion of the proximity measure used in (Siolas and
d’Alche Buc, 2000), though more refined alterna-
tives are also possible (for example using the con-
ceptual density as in (Basili et al., 2005)). Given
the acyclic nature of the semantic hierarchies, each
synset can be represented by a group of paths that
follows the hypernym relations and finish in one of
the top level concepts. Two paths can then be com-
pared by counting how many steps from the roots
they have in common. This number must then be
normalized dividing by the square root of the prod-
uct between the path lengths. In this way one can
accounts for the unbalancing that arise from dif-
ferent parts of the hierarchies being differently de-
tailed. Given two paths π and π&apos;, let l and l&apos; be
their lengths and n be the size of their common
part, the resulting kernel is:
</bodyText>
<equation confidence="0.986955">
n
k(π,π&apos;) = √ (7)
l · l&apos;
</equation>
<bodyText confidence="0.999904352941177">
The demonstration that k is positive definite arise
from the fact that n can be computed as a posi-
tive kernel k* by summing the exact match ker-
nels between the corresponding positions in π and
π&apos; seen as sequences of synset identifiers. The
lengths l and l&apos; can then be evaluated as k*(π, π)
and k*(π&apos;, π&apos;) and k is the resulting normalized
version of k*.
The kernel between two synsets σ and σ&apos; can
then be computed by the multi-set kernel (G¨artner
et al., 2002a) between their corresponding paths.
Synsets are organized into forty-five lexicogra-
pher files based on syntactic category and logical
groupings. Additional information can be derived
by comparing the identifiers λ and λ&apos; of these file
associated to σ and σ&apos;. The resulting synset kernel
is:
</bodyText>
<equation confidence="0.997371">
Eκσ(σ, σ&apos;) = δ(λ, λ&apos;) + E k(π, π&apos;) (8)
πEII π&apos;EII&apos;
</equation>
<bodyText confidence="0.999755">
where Π is the set of paths originating from σ and
the exact match kernel δ(λ, λ&apos;) is 1 if λ ≡ λ&apos; and
0 otherwise. Finally, the kernel κω between two
words is itself a multi-set kernel between the cor-
responding sets of synsets:
</bodyText>
<equation confidence="0.9879605">
Eκω(ω, ω&apos;) = E κσ(σ, σ&apos;) (9)
σEE σ&apos;EE&apos;
</equation>
<bodyText confidence="0.998364">
where Σ are the synsets associated to the word ω.
</bodyText>
<subsectionHeader confidence="0.79981">
5.2 PP Attachment Experimental Results
</subsectionHeader>
<bodyText confidence="0.999912875">
The experiments have been performed using the
Wall-Street Journal dataset described in (Ratna-
parkhi et al., 1994). This dataset contains 20, 800
training examples and 3,097 testing examples.
Each phrase x in the dataset is reduced to a verb
xv, its object noun xn1 and prepositional phrase
formed by a preposition xp and a noun xn2. The
target is either V or N whether the phrase is at-
tached to the verb or the noun. Data have been pre-
processed by assigning to all the words their cor-
responding synsets. Additional meanings derived
from specific synsets have been attached to the
words as described in (Stetina and Nagao, 1997).
The kernel between two phrases x and x&apos; is then
computed by combining the kernels between sin-
gle words using either the sum or the product.
</bodyText>
<page confidence="0.997726">
21
</page>
<table confidence="0.999365857142857">
Method Acc Pre Rec
S 84.6% + 0.65% 90.8% 82.2%
P 84.8% + 0.65% 92.2% 81.0%
SW 85.4% + 0.64% 90.9% 83.6%
SWL 85.3% + 0.64% 91.1% 83.2%
PW 85.9% + 0.63% 92.2% 83.1%
PWL 86.2% + 0.62% 92.1% 83.7%
</table>
<tableCaption confidence="0.802146666666667">
Table 3: Summary of the experimental results on
the PP attachment problem for various kernel pa-
rameters.
</tableCaption>
<bodyText confidence="0.996809947368421">
Results of the experiments are reported in Tab. 3
for various kernels parameters: S or P denote if
the sum or product of the kernels between words
are used, W denotes that WordNet semantic infor-
mation is added (otherwise the kernel between two
words is just the exact match kernel) and L denotes
that lexicographer files identifiers are used. An ad-
ditional gaussian kernel is used on top of Kpp. The
C and γ parameters are selected using an inde-
pendent validation set. For each setting, accuracy,
precision and recall values on the test data are re-
ported, along with the standard deviation of the es-
timated binomial distribution of errors. The results
demonstrate that semantic information can help in
resolving PP ambiguities. A small difference ex-
ists between taking the product instead of the sum
of word kernels, and an additional increase in the
amount of information available to the learner is
given by the use of lexicographer files identifiers.
</bodyText>
<sectionHeader confidence="0.535517" genericHeader="method">
6 Using declarative knowledge for NLP
kernel integration
</sectionHeader>
<bodyText confidence="0.998752176470588">
Data objects in NLP often require complex repre-
sentations; suffice it to say that a sentence is nat-
urally represented as a variable length sequence
of word tokens and that shallow/deep parsers are
reliably used to enrich these representations with
links between words to form parse trees. Finally,
additional complexity can be introduced by in-
cluding semantic information. Various facets of
this richness of representations have been exten-
sively investigated, including the expressiveness
of various grammar formalisms, the exploitation
of lexical representation (e.g. verb subcategoriza-
tion, semantic tagging), and the use of machine
readable sources of generic or specialized knowl-
edge (dictionaries, thesauri, domain specific on-
tologies). All these efforts are capable to address
language specific sub-problems but their integra-
tion into a coherent framework is a difficult feat.
Recent ideas for constructing kernel functions
starting from logical representations may offer an
appealing solution. G¨artner et al. (2002) have pro-
posed a framework for defining kernels on higher-
order logic individuals. Cumby and Roth (2003)
used description logics to represent knowledge
jointly with propositionalization for defining a ker-
nel function. Frasconi et al. (2004) proposed
kernels for handling supervised learning in a set-
ting similar to that of inductive logic programming
where data is represented as a collection of facts
and background knowledge by a declarative pro-
gram in first-order logic. In this section, we briefly
review this approach and suggest a possible way of
exploiting it for the integration of different sources
of knowledge that may be available in NLP.
</bodyText>
<subsectionHeader confidence="0.993316">
6.1 Declarative Kernels
</subsectionHeader>
<bodyText confidence="0.999990692307692">
The definition of decomposition kernels as re-
ported in Section 2 is very general and covers al-
most all kernels for discrete structured data de-
veloped in the literature so far. Different kernels
are designed by defining the relation decompos-
ing an example into its “parts”, and specifying
kernels for individual parts. In (Frasconi et al.,
2004) we proposed a systematic approach to such
design, consisting in formally defining a relation
by the set of axioms it must satisfy. We relied
on mereotopology (Varzi, 1996) (i.e. the theory
of parts and places) in order to give a formal def-
inition of the intuitive concepts of parthood and
connection. The formalization of mereotopolog-
ical relations allows to automatically deduce in-
stances of such relations on the data, by exploit-
ing the background knowledge which is typically
available on structured domains. In (Frasconi et
al., 2004) we introduced declarative kernels (DK)
as a set of kernels working on mereotopological
relations, such as that of proper parthood (gyP) or
more complex relations based on connected parts.
A typed syntax for objects was introduced in order
to provide additional flexibility in defining kernels
on instances of the given relation. A basic kernel
on parts KP was defined as follows:
</bodyText>
<equation confidence="0.776761333333333">
�KP(x, x&apos;)= ST(s, s&apos;)(n(s, s&apos;)+KP(s, s&apos;))(10)
3 ≺P 2
3 ≺P 2,
</equation>
<bodyText confidence="0.991755">
where ST matches objects of the same type and n
is a kernel over object attributes.
</bodyText>
<page confidence="0.991398">
22
</page>
<bodyText confidence="0.999708">
Declarative kernels were tested in (Frasconi et
al., 2004) on a number of domains with promising
results, including a biomedical information extrac-
tion task (Goadrich et al., 2004) aimed at detecting
protein-localization relationships within Medline
abstracts. A DK on parts as the one defined in
Eq. (10) outperformed state-of-the-art ILP-based
systems Aleph and Gleaner (Goadrich et al., 2004)
in such information extraction task, and required
about three orders of magnitude less training time.
</bodyText>
<subsectionHeader confidence="0.633612">
6.2 Weighted Decomposition Declarative
Kernels
</subsectionHeader>
<bodyText confidence="0.999600186046511">
Declarative kernels can be combined with WDK
in a rather straightforward way, thus taking the ad-
vantages of both methods. A simple approach is
that of using proper parthood in place of selec-
tors, and topology to recover the context of each
proper part. A weighted decomposition declara-
tive kernel (WD2K) of this kind would be defined
as in Eq. (10) simply adding to the attribute ker-
nel κ a context kernel that compares the surround-
ing of a pair of objects—as defined by the topol-
ogy relation—using some aggregate kernel such as
PPK or HIK (see Section 3). Note that such defini-
tion extends WDK by adding recursion to the con-
cept of comparison by selector, and DK by adding
contexts to the kernel between parts. Multiple con-
texts can be easily introduced by employing differ-
ent notions of topology, provided they are consis-
tent with mereotopological axioms. As an exam-
ple, if objects are words in a textual document, we
can define l-connection as the relation for which
two words are l-connected if there are consequen-
tial within the text with at most l words in be-
tween, and obtain growingly large contexts by in-
creasing l. Moreover, an extended representation
of words, as the one employing WordNet semantic
information, could be easily plugged in by includ-
ing a kernel for synsets such as that in Section 5.1
into the kernel κ on word attributes. Additional
relations could be easily formalized in order to ex-
ploit specific linguisitc knowledge: a causal rela-
tion would allow to distinguish between preceding
and following context so to take into consideration
word order; an underlap relation, associating two
objects being parts of the same super-object (i.e.
pre-terminals dominated by the same non-terminal
node), would be able to express commanding no-
tions.
The promising results obtained with declarative
kernels (where only very simple lexical informa-
tion was used) together with the declarative ease
to integrate arbitrary kernels on specific parts are
all encouraging signs that boost our confidence in
this line of research.
</bodyText>
<sectionHeader confidence="0.995827" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999322369565217">
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of wordnet seman-
tics via kernel-based learning. In 9th Conference
on Computational Natural Language Learning, Ann
Arbor(MI), USA.
S. Ben-David, N. Eiron, and H. U. Simon. 2002. Lim-
itations of learning via embeddings in euclidean half
spaces. J. of Mach. Learning Research, 3:441–461.
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Pro-
ceedings of the Fortieth Annual Meeting on Associa-
tion for Computational Linguistics, pages 263–270,
Philadelphia, PA, USA.
C. Cortes, P. Haffner, and M. Mohri. 2004. Ratio-
nal kernels: Theory and algorithms. J. of Machine
Learning Research, 5:1035–1062.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proc. of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 423–429.
C. M. Cumby and D. Roth. 2003. On kernel meth-
ods for relational learning. In Proc. Int. Conference
on Machine Learning (ICML’03), pages 107–114,
Washington, DC, USA.
H. Daum´e III and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods
for structured prediction. In International Confer-
ence on Machine Learning (ICML), pages 169–176,
Bonn, Germany.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
P. Frasconi, S. Muggleton, H. Lodhi, and A. Passerini.
2004. Declarative kernels. Technical Report RT
2/2004, Universit`a di Firenze.
T. G¨artner, P. A. Flach, A. Kowalczyk, and A. J. Smola.
2002a. Multi-instance kernels. In C. Sammut and
A. Hoffmann, editors, Proceedings of the 19th In-
ternational Conference on Machine Learning, pages
179–186. Morgan Kaufmann.
T. G¨artner, J.W. Lloyd, and P.A. Flach. 2002b. Ker-
nels for structured data. In S. Matwin and C. Sam-
mut, editors, Proceedings of the 12th International
Conference on Inductive Logic Programming, vol-
ume 2583 of Lecture Notes in Artificial Intelligence,
pages 66–83. Springer-Verlag.
</reference>
<page confidence="0.970396">
23
</page>
<reference confidence="0.999940234042554">
T. G¨artner. 2003. A survey of kernels for structured
data. SIGKDD Explorations Newsletter, 5(1):49–
58.
M. Goadrich, L. Oliphant, and J. W. Shavlik. 2004.
Learning ensembles of first-order clauses for recall-
precision curves: A case study in biomedical infor-
mation extraction. In Proc. 14th Int. Conf. on Induc-
tive Logic Programming, ILP ’04, pages 98–115.
D. Haussler. 1999. Convolution kernels on discrete
structures. Technical Report UCSC-CRL-99-10,
University of California, Santa Cruz.
T. Horv´ath, T. G¨artner, and S. Wrobel. 2004. Cyclic
pattern kernels for predictive graph mining. In Pro-
ceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 158–167. ACM Press.
T. Jebara, R. Kondor, and A. Howard. 2004. Proba-
bility product kernels. J. Mach. Learn. Res., 5:819–
844.
H. Kashima and T. Koyanagi. 2002. Kernels for
Semi–Structured Data. In Proceedings of the Nine-
teenth International Conference on Machine Learn-
ing, pages 291–298.
H. Kashima, K. Tsuda, and A. Inokuchi. 2003.
Marginalized kernels between labeled graphs. In
Proceedings of the Twentieth International Confer-
ence on Machine Learning, pages 321–328, Wash-
ington, DC, USA.
C. S. Leslie, E. Eskin, and W. S. Noble. 2002. The
spectrum kernel: A string kernel for SVM protein
classification. In Pacific Symposium on Biocomput-
ing, pages 566–575.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristian-
ini, and C. Watkins. 2002. Text classification us-
ing string kernels. Journal of Machine Learning Re-
search, 2:419–444.
S. Menchetti, F. Costa, and P. Frasconi. 2005.
Weighted decomposition kernels. In Proceedings of
the Twenty-second International Conference on Ma-
chine Learning, pages 585–592, Bonn, Germany.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In 42-th Con-
ference on Association for Computational Linguis-
tic, Barcelona, Spain.
F. Odone, A. Barla, and A. Verri. 2005. Building ker-
nels from binary strings for image matching. IEEE
Transactions on Image Processing, 14(2):169–180.
A Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase
attachment. In Proceedings of the ARPA Human
Language Technology Workshop, pages 250–255,
Plainsboro, NJ.
C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002.
Syllables and other string kernel extensions. In Pro-
ceedings of the Nineteenth International Conference
on Machine Learning, pages 530–537.
B. Sch¨olkopf, J. Weston, E. Eskin, C. S. Leslie, and
W. S. Noble. 2002. A kernel approach for learn-
ing from almost orthogonal patterns. In Proc. of
ECML’02, pages 511–528.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
G. Siolas and F. d’Alche Buc. 2000. Support vector
machines based on a semantic kernel for text cate-
gorization. In Proceedings of the IEEE-INNS-ENNS
International Joint Conference on Neural Networks,
volume 5, pages 205 – 209.
A.J. Smola and R. Kondor. 2003. Kernels and regular-
ization on graphs. In B. Sch¨olkopf and M.K. War-
muth, editors, 16th Annual Conference on Compu-
tational Learning Theory and 7th Kernel Workshop,
COLT/Kernel 2003, volume 2777 of Lecture Notes
in Computer Science, pages 144–158. Springer.
J Stetina and M Nagao. 1997. Corpus based pp attach-
ment ambiguity resolution with a semantic dictio-
nary. In Proceedings of the Fifth Workshop on Very
Large Corpora, pages 66–80, Beijing, China.
J. Suzuki, H. Isozaki, and E. Maeda. 2004. Convo-
lution kernels with feature selection for natural lan-
guage processing tasks. In Proc. of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 119–126.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Proc.
21st Int. Conf. on Machine Learning, pages 823–
830, Banff, Alberta, Canada.
A.C. Varzi. 1996. Parts, wholes, and part-whole re-
lations: the prospects of mereotopology. Data and
Knowledge Engineering, 20:259–286.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Ma-
chine Learning Research, 3:1083–1106.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.470749">
<title confidence="0.999932">Decomposition Kernels for Natural Language Processing</title>
<author confidence="0.986225">Fabrizio Costa Sauro Menchetti Alessio Ceroni Andrea Passerini Paolo</author>
<affiliation confidence="0.817940333333333">Dipartimento di Sistemi e Universit`a degli Studi di via di S. Marta 3, 50139 Firenze,</affiliation>
<email confidence="0.999167">dsi.unifi.it</email>
<abstract confidence="0.997888133333333">We propose a simple solution to the sequence labeling problem based on an extension of weighted decomposition kernels. We additionally introduce a multiinstance kernel approach for representing lexical word sense information. These new ideas have been preliminarily tested on named entity recognition and PP attachment disambiguation. We finally suggest how these techniques could be potentially merged using a declarative formalism that may provide a basis for the integration of multiple sources of information when using kernel-based learning in NLP.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Effective use of wordnet semantics via kernel-based learning.</title>
<date>2005</date>
<booktitle>In 9th Conference on Computational Natural Language Learning,</booktitle>
<location>Ann Arbor(MI), USA.</location>
<contexts>
<context position="17852" citStr="Basili et al., 2005" startWordPosition="3046" endWordPosition="3049"> hypernym (and its opposite hyponym) relation defines a semantic hierarchy of synsets that can be represented as a directed acyclic graph. The different lexical categories (verbs, nouns, adjectives and adverbs) are contained in distinct hierarchies and each one is rooted by many synsets. Several metrics have been devised to compute a similarity score between two words using WordNet. In the following we resort to a multiset version of the proximity measure used in (Siolas and d’Alche Buc, 2000), though more refined alternatives are also possible (for example using the conceptual density as in (Basili et al., 2005)). Given the acyclic nature of the semantic hierarchies, each synset can be represented by a group of paths that follows the hypernym relations and finish in one of the top level concepts. Two paths can then be compared by counting how many steps from the roots they have in common. This number must then be normalized dividing by the square root of the product between the path lengths. In this way one can accounts for the unbalancing that arise from different parts of the hierarchies being differently detailed. Given two paths π and π&apos;, let l and l&apos; be their lengths and n be the size of their c</context>
</contexts>
<marker>Basili, Cammisa, Moschitti, 2005</marker>
<rawString>Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2005. Effective use of wordnet semantics via kernel-based learning. In 9th Conference on Computational Natural Language Learning, Ann Arbor(MI), USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ben-David</author>
<author>N Eiron</author>
<author>H U Simon</author>
</authors>
<title>Limitations of learning via embeddings in euclidean half spaces.</title>
<date>2002</date>
<journal>J. of Mach. Learning Research,</journal>
<pages>3--441</pages>
<contexts>
<context position="3332" citStr="Ben-David et al., 2002" startWordPosition="501" endWordPosition="504">cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf et al., 2002), adversely affecting generalization in spite of using large margin classifiers (Ben-David et al., 2002). Possible cures include extensive use of prior knowledge to guide the choice of relevant parts (Cumby and Roth, 2003; Frasconi et al., 2004), the use of feature selection (Suzuki et al., 2004), and soft matches (Saunders et al., 2002). In (Menchetti et al., 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts. In a 17 weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contex</context>
</contexts>
<marker>Ben-David, Eiron, Simon, 2002</marker>
<rawString>S. Ben-David, N. Eiron, and H. U. Simon. 2002. Limitations of learning via embeddings in euclidean half spaces. J. of Mach. Learning Research, 3:441–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the Fortieth Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="2053" citStr="Collins and Duffy, 2002" startWordPosition="305" endWordPosition="308">ffective learning algorithms, it is necessary to cope with the inherent structure that characterize linguistic entities. Kernel methods (see e.g. Shawe-Taylor and Cristianini, 2004) are well suited to handle learning tasks in structured domains as the statistical side of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the Fortieth Annual Meeting on Association for Computational Linguistics, pages 263–270, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>P Haffner</author>
<author>M Mohri</author>
</authors>
<title>Rational kernels: Theory and algorithms.</title>
<date>2004</date>
<journal>J. of Machine Learning Research,</journal>
<pages>5--1035</pages>
<contexts>
<context position="2471" citStr="Cortes et al., 2004" startWordPosition="371" endWordPosition="374">l-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of disti</context>
</contexts>
<marker>Cortes, Haffner, Mohri, 2004</marker>
<rawString>C. Cortes, P. Haffner, and M. Mohri. 2004. Rational kernels: Theory and algorithms. J. of Machine Learning Research, 5:1035–1062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="2271" citStr="Culotta and Sorensen, 2004" startWordPosition="337" endWordPosition="341">g tasks in structured domains as the statistical side of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>A. Culotta and J. Sorensen. 2004. Dependency tree kernels for relation extraction. In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Cumby</author>
<author>D Roth</author>
</authors>
<title>On kernel methods for relational learning.</title>
<date>2003</date>
<booktitle>In Proc. Int. Conference on Machine Learning (ICML’03),</booktitle>
<pages>107--114</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2099" citStr="Cumby and Roth, 2003" startWordPosition="312" endWordPosition="315">ope with the inherent structure that characterize linguistic entities. Kernel methods (see e.g. Shawe-Taylor and Cristianini, 2004) are well suited to handle learning tasks in structured domains as the statistical side of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described a</context>
<context position="3449" citStr="Cumby and Roth, 2003" startWordPosition="521" endWordPosition="524">ractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf et al., 2002), adversely affecting generalization in spite of using large margin classifiers (Ben-David et al., 2002). Possible cures include extensive use of prior knowledge to guide the choice of relevant parts (Cumby and Roth, 2003; Frasconi et al., 2004), the use of feature selection (Suzuki et al., 2004), and soft matches (Saunders et al., 2002). In (Menchetti et al., 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts. In a 17 weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures. Here we introduce a position-dependent version of WDK that can solve sequence labeling problems w</context>
<context position="22884" citStr="Cumby and Roth (2003)" startWordPosition="3911" endWordPosition="3914">ar formalisms, the exploitation of lexical representation (e.g. verb subcategorization, semantic tagging), and the use of machine readable sources of generic or specialized knowledge (dictionaries, thesauri, domain specific ontologies). All these efforts are capable to address language specific sub-problems but their integration into a coherent framework is a difficult feat. Recent ideas for constructing kernel functions starting from logical representations may offer an appealing solution. G¨artner et al. (2002) have proposed a framework for defining kernels on higherorder logic individuals. Cumby and Roth (2003) used description logics to represent knowledge jointly with propositionalization for defining a kernel function. Frasconi et al. (2004) proposed kernels for handling supervised learning in a setting similar to that of inductive logic programming where data is represented as a collection of facts and background knowledge by a declarative program in first-order logic. In this section, we briefly review this approach and suggest a possible way of exploiting it for the integration of different sources of knowledge that may be available in NLP. 6.1 Declarative Kernels The definition of decompositi</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>C. M. Cumby and D. Roth. 2003. On kernel methods for relational learning. In Proc. Int. Conference on Machine Learning (ICML’03), pages 107–114, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>169--176</pages>
<location>Bonn, Germany.</location>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>H. Daum´e III and D. Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In International Conference on Machine Learning (ICML), pages 169–176, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Frasconi</author>
<author>S Muggleton</author>
<author>H Lodhi</author>
<author>A Passerini</author>
</authors>
<title>Declarative kernels.</title>
<date>2004</date>
<tech>Technical Report RT 2/2004,</tech>
<institution>Universit`a di Firenze.</institution>
<contexts>
<context position="3473" citStr="Frasconi et al., 2004" startWordPosition="525" endWordPosition="528">pproach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf et al., 2002), adversely affecting generalization in spite of using large margin classifiers (Ben-David et al., 2002). Possible cures include extensive use of prior knowledge to guide the choice of relevant parts (Cumby and Roth, 2003; Frasconi et al., 2004), the use of feature selection (Suzuki et al., 2004), and soft matches (Saunders et al., 2002). In (Menchetti et al., 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts. In a 17 weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures. Here we introduce a position-dependent version of WDK that can solve sequence labeling problems without searching the out</context>
<context position="23020" citStr="Frasconi et al. (2004)" startWordPosition="3930" endWordPosition="3933">ble sources of generic or specialized knowledge (dictionaries, thesauri, domain specific ontologies). All these efforts are capable to address language specific sub-problems but their integration into a coherent framework is a difficult feat. Recent ideas for constructing kernel functions starting from logical representations may offer an appealing solution. G¨artner et al. (2002) have proposed a framework for defining kernels on higherorder logic individuals. Cumby and Roth (2003) used description logics to represent knowledge jointly with propositionalization for defining a kernel function. Frasconi et al. (2004) proposed kernels for handling supervised learning in a setting similar to that of inductive logic programming where data is represented as a collection of facts and background knowledge by a declarative program in first-order logic. In this section, we briefly review this approach and suggest a possible way of exploiting it for the integration of different sources of knowledge that may be available in NLP. 6.1 Declarative Kernels The definition of decomposition kernels as reported in Section 2 is very general and covers almost all kernels for discrete structured data developed in the literatu</context>
<context position="24338" citStr="Frasconi et al., 2004" startWordPosition="4144" endWordPosition="4147">ts “parts”, and specifying kernels for individual parts. In (Frasconi et al., 2004) we proposed a systematic approach to such design, consisting in formally defining a relation by the set of axioms it must satisfy. We relied on mereotopology (Varzi, 1996) (i.e. the theory of parts and places) in order to give a formal definition of the intuitive concepts of parthood and connection. The formalization of mereotopological relations allows to automatically deduce instances of such relations on the data, by exploiting the background knowledge which is typically available on structured domains. In (Frasconi et al., 2004) we introduced declarative kernels (DK) as a set of kernels working on mereotopological relations, such as that of proper parthood (gyP) or more complex relations based on connected parts. A typed syntax for objects was introduced in order to provide additional flexibility in defining kernels on instances of the given relation. A basic kernel on parts KP was defined as follows: �KP(x, x&apos;)= ST(s, s&apos;)(n(s, s&apos;)+KP(s, s&apos;))(10) 3 ≺P 2 3 ≺P 2, where ST matches objects of the same type and n is a kernel over object attributes. 22 Declarative kernels were tested in (Frasconi et al., 2004) on a number </context>
</contexts>
<marker>Frasconi, Muggleton, Lodhi, Passerini, 2004</marker>
<rawString>P. Frasconi, S. Muggleton, H. Lodhi, and A. Passerini. 2004. Declarative kernels. Technical Report RT 2/2004, Universit`a di Firenze.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G¨artner</author>
<author>P A Flach</author>
<author>A Kowalczyk</author>
<author>A J Smola</author>
</authors>
<title>Multi-instance kernels.</title>
<date>2002</date>
<booktitle>Proceedings of the 19th International Conference on Machine Learning,</booktitle>
<pages>179--186</pages>
<editor>In C. Sammut and A. Hoffmann, editors,</editor>
<publisher>Morgan Kaufmann.</publisher>
<marker>G¨artner, Flach, Kowalczyk, Smola, 2002</marker>
<rawString>T. G¨artner, P. A. Flach, A. Kowalczyk, and A. J. Smola. 2002a. Multi-instance kernels. In C. Sammut and A. Hoffmann, editors, Proceedings of the 19th International Conference on Machine Learning, pages 179–186. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G¨artner</author>
<author>J W Lloyd</author>
<author>P A Flach</author>
</authors>
<title>Kernels for structured data.</title>
<date>2002</date>
<booktitle>Proceedings of the 12th International Conference on Inductive Logic Programming,</booktitle>
<volume>2583</volume>
<pages>66--83</pages>
<editor>In S. Matwin and C. Sammut, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>G¨artner, Lloyd, Flach, 2002</marker>
<rawString>T. G¨artner, J.W. Lloyd, and P.A. Flach. 2002b. Kernels for structured data. In S. Matwin and C. Sammut, editors, Proceedings of the 12th International Conference on Inductive Logic Programming, volume 2583 of Lecture Notes in Artificial Intelligence, pages 66–83. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G¨artner</author>
</authors>
<title>A survey of kernels for structured data.</title>
<date>2003</date>
<journal>SIGKDD Explorations Newsletter,</journal>
<volume>5</volume>
<issue>1</issue>
<pages>58</pages>
<marker>G¨artner, 2003</marker>
<rawString>T. G¨artner. 2003. A survey of kernels for structured data. SIGKDD Explorations Newsletter, 5(1):49– 58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Goadrich</author>
<author>L Oliphant</author>
<author>J W Shavlik</author>
</authors>
<title>Learning ensembles of first-order clauses for recallprecision curves: A case study in biomedical information extraction.</title>
<date>2004</date>
<booktitle>In Proc. 14th Int. Conf. on Inductive Logic Programming, ILP ’04,</booktitle>
<pages>98--115</pages>
<contexts>
<context position="25047" citStr="Goadrich et al., 2004" startWordPosition="4262" endWordPosition="4265"> relations, such as that of proper parthood (gyP) or more complex relations based on connected parts. A typed syntax for objects was introduced in order to provide additional flexibility in defining kernels on instances of the given relation. A basic kernel on parts KP was defined as follows: �KP(x, x&apos;)= ST(s, s&apos;)(n(s, s&apos;)+KP(s, s&apos;))(10) 3 ≺P 2 3 ≺P 2, where ST matches objects of the same type and n is a kernel over object attributes. 22 Declarative kernels were tested in (Frasconi et al., 2004) on a number of domains with promising results, including a biomedical information extraction task (Goadrich et al., 2004) aimed at detecting protein-localization relationships within Medline abstracts. A DK on parts as the one defined in Eq. (10) outperformed state-of-the-art ILP-based systems Aleph and Gleaner (Goadrich et al., 2004) in such information extraction task, and required about three orders of magnitude less training time. 6.2 Weighted Decomposition Declarative Kernels Declarative kernels can be combined with WDK in a rather straightforward way, thus taking the advantages of both methods. A simple approach is that of using proper parthood in place of selectors, and topology to recover the context of </context>
</contexts>
<marker>Goadrich, Oliphant, Shavlik, 2004</marker>
<rawString>M. Goadrich, L. Oliphant, and J. W. Shavlik. 2004. Learning ensembles of first-order clauses for recallprecision curves: A case study in biomedical information extraction. In Proc. 14th Int. Conf. on Inductive Logic Programming, ILP ’04, pages 98–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical Report UCSC-CRL-99-10,</tech>
<institution>University of California,</institution>
<location>Santa Cruz.</location>
<contexts>
<context position="2778" citStr="Haussler, 1999" startWordPosition="422" endWordPosition="423">nd Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf et al., 2002), adversely affecting generalization in spite of using large margin classifiers (Ben-David et al., 2002). Possible cures include extensive use of prio</context>
<context position="4915" citStr="Haussler, 1999" startWordPosition="753" endWordPosition="754">els and its weighted variant. In Section 4 we introduce a version of WDK for solving supervised sequence labeling tasks and report a preliminary evaluation on a named entity recognition problem. In Section 5 we suggest a novel multi-instance approach for representing WordNet information and present an application to the PP attachment ambiguity resolution problem. In Section 6 we discuss how these ideas could be merged using a declarative formalism in order to integrate multiple sources of information when using kernelbased learning in NLP. 2 Decomposition Kernels An R-decomposition structure (Haussler, 1999; Shawe-Taylor and Cristianini, 2004) on a set X is a triple R = ( X~, R,~k) where X~ = (X1, ... , XD) is a D–tuple of non–empty subsets of X, R is a finite relation on X1 x · · · x XD x X, and k~ = (k1, ... , kD) is a D–tuple of positive definite kernel functions kd : Xd x Xd H IR. R(~x, x) is true iff x~ is a tuple of “parts” for x — i.e. x~ is a decomposition of x. Note that this definition of “parts” is very general and does not require the parthood relation to obey any specific mereological axioms, such as those that will be introduced in Section 6. For any x E X, let R−1(x) = {(x1, ... ,</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>D. Haussler. 1999. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, University of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Horv´ath</author>
<author>T G¨artner</author>
<author>S Wrobel</author>
</authors>
<title>Cyclic pattern kernels for predictive graph mining.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>158--167</pages>
<publisher>ACM Press.</publisher>
<marker>Horv´ath, G¨artner, Wrobel, 2004</marker>
<rawString>T. Horv´ath, T. G¨artner, and S. Wrobel. 2004. Cyclic pattern kernels for predictive graph mining. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 158–167. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Jebara</author>
<author>R Kondor</author>
<author>A Howard</author>
</authors>
<title>Probability product kernels.</title>
<date>2004</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>5</volume>
<pages>844</pages>
<contexts>
<context position="7689" citStr="Jebara et al., 2004" startWordPosition="1273" endWordPosition="1276">paring attributed graphs (where the selector is a single vertex and the context is the subgraph reachable from the selector within a short path). The definition is completed by introducing a kernel on selectors and a kernel on contexts. The former can be chosen to be the exact matching kernel, δ, on S x S, defined as δ(s, s&apos;) = 1 if s = s&apos; and δ(s, s&apos;) = 0 otherwise. The latter, κd, is a kernel on Zd x Zd and provides a soft similarity measure based on attribute frequencies. Several options are available for context kernels, including the discrete version of probability product kernels (PPK) (Jebara et al., 2004) and histogram intersection kernels (HIK) (Odone et al., 2005). Assuming there are n categorical attributes, each taking on mi distinct values, the context kernel can be defined as: n κd(z, z&apos;) = ki(z, z&apos;) (2) i=1 where ki is a kernel on the i-th attribute. Denote by pi(j) the observed frequency of value j in z. Then 18 ki can be defined as a HIK or a PPK respectively: ki(z, z&apos;) = Emi min{pi(j), p&apos;i(j)} (3) j=1 ki(z, z&apos;) = Emi � pi(j) - p&apos;i(j) (4) j=1 This setting results in the following general form of the kernel: EK(x, x&apos;) = (s, ~z) ∈ R−1(x) (s&apos;, ~z&apos;) ∈ R−1(x&apos;) where fve can replace the sum</context>
</contexts>
<marker>Jebara, Kondor, Howard, 2004</marker>
<rawString>T. Jebara, R. Kondor, and A. Howard. 2004. Probability product kernels. J. Mach. Learn. Res., 5:819– 844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kashima</author>
<author>T Koyanagi</author>
</authors>
<title>Kernels for Semi–Structured Data.</title>
<date>2002</date>
<booktitle>In Proceedings of the Nineteenth International Conference on Machine Learning,</booktitle>
<pages>291--298</pages>
<contexts>
<context position="2532" citStr="Kashima and Koyanagi, 2002" startWordPosition="380" endWordPosition="383">portance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the</context>
</contexts>
<marker>Kashima, Koyanagi, 2002</marker>
<rawString>H. Kashima and T. Koyanagi. 2002. Kernels for Semi–Structured Data. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 291–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kashima</author>
<author>K Tsuda</author>
<author>A Inokuchi</author>
</authors>
<title>Marginalized kernels between labeled graphs.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning,</booktitle>
<pages>321--328</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2616" citStr="Kashima et al., 2003" startWordPosition="394" endWordPosition="397">finement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf e</context>
</contexts>
<marker>Kashima, Tsuda, Inokuchi, 2003</marker>
<rawString>H. Kashima, K. Tsuda, and A. Inokuchi. 2003. Marginalized kernels between labeled graphs. In Proceedings of the Twentieth International Conference on Machine Learning, pages 321–328, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Leslie</author>
<author>E Eskin</author>
<author>W S Noble</author>
</authors>
<title>The spectrum kernel: A string kernel for SVM protein classification.</title>
<date>2002</date>
<booktitle>In Pacific Symposium on Biocomputing,</booktitle>
<pages>566--575</pages>
<contexts>
<context position="2449" citStr="Leslie et al., 2002" startWordPosition="367" endWordPosition="370">atter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth </context>
</contexts>
<marker>Leslie, Eskin, Noble, 2002</marker>
<rawString>C. S. Leslie, E. Eskin, and W. S. Noble. 2002. The spectrum kernel: A string kernel for SVM protein classification. In Pacific Symposium on Biocomputing, pages 566–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>C Saunders</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="2428" citStr="Lodhi et al., 2002" startWordPosition="363" endWordPosition="366">nel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the </context>
<context position="12664" citStr="Lodhi et al., 2002" startWordPosition="2191" endWordPosition="2194">have the same trait as extracted by ξ and the match is then weighted proportionally to the frequency count of identical words preceding and following it. The kernel version with D=6 adds two parts called zLO (LeftOther) and zRO (RightOther) defined as x[t+ 1..|r|] and x[1..t] respectively; these represent the remaining sequence parts so that x = zLL ◦ zLR ◦ zLO and x = zRL ◦ zRR ◦ zRO. Note that the WDK transforms the sentence in a bag of enriched words computed in a preprocessing phase thus achieving a significant reduction in computational complexity (compared to the recursive procedure in (Lodhi et al., 2002)). 4.1 Named Entity Recognition Experimental Results Named entities are phrases that contain the names of persons, organizations, locations, times and quantities. For example in the following sentence: [PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid]. we are interested in predicting that Wolff and Del Bosque are people’s names, that Argentina is a name of a location and that Real Madrid is a name of an organization. The chosen dataset is provided by the shared task of CoNLL–2002 (Saunders et al., </context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Menchetti</author>
<author>F Costa</author>
<author>P Frasconi</author>
</authors>
<title>Weighted decomposition kernels.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty-second International Conference on Machine Learning,</booktitle>
<pages>585--592</pages>
<location>Bonn, Germany.</location>
<contexts>
<context position="3596" citStr="Menchetti et al., 2005" startWordPosition="546" endWordPosition="549">, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf et al., 2002), adversely affecting generalization in spite of using large margin classifiers (Ben-David et al., 2002). Possible cures include extensive use of prior knowledge to guide the choice of relevant parts (Cumby and Roth, 2003; Frasconi et al., 2004), the use of feature selection (Suzuki et al., 2004), and soft matches (Saunders et al., 2002). In (Menchetti et al., 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts. In a 17 weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures. Here we introduce a position-dependent version of WDK that can solve sequence labeling problems without searching the output space, as required by other recently proposed kernel-based solutions (Tsochantaridis et al., 2004; Daum´e III and Marcu</context>
<context position="6930" citStr="Menchetti et al., 2005" startWordPosition="1136" endWordPosition="1139">ussed in the Introduction, however, taking all possible subparts into account may lead to poor predictivity because of the combinatorial explosion of the feature space. 3 Weighted Decomposition Kernels A weighted decomposition kernel (WDK) is characterized by the following decomposition structure: R = ( X~,R,(δ,κ1,...,κD)) where X~ = (S, Z1, ... , ZD), R(s, z1, ... , zD, x) is true iff s E S is a subpart of x called the selector and z~ = (z1, ... , zD) E Z1x· · ·xZD is a tuple of subparts of x called the contexts of s in x. Precise definitions of s and z~ are domain-dependent. For example in (Menchetti et al., 2005) we present two formulations, one for comparing whole sequences (where both the selector and the context are subsequences), and one for comparing attributed graphs (where the selector is a single vertex and the context is the subgraph reachable from the selector within a short path). The definition is completed by introducing a kernel on selectors and a kernel on contexts. The former can be chosen to be the exact matching kernel, δ, on S x S, defined as δ(s, s&apos;) = 1 if s = s&apos; and δ(s, s&apos;) = 0 otherwise. The latter, κd, is a kernel on Zd x Zd and provides a soft similarity measure based on attr</context>
</contexts>
<marker>Menchetti, Costa, Frasconi, 2005</marker>
<rawString>S. Menchetti, F. Costa, and P. Frasconi. 2005. Weighted decomposition kernels. In Proceedings of the Twenty-second International Conference on Machine Learning, pages 585–592, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In 42-th Conference on Association for Computational Linguistic,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2313" citStr="Moschitti, 2004" startWordPosition="346" endWordPosition="347"> of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In 42-th Conference on Association for Computational Linguistic, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Odone</author>
<author>A Barla</author>
<author>A Verri</author>
</authors>
<title>Building kernels from binary strings for image matching.</title>
<date>2005</date>
<journal>IEEE Transactions on Image Processing,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="7751" citStr="Odone et al., 2005" startWordPosition="1282" endWordPosition="1285"> and the context is the subgraph reachable from the selector within a short path). The definition is completed by introducing a kernel on selectors and a kernel on contexts. The former can be chosen to be the exact matching kernel, δ, on S x S, defined as δ(s, s&apos;) = 1 if s = s&apos; and δ(s, s&apos;) = 0 otherwise. The latter, κd, is a kernel on Zd x Zd and provides a soft similarity measure based on attribute frequencies. Several options are available for context kernels, including the discrete version of probability product kernels (PPK) (Jebara et al., 2004) and histogram intersection kernels (HIK) (Odone et al., 2005). Assuming there are n categorical attributes, each taking on mi distinct values, the context kernel can be defined as: n κd(z, z&apos;) = ki(z, z&apos;) (2) i=1 where ki is a kernel on the i-th attribute. Denote by pi(j) the observed frequency of value j in z. Then 18 ki can be defined as a HIK or a PPK respectively: ki(z, z&apos;) = Emi min{pi(j), p&apos;i(j)} (3) j=1 ki(z, z&apos;) = Emi � pi(j) - p&apos;i(j) (4) j=1 This setting results in the following general form of the kernel: EK(x, x&apos;) = (s, ~z) ∈ R−1(x) (s&apos;, ~z&apos;) ∈ R−1(x&apos;) where fve can replace the summation of kernels with Hd=1 1 + nd(zd, z&apos;d). Compared to kerne</context>
</contexts>
<marker>Odone, Barla, Verri, 2005</marker>
<rawString>F. Odone, A. Barla, and A. Verri. 2005. Building kernels from binary strings for image matching. IEEE Transactions on Image Processing, 14(2):169–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>250--255</pages>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="19790" citStr="Ratnaparkhi et al., 1994" startWordPosition="3400" endWordPosition="3404">comparing the identifiers λ and λ&apos; of these file associated to σ and σ&apos;. The resulting synset kernel is: Eκσ(σ, σ&apos;) = δ(λ, λ&apos;) + E k(π, π&apos;) (8) πEII π&apos;EII&apos; where Π is the set of paths originating from σ and the exact match kernel δ(λ, λ&apos;) is 1 if λ ≡ λ&apos; and 0 otherwise. Finally, the kernel κω between two words is itself a multi-set kernel between the corresponding sets of synsets: Eκω(ω, ω&apos;) = E κσ(σ, σ&apos;) (9) σEE σ&apos;EE&apos; where Σ are the synsets associated to the word ω. 5.2 PP Attachment Experimental Results The experiments have been performed using the Wall-Street Journal dataset described in (Ratnaparkhi et al., 1994). This dataset contains 20, 800 training examples and 3,097 testing examples. Each phrase x in the dataset is reduced to a verb xv, its object noun xn1 and prepositional phrase formed by a preposition xp and a noun xn2. The target is either V or N whether the phrase is attached to the verb or the noun. Data have been preprocessed by assigning to all the words their corresponding synsets. Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). The kernel between two phrases x and x&apos; is then computed by combining the kernels be</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>A Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In Proceedings of the ARPA Human Language Technology Workshop, pages 250–255, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Saunders</author>
<author>H Tschach</author>
<author>J Shawe-Taylor</author>
</authors>
<title>Syllables and other string kernel extensions.</title>
<date>2002</date>
<booktitle>In Proceedings of the Nineteenth International Conference on Machine Learning,</booktitle>
<pages>530--537</pages>
<contexts>
<context position="3567" citStr="Saunders et al., 2002" startWordPosition="541" endWordPosition="544">tween smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf et al., 2002), adversely affecting generalization in spite of using large margin classifiers (Ben-David et al., 2002). Possible cures include extensive use of prior knowledge to guide the choice of relevant parts (Cumby and Roth, 2003; Frasconi et al., 2004), the use of feature selection (Suzuki et al., 2004), and soft matches (Saunders et al., 2002). In (Menchetti et al., 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts. In a 17 weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures. Here we introduce a position-dependent version of WDK that can solve sequence labeling problems without searching the output space, as required by other recently proposed kernel-based solutions (Tsochantaridis et al</context>
<context position="13269" citStr="Saunders et al., 2002" startWordPosition="2293" endWordPosition="2296">dhi et al., 2002)). 4.1 Named Entity Recognition Experimental Results Named entities are phrases that contain the names of persons, organizations, locations, times and quantities. For example in the following sentence: [PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid]. we are interested in predicting that Wolff and Del Bosque are people’s names, that Argentina is a name of a location and that Real Madrid is a name of an organization. The chosen dataset is provided by the shared task of CoNLL–2002 (Saunders et al., 2002) which concerns language–independent named entity recognition. There are four types of phrases: person names (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC), combined with two tags, B to denote the first item of a phrase and I for any non–initial word; all other phrases are classified as (OTHER). Of the two available languages (Spanish and Dutch), we run experiments only on the Spanish dataset which is a collection of news wire articles made available by the Spanish EFE News Agency. We select a subset of 300 sentences for training and we evaluate the performance on t</context>
</contexts>
<marker>Saunders, Tschach, Shawe-Taylor, 2002</marker>
<rawString>C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002. Syllables and other string kernel extensions. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 530–537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>J Weston</author>
<author>E Eskin</author>
<author>C S Leslie</author>
<author>W S Noble</author>
</authors>
<title>A kernel approach for learning from almost orthogonal patterns.</title>
<date>2002</date>
<booktitle>In Proc. of ECML’02,</booktitle>
<pages>511--528</pages>
<marker>Sch¨olkopf, Weston, Eskin, Leslie, Noble, 2002</marker>
<rawString>B. Sch¨olkopf, J. Weston, E. Eskin, C. S. Leslie, and W. S. Noble. 2002. A kernel approach for learning from almost orthogonal patterns. In Proc. of ECML’02, pages 511–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1610" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="235" endWordPosition="238">g and other data driven approaches. In particular, several subproblems related to information extraction can be formulated in the supervised learning framework, where statistical learning has rapidly become one of the preferred methods of choice. A common characteristic of many NLP problems is the relational and structured nature of the representations that describe data and that are internally used by various algorithms. Hence, in order to develop effective learning algorithms, it is necessary to cope with the inherent structure that characterize linguistic entities. Kernel methods (see e.g. Shawe-Taylor and Cristianini, 2004) are well suited to handle learning tasks in structured domains as the statistical side of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations betwee</context>
<context position="4952" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="755" endWordPosition="758">hted variant. In Section 4 we introduce a version of WDK for solving supervised sequence labeling tasks and report a preliminary evaluation on a named entity recognition problem. In Section 5 we suggest a novel multi-instance approach for representing WordNet information and present an application to the PP attachment ambiguity resolution problem. In Section 6 we discuss how these ideas could be merged using a declarative formalism in order to integrate multiple sources of information when using kernelbased learning in NLP. 2 Decomposition Kernels An R-decomposition structure (Haussler, 1999; Shawe-Taylor and Cristianini, 2004) on a set X is a triple R = ( X~, R,~k) where X~ = (X1, ... , XD) is a D–tuple of non–empty subsets of X, R is a finite relation on X1 x · · · x XD x X, and k~ = (k1, ... , kD) is a D–tuple of positive definite kernel functions kd : Xd x Xd H IR. R(~x, x) is true iff x~ is a tuple of “parts” for x — i.e. x~ is a decomposition of x. Note that this definition of “parts” is very general and does not require the parthood relation to obey any specific mereological axioms, such as those that will be introduced in Section 6. For any x E X, let R−1(x) = {(x1, ... , xD) E X~ : R(~x, x)} denote the mult</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Siolas</author>
<author>F d’Alche Buc</author>
</authors>
<title>Support vector machines based on a semantic kernel for text categorization.</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks,</booktitle>
<volume>5</volume>
<pages>205--209</pages>
<marker>Siolas, Buc, 2000</marker>
<rawString>G. Siolas and F. d’Alche Buc. 2000. Support vector machines based on a semantic kernel for text categorization. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks, volume 5, pages 205 – 209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>R Kondor</author>
</authors>
<title>Kernels and regularization on graphs.</title>
<date>2003</date>
<booktitle>16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003,</booktitle>
<volume>2777</volume>
<pages>144--158</pages>
<editor>In B. Sch¨olkopf and M.K. Warmuth, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2594" citStr="Smola and Kondor, 2003" startWordPosition="389" endWordPosition="393">d include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and flexible approach for defining the similarity between structured objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly </context>
</contexts>
<marker>Smola, Kondor, 2003</marker>
<rawString>A.J. Smola and R. Kondor. 2003. Kernels and regularization on graphs. In B. Sch¨olkopf and M.K. Warmuth, editors, 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, volume 2777 of Lecture Notes in Computer Science, pages 144–158. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Stetina</author>
<author>M Nagao</author>
</authors>
<title>Corpus based pp attachment ambiguity resolution with a semantic dictionary.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Workshop on Very Large Corpora,</booktitle>
<pages>66--80</pages>
<location>Beijing, China.</location>
<contexts>
<context position="20304" citStr="Stetina and Nagao, 1997" startWordPosition="3493" endWordPosition="3496"> experiments have been performed using the Wall-Street Journal dataset described in (Ratnaparkhi et al., 1994). This dataset contains 20, 800 training examples and 3,097 testing examples. Each phrase x in the dataset is reduced to a verb xv, its object noun xn1 and prepositional phrase formed by a preposition xp and a noun xn2. The target is either V or N whether the phrase is attached to the verb or the noun. Data have been preprocessed by assigning to all the words their corresponding synsets. Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). The kernel between two phrases x and x&apos; is then computed by combining the kernels between single words using either the sum or the product. 21 Method Acc Pre Rec S 84.6% + 0.65% 90.8% 82.2% P 84.8% + 0.65% 92.2% 81.0% SW 85.4% + 0.64% 90.9% 83.6% SWL 85.3% + 0.64% 91.1% 83.2% PW 85.9% + 0.63% 92.2% 83.1% PWL 86.2% + 0.62% 92.1% 83.7% Table 3: Summary of the experimental results on the PP attachment problem for various kernel parameters. Results of the experiments are reported in Tab. 3 for various kernels parameters: S or P denote if the sum or product of the kernels between words are used, </context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>J Stetina and M Nagao. 1997. Corpus based pp attachment ambiguity resolution with a semantic dictionary. In Proceedings of the Fifth Workshop on Very Large Corpora, pages 66–80, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>E Maeda</author>
</authors>
<title>Convolution kernels with feature selection for natural language processing tasks.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>119--126</pages>
<contexts>
<context position="3525" citStr="Suzuki et al., 2004" startWordPosition="534" endWordPosition="537"> objects starting from the similarity between smaller parts. However, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size. When too many dimensions in the feature space are irrelevant, the Gram matrix will be nearly diagonal (Sch¨olkopf et al., 2002), adversely affecting generalization in spite of using large margin classifiers (Ben-David et al., 2002). Possible cures include extensive use of prior knowledge to guide the choice of relevant parts (Cumby and Roth, 2003; Frasconi et al., 2004), the use of feature selection (Suzuki et al., 2004), and soft matches (Saunders et al., 2002). In (Menchetti et al., 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts. In a 17 weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures. Here we introduce a position-dependent version of WDK that can solve sequence labeling problems without searching the output space, as required by other recently proposed ke</context>
</contexts>
<marker>Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>J. Suzuki, H. Isozaki, and E. Maeda. 2004. Convolution kernels with feature selection for natural language processing tasks. In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proc. 21st Int. Conf. on Machine Learning,</booktitle>
<pages>823--830</pages>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="2129" citStr="Tsochantaridis et al., 2004" startWordPosition="316" endWordPosition="319">structure that characterize linguistic entities. Kernel methods (see e.g. Shawe-Taylor and Cristianini, 2004) are well suited to handle learning tasks in structured domains as the statistical side of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution</context>
<context position="4174" citStr="Tsochantaridis et al., 2004" startWordPosition="631" endWordPosition="635">unders et al., 2002). In (Menchetti et al., 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts. In a 17 weighted decomposition kernel (WDK) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures. Here we introduce a position-dependent version of WDK that can solve sequence labeling problems without searching the output space, as required by other recently proposed kernel-based solutions (Tsochantaridis et al., 2004; Daum´e III and Marcu, 2005). The paper is organized as follows. In the next two sections we briefly review decomposition kernels and its weighted variant. In Section 4 we introduce a version of WDK for solving supervised sequence labeling tasks and report a preliminary evaluation on a named entity recognition problem. In Section 5 we suggest a novel multi-instance approach for representing WordNet information and present an application to the PP attachment ambiguity resolution problem. In Section 6 we discuss how these ideas could be merged using a declarative formalism in order to integrate</context>
<context position="9241" citStr="Tsochantaridis et al., 2004" startWordPosition="1543" endWordPosition="1547">ructures being compared. 4 WDK for sequence labeling and applications to NER In a sequence labeling task we want to map input sequences to output sequences, or, more precisely, we want to map each element of an input sequence that takes label from a source alphabet to an element with label in a destination alphabet. Here we cast the sequence labeling task into position specific classification, where different sequence positions give independent examples. This is different from previous approaches in the literature where the sequence labeling problem is solved by searching in the output space (Tsochantaridis et al., 2004; Daum´e III and Marcu, 2005). Although the method lacks the potential for collectively labeling all positions simultaneously, it results in a much more efficient algorithm. In the remainder of the section we introduce a specialized version of the weighted decomposition kernel suitable for a sequence transduction task originating in the natural language processing domain: the named entity recognition (NER) problem, where we map sentences to sequences of a reduced number of named entities (see Sec.4.1). More formally, given a finite dictionary E of words and an input sentence x E E*, our input </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proc. 21st Int. Conf. on Machine Learning, pages 823– 830, Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Varzi</author>
</authors>
<title>Parts, wholes, and part-whole relations: the prospects of mereotopology.</title>
<date>1996</date>
<journal>Data and Knowledge Engineering,</journal>
<pages>20--259</pages>
<contexts>
<context position="23971" citStr="Varzi, 1996" startWordPosition="4087" endWordPosition="4088">gration of different sources of knowledge that may be available in NLP. 6.1 Declarative Kernels The definition of decomposition kernels as reported in Section 2 is very general and covers almost all kernels for discrete structured data developed in the literature so far. Different kernels are designed by defining the relation decomposing an example into its “parts”, and specifying kernels for individual parts. In (Frasconi et al., 2004) we proposed a systematic approach to such design, consisting in formally defining a relation by the set of axioms it must satisfy. We relied on mereotopology (Varzi, 1996) (i.e. the theory of parts and places) in order to give a formal definition of the intuitive concepts of parthood and connection. The formalization of mereotopological relations allows to automatically deduce instances of such relations on the data, by exploiting the background knowledge which is typically available on structured domains. In (Frasconi et al., 2004) we introduced declarative kernels (DK) as a set of kernels working on mereotopological relations, such as that of proper parthood (gyP) or more complex relations based on connected parts. A typed syntax for objects was introduced in</context>
</contexts>
<marker>Varzi, 1996</marker>
<rawString>A.C. Varzi. 1996. Parts, wholes, and part-whole relations: the prospects of mereotopology. Data and Knowledge Engineering, 20:259–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="2242" citStr="Zelenko et al., 2003" startWordPosition="333" endWordPosition="336">ited to handle learning tasks in structured domains as the statistical side of a learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function. As a matter of facts, kernel-based statistical learning has gained substantial importance in the NLP field. Applications are numerous and diverse and include for example refinement of statistical parsers (Collins and Duffy, 2002), tagging named entities (Cumby and Roth, 2003; Tsochantaridis et al., 2004), syntactic chunking (Daum´e III and Marcu, 2005), extraction of relations between entities (Zelenko et al., 2003; Culotta and Sorensen, 2004), semantic role labeling (Moschitti, 2004). The literature is rich with examples of kernels on discrete data structures such as sequences (Lodhi et al., 2002; Leslie et al., 2002; Cortes et al., 2004), trees (Collins and Duffy, 2002; Kashima and Koyanagi, 2002), and annotated graphs (G¨artner, 2003; Smola and Kondor, 2003; Kashima et al., 2003; Horv´ath et al., 2004). Kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (Haussler, 1999). Thanks to its generality, decomposition is an attractive and f</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>