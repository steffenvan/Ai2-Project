<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000198">
<title confidence="0.603773">
Assigning Domains to Speech Recognition Hypotheses
Klaus R¨uggenmann and Iryna Gurevych
EML Research gGmbH
</title>
<address confidence="0.654035">
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</address>
<email confidence="0.764012">
trueggenmann,gurevychl@eml-r.villa-bosch.de
</email>
<sectionHeader confidence="0.996906" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994785714286">
We present the results of experiments
aimed at assigning domains to speech
recognition hypotheses (SRH). The meth-
ods rely on high-level linguistic repre-
sentations of SRHs as sets of ontolog-
ical concepts. We experimented with
two domain models and evaluated their
performance against a statistical, word-
based model. Our hand-annotated and
tf*idf-based models yielded a precision
of 88,39% and 82,59% respectively, com-
pared to 93,14% for the word-based base-
line model. These results are explained in
terms of our experimental setup.
</bodyText>
<sectionHeader confidence="0.993106" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999673897959184">
High-level linguistic knowledge has been shown to
have the potential of improving the state of the art
in automatic speech recognition (ASR). Such know-
ledge can be integrated in the ASR component (Gao,
2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya
et al., 2003; Taylor et al., 2000). Alternatively,
it may be included in the processing pipeline at a
later stage, namely at the interface between the au-
tomatic speech recognizer and the spoken language
understanding component (Gurevych et al., 2003a;
Gurevych and Porzel, 2003).
In any of these cases, it is necessary to provide a
systematic account of domain and world knowledge.
These types of knowledge have largely been ignored
so far in ASR research. The reason for this state of
affairs lies in the fact that the manual construction of
appropriate knowledge sources for broad domains is
extremely costly. Also, easy domain portability is
an important requirement for any ASR system. The
emergence of wide coverage linguistic knowledge
bases for multiple languages, such as WordNet (Fell-
baum, 1998), FrameNet (Baker et al., 1998; Baker et
al., 2003), PropBank (Palmer et al., 2003; Xue et al.,
2004) is likely to change this situation.
Domain recognition, which is the central topic of
this paper, can be thought of as high-level seman-
tic tagging of utterances. We expect significant im-
provements in the performance of the ASR compo-
nent of the system if information about the current
domain of discourse is available. An obvious intu-
ition behind this expectation is that knowing the cur-
rent domain of discourse narrows down the search
space of the speech recognizer. It also allows to
rule out incoherent speech recognition hypotheses as
well as those which do not fit in a given domain.
Apart from that, there are additional important
reasons for the inclusion of information about the
current domain in any spoken language process-
ing (SLP) system. Current SLP systems deal
not only with a single, but with multiple do-
mains, e.g., Levin et al. (2000), Itou et al. (2001),
Wahlster et al. (2001). In fact, the development of
multi-domain systems is one of the new research di-
rections in SLP, which makes the issue of automati-
cally assigning domains to utterances especially im-
portant. This type of knowledge can be effectively
utilized at different stages of the spoken language
and multi-domain input processing in the following
ways:
</bodyText>
<listItem confidence="0.80840775">
• optimizing the performance of the speech rec-
ognizer;
• improving the performance of the dialogue
manager, e.g., if a domain change occurred in
the discourse;
• dynamic loading of resources, e.g. speech rec-
ognizer lexicons or dialogue plans, especially
in mobile environments.
</listItem>
<bodyText confidence="0.99989725">
Here, we present the results of research directed
at automatic assigning of domains to speech recog-
nition hypotheses. In Section 2, we briefly introduce
the knowledge sources in our experiments, such as
the ontology, the lexicon and domain models. The
data and annotation experiments will be presented in
Section 3, followed by the detailed description of the
domain classification algorithms in Section 4. Sec-
tion 5 will give the evaluation results for the linguis-
tically motivated conceptual as well as purely statis-
tical models. Conclusions and some future research
directions can be found in Section 6.
</bodyText>
<sectionHeader confidence="0.830011" genericHeader="introduction">
2 High-Level Knowledge Sources
</sectionHeader>
<subsectionHeader confidence="0.980848">
2.1 Ontology and lexicon
</subsectionHeader>
<bodyText confidence="0.999632347826087">
Current SLP systems often employ multi-
domain ontologies representing the relevant
world and discourse knowledge. The know-
ledge encoded in such an ontology can be
applied to a variety of natural language pro-
cessing tasks, e.g. Mahesh and Nirenburg (1995),
Flycht-Eriksson (2003).
Our ontology models the domains Electronic Pro-
gram Guide, Interaction Management, Cinema In-
formation, Personal Assistance, Route Planning,
Sights, Home Appliances Control and Off Talk.
The hierarchically structured ontology consists of
ca. 720 concepts and 230 properties specifying rela-
tions between concepts. For example every instance
of the concept Process features the relations
hasBeginTime, hasEndTime and hasState.
A detailed description of the ontology employed in
our experiments is given in Gurevych et al. (2003b).
Ontological concepts are high-level units. They
allow to reduce the amount of information needed to
represent relations existing between individual lex-
emes and to effectively incorporate this knowledge
into automatic language processing. E.g., there may
exist a large number of movies in a cinema reser-
vation system. All of them will be represented by
the concept Movie, thus allowing to map a variety
of lexical items (instances) to a single unit (concept)
describing their meaning and the relations to other
concepts in a generic way.
We did not use the structure of the ontology in
an explicit way in the reported experiments. The
knowledge was used implicitly to come up with a
set of ontological concepts needed to represent the
user’s utterance.
The high-level domain knowledge represented in
the ontology is linked with the language-specific
knowledge through a lexicon. The lexicon con-
tains ca. 3600 entries of lexical items and their
senses (0 or more), encoded as concepts in the
ontology. E.g., the word am is mapped to the onto-
logical concepts StaticSpatialProcess
as in the utterance I am in New York,
SelfIdentificationProcess as in the
utterance I am Peter Smith, and NONE, if the lexeme
has a grammatical function only, e.g., I am going to
read a book.
</bodyText>
<subsectionHeader confidence="0.998278">
2.2 Domain models
</subsectionHeader>
<bodyText confidence="0.99999105">
For scoring high-level linguistic representations of
utterances we use a domain model. A domain model
is a two-dimensional matrix DM with the dimen-
sions (#d × #c), where #d and #c denote the
overall number of domain categories and ontologi-
cal concepts, respectively. This can be formalized
as: DM = (Sdc)d=1,...,#d,c=1,...,#c, where the ma-
trix elements Sdc are domain specificity scores of
individual concepts.
We experimented with two different domain mod-
els. The first model DManno was obtained through
direct annotation of concepts with respect to do-
mains as reported in Section 3.2. The second domain
model DMtf*idf resulted from statistical analysis of
Dataset 1 (described in Section 3.1). In this case,
we computed the term frequency - inverse document
frequency (tf*idf) score (Salton and Buckley, 1988)
of each concept for individual domains. In the case
of human annotations, we deal with binary values,
whereas tf*idf scores range over the interval [0,1].
</bodyText>
<sectionHeader confidence="0.968688" genericHeader="method">
3 Data and Annotation Experiments
</sectionHeader>
<bodyText confidence="0.9998825">
We performed a number of annotation experiments.
The purpose of these experiments was to:
</bodyText>
<listItem confidence="0.997882285714286">
• investigate the reliability of the annotations;
• create a domain model based on human anno-
tations;
• produce a training dataset for statistical classi-
fiers;
• set a Gold Standard as a test dataset for the
evaluation.
</listItem>
<bodyText confidence="0.998787714285714">
All annotation experiments were conducted on
data collected in hidden-operator tests following
the paradigm described in Rapp and Strube (2002).
Subjects were asked to verbalize a predefined inten-
tion in each of their turns, the system’s reaction was
simulated by a human operator. We collected ut-
terances from 29 subjects in 8 dialogues with the
system each. All user turns were recorded in sep-
arate audio files. These audio files were processed
by two versions of our dialogue system with differ-
ent speech recognition modules. Data describing our
corpora is given in Table 1. The first and the sec-
ond system’s runs are referred to as Dataset 1 and
Dataset 2 respectively.
</bodyText>
<table confidence="0.936167">
Dataset 1 Dataset 2
Number of dialogues 232 95
Number of utterances 1479 552
Number of SRHs 2.239 1.375
Number of coherent SRHs 1511 867
Number of incoherent SRHs 728 508
</table>
<tableCaption confidence="0.999396">
Table 1: Descriptive corpus statistics.
</tableCaption>
<bodyText confidence="0.999950076923077">
The corpora obtained from these experiments
were further transformed into a set of annotation
files, which can be read into GUI-based annotation
tools, e.g., MMAX (M¨uller and Strube, 2003). This
tool can be adopted for annotating different levels of
information, e.g., semantic coherence and domains
of utterances, the best speech recognition hypothe-
sis in the N-best list, as well as domains of individual
concepts. The two annotators were trained with the
help of an annotation manual. A reconciled version
of both annotations resulted in the Gold Standard.
In the following, we present the results of our anno-
tation experiments.
</bodyText>
<subsectionHeader confidence="0.988168">
3.1 Coherence, domains of SRHs in Dataset 1
</subsectionHeader>
<bodyText confidence="0.999933916666667">
The first experiment was aimed at annotating the
speech recognition hypotheses (SRH) from Dataset
1 w.r.t. their domains. This process was two-staged.
In the first stage, the annotators labeled randomly
mixed SRHs, i.e. SRHs without discourse context,
for their semantic coherence as coherent or incoher-
ent. In the second stage, coherent SRHs were la-
beled for their domains, resulting in a corpus of 1511
hypotheses labeled for at least one domain category.
The numbers for ambiguous domain attributions can
be found in Table 2. The class distribution is given
in Table 3.
</bodyText>
<table confidence="0.7398832">
Number of domains Annotator 1
1 90.06% 87.11%
2 6.94% 11.27%
3 3.01% 1.28%
4 0% 0.35%
</table>
<tableCaption confidence="0.976025">
Table 2: Multiple domain assignments in Dataset 1.
</tableCaption>
<table confidence="0.999560222222222">
Annotator 1 Annotator 2
Electr. Program Guide 14.43% 14.86%
Interaction Management 15.56% 15.17%
Cinema Information 5.32% 8.7%
Personal Assistance 0.31% 0.3%
Route Planning 37.05% 36%
Sights 12.49% 12.74%
Home Appliances Control 14.12% 11.22%
Off Talk 0.72% 1.01%
</table>
<tableCaption confidence="0.97541">
Table 3: Class distribution for domain assignments.
</tableCaption>
<table confidence="0.999883444444444">
P(A) P(E) Kappa
Electr. Program Guide 0.9743 0.7246 0.9066
Interaction Management 0.9836 0.7107 0.9434
Cinema Information 0.9661 0.8506 0.7229
Personal Assistance 0.9953 0.9930 0.3310
Route Planning 0.9777 0.5119 0.9544
Sights 0.9731 0.7629 0.8865
Home Appliances Control 0.9626 0.7504 0.8501
Off Talk 0.9871 0.9780 0.4145
</table>
<tableCaption confidence="0.999727">
Table 4: Kappa coefficient for separate domains.
</tableCaption>
<bodyText confidence="0.995105">
Table 4 presents the Kappa coefficient values
computed for individual categories. P(A) is the per-
centage of agreement between annotators. P(E) is
the percentage we expect them to agree by chance.
The annotations are generally considered to be reli-
able if K &gt; 0.8. This is true for all classes except
those which occur very rarely on our data.
</bodyText>
<subsectionHeader confidence="0.99981">
3.2 Domains of ontological concepts
</subsectionHeader>
<bodyText confidence="0.993754647058824">
In the second experiment, ontological concepts were
annotated with zero or more domain categories.1 We
&apos;Top-level concepts like Event are typically not domain-
specific. Therefore, they will not be assigned any domains.
Annotator 2
extracted 231 concepts from the lexicon, which is a
subset of ontological concepts relevant for our cor-
pus of SRHs. The annotators were given the tex-
tual descriptions of all concepts. These definitions
are supplied with the ontology. We computed two
kinds of inter-annotator agreement. In the first case,
we calculated the percentage of concepts, for which
the annotators agreed on all domain categories, re-
sulting in ca. 47.62% (CONCabs, see Figure 1). In
the second case, the agreement on individual domain
decisions (1848 overall) was computed, ca. 86.85%
(CONCindiv, see Figure 1).
</bodyText>
<subsectionHeader confidence="0.961495">
3.3 Best conceptual representation and
domains of SRHs in Dataset 2
</subsectionHeader>
<bodyText confidence="0.994083192307692">
As will be evident from Section 4.1, each SRH can
be mapped to a set of possible interpretations, which
are called conceptual representations (CR). In this
experiment, the best conceptual representation and
the domains of coherent SRHs from Dataset 2 were
annotated. As our system operates on the basis of
CR, it is necessary to disambiguate them in a pre-
processing step.
867 SRHs used in this experiment are mapped to
2853 CR, i.e. on average each SRH is mapped to
3.29 CR. The annotators’ agreement on the task of
determining the best CR reached ca. 88.93%.
For the task of domain annotation, again, we com-
puted the absolute agreement, when the annotators
agreed on all domains for a given SRH. This resulted
in ca. 92.5% (SRHabs, see Figure 1). The agree-
ment on individual domain decisions (6936 over-
all) yielded ca. 98.92% (SRHindiv, see Figure 1).
As the Figure 1 suggests, annotating utterances with
domains is an easier task for humans than annotat-
ing ontological concepts with the same information.
One possible reason for this is that even for an iso-
lated SRH of an utterance there is at least some lo-
cal context available, which clarifies its high-level
meaning to some extent. An isolated concept has no
defining context whatsoever.
</bodyText>
<sectionHeader confidence="0.99818" genericHeader="method">
4 Domain Classification
</sectionHeader>
<bodyText confidence="0.999773">
In this section, we present the algorithms employed
for assigning domains to speech recognition hy-
potheses. The system called DOMSCORE performs
several processing steps, each of which will be de-
</bodyText>
<figureCaption confidence="0.905448666666667">
Figure 1: Agreement in % on domain annotations
for concepts and SRHs. Absolute agreement (CON-
Cabs, SRHabs) means that annotators agreed on
all domains. Individual agreement (CONCindiv,
SRHindiv) refers to identical individual domain de-
cisions.
</figureCaption>
<bodyText confidence="0.854049">
scribed separately in the respective subsections.
</bodyText>
<subsectionHeader confidence="0.702229">
4.1 From SRHs to conceptual representations
</subsectionHeader>
<bodyText confidence="0.868483222222222">
SRH is a set of words W = {w1, ..., wn}. DOM-
SCORE operates on high-level representations of
SRHs as conceptual representations (CR). CR is
a set of ontological concepts CR = {c1, ..., cn}.
Conceptual representations are obtained from W
through the process called word-to-concept map-
ping. In this process, all possible ontological senses
corresponding to individual words in the lexicon are
permutated resulting in a set I of possible interpre-
tations I = {CR1, ..., CRn} for each speech recog-
nition hypothesis.
For example, in our data a user formulated the
query concerning the TV program, as:2
(1) Und was f¨ur Spielfilme kommen
And which movies come
heute abend
tonight
This utterance resulted in the following SRHs:
</bodyText>
<footnote confidence="0.8615565">
2All examples are displayed with the German original and a
glossed translation.
</footnote>
<table confidence="0.822355142857143">
SRH1 Was f¨ur Spielfilme kommen heute abend as the average score of all concepts in CR for this
Which movies come tonight domain. For a given domain model DM, this for-
mally means:
SRH2 Was f¨ur kommen heute abend �n Sd,i
Which come tonight 1
SCR(d) = n
i=1
</table>
<bodyText confidence="0.985548714285714">
The two hypotheses have two conceptual represen-
tations each. This is due to the lexical ambiguity
of the word come as either MotionProcess or
WatchProcess in German. Movie in SRH1 is
mapped to Broadcast. As a consequence, the
permutation yields CR1a,1b for SRH1 and CR2a,2b
for SRH2:
</bodyText>
<footnote confidence="0.908957">
CR1a: {Broadcast, MotionProcess}
CR1b: {Broadcast, WatchProcess}
CR2a: {MotionProcess}
CR2b: {WatchProcess}
</footnote>
<bodyText confidence="0.5318625">
In Tables 5 and 6, the domain specificity scores
Sdc for all concepts of Example 1 are given.
</bodyText>
<table confidence="0.999302333333333">
Broadcast Motion Watch
Electr. Program Guide 1 0 1
Interaction Management 0 0 0
Cinema Information 0 0 1
Personal Assistance 0 0 0
Route Planning 0 1 1
Sights 0 0 1
Home Appliances Control 1 0 0
Off Talk 0 0 0
</table>
<tableCaption confidence="0.9837655">
Table 5: Matrix DManno derived from human anno-
tations.
</tableCaption>
<table confidence="0.999814666666667">
Broadcast Motion Watch
Electr. Program Guide 1 0.496 0.744
Interaction Management 0 0 0
Cinema Information 0.283 0.178 0.043
Personal Assistance 0 0 0
Route Planning 0 0.689 0.044
Sights 0 0.020 0.079
Home Appliances Control 0.494 0.027 0.147
Off Talk 0 0.238 0.374
</table>
<tableCaption confidence="0.993104">
Table 6: Matrix DMtf.idf derived from the anno-
tated corpus.
</tableCaption>
<subsectionHeader confidence="0.922317">
4.2 Domain classification of CR
</subsectionHeader>
<bodyText confidence="0.9991894">
The domain specificity score S of the conceptual
representation CR for the domain d is, then, defined
where n is the number of concepts in the respective
CR. As each CR is scored for all domains d, the
output of DOmSCORE is a set of domain scores:
</bodyText>
<equation confidence="0.940616">
SCR = {Sd1, ..., S#d}
</equation>
<bodyText confidence="0.99640975">
where #d is the number of domain categories.
Tables 7 and 8 display the results of the domain
scoring algorithm for the conceptual representations
of Example 1.
</bodyText>
<table confidence="0.9979539">
SRH1 SRH2
CR1a CR1b CR2a CR2b
Electr. Program Guide 0.5 1.0 0 1.0
Interaction Management 0 0 0 0
Cinema Information 0 0.5 0 1.0
Personal Assistance 0 0 0 0
Route Planning 0.5 0.5 1.0 1.0
Sights 0 0.5 0 1.0
Home Appliances Control 0.5 0.5 0 0
Off Talk 0 0 0 0
</table>
<tableCaption confidence="0.962613">
Table 7: Domain scores on the basis of DManno.
</tableCaption>
<table confidence="0.9997255">
SRH1 SRH2
CR1a CR1b CR2a CR2b
Electr. Program Guide 0.748 0.872 0.496 0.744
Interaction Management 0 0 0 0
Cinema Information 0.231 0.163 0.178 0.043
Personal Asssitance 0 0 0 0
Route Planning 0.344 0.022 0.689 0.044
Sights 0.01 0.04 0.02 0.079
Home Appliances Control 0.26 0.32 0.027 0.147
Off Talk 0.119 0.187 0.238 0.374
</table>
<tableCaption confidence="0.999469">
Table 8: Domain scores on the basis of DMtf.idf.
</tableCaption>
<bodyText confidence="0.999712">
In the Gold Standard evaluation data, SRH1 was
annotated as the best SRH and attributed the do-
main Electronic Program Guide, CR1b was selected
as its best conceptual representation. As can be seen
in the above tables, this CR1b gets the highest do-
main score for Electronic Program Guide on the ba-
sis of both DManno and DMtf.idf . Consequently,
both domain models attribute this domain to SRH1.
SRH2 was not labeled with any domains in the
Gold Standard, as this hypothesis is an incoherent
one and hence cannot be considered to belong to
any domain at all. According to DManno, its rep-
resentation CR2a gets a single score 1 for the do-
main Route Planning and CR2b gets multiple equal
scores. DOMSCORE interprets a single score as a
more reliable indicator for a specific domain than
multiple equal scores and assigns the domain Route
Planning to SRH2. On the basis of DMtf*idf the
highest overall score for CR2a,2b is the one for do-
main Electronic Program Guide. Therefore, the
model will assign this domain to SRH2.
</bodyText>
<subsectionHeader confidence="0.997868">
4.3 Word2Concept ratio
</subsectionHeader>
<bodyText confidence="0.9998278">
In previous experiments (Gurevych et al., 2003a),
we found that when operating on sets of concepts
as representations of speech recognition hypotheses,
the ratio of the number of ontological concepts n in
a given CR and the total number of words w in the
respective SRH must be accounted for. This relation
is defined by the ratio R = n/w.
The idea is to prevent an incoherent SRH contain-
ing many function words with zero concept map-
pings, represented by a single concept in the ex-
treme, from being classified as coherent. Exper-
imental results indicate that the optimal threshold
R should be set to 0.33. This means that if there
are more than three words corresponding to a single
concept on average, the SRH is likely to be incoher-
ent and should be excluded from processing.
DOMSCORE implements this as apost-processing
technique. For both conceptual representations of
SRH1 the ratio is R = 1/3, whereas for those of
SRH2, we find R = 1/5. This value is under the
threshold, which means that SRH2 is considered in-
coherent and its domain scores are dropped. Finally,
this results in both models assigning the single do-
main Electronic Program Guide as the best one to
the utterance in Example 1.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998199">
5.1 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999979851851852">
The evaluation of the algorithms and domain mod-
els presented herein poses a methodological prob-
lem. As stated in Section 3.3, the annotators were
allowed to assign 1 or more domains to an SRH, so
the number of domain categories varies in the Gold
Standard data. The output of DOMSCORE, however,
is a set with confidence values for all domains rang-
ing from 0 to 1. To the best of our knowledge, there
exists no evaluation method that allows the straight-
forward evaluation of these confidence sets against
the varying number of binary domain decisions.
As a consequence, we restricted the evaluation to
the subset of 758 SRHs unambiguously annotated
for a single domain in Dataset 2. For each SRH
we compared the recognized domain of its best CR
with the annotated domain. This recognized domain
is the one that was scored the highest confidence by
DOMSCORE. In this way we measured the precision
on recognizing the best domain of an SRH. The best
conceptual representation of an SRH had been previ-
ously disambiguated by humans as reported in Sec-
tion 3.3. Alternatively, this kind of disambiguation
can be performed automatically, e.g., with the help
of the system presented in Gurevych et al. (2003a).
The system scores semantic coherence of SRHs,
where the best CR is the one with the highest se-
mantic coherence.
</bodyText>
<subsectionHeader confidence="0.911115">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9966542">
We included two baselines in this evaluation. As as-
signing domains to speech recognition hypotheses
is a classification task, the majority class frequency
can serve as a first baseline. For a second base-
line, we trained a statistical classifier employing the
k-nearest neighbour method using Dataset 1. This
dataset had also been employed to create the tf*idf
model. The statistical classifier treated each SRH as
a bag of words or bag of concepts labeled with do-
main categories.
</bodyText>
<figureCaption confidence="0.909088">
Figure 2: Precision on domain assignments.
The results of DOMSCORE employing the hand-
</figureCaption>
<bodyText confidence="0.999954166666667">
annotated and tf*idf domain models as well as
the baseline systems’ performances are displayed
in Figure 2. The diagram shows that all sys-
tems clearly outperform the majority class base-
line. The hand-annotated domain model (precision
88.39%) outperforms the tf*idf domain model (pre-
cision 82.59%). The model created by humans turns
out to be of higher quality than the automatically
computed one. However, the k-nearest neighbour
baseline with words as features performs better (pre-
cision 93.14%) than the other methods employing
ontological concepts as representations.
</bodyText>
<subsectionHeader confidence="0.965486">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999406333333333">
We believe that this finding can be explained in
terms of our experimental setup which favours the
statistical model. Table 9 gives the absolute fre-
quency for all domain categories in the evaluation
data. As the data implies, three of the possible cate-
gories are missing in the data.
</bodyText>
<table confidence="0.999308">
Number of instances
Electr. Program Guide 74
Interaction Management 85
Cinema Information 0
Personal Assistance 0
Route Planning 385
Sights 150
Home Appliances Control 64
Off Talk 0
</table>
<tableCaption confidence="0.999274">
Table 9: Class distribution in the evaluation dataset.
</tableCaption>
<bodyText confidence="0.999963914285714">
The main reason for our results, however, lies in
the controlled experimental setup of the data col-
lection. Subjects had to verbalize pre-defined in-
tentions in 8 scenarios, e.g. record a specific pro-
gram on TV or ask for information regarding a given
historical sight. Naturally, this leads to restricted
man-machine interactions using controlled vocabu-
lary. As a result, there is rather limited lexical vari-
ation in the data. This is unfortunate for illustrat-
ing the strengths of high-level ontological represen-
tations.
In our opinion, the power of ontological represen-
tations is just their ability to reduce multiple lexi-
cal surface realizations of the same concept to a sin-
gle unit, thus representing the meaning of multiple
words in a compact way. This effect could not be
exploited in a due way given the test corpora in these
experiments. We expect a better performance of
concept-based methods as compared to word-based
ones in broader domains.
An additional important point to consider is the
portability of the domain recognition approach. Sta-
tistical models, e.g., tf*idf and k-nearest neighbour
rely on substantial amounts of annotated data when
moving to new domains. Such data is difficult to
obtain and requires expensive human efforts for an-
notation. When the manually created domain model
is employed for the domain classification task, the
extension of knowledge sources to a new domain
boils down to extending the list of concepts with
some additional ones and annotating them for do-
mains. These new concepts are part of the extension
of the system’s general ontology, which is not cre-
ated specifically for domain classification, but em-
ployed for many purposes in the system.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999973725">
In this paper, we presented a system which de-
termines domains of speech recognition hypothe-
ses. Our approach incorporates high-level semantic
knowledge encoded in a domain model of ontologi-
cal concepts. We believe that this type of semantic
information has the potential to improve the perfor-
mance of the automatic speech recognizer, as well
as other components of spoken language processing
systems.
Basically, information about the current domain
of discourse is a type of contextual knowledge. One
of the future challenges will be to find ways of
including this high-level semantic knowledge into
SLP systems in the most beneficial way. It remains
to be studied how to integrate semantic processing
into the architecture, including speech recognition
and discourse processing.
An important aspect of the scalability of our
methods is their dependence on concept-based do-
main models. A natural extension would be to re-
place hand-crafted ontological concepts with, e.g.,
WordNet concepts. The structure of WordNet can
then be used to determine high-level domain con-
cepts that can replace human domain annotations.
One of the evident problems with this approach is,
however, the high level of lexical ambiguity of the
WordNet concepts. Apparently, the problem of am-
biguity scales up together with the coverage of the
respective knowledge source.
Another remaining challenge is to define the
methodology for the evaluation of methods such as
proposed herein. We have to think about appropri-
ate evaluation metrics as well as reference corpora.
Following the practices in other NLP fields, such as
semantic text analysis (SENSEVAL), message and
document understanding conferences (MUC/DUC),
it is desirable to conduct rigourous large-scale eval-
uations. This should facilitate the progress in study-
ing the effects of individual methods and cross-
system comparisons.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987617204301">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal ofLexicography, 16.3:281–296.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Annika Flycht-Eriksson. 2003. Representing knowledge
of dialogue, domain, task and user in dialogue systems
- how and why? Electronic Transactions on Artificial
Intelligence, 3:5–32.
Yuqing Gao, Bowen Zhou, Zijian Diao, Jeffrey Sorensen,
and Michael Picheny. 2003. MARS: A statistical
semantic parsing and generation-based multilingual
automatic translation system. Machine Translation,
17(3):185 – 212.
Yuqing Gao. 2003. Coupling vs. unifying: Modeling
techniques for speech-to-speech translation. In Pro-
ceedings of Eurospeech, pages 365 – 368, Geneva,
Switzerland, 1-4 September.
Iryna Gurevych and Robert Porzel. 2003. Using
knowledge-based scores for identifying best speech
recognition hypotheses. In Proceedings of ISCA Tu-
torial and Research Workshop on Error Handling in
Spoken Dialogue Systems, pages 77 – 81, Chateau-
d’Oex-Vaud, Switzerland, 28-31 August.
Iryna Gurevych, Rainer Malaka, Robert Porzel, and
Hans-Peter Zorn. 2003a. Semantic coherence scoring
using an ontology. In Proceedings of the HLT-NAACL
Conference, pages 88–95, 27 May - 1 June.
Iryna Gurevych, Robert Porzel, Elena Slinko, Nor-
bert Pfleger, Jan Alexandersson, and Stefan Merten.
2003b. Less is more: Using a single knowledge rep-
resentation in dialogue systems. In Proceedings of
the HLT-NAACL’03 Workshop on Text Meaning, pages
14–21, Edmonton, Canada, 31 May.
Katunobu Itou, Atsushi Fujii, and Tetsuya Ishikawa.
2001. Language modeling for multi-domain speech-
driven text retrieval. In Proceedings of IEEE Auto-
matic Speech Recognition and Understanding Work-
shop, December.
Lori Levin, Alon Lavie, Monika Woszczyna, Donna
Gates, Marsal Gavalda, Detlef Koll, and Alex Waibel.
2000. The JANUS-III translation system: Speech-
to-speech translation in multiple domains. Machine
Translation, 15(1-2):3 – 25.
K. Mahesh and S. Nirenburg. 1995. A Situated Ontol-
ogy for Practical NLP. In Workshop on Basic On-
tological Issues in Knowledge Sharing, International
Joint Conference on Artificial Intelligence (IJCAI-95),
Montreal, Canada, 19-20 August.
Christoph M¨uller and Michael Strube. 2003. Multi-level
annotation in MMAX. In Proceedings of the 4th SIG-
dial Workshop on Discourse and Dialogue, pages 198–
207, Sapporo, Japan, 4-5 July.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2003. The Proposition Bank: An annotated corpus of
semantic roles. Submitted to Computational Linguis-
tics, December.
Stefan Rapp and Michael Strube. 2002. An iterative data
collection approach for multimodal dialogue systems.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation, pages 661–665,
Las Palmas, Canary Island, Spain, 29-31 May.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513–
523.
Ruhi Sarikaya, Yuqing Gao, and Michael Picheny. 2003.
Word level confidence measurement using semantic
features. In Proceedings of ICASSP, Hong Kong,
April.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.
Paul Taylor, Simon King, Steve Isard, and Helen Wright.
2000. Intonation and dialogue context as constraints
for speech recognition. Language and Speech, 41(3-
4):493–512.
Wolfgang Wahlster, Norbert Reithinger, and Anselm
Blocher. 2001. SmartKom: Multimodal communi-
cation with a life-like character. In Proceedings of the
7th European Conference on Speech Communication
and Technology, pages 1547–1550.
Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha
Palmer. 2004. The Penn Chinese Treebank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 10(4):1–30, June.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999967">Assigning Domains to Speech Recognition Hypotheses</title>
<author confidence="0.99843">Klaus R¨uggenmann</author>
<author confidence="0.99843">Iryna Gurevych</author>
<affiliation confidence="0.851128">EML Research Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.989887">69118 Heidelberg,</address>
<abstract confidence="0.956599833333333">We present the results of experiments aimed at assigning domains to speech recognition hypotheses (SRH). The methods rely on high-level linguistic representations of SRHs as sets of ontological concepts. We experimented with two domain models and evaluated their performance against a statistical, wordbased model. Our hand-annotated and tf*idf-based models yielded a precision of 88,39% and 82,59% respectively, compared to 93,14% for the word-based baseline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al., 2003), PropBank (Palmer et al., 2003; Xue et al., 2004) is likely to change this situation. Domain recognition, which is the central topic of this paper, can be thought of as high-level semantic tagging of utterances. We expect significant improvements in the performance of the ASR component of the system if information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypotheses as well as those which do not fit in a given domain. Apart from that, there are additional important reasons for the inclusion of information about the current domain in any spoken language processing (SLP) system. Current SLP systems deal not only with a single, but with multiple domains, e.g., Levin et al. (2000), Itou et al. (2001), Wahlster et al. (2001). In fact, the development of multi-domain systems is one of the new research directions in SLP, which makes the issue of automatically assigning domains to utterances especially important. This type of knowledge can be effectively utilized at different stages of the spoken language and multi-domain input processing in the following ways: • optimizing the performance of the speech recognizer; • improving the performance of the dialogue manager, e.g., if a domain change occurred in the discourse; • dynamic loading of resources, e.g. speech recognizer lexicons or dialogue plans, especially in mobile environments. Here, we present the results of research directed at automatic assigning of domains to speech recognition hypotheses. In Section 2, we briefly introduce the knowledge sources in our experiments, such as the ontology, the lexicon and domain models. The data and annotation experiments will be presented in Section 3, followed by the detailed description of the domain classification algorithms in Section 4. Section 5 will give the evaluation results for the linguistically motivated conceptual as well as purely statistical models. Conclusions and some future research directions can be found in Section 6. 2 High-Level Knowledge Sources 2.1 Ontology and lexicon Current SLP systems often employ multidomain ontologies representing the relevant and discourse knowledge. The ledge encoded in such an ontology can be applied to a variety of natural language processing tasks, e.g. Mahesh and Nirenburg (1995), Flycht-Eriksson (2003). Our ontology models the domains Electronic Program Guide, Interaction Management, Cinema Information, Personal Assistance, Route Planning, Sights, Home Appliances Control and Off Talk. The hierarchically structured ontology consists of ca. 720 concepts and 230 properties specifying relations between concepts. For example every instance the concept the relations A detailed description of the ontology employed in our experiments is given in Gurevych et al. (2003b). Ontological concepts are high-level units. They allow to reduce the amount of information needed to represent relations existing between individual lexemes and to effectively incorporate this knowledge into automatic language processing. E.g., there may exist a large number of movies in a cinema reservation system. All of them will be represented by concept thus allowing to map a variety of lexical items (instances) to a single unit (concept) describing their meaning and the relations to other concepts in a generic way. We did not use the structure of the ontology in an explicit way in the reported experiments. The knowledge was used implicitly to come up with a set of ontological concepts needed to represent the user’s utterance. The high-level domain knowledge represented in the ontology is linked with the language-specific knowledge through a lexicon. The lexicon contains ca. 3600 entries of lexical items and their senses (0 or more), encoded as concepts in the E.g., the word mapped to the ontoconcepts in the utterance am in New in the am Peter and if the lexeme a grammatical function only, e.g., am going to a 2.2 Domain models For scoring high-level linguistic representations of utterances we use a domain model. A domain model a two-dimensional matrix the dimenwhere the overall number of domain categories and ontological concepts, respectively. This can be formalized where the maelements domain specificity scores of individual concepts. We experimented with two different domain mod- The first model obtained through direct annotation of concepts with respect to domains as reported in Section 3.2. The second domain from statistical analysis of 1 in Section 3.1). In this case, computed the frequency inverse document score (Salton and Buckley, 1988) of each concept for individual domains. In the case of human annotations, we deal with binary values, whereas tf*idf scores range over the interval [0,1]. 3 Data and Annotation Experiments We performed a number of annotation experiments. The purpose of these experiments was to: • investigate the reliability of the annotations; • create a domain model based on human annotations; • produce a training dataset for statistical classifiers; set a Standard a test dataset for the evaluation. All annotation experiments were conducted on data collected in hidden-operator tests following the paradigm described in Rapp and Strube (2002). Subjects were asked to verbalize a predefined intention in each of their turns, the system’s reaction was simulated by a human operator. We collected utterances from 29 subjects in 8 dialogues with the system each. All user turns were recorded in separate audio files. These audio files were processed by two versions of our dialogue system with different speech recognition modules. Data describing our corpora is given in Table 1. The first and the secsystem’s runs are referred to as 1 2 Dataset 1 Dataset 2 Number of dialogues 232 95 Number of utterances 1479 552 Number of SRHs 2.239 1.375 Number of coherent SRHs 1511 867 Number of incoherent SRHs 728 508 Table 1: Descriptive corpus statistics. The corpora obtained from these experiments were further transformed into a set of annotation files, which can be read into GUI-based annotation tools, e.g., MMAX (M¨uller and Strube, 2003). This tool can be adopted for annotating different levels of information, e.g., semantic coherence and domains of utterances, the best speech recognition hypothesis in the N-best list, as well as domains of individual concepts. The two annotators were trained with the help of an annotation manual. A reconciled version both annotations resulted in the In the following, we present the results of our annotation experiments. Coherence, domains of SRHs in 1 The first experiment was aimed at annotating the recognition hypotheses from their domains. This process was two-staged. In the first stage, the annotators labeled randomly mixed SRHs, i.e. SRHs without discourse context, their semantic coherence as incoher- In the second stage, coherent SRHs were labeled for their domains, resulting in a corpus of 1511 hypotheses labeled for at least one domain category. The numbers for ambiguous domain attributions can be found in Table 2. The class distribution is given in Table 3. Number of domains Annotator 1 1 90.06% 87.11% 2 6.94% 11.27% 3 3.01% 1.28% 4 0% 0.35% 2: Multiple domain assignments in</abstract>
<note confidence="0.947484210526315">Annotator 1 Electr. Program Guide 14.43% 14.86% Interaction Management 15.56% 15.17% Cinema Information 5.32% 8.7% Personal Assistance 0.31% 0.3% Route Planning 37.05% 36% Sights 12.49% 12.74% Home Appliances Control 14.12% 11.22% Off Talk 0.72% 1.01% Table 3: Class distribution for domain assignments. P(A) P(E) Kappa Electr. Program Guide 0.9743 0.7246 0.9066 Interaction Management 0.9836 0.7107 0.9434 Cinema Information 0.9661 0.8506 0.7229 Personal Assistance 0.9953 0.9930 0.3310 Route Planning 0.9777 0.5119 0.9544 Sights 0.9731 0.7629 0.8865 Home Appliances Control 0.9626 0.7504 0.8501 Off Talk 0.9871 0.9780 0.4145</note>
<abstract confidence="0.99780985">Table 4: Kappa coefficient for separate domains. Table 4 presents the Kappa coefficient values for individual categories. the perof agreement between annotators. the percentage we expect them to agree by chance. The annotations are generally considered to be reliif &gt; This is true for all classes except those which occur very rarely on our data. 3.2 Domains of ontological concepts In the second experiment, ontological concepts were with zero or more domain We concepts like typically not domainspecific. Therefore, they will not be assigned any domains. Annotator 2 extracted 231 concepts from the lexicon, which is a subset of ontological concepts relevant for our corpus of SRHs. The annotators were given the textual descriptions of all concepts. These definitions are supplied with the ontology. We computed two kinds of inter-annotator agreement. In the first case, we calculated the percentage of concepts, for which the annotators agreed on all domain categories, resulting in ca. 47.62% (CONCabs, see Figure 1). In the second case, the agreement on individual domain decisions (1848 overall) was computed, ca. 86.85% (CONCindiv, see Figure 1). 3.3 Best conceptual representation and of SRHs in 2 As will be evident from Section 4.1, each SRH can be mapped to a set of possible interpretations, which called representations In this experiment, the best conceptual representation and domains of coherent SRHs from 2 annotated. As our system operates on the basis of CR, it is necessary to disambiguate them in a preprocessing step. 867 SRHs used in this experiment are mapped to 2853 CR, i.e. on average each SRH is mapped to 3.29 CR. The annotators’ agreement on the task of determining the best CR reached ca. 88.93%. For the task of domain annotation, again, we computed the absolute agreement, when the annotators agreed on all domains for a given SRH. This resulted in ca. 92.5% (SRHabs, see Figure 1). The agreement on individual domain decisions (6936 overall) yielded ca. 98.92% (SRHindiv, see Figure 1). As the Figure 1 suggests, annotating utterances with domains is an easier task for humans than annotating ontological concepts with the same information. One possible reason for this is that even for an isolated SRH of an utterance there is at least some local context available, which clarifies its high-level meaning to some extent. An isolated concept has no defining context whatsoever. 4 Domain Classification In this section, we present the algorithms employed for assigning domains to speech recognition hy- The system called processing steps, each of which will be de- Figure 1: Agreement in % on domain annotations for concepts and SRHs. Absolute agreement (CON- Cabs, SRHabs) means that annotators agreed on all domains. Individual agreement (CONCindiv, SRHindiv) refers to identical individual domain decisions. scribed separately in the respective subsections. 4.1 From SRHs to conceptual representations is a set of words ..., on high-level representations of as representations set of ontological concepts ..., representations are obtained from the process called map- In this process, all possible ontological senses corresponding to individual words in the lexicon are resulting in a set possible interpre- ..., each speech recognition hypothesis. For example, in our data a user formulated the concerning the TV program, was f¨ur Spielfilme kommen And which movies come heute abend tonight This utterance resulted in the following SRHs: examples are displayed with the German original and a glossed translation. f¨ur Spielfilme kommen heute abend the average score of all concepts in this movies come tonight domain. For a given domain model this formally means: f¨ur kommen come heute abend tonight 1 Which The two hypotheses have two conceptual representations each. This is due to the lexical ambiguity the word either German. to As a consequence, the yields In Tables 5 and 6, the domain specificity scores all concepts of Example 1 are given.</abstract>
<note confidence="0.80871825">Broadcast Motion Watch Electr. Program Guide 1 0 1 Interaction Management 0 0 0 Cinema Information 0 0 1 Personal Assistance 0 0 0 Route Planning 0 1 1 Sights 0 0 1 Home Appliances Control 1 0 0 Off Talk 0 0 0 5: Matrix from human annotations. Broadcast Motion Watch Electr. Program Guide 1 0.496 0.744 Interaction Management 0 0 0 Cinema Information 0.283 0.178 0.043 Personal Assistance 0 0 0 Route Planning 0 0.689 0.044 Sights 0 0.020 0.079 Home Appliances Control 0.494 0.027 0.147 Off Talk 0 0.238 0.374</note>
<abstract confidence="0.927577538461538">6: Matrix from the annotated corpus. 4.2 Domain classification of CR domain specificity score the conceptual the domain then, defined the number of concepts in the respective As each scored for all domains the of a set of domain scores: ..., the number of domain categories. Tables 7 and 8 display the results of the domain scoring algorithm for the conceptual representations of Example 1.</abstract>
<note confidence="0.96734794117647">Electr. Program Guide 0.5 1.0 0 1.0 Interaction Management 0 0 0 0 Cinema Information 0 0.5 0 1.0 Personal Assistance 0 0 0 0 Route Planning 0.5 0.5 1.0 1.0 Sights 0 0.5 0 1.0 Home Appliances Control 0.5 0.5 0 0 Off Talk 0 0 0 0 7: Domain scores on the basis of Electr. Program Guide 0.748 0.872 0.496 0.744 Interaction Management 0 0 0 0 Cinema Information 0.231 0.163 0.178 0.043 Personal Asssitance 0 0 0 0 Route Planning 0.344 0.022 0.689 0.044 Sights 0.01 0.04 0.02 0.079 Home Appliances Control 0.26 0.32 0.027 0.147 Off Talk 0.119 0.187 0.238 0.374</note>
<abstract confidence="0.995400972222222">8: Domain scores on the basis of the Standard data, as the best attributed the do- Program selected as its best conceptual representation. As can be seen the above tables, this the highest doscore for Program Guide the baof both and Consequently, domain models attribute this domain to not labeled with any domains in the as this hypothesis is an incoherent one and hence cannot be considered to belong to domain at all. According to its repa single score 1 for the do- Planning multiple equal a single score as a more reliable indicator for a specific domain than equal scores and assigns the domain On the basis of overall score for the one for do- Program Therefore, the will assign this domain to In previous experiments (Gurevych et al., 2003a), we found that when operating on sets of concepts as representations of speech recognition hypotheses, ratio of the number of ontological concepts given the total number of words the respective SRH must be accounted for. This relation defined by the ratio The idea is to prevent an incoherent SRH containing many function words with zero concept mappings, represented by a single concept in the extreme, from being classified as coherent. Experimental results indicate that the optimal threshold be set to 0.33. This means that if there are more than three words corresponding to a single concept on average, the SRH is likely to be incoherent and should be excluded from processing. this as apost-processing technique. For both conceptual representations of ratio is whereas for those of we find This value is under the which means that considered incoherent and its domain scores are dropped. Finally, this results in both models assigning the single do- Program Guide the best one to the utterance in Example 1. 5 Evaluation 5.1 Evaluation metrics The evaluation of the algorithms and domain models presented herein poses a methodological problem. As stated in Section 3.3, the annotators were allowed to assign 1 or more domains to an SRH, so number of domain categories varies in the The output of however, is a set with confidence values for all domains ranging from 0 to 1. To the best of our knowledge, there exists no evaluation method that allows the straightforward evaluation of these confidence sets against the varying number of binary domain decisions. As a consequence, we restricted the evaluation to the subset of 758 SRHs unambiguously annotated a single domain in For each SRH compared the recognized domain of its best with the annotated domain. This recognized domain is the one that was scored the highest confidence by In this way we measured the precision on recognizing the best domain of an SRH. The best conceptual representation of an SRH had been previously disambiguated by humans as reported in Section 3.3. Alternatively, this kind of disambiguation can be performed automatically, e.g., with the help of the system presented in Gurevych et al. (2003a). The system scores semantic coherence of SRHs, the best the one with the highest semantic coherence. 5.2 Results We included two baselines in this evaluation. As assigning domains to speech recognition hypotheses a classification task, the class can serve as a first baseline. For a second baseline, we trained a statistical classifier employing the neighbour using This dataset had also been employed to create the tf*idf model. The statistical classifier treated each SRH as of words of concepts with domain categories. Figure 2: Precision on domain assignments. results of the handannotated and tf*idf domain models as well as the baseline systems’ performances are displayed in Figure 2. The diagram shows that all sysclearly outperform the class baseline. The hand-annotated domain model (precision 88.39%) outperforms the tf*idf domain model (precision 82.59%). The model created by humans turns out to be of higher quality than the automatically one. However, the neighbour baseline with words as features performs better (precision 93.14%) than the other methods employing ontological concepts as representations. 5.3 Discussion We believe that this finding can be explained in terms of our experimental setup which favours the statistical model. Table 9 gives the absolute frequency for all domain categories in the evaluation data. As the data implies, three of the possible categories are missing in the data.</abstract>
<note confidence="0.935128333333333">Number of instances Electr. Program Guide 74 Interaction Management 85 Cinema Information 0 Personal Assistance 0 Route Planning 385 Sights 150 Home Appliances Control 64 Off Talk 0</note>
<abstract confidence="0.987200766233766">Table 9: Class distribution in the evaluation dataset. The main reason for our results, however, lies in the controlled experimental setup of the data collection. Subjects had to verbalize pre-defined intentions in 8 scenarios, e.g. record a specific program on TV or ask for information regarding a given historical sight. Naturally, this leads to restricted man-machine interactions using controlled vocabulary. As a result, there is rather limited lexical variation in the data. This is unfortunate for illustrating the strengths of high-level ontological representations. In our opinion, the power of ontological representations is just their ability to reduce multiple lexical surface realizations of the same concept to a single unit, thus representing the meaning of multiple words in a compact way. This effect could not be exploited in a due way given the test corpora in these experiments. We expect a better performance of concept-based methods as compared to word-based ones in broader domains. An additional important point to consider is the portability of the domain recognition approach. Stamodels, e.g., tf*idf and neighbour rely on substantial amounts of annotated data when moving to new domains. Such data is difficult to obtain and requires expensive human efforts for annotation. When the manually created domain model is employed for the domain classification task, the extension of knowledge sources to a new domain boils down to extending the list of concepts with some additional ones and annotating them for domains. These new concepts are part of the extension of the system’s general ontology, which is not created specifically for domain classification, but employed for many purposes in the system. 6 Conclusions In this paper, we presented a system which determines domains of speech recognition hypotheses. Our approach incorporates high-level semantic knowledge encoded in a domain model of ontological concepts. We believe that this type of semantic information has the potential to improve the performance of the automatic speech recognizer, as well as other components of spoken language processing systems. Basically, information about the current domain of discourse is a type of contextual knowledge. One of the future challenges will be to find ways of including this high-level semantic knowledge into SLP systems in the most beneficial way. It remains to be studied how to integrate semantic processing into the architecture, including speech recognition and discourse processing. An important aspect of the scalability of our methods is their dependence on concept-based domain models. A natural extension would be to replace hand-crafted ontological concepts with, e.g., WordNet concepts. The structure of WordNet can then be used to determine high-level domain concepts that can replace human domain annotations. One of the evident problems with this approach is, however, the high level of lexical ambiguity of the WordNet concepts. Apparently, the problem of ambiguity scales up together with the coverage of the respective knowledge source. Another remaining challenge is to define the methodology for the evaluation of methods such as proposed herein. We have to think about appropriate evaluation metrics as well as reference corpora. Following the practices in other NLP fields, such as semantic text analysis (SENSEVAL), message and document understanding conferences (MUC/DUC), it is desirable to conduct rigourous large-scale evaluations. This should facilitate the progress in studying the effects of individual methods and crosssystem comparisons.</abstract>
<title confidence="0.922492">References</title>
<author confidence="0.959695">Collin F Baker</author>
<author confidence="0.959695">Charles J Fillmore</author>
<author confidence="0.959695">John B Lowe</author>
<affiliation confidence="0.662418">The Berkeley FrameNet Project. In Proceed-</affiliation>
<address confidence="0.741477">of Montreal, Canada.</address>
<note confidence="0.833037571428571">Collin F. Baker, Charles J. Fillmore, and Beau Cronin. The structure of the FrameNet database. Inter- Journal 16.3:281–296. Fellbaum, editor. 1998. An Elec- Lexical MIT Press, Cambridge, Mass. Annika Flycht-Eriksson. 2003. Representing knowledge</note>
<abstract confidence="0.768369666666667">of dialogue, domain, task and user in dialogue systems how and why? Transactions on Artificial 3:5–32. Yuqing Gao, Bowen Zhou, Zijian Diao, Jeffrey Sorensen, and Michael Picheny. 2003. MARS: A statistical semantic parsing and generation-based multilingual translation system. 17(3):185 – 212. Yuqing Gao. 2003. Coupling vs. unifying: Modeling for speech-to-speech translation. In Proof pages 365 – 368, Geneva, Switzerland, 1-4 September. Iryna Gurevych and Robert Porzel. 2003. Using knowledge-based scores for identifying best speech hypotheses. In of ISCA Tutorial and Research Workshop on Error Handling in Dialogue pages 77 – 81, Chateaud’Oex-Vaud, Switzerland, 28-31 August. Iryna Gurevych, Rainer Malaka, Robert Porzel, and Hans-Peter Zorn. 2003a. Semantic coherence scoring an ontology. In of the HLT-NAACL pages 88–95, 27 May - 1 June. Iryna Gurevych, Robert Porzel, Elena Slinko, Norbert Pfleger, Jan Alexandersson, and Stefan Merten. 2003b. Less is more: Using a single knowledge repin dialogue systems. In of HLT-NAACL’03 Workshop on Text pages 14–21, Edmonton, Canada, 31 May. Katunobu Itou, Atsushi Fujii, and Tetsuya Ishikawa. 2001. Language modeling for multi-domain speechtext retrieval. In of IEEE Automatic Speech Recognition and Understanding Work- December.</abstract>
<note confidence="0.85654053125">Lori Levin, Alon Lavie, Monika Woszczyna, Donna Gates, Marsal Gavalda, Detlef Koll, and Alex Waibel. 2000. The JANUS-III translation system: Speechtranslation in multiple domains. 15(1-2):3 – 25. K. Mahesh and S. Nirenburg. 1995. A Situated Ontolfor Practical NLP. In on Basic Ontological Issues in Knowledge Sharing, International Conference on Artificial Intelligence Montreal, Canada, 19-20 August. Christoph M¨uller and Michael Strube. 2003. Multi-level in MMAX. In of the 4th SIG- Workshop on Discourse and pages 198– 207, Sapporo, Japan, 4-5 July. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2003. The Proposition Bank: An annotated corpus of roles. to Computational Linguis- December. Stefan Rapp and Michael Strube. 2002. An iterative data collection approach for multimodal dialogue systems. of the 3rd International Conference on Resources and pages 661–665, Las Palmas, Canary Island, Spain, 29-31 May. Gerard Salton and Christopher Buckley. 1988. Termapproaches in automatic text retrieval. In- Processing and 24(5):513– 523. Ruhi Sarikaya, Yuqing Gao, and Michael Picheny. 2003. Word level confidence measurement using semantic In of Hong Kong, April. Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-</note>
<author confidence="0.715169333333333">Dialogue act modeling for</author>
<note confidence="0.886995272727273">automatic tagging and recognition of conversational 26(3):339–373. Paul Taylor, Simon King, Steve Isard, and Helen Wright. 2000. Intonation and dialogue context as constraints speech recognition. and 41(3- 4):493–512. Wolfgang Wahlster, Norbert Reithinger, and Anselm Blocher. 2001. SmartKom: Multimodal communiwith a life-like character. In of the 7th European Conference on Speech Communication pages 1547–1550.</note>
<title confidence="0.559552">Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha</title>
<author confidence="0.710464">The Penn Chinese Treebank Phrase</author>
<affiliation confidence="0.832121">Annotation of a Large Corpus. Lan-</affiliation>
<address confidence="0.895569">10(4):1–30, June.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1848" citStr="Baker et al., 1998" startWordPosition="277" endWordPosition="280"> component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al., 2003), PropBank (Palmer et al., 2003; Xue et al., 2004) is likely to change this situation. Domain recognition, which is the central topic of this paper, can be thought of as high-level semantic tagging of utterances. We expect significant improvements in the performance of the ASR component of the system if information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypo</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>Beau Cronin</author>
</authors>
<date>2003</date>
<booktitle>The structure of the FrameNet database. International Journal ofLexicography,</booktitle>
<pages>16--3</pages>
<contexts>
<context position="1869" citStr="Baker et al., 2003" startWordPosition="281" endWordPosition="284"> et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al., 2003), PropBank (Palmer et al., 2003; Xue et al., 2004) is likely to change this situation. Domain recognition, which is the central topic of this paper, can be thought of as high-level semantic tagging of utterances. We expect significant improvements in the performance of the ASR component of the system if information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypotheses as well as tho</context>
</contexts>
<marker>Baker, Fillmore, Cronin, 2003</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and Beau Cronin. 2003. The structure of the FrameNet database. International Journal ofLexicography, 16.3:281–296.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annika Flycht-Eriksson</author>
</authors>
<title>Representing knowledge of dialogue, domain, task and user in dialogue systems - how and why?</title>
<date>2003</date>
<journal>Electronic Transactions on Artificial Intelligence,</journal>
<pages>3--5</pages>
<contexts>
<context position="4374" citStr="Flycht-Eriksson (2003)" startWordPosition="687" endWordPosition="688">llowed by the detailed description of the domain classification algorithms in Section 4. Section 5 will give the evaluation results for the linguistically motivated conceptual as well as purely statistical models. Conclusions and some future research directions can be found in Section 6. 2 High-Level Knowledge Sources 2.1 Ontology and lexicon Current SLP systems often employ multidomain ontologies representing the relevant world and discourse knowledge. The knowledge encoded in such an ontology can be applied to a variety of natural language processing tasks, e.g. Mahesh and Nirenburg (1995), Flycht-Eriksson (2003). Our ontology models the domains Electronic Program Guide, Interaction Management, Cinema Information, Personal Assistance, Route Planning, Sights, Home Appliances Control and Off Talk. The hierarchically structured ontology consists of ca. 720 concepts and 230 properties specifying relations between concepts. For example every instance of the concept Process features the relations hasBeginTime, hasEndTime and hasState. A detailed description of the ontology employed in our experiments is given in Gurevych et al. (2003b). Ontological concepts are high-level units. They allow to reduce the amo</context>
</contexts>
<marker>Flycht-Eriksson, 2003</marker>
<rawString>Annika Flycht-Eriksson. 2003. Representing knowledge of dialogue, domain, task and user in dialogue systems - how and why? Electronic Transactions on Artificial Intelligence, 3:5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Gao</author>
<author>Bowen Zhou</author>
<author>Zijian Diao</author>
<author>Jeffrey Sorensen</author>
<author>Michael Picheny</author>
</authors>
<title>MARS: A statistical semantic parsing and generation-based multilingual automatic translation system.</title>
<date>2003</date>
<journal>Machine Translation,</journal>
<volume>17</volume>
<issue>3</issue>
<pages>212</pages>
<contexts>
<context position="981" citStr="Gao et al., 2003" startWordPosition="138" endWordPosition="141">sentations of SRHs as sets of ontological concepts. We experimented with two domain models and evaluated their performance against a statistical, wordbased model. Our hand-annotated and tf*idf-based models yielded a precision of 88,39% and 82,59% respectively, compared to 93,14% for the word-based baseline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge so</context>
</contexts>
<marker>Gao, Zhou, Diao, Sorensen, Picheny, 2003</marker>
<rawString>Yuqing Gao, Bowen Zhou, Zijian Diao, Jeffrey Sorensen, and Michael Picheny. 2003. MARS: A statistical semantic parsing and generation-based multilingual automatic translation system. Machine Translation, 17(3):185 – 212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Gao</author>
</authors>
<title>Coupling vs. unifying: Modeling techniques for speech-to-speech translation.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>365--368</pages>
<location>Geneva,</location>
<contexts>
<context position="963" citStr="Gao, 2003" startWordPosition="136" endWordPosition="137">istic representations of SRHs as sets of ontological concepts. We experimented with two domain models and evaluated their performance against a statistical, wordbased model. Our hand-annotated and tf*idf-based models yielded a precision of 88,39% and 82,59% respectively, compared to 93,14% for the word-based baseline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of approp</context>
</contexts>
<marker>Gao, 2003</marker>
<rawString>Yuqing Gao. 2003. Coupling vs. unifying: Modeling techniques for speech-to-speech translation. In Proceedings of Eurospeech, pages 365 – 368, Geneva, Switzerland, 1-4 September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Robert Porzel</author>
</authors>
<title>Using knowledge-based scores for identifying best speech recognition hypotheses.</title>
<date>2003</date>
<journal>81, Chateaud’Oex-Vaud, Switzerland,</journal>
<booktitle>In Proceedings of ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems,</booktitle>
<pages>77</pages>
<contexts>
<context position="1292" citStr="Gurevych and Porzel, 2003" startWordPosition="187" endWordPosition="190">seline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al., 2003), PropBank (Palmer et a</context>
</contexts>
<marker>Gurevych, Porzel, 2003</marker>
<rawString>Iryna Gurevych and Robert Porzel. 2003. Using knowledge-based scores for identifying best speech recognition hypotheses. In Proceedings of ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems, pages 77 – 81, Chateaud’Oex-Vaud, Switzerland, 28-31 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Rainer Malaka</author>
<author>Robert Porzel</author>
<author>Hans-Peter Zorn</author>
</authors>
<title>Semantic coherence scoring using an ontology.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL Conference,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1263" citStr="Gurevych et al., 2003" startWordPosition="183" endWordPosition="186">4% for the word-based baseline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al.,</context>
<context position="4899" citStr="Gurevych et al. (2003" startWordPosition="759" endWordPosition="762">y of natural language processing tasks, e.g. Mahesh and Nirenburg (1995), Flycht-Eriksson (2003). Our ontology models the domains Electronic Program Guide, Interaction Management, Cinema Information, Personal Assistance, Route Planning, Sights, Home Appliances Control and Off Talk. The hierarchically structured ontology consists of ca. 720 concepts and 230 properties specifying relations between concepts. For example every instance of the concept Process features the relations hasBeginTime, hasEndTime and hasState. A detailed description of the ontology employed in our experiments is given in Gurevych et al. (2003b). Ontological concepts are high-level units. They allow to reduce the amount of information needed to represent relations existing between individual lexemes and to effectively incorporate this knowledge into automatic language processing. E.g., there may exist a large number of movies in a cinema reservation system. All of them will be represented by the concept Movie, thus allowing to map a variety of lexical items (instances) to a single unit (concept) describing their meaning and the relations to other concepts in a generic way. We did not use the structure of the ontology in an explicit</context>
<context position="17884" citStr="Gurevych et al., 2003" startWordPosition="2902" endWordPosition="2905">pothesis is an incoherent one and hence cannot be considered to belong to any domain at all. According to DManno, its representation CR2a gets a single score 1 for the domain Route Planning and CR2b gets multiple equal scores. DOMSCORE interprets a single score as a more reliable indicator for a specific domain than multiple equal scores and assigns the domain Route Planning to SRH2. On the basis of DMtf*idf the highest overall score for CR2a,2b is the one for domain Electronic Program Guide. Therefore, the model will assign this domain to SRH2. 4.3 Word2Concept ratio In previous experiments (Gurevych et al., 2003a), we found that when operating on sets of concepts as representations of speech recognition hypotheses, the ratio of the number of ontological concepts n in a given CR and the total number of words w in the respective SRH must be accounted for. This relation is defined by the ratio R = n/w. The idea is to prevent an incoherent SRH containing many function words with zero concept mappings, represented by a single concept in the extreme, from being classified as coherent. Experimental results indicate that the optimal threshold R should be set to 0.33. This means that if there are more than th</context>
<context position="20268" citStr="Gurevych et al. (2003" startWordPosition="3313" endWordPosition="3316">ricted the evaluation to the subset of 758 SRHs unambiguously annotated for a single domain in Dataset 2. For each SRH we compared the recognized domain of its best CR with the annotated domain. This recognized domain is the one that was scored the highest confidence by DOMSCORE. In this way we measured the precision on recognizing the best domain of an SRH. The best conceptual representation of an SRH had been previously disambiguated by humans as reported in Section 3.3. Alternatively, this kind of disambiguation can be performed automatically, e.g., with the help of the system presented in Gurevych et al. (2003a). The system scores semantic coherence of SRHs, where the best CR is the one with the highest semantic coherence. 5.2 Results We included two baselines in this evaluation. As assigning domains to speech recognition hypotheses is a classification task, the majority class frequency can serve as a first baseline. For a second baseline, we trained a statistical classifier employing the k-nearest neighbour method using Dataset 1. This dataset had also been employed to create the tf*idf model. The statistical classifier treated each SRH as a bag of words or bag of concepts labeled with domain cate</context>
</contexts>
<marker>Gurevych, Malaka, Porzel, Zorn, 2003</marker>
<rawString>Iryna Gurevych, Rainer Malaka, Robert Porzel, and Hans-Peter Zorn. 2003a. Semantic coherence scoring using an ontology. In Proceedings of the HLT-NAACL Conference, pages 88–95, 27 May - 1 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Robert Porzel</author>
<author>Elena Slinko</author>
<author>Norbert Pfleger</author>
<author>Jan Alexandersson</author>
<author>Stefan Merten</author>
</authors>
<title>Less is more: Using a single knowledge representation in dialogue systems.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL’03 Workshop on Text Meaning,</booktitle>
<pages>14--21</pages>
<location>Edmonton, Canada, 31</location>
<contexts>
<context position="1263" citStr="Gurevych et al., 2003" startWordPosition="183" endWordPosition="186">4% for the word-based baseline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al.,</context>
<context position="4899" citStr="Gurevych et al. (2003" startWordPosition="759" endWordPosition="762">y of natural language processing tasks, e.g. Mahesh and Nirenburg (1995), Flycht-Eriksson (2003). Our ontology models the domains Electronic Program Guide, Interaction Management, Cinema Information, Personal Assistance, Route Planning, Sights, Home Appliances Control and Off Talk. The hierarchically structured ontology consists of ca. 720 concepts and 230 properties specifying relations between concepts. For example every instance of the concept Process features the relations hasBeginTime, hasEndTime and hasState. A detailed description of the ontology employed in our experiments is given in Gurevych et al. (2003b). Ontological concepts are high-level units. They allow to reduce the amount of information needed to represent relations existing between individual lexemes and to effectively incorporate this knowledge into automatic language processing. E.g., there may exist a large number of movies in a cinema reservation system. All of them will be represented by the concept Movie, thus allowing to map a variety of lexical items (instances) to a single unit (concept) describing their meaning and the relations to other concepts in a generic way. We did not use the structure of the ontology in an explicit</context>
<context position="17884" citStr="Gurevych et al., 2003" startWordPosition="2902" endWordPosition="2905">pothesis is an incoherent one and hence cannot be considered to belong to any domain at all. According to DManno, its representation CR2a gets a single score 1 for the domain Route Planning and CR2b gets multiple equal scores. DOMSCORE interprets a single score as a more reliable indicator for a specific domain than multiple equal scores and assigns the domain Route Planning to SRH2. On the basis of DMtf*idf the highest overall score for CR2a,2b is the one for domain Electronic Program Guide. Therefore, the model will assign this domain to SRH2. 4.3 Word2Concept ratio In previous experiments (Gurevych et al., 2003a), we found that when operating on sets of concepts as representations of speech recognition hypotheses, the ratio of the number of ontological concepts n in a given CR and the total number of words w in the respective SRH must be accounted for. This relation is defined by the ratio R = n/w. The idea is to prevent an incoherent SRH containing many function words with zero concept mappings, represented by a single concept in the extreme, from being classified as coherent. Experimental results indicate that the optimal threshold R should be set to 0.33. This means that if there are more than th</context>
<context position="20268" citStr="Gurevych et al. (2003" startWordPosition="3313" endWordPosition="3316">ricted the evaluation to the subset of 758 SRHs unambiguously annotated for a single domain in Dataset 2. For each SRH we compared the recognized domain of its best CR with the annotated domain. This recognized domain is the one that was scored the highest confidence by DOMSCORE. In this way we measured the precision on recognizing the best domain of an SRH. The best conceptual representation of an SRH had been previously disambiguated by humans as reported in Section 3.3. Alternatively, this kind of disambiguation can be performed automatically, e.g., with the help of the system presented in Gurevych et al. (2003a). The system scores semantic coherence of SRHs, where the best CR is the one with the highest semantic coherence. 5.2 Results We included two baselines in this evaluation. As assigning domains to speech recognition hypotheses is a classification task, the majority class frequency can serve as a first baseline. For a second baseline, we trained a statistical classifier employing the k-nearest neighbour method using Dataset 1. This dataset had also been employed to create the tf*idf model. The statistical classifier treated each SRH as a bag of words or bag of concepts labeled with domain cate</context>
</contexts>
<marker>Gurevych, Porzel, Slinko, Pfleger, Alexandersson, Merten, 2003</marker>
<rawString>Iryna Gurevych, Robert Porzel, Elena Slinko, Norbert Pfleger, Jan Alexandersson, and Stefan Merten. 2003b. Less is more: Using a single knowledge representation in dialogue systems. In Proceedings of the HLT-NAACL’03 Workshop on Text Meaning, pages 14–21, Edmonton, Canada, 31 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katunobu Itou</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Language modeling for multi-domain speechdriven text retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop,</booktitle>
<contexts>
<context position="2791" citStr="Itou et al. (2001)" startWordPosition="438" endWordPosition="441"> information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypotheses as well as those which do not fit in a given domain. Apart from that, there are additional important reasons for the inclusion of information about the current domain in any spoken language processing (SLP) system. Current SLP systems deal not only with a single, but with multiple domains, e.g., Levin et al. (2000), Itou et al. (2001), Wahlster et al. (2001). In fact, the development of multi-domain systems is one of the new research directions in SLP, which makes the issue of automatically assigning domains to utterances especially important. This type of knowledge can be effectively utilized at different stages of the spoken language and multi-domain input processing in the following ways: • optimizing the performance of the speech recognizer; • improving the performance of the dialogue manager, e.g., if a domain change occurred in the discourse; • dynamic loading of resources, e.g. speech recognizer lexicons or dialogue</context>
</contexts>
<marker>Itou, Fujii, Ishikawa, 2001</marker>
<rawString>Katunobu Itou, Atsushi Fujii, and Tetsuya Ishikawa. 2001. Language modeling for multi-domain speechdriven text retrieval. In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Levin</author>
<author>Alon Lavie</author>
<author>Monika Woszczyna</author>
<author>Donna Gates</author>
<author>Marsal Gavalda</author>
<author>Detlef Koll</author>
<author>Alex Waibel</author>
</authors>
<title>The JANUS-III translation system: Speechto-speech translation in multiple domains.</title>
<date>2000</date>
<journal>Machine Translation, 15(1-2):3 –</journal>
<volume>25</volume>
<contexts>
<context position="2771" citStr="Levin et al. (2000)" startWordPosition="434" endWordPosition="437">nent of the system if information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypotheses as well as those which do not fit in a given domain. Apart from that, there are additional important reasons for the inclusion of information about the current domain in any spoken language processing (SLP) system. Current SLP systems deal not only with a single, but with multiple domains, e.g., Levin et al. (2000), Itou et al. (2001), Wahlster et al. (2001). In fact, the development of multi-domain systems is one of the new research directions in SLP, which makes the issue of automatically assigning domains to utterances especially important. This type of knowledge can be effectively utilized at different stages of the spoken language and multi-domain input processing in the following ways: • optimizing the performance of the speech recognizer; • improving the performance of the dialogue manager, e.g., if a domain change occurred in the discourse; • dynamic loading of resources, e.g. speech recognizer </context>
</contexts>
<marker>Levin, Lavie, Woszczyna, Gates, Gavalda, Koll, Waibel, 2000</marker>
<rawString>Lori Levin, Alon Lavie, Monika Woszczyna, Donna Gates, Marsal Gavalda, Detlef Koll, and Alex Waibel. 2000. The JANUS-III translation system: Speechto-speech translation in multiple domains. Machine Translation, 15(1-2):3 – 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Mahesh</author>
<author>S Nirenburg</author>
</authors>
<title>A Situated Ontology for Practical NLP.</title>
<date>1995</date>
<booktitle>In Workshop on Basic Ontological Issues in Knowledge Sharing, International Joint Conference on Artificial Intelligence (IJCAI-95),</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="4350" citStr="Mahesh and Nirenburg (1995)" startWordPosition="683" endWordPosition="686">be presented in Section 3, followed by the detailed description of the domain classification algorithms in Section 4. Section 5 will give the evaluation results for the linguistically motivated conceptual as well as purely statistical models. Conclusions and some future research directions can be found in Section 6. 2 High-Level Knowledge Sources 2.1 Ontology and lexicon Current SLP systems often employ multidomain ontologies representing the relevant world and discourse knowledge. The knowledge encoded in such an ontology can be applied to a variety of natural language processing tasks, e.g. Mahesh and Nirenburg (1995), Flycht-Eriksson (2003). Our ontology models the domains Electronic Program Guide, Interaction Management, Cinema Information, Personal Assistance, Route Planning, Sights, Home Appliances Control and Off Talk. The hierarchically structured ontology consists of ca. 720 concepts and 230 properties specifying relations between concepts. For example every instance of the concept Process features the relations hasBeginTime, hasEndTime and hasState. A detailed description of the ontology employed in our experiments is given in Gurevych et al. (2003b). Ontological concepts are high-level units. They</context>
</contexts>
<marker>Mahesh, Nirenburg, 1995</marker>
<rawString>K. Mahesh and S. Nirenburg. 1995. A Situated Ontology for Practical NLP. In Workshop on Basic Ontological Issues in Knowledge Sharing, International Joint Conference on Artificial Intelligence (IJCAI-95), Montreal, Canada, 19-20 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>Multi-level annotation in MMAX.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>198--207</pages>
<location>Sapporo,</location>
<marker>M¨uller, Strube, 2003</marker>
<rawString>Christoph M¨uller and Michael Strube. 2003. Multi-level annotation in MMAX. In Proceedings of the 4th SIGdial Workshop on Discourse and Dialogue, pages 198– 207, Sapporo, Japan, 4-5 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2003</date>
<note>Submitted to Computational Linguistics,</note>
<contexts>
<context position="1900" citStr="Palmer et al., 2003" startWordPosition="286" endWordPosition="289">rzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al., 2003), PropBank (Palmer et al., 2003; Xue et al., 2004) is likely to change this situation. Domain recognition, which is the central topic of this paper, can be thought of as high-level semantic tagging of utterances. We expect significant improvements in the performance of the ASR component of the system if information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypotheses as well as those which do not fit in a given </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2003</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2003. The Proposition Bank: An annotated corpus of semantic roles. Submitted to Computational Linguistics, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Rapp</author>
<author>Michael Strube</author>
</authors>
<title>An iterative data collection approach for multimodal dialogue systems.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>661--665</pages>
<location>Las Palmas, Canary Island,</location>
<contexts>
<context position="7644" citStr="Rapp and Strube (2002)" startWordPosition="1200" endWordPosition="1203">individual domains. In the case of human annotations, we deal with binary values, whereas tf*idf scores range over the interval [0,1]. 3 Data and Annotation Experiments We performed a number of annotation experiments. The purpose of these experiments was to: • investigate the reliability of the annotations; • create a domain model based on human annotations; • produce a training dataset for statistical classifiers; • set a Gold Standard as a test dataset for the evaluation. All annotation experiments were conducted on data collected in hidden-operator tests following the paradigm described in Rapp and Strube (2002). Subjects were asked to verbalize a predefined intention in each of their turns, the system’s reaction was simulated by a human operator. We collected utterances from 29 subjects in 8 dialogues with the system each. All user turns were recorded in separate audio files. These audio files were processed by two versions of our dialogue system with different speech recognition modules. Data describing our corpora is given in Table 1. The first and the second system’s runs are referred to as Dataset 1 and Dataset 2 respectively. Dataset 1 Dataset 2 Number of dialogues 232 95 Number of utterances 1</context>
</contexts>
<marker>Rapp, Strube, 2002</marker>
<rawString>Stefan Rapp and Michael Strube. 2002. An iterative data collection approach for multimodal dialogue systems. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 661–665, Las Palmas, Canary Island, Spain, 29-31 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<pages>523</pages>
<contexts>
<context position="7001" citStr="Salton and Buckley, 1988" startWordPosition="1098" endWordPosition="1101"> the overall number of domain categories and ontological concepts, respectively. This can be formalized as: DM = (Sdc)d=1,...,#d,c=1,...,#c, where the matrix elements Sdc are domain specificity scores of individual concepts. We experimented with two different domain models. The first model DManno was obtained through direct annotation of concepts with respect to domains as reported in Section 3.2. The second domain model DMtf*idf resulted from statistical analysis of Dataset 1 (described in Section 3.1). In this case, we computed the term frequency - inverse document frequency (tf*idf) score (Salton and Buckley, 1988) of each concept for individual domains. In the case of human annotations, we deal with binary values, whereas tf*idf scores range over the interval [0,1]. 3 Data and Annotation Experiments We performed a number of annotation experiments. The purpose of these experiments was to: • investigate the reliability of the annotations; • create a domain model based on human annotations; • produce a training dataset for statistical classifiers; • set a Gold Standard as a test dataset for the evaluation. All annotation experiments were conducted on data collected in hidden-operator tests following the p</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>Yuqing Gao</author>
<author>Michael Picheny</author>
</authors>
<title>Word level confidence measurement using semantic features.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP, Hong Kong,</booktitle>
<contexts>
<context position="1026" citStr="Sarikaya et al., 2003" startWordPosition="146" endWordPosition="149">l concepts. We experimented with two domain models and evaluated their performance against a statistical, wordbased model. Our hand-annotated and tf*idf-based models yielded a precision of 88,39% and 82,59% respectively, compared to 93,14% for the word-based baseline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. </context>
</contexts>
<marker>Sarikaya, Gao, Picheny, 2003</marker>
<rawString>Ruhi Sarikaya, Yuqing Gao, and Michael Picheny. 2003. Word level confidence measurement using semantic features. In Proceedings of ICASSP, Hong Kong, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taylor</author>
<author>Simon King</author>
<author>Steve Isard</author>
<author>Helen Wright</author>
</authors>
<title>Intonation and dialogue context as constraints for speech recognition. Language and Speech,</title>
<date>2000</date>
<pages>41--3</pages>
<contexts>
<context position="1048" citStr="Taylor et al., 2000" startWordPosition="150" endWordPosition="153">nted with two domain models and evaluated their performance against a statistical, wordbased model. Our hand-annotated and tf*idf-based models yielded a precision of 88,39% and 82,59% respectively, compared to 93,14% for the word-based baseline model. These results are explained in terms of our experimental setup. 1 Motivation High-level linguistic knowledge has been shown to have the potential of improving the state of the art in automatic speech recognition (ASR). Such knowledge can be integrated in the ASR component (Gao, 2003; Gao et al., 2003; Stolcke et al., 2000; Sarikaya et al., 2003; Taylor et al., 2000). Alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the automatic speech recognizer and the spoken language understanding component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain port</context>
</contexts>
<marker>Taylor, King, Isard, Wright, 2000</marker>
<rawString>Paul Taylor, Simon King, Steve Isard, and Helen Wright. 2000. Intonation and dialogue context as constraints for speech recognition. Language and Speech, 41(3-4):493–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Norbert Reithinger</author>
<author>Anselm Blocher</author>
</authors>
<title>SmartKom: Multimodal communication with a life-like character.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th European Conference on Speech Communication and Technology,</booktitle>
<pages>1547--1550</pages>
<contexts>
<context position="2815" citStr="Wahlster et al. (2001)" startWordPosition="442" endWordPosition="445">he current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypotheses as well as those which do not fit in a given domain. Apart from that, there are additional important reasons for the inclusion of information about the current domain in any spoken language processing (SLP) system. Current SLP systems deal not only with a single, but with multiple domains, e.g., Levin et al. (2000), Itou et al. (2001), Wahlster et al. (2001). In fact, the development of multi-domain systems is one of the new research directions in SLP, which makes the issue of automatically assigning domains to utterances especially important. This type of knowledge can be effectively utilized at different stages of the spoken language and multi-domain input processing in the following ways: • optimizing the performance of the speech recognizer; • improving the performance of the dialogue manager, e.g., if a domain change occurred in the discourse; • dynamic loading of resources, e.g. speech recognizer lexicons or dialogue plans, especially in mo</context>
</contexts>
<marker>Wahlster, Reithinger, Blocher, 2001</marker>
<rawString>Wolfgang Wahlster, Norbert Reithinger, and Anselm Blocher. 2001. SmartKom: Multimodal communication with a life-like character. In Proceedings of the 7th European Conference on Speech Communication and Technology, pages 1547–1550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="1919" citStr="Xue et al., 2004" startWordPosition="290" endWordPosition="293">f these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al., 2003), PropBank (Palmer et al., 2003; Xue et al., 2004) is likely to change this situation. Domain recognition, which is the central topic of this paper, can be thought of as high-level semantic tagging of utterances. We expect significant improvements in the performance of the ASR component of the system if information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypotheses as well as those which do not fit in a given domain. Apart from </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2004</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha Palmer. 2004. The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 10(4):1–30, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>