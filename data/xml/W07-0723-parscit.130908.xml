<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002264">
<title confidence="0.8522175">
Getting to know Moses:
Initial experiments on German–English factored translation
</title>
<author confidence="0.995775">
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg
</author>
<affiliation confidence="0.989629">
Department of Computer and Information Science
</affiliation>
<address confidence="0.683841">
Linköpings universitet, Sweden
</address>
<email confidence="0.997926">
{marho,sarst,lah}@ida.liu.se
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999531">
We present results and experiences from
our experiments with phrase-based statisti-
cal machine translation using Moses. The
paper is based on the idea of using an off-
the-shelf parser to supply linguistic infor-
mation to a factored translation model and
compare the results of German–English
translation to the shared task baseline sys-
tem based on word form. We report partial
results for this model and results for two
simplified setups. Our best setup takes ad-
vantage of the parser’s lemmatization and
decompounding. A qualitative analysis of
compound translation shows that decom-
pounding improves translation quality.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997775862069">
One of the stated goals for the shared task of this
workshop is “to offer newcomers a smooth start
with hands-on experience in state-of-the-art statis-
tical machine translation methods”. As our previ-
ous research in machine translation has been
mainly concerned with rule-based methods, we
jumped at this offer.
We chose to work on German-to-English trans-
lation for two reasons. Our primary practical inter-
est lies with translation between Swedish and Eng-
lish, and of the languages offered for the shared
task, German is the one closest in structure to
Swedish. While there are differences in word order
and morphology between Swedish and German,
there are also similarities, e.g., that both languages
represent nominal compounds as single ortho-
graphic words. We chose the direction from Ger-
man to English because our knowledge of English
is better than our knowledge of German, making it
easier to judge the quality of translation output.
Experiments were performed on the Europarl data.
With factored statistical machine translation,
different levels of linguistic information can be
taken into account during training of a statistical
translation system and decoding. In our experi-
ments we combined syntactic and morphological
factors from an off-the-shelf parser with the fac-
tored translation framework in Moses (Moses,
2007). We wanted to test the following hypotheses:
</bodyText>
<listItem confidence="0.999610333333333">
• Translation models based on lemmas will im-
prove translation quality (Popovič and Ney,
2004)
• Decompounding German nominal compounds
will improve translation quality (Koehn and
Knight, 2003)
• Re-ordering models based on word forms and
parts-of-speech will improve translation qual-
ity (Zens and Ney, 2006).
</listItem>
<sectionHeader confidence="0.759634" genericHeader="method">
2 The parser
</sectionHeader>
<bodyText confidence="0.9999073">
The parser, Machinese Syntax, is a commercially
available dependency parser from Connexor Oy 1.
It provides each word with lemma, part-of-speech,
morphological features and dependency relations
(see Figure 1). In addition, the lemmas of com-
pounds are marked by a ‘#’ separating the two
parts of the compound. For the shared task we only
used shallow linguistic information: lemma, part-
of-speech and morphology. The compound bound-
ary identification was used to split noun com-
</bodyText>
<footnote confidence="0.892223">
1 Connexor Oy, http://www.connexor.com.
</footnote>
<page confidence="0.95317">
181
</page>
<bodyText confidence="0.75479025">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 181–184,
Prague, June 2007. c�2007 Association for Computational Linguistics
pounds to make the German input more similar to
English text.
</bodyText>
<figure confidence="0.962516">
1 Mit mit pm&gt;2 @PREMARK PREP
2 Blick blick advl&gt;10 @NH N MSC SG DAT
3 auf auf pm&gt;5 @PREMARK PREP
</figure>
<figureCaption confidence="0.999949">
Figure 1. Example of parser output
</figureCaption>
<bodyText confidence="0.999542833333333">
We used the parser’s tokenization as given. Some
common multiword units, such as ‘at all’ and ‘von
heute’, are treated as single words by the parser
(cf. Niessen and Ney, 2004). The German parser
also splits contracted prepositions and determiners
like ‘zum’ – ‘zu dem’ (“to the”).
</bodyText>
<sectionHeader confidence="0.984759" genericHeader="method">
3 System description
</sectionHeader>
<bodyText confidence="0.999970857142857">
For our experiments with Moses we basically fol-
lowed the shared task baseline system setup to
train our factored translation models. After training
a statistical model, minimum error-rate tuning was
performed to tune the model parameters. All ex-
periments were performed on an AMD 64 Athlon
4000+ processor with 4 Gb of RAM and 32 bit
Linux (Ubuntu).
Since time as well as computer resources were
limited we designed a model that we hoped would
make the best use of all available factors. This
model turned out to be too complex for our ma-
chine and in later experiments we abandoned it for
a simpler model.
</bodyText>
<subsectionHeader confidence="0.999829">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999992162162162">
In the pre-processing step we used the standard
pre-processing of the shared task baseline system,
parsed the German and English texts and processed
the output to obtain four factors: word form,
lemma, part-of-speech and morphology. Missing
values for lemma, part-of-speech and morphology
were replaced with default values.
Noun compounds are very frequent in German,
2.9% of all tokens in the tuning corpus were identi-
fied by the parser as noun compounds. Compounds
tend to lead to sparse data problems and splitting
them has been shown to improve German-English
translation (Koehn and Knight, 2003). Thus we
decided to decompund German noun compounds
identified as such by our parser.
We used a simple strategy to remove fillers and
to correct some obvious mistakes. We removed the
filler ‘-s’ that appear before a marked split unless it
was one of ‘-ss’, ‘-urs’, ‘-eis’ or ‘-us’. This applied
to 35% of the noun compounds in the tuning cor-
pus. The fillers were removed both in the word
form and the lemma (see Figure 2).
There were some mistakes made by the parser,
for instance on compounds containing the word
‘nahmen’ which was incorrectly split as ‘stel-
lungn#ahmen’ instead of ‘stellung#nahmen’
(“statement”). These splits were corrected by mov-
ing the ‘n’ to the right side of the split.
We then split noun-lemmas on hyphens unless
there were numbers on either side of it and on the
places marked by ‘#’. Word forms were split in the
corresponding places as the lemmas.
The part-of-speech and morphology of the last
word in the compound is the same as for the whole
compound. For the other parts we hypothesized
that part-of-speech is Noun and the morphology is
unknown, marked by the tag UNK.
</bodyText>
<figure confidence="0.8542934">
Parser output:
unionsländer unions#land N NEU PL ACC
Factored output:
union|union|N|UNK
länder|land|N|NEU_PL_ACC
</figure>
<figureCaption confidence="0.9861795">
Figure 2. Compound splitting for ‘unionsländer’
(“countries in the union”)
</figureCaption>
<bodyText confidence="0.999741666666667">
These strategies are quite crude and could be fur-
ther refined by studying the parser output thor-
oughly to pinpoint more problems.
</bodyText>
<subsectionHeader confidence="0.9224795">
3.2 Training translation models with linguis-
tic factors
</subsectionHeader>
<bodyText confidence="0.999913166666667">
After pre-processing, the German–English Eu-
roparl training data contains four factors: 0: word
form, 1: lemma, 2: part-of-speech, 3: morphology.
As a first step in training our translation models we
performed word alignment on lemmas as this could
potentially improve word alignment.
</bodyText>
<subsectionHeader confidence="0.8606">
3.2.1 First setup
</subsectionHeader>
<bodyText confidence="0.999932333333334">
Factored translation requires a number of decoding
steps, which are either mapping steps mapping a
source factor to a target factor or generation steps
generating a target factor from other target factors.
Our first setup contained three mapping steps, T0–
T2, and one generation step, G0.
</bodyText>
<page confidence="0.922819">
182
</page>
<equation confidence="0.7333665">
T0: 0-0 (word – word)
T1: 1-1 (lemma – lemma)
T2: 2,3-2,3 (pos+morph – pos+morph)
G0: 1,2,3-0 (lemma+pos+morph – word)
</equation>
<bodyText confidence="0.99985647826087">
With the generation step, word forms that did not
appear in the training data may still get translated
if the lemma, part-of-speech and morphology can
be translated separately and the target word form
can be generated from these factors.
Word order varies a great deal between German
and English. This is especially true for the place-
ment of verbs. To model word order changes we
included part-of-speech information and created
two reordering models, one based on word form
(0), the other on part-of-speech (2):
0-0.msd-bidirectional-fe
2-2.msd-bidirectional-fe
The decoding times for this setup turned out to be
unmanageable. In the first iteration of parameter
tuning, decoding times were approx. 6
min/sentence. In the second iteration decoding
time increased to approx. 30 min/sentence. Re-
moving one of the reordering models did not result
in a significant change in decoding time. Just trans-
lating the 2000 sentences of test data with untuned
parameters would take several days. We inter-
rupted the tuning and abandoned this setup.
</bodyText>
<subsectionHeader confidence="0.802467">
3.2.2 Second setup
</subsectionHeader>
<bodyText confidence="0.9826755">
Because of the excessive decoding times of the
first factored setup we resorted to a simpler system
that only used the word form factor for the transla-
tion and reordering models. This setup differs from
the shared task baseline in the following ways:
First, it uses the tokenization provided by the
parser. Second, alignment was performed on the
lemma factor. Third, German compounds were
split using the method described above. To speed
up tuning and decoding, we only used the first 200
sentences of development data (dev2006) for tun-
ing and reduced stack size to 50.
T0: 0-0 (word – word)
R: 0-0.msd-bidirectional-fe
</bodyText>
<subsectionHeader confidence="0.574447">
3.2.3 Third setup
</subsectionHeader>
<bodyText confidence="0.9997332">
To test our hypothesis that word reordering would
benefit from part-of-speech information we created
another simpler model. This setup has two map-
ping steps, T0 and T1, and a reordering model
based on part-of-speech.
</bodyText>
<equation confidence="0.529332666666667">
T0: 0-0 (word – word)
T1: 2,3-2,3 (pos+morph – pos+morph)
R: 2-2.msd-bidirectional-fe
</equation>
<sectionHeader confidence="0.99984" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999981857142857">
We compared our systems to a baseline system
with the same setup as the WMT2007 shared task
baseline system but tuned with our system’s sim-
plified tuning settings (200 instead of 2000 tuning
sentences, stack size 50). Table 1 shows the Bleu
improvement on the 200 sentences development
data from the first and last iteration of tuning.
</bodyText>
<table confidence="0.999232666666667">
System Dev2006 (200)
1st iteration Last iteration
Baseline 19.56 27.07
First 21.68 -
Second 20.43 27.16
Third 20.72 24.72
</table>
<tableCaption confidence="0.990372">
Table 1. Bleu scores on 200 sentences of tuning
</tableCaption>
<bodyText confidence="0.9738712">
data before and after tuning
The final test of our systems was performed on the
development test corpus (devtest2006) using stack
size 50. The results are shown in Table 2. The low
Bleu score for the third setup implies that reorder-
ing on part-of-speech is not enough on its own.
The second setup performed best with a slightly
higher Bleu score than the baseline. We used the
second setup to translate test data for our submis-
sion to the shared task.
</bodyText>
<table confidence="0.9991902">
System Devtest2006 (NIST/Bleu)
Baseline 6.7415 / 25.94
First -
Second 6.8036 / 26.04
Third 6.5504 / 24.57
</table>
<tableCaption confidence="0.9910965">
Table 2. NIST and Bleu scores on development
test data
</tableCaption>
<subsectionHeader confidence="0.995606">
4.1 Decompounding
</subsectionHeader>
<bodyText confidence="0.99975075">
We have evaluated the decompounding strategy by
analyzing how the first 75 identified noun com-
pounds of the devtest corpus were translated by our
second setup compared to the baseline. The sample
</bodyText>
<page confidence="0.998132">
183
</page>
<bodyText confidence="0.999715933333334">
excluded doubles and compounds that had no clear
translation in the reference corpus.
Out of these 75 compounds 74 were nouns that
were correctly split and 1 was an adjective that was
split incorrectly: ‘allumfass#ende’. Despite that it
was incorrectly identified and split it was trans-
lated satisfyingly to ‘comprehensive’.
The translations were grouped into the catego-
ries shown in Table 3. The 75 compounds were
classified into these categories for our second sys-
tem and the baseline system, as shown in Table 4.
As can be seen the compounds were handled better
by our system, which had 62 acceptable transla-
tions (C or V) compared to 48 for the baseline and
did not leave any noun compounds untranslated.
</bodyText>
<table confidence="0.997379210526316">
Category Example
C-correct Regelungsentwurf
Draft regulation
Ref: Draft regulation
V-variant Schlachthöfen
Abattoirs
Ref: Slaughter houses
P-partly correct Anpassungsdruck
Pressure
Ref: Pressure for adaption
F-wrong form Länderberichte
Country report
Ref: Country reports
W-wrong Erbonkel
Uncle dna
Ref: Sugar daddy
U-untranslated Schlussentwurf
Schlussentwurf
Ref: Final draft
</table>
<tableCaption confidence="0.989983">
Table 3. Classification scheme with examples for
compound translations
</tableCaption>
<table confidence="0.999793">
Baseline system
Second system C V P W U F Tot
C 36 1 3 3 1 44
V 1 9 2 1 5 18
P 3 2 5
W 1 2 3
U 0
F 1 4 5
Tot 38 10 8 2 12 5 75
</table>
<tableCaption confidence="0.984002">
Table 4. Classification of 75 compounds from our
</tableCaption>
<bodyText confidence="0.954227909090909">
second system and the baseline system
Decompounding of nouns reduced the number
of untranslated words, but there were still some
left. Among these were cases that can be handled
such as separable prefix verbs like ‘aufzeigten’
(“pointed out”) (Niessen and Ney, 2000) or adjec-
tive compounds such as ‘multidimensionale’
(“multi dimensional”). There were also some noun
compounds left which indicates that we might need
a better decompounding strategy than the one used
by the parser (see e.g. Koehn and Knight, 2003).
</bodyText>
<subsectionHeader confidence="0.922213">
4.2 Experiences and future plans
</subsectionHeader>
<bodyText confidence="0.999958076923077">
With the computer equipment at our disposal,
training of the models and tuning of the parameters
turned out to be a very time-consuming task. For
this reason, the number of system setups we could
test was small, and much fewer than we had hoped
for. Thus it is too early to draw any conclusions as
regards our hypotheses, but we plan to perform
more tests in the future, also on Swedish–English
data. The parser&apos;s ability to identify compounds
that can be split before training seems to give a
definite improvement, however, and is a feature
that can likely be exploited also for Swedish-to-
English translation with Moses.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719826086957">
Koehn, Philipp and Kevin Knight, 2003. Empirical
methods for compound splitting. In Proceedings of
EACL 2003, 187-194. Budapest, Hungary.
Moses – a factored phrase-based beam-search decoder
for machine translation. 13 April 2007, URL:
http://www.statmt.org/moses/ .
Niessen, Sonja and Hermann Ney, 2004. Statistical ma-
chine translation with scarce resources using mor-
pho-syntactic information. Computational Linguis-
tics, 181-204 .
Niessen, Sonja and Hermann Ney, 2000. Improving
SMT Quality with Morpho-syntactic Analysis. In
Proceedings of Coling 2000. 1081-1085. Saar-
brücken, Germany.
Popovič, Maja and Hermann Ney, 2004. Improving
Word Alignment Quality using Morpho-Syntactic In-
formation. In Proceedings of Coling 2004, 310-314,
Geneva, Switzerland.
Zens, Richard and Hermann Ney, 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion. In HLT-NAACL: Proceedings of the Workshop
on Statistical Machine Translation, 55-63, New York
City, NY.
</reference>
<page confidence="0.998704">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489955">
<title confidence="0.9556435">Getting to know Moses: Initial experiments on German–English factored translation</title>
<author confidence="0.994996">Maria Holmqvist</author>
<author confidence="0.994996">Sara Stymne</author>
<author confidence="0.994996">Lars</author>
<affiliation confidence="0.99626">Department of Computer and Information</affiliation>
<address confidence="0.552225">Linköpings universitet, Sweden</address>
<email confidence="0.939727">marho@ida.liu.se</email>
<email confidence="0.939727">sarst@ida.liu.se</email>
<email confidence="0.939727">lah@ida.liu.se</email>
<abstract confidence="0.9971995625">We present results and experiences from our experiments with phrase-based statistical machine translation using Moses. The paper is based on the idea of using an offthe-shelf parser to supply linguistic information to a factored translation model and compare the results of German–English translation to the shared task baseline system based on word form. We report partial results for this model and results for two simplified setups. Our best setup takes advantage of the parser’s lemmatization and decompounding. A qualitative analysis of compound translation shows that decompounding improves translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL 2003,</booktitle>
<pages>187--194</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2445" citStr="Koehn and Knight, 2003" startWordPosition="362" endWordPosition="365">riments were performed on the Europarl data. With factored statistical machine translation, different levels of linguistic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1. It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic information: lemma, partof-speech and morphology. The compound boundary identification was used to split noun</context>
<context position="4969" citStr="Koehn and Knight, 2003" startWordPosition="770" endWordPosition="773">el. 3.1 Pre-processing In the pre-processing step we used the standard pre-processing of the shared task baseline system, parsed the German and English texts and processed the output to obtain four factors: word form, lemma, part-of-speech and morphology. Missing values for lemma, part-of-speech and morphology were replaced with default values. Noun compounds are very frequent in German, 2.9% of all tokens in the tuning corpus were identified by the parser as noun compounds. Compounds tend to lead to sparse data problems and splitting them has been shown to improve German-English translation (Koehn and Knight, 2003). Thus we decided to decompund German noun compounds identified as such by our parser. We used a simple strategy to remove fillers and to correct some obvious mistakes. We removed the filler ‘-s’ that appear before a marked split unless it was one of ‘-ss’, ‘-urs’, ‘-eis’ or ‘-us’. This applied to 35% of the noun compounds in the tuning corpus. The fillers were removed both in the word form and the lemma (see Figure 2). There were some mistakes made by the parser, for instance on compounds containing the word ‘nahmen’ which was incorrectly split as ‘stellungn#ahmen’ instead of ‘stellung#nahmen</context>
<context position="12350" citStr="Koehn and Knight, 2003" startWordPosition="1982" endWordPosition="1985"> 2 1 5 18 P 3 2 5 W 1 2 3 U 0 F 1 4 5 Tot 38 10 8 2 12 5 75 Table 4. Classification of 75 compounds from our second system and the baseline system Decompounding of nouns reduced the number of untranslated words, but there were still some left. Among these were cases that can be handled such as separable prefix verbs like ‘aufzeigten’ (“pointed out”) (Niessen and Ney, 2000) or adjective compounds such as ‘multidimensionale’ (“multi dimensional”). There were also some noun compounds left which indicates that we might need a better decompounding strategy than the one used by the parser (see e.g. Koehn and Knight, 2003). 4.2 Experiences and future plans With the computer equipment at our disposal, training of the models and tuning of the parameters turned out to be a very time-consuming task. For this reason, the number of system setups we could test was small, and much fewer than we had hoped for. Thus it is too early to draw any conclusions as regards our hypotheses, but we plan to perform more tests in the future, also on Swedish–English data. The parser&apos;s ability to identify compounds that can be split before training seems to give a definite improvement, however, and is a feature that can likely be expl</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Koehn, Philipp and Kevin Knight, 2003. Empirical methods for compound splitting. In Proceedings of EACL 2003, 187-194. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses</author>
</authors>
<title>a factored phrase-based beam-search decoder for machine translation. 13</title>
<date>2007</date>
<note>URL: http://www.statmt.org/moses/ .</note>
<contexts>
<context position="2207" citStr="Moses, 2007" startWordPosition="329" endWordPosition="330">al compounds as single orthographic words. We chose the direction from German to English because our knowledge of English is better than our knowledge of German, making it easier to judge the quality of translation output. Experiments were performed on the Europarl data. With factored statistical machine translation, different levels of linguistic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1. It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemma</context>
</contexts>
<marker>Moses, 2007</marker>
<rawString>Moses – a factored phrase-based beam-search decoder for machine translation. 13 April 2007, URL: http://www.statmt.org/moses/ .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morpho-syntactic information.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>181--204</pages>
<contexts>
<context position="3617" citStr="Niessen and Ney, 2004" startWordPosition="550" endWordPosition="553">und boundary identification was used to split noun com1 Connexor Oy, http://www.connexor.com. 181 Proceedings of the Second Workshop on Statistical Machine Translation, pages 181–184, Prague, June 2007. c�2007 Association for Computational Linguistics pounds to make the German input more similar to English text. 1 Mit mit pm&gt;2 @PREMARK PREP 2 Blick blick advl&gt;10 @NH N MSC SG DAT 3 auf auf pm&gt;5 @PREMARK PREP Figure 1. Example of parser output We used the parser’s tokenization as given. Some common multiword units, such as ‘at all’ and ‘von heute’, are treated as single words by the parser (cf. Niessen and Ney, 2004). The German parser also splits contracted prepositions and determiners like ‘zum’ – ‘zu dem’ (“to the”). 3 System description For our experiments with Moses we basically followed the shared task baseline system setup to train our factored translation models. After training a statistical model, minimum error-rate tuning was performed to tune the model parameters. All experiments were performed on an AMD 64 Athlon 4000+ processor with 4 Gb of RAM and 32 bit Linux (Ubuntu). Since time as well as computer resources were limited we designed a model that we hoped would make the best use of all avai</context>
</contexts>
<marker>Niessen, Ney, 2004</marker>
<rawString>Niessen, Sonja and Hermann Ney, 2004. Statistical machine translation with scarce resources using morpho-syntactic information. Computational Linguistics, 181-204 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT Quality with Morpho-syntactic Analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>1081--1085</pages>
<location>Saarbrücken, Germany.</location>
<contexts>
<context position="12102" citStr="Niessen and Ney, 2000" startWordPosition="1943" endWordPosition="1946">ts W-wrong Erbonkel Uncle dna Ref: Sugar daddy U-untranslated Schlussentwurf Schlussentwurf Ref: Final draft Table 3. Classification scheme with examples for compound translations Baseline system Second system C V P W U F Tot C 36 1 3 3 1 44 V 1 9 2 1 5 18 P 3 2 5 W 1 2 3 U 0 F 1 4 5 Tot 38 10 8 2 12 5 75 Table 4. Classification of 75 compounds from our second system and the baseline system Decompounding of nouns reduced the number of untranslated words, but there were still some left. Among these were cases that can be handled such as separable prefix verbs like ‘aufzeigten’ (“pointed out”) (Niessen and Ney, 2000) or adjective compounds such as ‘multidimensionale’ (“multi dimensional”). There were also some noun compounds left which indicates that we might need a better decompounding strategy than the one used by the parser (see e.g. Koehn and Knight, 2003). 4.2 Experiences and future plans With the computer equipment at our disposal, training of the models and tuning of the parameters turned out to be a very time-consuming task. For this reason, the number of system setups we could test was small, and much fewer than we had hoped for. Thus it is too early to draw any conclusions as regards our hypothe</context>
</contexts>
<marker>Niessen, Ney, 2000</marker>
<rawString>Niessen, Sonja and Hermann Ney, 2000. Improving SMT Quality with Morpho-syntactic Analysis. In Proceedings of Coling 2000. 1081-1085. Saarbrücken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovič</author>
<author>Hermann Ney</author>
</authors>
<title>Improving Word Alignment Quality using Morpho-Syntactic Information.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>310--314</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2346" citStr="Popovič and Ney, 2004" startWordPosition="349" endWordPosition="352">er than our knowledge of German, making it easier to judge the quality of translation output. Experiments were performed on the Europarl data. With factored statistical machine translation, different levels of linguistic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1. It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic informatio</context>
</contexts>
<marker>Popovič, Ney, 2004</marker>
<rawString>Popovič, Maja and Hermann Ney, 2004. Improving Word Alignment Quality using Morpho-Syntactic Information. In Proceedings of Coling 2004, 310-314, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>HLT-NAACL: Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<location>New York City, NY.</location>
<contexts>
<context position="2560" citStr="Zens and Ney, 2006" startWordPosition="380" endWordPosition="383">tic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1. It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic information: lemma, partof-speech and morphology. The compound boundary identification was used to split noun com1 Connexor Oy, http://www.connexor.com. 181 Proceedings of the Second Workshop on Statistical Machine Translati</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Zens, Richard and Hermann Ney, 2006. Discriminative Reordering Models for Statistical Machine Translation. In HLT-NAACL: Proceedings of the Workshop on Statistical Machine Translation, 55-63, New York City, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>