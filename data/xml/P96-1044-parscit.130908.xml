<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.977093">
Linguistic Structure as Composition and Perturbation
</title>
<author confidence="0.799999">
Carl de Marcken
</author>
<affiliation confidence="0.657311">
MIT AT Laboratory, NE43-769
</affiliation>
<address confidence="0.767158">
545 Technology Square
Cambridge, MA, 02139, USA
</address>
<email confidence="0.985365">
cgdemarc©ai.mit.edu
</email>
<sectionHeader confidence="0.995033" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976">
This paper discusses the problem of learn-
ing language from unprocessed text and
speech signals, concentrating on the prob-
lem of learning a lexicon. In particular, it
argues for a representation of language in
which linguistic parameters like words are
built by perturbing a composition of exist-
ing parameters. The power of the represen-
tation is demonstrated by several examples
in text segmentation and compression, ac-
quisition of a lexicon from raw speech, and
the acquisition of mappings between text
and artificial representations of meaning.
</bodyText>
<sectionHeader confidence="0.990016" genericHeader="introduction">
1 Motivation
</sectionHeader>
<bodyText confidence="0.9999462">
Language is a robust and necessarily redundant
communication mechanism. Its redundancies com-
monly manifest themselves as predictable patterns
in speech and text signals, and it is largely these
patterns that enable text and speech compression.
Naturally, many patterns in text and speech re-
flect interesting properties of language. For ex-
ample, the is both an unusually frequent sequence
of letters and an English word. This suggests us-
ing compression as a means of acquiring under-
lying properties of language from surface signals.
The general methodology of language-learning-by-
compression is not new. Some notable early propo-
nents included Chomsky (1955), Solomonoff (1960)
and Harris (1968), and compression has been used
as the basis for a wide variety of computer programs
that attack unsupervised learning in language; see
(Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke,
1994; Chen, 1995; Cartwright and Brent, 1994)
among others.
</bodyText>
<subsectionHeader confidence="0.990275">
1.1 Patterns and Language
</subsectionHeader>
<bodyText confidence="0.9998605">
Unfortunately, while surface patterns often reflect
interesting linguistic mechanisms and parameters,
they do not always do so. Three classes of exam-
ples serve to illustrate this.
</bodyText>
<subsubsectionHeader confidence="0.451104">
1.1.1 Extralinguistic Patterns
</subsubsectionHeader>
<bodyText confidence="0.999522785714286">
The sequence it was a dark and stormy night is
a pattern in the sense it occurs in text far more
frequently than the frequencies of its letters would
suggest, but that does not make it a lexical or gram-
matical primitive: it is the product of a complex
mixture of linguistic and extra-linguistic processes.
Such patterns can be indistinguishable from desired
ones. For example, in the Brown corpus (Francis and
Kucera, 1982) scratching her nose occurs 5 times,
a corpus-specific idiosyncrasy. This phrase has the
same structure as the idiom kicking the bucket. It is
difficult to imagine any induction algorithm learn-
ing kicking the bucket from this corpus without also
(mistakenly) learning scratching her nose.
</bodyText>
<subsubsectionHeader confidence="0.663472">
1.1.2 The Definition of Interesting
</subsubsectionHeader>
<bodyText confidence="0.999953909090909">
This discussion presumes there is a set of desired
patterns to extract from input signals. What is this
set? For example, is kicking the bucket a proper lexi-
cal unit? The answer depends on factors external to
the unsupervised learning framework. For the pur-
poses of machine translation or information retrieval
this sequence is an important idiom, but with re-
spect to speech recognition it is unremarkable. Sim-
ilar questions could be asked of subword units like
syllables. Plainly, the answers depends on the learn-
ing context, and not on the signal itself.
</bodyText>
<subsectionHeader confidence="0.588258">
1.1.3 The Definition of Pattern
</subsectionHeader>
<bodyText confidence="0.999680785714286">
Any statistical definition of pattern depends on
an underlying model. For instance, the sequence the
dog occurs much more frequently than one would
expect given an independence assumption about let-
ters. But for a model with knowledge of syntax and
word frequencies, there is nothing remarkable about
the phrase. Since all existing models have flaws, pat-
terns will always be learned that are artifacts of im-
perfections in the learning algorithm.
These examples seem to imply that unsupervised
induction will never converge to ideal grammars and
lexicons. While there is truth to this, the rest of this
paper describes a representation of language that
bypasses many of the apparent difficulties.
</bodyText>
<page confidence="0.996923">
335
</page>
<figure confidence="0.98134">
[national football league]
I
[national] [football] [league]
[nation] [al] [foot] [ball] [lea] [gue]
\ A /
[n] [t] [b] [1] [g]
</figure>
<figureCaption confidence="0.999805">
Figure 1: A compositional representation.
</figureCaption>
<sectionHeader confidence="0.963394" genericHeader="method">
2 A Compositional Representation
</sectionHeader>
<bodyText confidence="0.999231414634146">
The examples in sections 1.1.1 and 1.1.2 seem to
imply that any unsupervised language learning pro-
gram that returns only one segmentation of the in-
put is bound to make many mistakes. And sec-
tion 1.1.3 implies that the decisions about linguistic
units must be made relative to their representations.
Both problems can be solved if linguistic units (for
now, words in the lexicon) are built by composition
of other units. For example, kicking the bucket might
be built by composing kicking, the and bucket.&apos; Of
course, if a word is merely the composition of its
parts, there is nothing interesting about it and no
reason to include it in the lexicon. So the motiva-
tion for including a word in the lexicon must be that
it function differently from its parts. Thus a word is
a perturbation of a composition.
In the case of kicking the bucket the perturbation is
one of both meaning and frequency. For scratching
her nose the perturbation may just be frequency.&apos;
This is a very natural representation from the view-
point of language. It correctly predicts that both
phrases inherit their sound and syntax from their
component words. At the same time it leaves open
the possibility that idiosyncratic information will be
attached to the whole, as with the meaning of kick-
ing the bucket. This structure is very much like the
class hierarchy of a modern programming language.
It is not the same thing as a context-free grammar,
since each word does not act in the same way as the
default composition of its components.
Figure 1 illustrates a recursive decomposition (un-
der concatenation) of the phrase national football
league. The phrase is broken into three words, each
of which are also decomposed in the lexicon. This
process bottoms out in the terminal characters. This
is a real decomposition achieved by a program de-
scribed in section 4. Not shown are the perturba-
&apos;A simple composition operator is concatenation, but
in section 6 a more interesting one is discussed.
zNaturally, an unsupervised learning algorithm with
no access to meaning will not treat them differently.
</bodyText>
<equation confidence="0.953647428571428">
Code Length Components
000 = cof 2 c., cf
001 = cih. 3 C1, C, ce
010 = 2 ,
0110 = ciome 4 cs, co, cm, C.
0111 =omeofthe 3 Csome Cott Cthe
10000 = • • •
</equation>
<figureCaption confidence="0.8933875">
Figure 2: A coding of the first few words of a hypo-
thetical lexicon. The first two columns can be coded
</figureCaption>
<bodyText confidence="0.95842825">
succinctly, leaving the cost of pointers to component
words as the dominant cost of both the lexicon and
the representation of the input.
tions (in this case merely frequency changes) that
distinguish each word from its parts. This general
framework extends to other perturbations. For ex-
ample, the word wanna is naturally thought of as
a composition of want and to with a sound change.
And in speech the three different words to, two and
too may well inherit the sound of a common ancestor
while introducing new syntactic and semantic prop-
erties.
</bodyText>
<subsectionHeader confidence="0.997017">
2.1 Coding
</subsectionHeader>
<bodyText confidence="0.999963884615385">
Of course, for this representation to be more than
an intuition both the composition and perturbation
operators must be exactly specified. In particular,
a code must be designed that enables a word (or a
sentence) to be expressed in terms of its parts. As a
simple example, suppose that the composition oper-
ator is concatenation, that terminals are characters,
and that the only perturbation operator is the abil-
ity to express the frequency of a word independently
of the frequency of its parts. Then to code either a
sentence of the input or a (nonterminal) word in the
lexicon, the number of component words in the rep-
resentation must be written, followed by a code for
each component word. Naturally, each word in the
lexicon must be associated with its code, and under
a near-optimal coding scheme like a Huffman code,
the code length will be related to the frequency of
the word. Thus, associating a word with a code sub-
stitutes for writing down the frequency of a word.
Furthermore, if words are written down in order of
decreasing frequency, a Huffman code for a large
lexicon can be specified using a negligible number
of bits. This and the near-negligible cost of writ-
ing down word lengths will not be discussed further.
Figure 2 presents a portion of an encoding of a hy-
pothetical lexicon.
</bodyText>
<subsectionHeader confidence="0.987555">
2.2 MDL
</subsectionHeader>
<bodyText confidence="0.999819333333333">
Given a coding scheme and a particular lexicon (and
a parsing algorithm) it is in theory possible to calcu-
late the minimum length encoding of a given input.
</bodyText>
<page confidence="0.995228">
336
</page>
<bodyText confidence="0.999930727272727">
Part of the encoding will be devoted to the lexicon,
the rest to representing the input in terms of the
lexicon. The lexicon that minimizes the combined
description length of the lexicon and the input max-
imally compresses the input. In the sense of Rissa-
nen&apos;s minimum description-length (MDL) principle
(Rissanen, 1978; Rissanen, 1989) this lexicon is the
theory that best explains the data, and one can hope
that the patterns in the lexicon reflect the underly-
ing mechanisms and parameters of the language that
generated the input.
</bodyText>
<subsectionHeader confidence="0.998698">
2.3 Properties of the Representation
</subsectionHeader>
<bodyText confidence="0.9992005">
Representing words in the lexicon as perturbations
of compositions has a number of desirable properties.
</bodyText>
<listItem confidence="0.977151034482759">
• The choice of composition and perturbation op-
erators captures a particular detailed theory of
language. They can be used, for instance, to
reference sophisticated phonological and mor-
phological mechanisms.
• The length of the description of a word is a mea-
sure of its linguistic plausibility, and can serve
as a buffer against learning unnatural coinci-
dences.
• Coincidences like scratching her nose do not ex-
clude desired structure, since they are further
broken down into components that they inherit
properties from.
• Structure is shared: the words blackbird and
blackberry can share the common substructure
associated with black, such as its sound and
meaning. As a consequence, data is pooled for
estimation, and representations are compact.
• Common irregular forms are compiled out. For
example, if went is represented in terms of go
(presumably to save the cost of unnecessarily
reproducing syntactic and semantic properties)
the complex sound change need only be repre-
sented once, not every time went is used.
• Since parameters (words) have compact repre-
sentations, they are cheap from a description
length standpoint, and many can be included
in the lexicon. This allows learning algorithms
to fit detailed statistical properties of the data.
</listItem>
<bodyText confidence="0.948943538461538">
This coding scheme is very similar to that found in
popular dictionary-based compression schemes like
LZ78 (Ziv and Lempel, 1978). It is capable of com-
pressing a sequence of identical characters of length
n to size 0(log n). However, in contrast to compres-
sion schemes like LZ78 that use deterministic rules
to add parameters to the dictionary (and do not ar-
rive at linguistically plausible parameters), it is pos-
sible to, perform more sophisticated searches in this
representation.
Start with lexicon of terminals.
Iterate
Iterate (EM)
Parse input and words using current lexicon.
Use word counts to update frequencies.
Add words to the lexicon.
Iterate (EM)
Parse input and words using current lexicon.
Use word counts to update frequencies.
Delete words from the lexicon.
Figure 3: An iterative search algorithm. Two it-
erations of the inner loops are usually sufficient for
convergence, and for the tests described in this pa-
per after 10 iterations of the outer loop there is little
change in the lexicon in terms of either compression
performance or structure.
</bodyText>
<sectionHeader confidence="0.995649" genericHeader="method">
3 A Search Algorithm
</sectionHeader>
<bodyText confidence="0.99998325">
Since the class of possible lexicons is infinite, the
minimization of description length is necessarily
heuristic. Given a fixed lexicon, the expectation-
maximization algorithm (Dempster et al., 1977) can
be used to arrive at a (locally) optimal set of fre-
quencies and codelengths for the words in the lex-
icon. For composition by concatenation, the algo-
rithm reduces to the special case of the Baum-Welch
procedure (Baum et al., 1970) discussed in (Deligne
and Bimbot, 1995). In general, however, the parsing
and reestimation involved in EM can be consider-
ably more complicated. To update the structure of
the lexicon, words can be added or deleted from it
if this is predicted to reduce the description length
of the input. This algorithm is summarized in fig-
ure 3.3
</bodyText>
<subsectionHeader confidence="0.99973">
3.1 Adding and Deleting Words
</subsectionHeader>
<bodyText confidence="0.999975076923077">
For words to be added to the lexicon, two things are
needed. The first is a means of hypothesizing candi-
date new words. The second is a means of evaluat-
ing candidates. One reasonable means of generating
candidates is to look at pairs (or triples) of words
that are composed in the parses of words and sen-
tences of the input. Since words are built by com-
posing other words and act like their composition, a
new word can be created from such a pair and substi-
tuted in place of the pair wherever the pair appears.
For example, if water and melon are frequently com-
posed, then a good candidate for a new word is water
o melon = watermelon, where o is the concatenation
</bodyText>
<footnote confidence="0.9996554">
3For the composition operators and test sets we have
looked at, using single (Viterbi) parses produces almost
exactly the same results (in terms of both compression
and lexical structure) as summing probabilities over mul-
tiple parses.
</footnote>
<page confidence="0.995437">
337
</page>
<bodyText confidence="0.99991344">
operator. In order to evaluate whether the addition
of such a new word is likely to reduce the description
length of the input, it is necessary to record during
the EM step the extra statistics of how many times
the composed pairs occur in parses.
The effect on description length of adding a new
word can not be exactly computed. Its addition
will not only affect other words, but may also cause
other words to be added or deleted. Furthermore, it
is more computationally efficient to add and delete
many words simultaneously, and this complicates
the estimation of the change in description length.
Fortunately, simple approximations of the change
are adequate. For example, if Viterbi analyses are
being used then the new word watermelon will com-
pletely take the place of all compositions of water
and melon. This reduces the counts of water and
melon accordingly, though they are each used once
in the representation of watermelon. If it is assumed
that no other word counts change, these assumptions
allow one to predict the counts and probabilities of
all words after the change. Since the codelength
of a. word w with probability p(w) is approximately
— log p(w), the total estimated change in description
length of adding a new word W to a lexicon L is
</bodyText>
<equation confidence="0.954379666666667">
A —c&apos; (W) log p&apos;(W) d.l. (changes) +
E(-e(w) log pi (w) c(w) log p(w))
wEL
</equation>
<bodyText confidence="0.999952125">
where c(w) is the count of the word w, primes indi-
cated counts and probabilities after the change and
d.1.(changes) represents the cost of writing down the
perturbations involved in the representation of W.
If A &lt; 0 the word is predicted to reduce the total
description length and is added to the lexicon. Sim-
ilar heuristics can be used to estimate the benefit of
deleting words.4
</bodyText>
<subsectionHeader confidence="0.999715">
3.2 Search Properties
</subsectionHeader>
<bodyText confidence="0.993942964285714">
A significant source of problems in traditional gram-
mar induction techniques is local minima (de Mar-
cken, 1995a; Pereira and Schabes, 1992; Carroll and
Charniak, 1992). The search algorithm described
above avoids many of these problems. The reason
is that hidden structure is largely a &amp;quot;compile-time&amp;quot;
phenomena. During parsing all that is important
about a word is its surface form and codelength. The
internal representation does not matter. Therefore,
the internal representation is free to reorganize at
any time; it has been decoupled. This allows struc-
ture to be built bottom up or for structure to emerge
inside already existing parameters. Furthermore,
since parameters (words) encode surface patterns, it
4See (de Marcken, 1995b) for more detailed discus-
sion of these estimations. The actual formulas used in
the tests presented in this paper are slightly more com-
plicated than presented here.
is relatively easy to determine when they are useful,
and their use is limited. They usually do not have
competing roles, in contrast, for instance, to hidden
nodes in neural networks. And since there are no
fixed number of parameters, when words do start to
have multiple disparate uses, they can be split with
common substructure shared. Finally, since add and
delete cycles can compensate for initial mistakes, in-
exact heuristics can be used for adding and deleting
words.
</bodyText>
<sectionHeader confidence="0.988016" genericHeader="method">
4 Concatenation Results
</sectionHeader>
<bodyText confidence="0.990679304347826">
The simplest reasonable instantiation of the
composition-and-perturbation framework is with the
concatenation operator and frequency perturbation.
This instantiation is easily tested on problems of text
segmentation and compression. Given a text docu-
ment, the search algorithm can be used to learn a
lexicon that minimizes its description length. For
testing purposes, spaces will be removed from input
text and true words will be defined to be minimal
sequences bordered by spaces in the original input).
The search algorithm parses the input as it com-
presses it, and can therefore output a segmentation
of the input in terms of words drawn from the lex-
icon. These words are themselves decomposed in
the lexicon, and can be considered to form a tree
that terminates in the characters of the sentence.
This tree can have no more than 0(n) nodes for a
sentence with n characters, though there are 0(n2)
possible &amp;quot;true words&amp;quot; in the input sentence; thus,
the tree contains considerable information. Define
recall to be the percentage of true words that oc-
cur at some level of the segmentation-tree. Define
crossing-bracket to be the percentage of true words
that violate the segmentation-tree structure.&apos;
The search algorithm was applied to two texts,
a lowercase version of the million-word Brown cor-
pus with spaces and punctuation removed, and 4
million characters of Chinese news articles in a two-
byte/character format. In the case of the Chinese,
which contains no inherent separators like spaces,
segmentation performance is measured relative to
another computer segmentation program that had
access to a (human-created) lexicon. The algorithm
was given the raw encoding and had to deduce the
internal two-byte structure. In the case of the Brown
corpus, word recall was 90.5% and crossing-brackets
was 1.7%. For the Chinese word recall was 96.9%
and crossing-brackets was 1.3%. In the case of both
English and Chinese, most of the unfound words
were words that occurred only once in the corpus.
Thus, the algorithm has done an extremely good job
of learning words and properly using them to seg-
ment the input. Furthermore, the crossing-bracket
&apos;The true word moon in the input [the][moon] is a
crossing-bracket violation of them in the segmentation
tree Ifthemllollonll.
</bodyText>
<page confidence="0.991711">
338
</page>
<figure confidence="0.998884448275862">
Rank Word
0 [s]
1 [the]
2 [and]
3 [a]
4 [of]
5 [in]
6 [to]
500 [students]
501 [material]
502 [um]
503 [words]
504 [period]
505 [class]
506 [question]
5000 [ [ing] [them]]
5001 [ [mon] [k]3
5002 [[re) [lax]]
5003 [[rig] [id]]
5004 [connect] [ad]]
5005 [ [i] [103
5006 [[hu] [t]]
26000 [ [pleural] [blood] [supply]]
26001 E [anordinary] [happy] [family]]
26002 [ [f] [Ems] [ibility] [of))
26003 [ [lunar) [brightness] [distribution]]
26004 [ [primarily] [diff] [using]]
26005 [[sodium] [tri] [polyphosphate]]
26006 [ [charcoal] [broil] [ed]]
</figure>
<figureCaption confidence="0.999267">
Figure 4: Sections of the lexicon learned from the
</figureCaption>
<bodyText confidence="0.974780777777778">
Brown corpus, ranked by frequency. The words in
the less-frequent half are listed with their first-level
decomposition. Word 5000 causes crossing-bracket
violations, and words 26002 and 26006 have internal
structure that causes recall violations.
measure indicates that the algorithm has made very
few clear mistakes. Of course, the hierarchical lexical
representation does not make a commitment to what
levels are &amp;quot;true words&amp;quot; and which are not; about
5 times more internal nodes exist than true words.
Experiments in section 5 demonstrate that for most
applications this is not only not a problem, but de-
sirable. Figure 4 displays some of the lexicon learned
from the Brown corpus.
The algorithm was also run as a compressor
on a lower-case version of the Brown corpus with
spaces and punctuation left in. All bits neces-
sary for exactly reproducing the input were counted.
Compression performance is 2.12 bits/char, signifi-
cantly lower than popular algorithms like gzip (2.95
bits/char). This is the best text compression result
on this corpus that we are aware of, and should not
be confused with lower figures that do not include
the cost of parameters. Furthermore, because the
compressed text is stored in terms of linguistic units
like words, it can be searched, indexed, and parsed
without decompression.
</bodyText>
<sectionHeader confidence="0.924762" genericHeader="method">
5 Learning Meanings
</sectionHeader>
<bodyText confidence="0.999972540540541">
Unsupervised learning algorithms are rarely used in
isolation. The goal of this work has been to ex-
plain how linguistic units like words can be learned,
so that other processes can make use of these
units. In this section a means of learning the map-
pings between words and artificial representations
of meanings is described. The composition-and-
perturbation encompasses this application neatly.
Imagine that text utterances are paired with rep-
resentations of meaning,6 and that the goal is to find
the minimum-length description of both the text and
the meaning. If there is mutual information between
the meaning and text portions of the input, then bet-
ter compression is achieved if the two streams are
compressed simultaneously. If a text word can have
some associated meaning, then writing down that
word to account for some portion of text also ac-
counts for some portion of the meaning of that text.
The remaining meaning can be written down more
succinctly. Thus, there is an incentive to associate
meaning with sound, although of course the associ-
ation pays a price in the description of the lexicon.
Although it is obviously a naive simplification,
many of the interesting properties of the composi-
tional representation surface even when meanings
are treating as sets of arbitrary symbols. A word is
now both a character sequence and a set of symbols.
The composition operator concatenates the charac-
ters and unions the meaning symbols. Of course,
there must be some way to alter the default meaning
of a word. One way to do this is to explicitly write
out any symbols that are present in the word&apos;s mean-
ing but not in its components, or vice versa. Thus,
the word red {RED} might be represented as ro eo
d+RED. Given an existing word berry {BERRY},
the red berry cranberry {RED BERRY} can be rep-
resented co ro ao no berry {BERRY}+RED.
</bodyText>
<subsectionHeader confidence="0.502891">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999941944444444">
To test the algorithm&apos;s ability to infer word mean-
ings, 10,000 utterances from an unsegmented textual
database of mothers&apos; speech to children were paired
with representations of meaning, constructed by as-
signing a unique symbol to each root word in the vo-
cabulary. For example, the sentence and what is he
painting a picture of? is paired with the unordered
meaning AND WHAT BE HE PAINT A PIC-
TURE OF. In the first experiment, the algorithm
received these pairs with no noise or ambiguity, us-
ing an encoding of meaning symbols such that each
symbol&apos;s length was 10 bits. After 8 iterations of
training without meaning and then a further 8 it-
erations with, the text sequences were parsed again
without access to the true meaning. The meanings
&apos;This framework is easily extended to handle multi-
ple ambiguous meanings (with and without priors) and
noise, but these extensions will not be discussed here.
</bodyText>
<page confidence="0.998041">
339
</page>
<bodyText confidence="0.999752269230769">
of the resulting word sequences were compared with
the true meanings. Symbol accuracy was 98.9%, re-
call was 93.6%. Used to differentiate the true mean-
ing from the meanings of the previous 20 sentences,
the program selected correctly 89.1% of the time, or
ranked the true meaning tied for first 10.8% of the
time.
A second test was performed in which the algo-
rithm received three possible meanings for each ut-
terance, the true one and also the meaning of the
two surrounding utterances. A uniform prior was
used. Symbol accuracy was again 98.9%, recall was
75.3%.
The final lexicon includes extended phrases, but
meanings tend to filter down to the proper level.
For instance, although the words duck, ducks, the
ducks and duckdrink all exist and contain the mean-
ing DUCK, the symbol is only written into the de-
scription of duck. All others inherit it. Similar re-
sults hold for similar experiments on the Brown cor-
pus. For example, scratching her nose inherits its
meaning completely from its parts, while kicking the
bucket does not. This is exactly the result argued
for in the motivation section of this paper, and illus-
trates why occasional extra words in the lexicon are
not a problem for most applications.
</bodyText>
<figure confidence="0.964809902439024">
6 Other Applications and Current
Work
rep(w)
NDrjmr]
[0 [auzn])
[tah] d]
[Ek[tid]]
[An (iin]]
[mcliindalr]z)
[ai] diiz]
sik [rti]]
[log ] [tairn]]
[sElc] [gin]]
[wAn]pn]
[v [en] [dr]]
[a [limin]
[ [mei] i[id
[be [liindal]]
Rgoul] d [rnin]s [mks]]
akmp] [au t]r]
[ga[vrrnin]]
[oubl][zahuou]]
[min]i[streiiin]]
[[tje]r[in]]
[linbl][hahwou]]
[s[An113] [&apos;NW]
[[pr] [pion] zl]
(bou][skg]i]
[[kg] [611]
[[goul]d[maiinz]]
Rkorpr][eitld]]
Rank
5392 (w3rmr]
5393 (0auzn]
5394 [tahId]
5395 [ektld]
5396 (Aniin]
5397 (meliindalrz]
8948 aidiiz]
8949 sikrti]
8950 (lop taim]
</figure>
<footnote confidence="0.884589157894737">
8951 (sekgIn]
8952 [wAnpA]
8953 (vendEr]
8954 (allmInei]
8955 [meliiD]
8956 beliindal]
9164 [gouldminswks]
9165 kmpS&apos;utr]
9166 [gavrmin]
9167 [oublzahuou]
9168 (ministreign]
9169 (tjerin]
9170 [hAblhahwou]
9171 sAmpoio]
9172 [prplouzl]
9173 [bouskgi]
9174 [kgedjil]
9175 [gouldmaiinz]
9176 [korpreitid]
</footnote>
<bodyText confidence="0.998242344827586">
We have performed other experiments using this rep-
resentation and search algorithm, on tasks in unsu-
pervised learning from speech and grammar induc-
tion.
Figure 5 contains a small portion of a lexicon
learned from 55,000 utterances of continuous speech
by multiple speakers. The utterances are taken from
dictated Wall Street Journal articles. The concate-
nation operators was used with phonemes as termi-
nals. A second layer was added to the framework
to map from phonemes to speech; these extensions
are described in more detail in (de Marcken, 1995b).
The sound model of each phoneme was learned sep-
arately using supervised training on different, seg-
mented speech. Although the phoneme model is ex-
tremely poor, many words are recognizable, and this
is the first significant lexicon learned directly from
spoken speech without supervision.
If the composition operator makes use of context,
then the representation extends naturally to a more
powerful form of context-free grammars, where com-
position is tree-insertion. In particular, if each word
is associated with a part-of-speech, and parts of
speech are permissible terminals in the lexicon, then
&amp;quot;words&amp;quot; become production rules. For example, a
word might be VP take off NP and represented
in terms of the composition of VP V P NP, V —■
take and P off. Furthermore, VP VP NP may
be represented in terms of VP —+ V PP and PP
</bodyText>
<figureCaption confidence="0.58724125">
Figure 5: Some words from a lexicon learned from
55,000 utterances of continuous, dictated Wall Street
Journal articles. Although many words are seem-
ingly random, words representing million dollars,
</figureCaption>
<bodyText confidence="0.99344375">
Goldman-Sachs, thousand, etc. are learned. Further-
more, as word 8950 (long time) shows, they are often
properly decomposed into components.
P NP. In this way syntactic structure emerges in the
internal representation of words. This sort of gram-
mar offers significant advantages over context-free
grammars in that non-independent rule expansions
can be accounted for. We are currently looking at
various methods for automatically acquiring parts of
speech; in initial experiments some of the first such
classes learned are the class of vowels, of consonants,
and of verb endings.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.989684666666667">
No previous unsupervised language-learning proce-
dure has produced structures that match so closely
with linguistic intuitions. We take this as a vindi-
cation of the perturbation-of-compositions represen-
tation. Its ability to capture the statistical and lin-
guistic idiosyncrasies of large structures without sac-
</bodyText>
<page confidence="0.988812">
340
</page>
<bodyText confidence="0.999834666666667">
rificing the obvious regularities within them makes it
a valuable tool for a wide variety of induction prob-
lems.
</bodyText>
<sectionHeader confidence="0.996384" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805161764706">
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ing in the statistical analysis of probabalistic functions
in markov chains. Annals of Mathematical Statistics,
41:164-171.
Glenn Carroll and Eugene Charniak. 1992. Learn-
ing probabalistic dependency grammars from labelled
text. In Working Notes, Fall Symposium Series,
AAAI, pages 25-31.
Timothy Andrew Cartwright and Michael R. Brent.
1994. Segmenting speech without a lexicon: Evidence
for a bootstrapping model of lexical acquisition. In
Proc. of the 16th Annual Meeting of the Cognitive Sci-
ence Society, Hillsdale, New Jersey.
Stanley F. Chen. 1995. Bayesian grammar induction for
language modeling. In Proc. 32nd Annual Meeting of
the Association for Computational Linguistics, pages
228-235, Cambridge, Massachusetts.
Noam A. Chomsky. 1955. The Logical Structure of Lin-
guistic Theory. Plenum Press, New York.
Carl de Marcken. 1995a. Lexical heads, phrase structure
and the induction of grammar. In Third Workshop on
Very Large Corpora, Cambridge, Massachusetts.
Carl de Marcken. 1995b. The unsupervised acquisition
of a lexicon from continuous speech. Memo A.I. Memo
1558, MIT Artificial Intelligence Lab., Cambridge,
Massachusetts.
Sabine Deligne and Frederic Bimbot. 1995. Language
modeling by variable length sequences: Theoretical
formulation and evaluation of multigrams. In Proceed-
ings of the International Conference on Speech and
Signal Processing, volume 1, pages 169-172.
A. P. Dempster, N. M. Liard, and D. B. Rubin. 1977.
Maximum lildihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
B(39):1-38.
T. Mark Ellison. 1992. The Machine Learning of Phono-
logical Structure. Ph.D. thesis, University of Western
Australia.
W. N. Francis and H. Kucera. 1982. Frequency analysis
of English usage: lexicon and grammar. Houghton-
Mifflin, Boston.
Zellig Harris. 1968. Mathematical Structure of Lan-
guage. Wiley, New York.
Donald Cort Olivier. 1968. Stochastic Grammars and
Language Acquisition Mechanisms. Ph.D. thesis, Har-
vard University, Cambridge, Massachusetts.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proc. 29th Annual Meeting of the Association for
Computational Linguistics, pages 128-135, Berkeley,
California.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14:465-471.
Jorma Rissanen. 1989. Stochastic Complexity in Statis-
tical Inquiry. World Scientific, Singapore.
R. J. Solomonoff. 1960. The mechanization of linguis-
tic learning. In Proceedings of the 2nd International
Conference on Cybernetics, pages 180-193.
Andreas Stolcke. 1994. Bayesian Learning of Proba-
balis ticLanguage Models. Ph.D. thesis, University of
California at Berkeley, Berkeley, CA.
J. Gerald Wolff. 1982. Language acquisition, data com-
pression and generalization. Language and Communi-
cation, 2(1):57-89.
J. Ziv and A. Lempel. 1978. Compression of individual
sequences by variable rate coding. IEEE Transactions
on Information Theory, 24:530-536.
</reference>
<page confidence="0.998716">
341
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003124">
<title confidence="0.99989">Linguistic Structure as Composition and Perturbation</title>
<author confidence="0.986018">Carl de_Marcken</author>
<affiliation confidence="0.551041">MIT AT Laboratory, NE43-769</affiliation>
<address confidence="0.997369">545 Technology Square Cambridge, MA, 02139, USA</address>
<email confidence="0.999728">cgdemarc©ai.mit.edu</email>
<abstract confidence="0.99156515467075">This paper discusses the problem of learning language from unprocessed text and speech signals, concentrating on the problem of learning a lexicon. In particular, it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters. The power of the representation is demonstrated by several examples in text segmentation and compression, acquisition of a lexicon from raw speech, and the acquisition of mappings between text and artificial representations of meaning. 1 Motivation Language is a robust and necessarily redundant communication mechanism. Its redundancies commonly manifest themselves as predictable patterns in speech and text signals, and it is largely these patterns that enable text and speech compression. Naturally, many patterns in text and speech reflect interesting properties of language. For exboth an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns sequence was a dark and stormy night is in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of linguistic and extra-linguistic processes. Such patterns can be indistinguishable from desired ones. For example, in the Brown corpus (Francis and 1982) her nose 5 times, a corpus-specific idiosyncrasy. This phrase has the structure as the idiom the bucket. is difficult to imagine any induction algorithm learnthe bucket this corpus without also learning her nose. 1.1.2 The Definition of Interesting This discussion presumes there is a set of desired patterns to extract from input signals. What is this For example, is the bucket proper lexical unit? The answer depends on factors external to the unsupervised learning framework. For the purposes of machine translation or information retrieval this sequence is an important idiom, but with respect to speech recognition it is unremarkable. Similar questions could be asked of subword units like syllables. Plainly, the answers depends on the learning context, and not on the signal itself. 1.1.3 The Definition of Pattern Any statistical definition of pattern depends on underlying model. For instance, the sequence much more frequently than one would expect given an independence assumption about letters. But for a model with knowledge of syntax and word frequencies, there is nothing remarkable about the phrase. Since all existing models have flaws, patterns will always be learned that are artifacts of imperfections in the learning algorithm. These examples seem to imply that unsupervised induction will never converge to ideal grammars and lexicons. While there is truth to this, the rest of this paper describes a representation of language that bypasses many of the apparent difficulties. 335 [national football league] I [national] [football] [league] [nation] [al] [foot] [ball] [lea] [gue] \ A / [n] [t] [b] [1] [g] Figure 1: A compositional representation. 2 A Compositional Representation The examples in sections 1.1.1 and 1.1.2 seem to imply that any unsupervised language learning program that returns only one segmentation of the input is bound to make many mistakes. And section 1.1.3 implies that the decisions about linguistic units must be made relative to their representations. Both problems can be solved if linguistic units (for now, words in the lexicon) are built by composition other units. For example, the bucket built by composing the course, if a word is merely the composition of its parts, there is nothing interesting about it and no reason to include it in the lexicon. So the motivation for including a word in the lexicon must be that it function differently from its parts. Thus a word is a perturbation of a composition. the case of the bucket perturbation is of both meaning and frequency. For nose perturbation may just be frequency.&apos; This is a very natural representation from the viewpoint of language. It correctly predicts that both phrases inherit their sound and syntax from their component words. At the same time it leaves open the possibility that idiosyncratic information will be to the whole, as with the meaning of kickthe bucket. structure is very much like the class hierarchy of a modern programming language. It is not the same thing as a context-free grammar, since each word does not act in the same way as the default composition of its components. Figure 1 illustrates a recursive decomposition (unconcatenation) of the phrase football phrase is broken into three words, each of which are also decomposed in the lexicon. This process bottoms out in the terminal characters. This is a real decomposition achieved by a program dein section 4. Not shown are the perturbasimple composition operator is but in section 6 a more interesting one is discussed. an unsupervised learning algorithm with no access to meaning will not treat them differently. CodeLengthComponents = 2 = 010 = 2 , = 4 Cott Cthe = • • Figure 2: A coding of the first few words of a hypothetical lexicon. The first two columns can be coded succinctly, leaving the cost of pointers to component words as the dominant cost of both the lexicon and the representation of the input. tions (in this case merely frequency changes) that distinguish each word from its parts. This general framework extends to other perturbations. For exthe word naturally thought of as composition of want and a sound change. in speech the three different words two well inherit the sound of a common ancestor while introducing new syntactic and semantic properties. 2.1 Coding Of course, for this representation to be more than an intuition both the composition and perturbation operators must be exactly specified. In particular, a code must be designed that enables a word (or a be expressed in terms of its parts. As a simple example, suppose that the composition operator is concatenation, that terminals are characters, and that the only perturbation operator is the ability to express the frequency of a word independently of the frequency of its parts. Then to code either a sentence of the input or a (nonterminal) word in the lexicon, the number of component words in the representation must be written, followed by a code for each component word. Naturally, each word in the lexicon must be associated with its code, and under a near-optimal coding scheme like a Huffman code, the code length will be related to the frequency of the word. Thus, associating a word with a code substitutes for writing down the frequency of a word. Furthermore, if words are written down in order of decreasing frequency, a Huffman code for a large lexicon can be specified using a negligible number of bits. This and the near-negligible cost of writing down word lengths will not be discussed further. Figure 2 presents a portion of an encoding of a hypothetical lexicon. 2.2 MDL Given a coding scheme and a particular lexicon (and a parsing algorithm) it is in theory possible to calcuminimum length of a given input. 336 Part of the encoding will be devoted to the lexicon, the rest to representing the input in terms of the lexicon. The lexicon that minimizes the combined description length of the lexicon and the input maximally compresses the input. In the sense of Rissanen&apos;s minimum description-length (MDL) principle (Rissanen, 1978; Rissanen, 1989) this lexicon is the theory that best explains the data, and one can hope that the patterns in the lexicon reflect the underlying mechanisms and parameters of the language that generated the input. 2.3 Properties of the Representation Representing words in the lexicon as perturbations of compositions has a number of desirable properties. • The choice of composition and perturbation operators captures a particular detailed theory of language. They can be used, for instance, to reference sophisticated phonological and morphological mechanisms. • The length of the description of a word is a measure of its linguistic plausibility, and can serve as a buffer against learning unnatural coincidences. Coincidences like her nose not exclude desired structure, since they are further broken down into components that they inherit properties from. Structure is shared: the words share the common substructure with as its sound and meaning. As a consequence, data is pooled for estimation, and representations are compact. • Common irregular forms are compiled out. For if went is represented in terms of (presumably to save the cost of unnecessarily reproducing syntactic and semantic properties) the complex sound change need only be represented once, not every time went is used. • Since parameters (words) have compact representations, they are cheap from a description length standpoint, and many can be included in the lexicon. This allows learning algorithms to fit detailed statistical properties of the data. This coding scheme is very similar to that found in popular dictionary-based compression schemes like LZ78 (Ziv and Lempel, 1978). It is capable of compressing a sequence of identical characters of length n to size 0(log n). However, in contrast to compression schemes like LZ78 that use deterministic rules to add parameters to the dictionary (and do not arrive at linguistically plausible parameters), it is possible to, perform more sophisticated searches in this representation. Start with lexicon of terminals. Iterate Iterate (EM) Parse input and words using current lexicon. Use word counts to update frequencies. Add words to the lexicon. Iterate (EM) Parse input and words using current lexicon. Use word counts to update frequencies. Delete words from the lexicon. Figure 3: An iterative search algorithm. Two iterations of the inner loops are usually sufficient for convergence, and for the tests described in this paper after 10 iterations of the outer loop there is little change in the lexicon in terms of either compression performance or structure. 3 A Search Algorithm Since the class of possible lexicons is infinite, the minimization of description length is necessarily heuristic. Given a fixed lexicon, the expectationmaximization algorithm (Dempster et al., 1977) can be used to arrive at a (locally) optimal set of frequencies and codelengths for the words in the lexicon. For composition by concatenation, the algorithm reduces to the special case of the Baum-Welch procedure (Baum et al., 1970) discussed in (Deligne and Bimbot, 1995). In general, however, the parsing and reestimation involved in EM can be considerably more complicated. To update the structure of the lexicon, words can be added or deleted from it if this is predicted to reduce the description length of the input. This algorithm is summarized in fig- 3.1 Adding and Deleting Words For words to be added to the lexicon, two things are needed. The first is a means of hypothesizing candidate new words. The second is a means of evaluating candidates. One reasonable means of generating candidates is to look at pairs (or triples) of words that are composed in the parses of words and sentences of the input. Since words are built by composing other words and act like their composition, a new word can be created from such a pair and substituted in place of the pair wherever the pair appears. example, if water and frequently comthen a good candidate for a new word is melon = watermelon, o is the concatenation the composition operators and test sets we have looked at, using single (Viterbi) parses produces almost exactly the same results (in terms of both compression and lexical structure) as summing probabilities over multiple parses. 337 operator. In order to evaluate whether the addition of such a new word is likely to reduce the description length of the input, it is necessary to record during the EM step the extra statistics of how many times the composed pairs occur in parses. The effect on description length of adding a new word can not be exactly computed. Its addition will not only affect other words, but may also cause other words to be added or deleted. Furthermore, it is more computationally efficient to add and delete many words simultaneously, and this complicates the estimation of the change in description length. Fortunately, simple approximations of the change are adequate. For example, if Viterbi analyses are used then the new word comtake the place of all compositions of reduces the counts of water and though they are each used once in the representation of watermelon. If it is assumed that no other word counts change, these assumptions allow one to predict the counts and probabilities of all words after the change. Since the codelength of a. word w with probability p(w) is approximately — log p(w), the total estimated change in description of adding a new word W to a lexicon is A —c&apos; (W) log p&apos;(W) d.l. (changes) + (w) c(w) log p(w)) wEL where c(w) is the count of the word w, primes indicated counts and probabilities after the change and d.1.(changes) represents the cost of writing down the perturbations involved in the representation of W. If A &lt; 0 the word is predicted to reduce the total description length and is added to the lexicon. Similar heuristics can be used to estimate the benefit of 3.2 Search Properties A significant source of problems in traditional grammar induction techniques is local minima (de Marcken, 1995a; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The search algorithm described above avoids many of these problems. The reason is that hidden structure is largely a &amp;quot;compile-time&amp;quot; phenomena. During parsing all that is important about a word is its surface form and codelength. The internal representation does not matter. Therefore, the internal representation is free to reorganize at any time; it has been decoupled. This allows structure to be built bottom up or for structure to emerge inside already existing parameters. Furthermore, since parameters (words) encode surface patterns, it Marcken, 1995b) for more detailed discussion of these estimations. The actual formulas used in the tests presented in this paper are slightly more complicated than presented here. easy to determine when they are useful, and their use is limited. They usually do not have competing roles, in contrast, for instance, to hidden nodes in neural networks. And since there are no fixed number of parameters, when words do start to have multiple disparate uses, they can be split with common substructure shared. Finally, since add and delete cycles can compensate for initial mistakes, inexact heuristics can be used for adding and deleting words. 4 Concatenation Results The simplest reasonable instantiation of the composition-and-perturbation framework is with the concatenation operator and frequency perturbation. This instantiation is easily tested on problems of text segmentation and compression. Given a text document, the search algorithm can be used to learn a lexicon that minimizes its description length. For testing purposes, spaces will be removed from input text and true words will be defined to be minimal sequences bordered by spaces in the original input). The search algorithm parses the input as it compresses it, and can therefore output a segmentation of the input in terms of words drawn from the lexicon. These words are themselves decomposed in the lexicon, and can be considered to form a tree that terminates in the characters of the sentence. tree can have no more than for a with n characters, though there are possible &amp;quot;true words&amp;quot; in the input sentence; thus, the tree contains considerable information. Define the percentage of true words that occur at some level of the segmentation-tree. Define be the percentage of true words that violate the segmentation-tree structure.&apos; The search algorithm was applied to two texts, a lowercase version of the million-word Brown corwith spaces and punctuation removed, and million characters of Chinese news articles in a twobyte/character format. In the case of the Chinese, which contains no inherent separators like spaces, segmentation performance is measured relative to another computer segmentation program that had access to a (human-created) lexicon. The algorithm was given the raw encoding and had to deduce the internal two-byte structure. In the case of the Brown corpus, word recall was 90.5% and crossing-brackets was 1.7%. For the Chinese word recall was 96.9% crossing-brackets was In case of both English and Chinese, most of the unfound words were words that occurred only once in the corpus. Thus, the algorithm has done an extremely good job of learning words and properly using them to segment the input. Furthermore, the crossing-bracket true word the input a violation of the segmentation 338 Rank Word 0 [s] 1 [the] 3 [a] 5 [in] 6 [to] 500 [students] 501 [material] 502 [um] 503 [words] 504 [period] 505 [class] 506 [question] 5000 [ [ing] [them]] 5001 [ [mon] [k]3 [[re) 5003 [[rig] [id]] 5004 [connect] [ad]] [ [103 5006 [[hu] [t]] [ [blood] 26001 E [anordinary] [happy] [family]] [ [f] [ibility] [of)) [lunar) [brightness] [distribution]] 26004 [ [primarily] [diff] [using]] 26005 [[sodium] [tri] [polyphosphate]] 26006 [ [charcoal] [broil] [ed]] Figure 4: Sections of the lexicon learned from the Brown corpus, ranked by frequency. The words in the less-frequent half are listed with their first-level decomposition. Word 5000 causes crossing-bracket violations, and words 26002 and 26006 have internal structure that causes recall violations. measure indicates that the algorithm has made very few clear mistakes. Of course, the hierarchical lexical representation does not make a commitment to what levels are &amp;quot;true words&amp;quot; and which are not; about 5 times more internal nodes exist than true words. Experiments in section 5 demonstrate that for most applications this is not only not a problem, but desirable. Figure 4 displays some of the lexicon learned from the Brown corpus. The algorithm was also run as a compressor on a lower-case version of the Brown corpus with spaces and punctuation left in. All bits necessary for exactly reproducing the input were counted. Compression performance is 2.12 bits/char, signifilower than popular algorithms like bits/char). This is the best text compression result on this corpus that we are aware of, and should not be confused with lower figures that do not include the cost of parameters. Furthermore, because the compressed text is stored in terms of linguistic units like words, it can be searched, indexed, and parsed without decompression. 5 Learning Meanings Unsupervised learning algorithms are rarely used in isolation. The goal of this work has been to explain how linguistic units like words can be learned, so that other processes can make use of these units. In this section a means of learning the mappings between words and artificial representations of meanings is described. The composition-andperturbation encompasses this application neatly. Imagine that text utterances are paired with repof and that the goal is to find the minimum-length description of both the text and the meaning. If there is mutual information between the meaning and text portions of the input, then better compression is achieved if the two streams are compressed simultaneously. If a text word can have some associated meaning, then writing down that word to account for some portion of text also accounts for some portion of the meaning of that text. The remaining meaning can be written down more succinctly. Thus, there is an incentive to associate meaning with sound, although of course the association pays a price in the description of the lexicon. Although it is obviously a naive simplification, many of the interesting properties of the compositional representation surface even when meanings are treating as sets of arbitrary symbols. A word is now both a character sequence and a set of symbols. The composition operator concatenates the characters and unions the meaning symbols. Of course, there must be some way to alter the default meaning of a word. One way to do this is to explicitly write out any symbols that are present in the word&apos;s meanbut not in its components, or versa. word might be represented as ro eo d+RED. Given an existing word berry {BERRY}, red berry BERRY} can be represented co ro ao no berry {BERRY}+RED. 5.1 Results To test the algorithm&apos;s ability to infer word meanings, 10,000 utterances from an unsegmented textual database of mothers&apos; speech to children were paired with representations of meaning, constructed by assigning a unique symbol to each root word in the vo- For example, the sentence what is he a picture of? paired with the unordered meaning AND WHAT BE HE PAINT A PIC- TURE OF. In the first experiment, the algorithm received these pairs with no noise or ambiguity, using an encoding of meaning symbols such that each symbol&apos;s length was 10 bits. After 8 iterations of training without meaning and then a further 8 iterations with, the text sequences were parsed again without access to the true meaning. The meanings &apos;This framework is easily extended to handle multiple ambiguous meanings (with and without priors) and noise, but these extensions will not be discussed here. 339 of the resulting word sequences were compared with the true meanings. Symbol accuracy was 98.9%, recall was 93.6%. Used to differentiate the true meaning from the meanings of the previous 20 sentences, the program selected correctly 89.1% of the time, or ranked the true meaning tied for first 10.8% of the time. A second test was performed in which the algorithm received three possible meanings for each utterance, the true one and also the meaning of the two surrounding utterances. A uniform prior was used. Symbol accuracy was again 98.9%, recall was 75.3%. The final lexicon includes extended phrases, but meanings tend to filter down to the proper level. instance, although the words ducks, the exist and contain the meaning DUCK, the symbol is only written into the deof others inherit it. Similar results hold for similar experiments on the Brown cor- For example, her nose its completely from its parts, while the not. This is exactly the result argued for in the motivation section of this paper, and illustrates why occasional extra words in the lexicon are not a problem for most applications. 6 Other Applications and Current Work NDrjmr] [0 [auzn]) [tah] d] [Ek[tid]] [An (iin]] [mcliindalr]z) [ai] diiz] sik [rti]] [log ] [tairn]] [sElc] [gin]] [wAn]pn] [v [en] [dr]] [a [limin] [ [mei] i[id [be [liindal]] Rgoul] d [rnin]s [mks]] akmp] [au t]r] [ga[vrrnin]] [oubl][zahuou]] [min]i[streiiin]] [[tje]r[in]] [linbl][hahwou]] [pion] (bou][skg]i] [[kg] [611] [[goul]d[maiinz]] Rkorpr][eitld]] Rank 5392 (w3rmr] 5393 (0auzn] 5394 [tahId] 5395 [ektld] 5396 (Aniin] 5397 (meliindalrz] 8948 aidiiz] 8949 sikrti] 8950 (lop taim] 8951 (sekgIn] 8952 [wAnpA] 8953 (vendEr] 8954 (allmInei] 8955 [meliiD] 8956 beliindal] 9164 [gouldminswks] 9165 kmpS&apos;utr] 9166 [gavrmin] 9167 [oublzahuou] 9168 (ministreign] 9169 (tjerin] 9170 [hAblhahwou] We have performed other experiments using this representation and search algorithm, on tasks in unsupervised learning from speech and grammar induction. Figure 5 contains a small portion of a lexicon learned from 55,000 utterances of continuous speech by multiple speakers. The utterances are taken from dictated Wall Street Journal articles. The concatenation operators was used with phonemes as terminals. A second layer was added to the framework to map from phonemes to speech; these extensions are described in more detail in (de Marcken, 1995b). The sound model of each phoneme was learned separately using supervised training on different, segmented speech. Although the phoneme model is extremely poor, many words are recognizable, and this is the first significant lexicon learned directly from spoken speech without supervision. If the composition operator makes use of context, then the representation extends naturally to a more powerful form of context-free grammars, where composition is tree-insertion. In particular, if each word is associated with a part-of-speech, and parts of speech are permissible terminals in the lexicon, then &amp;quot;words&amp;quot; become production rules. For example, a might be take off NP represented terms of the composition of V P NP, V off. VP NP represented in terms of V PP Figure 5: Some words from a lexicon learned from 55,000 utterances of continuous, dictated Wall Street Journal articles. Although many words are seemrandom, words representing dollars, thousand, etc. learned. Furtheras word 8950 shows, they are often properly decomposed into components. NP. this way syntactic structure emerges in the internal representation of words. This sort of grammar offers significant advantages over context-free grammars in that non-independent rule expansions can be accounted for. We are currently looking at various methods for automatically acquiring parts of speech; in initial experiments some of the first such classes learned are the class of vowels, of consonants, and of verb endings. 7 Conclusions No previous unsupervised language-learning procedure has produced structures that match so closely with linguistic intuitions. We take this as a vindication of the perturbation-of-compositions representation. Its ability to capture the statistical and linidiosyncrasies of large structures without sac- 340 rificing the obvious regularities within them makes it a valuable tool for a wide variety of induction problems. References Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A maximization technique occuring in the statistical analysis of probabalistic functions markov chains. of Mathematical Statistics,</abstract>
<note confidence="0.912555984126984">41:164-171. Glenn Carroll and Eugene Charniak. 1992. Learning probabalistic dependency grammars from labelled In Notes, Fall Symposium Series, 25-31. Timothy Andrew Cartwright and Michael R. Brent. 1994. Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. In Proc. of the 16th Annual Meeting of the Cognitive Sci- Society, New Jersey. Stanley F. Chen. 1995. Bayesian grammar induction for modeling. In 32nd Annual Meeting of Association for Computational Linguistics, 228-235, Cambridge, Massachusetts. A. Chomsky. 1955. Logical Structure of Lin- Theory. Press, New York. Carl de Marcken. 1995a. Lexical heads, phrase structure the induction of grammar. In Workshop on Large Corpora, Massachusetts. Carl de Marcken. 1995b. The unsupervised acquisition of a lexicon from continuous speech. Memo A.I. Memo 1558, MIT Artificial Intelligence Lab., Cambridge, Massachusetts. Sabine Deligne and Frederic Bimbot. 1995. Language modeling by variable length sequences: Theoretical and evaluation of multigrams. In Proceedings of the International Conference on Speech and Processing, 1, pages 169-172. A. P. Dempster, N. M. Liard, and D. B. Rubin. 1977. Maximum lildihood from incomplete data via the EM Journal the Royal Statistical Society, B(39):1-38. Mark Ellison. 1992. Machine Learning of Phono- Structure. thesis, University of Western Australia. N. Francis and H. Kucera. 1982. Frequency English usage: lexicon and Houghton- Mifflin, Boston. Harris. 1968. Structure of Lan- New York. Cort Olivier. 1968. Grammars and Acquisition Mechanisms. thesis, Harvard University, Cambridge, Massachusetts. Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. 29th Annual Meeting of the Association for Linguistics, 128-135, Berkeley, California. Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465-471. Rissanen. 1989. Complexity in Statis- World Scientific, Singapore. R. J. Solomonoff. 1960. The mechanization of linguislearning. In of the 2nd International on Cybernetics, 180-193. Stolcke. 1994. Bayesian Learning ProbaticLanguage Models. thesis, University of California at Berkeley, Berkeley, CA. J. Gerald Wolff. 1982. Language acquisition, data comand generalization. Communication, 2(1):57-89. J. Ziv and A. Lempel. 1978. Compression of individual by variable rate coding. Transactions</note>
<affiliation confidence="0.859548">Information Theory,</affiliation>
<address confidence="0.87341">341</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
<author>George Soules</author>
<author>Norman Weiss</author>
</authors>
<title>A maximization technique occuring in the statistical analysis of probabalistic functions in markov chains.</title>
<date>1970</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>41--164</pages>
<contexts>
<context position="11955" citStr="Baum et al., 1970" startWordPosition="1958" endWordPosition="1961"> tests described in this paper after 10 iterations of the outer loop there is little change in the lexicon in terms of either compression performance or structure. 3 A Search Algorithm Since the class of possible lexicons is infinite, the minimization of description length is necessarily heuristic. Given a fixed lexicon, the expectationmaximization algorithm (Dempster et al., 1977) can be used to arrive at a (locally) optimal set of frequencies and codelengths for the words in the lexicon. For composition by concatenation, the algorithm reduces to the special case of the Baum-Welch procedure (Baum et al., 1970) discussed in (Deligne and Bimbot, 1995). In general, however, the parsing and reestimation involved in EM can be considerably more complicated. To update the structure of the lexicon, words can be added or deleted from it if this is predicted to reduce the description length of the input. This algorithm is summarized in figure 3.3 3.1 Adding and Deleting Words For words to be added to the lexicon, two things are needed. The first is a means of hypothesizing candidate new words. The second is a means of evaluating candidates. One reasonable means of generating candidates is to look at pairs (o</context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A maximization technique occuring in the statistical analysis of probabalistic functions in markov chains. Annals of Mathematical Statistics, 41:164-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Learning probabalistic dependency grammars from labelled text.</title>
<date>1992</date>
<booktitle>In Working Notes, Fall Symposium Series, AAAI,</booktitle>
<pages>25--31</pages>
<contexts>
<context position="15129" citStr="Carroll and Charniak, 1992" startWordPosition="2511" endWordPosition="2514">) d.l. (changes) + E(-e(w) log pi (w) c(w) log p(w)) wEL where c(w) is the count of the word w, primes indicated counts and probabilities after the change and d.1.(changes) represents the cost of writing down the perturbations involved in the representation of W. If A &lt; 0 the word is predicted to reduce the total description length and is added to the lexicon. Similar heuristics can be used to estimate the benefit of deleting words.4 3.2 Search Properties A significant source of problems in traditional grammar induction techniques is local minima (de Marcken, 1995a; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The search algorithm described above avoids many of these problems. The reason is that hidden structure is largely a &amp;quot;compile-time&amp;quot; phenomena. During parsing all that is important about a word is its surface form and codelength. The internal representation does not matter. Therefore, the internal representation is free to reorganize at any time; it has been decoupled. This allows structure to be built bottom up or for structure to emerge inside already existing parameters. Furthermore, since parameters (words) encode surface patterns, it 4See (de Marcken, 1995b) for more detailed discussion </context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Learning probabalistic dependency grammars from labelled text. In Working Notes, Fall Symposium Series, AAAI, pages 25-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Andrew Cartwright</author>
<author>Michael R Brent</author>
</authors>
<title>Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition.</title>
<date>1994</date>
<booktitle>In Proc. of the 16th Annual Meeting of the Cognitive Science Society,</booktitle>
<location>Hillsdale, New Jersey.</location>
<contexts>
<context position="1660" citStr="Cartwright and Brent, 1994" startWordPosition="245" endWordPosition="248">resting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of linguistic and extra-linguistic processes. Such patterns can be indistinguishable fro</context>
</contexts>
<marker>Cartwright, Brent, 1994</marker>
<rawString>Timothy Andrew Cartwright and Michael R. Brent. 1994. Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. In Proc. of the 16th Annual Meeting of the Cognitive Science Society, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Bayesian grammar induction for language modeling.</title>
<date>1995</date>
<booktitle>In Proc. 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>228--235</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1631" citStr="Chen, 1995" startWordPosition="243" endWordPosition="244">reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of linguistic and extra-linguistic processes. Such patterns</context>
</contexts>
<marker>Chen, 1995</marker>
<rawString>Stanley F. Chen. 1995. Bayesian grammar induction for language modeling. In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, pages 228-235, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam A Chomsky</author>
</authors>
<title>The Logical Structure of Linguistic Theory.</title>
<date>1955</date>
<publisher>Plenum Press,</publisher>
<location>New York.</location>
<contexts>
<context position="1389" citStr="Chomsky (1955)" startWordPosition="206" endWordPosition="207">dant communication mechanism. Its redundancies commonly manifest themselves as predictable patterns in speech and text signals, and it is largely these patterns that enable text and speech compression. Naturally, many patterns in text and speech reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occu</context>
</contexts>
<marker>Chomsky, 1955</marker>
<rawString>Noam A. Chomsky. 1955. The Logical Structure of Linguistic Theory. Plenum Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Lexical heads, phrase structure and the induction of grammar.</title>
<date>1995</date>
<booktitle>In Third Workshop on Very Large Corpora,</booktitle>
<location>Cambridge, Massachusetts.</location>
<marker>de Marcken, 1995</marker>
<rawString>Carl de Marcken. 1995a. Lexical heads, phrase structure and the induction of grammar. In Third Workshop on Very Large Corpora, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>The unsupervised acquisition of a lexicon from continuous speech.</title>
<date>1995</date>
<booktitle>Memo A.I. Memo 1558, MIT Artificial Intelligence Lab.,</booktitle>
<location>Cambridge, Massachusetts.</location>
<marker>de Marcken, 1995</marker>
<rawString>Carl de Marcken. 1995b. The unsupervised acquisition of a lexicon from continuous speech. Memo A.I. Memo 1558, MIT Artificial Intelligence Lab., Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Deligne</author>
<author>Frederic Bimbot</author>
</authors>
<title>Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>169--172</pages>
<contexts>
<context position="11995" citStr="Deligne and Bimbot, 1995" startWordPosition="1964" endWordPosition="1967">ter 10 iterations of the outer loop there is little change in the lexicon in terms of either compression performance or structure. 3 A Search Algorithm Since the class of possible lexicons is infinite, the minimization of description length is necessarily heuristic. Given a fixed lexicon, the expectationmaximization algorithm (Dempster et al., 1977) can be used to arrive at a (locally) optimal set of frequencies and codelengths for the words in the lexicon. For composition by concatenation, the algorithm reduces to the special case of the Baum-Welch procedure (Baum et al., 1970) discussed in (Deligne and Bimbot, 1995). In general, however, the parsing and reestimation involved in EM can be considerably more complicated. To update the structure of the lexicon, words can be added or deleted from it if this is predicted to reduce the description length of the input. This algorithm is summarized in figure 3.3 3.1 Adding and Deleting Words For words to be added to the lexicon, two things are needed. The first is a means of hypothesizing candidate new words. The second is a means of evaluating candidates. One reasonable means of generating candidates is to look at pairs (or triples) of words that are composed in</context>
</contexts>
<marker>Deligne, Bimbot, 1995</marker>
<rawString>Sabine Deligne and Frederic Bimbot. 1995. Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams. In Proceedings of the International Conference on Speech and Signal Processing, volume 1, pages 169-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Liard</author>
<author>D B Rubin</author>
</authors>
<title>Maximum lildihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="11721" citStr="Dempster et al., 1977" startWordPosition="1916" endWordPosition="1919">se input and words using current lexicon. Use word counts to update frequencies. Delete words from the lexicon. Figure 3: An iterative search algorithm. Two iterations of the inner loops are usually sufficient for convergence, and for the tests described in this paper after 10 iterations of the outer loop there is little change in the lexicon in terms of either compression performance or structure. 3 A Search Algorithm Since the class of possible lexicons is infinite, the minimization of description length is necessarily heuristic. Given a fixed lexicon, the expectationmaximization algorithm (Dempster et al., 1977) can be used to arrive at a (locally) optimal set of frequencies and codelengths for the words in the lexicon. For composition by concatenation, the algorithm reduces to the special case of the Baum-Welch procedure (Baum et al., 1970) discussed in (Deligne and Bimbot, 1995). In general, however, the parsing and reestimation involved in EM can be considerably more complicated. To update the structure of the lexicon, words can be added or deleted from it if this is predicted to reduce the description length of the input. This algorithm is summarized in figure 3.3 3.1 Adding and Deleting Words Fo</context>
</contexts>
<marker>Dempster, Liard, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Liard, and D. B. Rubin. 1977. Maximum lildihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B(39):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
</authors>
<title>The Machine Learning of Phonological Structure.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Western Australia.</institution>
<contexts>
<context position="1604" citStr="Ellison, 1992" startWordPosition="239" endWordPosition="240">y patterns in text and speech reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of linguistic and extra-linguist</context>
</contexts>
<marker>Ellison, 1992</marker>
<rawString>T. Mark Ellison. 1992. The Machine Learning of Phonological Structure. Ph.D. thesis, University of Western Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Frequency analysis of English usage: lexicon and grammar. HoughtonMifflin,</title>
<date>1982</date>
<location>Boston.</location>
<contexts>
<context position="2335" citStr="Francis and Kucera, 1982" startWordPosition="353" endWordPosition="356">tely, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of linguistic and extra-linguistic processes. Such patterns can be indistinguishable from desired ones. For example, in the Brown corpus (Francis and Kucera, 1982) scratching her nose occurs 5 times, a corpus-specific idiosyncrasy. This phrase has the same structure as the idiom kicking the bucket. It is difficult to imagine any induction algorithm learning kicking the bucket from this corpus without also (mistakenly) learning scratching her nose. 1.1.2 The Definition of Interesting This discussion presumes there is a set of desired patterns to extract from input signals. What is this set? For example, is kicking the bucket a proper lexical unit? The answer depends on factors external to the unsupervised learning framework. For the purposes of machine t</context>
</contexts>
<marker>Francis, Kucera, 1982</marker>
<rawString>W. N. Francis and H. Kucera. 1982. Frequency analysis of English usage: lexicon and grammar. HoughtonMifflin, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structure of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="1426" citStr="Harris (1968)" startWordPosition="211" endWordPosition="212">ndancies commonly manifest themselves as predictable patterns in speech and text signals, and it is largely these patterns that enable text and speech compression. Naturally, many patterns in text and speech reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than t</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical Structure of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Cort Olivier</author>
</authors>
<title>Stochastic Grammars and Language Acquisition Mechanisms.</title>
<date>1968</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1576" citStr="Olivier, 1968" startWordPosition="235" endWordPosition="236"> compression. Naturally, many patterns in text and speech reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of l</context>
</contexts>
<marker>Olivier, 1968</marker>
<rawString>Donald Cort Olivier. 1968. Stochastic Grammars and Language Acquisition Mechanisms. Ph.D. thesis, Harvard University, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proc. 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>Berkeley, California.</location>
<contexts>
<context position="15100" citStr="Pereira and Schabes, 1992" startWordPosition="2507" endWordPosition="2510">con L is A —c&apos; (W) log p&apos;(W) d.l. (changes) + E(-e(w) log pi (w) c(w) log p(w)) wEL where c(w) is the count of the word w, primes indicated counts and probabilities after the change and d.1.(changes) represents the cost of writing down the perturbations involved in the representation of W. If A &lt; 0 the word is predicted to reduce the total description length and is added to the lexicon. Similar heuristics can be used to estimate the benefit of deleting words.4 3.2 Search Properties A significant source of problems in traditional grammar induction techniques is local minima (de Marcken, 1995a; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The search algorithm described above avoids many of these problems. The reason is that hidden structure is largely a &amp;quot;compile-time&amp;quot; phenomena. During parsing all that is important about a word is its surface form and codelength. The internal representation does not matter. Therefore, the internal representation is free to reorganize at any time; it has been decoupled. This allows structure to be built bottom up or for structure to emerge inside already existing parameters. Furthermore, since parameters (words) encode surface patterns, it 4See (de Marcken, 1995b) </context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proc. 29th Annual Meeting of the Association for Computational Linguistics, pages 128-135, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<pages>14--465</pages>
<contexts>
<context position="8822" citStr="Rissanen, 1978" startWordPosition="1458" endWordPosition="1459">writing down word lengths will not be discussed further. Figure 2 presents a portion of an encoding of a hypothetical lexicon. 2.2 MDL Given a coding scheme and a particular lexicon (and a parsing algorithm) it is in theory possible to calculate the minimum length encoding of a given input. 336 Part of the encoding will be devoted to the lexicon, the rest to representing the input in terms of the lexicon. The lexicon that minimizes the combined description length of the lexicon and the input maximally compresses the input. In the sense of Rissanen&apos;s minimum description-length (MDL) principle (Rissanen, 1978; Rissanen, 1989) this lexicon is the theory that best explains the data, and one can hope that the patterns in the lexicon reflect the underlying mechanisms and parameters of the language that generated the input. 2.3 Properties of the Representation Representing words in the lexicon as perturbations of compositions has a number of desirable properties. • The choice of composition and perturbation operators captures a particular detailed theory of language. They can be used, for instance, to reference sophisticated phonological and morphological mechanisms. • The length of the description of </context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465-471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<date>1989</date>
<booktitle>Stochastic Complexity in Statistical Inquiry. World Scientific,</booktitle>
<contexts>
<context position="8839" citStr="Rissanen, 1989" startWordPosition="1460" endWordPosition="1461">d lengths will not be discussed further. Figure 2 presents a portion of an encoding of a hypothetical lexicon. 2.2 MDL Given a coding scheme and a particular lexicon (and a parsing algorithm) it is in theory possible to calculate the minimum length encoding of a given input. 336 Part of the encoding will be devoted to the lexicon, the rest to representing the input in terms of the lexicon. The lexicon that minimizes the combined description length of the lexicon and the input maximally compresses the input. In the sense of Rissanen&apos;s minimum description-length (MDL) principle (Rissanen, 1978; Rissanen, 1989) this lexicon is the theory that best explains the data, and one can hope that the patterns in the lexicon reflect the underlying mechanisms and parameters of the language that generated the input. 2.3 Properties of the Representation Representing words in the lexicon as perturbations of compositions has a number of desirable properties. • The choice of composition and perturbation operators captures a particular detailed theory of language. They can be used, for instance, to reference sophisticated phonological and morphological mechanisms. • The length of the description of a word is a measu</context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>Jorma Rissanen. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Solomonoff</author>
</authors>
<title>The mechanization of linguistic learning.</title>
<date>1960</date>
<booktitle>In Proceedings of the 2nd International Conference on Cybernetics,</booktitle>
<pages>180--193</pages>
<contexts>
<context position="1408" citStr="Solomonoff (1960)" startWordPosition="208" endWordPosition="209">on mechanism. Its redundancies commonly manifest themselves as predictable patterns in speech and text signals, and it is largely these patterns that enable text and speech compression. Naturally, many patterns in text and speech reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more</context>
</contexts>
<marker>Solomonoff, 1960</marker>
<rawString>R. J. Solomonoff. 1960. The mechanization of linguistic learning. In Proceedings of the 2nd International Conference on Cybernetics, pages 180-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Bayesian Learning of Probabalis ticLanguage Models.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="1619" citStr="Stolcke, 1994" startWordPosition="241" endWordPosition="242">ext and speech reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of linguistic and extra-linguistic processes. S</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Andreas Stolcke. 1994. Bayesian Learning of Probabalis ticLanguage Models. Ph.D. thesis, University of California at Berkeley, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerald Wolff</author>
</authors>
<title>Language acquisition, data compression and generalization.</title>
<date>1982</date>
<journal>Language and Communication,</journal>
<pages>2--1</pages>
<contexts>
<context position="1589" citStr="Wolff, 1982" startWordPosition="237" endWordPosition="238">aturally, many patterns in text and speech reflect interesting properties of language. For example, the is both an unusually frequent sequence of letters and an English word. This suggests using compression as a means of acquiring underlying properties of language from surface signals. The general methodology of language-learning-bycompression is not new. Some notable early proponents included Chomsky (1955), Solomonoff (1960) and Harris (1968), and compression has been used as the basis for a wide variety of computer programs that attack unsupervised learning in language; see (Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke, 1994; Chen, 1995; Cartwright and Brent, 1994) among others. 1.1 Patterns and Language Unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so. Three classes of examples serve to illustrate this. 1.1.1 Extralinguistic Patterns The sequence it was a dark and stormy night is a pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it a lexical or grammatical primitive: it is the product of a complex mixture of linguistic and</context>
</contexts>
<marker>Wolff, 1982</marker>
<rawString>J. Gerald Wolff. 1982. Language acquisition, data compression and generalization. Language and Communication, 2(1):57-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ziv</author>
<author>A Lempel</author>
</authors>
<title>Compression of individual sequences by variable rate coding.</title>
<date>1978</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>24--530</pages>
<contexts>
<context position="10565" citStr="Ziv and Lempel, 1978" startWordPosition="1731" endWordPosition="1734">lar forms are compiled out. For example, if went is represented in terms of go (presumably to save the cost of unnecessarily reproducing syntactic and semantic properties) the complex sound change need only be represented once, not every time went is used. • Since parameters (words) have compact representations, they are cheap from a description length standpoint, and many can be included in the lexicon. This allows learning algorithms to fit detailed statistical properties of the data. This coding scheme is very similar to that found in popular dictionary-based compression schemes like LZ78 (Ziv and Lempel, 1978). It is capable of compressing a sequence of identical characters of length n to size 0(log n). However, in contrast to compression schemes like LZ78 that use deterministic rules to add parameters to the dictionary (and do not arrive at linguistically plausible parameters), it is possible to, perform more sophisticated searches in this representation. Start with lexicon of terminals. Iterate Iterate (EM) Parse input and words using current lexicon. Use word counts to update frequencies. Add words to the lexicon. Iterate (EM) Parse input and words using current lexicon. Use word counts to updat</context>
</contexts>
<marker>Ziv, Lempel, 1978</marker>
<rawString>J. Ziv and A. Lempel. 1978. Compression of individual sequences by variable rate coding. IEEE Transactions on Information Theory, 24:530-536.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>