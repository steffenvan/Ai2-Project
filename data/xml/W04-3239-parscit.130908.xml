<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.990286">
A Boosting Algorithm for Classification of Semi-Structured Text
</title>
<author confidence="0.995428">
Taku Kudo∗ Yuji Matsumoto
</author>
<affiliation confidence="0.997475">
Graduate School of Information Science,
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.790471">
8916-5 Takayama, Ikoma Nara Japan
</address>
<email confidence="0.990368">
Itaku-ku,matsul@is.naist.jp
</email>
<sectionHeader confidence="0.984381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910764705882">
The focus of research in text classification has ex-
panded from simple topic identification to more
challenging tasks such as opinion/modality identi-
fication. Unfortunately, the latter goals exceed the
ability of the traditional bag-of-word representation
approach, and a richer, more structural representa-
tion is required. Accordingly, learning algorithms
must be created that can handle the structures ob-
served in texts. In this paper, we propose a Boosting
algorithm that captures sub-structures embedded in
texts. The proposal consists of i) decision stumps
that use subtrees as features and ii) the Boosting al-
gorithm which employs the subtree-based decision
stumps as weak learners. We also discuss the rela-
tion between our algorithm and SVMs with tree ker-
nel. Two experiments on opinion/modality classifi-
cation confirm that subtree features are important.
</bodyText>
<sectionHeader confidence="0.992544" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981023113924051">
Text classification plays an important role in orga-
nizing the online texts available on the World Wide
Web, Internet news, and E-mails. Until recently, a
number of machine learning algorithms have been
applied to this problem and have been proven suc-
cessful in many domains (Sebastiani, 2002).
In the traditional text classification tasks, one has
to identify predefined text “topics”, such as politics,
finance, sports or entertainment. For learning algo-
rithms to identify these topics, a text is usually rep-
resented as a bag-of-words, where a text is regarded
as a multi-set (i.e., a bag) of words and the word or-
der or syntactic relations appearing in the original
text is ignored. Even though the bag-of-words rep-
resentation is naive and does not convey the mean-
ing of the original text, reasonable accuracy can be
obtained. This is because each word occurring in
the text is highly relevant to the predefined “topics”
to be identified.
∗At present, NTT Communication Science Laboratories,
2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan
taku@cslab.kecl.ntt.co.jp
Given that a number of successes have been re-
ported in the field of traditional text classification,
the focus of recent research has expanded from sim-
ple topic identification to more challenging tasks
such as opinion/modality identification. Example
includes categorization of customer E-mails and re-
views by types of claims, modalities or subjectiv-
ities (Turney, 2002; Wiebe, 2000). For the lat-
ter, the traditional bag-of-words representation is
not sufficient, and a richer, structural representa-
tion is required. A straightforward way to ex-
tend the traditional bag-of-words representation is
to heuristically add new types of features to the
original bag-of-words features, such as fixed-length
n-grams (e.g., word bi-gram or tri-gram) or fixed-
length syntactic relations (e.g., modifier-head rela-
tions). These ad-hoc solutions might give us rea-
sonable performance, however, they are highly task-
dependent and require careful design to create the
“optimal” feature set for each task.
Generally speaking, by using text processing sys-
tems, a text can be converted into a semi-structured
text annotated with parts-of-speech, base-phrase in-
formation or syntactic relations. This information
is useful in identifying opinions or modalities con-
tained in the text. We think that it is more useful to
propose a learning algorithm that can automatically
capture relevant structural information observed in
text, rather than to heuristically add this informa-
tion as new features. From these points of view, this
paper proposes a classification algorithm that cap-
tures sub-structures embedded in text. To simplify
the problem, we first assume that a text to be classi-
fied is represented as a labeled ordered tree, which
is a general data structure and a simple abstraction
of text. Note that word sequence, base-phrase anno-
tation, dependency tree and an XML document can
be modeled as a labeled ordered tree.
The algorithm proposed here has the following
characteristics: i) It performs learning and classifi-
cation using structural information of text. ii) It uses
a set of all subtrees (bag-of-subtrees) for the feature
set without any constraints. iii) Even though the size
of the candidate feature set becomes quite large, it
automatically selects a compact and relevant feature
set based on Boosting.
This paper is organized as follows. First, we
describe the details of our Boosting algorithm in
which the subtree-based decision stumps are ap-
plied as weak learners. Second, we show an imple-
mentation issue related to constructing an efficient
learning algorithm. We also discuss the relation be-
tween our algorithm and SVMs (Boser et al., 1992)
with tree kernel (Collins and Duffy, 2002; Kashima
and Koyanagi, 2002). Two experiments on the opin-
ion and modality classification tasks are employed
to confirm that subtree features are important.
</bodyText>
<sectionHeader confidence="0.986909" genericHeader="method">
2 Classifier for Trees
</sectionHeader>
<bodyText confidence="0.999975769230769">
We first assume that a text to be classified is repre-
sented as a labeled ordered tree. The focused prob-
lem can be formalized as a general problem, called
the tree classification problem.
The tree classification problem is to induce a
mapping f(x) : X → {±1}, from given training
examples T = {hxi, yii}Li=1, where xi ∈ X is a
labeled ordered tree and yi ∈ {±1} is a class label
associated with each training data (we focus here
on the problem of binary classification.). The im-
portant characteristic is that the input example xi is
represented not as a numerical feature vector (bag-
of-words) but a labeled ordered tree.
</bodyText>
<subsectionHeader confidence="0.969812">
2.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.9938325">
Let us introduce a labeled ordered tree (or simply
tree), its definition and notations, first.
</bodyText>
<construct confidence="0.890067083333333">
Definition 1 Labeled ordered tree (Tree)
A labeled ordered tree is a tree where each node
is associated with a label and is ordered among its
siblings, that is, there are a first child, second child,
third child, etc.
Definition 2 Subtree
Let t and u be labeled ordered trees. We say that
t matches u, or t is a subtree of u (t ⊆ u), if
there exists a one-to-one function ψ from nodes in
t to u, satisfying the conditions: (1) ψ preserves the
parent-daughter relation, (2) ψ preserves the sib-
ling relation, (3) ψ preserves the labels.
</construct>
<bodyText confidence="0.999885333333333">
We denote the number of nodes in t as |t|. Figure 1
shows an example of a labeled ordered tree and its
subtree and non-subtree.
</bodyText>
<subsectionHeader confidence="0.999377">
2.2 Decision Stumps
</subsectionHeader>
<bodyText confidence="0.9994485">
Decision stumps are simple classifiers, where the
final decision is made by only a single hypothesis
</bodyText>
<figureCaption confidence="0.999002">
Figure 1: Labeled ordered tree and subtree relation
</figureCaption>
<bodyText confidence="0.95636625">
or feature. Boostexter (Schapire and Singer, 2000)
uses word-based decision stumps for topic-based
text classification. To classify trees, we here extend
the decision stump definition as follows.
Definition 3 Decision Stumps for Trees
Let t and x be labeled ordered trees, and y be a
class label (y ∈ {±1}), a decision stump classifier
for trees is given by
</bodyText>
<equation confidence="0.998191">
def y t ⊆ x
h(t,y) (x) =−y otherwise.
</equation>
<bodyText confidence="0.935441333333333">
The parameter for classification is the tuple ht, yi,
hereafter referred to as the rule of the decision
stumps.
The decision stumps are trained to find rule hˆt, ˆyi
that minimizes the error rate for the given training
data T = {hxi, yii}Li=1:
</bodyText>
<equation confidence="0.898886923076923">
hˆt, ˆyi =argmin
tEF,yE{f1}
where F is a set of candidate trees or a feature set
(i.e., F = UL
i=1{t|t ⊆ xi}).
The gain function for rule ht, yi is defined as
L
gain(ht, yi) def = yih(t,y)(xi). (2)
i=1
Using the gain, the search problem given in (1)
becomes equivalent to the following problem:
hˆt, ˆyi =argmax gain(ht, yi).
tEF,yE{f1}
</equation>
<bodyText confidence="0.999308">
In this paper, we will use gain instead of error rate
for clarity.
</bodyText>
<subsectionHeader confidence="0.999901">
2.3 Applying Boosting
</subsectionHeader>
<bodyText confidence="0.999820222222222">
The decision stumps classifiers for trees are too in-
accurate to be applied to real applications, since
the final decision relies on the existence of a sin-
gle tree. However, accuracies can be boosted by
the Boosting algorithm (Freund and Schapire, 1996;
Schapire and Singer, 2000). Boosting repeatedly
calls a given weak learner to finally produce hy-
pothesis f, which is a linear combination of K hy-
potheses produced by the prior weak learners, i,e.:
</bodyText>
<equation confidence="0.9960672">
f(x) = sgn(EKk=1 αkh(tk,yk)(x)).
(1 − yih(t,y)(xi)),(1)
1 L
2L
i=1
</equation>
<bodyText confidence="0.980243">
A weak learner is built at each iteration k
with different distributions or weights d(k) =
(d(k)
</bodyText>
<equation confidence="0.84106225">
i , ... , d(k)
L ), (where ENi=1 d(k)
i = 1, d(k)
i &gt; 0).
</equation>
<bodyText confidence="0.9984232">
The weights are calculated in such a way that hard
examples are focused on more than easier examples.
To use the decision stumps as the weak learner of
Boosting, we redefine the gain function (2) as fol-
lows:
</bodyText>
<equation confidence="0.999180666666667">
L
gain((t, y)) def = yidih(t,y)(xi). (3)
i=1
</equation>
<bodyText confidence="0.999878857142857">
There exist many Boosting algorithm variants,
however, the original and the best known algorithm
is AdaBoost (Freund and Schapire, 1996). We here
use Arc-GV (Breiman, 1999) instead of AdaBoost,
since Arc-GV asymptotically maximizes the margin
and shows faster convergence to the optimal solu-
tion than AdaBoost.
</bodyText>
<sectionHeader confidence="0.973688" genericHeader="method">
3 Efficient Computation
</sectionHeader>
<bodyText confidence="0.898485090909091">
In this section, we introduce an efficient and prac-
tical algorithm to find the optimal rule (ˆt, ˆy) from
given training data. This problem is formally de-
fined as follows.
Problem 1 Find Optimal Rule
Let T = {(x1, y1, d1), ..., (xL, yL, dL)} be train-
ing data, where, xi is a labeled ordered tree,
yi E {+1} is a class label associated with xi
and di (EL i=1 di = 1, di &gt; 0) is a normal-
ized weight assigned to xi. Given T, find the
optimal rule (ˆt, ˆy) that maximizes the gain, i.e.,
</bodyText>
<equation confidence="0.75359">
(ˆt, ˆy) = argmaxtEF,yE{f1} diyih(t,y), where F =
ULi=1{t|t C xi}.
</equation>
<bodyText confidence="0.999943222222222">
The most naive and exhaustive method, in which
we first enumerate all subtrees F and then calcu-
late the gains for all subtrees, is usually impractical,
since the number of subtrees is exponential to its
size. We thus adopt an alternative strategy to avoid
such exhaustive enumeration.
The method to find the optimal rule is modeled as
a variant of the branch-and-bound algorithm, and is
summarized in the following strategies:
</bodyText>
<listItem confidence="0.977298571428571">
1. Define a canonical search space in which a
whole set of subtrees of a set of trees can be
enumerated.
2. Find the optimal rule by traversing this search
space.
3. Prune search space by proposing a criterion
with respect to the upper bound of the gain.
</listItem>
<bodyText confidence="0.999673">
We will describe these steps more precisely in the
following subsections.
</bodyText>
<subsectionHeader confidence="0.998652">
3.1 Efficient Enumeration of Trees
</subsectionHeader>
<bodyText confidence="0.897262555555555">
Abe and Zaki independently proposed an efficient
method, rightmost-extension, to enumerate all sub-
trees from a given tree (Abe et al., 2002; Zaki,
2002). First, the algorithm starts with a set of trees
consisting of single nodes, and then expands a given
tree of size (k − 1) by attaching a new node to this
tree to obtain trees of size k. However, it would
be inefficient to expand nodes at arbitrary positions
of the tree, as duplicated enumeration is inevitable.
The algorithm, rightmost extension, avoids such du-
plicated enumerations by restricting the position of
attachment. We here give the definition of rightmost
extension to describe this restriction in detail.
Definition 4 Rightmost Extension (Abe et al., 2002;
Zaki, 2002)
Let t and t&apos; be labeled ordered trees. We say t&apos; is a
rightmost extension of t, if and only if t and t&apos; satisfy
the following three conditions:
</bodyText>
<listItem confidence="0.980373166666667">
(1) t&apos; is created by adding a single node to t, (i.e.,
t C t&apos; and |t |+ 1 = |t&apos;|).
(2) A node is added to a node existing on the unique
path from the root to the rightmost leaf (rightmost-
path) in t.
(3) A node is added as the rightmost sibling.
</listItem>
<bodyText confidence="0.999798888888889">
Consider Figure 2, which illustrates example tree t
with the labels drawn from the set G = {a, b, c}.
For the sake of convenience, each node in this figure
has its original number (depth-first enumeration).
The rightmost-path of the tree t is (a(c(b))), and
occurs at positions 1, 4 and 6 respectively. The set
of rightmost extended trees is then enumerated by
simply adding a single node to a node on the right-
most path. Since there are three nodes on the right-
most path and the size of the label set is 3 (= |G|),
a total of 9 trees are enumerated from the original
tree t. Note that rightmost extension preserves the
prefix ordering of nodes in t (i.e., nodes at posi-
tions 1..|t |are preserved). By repeating the process
of rightmost-extension recursively, we can create a
search space in which all trees drawn from the set G
are enumerated. Figure 3 shows a snapshot of such
a search space.
</bodyText>
<subsectionHeader confidence="0.99878">
3.2 Upper bound of gain
</subsectionHeader>
<bodyText confidence="0.99988975">
Rightmost extension defines a canonical search
space in which one can enumerate all subtrees from
a given set of trees. We here consider an upper
bound of the gain that allows subspace pruning in
</bodyText>
<figureCaption confidence="0.99924">
Figure 2: Rightmost extension
Figure 3: Recursion using rightmost extension
</figureCaption>
<bodyText confidence="0.99973375">
this canonical search space. The following theo-
rem, an extension of Morhishita (Morhishita, 2002),
gives a convenient way of computing a tight upper
bound on gain(ht&apos;, yi) for any super-tree t&apos; of t.
</bodyText>
<equation confidence="0.978850285714286">
Theorem 1 Upper bound of the gain: µ(t)
For any t0 ⊇ t and y ∈ {±1}, the gain of ht0, yi is
bounded by µ(t) (i.e., gain(ht0yi) ≤ µ(t)), where µ(t)
is given by
( E
µ(t) def = max 2
{i|yi=+1,t⊆xi}
)yi · di .
{i|yi=−1,t⊆xi}
Proof 1
L
gain(ht0, yi) = E diyihht1,yi(xi)
i=1
L
= E diyi · y · (2I(t0 ⊆ xi) − 1)
i=1
If we focus on the case y = +1, then
gain(ht0,+1i) = 2 E yidi −
{i|t1⊆xi}
Thus, for any t0 ⊇ t and y ∈ {±1},
gain(ht0, yi) ≤ µ(t) ✷
</equation>
<bodyText confidence="0.9851244">
We can efficiently prune the search space spanned
by right most extension using the upper bound of
gain u(t). During the traverse of the subtree lat-
tice built by the recursive process of rightmost ex-
tension, we always maintain the temporally subop-
timal gain τ among all gains calculated previously.
If µ(t) &lt; τ, the gain of any super-tree t&apos; ⊇ t is no
greater than τ, and therefore we can safely prune
the search space spanned from the subtree t. If
µ(t) ≥ τ, in contrast, we cannot prune this space,
since there might exist a super-tree t&apos; ⊇ t such
that gain(t&apos;) ≥ τ. We can also prune the space
with respect to the expanded single node s. Even
if µ(t) ≥ τ and a node s is attached to the tree t,
we can ignore the space spanned from the tree t&apos; if
µ(s) &lt; τ, since no super-tree of s can yield optimal
gain.
Figure 4 presents a pseudo code of the algorithm
Find Optimal Rule. The two pruning are marked
with (1) and (2) respectively.
</bodyText>
<sectionHeader confidence="0.938756" genericHeader="method">
4 Relation to SVMs with Tree Kernel
</sectionHeader>
<bodyText confidence="0.990483470588235">
Recent studies (Breiman, 1999; Schapire et al.,
1997; R¨atsch et al., 2001) have shown that both
Boosting and SVMs (Boser et al., 1992) have a
similar strategy; constructing an optimal hypothe-
sis that maximizes the smallest margin between the
positive and negative examples. We here describe
a connection between our Boosting algorithm and
SVMs with tree kernel (Collins and Duffy, 2002;
Kashima and Koyanagi, 2002).
Tree kernel is one of the convolution kernels, and
implicitly maps the example represented in a la-
beled ordered tree into all subtree spaces. The im-
plicit mapping defined by tree kernel is given as:
Φ(x)=(I(t1 ⊆ x), ... , I(t|F |⊆ x)), where tj∈F,
x ∈ X and I(·) is the indicator function 1.
The final hypothesis of SVMs with tree kernel
can be given by
</bodyText>
<figure confidence="0.986286717948718">
rightmost extension
t’
1
a
{a,b,c
}
t
7
1
a
rightmost- path
c
1
a
b
2 4
c
b
2 4
c
3
1
a
3 c 5 a 6 b
b
2 4
c
{a,b,c
}
{a,b,c
}
L { a , b , c
}
b
2 4
3 c 5 a 6 b
c 5 a 6 b
3 c 5 a 6 b
7
</figure>
<page confidence="0.965436">
7
</page>
<equation confidence="0.989568583333333">
di −
L
E
i=1
yi · di,
2 E di +
L
E
i=1
L
E
i=1
yi · di
≤ 2 E di −
{i|yi=+1,t1⊆xi}
L
E
i=1
yi · di
f(x) = sgn(w · Φ(x) − b)
E= sgn( wt · I(t ⊆ x) − b). (4)
t∈F
≤ 2 E di −
{i|yi=+1,t⊆xi}
</equation>
<bodyText confidence="0.95338">
Similarly, the final hypothesis of our boosting al-
gorithm can be reformulated as a linear classifier:
</bodyText>
<equation confidence="0.992146444444444">
L
E
i=1
yi · di,
since |{i|yi = +1,t0 ⊆ xi} |≤ |{i|yi = +1,t ⊆ xi}|
for any t0 ⊇ t. Similarly,
E
gain(ht0, −1i) ≤ 2
{i|yi=−1,t⊆xi}
</equation>
<bodyText confidence="0.92682125">
1Strictly speaking, tree kernel uses the cardinality of each
substructure. However, it makes little difference since a given
tree is often sparse in NLP and the cardinality of substructures
will be approximated by their existence.
</bodyText>
<equation confidence="0.9971906">
di +
L
E
i=1
yi · di
</equation>
<sectionHeader confidence="0.453473" genericHeader="method">
Algorithm: Find Optimal Rule
</sectionHeader>
<bodyText confidence="0.692637">
argument: T = {hx1, y1, d1i ..., hxL, yL, dLi}
(xi a tree, yi ∈ {±1} is a class, and
di (PLi=1 di = 1, di ≥ 0) is a weight)
returns: Optimal rule hˆt, ˆyi
begin
</bodyText>
<equation confidence="0.882558060606061">
τ = 0 // suboptimal value
function project (t)
if µ(t) ≤ τ then return ... (1)
y&apos; = argmaxyE{±1} gain(ht, yi)
if gain(ht, y&apos;i) &gt; τ then
hˆt, ˆyi = ht, y&apos;i
τ = gain(hˆt, ˆyi) // suboptimal solution
end
foreach t&apos; ∈ {set of trees that are
rightmost extension of t }
s =single node added by RME
if µ(s) ≤ τ then continue ... (2)
project(t&apos;)
end
end
// for each single node
foreach t&apos; ∈ {t|t ∈ ∪Li=1{t|t ⊆ xi)}, |t |= 1}
project (t&apos;)
end
return hˆt, ˆyi
end
Figure 4: Algorithm: Find Optimal Rule
K
f(x) = sgn( X αkh(tk,yk)(x))
k=1
K
= sgn( X αk · yk(2I(tk ⊆ x) − 1))
k=1
X= sgn( wt · I(t ⊆ x) − b), (5)
tEY
where K Xykαk, wt = 2 · yk · αk.
b= X {kIt=tkI
k=1
</equation>
<bodyText confidence="0.999970052631579">
We can thus see that both algorithms are essentially
the same in terms of their feature space. The dif-
ference between them is the metric of margin; the
margin of Boosting is measured in l1-norm, while,
that of SVMs is measured in l2-norm. The question
one might ask is how the difference is expressed in
practice. The difference between them can be ex-
plained by sparseness.
It is well known that the solution or separating
hyperplane of SVMs is expressed as a linear com-
bination of the training examples using some coeffi-
cients A, (i.e., w = PL i=1 AiΦ(xi)). Maximizing l2-
norm margin gives a sparse solution in the example
space, (i.e., most of Ai becomes 0). Examples that
have non-zero coefficient are called support vectors
that form the final solution. Boosting, in contrast,
performs the computation explicitly in the feature
space. The concept behind Boosting is that only a
few hypotheses are needed to express the final so-
lution. The l1-norm margin allows us to realize this
property. Boosting thus finds a sparse solution in
the feature space.
The accuracies of these two methods depends on
the given training data. However, we argue that
Boosting has the following practical advantages.
First, sparse hypotheses allow us to build an effi-
cient classification algorithm. The complexity of
SVMs with tree kernel is O(L&apos;|N1||N2|), where N1
and N2 are trees, and L&apos; is the number of support
vectors, which is too heavy to realize real applica-
tions. Boosting, in contrast, runs faster, since the
complexity depends only on the small number of de-
cision stumps. Second, sparse hypotheses are use-
ful in practice as they provide “transparent” models
with which we can analyze how the model performs
or what kind of features are useful. It is difficult to
give such analysis with kernel methods, since they
define the feature space implicitly.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997583">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.9992795">
We conducted two experiments in sentence classifi-
cation.
</bodyText>
<listItem confidence="0.986482">
• PHS review classification (PHS)
</listItem>
<bodyText confidence="0.9996519">
The goal of this task is to classify reviews (in
Japanese) for PHS2 as positive reviews or neg-
ative reviews. A total of 5,741 sentences were
collected from a Web-based discussion BBS
on PHS, in which users are directed to submit
positive reviews separately from negative re-
views. The unit of classification is a sentence.
The categories to be identified are “positive” or
“negative” with the numbers 2,679 and 3,062
respectively.
</bodyText>
<listItem confidence="0.991086">
• Modality identification (MOD)
</listItem>
<bodyText confidence="0.998806666666667">
This task is to classify sentences (in Japanese)
by modality. A total of 1,710 sentences from a
Japanese newspaper were manually annotated
</bodyText>
<note confidence="0.5473345">
2PHS (Personal Handyphone System) is a cell phone sys-
tem developed in Japan in 1989.
</note>
<bodyText confidence="0.999333555555556">
according to Tamura’s taxonomy (Tamura and
Wada, 1996). The unit of classification is a
sentence. The categories to be identified are
“opinion”, “assertion” or “description” with
the numbers 159, 540, and 1,011 respectively.
To employ learning and classification, we have to
represent a given sentence as a labeled ordered tree.
In this paper, we use the following three representa-
tion forms.
</bodyText>
<listItem confidence="0.98363">
• bag-of-words (bow), baseline
</listItem>
<bodyText confidence="0.999625166666667">
Ignoring structural information embedded in
text, we simply represent a text as a set
of words. This is exactly the same setting
as Boostexter. Word boundaries are identi-
fied using a Japanese morphological analyzer,
ChaSen3.
</bodyText>
<listItem confidence="0.978851">
• Dependency (dep)
</listItem>
<bodyText confidence="0.999750125">
We represent a text in a word-based depen-
dency tree. We first use CaboCha4 to obtain a
chunk-based dependency tree of the text. The
chunk approximately corresponds to the base-
phrase in English. By identifying the head
word in the chunk, a chunk-based dependency
tree is converted into a word-based dependency
tree.
</bodyText>
<listItem confidence="0.98017">
• N-gram (ngram)
</listItem>
<bodyText confidence="0.99989055">
It is the word-based dependency tree that as-
sumes that each word simply modifies the next
word. Any subtree of this structure becomes a
word n-gram.
We compared the performance of our Boosting al-
gorithm and support vector machines (SVMs) with
bag-of-words kernel and tree kernel according to
their F-measure in 5-fold cross validation. Although
there exist some extensions for tree kernel (Kashima
and Koyanagi, 2002), we use the original tree ker-
nel by Collins (Collins and Duffy, 2002), where all
subtrees of a tree are used as distinct features. This
setting yields a fair comparison in terms of feature
space. To extend a binary classifier to a multi-class
classifier, we use the one-vs-rest method. Hyperpa-
rameters, such as number of iterations K in Boost-
ing and soft-margin parameter C in SVMs were se-
lected by using cross-validation. We implemented
SVMs with tree kernel based on TinySVM5 with
custom kernels incorporated therein.
</bodyText>
<footnote confidence="0.996707666666667">
3http://chasen.naist.jp/
4http://chasen.naist.jp/˜ taku/software/cabocha/
5http://chasen.naist.jp/˜taku/software/tinysvm
</footnote>
<subsectionHeader confidence="0.957326">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9999355">
Table 1 summarizes the results of PHS and MOD
tasks. To examine the statistical significance of the
results, we employed a McNemar’s paired test, a
variant of the sign test, on the labeling disagree-
ments. This table also includes the results of sig-
nificance tests.
</bodyText>
<subsectionHeader confidence="0.921085">
5.2.1 Effects of structural information
</subsectionHeader>
<bodyText confidence="0.999953625">
In all tasks and categories, our subtree-based Boost-
ing algorithm (dep/ngram) performs better than the
baseline method (bow). This result supports our first
intuition that structural information within texts is
important when classifying a text by opinions or
modalities, not by topics. We also find that there
are no significant differences in accuracy between
dependency and n-gram (in all cases, p &gt; 0.2).
</bodyText>
<subsectionHeader confidence="0.782419">
5.2.2 Comparison with Tree Kernel
</subsectionHeader>
<bodyText confidence="0.999922117647059">
When using the bag-of-words feature, no signifi-
cant differences in accuracy are observed between
Boosting and SVMs. When structural information
is used in training and classification, Boosting per-
forms slightly better than SVMs with tree kernel.
The differences are significant when we use de-
pendency features in the MOD task. SVMs show
worse performance depending on tasks and cate-
gories, (e.g., 24.2 F-measure in the smallest cate-
gory “opinion” in the MOD task).
When a convolution kernel is applied to sparse
data, kernel dot products between almost the same
instances become much larger than those between
different instances. This is because the number of
common features between similar instances expo-
nentially increases with size. This sometimes leads
to overfitting in training , where a test instance very
close to an instance in training data is correctly clas-
sified, and other instances are classified as a de-
fault class. This problem can be tackled by several
heuristic approaches: i) employing a decay factor to
reduce the weights of large sub-structures (Collins
and Duffy, 2002; Kashima and Koyanagi, 2002).
ii) substituting kernel dot products for the Gaussian
function to smooth the original kernel dot products
(Haussler, 1999). These approaches may achieve
better accuracy, however, they yield neither the fast
classification nor the interpretable feature space tar-
geted by this paper. Moreover, we cannot give a fair
comparison in terms of the same feature space. The
selection of optimal hyperparameters, such as decay
factors in the first approach and smoothing parame-
ters in the second approach, is also left to as an open
question.
</bodyText>
<tableCaption confidence="0.999562">
Table 1: Results of Experiments on PHS / MOD, F-measure, precision (%), and recall (%)
</tableCaption>
<table confidence="0.9950895">
PHS MOD
opinion assertion description
Boosting bow 76.0 (76.1 / 75.9) 59.6 (59.4 / 60.0) 70.0 (70.4 / 69.9) 82.2 (81.0 / 83.5)
dep 78.7 (79.1 / 78.4) 78.7* (90.2 / 70.0) 86.7* (88.0 / 85.6) 91.7* (91.1 / 92.4)
n-gram 79.3 (79.8 / 78.5) 76.7* (87.2 / 68.6) 87.2 (86.9 / 87.4) 91.6 (91.0 / 92.2)
SVMs bow 76.8 (78.3 / 75.4) 57.2 (79.0 / 48.4) 71.3 (64.3 / 80.0) 82.1 (82.7 / 81.5)
dep 77.0 (80.7 / 73.6) 24.2 (95.7 / 13.8) 81.7 (86.7 / 77.2) 87.6 (86.1 / 89.2)
n-gram 78.9 (80.4 / 77.5) 57.5 (98.0 / 40.9) 84.1 (90.1 / 78.9) 90.1 (88.2 / 92.0)
</table>
<bodyText confidence="0.860924">
We employed a McNemar’s paired test on the labeling disagreements. Underlined results indicate that there is a significant differ-
ence (p &lt; 0.01) against the baseline (bow). If there is a statistical difference (p &lt; 0.01) between Boosting and SVMs with the
same feature representation (bow / dep / n-gram), better results are asterisked.
</bodyText>
<subsectionHeader confidence="0.783081">
5.2.3 Merits of our algorithm
</subsectionHeader>
<bodyText confidence="0.995933256410257">
In the previous section, we described the merits of
our Boosting algorithm. We experimentally verified
these merits from the results of the PHS task.
As illustrated in section 4, our method can auto-
matically select relevant and compact features from
a number of feature candidates. In the PHS task,
a total 1,793 features (rules) were selected, while
the set sizes of distinct uni-gram, bi-gram and tri-
gram appearing in the data were 4,211, 24,206, and
43,658 respectively. Even though all subtrees are
used as feature candidates, Boosting selects a small
and highly relevant subset of features. When we
explicitly enumerate the subtrees used in tree ker-
nel, the number of active (non-zero) features might
amount to ten thousand or more.
Table 2 shows examples of extracted support fea-
tures (pairs of feature (tree) t and weight wt in (Eq.
5)) in the PHS task.
A. Features including the word “にくい (hard, dif-
ficult)”
In general, “にくい (hard, difficult)” is an ad-
jective expressing negative opinions. Most
of features including “にくい” are assigned
a negative weight (negative opinion). How-
ever, only one feature “切れに くい (hard to
cut off)” has a positive weight. This result
strongly reflects the domain knowledge, PHS
(cell phone reviews).
B. Features including the word “使う (use)”
“使う (use)” is a neutral expression for opin-
ion classifications. However, the weight varies
according to the surrounding context: 1) “使い
たい (want to use)” → positive, 2) “使い やす
い (be easy to use)” → positive, 3) “使い やす
か った (was easy to use)” (past form) → neg-
ative, 4) “の ほうが 使い やすい (... is easier
to use than ..)” (comparative) → negative.
C. Features including the word “充電 (recharge)”
Features reflecting the domain knowledge are
</bodyText>
<tableCaption confidence="0.993827">
Table 2: Examples of features in PHS dataset
</tableCaption>
<table confidence="0.991429857142857">
keyword wt subtree t (support features)
A. にくい 0.0004 切れる にくい (be hard to cut off)
(hard, -0.0006 読む にくい (be hard to read)
difficult) -0.0007 使う にくい (be hard to use)
-0.0017 にくい (be hard to)
B. 使う 0.0027 使う たい (want to use)
(use) 0.0002 使う (use)
0.0002 使う てる (be in use)
0.0001 使う やすい (be easy to use)
-0.0001 使う やすい た (was easy to use)
-0.0007 使う にくい (be hard to use)
-0.0019 方 が 使う やすい (is easier to use than)
C. 充電 0.0028 充電 時間 が 短い (recharging time is short)
(recharge) -0.0041 充電 時間 が 長い (recharging time is long)
</table>
<bodyText confidence="0.991994590909091">
extracted: 1) “充電 時間 が 短い (recharging
time is short)” → positive, 2) “充電 時間
長い (recharging time is long)” → negative.
These features are interesting, since we cannot
determine the correct label (positive/negative)
by using just the bag-of-words features, such
as “recharge”, “short” or “long” alone.
Table 3 illustrates an example of actual classifica-
tion. For the input sentence “液晶が大きくて, 綺麗,
見やすい (The LCD is large, beautiful, and easy to
see.)”, the system outputs the features applied to this
classification along with their weights wt. This in-
formation allows us to analyze how the system clas-
sifies the input sentence in a category and what kind
of features are used in the classification. We can-
not perform these analyses with tree kernel, since it
defines their feature space implicitly.
The testing speed of our Boosting algorithm is
much higher than that of SVMs with tree kernel. In
the PHS task, the speeds of Boosting and SVMs are
0.531 sec./5,741 instances and 255.42 sec./5,741 in-
stances respectively 6. We can say that Boosting is
</bodyText>
<tableCaption confidence="0.67750575">
6We ran these tests on a Linux PC with XEON 2.4Ghz dual
processors and 4.0Gbyte main memory.
Table 3: A running example
Input: 液晶が大きくて綺麗, 見やすい.
</tableCaption>
<table confidence="0.958493076923077">
The LCD is large, beautiful and easy to see.
wt subtree t (support features)
0.00368 やすい (be easy to)
0.00352 綺麗 (beautiful)
0.00237 見る やすい (be easy to see)
0.00174 が 大きい (... is large)
0.00107 液晶 が 大きい (The LCD is large)
0.00074 液晶 が (The LCD is ...)
0.00058 液晶 (The LCD)
0.00027 て (a particle for coordination)
0.00036 見る (see)
-0.00001 大きい (large)
-0.00052 が (a nominative case marker)
</table>
<bodyText confidence="0.9978232">
about 480 times faster than SVMs with tree kernel.
Even though the potential size of search space
is huge, the pruning criterion proposed in this pa-
per effectively prunes the search space. The prun-
ing conditions in Fig.4 are fulfilled with more than
90% probabitity. The training speed of our method
is 1,384 sec./5,741 instances when we set K =
60, 000 (# of iterations for Boosting). It takes
only 0.023 (=1,384/60,000) sec. to invoke the weak
learner, Find Optimal Rule.
</bodyText>
<sectionHeader confidence="0.99729" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99998575">
In this paper, we focused on an algorithm for the
classification of semi-structured text in which a sen-
tence is represented as a labeled ordered tree7. Our
proposal consists of i) decision stumps that use
subtrees as features and ii) Boosting algorithm in
which the subtree-based decision stumps are ap-
plied as weak learners. Two experiments on opin-
ion/modality classification tasks confirmed that sub-
tree features are important.
One natural extension is to adopt confidence rated
predictions to the subtree-based weak learners. This
extension is also found in BoosTexter and shows
better performance than binary-valued learners.
In our experiments, n-gram features showed com-
parable performance to dependency features. We
would like to apply our method to other applications
where instances are represented in a tree and their
subtrees play an important role in classifications
(e.g., parse re-ranking (Collins and Duffy, 2002)
and information extraction).
</bodyText>
<sectionHeader confidence="0.994324" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99945249122807">
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki
Arimura, and Setsuo Arikawa. 2002. Optimized
7An implementation of our Boosting algorithm is available
at http://chasen.org/˜ taku/software/bact/
substructure discovery for semi-structured data.
In Proc. ofPKDD, pages 1–14.
Bernhard Boser, Isabelle Guyon, and Vladimir Vap-
nik. 1992. A training algorithm for optimal mar-
gin classifiers. In In Proc of 5th COLT, pages
144–152.
Leo Breiman. 1999. Prediction games and arch-
ing algoritms. Neural Computation, 11(7):1493
– 1518.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures, and the voted perceptron.
In Proc. ofACL.
Yoav Freund and Robert E. Schapire. 1996. A
decision-theoretic generalization of on-line learn-
ing and an application to boosting. Journal of
Computer and System Sicences, 55(1):119–139.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical report, UC Santa Cruz
(UCS-CRL-99-10).
Hisashi Kashima and Teruo Koyanagi. 2002. Svm
kernels for semi-structured data. In Proc. of
ICML, pages 291–298.
Shinichi Morhishita. 2002. Computing optimal hy-
potheses efficiently for boosting. In Progress in
Discovery Science, pages 471–481. Springer.
Gunnar. R¨atsch, Takashi. Onoda, and Klaus-Robert
M¨uller. 2001. Soft margins for AdaBoost. Ma-
chine Learning, 42(3):287–320.
Robert E. Schapire and Yoram Singer. 2000. Boos-
Texter: A boosting-based system for text catego-
rization. Machine Learning, 39(2/3):135–168.
Robert E. Schapire, Yoav Freund, Peter Bartlett, and
Wee Sun Lee. 1997. Boosting the margin: a new
explanation for the effectiveness of voting meth-
ods. In Proc. ofICML, pages 322–330.
Fabrizio Sebastiani. 2002. Machine learning in
automated text categorization. ACM Computing
Surveys, 34(1):1–47.
Naoyoshi Tamura and Keiji Wada. 1996. Text
structuring by composition and decomposition of
segments (in Japanese). Journal of Natural Lan-
guage Processing, 5(1).
Peter D. Turney. 2002. Thumbs up or thumbs
down? semantic orientation applied to unsuper-
vised classification of reviews. In Proc. of ACL,
pages 417–424.
Janyce M. Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proc. ofAAAI/IAAI, pages
735–740.
Mohammed Zaki. 2002. Efficiently mining fre-
quent trees in a forest. In Proc. of SIGKDD,
pages 71–80.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820840">
<title confidence="0.983903">A Boosting Algorithm for Classification of Semi-Structured Text</title>
<author confidence="0.858393">Matsumoto</author>
<affiliation confidence="0.9989575">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.993402">8916-5 Takayama, Ikoma Nara</address>
<abstract confidence="0.998001833333333">The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification. Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required. Accordingly, learning algorithms must be created that can handle the structures observed in texts. In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts. The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners. We also discuss the relation between our algorithm and SVMs with tree kernel. Two experiments on opinion/modality classification confirm that subtree features are important.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenji Abe</author>
<author>Shinji Kawasoe</author>
<author>Tatsuya Asai</author>
<author>Hiroki Arimura</author>
<author>Setsuo Arikawa</author>
</authors>
<title>Optimized 7An implementation of our Boosting algorithm is available at http://chasen.org/˜ taku/software/bact/ substructure discovery for semi-structured data. In</title>
<date>2002</date>
<booktitle>Proc. ofPKDD,</booktitle>
<pages>1--14</pages>
<contexts>
<context position="10433" citStr="Abe et al., 2002" startWordPosition="1715" endWordPosition="1718">l rule is modeled as a variant of the branch-and-bound algorithm, and is summarized in the following strategies: 1. Define a canonical search space in which a whole set of subtrees of a set of trees can be enumerated. 2. Find the optimal rule by traversing this search space. 3. Prune search space by proposing a criterion with respect to the upper bound of the gain. We will describe these steps more precisely in the following subsections. 3.1 Efficient Enumeration of Trees Abe and Zaki independently proposed an efficient method, rightmost-extension, to enumerate all subtrees from a given tree (Abe et al., 2002; Zaki, 2002). First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (k − 1) by attaching a new node to this tree to obtain trees of size k. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable. The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of attachment. We here give the definition of rightmost extension to describe this restriction in detail. Definition 4 Rightmost Extension (Abe et al., 2002; Zaki, 2002) Let</context>
</contexts>
<marker>Abe, Kawasoe, Asai, Arimura, Arikawa, 2002</marker>
<rawString>Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and Setsuo Arikawa. 2002. Optimized 7An implementation of our Boosting algorithm is available at http://chasen.org/˜ taku/software/bact/ substructure discovery for semi-structured data. In Proc. ofPKDD, pages 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Boser</author>
<author>Isabelle Guyon</author>
<author>Vladimir Vapnik</author>
</authors>
<title>A training algorithm for optimal margin classifiers. In</title>
<date>1992</date>
<booktitle>In Proc of 5th COLT,</booktitle>
<pages>144--152</pages>
<contexts>
<context position="4835" citStr="Boser et al., 1992" startWordPosition="733" endWordPosition="736">al information of text. ii) It uses a set of all subtrees (bag-of-subtrees) for the feature set without any constraints. iii) Even though the size of the candidate feature set becomes quite large, it automatically selects a compact and relevant feature set based on Boosting. This paper is organized as follows. First, we describe the details of our Boosting algorithm in which the subtree-based decision stumps are applied as weak learners. Second, we show an implementation issue related to constructing an efficient learning algorithm. We also discuss the relation between our algorithm and SVMs (Boser et al., 1992) with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Two experiments on the opinion and modality classification tasks are employed to confirm that subtree features are important. 2 Classifier for Trees We first assume that a text to be classified is represented as a labeled ordered tree. The focused problem can be formalized as a general problem, called the tree classification problem. The tree classification problem is to induce a mapping f(x) : X → {±1}, from given training examples T = {hxi, yii}Li=1, where xi ∈ X is a labeled ordered tree and yi ∈ {±1} is a class label </context>
<context position="14356" citStr="Boser et al., 1992" startWordPosition="2453" endWordPosition="2456">e this space, since there might exist a super-tree t&apos; ⊇ t such that gain(t&apos;) ≥ τ. We can also prune the space with respect to the expanded single node s. Even if µ(t) ≥ τ and a node s is attached to the tree t, we can ignore the space spanned from the tree t&apos; if µ(s) &lt; τ, since no super-tree of s can yield optimal gain. Figure 4 presents a pseudo code of the algorithm Find Optimal Rule. The two pruning are marked with (1) and (2) respectively. 4 Relation to SVMs with Tree Kernel Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples. We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Tree kernel is one of the convolution kernels, and implicitly maps the example represented in a labeled ordered tree into all subtree spaces. The implicit mapping defined by tree kernel is given as: Φ(x)=(I(t1 ⊆ x), ... , I(t|F |⊆ x)), where tj∈F, x ∈ X and I(·) is the indicator function 1. The final hypothesis of SV</context>
</contexts>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>Bernhard Boser, Isabelle Guyon, and Vladimir Vapnik. 1992. A training algorithm for optimal margin classifiers. In In Proc of 5th COLT, pages 144–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Prediction games and arching algoritms.</title>
<date>1999</date>
<journal>Neural Computation,</journal>
<volume>11</volume>
<issue>7</issue>
<pages>1518</pages>
<contexts>
<context position="8789" citStr="Breiman, 1999" startWordPosition="1430" endWordPosition="1431">(1 − yih(t,y)(xi)),(1) 1 L 2L i=1 A weak learner is built at each iteration k with different distributions or weights d(k) = (d(k) i , ... , d(k) L ), (where ENi=1 d(k) i = 1, d(k) i &gt; 0). The weights are calculated in such a way that hard examples are focused on more than easier examples. To use the decision stumps as the weak learner of Boosting, we redefine the gain function (2) as follows: L gain((t, y)) def = yidih(t,y)(xi). (3) i=1 There exist many Boosting algorithm variants, however, the original and the best known algorithm is AdaBoost (Freund and Schapire, 1996). We here use Arc-GV (Breiman, 1999) instead of AdaBoost, since Arc-GV asymptotically maximizes the margin and shows faster convergence to the optimal solution than AdaBoost. 3 Efficient Computation In this section, we introduce an efficient and practical algorithm to find the optimal rule (ˆt, ˆy) from given training data. This problem is formally defined as follows. Problem 1 Find Optimal Rule Let T = {(x1, y1, d1), ..., (xL, yL, dL)} be training data, where, xi is a labeled ordered tree, yi E {+1} is a class label associated with xi and di (EL i=1 di = 1, di &gt; 0) is a normalized weight assigned to xi. Given T, find the optima</context>
<context position="14250" citStr="Breiman, 1999" startWordPosition="2436" endWordPosition="2437">n safely prune the search space spanned from the subtree t. If µ(t) ≥ τ, in contrast, we cannot prune this space, since there might exist a super-tree t&apos; ⊇ t such that gain(t&apos;) ≥ τ. We can also prune the space with respect to the expanded single node s. Even if µ(t) ≥ τ and a node s is attached to the tree t, we can ignore the space spanned from the tree t&apos; if µ(s) &lt; τ, since no super-tree of s can yield optimal gain. Figure 4 presents a pseudo code of the algorithm Find Optimal Rule. The two pruning are marked with (1) and (2) respectively. 4 Relation to SVMs with Tree Kernel Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples. We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Tree kernel is one of the convolution kernels, and implicitly maps the example represented in a labeled ordered tree into all subtree spaces. The implicit mapping defined by tree kernel is given as: Φ(x)=(I(t1 ⊆ </context>
</contexts>
<marker>Breiman, 1999</marker>
<rawString>Leo Breiman. 1999. Prediction games and arching algoritms. Neural Computation, 11(7):1493 – 1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In</title>
<date>2002</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="4877" citStr="Collins and Duffy, 2002" startWordPosition="740" endWordPosition="743">set of all subtrees (bag-of-subtrees) for the feature set without any constraints. iii) Even though the size of the candidate feature set becomes quite large, it automatically selects a compact and relevant feature set based on Boosting. This paper is organized as follows. First, we describe the details of our Boosting algorithm in which the subtree-based decision stumps are applied as weak learners. Second, we show an implementation issue related to constructing an efficient learning algorithm. We also discuss the relation between our algorithm and SVMs (Boser et al., 1992) with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Two experiments on the opinion and modality classification tasks are employed to confirm that subtree features are important. 2 Classifier for Trees We first assume that a text to be classified is represented as a labeled ordered tree. The focused problem can be formalized as a general problem, called the tree classification problem. The tree classification problem is to induce a mapping f(x) : X → {±1}, from given training examples T = {hxi, yii}Li=1, where xi ∈ X is a labeled ordered tree and yi ∈ {±1} is a class label associated with each training data (we foc</context>
<context position="14607" citStr="Collins and Duffy, 2002" startWordPosition="2491" endWordPosition="2494"> the tree t&apos; if µ(s) &lt; τ, since no super-tree of s can yield optimal gain. Figure 4 presents a pseudo code of the algorithm Find Optimal Rule. The two pruning are marked with (1) and (2) respectively. 4 Relation to SVMs with Tree Kernel Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples. We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Tree kernel is one of the convolution kernels, and implicitly maps the example represented in a labeled ordered tree into all subtree spaces. The implicit mapping defined by tree kernel is given as: Φ(x)=(I(t1 ⊆ x), ... , I(t|F |⊆ x)), where tj∈F, x ∈ X and I(·) is the indicator function 1. The final hypothesis of SVMs with tree kernel can be given by rightmost extension t’ 1 a {a,b,c } t 7 1 a rightmost- path c 1 a b 2 4 c b 2 4 c 3 1 a 3 c 5 a 6 b b 2 4 c {a,b,c } {a,b,c } L { a , b , c } b 2 4 3 c 5 a 6 b c 5 a 6 b 3 c 5 a 6 b 7 7 di − L E i=1 yi · di, 2 E di </context>
<context position="20841" citStr="Collins and Duffy, 2002" startWordPosition="3641" endWordPosition="3644">y identifying the head word in the chunk, a chunk-based dependency tree is converted into a word-based dependency tree. • N-gram (ngram) It is the word-based dependency tree that assumes that each word simply modifies the next word. Any subtree of this structure becomes a word n-gram. We compared the performance of our Boosting algorithm and support vector machines (SVMs) with bag-of-words kernel and tree kernel according to their F-measure in 5-fold cross validation. Although there exist some extensions for tree kernel (Kashima and Koyanagi, 2002), we use the original tree kernel by Collins (Collins and Duffy, 2002), where all subtrees of a tree are used as distinct features. This setting yields a fair comparison in terms of feature space. To extend a binary classifier to a multi-class classifier, we use the one-vs-rest method. Hyperparameters, such as number of iterations K in Boosting and soft-margin parameter C in SVMs were selected by using cross-validation. We implemented SVMs with tree kernel based on TinySVM5 with custom kernels incorporated therein. 3http://chasen.naist.jp/ 4http://chasen.naist.jp/˜ taku/software/cabocha/ 5http://chasen.naist.jp/˜taku/software/tinysvm 5.2 Results and Discussion T</context>
<context position="23280" citStr="Collins and Duffy, 2002" startWordPosition="4013" endWordPosition="4016"> a convolution kernel is applied to sparse data, kernel dot products between almost the same instances become much larger than those between different instances. This is because the number of common features between similar instances exponentially increases with size. This sometimes leads to overfitting in training , where a test instance very close to an instance in training data is correctly classified, and other instances are classified as a default class. This problem can be tackled by several heuristic approaches: i) employing a decay factor to reduce the weights of large sub-structures (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). ii) substituting kernel dot products for the Gaussian function to smooth the original kernel dot products (Haussler, 1999). These approaches may achieve better accuracy, however, they yield neither the fast classification nor the interpretable feature space targeted by this paper. Moreover, we cannot give a fair comparison in terms of the same feature space. The selection of optimal hyperparameters, such as decay factors in the first approach and smoothing parameters in the second approach, is also left to as an open question. Table 1: Results of Experiments on P</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and an application to boosting.</title>
<date>1996</date>
<journal>Journal of Computer and System Sicences,</journal>
<volume>55</volume>
<issue>1</issue>
<contexts>
<context position="7943" citStr="Freund and Schapire, 1996" startWordPosition="1278" endWordPosition="1281">idate trees or a feature set (i.e., F = UL i=1{t|t ⊆ xi}). The gain function for rule ht, yi is defined as L gain(ht, yi) def = yih(t,y)(xi). (2) i=1 Using the gain, the search problem given in (1) becomes equivalent to the following problem: hˆt, ˆyi =argmax gain(ht, yi). tEF,yE{f1} In this paper, we will use gain instead of error rate for clarity. 2.3 Applying Boosting The decision stumps classifiers for trees are too inaccurate to be applied to real applications, since the final decision relies on the existence of a single tree. However, accuracies can be boosted by the Boosting algorithm (Freund and Schapire, 1996; Schapire and Singer, 2000). Boosting repeatedly calls a given weak learner to finally produce hypothesis f, which is a linear combination of K hypotheses produced by the prior weak learners, i,e.: f(x) = sgn(EKk=1 αkh(tk,yk)(x)). (1 − yih(t,y)(xi)),(1) 1 L 2L i=1 A weak learner is built at each iteration k with different distributions or weights d(k) = (d(k) i , ... , d(k) L ), (where ENi=1 d(k) i = 1, d(k) i &gt; 0). The weights are calculated in such a way that hard examples are focused on more than easier examples. To use the decision stumps as the weak learner of Boosting, we redefine the g</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sicences, 55(1):119–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report,</tech>
<pages>99--10</pages>
<institution>UC Santa Cruz</institution>
<contexts>
<context position="23433" citStr="Haussler, 1999" startWordPosition="4037" endWordPosition="4038"> This is because the number of common features between similar instances exponentially increases with size. This sometimes leads to overfitting in training , where a test instance very close to an instance in training data is correctly classified, and other instances are classified as a default class. This problem can be tackled by several heuristic approaches: i) employing a decay factor to reduce the weights of large sub-structures (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). ii) substituting kernel dot products for the Gaussian function to smooth the original kernel dot products (Haussler, 1999). These approaches may achieve better accuracy, however, they yield neither the fast classification nor the interpretable feature space targeted by this paper. Moreover, we cannot give a fair comparison in terms of the same feature space. The selection of optimal hyperparameters, such as decay factors in the first approach and smoothing parameters in the second approach, is also left to as an open question. Table 1: Results of Experiments on PHS / MOD, F-measure, precision (%), and recall (%) PHS MOD opinion assertion description Boosting bow 76.0 (76.1 / 75.9) 59.6 (59.4 / 60.0) 70.0 (70.4 / </context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report, UC Santa Cruz (UCS-CRL-99-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisashi Kashima</author>
<author>Teruo Koyanagi</author>
</authors>
<title>Svm kernels for semi-structured data. In</title>
<date>2002</date>
<booktitle>Proc. of ICML,</booktitle>
<pages>291--298</pages>
<contexts>
<context position="4906" citStr="Kashima and Koyanagi, 2002" startWordPosition="744" endWordPosition="747">of-subtrees) for the feature set without any constraints. iii) Even though the size of the candidate feature set becomes quite large, it automatically selects a compact and relevant feature set based on Boosting. This paper is organized as follows. First, we describe the details of our Boosting algorithm in which the subtree-based decision stumps are applied as weak learners. Second, we show an implementation issue related to constructing an efficient learning algorithm. We also discuss the relation between our algorithm and SVMs (Boser et al., 1992) with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Two experiments on the opinion and modality classification tasks are employed to confirm that subtree features are important. 2 Classifier for Trees We first assume that a text to be classified is represented as a labeled ordered tree. The focused problem can be formalized as a general problem, called the tree classification problem. The tree classification problem is to induce a mapping f(x) : X → {±1}, from given training examples T = {hxi, yii}Li=1, where xi ∈ X is a labeled ordered tree and yi ∈ {±1} is a class label associated with each training data (we focus here on the problem of bin</context>
<context position="14636" citStr="Kashima and Koyanagi, 2002" startWordPosition="2495" endWordPosition="2498"> since no super-tree of s can yield optimal gain. Figure 4 presents a pseudo code of the algorithm Find Optimal Rule. The two pruning are marked with (1) and (2) respectively. 4 Relation to SVMs with Tree Kernel Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples. We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Tree kernel is one of the convolution kernels, and implicitly maps the example represented in a labeled ordered tree into all subtree spaces. The implicit mapping defined by tree kernel is given as: Φ(x)=(I(t1 ⊆ x), ... , I(t|F |⊆ x)), where tj∈F, x ∈ X and I(·) is the indicator function 1. The final hypothesis of SVMs with tree kernel can be given by rightmost extension t’ 1 a {a,b,c } t 7 1 a rightmost- path c 1 a b 2 4 c b 2 4 c 3 1 a 3 c 5 a 6 b b 2 4 c {a,b,c } {a,b,c } L { a , b , c } b 2 4 3 c 5 a 6 b c 5 a 6 b 3 c 5 a 6 b 7 7 di − L E i=1 yi · di, 2 E di + L E i=1 L E i=1 yi · di ≤ 2</context>
<context position="20771" citStr="Kashima and Koyanagi, 2002" startWordPosition="3628" endWordPosition="3631">text. The chunk approximately corresponds to the basephrase in English. By identifying the head word in the chunk, a chunk-based dependency tree is converted into a word-based dependency tree. • N-gram (ngram) It is the word-based dependency tree that assumes that each word simply modifies the next word. Any subtree of this structure becomes a word n-gram. We compared the performance of our Boosting algorithm and support vector machines (SVMs) with bag-of-words kernel and tree kernel according to their F-measure in 5-fold cross validation. Although there exist some extensions for tree kernel (Kashima and Koyanagi, 2002), we use the original tree kernel by Collins (Collins and Duffy, 2002), where all subtrees of a tree are used as distinct features. This setting yields a fair comparison in terms of feature space. To extend a binary classifier to a multi-class classifier, we use the one-vs-rest method. Hyperparameters, such as number of iterations K in Boosting and soft-margin parameter C in SVMs were selected by using cross-validation. We implemented SVMs with tree kernel based on TinySVM5 with custom kernels incorporated therein. 3http://chasen.naist.jp/ 4http://chasen.naist.jp/˜ taku/software/cabocha/ 5http</context>
<context position="23309" citStr="Kashima and Koyanagi, 2002" startWordPosition="4017" endWordPosition="4020">applied to sparse data, kernel dot products between almost the same instances become much larger than those between different instances. This is because the number of common features between similar instances exponentially increases with size. This sometimes leads to overfitting in training , where a test instance very close to an instance in training data is correctly classified, and other instances are classified as a default class. This problem can be tackled by several heuristic approaches: i) employing a decay factor to reduce the weights of large sub-structures (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). ii) substituting kernel dot products for the Gaussian function to smooth the original kernel dot products (Haussler, 1999). These approaches may achieve better accuracy, however, they yield neither the fast classification nor the interpretable feature space targeted by this paper. Moreover, we cannot give a fair comparison in terms of the same feature space. The selection of optimal hyperparameters, such as decay factors in the first approach and smoothing parameters in the second approach, is also left to as an open question. Table 1: Results of Experiments on PHS / MOD, F-measure, precisio</context>
</contexts>
<marker>Kashima, Koyanagi, 2002</marker>
<rawString>Hisashi Kashima and Teruo Koyanagi. 2002. Svm kernels for semi-structured data. In Proc. of ICML, pages 291–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinichi Morhishita</author>
</authors>
<title>Computing optimal hypotheses efficiently for boosting.</title>
<date>2002</date>
<booktitle>In Progress in Discovery Science,</booktitle>
<pages>471--481</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12707" citStr="Morhishita, 2002" startWordPosition="2124" endWordPosition="2125">ions 1..|t |are preserved). By repeating the process of rightmost-extension recursively, we can create a search space in which all trees drawn from the set G are enumerated. Figure 3 shows a snapshot of such a search space. 3.2 Upper bound of gain Rightmost extension defines a canonical search space in which one can enumerate all subtrees from a given set of trees. We here consider an upper bound of the gain that allows subspace pruning in Figure 2: Rightmost extension Figure 3: Recursion using rightmost extension this canonical search space. The following theorem, an extension of Morhishita (Morhishita, 2002), gives a convenient way of computing a tight upper bound on gain(ht&apos;, yi) for any super-tree t&apos; of t. Theorem 1 Upper bound of the gain: µ(t) For any t0 ⊇ t and y ∈ {±1}, the gain of ht0, yi is bounded by µ(t) (i.e., gain(ht0yi) ≤ µ(t)), where µ(t) is given by ( E µ(t) def = max 2 {i|yi=+1,t⊆xi} )yi · di . {i|yi=−1,t⊆xi} Proof 1 L gain(ht0, yi) = E diyihht1,yi(xi) i=1 L = E diyi · y · (2I(t0 ⊆ xi) − 1) i=1 If we focus on the case y = +1, then gain(ht0,+1i) = 2 E yidi − {i|t1⊆xi} Thus, for any t0 ⊇ t and y ∈ {±1}, gain(ht0, yi) ≤ µ(t) ✷ We can efficiently prune the search space spanned by righ</context>
</contexts>
<marker>Morhishita, 2002</marker>
<rawString>Shinichi Morhishita. 2002. Computing optimal hypotheses efficiently for boosting. In Progress in Discovery Science, pages 471–481. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Onoda R¨atsch</author>
<author>Klaus-Robert M¨uller</author>
</authors>
<title>Soft margins for AdaBoost.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>42</volume>
<issue>3</issue>
<marker>R¨atsch, M¨uller, 2001</marker>
<rawString>Gunnar. R¨atsch, Takashi. Onoda, and Klaus-Robert M¨uller. 2001. Soft margins for AdaBoost. Machine Learning, 42(3):287–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="6676" citStr="Schapire and Singer, 2000" startWordPosition="1054" endWordPosition="1057">d ordered trees. We say that t matches u, or t is a subtree of u (t ⊆ u), if there exists a one-to-one function ψ from nodes in t to u, satisfying the conditions: (1) ψ preserves the parent-daughter relation, (2) ψ preserves the sibling relation, (3) ψ preserves the labels. We denote the number of nodes in t as |t|. Figure 1 shows an example of a labeled ordered tree and its subtree and non-subtree. 2.2 Decision Stumps Decision stumps are simple classifiers, where the final decision is made by only a single hypothesis Figure 1: Labeled ordered tree and subtree relation or feature. Boostexter (Schapire and Singer, 2000) uses word-based decision stumps for topic-based text classification. To classify trees, we here extend the decision stump definition as follows. Definition 3 Decision Stumps for Trees Let t and x be labeled ordered trees, and y be a class label (y ∈ {±1}), a decision stump classifier for trees is given by def y t ⊆ x h(t,y) (x) =−y otherwise. The parameter for classification is the tuple ht, yi, hereafter referred to as the rule of the decision stumps. The decision stumps are trained to find rule hˆt, ˆyi that minimizes the error rate for the given training data T = {hxi, yii}Li=1: hˆt, ˆyi =</context>
<context position="7971" citStr="Schapire and Singer, 2000" startWordPosition="1282" endWordPosition="1285">t (i.e., F = UL i=1{t|t ⊆ xi}). The gain function for rule ht, yi is defined as L gain(ht, yi) def = yih(t,y)(xi). (2) i=1 Using the gain, the search problem given in (1) becomes equivalent to the following problem: hˆt, ˆyi =argmax gain(ht, yi). tEF,yE{f1} In this paper, we will use gain instead of error rate for clarity. 2.3 Applying Boosting The decision stumps classifiers for trees are too inaccurate to be applied to real applications, since the final decision relies on the existence of a single tree. However, accuracies can be boosted by the Boosting algorithm (Freund and Schapire, 1996; Schapire and Singer, 2000). Boosting repeatedly calls a given weak learner to finally produce hypothesis f, which is a linear combination of K hypotheses produced by the prior weak learners, i,e.: f(x) = sgn(EKk=1 αkh(tk,yk)(x)). (1 − yih(t,y)(xi)),(1) 1 L 2L i=1 A weak learner is built at each iteration k with different distributions or weights d(k) = (d(k) i , ... , d(k) L ), (where ENi=1 d(k) i = 1, d(k) i &gt; 0). The weights are calculated in such a way that hard examples are focused on more than easier examples. To use the decision stumps as the weak learner of Boosting, we redefine the gain function (2) as follows:</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoav Freund</author>
<author>Peter Bartlett</author>
<author>Wee Sun Lee</author>
</authors>
<title>Boosting the margin: a new explanation for the effectiveness of voting methods.</title>
<date>1997</date>
<booktitle>In Proc. ofICML,</booktitle>
<pages>322--330</pages>
<contexts>
<context position="14273" citStr="Schapire et al., 1997" startWordPosition="2438" endWordPosition="2441">the search space spanned from the subtree t. If µ(t) ≥ τ, in contrast, we cannot prune this space, since there might exist a super-tree t&apos; ⊇ t such that gain(t&apos;) ≥ τ. We can also prune the space with respect to the expanded single node s. Even if µ(t) ≥ τ and a node s is attached to the tree t, we can ignore the space spanned from the tree t&apos; if µ(s) &lt; τ, since no super-tree of s can yield optimal gain. Figure 4 presents a pseudo code of the algorithm Find Optimal Rule. The two pruning are marked with (1) and (2) respectively. 4 Relation to SVMs with Tree Kernel Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples. We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Tree kernel is one of the convolution kernels, and implicitly maps the example represented in a labeled ordered tree into all subtree spaces. The implicit mapping defined by tree kernel is given as: Φ(x)=(I(t1 ⊆ x), ... , I(t|F |⊆ x)),</context>
</contexts>
<marker>Schapire, Freund, Bartlett, Lee, 1997</marker>
<rawString>Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. 1997. Boosting the margin: a new explanation for the effectiveness of voting methods. In Proc. ofICML, pages 322–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1411" citStr="Sebastiani, 2002" startWordPosition="205" endWordPosition="206">n stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners. We also discuss the relation between our algorithm and SVMs with tree kernel. Two experiments on opinion/modality classification confirm that subtree features are important. 1 Introduction Text classification plays an important role in organizing the online texts available on the World Wide Web, Internet news, and E-mails. Until recently, a number of machine learning algorithms have been applied to this problem and have been proven successful in many domains (Sebastiani, 2002). In the traditional text classification tasks, one has to identify predefined text “topics”, such as politics, finance, sports or entertainment. For learning algorithms to identify these topics, a text is usually represented as a bag-of-words, where a text is regarded as a multi-set (i.e., a bag) of words and the word order or syntactic relations appearing in the original text is ignored. Even though the bag-of-words representation is naive and does not convey the meaning of the original text, reasonable accuracy can be obtained. This is because each word occurring in the text is highly relev</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoyoshi Tamura</author>
<author>Keiji Wada</author>
</authors>
<title>Text structuring by composition and decomposition of segments (in Japanese).</title>
<date>1996</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="19408" citStr="Tamura and Wada, 1996" startWordPosition="3412" endWordPosition="3415">f 5,741 sentences were collected from a Web-based discussion BBS on PHS, in which users are directed to submit positive reviews separately from negative reviews. The unit of classification is a sentence. The categories to be identified are “positive” or “negative” with the numbers 2,679 and 3,062 respectively. • Modality identification (MOD) This task is to classify sentences (in Japanese) by modality. A total of 1,710 sentences from a Japanese newspaper were manually annotated 2PHS (Personal Handyphone System) is a cell phone system developed in Japan in 1989. according to Tamura’s taxonomy (Tamura and Wada, 1996). The unit of classification is a sentence. The categories to be identified are “opinion”, “assertion” or “description” with the numbers 159, 540, and 1,011 respectively. To employ learning and classification, we have to represent a given sentence as a labeled ordered tree. In this paper, we use the following three representation forms. • bag-of-words (bow), baseline Ignoring structural information embedded in text, we simply represent a text as a set of words. This is exactly the same setting as Boostexter. Word boundaries are identified using a Japanese morphological analyzer, ChaSen3. • Dep</context>
</contexts>
<marker>Tamura, Wada, 1996</marker>
<rawString>Naoyoshi Tamura and Keiji Wada. 1996. Text structuring by composition and decomposition of segments (in Japanese). Journal of Natural Language Processing, 5(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="2565" citStr="Turney, 2002" startWordPosition="383" endWordPosition="384">is because each word occurring in the text is highly relevant to the predefined “topics” to be identified. ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp Given that a number of successes have been reported in the field of traditional text classification, the focus of recent research has expanded from simple topic identification to more challenging tasks such as opinion/modality identification. Example includes categorization of customer E-mails and reviews by types of claims, modalities or subjectivities (Turney, 2002; Wiebe, 2000). For the latter, the traditional bag-of-words representation is not sufficient, and a richer, structural representation is required. A straightforward way to extend the traditional bag-of-words representation is to heuristically add new types of features to the original bag-of-words features, such as fixed-length n-grams (e.g., word bi-gram or tri-gram) or fixedlength syntactic relations (e.g., modifier-head relations). These ad-hoc solutions might give us reasonable performance, however, they are highly taskdependent and require careful design to create the “optimal” feature se</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proc. of ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proc. ofAAAI/IAAI,</booktitle>
<pages>735--740</pages>
<contexts>
<context position="2579" citStr="Wiebe, 2000" startWordPosition="385" endWordPosition="386">h word occurring in the text is highly relevant to the predefined “topics” to be identified. ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp Given that a number of successes have been reported in the field of traditional text classification, the focus of recent research has expanded from simple topic identification to more challenging tasks such as opinion/modality identification. Example includes categorization of customer E-mails and reviews by types of claims, modalities or subjectivities (Turney, 2002; Wiebe, 2000). For the latter, the traditional bag-of-words representation is not sufficient, and a richer, structural representation is required. A straightforward way to extend the traditional bag-of-words representation is to heuristically add new types of features to the original bag-of-words features, such as fixed-length n-grams (e.g., word bi-gram or tri-gram) or fixedlength syntactic relations (e.g., modifier-head relations). These ad-hoc solutions might give us reasonable performance, however, they are highly taskdependent and require careful design to create the “optimal” feature set for each tas</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce M. Wiebe. 2000. Learning subjective adjectives from corpora. In Proc. ofAAAI/IAAI, pages 735–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Zaki</author>
</authors>
<title>Efficiently mining frequent trees in a forest.</title>
<date>2002</date>
<booktitle>In Proc. of SIGKDD,</booktitle>
<pages>71--80</pages>
<contexts>
<context position="10446" citStr="Zaki, 2002" startWordPosition="1719" endWordPosition="1720">as a variant of the branch-and-bound algorithm, and is summarized in the following strategies: 1. Define a canonical search space in which a whole set of subtrees of a set of trees can be enumerated. 2. Find the optimal rule by traversing this search space. 3. Prune search space by proposing a criterion with respect to the upper bound of the gain. We will describe these steps more precisely in the following subsections. 3.1 Efficient Enumeration of Trees Abe and Zaki independently proposed an efficient method, rightmost-extension, to enumerate all subtrees from a given tree (Abe et al., 2002; Zaki, 2002). First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (k − 1) by attaching a new node to this tree to obtain trees of size k. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable. The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of attachment. We here give the definition of rightmost extension to describe this restriction in detail. Definition 4 Rightmost Extension (Abe et al., 2002; Zaki, 2002) Let t and t&apos; be </context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed Zaki. 2002. Efficiently mining frequent trees in a forest. In Proc. of SIGKDD, pages 71–80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>