<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<note confidence="0.996330333333333">
Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
Disambiguation: Recent Successes and Future Directions, Philadelphia,
July 2002, pp. 67-73. Association for Computational Linguistics.
</note>
<title confidence="0.996255">
Unsupervised Italian Word Sense Disambiguation using
WordNets and Unlabeled Corpora
</title>
<author confidence="0.997673">
Radu Florian and Richard Wicentowski
</author>
<affiliation confidence="0.9650815">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.609276">
{rflorian,richardw}@cs .jhu .edu
</email>
<sectionHeader confidence="0.959825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909307692308">
This paper presents a novel method
for unsupervised word sense disam-
biguation, which combines multiple in-
formation sources, including seman-
tic relations, large unlabeled corpora,
and cross-lingual distributional statis-
tics. This method extends and builds
on the JHU system that participated in
the SENSEVAL2 exercise. Experiments
performed on the SENSEVAL2 Italian
lexical-sample data show significant im-
provements over previously published
results on this data set.
</bodyText>
<sectionHeader confidence="0.996302" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891545454546">
The goal of this paper is to present an unsuper-
vised word sense disambiguation system which
extends the JHU system for the Italian lexical
sample task which participated in the SENSE-
VAL2 exercise (Yarowsky et al., 2001). Our sys-
tem combines word semantic relations, large un-
labeled corpora and cross-lingual distributional
statistics. The combining system reduces the
word sense error rate by 8.2% absolute (13.6%
relative error reduction), when compared to the
best system submitted in SENSEVAL2.
</bodyText>
<subsectionHeader confidence="0.927274">
1.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.999215507692308">
Several approaches that address the problem of
unsupervised word sense disambiguation (WSD
henceforth) have been presented in the past
few years. In one of the most widely-cited
unsupervised WSD systems, Yarowsky (1995)
uses a very small seed set (2-3 examples) to
bootstrap a WSD algorithm based on decision
lists; the algorithm yields highly accurate re-
sults, competitive with similar supervised sys-
tems. Schutze (1998) creates word vectors by
extracting ambiguous words and their contexts
from an unlabeled corpus. After clustering the
vectors&apos;, the disambiguation is performed by se-
lecting the sense centroid closest to the test word
vector. Pedersen and Bruce (1998) use an EM-
based algorithm to group sentences containing
the target word into unlabeled clusters which are
then mapped to sense tags.
The existence of the freely available word sense
relation database WordNet (Miller, 1995) has
enabled the conception of several unsupervised
WSD systems, including those by Resnik (1997)
who uses syntactically parsed corpora, partially
hand-labeled with senses from WordNet (Miller,
1995), to train a selectional preference system,
and McCarthy et al. (2001), which uses a selec-
tional preference model similar to Resnik (1997),
but without the use of any labeled data, in the
&amp;quot;one sense per discourse&amp;quot; paradigm (Gale et al.,
1992), achieving good precision in the SENSE-
VAL2 English all-words task. Using a cross-
lingual approach, Rigau et al. (1997) investi-
gates an unsupervised system using a combina-
tion of monolingual dictionaries, bilingual dictio-
naries and the English WordNet. The bilingual
dictionaries were used to map words from Span-
ish and French into English in order to leverage
the semantic relationships in the English Word-
Net. In an approach similar in spirit to the one
presented here, Mihalcea and Moldovan (1999)
use the entire English WordNet to find sentences
in online texts containing high-confidence exam-
ples of the target word. The WordNet glosses
and acquired sentences are used as training data
to automatically create a large sense-tagged cor-
pus.
In work on the same dataset as used in this
research, Magnini et al. (2001) manually anno-
Similar to Yarowsky (1995), Schutze (1998)
tested his algorithm only on words with two senses.
tates the relevant Italian synsets with a seman-
tic class. The test samples are assigned to one
of these classes using a supervised algorithm
trained on an annotated English corpus and then
these classes were mapped back to the WordNet
synsets.
Unsupervised word sense disambiguation us-
ing WordNet relations also has also been used
in real-world tasks. Some of the earlier ex-
amples include Voorhees (1993) (using the hy-
ponym/hypernym relations from WordNet) and
Sussna (1993) (using weighted relations derived
from WordNet), employing word sense disam-
biguation to increase the performance of infor-
mation retrieval systems.
</bodyText>
<sectionHeader confidence="0.973518" genericHeader="introduction">
2 Feature Representation
</sectionHeader>
<bodyText confidence="0.9969505">
In the model presented in this research, docu-
ments� are represented as bags of words and/or
lemmas; in addition, local n-grams around the
ambiguous word are also part of the document&apos;s
vector:
d =(dl ... d1v1), dj = c�NWS
where c� is the number of times the feature j3 ap-
pears in document d, N is the number of words
in d and Wj is a weight associated with feature
j4. Confusion between the same word partici-
pating in multiple feature roles is avoided by ap-
pending the feature values with their positional
type (e.g. uomo_L is different from uomo in un-
marked bag-of-words context).
All test documents were part-of-speech tagged
using the Italian version of the decision tree-
based POS tagger described in Schmid (1994).
Extracting lemma information from Italian is
an important process — Section 6 evaluates a
scenario where lemmatization is not used, and
shows that a substantial decrease in perfor-
mance occurs. Lemmatization is performed us-
ing the supervised morphological analyzer from
Throughout the paper, we will use the word doc-
ument to denote the actual context where the am-
biguous word appears.
</bodyText>
<footnote confidence="0.8786848">
3A feature can be a word, a lemma or a local
ngram; the model uses either words or lemmas, but
never both.
4The weight Wj depends on the type of the fea-
ture fj: for the bag-of-word features, this weight
is inversely proportional to the distance between
the target word and the feature, while for extended
ngram features it is a empirically estimated weight
(same value used in a similar English sense classi-
fier).
</footnote>
<table confidence="0.9958256">
morph. analyzer POS tagger
token type token type
verbs 98.1% 99.0% 99.9% 99.4%
nouns 99.5% 98.7% 96.2% 94.8%
adJs 96.7% 99.6% 85.3% 98.0%
</table>
<tableCaption confidence="0.8953625">
Table 1: Lemmatization and POS tagging ac-
curacy
</tableCaption>
<bodyText confidence="0.997738636363636">
Yarowsky and Wicentowski (2000), trained on
the filtered output of the POS tagger. We tested
the accuracy of the morphological analyzer by
randomly selecting 500 adJectives, 500 nouns and
500 verbs5 in proportion to their token frequency
in the unannotated corpus and had them hand-
checked by a native speaker. Table 1 presents the
POS and lemmatization accuracy for these 1500
words; the lemmatization accuracy is reported
only on examples which were correctly labeled
by the POS tagger.
</bodyText>
<sectionHeader confidence="0.998045" genericHeader="method">
3 Information Sources
</sectionHeader>
<subsectionHeader confidence="0.994559">
3.1 Italian WordNet
</subsectionHeader>
<bodyText confidence="0.977189111111111">
Since no training data was available for this
task, we rely on alternative sources of informa-
tion for inducing sense classification. One cen-
tral resource in this process is the ItalWordNet,
version 1.0, developed by the Italian National
ProJect, SI-TAL (Roventini et al., 2000), which
was provided with the data. This structure de-
scribes various semantic relationships between
words (usually binary relations), including:
</bodyText>
<listItem confidence="0.8746378125">
• synonymy — word a is a synonym of word v
if word a has nearly the same definition as
word v.
• antonymy — word a is an antonym of word v
if words a and v have nearly opposite mean-
ings.
• hyperonymy - word a is a hyperonym of
word v if a is a generalization of word v;
• hyponymy — the reverse of the hyperonymy
relation;
• meronymy — word a is a meronym of word v
if the obJect represented by a has the obJect
represented by v as a part (for instance, car
is a meronym of wheel);
• holonymy — is the reverse relation of
meronymy;
</listItem>
<bodyText confidence="0.972757">
ItalWordNet is not a freely available resource;
only the parts that were provided with the task
have been used in this research&apos;.
5As identified by the POS tagger.
sOf the 40248 synsets present in the ItalWordNet,
only 616 were provided .
</bodyText>
<table confidence="0.985184428571429">
Relation Number of relations
hypernym/hyponym 4126
meronym/holonym 106
cause 70
antonym 64
other 576
Total relations 4942
</table>
<tableCaption confidence="0.822907">
Table 2: ItalWordNet statistics for the provided
subset
</tableCaption>
<bodyText confidence="0.999748058823529">
Of the 83 words that are part of the evalua-
tion, 82 of them had entries in the ItalWordNet;
one word, bello, had no direct entry, but there
were entries related to this particular word in
the other entries, and we used those as inductive
bias in the classification. Table 2 shows the num-
ber of different relations present in the selected
ItalWordNet.
Intuitively, some of the WordNet relations
are more useful than others for sense disam-
biguation. For instance, the synonymy and
near-synonymy relations are more relevant than
the role_instrument relation. To address this
problem, each relation is assigned a intuitively-
motivated weight7; each relation influences the
overall behavior of the algorithm proportionally
to its weight.
</bodyText>
<subsectionHeader confidence="0.992741">
3.2 Relations to the English WordNet
</subsectionHeader>
<bodyText confidence="0.999047416666667">
In addition to relations among Italian words, the
ItalWordNet contains links to the English Word-
Net senses of the corresponding translations (if
any exist). In some cases, direct translations are
not present, but a relation to a English WordNet
sense is present (such as eq_has_hyponym or
eq_generalization). These resources provide ac-
cess to an independent information source — the
distributional frequency of these sense as found
in the English WordNet (which is present in ver-
sion 1.7) (Miller, 1995). This information is used
to obtain a second word sense classifier, used in
system combination (as presented in Section 5).
Since the English senses in ItalWordNet are
the senses in WordNet 1.5, we used the sense
mappings wn1.5 —� wn1.6 —� wn1.7, as de-
scribed in Daude et al. (1999)8.
Since no training data was available, the weights
could not be adjusted to minimize error rate. An al-
ternative would be to estimate the weights on known
classifications, e.g. English, and assume that they
are language independent.
&amp;quot;The mappings were obtained from
http:11www.lsi.upc.es1-nlp1tools1mapping.html
</bodyText>
<sectionHeader confidence="0.970048" genericHeader="method">
4 The Algorithm
</sectionHeader>
<bodyText confidence="0.996487172413793">
At a high level, the proposed algorithm for dis-
ambiguating a word v consists of first identify-
ing words w that are similar in sense with word
v, and selecting contexts of 2-3 sentences con-
taining words the words w (including contexts
containing the word v itself), creating sense cen-
troids using these contexts, and bootstrapping
a K-means clustering algorithm with the initial
seeds.
The algorithmic framework used in this re-
search is based on the following assumption:
Assumption 1 If a word u that has a sense s,,,
similar to the sense s„ of word v (as identified
by a relation in ItalWordNet between s,,, and s„),
then any context containing word u is indicative
of sense s,,.
Assumption 1 can yield poor results in cases
where two senses of word w are associated with
different senses of the same word a (such as press
and suit); in this case, sentences corresponding
to word a will contribute to both senses of word
w associated with u. The hope in this case is
that all the words participating in disambigua-
tion will cooperate in increasing the likelihood
of the correct sense, and the effect of ambigu-
ous examples will constitute white noise in the
mass of relevant distinctions. If the noise is ac-
tually biased, the algorithm may fail to identify
the correct sense.
</bodyText>
<subsectionHeader confidence="0.986959">
4.1 Identifying Relevant Contexts
</subsectionHeader>
<bodyText confidence="0.979928222222222">
From an engineering point of view, the Ital-
WordNet is considered to be a set of relations
W defined on the set of word-sense pairs. For-
mally, to identify the degree to which two senses
are related, we construct a weighted multigraph
GW = (V, E), where the set of vertices V is de-
fined as
V = {(v, s) Is is a sense of v}
and the set of edges is defined as
</bodyText>
<equation confidence="0.796687">
E = {((u, su) , (v, s„)) IIr E W s.t.
((u, SO , (v, SM E r }
</equation>
<bodyText confidence="0.997850125">
The weight associated with an edge, W (e)
is the weight of the relation associated with the
edge; we will interpret these weights as distances
rather than similarities (smaller weights indicate
more similar senses).
To identify the set of words that are related
in meaning with an ambiguous word w, we start
from the senses corresponding to word w, Go =
</bodyText>
<equation confidence="0.9663952">
I(
~ 81) , ... , (w, sn)1, and we then expand the
set G in the graph GW as follows
ck+1 = Ek u 11111&apos; E ck, r E w s.t. (1,1&apos;) E r}
(1)
</equation>
<bodyText confidence="0.9974752">
Intuitively, we expand the set of words that are
related to the ambiguous word w by examin-
ing the relations r in which its senses are in-
volved, after which we expand the newly ob-
tained senses, and so on. The relationships are
expanded by examining the sense of each node,
but the final output will extract the words asso-
ciated with those senses&apos;. Once the final set GK
is computed, the relevance of each word in it is
computed by
</bodyText>
<equation confidence="0.848662666666667">
W (l) = p path from Co to l
min wa (P) (2)
and wC (lo, ... , In) =Ei wC (Ii, li+1).
</equation>
<bodyText confidence="0.999935625">
The next step in the algorithm is to extract
contexts cl associated with each word l in GK
— for this purpose, we used a corpus of clean
Italian newspaper text (extracted from Corriere
Della Sera, 1993). After expanding the set G1
(5543 words; initially, there are 83 ambiguous
words), the selected contexts formed a corpus of
approximately 700M wordsio .
</bodyText>
<subsectionHeader confidence="0.98186">
4.2 Automatic Sense Clustering
</subsectionHeader>
<bodyText confidence="0.9960105625">
Algorithm 1 presents the proposed K-means-
style clustering procedure, consisting of two ma-
Jor parts: computing the initial sense centroids,
and the the application of K-means clustering.
The initial centroids are computed by seeding
given them by the contexts cl; each such context
has a similarity to a particular centroid, inher-
ited from the word that induced the context.
In the following step, the test documents — in-
cluding the contexts cl containing the ambiguous
word — are assigned to the sense centroids (equa-
tion (4)), by computing the similarity between
their corresponding vectors and the sense cen-
troid vectors. There are several choices for the
similarity measure; the one displayed in equation
(4) is computing the similarity as
</bodyText>
<equation confidence="0.999779">
� (��) � (�����)
��� (��� ��) = � (�����) =
P (ct) (7)
</equation>
<bodyText confidence="0.9032202">
9Unfortunately, the version of Italian WordNet
we had access to has only a small subset of the rela-
tions, so we were forced to stopat k = I.
1oDocuments appear in several word lists and the
numbers include punctuation.
</bodyText>
<sectionHeader confidence="0.587731" genericHeader="method">
Algorithm I K-means-style WSD
</sectionHeader>
<bodyText confidence="0.99961">
and makes use of the naive Bayes assumption
that the words in document cl are independent
given the sense, yielding
</bodyText>
<equation confidence="0.964056">
P (siIcl) �=
</equation>
<bodyText confidence="0.944995">
Other possible choices for the similarity
sim (cl, si) include Bayes ratio (Gale et al., 1992)
</bodyText>
<equation confidence="0.999546333333333">
P (siIcl) = P (si) 1 1
P (-si) WEct P (WI-8i)
P(wIsi) (9)
</equation>
<bodyText confidence="0.841851">
and cosine similarity
</bodyText>
<equation confidence="0.994503">
P (siIcl) = (si, cl) (10)
II8i112 IICII12
</equation>
<bodyText confidence="0.99608">
Once the similarities P (siIcl) have been com-
puted, the centroids (si)i can be updated as in
</bodyText>
<listItem confidence="0.878052333333333">
1. Input: the ambiguous word w.
2. Create the extended set of lemmas GK as de-
scribed in equation (�).
3. For each lemma l E CK, select contexts cl sur-
rounding l from a large unlabeled corpus.
4. Assign the contexts cl to centroids correspond-
</listItem>
<bodyText confidence="0.417547">
ing to the senses of word w : S1 ... SN:
</bodyText>
<equation confidence="0.679209">
W (r) • cl (3)
</equation>
<bodyText confidence="0.614807333333333">
5. Compute the similarity of each context ct (cor-
responding to the test samples) to the centroid
Si. For example:
</bodyText>
<equation confidence="0.954466466666667">
E
si =
P (w1Si)
P (Si) II
wEct
n
j=1
P (Sj) F1
wEct
P (wISj) (4)
�
ct test context
Sim (ct, Si) • ct (5)
Si =
6. Assign all the test centroids ct to senses Si,
</equation>
<bodyText confidence="0.790378833333333">
based on the similarity between the centroids
ct and Si:
7. Repeat from step 6 until convergence or a de-
sired number of iterations is reached.
8. Classify each test document t with the sense cor-
responding to the closest centroid
</bodyText>
<equation confidence="0.997649142857143">
S� (t) = arg max Sim (t, Si) (6)
i=1...n
Sim (ct, Si) =
P (si) F1 P (wlsi)
WEct (8)
E P (sj) F1 P (��sj)
j=1 WEct
</equation>
<bodyText confidence="0.9267987">
equation (5) — the centroid assignment shown in
(10) is a soft one — each document will contribute
to every centroid, with a ratio corresponding to
their similarity. An alternative is to use hard
document assignment
By using this assumption, we obtain another
classifier, as depicted in Algorithm 2. Even
though this classifier is relatively simple, it ob-
tains reasonably good results, as we will see in
the experimental section.
</bodyText>
<equation confidence="0.913268">
S~arg max �sim (cl, sj) , i~ ct
Ct (11)
si =~
</equation>
<bodyText confidence="0.9936615">
where each document will contribute only to the
centroid closest to it; S is the Kronecker symbol:
</bodyText>
<equation confidence="0.9987615">
0 if X �4g
S (X, g) = 1 if X = g
</equation>
<sectionHeader confidence="0.463828" genericHeader="method">
Combining Information Sources
</sectionHeader>
<bodyText confidence="0.9999505">
It is quite useful for a classification task to have
access to multiple information sources; it follows
from a standard argument that the information
one has about a process (as measured by the
entropy of the process) can only decrease if one
obtains additional information:
</bodyText>
<equation confidence="0.69977">
H (AIB) G H (A)
</equation>
<bodyText confidence="0.993129466666667">
Several studies in the machine learning commu-
nity have shown that combining the information
obtained from several classifiers not only results
in improved performance, but also improves ro-
bustness. Even in cases where one has access to
several structurally different information sources
that are not easily integrable (Stevenson and
Wilks, 2001), it might be more beneficial to con-
struct separate classifiers for each type of con-
text and combine their outputs, than construct
a more complex classifier that tries to handle the
combination internally.
Aside from the output of the classifier de-
scribed in Section 4, we have access to two other
information sources:
</bodyText>
<listItem confidence="0.846983857142857">
• the English distributional usage of the
translation of a particular Italian word
sense;
• the output of another word sense disam-
biguation system (Magnini et al., 2001)
(downloadable from the SENSEVAL2 web
site).
</listItem>
<bodyText confidence="0.958490583333333">
In using the English distributional data, we make
the following assumption:
Assumption 2 If an Italian word sense sI has
an English translation in sense sE, then the us-
age of sense sE in English is characteristic of the
usage of sense sI in Italian.
Given N classifiers (possibly having prob-
abilistic output), an easy and effective way
of combining their output is through voting
(Brill and Wu, 1998; van Halteren et al., 1998;
Yarowsky et al., 2001), by computing the output
classification as
</bodyText>
<equation confidence="0.98962">
s� = arg max
, ~
i
</equation>
<bodyText confidence="0.9999578">
where si (d) = arg max, Pi (sId). In other words,
each classifier votes for the sense which it con-
siders most likely, weighted by the probability of
that sense. In the end, the sense that has been
voted the most winsil.
</bodyText>
<sectionHeader confidence="0.994133" genericHeader="evaluation">
6 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.998079666666667">
The test data in the Italian lexical-sample task
consists of 3889 contexts of 1 to 3 sentences, for
83 ambiguous words.
</bodyText>
<subsectionHeader confidence="0.9243455">
6.1 Influence o f Morphological Analysis
on Performance
</subsectionHeader>
<bodyText confidence="0.999513916666667">
To investigate the impact of using the morpho-
logical analyzer, we created a second set Ek de-
rived from an unlemmatized corpus. This means
that we do not include documents which are as-
sociated with inflections of words related by Ital-
WordNet, and that the clustering algorithm is
run on this unlemmatized data set.
&amp;quot;Ties are broken in favor of the sense which ap-
pears first in the ItalWordNet; this strategy proved
to perform the best on the other SENSEVAL2 tasks
we experimented on (only then the order was lexico-
graphical).
</bodyText>
<figure confidence="0.995360166666667">
1. For each sense si of ambiguous word v
countst (sm)= ~ countE, (cj)
ej translation of si
2. Compute the sense probability
countlt (si)
Pv (si) =
~ countit (s
Pt (s)
j
3. For a test word t, return
s� = arg max
S
J)
S (s, si (d)) P (sId)
(12)
Algorithm 2 English Most-Likely Classifier
0 1e+08 2e+08 3e+08 4e+08 5e+08 6e+08 7e+08
Size of unlabeled corpus (words)
</figure>
<figureCaption confidence="0.9928475">
Figure 1: Sense classification accuracy versus
initial unlabeled corpus size
</figureCaption>
<bodyText confidence="0.999957866666667">
As presented in Table 3, line 2, there is a sig-
nificant decrease in performance when the mor-
phological analyzer was omitted. Using the same
initial corpus, approximately 75% of the origi-
nally extracted documents were selected12; how-
ever, our algorithm on the set Ek achieves only
34.6% accuracy, significantly lower than the re-
sults obtained by the full system on 75% of the
data, 36.6% (as shown in Figure 1).
The unlemmatized system&apos;s performance is
most comparable to the performance achieved
when using only 10% of the corpus. In other
words, it takes a corpus 10 times larger in or-
der to compensate for the inability to perform
morphological analysis.
</bodyText>
<subsectionHeader confidence="0.999612">
6.2 Classification Results
</subsectionHeader>
<bodyText confidence="0.970622619047619">
Figure 1 presents the results obtained by the al-
gorithm presented in Section 4, for varying sizes
of the unlabeled corpus. The performance in-
creases from 34.5% at 70M words to 37.1% at
700M words (the difference in performance is
statistically significant at a confidence level of
10-3).
The experiments we ran to test the perfor-
mance of the classifier are presented in Table
3. There are many baselines against which one
can measure performance. Line 1 of Table 3
shows the estimated performance of a system
that chooses a sense at random. In line 2, the
performance of the system is evaluated by run-
ning the system without lemmatization, as dis-
cussed in Section 6.1. Line 4 presents the perfor-
mance of the K-means system presented in Sec-
tion 4, and line 3 presents the performance of the
12The selection is done originally by using lemmas
rather than words; when using words, fewer contexts
will be selected.
</bodyText>
<table confidence="0.998115615384615">
System Accuracy
Fine Coarse
1 Random Choice 25.3% -
2 w/o Morphology 34.7% 41.8%
3 JHU01 35.3% 42.3%
4 k-Means System 37.1% 44.0%
5 Magnini01 39.0% 46.3%
6 Italian Most Likely 40.8% 45.3%
7 English Most Likely 38.9% 45.3%
8 I-ML(5) + E-ML(6) 46.4% 51.9%
9 Final system 47.2% 53.7%
10 Oracle Most Likely 65.4% 69.3%
11 Oracle Voting 73.4% 77.0%
</table>
<tableCaption confidence="0.956653">
Table 3: Sense classification accuracy for differ-
ent variations of the system
</tableCaption>
<bodyText confidence="0.988845243243243">
original JHU system. The difference between the
2 consists mainly in the size of unlabeled corpus
and quality of lemmatization.
Perhaps the most interesting result in Table 3
is the fact that the estimation of the most likely
sense for a given word can be done more ro-
bustly than the estimation of individual senses.
By making the system return the most likely
sense (of the senses output by the clustering al-
gorithm) for a given word, the performance in-
creases by nearly 4% (line 6 in the table), yield-
ing the best individual system results.
The classifier based on the sense distributions
on the English WordNet, described in Algorithm
2, yields good performance (line 7), comparable
with the best SENSEVAL2 system (line 5).
By combining the two most likely systems (the
one obtained on the Italian data by using K-
means clustering and the one obtained from the
English WordNet), one can obtain an impressive
performance of 46.4%, and adding the best com-
peting system from the Italian SENSEVAL2 ex-
ercise, we obtain a performance of 47.2%. The
difference in performance between the last two
systems is not, however, statistically significant
at a confidence level of 0.05.
As a final observation on the system perfor-
mance, it is interesting to remark that there is
still a long way to go before obtaining results
that are competitive with the most-likely ora-
cle performance, listed in the table on line 10.
This oracle returns for each sample the &amp;quot;true&amp;quot;
most-likely sense (computed on the test data);
it obtains a performance of 65.3%, substantially
better than the other results obtained on this
data. This oracle is outperformed by a voting
oracle, which returns the correct sense if at least
</bodyText>
<figure confidence="0.937885555555556">
37.5
37
36.5
36
35.5
35
Sense Classification Accuracy
34.5
one classifier predicted it, (line 11 of Table 3)13.
</figure>
<sectionHeader confidence="0.827243" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.997729">
In conclusion, we have presented a novel method
of word sense classification by using large
amounts of unlabeled data, word semantic re-
lations both in the target and a second lan-
guage. The procedure integrates these knowl-
edge sources to provide a more robust estima-
tion. The performance obtained, while still lower
than the true most likely classification, substan-
tially outperforms previously published results
on this data set.
As future work, we plan to integrate more
sophisticated syntactic knowledge/features into
the model, to develop a better weighting scheme
of individual semantic relations by training on
labeled text (in another language, e.g. English)
and also to improve the balance of the per sense
training samples.
</bodyText>
<sectionHeader confidence="0.994899" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999749692307692">
The authors would like to thank David Yarowsky
for his useful comments and support, Gideon
Mann for his helpful comments on an early draft
of this paper, the Johns Hopkins University NLP
lab and JHU SENSEVAL2 team for a creating
a stimulating research environment, to Paola
Virga for doing that annoying annotation, and to
the anonymous reviewers for their helpful com-
ments and suggestions, especially in identifying
a mismatch between the English WordNet ver-
sions. This work was partially supported by
NSF grant IIS-9985033 and ONR/MURI con-
tract N00014-01-1-0685.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999204479452055">
E. Brill and J. Wu. 1998. Classifier combination for
improved lexical disambiguation. In Proceedings
of COLING-ACL&apos;98, pages 191-195.
J. Daude, L. Padro, and G. Rigau. 1999. Experi-
ments on applying relaxation labeling to map mul-
tilingual hierarchies. Technical Report LSI-99-5-
R, Software Department. UPC.
W. Gale, K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the Humanities, 26:415-
439.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2001. Using domain information
for word sense disambiguation. In Proceedings of
SENSEVAL2.
13The voting oracle performance effectively consti-
tutes an upper bound on a voting system&apos;s perfor-
mance.
D. McCarthy, J. Carroll, and J. Preiss. 2001. Dis-
ambiguating noun and verb senses using automat-
ically acquired selectional preferences. In Proceed-
ings of SENSEVAL2.
R. Mihalcea and D. Moldovan. 1999. An automatic
method for generating sense tagged corpora. In
Proceedings of AAAI &apos;99, pages 461-466.
G. A. Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38(11).
T. Pedersen and R. Bruce. 1998. Knowledge lean
word sense disambiguation. In Proceedings of the
Fifteenth National Conference on Artificial Intel-
ligence, pages 800-805.
P. Resnik. 1997. Selectional preference and sense dis-
ambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Seman-
tics, pages 52-57.
G. Rigau, J. Atserias, and E. Agirre. 1997. Combin-
ing unsupervised lexical knowledge methods for
word sense disambiguation. In Proceedings of ACL
&apos;97, pages 48-55.
A. Roventini, A. Alonge, F. Bertagna, B. Magnini,
and N. Calzolari. 2000. ItalWordNet: a large
semantic database for Italian. In Proceedings of
LREC-2000, pages 783-790.
H. Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Con-
ference on New Methods in Language Processing.
H. Schutze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97-124.
M. Stevenson and Y. Wilks. 2001. The interaction of
knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321-349.
M. Sussna. 1993. Word sense disambiguation for
free-text indexing using a massive semantic net-
work. In Proceedings of CIKM &apos;93.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998.
Improving data driven wordclass tagging by sys-
tem combination. In Proceedings of COLING-
ACL&apos;98, pages 491-497.
E. Voorhees. 1993. Using WordNet to disambiguate
word senses for text retrieval. In Proceedings of
SIGIR &apos;93, pages 171-180.
D. Yarowsky and R. Wicentowski. 2000. Minimally
supervised morphological analysis by multimodal
alignment. In Proceedings of ACL-2000, pages
207-216.
D. Yarowsky, S. Cucerzan, R. Florian, C. Schafer,
and R. Wicentowski. 2001. The Johns Hopkins
SENSEVAL2 system descriptions. In Proceedings of
SENSEVAL2.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189-196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.277332">
<note confidence="0.948021666666667">Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, July 2002, pp. 67-73. Association for Computational Linguistics.</note>
<title confidence="0.8055975">Unsupervised Italian Word Sense Disambiguation using WordNets and Unlabeled Corpora</title>
<author confidence="0.940742">Florian</author>
<affiliation confidence="0.909785">Center for Language and Speech</affiliation>
<address confidence="0.509">Johns Hopkins</address>
<email confidence="0.955705">rflorian@cs.jhu.edu</email>
<email confidence="0.955705">richardw@cs.jhu.edu</email>
<abstract confidence="0.999230642857143">This paper presents a novel method for unsupervised word sense disambiguation, which combines multiple information sources, including semantic relations, large unlabeled corpora, and cross-lingual distributional statistics. This method extends and builds on the JHU system that participated in Experiments on the lexical-sample data show significant improvements over previously published results on this data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<pages>191--195</pages>
<contexts>
<context position="17538" citStr="Brill and Wu, 1998" startWordPosition="2973" endWordPosition="2976">ces: • the English distributional usage of the translation of a particular Italian word sense; • the output of another word sense disambiguation system (Magnini et al., 2001) (downloadable from the SENSEVAL2 web site). In using the English distributional data, we make the following assumption: Assumption 2 If an Italian word sense sI has an English translation in sense sE, then the usage of sense sE in English is characteristic of the usage of sense sI in Italian. Given N classifiers (possibly having probabilistic output), an easy and effective way of combining their output is through voting (Brill and Wu, 1998; van Halteren et al., 1998; Yarowsky et al., 2001), by computing the output classification as s� = arg max , ~ i where si (d) = arg max, Pi (sId). In other words, each classifier votes for the sense which it considers most likely, weighted by the probability of that sense. In the end, the sense that has been voted the most winsil. 6 Experimental Evaluation The test data in the Italian lexical-sample task consists of 3889 contexts of 1 to 3 sentences, for 83 ambiguous words. 6.1 Influence o f Morphological Analysis on Performance To investigate the impact of using the morphological analyzer, w</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of COLING-ACL&apos;98, pages 191-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daude</author>
<author>L Padro</author>
<author>G Rigau</author>
</authors>
<title>Experiments on applying relaxation labeling to map multilingual hierarchies.</title>
<date>1999</date>
<tech>Technical Report LSI-99-5-R,</tech>
<institution>Software Department. UPC.</institution>
<contexts>
<context position="9488" citStr="Daude et al. (1999)" startWordPosition="1519" endWordPosition="1522">ses, direct translations are not present, but a relation to a English WordNet sense is present (such as eq_has_hyponym or eq_generalization). These resources provide access to an independent information source — the distributional frequency of these sense as found in the English WordNet (which is present in version 1.7) (Miller, 1995). This information is used to obtain a second word sense classifier, used in system combination (as presented in Section 5). Since the English senses in ItalWordNet are the senses in WordNet 1.5, we used the sense mappings wn1.5 —� wn1.6 —� wn1.7, as described in Daude et al. (1999)8. Since no training data was available, the weights could not be adjusted to minimize error rate. An alternative would be to estimate the weights on known classifications, e.g. English, and assume that they are language independent. &amp;quot;The mappings were obtained from http:11www.lsi.upc.es1-nlp1tools1mapping.html 4 The Algorithm At a high level, the proposed algorithm for disambiguating a word v consists of first identifying words w that are similar in sense with word v, and selecting contexts of 2-3 sentences containing words the words w (including contexts containing the word v itself), creati</context>
</contexts>
<marker>Daude, Padro, Rigau, 1999</marker>
<rawString>J. Daude, L. Padro, and G. Rigau. 1999. Experiments on applying relaxation labeling to map multilingual hierarchies. Technical Report LSI-99-5-R, Software Department. UPC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<contexts>
<context position="2755" citStr="Gale et al., 1992" startWordPosition="399" endWordPosition="402">containing the target word into unlabeled clusters which are then mapped to sense tags. The existence of the freely available word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed corpora, partially hand-labeled with senses from WordNet (Miller, 1995), to train a selectional preference system, and McCarthy et al. (2001), which uses a selectional preference model similar to Resnik (1997), but without the use of any labeled data, in the &amp;quot;one sense per discourse&amp;quot; paradigm (Gale et al., 1992), achieving good precision in the SENSEVAL2 English all-words task. Using a crosslingual approach, Rigau et al. (1997) investigates an unsupervised system using a combination of monolingual dictionaries, bilingual dictionaries and the English WordNet. The bilingual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet. In an approach similar in spirit to the one presented here, Mihalcea and Moldovan (1999) use the entire English WordNet to find sentences in online texts containing high-confidence examples</context>
<context position="14202" citStr="Gale et al., 1992" startWordPosition="2367" endWordPosition="2370">r the similarity measure; the one displayed in equation (4) is computing the similarity as � (��) � (�����) ��� (��� ��) = � (�����) = P (ct) (7) 9Unfortunately, the version of Italian WordNet we had access to has only a small subset of the relations, so we were forced to stopat k = I. 1oDocuments appear in several word lists and the numbers include punctuation. Algorithm I K-means-style WSD and makes use of the naive Bayes assumption that the words in document cl are independent given the sense, yielding P (siIcl) �= Other possible choices for the similarity sim (cl, si) include Bayes ratio (Gale et al., 1992) P (siIcl) = P (si) 1 1 P (-si) WEct P (WI-8i) P(wIsi) (9) and cosine similarity P (siIcl) = (si, cl) (10) II8i112 IICII12 Once the similarities P (siIcl) have been computed, the centroids (si)i can be updated as in 1. Input: the ambiguous word w. 2. Create the extended set of lemmas GK as described in equation (�). 3. For each lemma l E CK, select contexts cl surrounding l from a large unlabeled corpus. 4. Assign the contexts cl to centroids corresponding to the senses of word w : S1 ... SN: W (r) • cl (3) 5. Compute the similarity of each context ct (corresponding to the test samples) to the</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<title>Using domain information for word sense disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL2.</booktitle>
<contexts>
<context position="3573" citStr="Magnini et al. (2001)" startWordPosition="532" endWordPosition="535">aries, bilingual dictionaries and the English WordNet. The bilingual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet. In an approach similar in spirit to the one presented here, Mihalcea and Moldovan (1999) use the entire English WordNet to find sentences in online texts containing high-confidence examples of the target word. The WordNet glosses and acquired sentences are used as training data to automatically create a large sense-tagged corpus. In work on the same dataset as used in this research, Magnini et al. (2001) manually annoSimilar to Yarowsky (1995), Schutze (1998) tested his algorithm only on words with two senses. tates the relevant Italian synsets with a semantic class. The test samples are assigned to one of these classes using a supervised algorithm trained on an annotated English corpus and then these classes were mapped back to the WordNet synsets. Unsupervised word sense disambiguation using WordNet relations also has also been used in real-world tasks. Some of the earlier examples include Voorhees (1993) (using the hyponym/hypernym relations from WordNet) and Sussna (1993) (using weighted </context>
<context position="17094" citStr="Magnini et al., 2001" startWordPosition="2898" endWordPosition="2901">where one has access to several structurally different information sources that are not easily integrable (Stevenson and Wilks, 2001), it might be more beneficial to construct separate classifiers for each type of context and combine their outputs, than construct a more complex classifier that tries to handle the combination internally. Aside from the output of the classifier described in Section 4, we have access to two other information sources: • the English distributional usage of the translation of a particular Italian word sense; • the output of another word sense disambiguation system (Magnini et al., 2001) (downloadable from the SENSEVAL2 web site). In using the English distributional data, we make the following assumption: Assumption 2 If an Italian word sense sI has an English translation in sense sE, then the usage of sense sE in English is characteristic of the usage of sense sI in Italian. Given N classifiers (possibly having probabilistic output), an easy and effective way of combining their output is through voting (Brill and Wu, 1998; van Halteren et al., 1998; Yarowsky et al., 2001), by computing the output classification as s� = arg max , ~ i where si (d) = arg max, Pi (sId). In other</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2001</marker>
<rawString>B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2001. Using domain information for word sense disambiguation. In Proceedings of SENSEVAL2.</rawString>
</citation>
<citation valid="false">
<title>13The voting oracle performance effectively constitutes an upper bound on a voting system&apos;s performance.</title>
<marker></marker>
<rawString>13The voting oracle performance effectively constitutes an upper bound on a voting system&apos;s performance.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>J Carroll</author>
<author>J Preiss</author>
</authors>
<title>Disambiguating noun and verb senses using automatically acquired selectional preferences.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL2.</booktitle>
<contexts>
<context position="2583" citStr="McCarthy et al. (2001)" startWordPosition="369" endWordPosition="372">ectors&apos;, the disambiguation is performed by selecting the sense centroid closest to the test word vector. Pedersen and Bruce (1998) use an EMbased algorithm to group sentences containing the target word into unlabeled clusters which are then mapped to sense tags. The existence of the freely available word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed corpora, partially hand-labeled with senses from WordNet (Miller, 1995), to train a selectional preference system, and McCarthy et al. (2001), which uses a selectional preference model similar to Resnik (1997), but without the use of any labeled data, in the &amp;quot;one sense per discourse&amp;quot; paradigm (Gale et al., 1992), achieving good precision in the SENSEVAL2 English all-words task. Using a crosslingual approach, Rigau et al. (1997) investigates an unsupervised system using a combination of monolingual dictionaries, bilingual dictionaries and the English WordNet. The bilingual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet. In an approach si</context>
</contexts>
<marker>McCarthy, Carroll, Preiss, 2001</marker>
<rawString>D. McCarthy, J. Carroll, and J. Preiss. 2001. Disambiguating noun and verb senses using automatically acquired selectional preferences. In Proceedings of SENSEVAL2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D Moldovan</author>
</authors>
<title>An automatic method for generating sense tagged corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI &apos;99,</booktitle>
<pages>461--466</pages>
<contexts>
<context position="3254" citStr="Mihalcea and Moldovan (1999)" startWordPosition="479" endWordPosition="482"> similar to Resnik (1997), but without the use of any labeled data, in the &amp;quot;one sense per discourse&amp;quot; paradigm (Gale et al., 1992), achieving good precision in the SENSEVAL2 English all-words task. Using a crosslingual approach, Rigau et al. (1997) investigates an unsupervised system using a combination of monolingual dictionaries, bilingual dictionaries and the English WordNet. The bilingual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet. In an approach similar in spirit to the one presented here, Mihalcea and Moldovan (1999) use the entire English WordNet to find sentences in online texts containing high-confidence examples of the target word. The WordNet glosses and acquired sentences are used as training data to automatically create a large sense-tagged corpus. In work on the same dataset as used in this research, Magnini et al. (2001) manually annoSimilar to Yarowsky (1995), Schutze (1998) tested his algorithm only on words with two senses. tates the relevant Italian synsets with a semantic class. The test samples are assigned to one of these classes using a supervised algorithm trained on an annotated English</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>R. Mihalcea and D. Moldovan. 1999. An automatic method for generating sense tagged corpora. In Proceedings of AAAI &apos;99, pages 461-466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: A lexical database.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2314" citStr="Miller, 1995" startWordPosition="333" endWordPosition="334"> algorithm based on decision lists; the algorithm yields highly accurate results, competitive with similar supervised systems. Schutze (1998) creates word vectors by extracting ambiguous words and their contexts from an unlabeled corpus. After clustering the vectors&apos;, the disambiguation is performed by selecting the sense centroid closest to the test word vector. Pedersen and Bruce (1998) use an EMbased algorithm to group sentences containing the target word into unlabeled clusters which are then mapped to sense tags. The existence of the freely available word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed corpora, partially hand-labeled with senses from WordNet (Miller, 1995), to train a selectional preference system, and McCarthy et al. (2001), which uses a selectional preference model similar to Resnik (1997), but without the use of any labeled data, in the &amp;quot;one sense per discourse&amp;quot; paradigm (Gale et al., 1992), achieving good precision in the SENSEVAL2 English all-words task. Using a crosslingual approach, Rigau et al. (1997) investigates an unsupervised system usin</context>
<context position="9205" citStr="Miller, 1995" startWordPosition="1471" endWordPosition="1472">nces the overall behavior of the algorithm proportionally to its weight. 3.2 Relations to the English WordNet In addition to relations among Italian words, the ItalWordNet contains links to the English WordNet senses of the corresponding translations (if any exist). In some cases, direct translations are not present, but a relation to a English WordNet sense is present (such as eq_has_hyponym or eq_generalization). These resources provide access to an independent information source — the distributional frequency of these sense as found in the English WordNet (which is present in version 1.7) (Miller, 1995). This information is used to obtain a second word sense classifier, used in system combination (as presented in Section 5). Since the English senses in ItalWordNet are the senses in WordNet 1.5, we used the sense mappings wn1.5 —� wn1.6 —� wn1.7, as described in Daude et al. (1999)8. Since no training data was available, the weights could not be adjusted to minimize error rate. An alternative would be to estimate the weights on known classifications, e.g. English, and assume that they are language independent. &amp;quot;The mappings were obtained from http:11www.lsi.upc.es1-nlp1tools1mapping.html 4 Th</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: A lexical database. Communications of the ACM, 38(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>Knowledge lean word sense disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth National Conference on Artificial Intelligence,</booktitle>
<pages>800--805</pages>
<contexts>
<context position="2092" citStr="Pedersen and Bruce (1998)" startWordPosition="296" endWordPosition="299"> unsupervised word sense disambiguation (WSD henceforth) have been presented in the past few years. In one of the most widely-cited unsupervised WSD systems, Yarowsky (1995) uses a very small seed set (2-3 examples) to bootstrap a WSD algorithm based on decision lists; the algorithm yields highly accurate results, competitive with similar supervised systems. Schutze (1998) creates word vectors by extracting ambiguous words and their contexts from an unlabeled corpus. After clustering the vectors&apos;, the disambiguation is performed by selecting the sense centroid closest to the test word vector. Pedersen and Bruce (1998) use an EMbased algorithm to group sentences containing the target word into unlabeled clusters which are then mapped to sense tags. The existence of the freely available word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed corpora, partially hand-labeled with senses from WordNet (Miller, 1995), to train a selectional preference system, and McCarthy et al. (2001), which uses a selectional preference model similar to Resnik (1997), but without the use of any labeled data</context>
</contexts>
<marker>Pedersen, Bruce, 1998</marker>
<rawString>T. Pedersen and R. Bruce. 1998. Knowledge lean word sense disambiguation. In Proceedings of the Fifteenth National Conference on Artificial Intelligence, pages 800-805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics,</booktitle>
<pages>52--57</pages>
<contexts>
<context position="2411" citStr="Resnik (1997)" startWordPosition="347" endWordPosition="348">h similar supervised systems. Schutze (1998) creates word vectors by extracting ambiguous words and their contexts from an unlabeled corpus. After clustering the vectors&apos;, the disambiguation is performed by selecting the sense centroid closest to the test word vector. Pedersen and Bruce (1998) use an EMbased algorithm to group sentences containing the target word into unlabeled clusters which are then mapped to sense tags. The existence of the freely available word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed corpora, partially hand-labeled with senses from WordNet (Miller, 1995), to train a selectional preference system, and McCarthy et al. (2001), which uses a selectional preference model similar to Resnik (1997), but without the use of any labeled data, in the &amp;quot;one sense per discourse&amp;quot; paradigm (Gale et al., 1992), achieving good precision in the SENSEVAL2 English all-words task. Using a crosslingual approach, Rigau et al. (1997) investigates an unsupervised system using a combination of monolingual dictionaries, bilingual dictionaries and the English WordNet. The </context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>P. Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics, pages 52-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rigau</author>
<author>J Atserias</author>
<author>E Agirre</author>
</authors>
<title>Combining unsupervised lexical knowledge methods for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL &apos;97,</booktitle>
<pages>48--55</pages>
<contexts>
<context position="2873" citStr="Rigau et al. (1997)" startWordPosition="418" endWordPosition="421">vailable word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed corpora, partially hand-labeled with senses from WordNet (Miller, 1995), to train a selectional preference system, and McCarthy et al. (2001), which uses a selectional preference model similar to Resnik (1997), but without the use of any labeled data, in the &amp;quot;one sense per discourse&amp;quot; paradigm (Gale et al., 1992), achieving good precision in the SENSEVAL2 English all-words task. Using a crosslingual approach, Rigau et al. (1997) investigates an unsupervised system using a combination of monolingual dictionaries, bilingual dictionaries and the English WordNet. The bilingual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet. In an approach similar in spirit to the one presented here, Mihalcea and Moldovan (1999) use the entire English WordNet to find sentences in online texts containing high-confidence examples of the target word. The WordNet glosses and acquired sentences are used as training data to automatically create a la</context>
</contexts>
<marker>Rigau, Atserias, Agirre, 1997</marker>
<rawString>G. Rigau, J. Atserias, and E. Agirre. 1997. Combining unsupervised lexical knowledge methods for word sense disambiguation. In Proceedings of ACL &apos;97, pages 48-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Roventini</author>
<author>A Alonge</author>
<author>F Bertagna</author>
<author>B Magnini</author>
<author>N Calzolari</author>
</authors>
<title>ItalWordNet: a large semantic database for Italian.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC-2000,</booktitle>
<pages>783--790</pages>
<contexts>
<context position="6839" citStr="Roventini et al., 2000" startWordPosition="1068" endWordPosition="1071">and 500 verbs5 in proportion to their token frequency in the unannotated corpus and had them handchecked by a native speaker. Table 1 presents the POS and lemmatization accuracy for these 1500 words; the lemmatization accuracy is reported only on examples which were correctly labeled by the POS tagger. 3 Information Sources 3.1 Italian WordNet Since no training data was available for this task, we rely on alternative sources of information for inducing sense classification. One central resource in this process is the ItalWordNet, version 1.0, developed by the Italian National ProJect, SI-TAL (Roventini et al., 2000), which was provided with the data. This structure describes various semantic relationships between words (usually binary relations), including: • synonymy — word a is a synonym of word v if word a has nearly the same definition as word v. • antonymy — word a is an antonym of word v if words a and v have nearly opposite meanings. • hyperonymy - word a is a hyperonym of word v if a is a generalization of word v; • hyponymy — the reverse of the hyperonymy relation; • meronymy — word a is a meronym of word v if the obJect represented by a has the obJect represented by v as a part (for instance, c</context>
</contexts>
<marker>Roventini, Alonge, Bertagna, Magnini, Calzolari, 2000</marker>
<rawString>A. Roventini, A. Alonge, F. Bertagna, B. Magnini, and N. Calzolari. 2000. ItalWordNet: a large semantic database for Italian. In Proceedings of LREC-2000, pages 783-790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="5046" citStr="Schmid (1994)" startWordPosition="776" endWordPosition="777">ddition, local n-grams around the ambiguous word are also part of the document&apos;s vector: d =(dl ... d1v1), dj = c�NWS where c� is the number of times the feature j3 appears in document d, N is the number of words in d and Wj is a weight associated with feature j4. Confusion between the same word participating in multiple feature roles is avoided by appending the feature values with their positional type (e.g. uomo_L is different from uomo in unmarked bag-of-words context). All test documents were part-of-speech tagged using the Italian version of the decision treebased POS tagger described in Schmid (1994). Extracting lemma information from Italian is an important process — Section 6 evaluates a scenario where lemmatization is not used, and shows that a substantial decrease in performance occurs. Lemmatization is performed using the supervised morphological analyzer from Throughout the paper, we will use the word document to denote the actual context where the ambiguous word appears. 3A feature can be a word, a lemma or a local ngram; the model uses either words or lemmas, but never both. 4The weight Wj depends on the type of the feature fj: for the bag-of-word features, this weight is inversel</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="1842" citStr="Schutze (1998)" startWordPosition="260" endWordPosition="261">statistics. The combining system reduces the word sense error rate by 8.2% absolute (13.6% relative error reduction), when compared to the best system submitted in SENSEVAL2. 1.1 Previous Work Several approaches that address the problem of unsupervised word sense disambiguation (WSD henceforth) have been presented in the past few years. In one of the most widely-cited unsupervised WSD systems, Yarowsky (1995) uses a very small seed set (2-3 examples) to bootstrap a WSD algorithm based on decision lists; the algorithm yields highly accurate results, competitive with similar supervised systems. Schutze (1998) creates word vectors by extracting ambiguous words and their contexts from an unlabeled corpus. After clustering the vectors&apos;, the disambiguation is performed by selecting the sense centroid closest to the test word vector. Pedersen and Bruce (1998) use an EMbased algorithm to group sentences containing the target word into unlabeled clusters which are then mapped to sense tags. The existence of the freely available word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed </context>
<context position="3629" citStr="Schutze (1998)" startWordPosition="542" endWordPosition="543">ngual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet. In an approach similar in spirit to the one presented here, Mihalcea and Moldovan (1999) use the entire English WordNet to find sentences in online texts containing high-confidence examples of the target word. The WordNet glosses and acquired sentences are used as training data to automatically create a large sense-tagged corpus. In work on the same dataset as used in this research, Magnini et al. (2001) manually annoSimilar to Yarowsky (1995), Schutze (1998) tested his algorithm only on words with two senses. tates the relevant Italian synsets with a semantic class. The test samples are assigned to one of these classes using a supervised algorithm trained on an annotated English corpus and then these classes were mapped back to the WordNet synsets. Unsupervised word sense disambiguation using WordNet relations also has also been used in real-world tasks. Some of the earlier examples include Voorhees (1993) (using the hyponym/hypernym relations from WordNet) and Sussna (1993) (using weighted relations derived from WordNet), employing word sense di</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>H. Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>Y Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--3</pages>
<contexts>
<context position="16606" citStr="Stevenson and Wilks, 2001" startWordPosition="2817" endWordPosition="2820">e useful for a classification task to have access to multiple information sources; it follows from a standard argument that the information one has about a process (as measured by the entropy of the process) can only decrease if one obtains additional information: H (AIB) G H (A) Several studies in the machine learning community have shown that combining the information obtained from several classifiers not only results in improved performance, but also improves robustness. Even in cases where one has access to several structurally different information sources that are not easily integrable (Stevenson and Wilks, 2001), it might be more beneficial to construct separate classifiers for each type of context and combine their outputs, than construct a more complex classifier that tries to handle the combination internally. Aside from the output of the classifier described in Section 4, we have access to two other information sources: • the English distributional usage of the translation of a particular Italian word sense; • the output of another word sense disambiguation system (Magnini et al., 2001) (downloadable from the SENSEVAL2 web site). In using the English distributional data, we make the following ass</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>M. Stevenson and Y. Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27(3):321-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sussna</author>
</authors>
<title>Word sense disambiguation for free-text indexing using a massive semantic network.</title>
<date>1993</date>
<booktitle>In Proceedings of CIKM &apos;93.</booktitle>
<contexts>
<context position="4156" citStr="Sussna (1993)" startWordPosition="627" endWordPosition="628">esearch, Magnini et al. (2001) manually annoSimilar to Yarowsky (1995), Schutze (1998) tested his algorithm only on words with two senses. tates the relevant Italian synsets with a semantic class. The test samples are assigned to one of these classes using a supervised algorithm trained on an annotated English corpus and then these classes were mapped back to the WordNet synsets. Unsupervised word sense disambiguation using WordNet relations also has also been used in real-world tasks. Some of the earlier examples include Voorhees (1993) (using the hyponym/hypernym relations from WordNet) and Sussna (1993) (using weighted relations derived from WordNet), employing word sense disambiguation to increase the performance of information retrieval systems. 2 Feature Representation In the model presented in this research, documents� are represented as bags of words and/or lemmas; in addition, local n-grams around the ambiguous word are also part of the document&apos;s vector: d =(dl ... d1v1), dj = c�NWS where c� is the number of times the feature j3 appears in document d, N is the number of words in d and Wj is a weight associated with feature j4. Confusion between the same word participating in multiple </context>
</contexts>
<marker>Sussna, 1993</marker>
<rawString>M. Sussna. 1993. Word sense disambiguation for free-text indexing using a massive semantic network. In Proceedings of CIKM &apos;93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL&apos;98,</booktitle>
<pages>491--497</pages>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLINGACL&apos;98, pages 491-497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Using WordNet to disambiguate word senses for text retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of SIGIR &apos;93,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="4086" citStr="Voorhees (1993)" startWordPosition="617" endWordPosition="618">large sense-tagged corpus. In work on the same dataset as used in this research, Magnini et al. (2001) manually annoSimilar to Yarowsky (1995), Schutze (1998) tested his algorithm only on words with two senses. tates the relevant Italian synsets with a semantic class. The test samples are assigned to one of these classes using a supervised algorithm trained on an annotated English corpus and then these classes were mapped back to the WordNet synsets. Unsupervised word sense disambiguation using WordNet relations also has also been used in real-world tasks. Some of the earlier examples include Voorhees (1993) (using the hyponym/hypernym relations from WordNet) and Sussna (1993) (using weighted relations derived from WordNet), employing word sense disambiguation to increase the performance of information retrieval systems. 2 Feature Representation In the model presented in this research, documents� are represented as bags of words and/or lemmas; in addition, local n-grams around the ambiguous word are also part of the document&apos;s vector: d =(dl ... d1v1), dj = c�NWS where c� is the number of times the feature j3 appears in document d, N is the number of words in d and Wj is a weight associated with </context>
</contexts>
<marker>Voorhees, 1993</marker>
<rawString>E. Voorhees. 1993. Using WordNet to disambiguate word senses for text retrieval. In Proceedings of SIGIR &apos;93, pages 171-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL-2000,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="6063" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="944" endWordPosition="947">3A feature can be a word, a lemma or a local ngram; the model uses either words or lemmas, but never both. 4The weight Wj depends on the type of the feature fj: for the bag-of-word features, this weight is inversely proportional to the distance between the target word and the feature, while for extended ngram features it is a empirically estimated weight (same value used in a similar English sense classifier). morph. analyzer POS tagger token type token type verbs 98.1% 99.0% 99.9% 99.4% nouns 99.5% 98.7% 96.2% 94.8% adJs 96.7% 99.6% 85.3% 98.0% Table 1: Lemmatization and POS tagging accuracy Yarowsky and Wicentowski (2000), trained on the filtered output of the POS tagger. We tested the accuracy of the morphological analyzer by randomly selecting 500 adJectives, 500 nouns and 500 verbs5 in proportion to their token frequency in the unannotated corpus and had them handchecked by a native speaker. Table 1 presents the POS and lemmatization accuracy for these 1500 words; the lemmatization accuracy is reported only on examples which were correctly labeled by the POS tagger. 3 Information Sources 3.1 Italian WordNet Since no training data was available for this task, we rely on alternative sources of information for</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>D. Yarowsky and R. Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of ACL-2000, pages 207-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>S Cucerzan</author>
<author>R Florian</author>
<author>C Schafer</author>
<author>R Wicentowski</author>
</authors>
<title>The Johns Hopkins SENSEVAL2 system descriptions.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL2.</booktitle>
<contexts>
<context position="1124" citStr="Yarowsky et al., 2001" startWordPosition="151" endWordPosition="154">which combines multiple information sources, including semantic relations, large unlabeled corpora, and cross-lingual distributional statistics. This method extends and builds on the JHU system that participated in the SENSEVAL2 exercise. Experiments performed on the SENSEVAL2 Italian lexical-sample data show significant improvements over previously published results on this data set. 1 Introduction The goal of this paper is to present an unsupervised word sense disambiguation system which extends the JHU system for the Italian lexical sample task which participated in the SENSEVAL2 exercise (Yarowsky et al., 2001). Our system combines word semantic relations, large unlabeled corpora and cross-lingual distributional statistics. The combining system reduces the word sense error rate by 8.2% absolute (13.6% relative error reduction), when compared to the best system submitted in SENSEVAL2. 1.1 Previous Work Several approaches that address the problem of unsupervised word sense disambiguation (WSD henceforth) have been presented in the past few years. In one of the most widely-cited unsupervised WSD systems, Yarowsky (1995) uses a very small seed set (2-3 examples) to bootstrap a WSD algorithm based on dec</context>
<context position="17589" citStr="Yarowsky et al., 2001" startWordPosition="2982" endWordPosition="2985">translation of a particular Italian word sense; • the output of another word sense disambiguation system (Magnini et al., 2001) (downloadable from the SENSEVAL2 web site). In using the English distributional data, we make the following assumption: Assumption 2 If an Italian word sense sI has an English translation in sense sE, then the usage of sense sE in English is characteristic of the usage of sense sI in Italian. Given N classifiers (possibly having probabilistic output), an easy and effective way of combining their output is through voting (Brill and Wu, 1998; van Halteren et al., 1998; Yarowsky et al., 2001), by computing the output classification as s� = arg max , ~ i where si (d) = arg max, Pi (sId). In other words, each classifier votes for the sense which it considers most likely, weighted by the probability of that sense. In the end, the sense that has been voted the most winsil. 6 Experimental Evaluation The test data in the Italian lexical-sample task consists of 3889 contexts of 1 to 3 sentences, for 83 ambiguous words. 6.1 Influence o f Morphological Analysis on Performance To investigate the impact of using the morphological analyzer, we created a second set Ek derived from an unlemmati</context>
</contexts>
<marker>Yarowsky, Cucerzan, Florian, Schafer, Wicentowski, 2001</marker>
<rawString>D. Yarowsky, S. Cucerzan, R. Florian, C. Schafer, and R. Wicentowski. 2001. The Johns Hopkins SENSEVAL2 system descriptions. In Proceedings of SENSEVAL2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1640" citStr="Yarowsky (1995)" startWordPosition="228" endWordPosition="229">he Italian lexical sample task which participated in the SENSEVAL2 exercise (Yarowsky et al., 2001). Our system combines word semantic relations, large unlabeled corpora and cross-lingual distributional statistics. The combining system reduces the word sense error rate by 8.2% absolute (13.6% relative error reduction), when compared to the best system submitted in SENSEVAL2. 1.1 Previous Work Several approaches that address the problem of unsupervised word sense disambiguation (WSD henceforth) have been presented in the past few years. In one of the most widely-cited unsupervised WSD systems, Yarowsky (1995) uses a very small seed set (2-3 examples) to bootstrap a WSD algorithm based on decision lists; the algorithm yields highly accurate results, competitive with similar supervised systems. Schutze (1998) creates word vectors by extracting ambiguous words and their contexts from an unlabeled corpus. After clustering the vectors&apos;, the disambiguation is performed by selecting the sense centroid closest to the test word vector. Pedersen and Bruce (1998) use an EMbased algorithm to group sentences containing the target word into unlabeled clusters which are then mapped to sense tags. The existence o</context>
<context position="3613" citStr="Yarowsky (1995)" startWordPosition="540" endWordPosition="541">WordNet. The bilingual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet. In an approach similar in spirit to the one presented here, Mihalcea and Moldovan (1999) use the entire English WordNet to find sentences in online texts containing high-confidence examples of the target word. The WordNet glosses and acquired sentences are used as training data to automatically create a large sense-tagged corpus. In work on the same dataset as used in this research, Magnini et al. (2001) manually annoSimilar to Yarowsky (1995), Schutze (1998) tested his algorithm only on words with two senses. tates the relevant Italian synsets with a semantic class. The test samples are assigned to one of these classes using a supervised algorithm trained on an annotated English corpus and then these classes were mapped back to the WordNet synsets. Unsupervised word sense disambiguation using WordNet relations also has also been used in real-world tasks. Some of the earlier examples include Voorhees (1993) (using the hyponym/hypernym relations from WordNet) and Sussna (1993) (using weighted relations derived from WordNet), employi</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>