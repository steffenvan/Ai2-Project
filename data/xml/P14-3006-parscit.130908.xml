<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002272">
<title confidence="0.991363">
An Exploration of Embeddings for Generalized Phrases
</title>
<author confidence="0.995917">
Wenpeng Yin and Hinrich Sch¨utze
</author>
<affiliation confidence="0.9983395">
Center for Information and Language Processing
University of Munich, Germany
</affiliation>
<email confidence="0.972846">
wenpeng@cis.lmu.de
</email>
<sectionHeader confidence="0.997101" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998828">
Deep learning embeddings have been suc-
cessfully used for many natural language
processing problems. Embeddings are
mostly computed for word forms although
lots of recent papers have extended this to
other linguistic units like morphemes and
word sequences. In this paper, we define
the concept of generalized phrase that in-
cludes conventional linguistic phrases as
well as skip-bigrams. We compute em-
beddings for generalized phrases and show
in experimental evaluations on corefer-
ence resolution and paraphrase identifica-
tion that such embeddings perform better
than word form embeddings.
</bodyText>
<sectionHeader confidence="0.993108" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999945421052632">
One advantage of recent work in deep learning on
natural language processing (NLP) is that linguis-
tic units are represented by rich and informative
embeddings. These embeddings support better
performance on a variety of NLP tasks (Collobert
et al., 2011) than symbolic linguistic representa-
tions that do not directly represent information
about similarity and other linguistic properties.
Embeddings are mostly derived for word forms al-
though a number of recent papers have extended
this to other linguistic units like morphemes (Lu-
ong et al., 2013), phrases and word sequences
(Socher et al., 2010; Mikolov et al., 2013).1 Thus,
an important question is: what are the basic lin-
guistic units that should be represented by embed-
dings in a deep learning NLP system? Building
on the prior work in (Socher et al., 2010; Mikolov
et al., 2013), we generalize the notion of phrase to
include skip-bigrams (SkipBs) and lexicon entries,
</bodyText>
<footnote confidence="0.963873333333333">
1Socher et al. use the term “word sequence”. Mikolov et
al. use the term “phrase” for word sequences that are mostly
frequent continuous collocations.
</footnote>
<bodyText confidence="0.999950341463415">
where lexicon entries can be both “continuous”
and “noncontinuous” linguistic phrases. Exam-
ples of skip-bigrams at distance 2 in the sentence
“this tea helped me to relax” are: “this*helped”,
“tea*me”, “helped*to” ... Examples of linguistic
phrases listed in a typical lexicon are continuous
phrases like “cold cuts” and “White House” that
only occur without intervening words and discon-
tinous phrases like “take over” and “turn off” that
can occur with intervening words. We consider
it promising to compute embeddings for these
phrases because many phrases, including the four
examples we just gave, are noncompositional or
weakly compositional, i.e., it is difficult to com-
pute the meaning of the phrase from the meaning
of its parts. We write gaps as “*” for SkipBs and
“ ” for phrases.
We can approach the question of what basic
linguistic units should have representations from
a practical as well as from a cognitive point of
view. In practical terms, we want representations
to be optimized for good generalization. There
are many situations where a particular task involv-
ing a word cannot be solved based on the word
itself, but it can be solved by analyzing the con-
text of the word. For example, if a coreference
resolution system needs to determine whether the
unknown word “Xiulan” (a Chinese first name)
in “he helped Xiulan to find a flat” refers to an
animate or an inanimate entity, then the SkipB
“helped*to” is a good indicator for the animacy of
the unknown word – whereas the unknown word
itself provides no clue.
From a cognitive point of view, it can be argued
that many basic units that the human cognitive sys-
tem uses have multiple words. Particularly con-
vincing examples for such units are phrasal verbs
in English, which often have a non-compositional
meaning. It is implausible to suppose that we
retrieve atomic representations for, say, “keep”,
“up”, “on” and “from” and then combine them to
</bodyText>
<page confidence="0.991185">
41
</page>
<subsubsectionHeader confidence="0.271954">
Proceedings of the ACL 2014 Student Research Workshop, pages 41–47,
</subsubsectionHeader>
<bodyText confidence="0.986921945945946">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
form the meanings of the expressions “keep your
head up,” “keep the pressure on,” “keep him from
laughing”. Rather, it is more plausible that we rec-
ognize “keep up”, “keep on” and “keep from” as
relevant basic linguistic units in these contexts and
that the human cognitive systems represents them
as units.
We can view SkipBs and discontinuous phrases
as extreme cases of treating two words that do not
occur next to each other as a unit. SkipBs are de-
fined purely statistically and we will consider any
pair of words as a potential SkipB in our exper-
iments below. In contrast, discontinuous phrases
are well motivated. It is clear that the words
“picked” and “up” in the sentences “I picked it
up” belong together and form a unit very similar to
the word “collected” in “I collected it”. The most
useful definition of discontinuous units probably
lies in between SkipBs and phrases: we definitely
want to include all phrases, but also some (but not
all) statistical SkipBs. The initial work presented
in this paper may help in finding a good “compro-
mise” definition.
This paper contributes to a preliminary inves-
tigation of generalized phrase embeddings and
shows that they are better suited than word em-
bedding for a coreference resolution classification
task and for paraphrase identification. Another
contribution lies in that the phrase embeddings we
release2 could be a valuable resource for others.
The remainder of this paper is organized as fol-
lows. Section 2 and Section 3 introduce how to
learn embeddings for SkipBs and phrases, respec-
tively. Experiments are provided in Section 4.
Subsequently, we analyze related work in Section
5, and conclude our work in Section 6.
</bodyText>
<sectionHeader confidence="0.876473" genericHeader="introduction">
2 Embedding learning for SkipBs
</sectionHeader>
<bodyText confidence="0.9980796">
With English Gigaword Corpus (Parker et al.,
2009), we use the skip-gram model as imple-
mented in word2vec3 (Mikolov et al., 2013) to in-
duce embeddings. Word2vec skip-gram scheme is
a neural network language model, using a given
word to predict its context words within a window
size. To be able to use word2vec directly with-
out code changes, we represent the corpus as a
sequence of sentences, each consisting of two to-
kens: a SkipB and a word that occurs between the
</bodyText>
<footnote confidence="0.999796666666667">
2http://www.cis.lmu.de/pub/
phraseEmbedding.txt.bz2
3https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.979030888888889">
two enclosing words of the SkipB. The distance
k between the two enclosing words can be var-
ied. In our experiments, we use either distance
k = 2 or distance 2 ≤ k ≤ 3. For example, for
k = 2, the trigram wi−1 wi wi+1 generates the sin-
gle sentence “wi−1*wi+1 wi”; and for 2 ≤ k ≤ 3,
the fourgram wi−2 wi−1 wi wi+1 generates the
four sentences “wi−2*wi wi−1”, “wi−1*wi+1 wi”,
“wi−2*wi+1 wi−1” and “wi−2*wi+1 wi”.
In this setup, the middle context of SkipBs are
kept (i.e., the second token in the new sentences),
and the surrounding context of words of original
sentences are also kept (i.e., the SkipB in the new
sentences). We can run word2vec without any
changes on the reformatted corpus to learn embed-
dings for SkipBs. As a baseline, we run word2vec
on the original corpus to compute embeddings for
words. Embedding size is set to 200.
</bodyText>
<sectionHeader confidence="0.96318" genericHeader="method">
3 Embedding learning for phrases
</sectionHeader>
<subsectionHeader confidence="0.997557">
3.1 Phrase collection
</subsectionHeader>
<bodyText confidence="0.999998">
Phrases defined by a lexicon have not been deeply
investigated before in deep learning. To collect
canonical phrase set, we extract two-word phrases
defined in Wiktionary4, and two-word phrases de-
fined in Wordnet (Miller and Fellbaum, 1998) to
form a collection of size 95218. This collection
contains phrases whose parts always occur next to
each other (e.g., “cold cuts”) and phrases whose
parts more often occur separated from each other
(e.g., “take (something) apart”).
</bodyText>
<subsectionHeader confidence="0.999893">
3.2 Identification of phrase continuity
</subsectionHeader>
<bodyText confidence="0.982643875">
Wiktionary and WordNet do not categorize
phrases as continuous or discontinous. So we need
a heuristic for determining this automatically.
For each phrase “A B”, we compute
[c1, c2, c3, c4, c5] where ci,1 ≤ i ≤ 5, indi-
cates there are ci occurrences of A and B in that
order with a distance of i. We compute these
statistics for a corpus consisting of Gigaword
and Wikipedia. We set the maximal distance
to 5 because discontinuous phrases are rarely
separated by more than 5 tokens.
If c1 is 10 times higher than (c2+c3+c4+c5)/4,
we classify “A B” as continuous, otherwise as dis-
continuous. Taking phrase “pick off” as an ex-
ample, it gets vector [1121, 632, 337, 348, 4052],
c1 (1121) is smaller than the average 1342.25, so
</bodyText>
<footnote confidence="0.9589635">
4http://en.wiktionary.org/wiki/
Wiktionary:Main_Page
</footnote>
<page confidence="0.998787">
42
</page>
<bodyText confidence="0.99892225">
“pick off” is set as “discontinuous”. Further con-
sider “Cornell University” which gets [14831, 16,
177, 331, 3471], satisfying above condition, hence
it is treated as a continuous phrase.
</bodyText>
<subsectionHeader confidence="0.99979">
3.3 Sentence reformatting
</subsectionHeader>
<bodyText confidence="0.993496666666667">
Given the continuity information of phrases,
sentence “· · · A · · · B · · · ” is reformated into
“···A B · · · A B · · · ” if “A B” is a discontinu-
ous phrase and is separated by maximal 4 words,
and sentence “· · · AB · · · ” into “· · · A B · · · ” if
“A B” is a continuous phrase.
In the first case, we use phrase “A B” to replace
each of its component words for the purpose of
making the context of both constituents available
to the phrase in learning. For the second situation,
it is natural to combine the two words directly to
form an independent semantic unit.
Word2vec is run on the reformatted corpus to
learn embeddings for both words and phrases.
Embedding size is also set to 200.
</bodyText>
<subsectionHeader confidence="0.999129">
3.4 Examples of phrase neighbors
</subsectionHeader>
<bodyText confidence="0.999986916666667">
Usually, compositional methods for learning rep-
resentations of multi-word text suffer from the dif-
ficulty in integrating word form representations,
like word embeddings. To our knowledge, there is
no released embeddings which can directly facil-
itate measuring the semantic affinity between lin-
guistic units of arbitrary lengths. Table 1 attempts
to provide some nearest neighbors for given typ-
ical phrases to show the promising perspective
of our work. Note that discontinuous phrases
like “turn off” have plausible single word nearest
neighbors like “unplug”.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999983714285714">
Our motivation for generalized phrases in Sec-
tion 1 was that they can be used to infer the at-
tributes of the context they enclose and that they
can capture non-compositional semantics. Our hy-
pothesis was that they are more suitable for this
than word embeddings. In this section we carry
out two experiments to test this hypothesis.
</bodyText>
<subsectionHeader confidence="0.998613">
4.1 Animacy classification for markables
</subsectionHeader>
<bodyText confidence="0.999987">
A markable in coreference resolution is a linguis-
tic expression that refers to an entity in the real
world or another linguistic expression. Examples
of markables include noun phrases (“the man”),
named entities (“Peter”) and nested nominal ex-
pressions (“their”). We address the task of ani-
macy classification of markables: classifying them
as animate/inanimate. This feature is useful for
coreference resolution systems because only ani-
mate markables can be referred to using masculine
and feminine pronouns in English like “him” and
“she”. Thus, this is an important clue for automat-
ically clustering the markables of a document into
correct coreference chains.
To create training and test sets, we extract all
39,689 coreference chains from the CoNLL2012
OntoNotes corpus.5 We label chains that con-
tain an animate pronoun markable (“she”, “her”,
“he”, “him” or “his”) and no inanimate pronoun
markable (“it” or “its”) as animate; and chains
that contain an inanimate pronoun markable and
no animate pronoun markable as inanimate. Other
chains are discarded.
We extract 39,942 markables and their contexts
from the 10,361 animate and inanimate chains.
The context of a markable is represented as a
SkipB: it is simply the pair of the two words occur-
ring to the left and right of the markable. The gold
label of a markable and its SkipB is the animacy
status of its chain: either animate or inanimate. We
divide all SkipBs having received an embedding in
the embedding learning phase into a training set of
11,301 (8097 animate, 3204 inanimate) and a bal-
anced test set of 4036.
We use LIBLINEAR (Fan et al., 2008) for clas-
sification, with penalty factors 3 and 1 for inan-
imate and animate classes, respectively, because
the training data are unbalanced.
</bodyText>
<subsectionHeader confidence="0.632059">
4.1.1 Experimental results
</subsectionHeader>
<bodyText confidence="0.999992">
We compare the following representations for an-
imacy classification of markables. (i) Phrase em-
bedding: Skip-bigram embeddings with skip dis-
tance k = 2 and 2 &lt; k &lt; 3; (ii) Word em-
bedding: concatenation of the embeddings of the
two enclosing words where the embeddings are
either standard word2vec embeddings (see Sec-
tion 2) or the embeddings published by (Collobert
et al., 2011);6 (iii) the one-hot vector representa-
tion of a SkipB: the concatentation of two one-hot
vectors of dimensionality V where V is the size
of the vocabulary. The first (resp. second) vector
</bodyText>
<footnote confidence="0.9993645">
5http://conll.cemantix.org/2012/data.
html
6http://metaoptimize.com/projects/
wordreprs/
</footnote>
<page confidence="0.997784">
43
</page>
<table confidence="0.99847175">
turn off caught up take over macular degeneration telephone interview
switch off mixed up take charge eye disease statement
unplug entangled replace diabetic retinopathy interview
turning off involved take control cataracts conference call
shut off enmeshed stay on periodontal disease teleconference
block out tangled retire epilepsy telephone call
turned off mired succeed glaucoma told
fiddle with engaged step down skin cancer said
</table>
<tableCaption confidence="0.9836446">
Table 1: Phrases and their nearest neighbors
is the one-hot vector for the left (resp. right) word
of the SkipB. Experimental results are shown in
Table 2.
Table 2: Classification accuracy. Mark “*” means
</tableCaption>
<bodyText confidence="0.991345576923077">
significantly lower than “phrase embedding”, k =
2; “†” means significantly lower than “phrase em-
bedding”, 2 &lt; k &lt; 3. As significance test, we use
the test of equal proportion, p &lt; .05, throughout.
The results show that phrase embeddings have
an obvious advantage in this classification task,
both for k = 2 and 2 &lt; k &lt; 3. This validates
our hypothesis that learning embeddings for dis-
continuous linguistic units is promising.
In our error analysis, we found two types of
frequent errors. (i) Unspecific SkipBs. Many
SkipBs are equally appropriate for animate and
inanimate markables. Examples of such SkipBs
include “take*in” and “then*goes”. (ii) Untypical
use of specific SkipBs. Even SkipBs that are spe-
cific with respect to what type of markable they
enclose sometimes occur with the “wrong” type
of markable. For example, most markables oc-
curring in the SkipB “of*whose” are animate be-
cause “whose” usually refers to an animate mark-
able. However, in the context “...the southeast-
ern area of Fujian whose economy is the most ac-
tive” the enclosed markable is Fujian, a province
of China. This example shows that “whose” occa-
sionally refers to an inanimate entity even though
these cases are infrequent.
</bodyText>
<subsubsectionHeader confidence="0.487689">
4.1.2 Nearest neighbors of SkipBs
</subsubsectionHeader>
<bodyText confidence="0.997851545454545">
Table 3 shows some SkipBs and their nearest
neighbors in descending order, where similarity is
computed with cosine measure.
A general phenomenon is that phrase embed-
dings capture high degree of consistency in infer-
ring the attributes of enclosed words. Considering
the neighbor list in the first column, we can esti-
mate that a verb probably appears as the middle
token. Furthermore, noun, pronoun, adjective and
adverb can roughly be inferred for the remaining
columns, respectively.7
</bodyText>
<subsectionHeader confidence="0.982024">
4.2 Paraphrase identification task
</subsectionHeader>
<bodyText confidence="0.999986111111111">
Paraphrase identification depends on semantic
analysis. Standard approaches are unlikely to as-
sign a high similarity score to the two sentences
“he started the machine” and “he turned the ma-
chine on”. In our approach, embedding of the
phrase “turned on” can greatly help us to infer cor-
rectly that the sentences are paraphrases. Hence,
phrase embeddings and in particular embeddings
of discontinuous phrases seem promising in para-
phrase detection task.
We use the Microsoft Paraphrase Corpus (Dolan
et al., 2004) for evaluation. It consists of a training
set with 2753 true paraphrase pairs and 1323 false
paraphrase pairs, along with a test set with 1147
true and 578 false pairs. After discarding pairs
in which neither sentence contains phrases, 3027
training pairs (2123 true vs. 904 false) and 1273
test pairs (871 true vs. 402 false) remain.
</bodyText>
<footnote confidence="0.9840665">
7A reviewer points out that this is only a suggestive anal-
ysis and that corpus statistics about these contexts would be
required to establish that phrase embeddings can predict part-
of-speech with high accuracy.
</footnote>
<figure confidence="0.95850975">
representation accuracy
phrase embedding
word embedding
k = 2
2 &lt; k &lt; 3
word2vec
Collobert et al.
0.703
0.700
0.668*†
0.662*†
one-hot vectors 0.638*†
</figure>
<page confidence="0.995963">
44
</page>
<table confidence="0.99747825">
who*afghanistan, some*told women*have with*responsibility he*worried
had*afghanistan other*told men*have of*responsibility she*worried
he*afghanistan two*told children*have and*responsibility was*worried
who*iraq –*told girls*have “*responsibility is*worried
have*afghanistan but*told parents*have that*responsibility said*worried
fighters*afghanistan one*told students*have ’s*responsibility that*worried
who*kosovo because*told young*have the* responsibility they*worried
was*afghanistan and*told people*have for*responsibility ’s*worried
</table>
<tableCaption confidence="0.99979">
Table 3: SkipBs and their nearest neighbors
</tableCaption>
<bodyText confidence="0.999987083333333">
We tackle the paraphrase identification task via
supervised binary classification. Sentence repre-
sentation equals to the addition over all the to-
ken embeddings (words as well as phrases). A
slight difference is that when dealing with a sen-
tence like “· · · A B · · · A B · · · ” we only consider
“A B” embedding once. The system “word em-
bedding” is based on the embeddings of single
words only. Subsequently, pair representation is
derived by concatenating the two sentence vectors.
This concatentation is then classified by LIBLIN-
EAR as “paraphrase” or “no paraphrase”.
</bodyText>
<subsectionHeader confidence="0.915776">
4.2.1 Experimental results and analysis
</subsectionHeader>
<bodyText confidence="0.9584175">
Table 4 shows the performance of two methods.
Phrase embeddings are apparently better. Most
work on paraphrase detection has devised intri-
cate features and achieves performance numbers
higher than what we report here (Ji and Eisenstein,
2013; Madnani et al., 2012; Blacoe and Lapata,
2012). Our objective is only to demonstrate the
superiority of considering phrase embedding over
merely word embedding in this standard task.
We are interested in how phrase embeddings
make an impact on this task. To that end, we per-
form an analysis on test examples where word em-
beddings are better than phrase embeddings and
vice versa.
Table 5 shows four pairs, of which “phrase em-
bedding” outperforms “word embedding” in the
</bodyText>
<tableCaption confidence="0.986614">
Table 4: Paraphrase task results.
</tableCaption>
<bodyText confidence="0.999979351351352">
first two examples, “word embedding” defeats
“phrase embedding” in the last two examples. In
the first pair, successful phrase detection enables
to split sentences into better units, thus the gener-
ated representation can convey the sentence mean-
ing more exactly.
The meaning difference in the second pair orig-
inates from the synonym substitution between
“take over as chief financial officer” and “fill
the position”. The embedding of the phrase
“take over” matches the embedding of the single
word “fill” in this context.
“Phrase embedding” in the third pair suffers
from wrong phrase detection. Actually, “in” and
“on” can not be treated as a sound phrase in that
situation even though “in on” is defined by Wik-
tionary. Indeed, this failure, to some extent, re-
sults from the shortcomings of our method in dis-
covering true phrases. Furthermore, figuring out
whether two words are a phrase might need to
analyse syntactic structure in depth. This work is
directly based on naive intuitive knowledge, acting
as an initial exploration. Profound investigation is
left as future work.
Our implementation discovers the contained
phrases in the fourth pair perfectly. Yet, “word em-
bedding” defeats “phrase embedding” still. The
pair is not a paraphrase partly because the numbers
are different; e.g., there is a big difference between
“5.8 basis points” and “50 basis points”. Only a
method that can correctly treat numerical informa-
tion can succeed here. However, the appearance of
phrases “central bank”, “interest rates” and “ba-
sis points” makes the non-numerical parts more
expressive and informative, leading to less dom-
inant for digital quantifications. On the contrary,
though “word embedding” fails to split the sen-
</bodyText>
<figure confidence="0.8354955">
Methods
Accuracy F1
baseline
word embedding
phrase embedding
0.684 0.803
0.695 0.805
0.713 0.812
</figure>
<page confidence="0.994406">
45
</page>
<table confidence="0.540001">
GWP sentence 1 sentence 2
1 0 1 Common side effects include
</table>
<bodyText confidence="0.98509835483871">
nasal congestion, runny nose, sore throat
and cough, the FDA said.
1 0 1 Douglas Robinson, a senior vice president
of finance, will take over as chief financial
officer on an interim basis .
The most common side effects after get-
ting the nasal spray were nasal congestion,
runny nose, sore throat and cough.
Douglas Robinson, CA senior
vice president, finance, will fill the
position in the interim.
1 1 0 They were being held Sunday in the Camden
County Jail on $ 100,000 bail each .
0 0 1 The interest rate sensitive two year Schatz
yield was down 5.8 basis points at 1.99 per-
cent .
The Jacksons remained in on Camden
County jail $ 100,000 bail.
The Swedish central bank cut inter-
est rates by 50 basis points to 3.0 percent
.
Table 5: Four typical sentence pairs in which the predictions of word embedding system and phrase
embedding system differ. G = gold annotation, W = prediction of word embedding system, P = prediction
of phrase embedding system. The formatting used by the system is shown. The original word order of
sentence 2 of the third pair is “· · · in Camden County jail on $ 100,000 bail”.
tences into better units, it weakens unexpectedly
the expressiveness of subordinate context. This
example demonstrates the difficulty of paraphrase
identification. Differing from simple similarity
tasks, two sentences are often not paraphrases
even though they may contain very similar words.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999969111111111">
To date, approaches to extend embedding (or
more generally “representation”) beyond individ-
ual words are either compositional or holistic
(Turney, 2012).
The best known work along the first line is by
(Socher et al., 2010; Socher et al., 2011; Socher
et al., 2012; Blacoe and Lapata, 2012), in which
distributed representations of phrases or even sen-
tences are calculated from the distributed repre-
sentations of their parts. This approach is only
plausible for units that are compositional, i.e.,
whose properties are systematically predictable
from their parts. As well, how to develop a ro-
bust composition function still faces big hurdles;
cf. Table 5.1 in (Mitchell and Lapata, 2010). Our
approach (as well as similar work on continuous
phrases) makes more sense for noncompositional
units.
Phrase representations can also be derived by
methods other than deep learning of embed-
dings, e.g., as vector space representations (Tur-
ney, 2012; Turney, 2013; Dinu et al., 2013). The
main point of this paper – generalizing phrases to
discontinuous phrases and computing representa-
tions for them – is orthogonal to this issue. It
would be interesting to evaluate other types of rep-
resentations for generalized phrases.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999934272727273">
We have argued that generalized phrases are part
of the inventory of linguistic units that we should
compute embeddings for and we have shown that
such embeddings are superior to word form em-
beddings in a coreference resolution task and stan-
dard paraphrase identification task.
In this paper we have presented initial work on
several problems that we plan to continue in the
future: (i) How should the inventory of continu-
ous and discontinous phrases be determined? We
used a purely statistical definition on the one hand
and dictionaries on the other. A combination of
the two methods would be desirable. (ii) How can
we distinguish between phrases that only occur in
continuous form and phrases that must or can oc-
cur discontinuously? (iii) Given a sentence that
contains the parts of a discontinuous phrase in cor-
rect order, how do we determine that the cooccur-
rence of the two parts constitutes an instance of
the discontinuous phrase? (iv) Which tasks benefit
most significantly from the introduction of gener-
alized phrases?
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999252333333333">
This work was funded by DFG (grant SCHU
2246/4). We thank Google for a travel grant to
support the presentation of this paper.
</bodyText>
<page confidence="0.999156">
46
</page>
<sectionHeader confidence="0.99633" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999766771084338">
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556. Association for Compu-
tational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the Workshop on Continuous Vector Space
Models and their Compositionality, pages 50–58.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th international conference
on Computational Linguistics, page 350. Associa-
tion for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Yangfeng Ji and Jacob Eisenstein. 2013. Discrimi-
native improvements to distributional sentence sim-
ilarity. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 891–896.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Conference on Computa-
tional Natural Language Learning, pages 104–113.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182–190. Asso-
ciation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. arXiv preprint arXiv:1310.4546.
George Miller and Christiane Fellbaum. 1998. Word-
net: An electronic lexical database.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388–1429.
Robert Parker, Linguistic Data Consortium, et al.
2009. English gigaword fourth edition. Linguistic
Data Consortium.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1–9.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.
Peter D Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
Peter D Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. Transactions of the Association for
Computational Linguistics, 1:353–366.
</reference>
<page confidence="0.999491">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.161549">
<title confidence="0.999876">An Exploration of Embeddings for Generalized Phrases</title>
<author confidence="0.943949">Wenpeng Yin</author>
<author confidence="0.943949">Hinrich</author>
<affiliation confidence="0.99782">Center for Information and Language University of Munich,</affiliation>
<email confidence="0.998227">wenpeng@cis.lmu.de</email>
<abstract confidence="0.995492437636762">Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although lots of recent papers have extended this to other linguistic units like morphemes and word sequences. In this paper, we define concept of phrase includes conventional linguistic phrases as well as skip-bigrams. We compute embeddings for generalized phrases and show in experimental evaluations on coreference resolution and paraphrase identification that such embeddings perform better than word form embeddings. 1 Motivation One advantage of recent work in deep learning on natural language processing (NLP) is that linguistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013), phrases and word sequences et al., 2010; Mikolov et al., Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? Building on the prior work in (Socher et al., 2010; Mikolov et al., 2013), we generalize the notion of phrase to and lexicon entries, et al. use the term “word sequence”. Mikolov et al. use the term “phrase” for word sequences that are mostly frequent continuous collocations. where lexicon entries can be both “continuous” “noncontinuous” Examples of skip-bigrams at distance 2 in the sentence “this tea helped me to relax” are: “this*helped”, “tea*me”, “helped*to” ... Examples of linguistic phrases listed in a typical lexicon are continuous phrases like “cold cuts” and “White House” that only occur without intervening words and discontinous phrases like “take over” and “turn off” that can occur with intervening words. We consider it promising to compute embeddings for these phrases because many phrases, including the four examples we just gave, are noncompositional or weakly compositional, i.e., it is difficult to compute the meaning of the phrase from the meaning of its parts. We write gaps as “*” for SkipBs and “ ” for phrases. We can approach the question of what basic linguistic units should have representations from a practical as well as from a cognitive point of view. In practical terms, we want representations to be optimized for good generalization. There are many situations where a particular task involva word cannot be solved based on word but it can be solved by analyzing conof the For example, if a coreference resolution system needs to determine whether the unknown word “Xiulan” (a Chinese first name) in “he helped Xiulan to find a flat” refers to an animate or an inanimate entity, then the SkipB “helped*to” is a good indicator for the animacy of the unknown word – whereas the unknown word itself provides no clue. From a cognitive point of view, it can be argued that many basic units that the human cognitive system uses have multiple words. Particularly convincing examples for such units are phrasal verbs in English, which often have a non-compositional meaning. It is implausible to suppose that we retrieve atomic representations for, say, “keep”, “up”, “on” and “from” and then combine them to 41 of the ACL 2014 Student Research pages 41–47, Maryland USA, June 22-27 2014. Association for Computational Linguistics form the meanings of the expressions “keep your head up,” “keep the pressure on,” “keep him from laughing”. Rather, it is more plausible that we recognize “keep up”, “keep on” and “keep from” as relevant basic linguistic units in these contexts and that the human cognitive systems represents them as units. We can view SkipBs and discontinuous phrases as extreme cases of treating two words that do not occur next to each other as a unit. SkipBs are defined purely statistically and we will consider any pair of words as a potential SkipB in our experiments below. In contrast, discontinuous phrases are well motivated. It is clear that the words “picked” and “up” in the sentences “I picked it up” belong together and form a unit very similar to the word “collected” in “I collected it”. The most useful definition of discontinuous units probably lies in between SkipBs and phrases: we definitely want to include all phrases, but also some (but not all) statistical SkipBs. The initial work presented in this paper may help in finding a good “compromise” definition. This paper contributes to a preliminary investigation of generalized phrase embeddings and shows that they are better suited than word embedding for a coreference resolution classification task and for paraphrase identification. Another contribution lies in that the phrase embeddings we could be a valuable resource for others. The remainder of this paper is organized as follows. Section 2 and Section 3 introduce how to learn embeddings for SkipBs and phrases, respectively. Experiments are provided in Section 4. Subsequently, we analyze related work in Section 5, and conclude our work in Section 6. 2 Embedding learning for SkipBs With English Gigaword Corpus (Parker et al., we use the model implein (Mikolov et al., 2013) to induce embeddings. Word2vec skip-gram scheme is a neural network language model, using a given word to predict its context words within a window size. To be able to use word2vec directly without code changes, we represent the corpus as a sequence of sentences, each consisting of two tokens: a SkipB and a word that occurs between the phraseEmbedding.txt.bz2 two enclosing words of the SkipB. The distance the two enclosing words can be varied. In our experiments, we use either distance 2 distance example, for 2, trigram the sinsentence and for fourgram the sentences and In this setup, the middle context of SkipBs are kept (i.e., the second token in the new sentences), and the surrounding context of words of original sentences are also kept (i.e., the SkipB in the new sentences). We can run word2vec without any changes on the reformatted corpus to learn embeddings for SkipBs. As a baseline, we run word2vec on the original corpus to compute embeddings for words. Embedding size is set to 200. 3 Embedding learning for phrases 3.1 Phrase collection Phrases defined by a lexicon have not been deeply investigated before in deep learning. To collect canonical phrase set, we extract two-word phrases in and two-word phrases defined in Wordnet (Miller and Fellbaum, 1998) to form a collection of size 95218. This collection contains phrases whose parts always occur next to each other (e.g., “cold cuts”) and phrases whose parts more often occur separated from each other (e.g., “take (something) apart”). 3.2 Identification of phrase continuity Wiktionary and WordNet do not categorize phrases as continuous or discontinous. So we need a heuristic for determining this automatically. For each phrase “A B”, we compute indithere are of A and B in that with a distance of We compute these statistics for a corpus consisting of Gigaword and Wikipedia. We set the maximal distance to 5 because discontinuous phrases are rarely separated by more than 5 tokens. 10 times higher than classify “A B” as otherwise as dis- Taking phrase “pick off” as an example, it gets vector [1121, 632, 337, 348, 4052], is smaller than the average 1342.25, so Wiktionary:Main_Page 42 “pick off” is set as “discontinuous”. Further consider “Cornell University” which gets [14831, 16, 177, 331, 3471], satisfying above condition, hence it is treated as a continuous phrase. 3.3 Sentence reformatting Given the continuity information of phrases, · · A · · · B · · · is reformated into B · · · A B · · · if “A B” is a discontinuous phrase and is separated by maximal 4 words, sentence · · AB · · · into · · A B · · · if “A B” is a continuous phrase. In the first case, we use phrase “A B” to replace each of its component words for the purpose of making the context of both constituents available to the phrase in learning. For the second situation, it is natural to combine the two words directly to form an independent semantic unit. Word2vec is run on the reformatted corpus to learn embeddings for both words and phrases. Embedding size is also set to 200. 3.4 Examples of phrase neighbors Usually, compositional methods for learning representations of multi-word text suffer from the difficulty in integrating word form representations, like word embeddings. To our knowledge, there is no released embeddings which can directly facilitate measuring the semantic affinity between linguistic units of arbitrary lengths. Table 1 attempts to provide some nearest neighbors for given typical phrases to show the promising perspective of our work. Note that discontinuous phrases like “turn off” have plausible single word nearest neighbors like “unplug”. 4 Experiments Our motivation for generalized phrases in Section 1 was that they can be used to infer the attributes of the context they enclose and that they can capture non-compositional semantics. Our hypothesis was that they are more suitable for this than word embeddings. In this section we carry out two experiments to test this hypothesis. 4.1 Animacy classification for markables coreference resolution is a linguistic expression that refers to an entity in the real world or another linguistic expression. Examples of markables include noun phrases (“the man”), named entities (“Peter”) and nested nominal ex- (“their”). We address the task of aniclassification markables: classifying them as animate/inanimate. This feature is useful for coreference resolution systems because only animate markables can be referred to using masculine and feminine pronouns in English like “him” and “she”. Thus, this is an important clue for automatically clustering the markables of a document into correct coreference chains. To create training and test sets, we extract all 39,689 coreference chains from the CoNLL2012 We label chains that contain an animate pronoun markable (“she”, “her”, “he”, “him” or “his”) and no inanimate pronoun markable (“it” or “its”) as animate; and chains that contain an inanimate pronoun markable and no animate pronoun markable as inanimate. Other chains are discarded. We extract 39,942 markables and their contexts from the 10,361 animate and inanimate chains. The context of a markable is represented as a SkipB: it is simply the pair of the two words occurring to the left and right of the markable. The gold label of a markable and its SkipB is the animacy status of its chain: either animate or inanimate. We divide all SkipBs having received an embedding in the embedding learning phase into a training set of 11,301 (8097 animate, 3204 inanimate) and a balanced test set of 4036. We use LIBLINEAR (Fan et al., 2008) for classification, with penalty factors 3 and 1 for inanimate and animate classes, respectively, because the training data are unbalanced. 4.1.1 Experimental results We compare the following representations for animacy classification of markables. (i) Phrase embedding: Skip-bigram embeddings with skip dis- 2 k &lt; (ii) Word embedding: concatenation of the embeddings of the two enclosing words where the embeddings are either standard word2vec embeddings (see Section 2) or the embeddings published by (Collobert al., (iii) the one-hot vector representation of a SkipB: the concatentation of two one-hot of dimensionality the size of the vocabulary. The first (resp. second) vector html wordreprs/ 43 turn off caught up take over macular degeneration telephone interview switch off mixed up take charge eye disease statement unplug entangled replace diabetic retinopathy interview turning off involved take control cataracts conference call shut off enmeshed stay on periodontal disease teleconference block out tangled retire epilepsy telephone call turned off mired succeed glaucoma told fiddle with engaged step down skin cancer said Table 1: Phrases and their nearest neighbors is the one-hot vector for the left (resp. right) word of the SkipB. Experimental results are shown in Table 2. Table 2: Classification accuracy. Mark “*” means lower than “phrase embedding”, means significantly lower than “phrase em- As significance test, we use test of equal proportion, p throughout. The results show that phrase embeddings have an obvious advantage in this classification task, for 2 This validates our hypothesis that learning embeddings for discontinuous linguistic units is promising. In our error analysis, we found two types of errors. (i) SkipBs. SkipBs are equally appropriate for animate and inanimate markables. Examples of such SkipBs “take*in” and “then*goes”. (ii) of specific SkipBs. SkipBs that are specific with respect to what type of markable they enclose sometimes occur with the “wrong” type of markable. For example, most markables occurring in the SkipB “of*whose” are animate because “whose” usually refers to an animate markable. However, in the context “...the southeastern area of Fujian whose economy is the most active” the enclosed markable is Fujian, a province of China. This example shows that “whose” occasionally refers to an inanimate entity even though these cases are infrequent. 4.1.2 Nearest neighbors of SkipBs Table 3 shows some SkipBs and their nearest neighbors in descending order, where similarity is computed with cosine measure. A general phenomenon is that phrase embeddings capture high degree of consistency in inferring the attributes of enclosed words. Considering the neighbor list in the first column, we can estithat a appears as the middle Furthermore, roughly be inferred for the remaining 4.2 Paraphrase identification task Paraphrase identification depends on semantic analysis. Standard approaches are unlikely to assign a high similarity score to the two sentences “he started the machine” and “he turned the machine on”. In our approach, embedding of the phrase “turned on” can greatly help us to infer correctly that the sentences are paraphrases. Hence, phrase embeddings and in particular embeddings of discontinuous phrases seem promising in paraphrase detection task. We use the Microsoft Paraphrase Corpus (Dolan et al., 2004) for evaluation. It consists of a training set with 2753 true paraphrase pairs and 1323 false paraphrase pairs, along with a test set with 1147 true and 578 false pairs. After discarding pairs in which neither sentence contains phrases, 3027 training pairs (2123 true vs. 904 false) and 1273 test pairs (871 true vs. 402 false) remain. reviewer points out that this is only a suggestive analysis and that corpus statistics about these contexts would be required to establish that phrase embeddings can predict partof-speech with high accuracy. representation accuracy phrase embedding word embedding 2 word2vec Collobert et al. 0.703 0.700 vectors 44 who*afghanistan, some*told women*have with*responsibility he*worried had*afghanistan other*told men*have of*responsibility she*worried he*afghanistan two*told children*have and*responsibility was*worried who*iraq –*told girls*have “*responsibility is*worried have*afghanistan but*told parents*have that*responsibility said*worried fighters*afghanistan one*told students*have ’s*responsibility that*worried who*kosovo because*told young*have the* responsibility they*worried was*afghanistan and*told people*have for*responsibility ’s*worried Table 3: SkipBs and their nearest neighbors We tackle the paraphrase identification task via supervised binary classification. Sentence representation equals to the addition over all the token embeddings (words as well as phrases). A slight difference is that when dealing with a senlike · · B · · B · · we only consider “A B” embedding once. The system “word embedding” is based on the embeddings of single words only. Subsequently, pair representation is derived by concatenating the two sentence vectors. This concatentation is then classified by LIBLIN- EAR as “paraphrase” or “no paraphrase”. 4.2.1 Experimental results and analysis Table 4 shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intricate features and achieves performance numbers higher than what we report here (Ji and Eisenstein, 2013; Madnani et al., 2012; Blacoe and Lapata, 2012). Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task. We are interested in how phrase embeddings make an impact on this task. To that end, we perform an analysis on test examples where word embeddings are better than phrase embeddings and vice versa. Table 5 shows four pairs, of which “phrase embedding” outperforms “word embedding” in the Table 4: Paraphrase task results. first two examples, “word embedding” defeats “phrase embedding” in the last two examples. In the first pair, successful phrase detection enables to split sentences into better units, thus the generated representation can convey the sentence meaning more exactly. The meaning difference in the second pair originates from the synonym substitution between “take over as chief financial officer” and “fill the position”. The embedding of the phrase “take over” matches the embedding of the single word “fill” in this context. “Phrase embedding” in the third pair suffers from wrong phrase detection. Actually, “in” and “on” can not be treated as a sound phrase in that situation even though “in on” is defined by Wiktionary. Indeed, this failure, to some extent, results from the shortcomings of our method in discovering true phrases. Furthermore, figuring out whether two words are a phrase might need to analyse syntactic structure in depth. This work is directly based on naive intuitive knowledge, acting as an initial exploration. Profound investigation is left as future work. Our implementation discovers the contained phrases in the fourth pair perfectly. Yet, “word embedding” defeats “phrase embedding” still. The pair is not a paraphrase partly because the numbers are different; e.g., there is a big difference between “5.8 basis points” and “50 basis points”. Only a method that can correctly treat numerical information can succeed here. However, the appearance of phrases “central bank”, “interest rates” and “basis points” makes the non-numerical parts more expressive and informative, leading to less dominant for digital quantifications. On the contrary, “word embedding” fails to split the sen- Methods Accuracy F1 baseline word embedding phrase embedding 0.684 0.803 0.695 0.805 0.713 0.812 45 GWP sentence 1 sentence 2 0 1 Common effects throat and cough, the FDA said. 0 1 Douglas Robinson, a senior president finance, will over chief financial officer on an interim basis . most common effects getthe nasal spray were throat cough. Douglas Robinson, CA senior finance, will fill the position in the interim.</abstract>
<note confidence="0.842521333333333">1 1 0 They were being held Sunday in the Camden County Jail on $ 100,000 bail each . 0 1 The rate two year Schatz</note>
<abstract confidence="0.986138136363637">was down 5.8 points 1.99 percent . Jacksons remained on County jail $ 100,000 bail. Swedish bank interrates 50 points 3.0 percent . Table 5: Four typical sentence pairs in which the predictions of word embedding system and phrase embedding system differ. G = gold annotation, W = prediction of word embedding system, P = prediction of phrase embedding system. The formatting used by the system is shown. The original word order of 2 of the third pair is · · Camden County jail on $ 100,000 bail”. tences into better units, it weakens unexpectedly the expressiveness of subordinate context. This example demonstrates the difficulty of paraphrase identification. Differing from simple similarity tasks, two sentences are often not paraphrases even though they may contain very similar words. 5 Related work To date, approaches to extend embedding (or more generally “representation”) beyond individwords are either (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by methods other than deep learning of embeddings, e.g., as vector space representations (Turney, 2012; Turney, 2013; Dinu et al., 2013). The main point of this paper – generalizing phrases to discontinuous phrases and computing representations for them – is orthogonal to this issue. It would be interesting to evaluate other types of representations for generalized phrases. 6 Conclusion and Future Work We have argued that generalized phrases are part of the inventory of linguistic units that we should compute embeddings for and we have shown that such embeddings are superior to word form embeddings in a coreference resolution task and standard paraphrase identification task. In this paper we have presented initial work on several problems that we plan to continue in the future: (i) How should the inventory of continuous and discontinous phrases be determined? We used a purely statistical definition on the one hand and dictionaries on the other. A combination of the two methods would be desirable. (ii) How can we distinguish between phrases that only occur in continuous form and phrases that must or can occur discontinuously? (iii) Given a sentence that contains the parts of a discontinuous phrase in correct order, how do we determine that the cooccurrence of the two parts constitutes an instance of the discontinuous phrase? (iv) Which tasks benefit</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17848" citStr="Blacoe and Lapata, 2012" startWordPosition="2868" endWordPosition="2871"> ” we only consider “A B” embedding once. The system “word embedding” is based on the embeddings of single words only. Subsequently, pair representation is derived by concatenating the two sentence vectors. This concatentation is then classified by LIBLINEAR as “paraphrase” or “no paraphrase”. 4.2.1 Experimental results and analysis Table 4 shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intricate features and achieves performance numbers higher than what we report here (Ji and Eisenstein, 2013; Madnani et al., 2012; Blacoe and Lapata, 2012). Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task. We are interested in how phrase embeddings make an impact on this task. To that end, we perform an analysis on test examples where word embeddings are better than phrase embeddings and vice versa. Table 5 shows four pairs, of which “phrase embedding” outperforms “word embedding” in the Table 4: Paraphrase task results. first two examples, “word embedding” defeats “phrase embedding” in the last two examples. In the first pair, successful phrase detection enabl</context>
<context position="21888" citStr="Blacoe and Lapata, 2012" startWordPosition="3535" endWordPosition="3538">y jail on $ 100,000 bail”. tences into better units, it weakens unexpectedly the expressiveness of subordinate context. This example demonstrates the difficulty of paraphrase identification. Differing from simple similarity tasks, two sentences are often not paraphrases even though they may contain very similar words. 5 Related work To date, approaches to extend embedding (or more generally “representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by methods other than deep learning of embeddings, e</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1047" citStr="Collobert et al., 2011" startWordPosition="149" endWordPosition="152">mes and word sequences. In this paper, we define the concept of generalized phrase that includes conventional linguistic phrases as well as skip-bigrams. We compute embeddings for generalized phrases and show in experimental evaluations on coreference resolution and paraphrase identification that such embeddings perform better than word form embeddings. 1 Motivation One advantage of recent work in deep learning on natural language processing (NLP) is that linguistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013), phrases and word sequences (Socher et al., 2010; Mikolov et al., 2013).1 Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? Building on the prior work in (Socher et al., 2010; Mikolov et al., 2013), we generalize t</context>
<context position="12468" citStr="Collobert et al., 2011" startWordPosition="2055" endWordPosition="2058">nimate) and a balanced test set of 4036. We use LIBLINEAR (Fan et al., 2008) for classification, with penalty factors 3 and 1 for inanimate and animate classes, respectively, because the training data are unbalanced. 4.1.1 Experimental results We compare the following representations for animacy classification of markables. (i) Phrase embedding: Skip-bigram embeddings with skip distance k = 2 and 2 &lt; k &lt; 3; (ii) Word embedding: concatenation of the embeddings of the two enclosing words where the embeddings are either standard word2vec embeddings (see Section 2) or the embeddings published by (Collobert et al., 2011);6 (iii) the one-hot vector representation of a SkipB: the concatentation of two one-hot vectors of dimensionality V where V is the size of the vocabulary. The first (resp. second) vector 5http://conll.cemantix.org/2012/data. html 6http://metaoptimize.com/projects/ wordreprs/ 43 turn off caught up take over macular degeneration telephone interview switch off mixed up take charge eye disease statement unplug entangled replace diabetic retinopathy interview turning off involved take control cataracts conference call shut off enmeshed stay on periodontal disease teleconference block out tangled r</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>50--58</pages>
<contexts>
<context position="22572" citStr="Dinu et al., 2013" startWordPosition="3641" endWordPosition="3644">s are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by methods other than deep learning of embeddings, e.g., as vector space representations (Turney, 2012; Turney, 2013; Dinu et al., 2013). The main point of this paper – generalizing phrases to discontinuous phrases and computing representations for them – is orthogonal to this issue. It would be interesting to evaluate other types of representations for generalized phrases. 6 Conclusion and Future Work We have argued that generalized phrases are part of the inventory of linguistic units that we should compute embeddings for and we have shown that such embeddings are superior to word form embeddings in a coreference resolution task and standard paraphrase identification task. In this paper we have presented initial work on seve</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. General estimation and evaluation of compositional distributional semantic models. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15663" citStr="Dolan et al., 2004" startWordPosition="2554" endWordPosition="2557">dverb can roughly be inferred for the remaining columns, respectively.7 4.2 Paraphrase identification task Paraphrase identification depends on semantic analysis. Standard approaches are unlikely to assign a high similarity score to the two sentences “he started the machine” and “he turned the machine on”. In our approach, embedding of the phrase “turned on” can greatly help us to infer correctly that the sentences are paraphrases. Hence, phrase embeddings and in particular embeddings of discontinuous phrases seem promising in paraphrase detection task. We use the Microsoft Paraphrase Corpus (Dolan et al., 2004) for evaluation. It consists of a training set with 2753 true paraphrase pairs and 1323 false paraphrase pairs, along with a test set with 1147 true and 578 false pairs. After discarding pairs in which neither sentence contains phrases, 3027 training pairs (2123 true vs. 904 false) and 1273 test pairs (871 true vs. 402 false) remain. 7A reviewer points out that this is only a suggestive analysis and that corpus statistics about these contexts would be required to establish that phrase embeddings can predict partof-speech with high accuracy. representation accuracy phrase embedding word embeddi</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11921" citStr="Fan et al., 2008" startWordPosition="1966" endWordPosition="1969">mate pronoun markable as inanimate. Other chains are discarded. We extract 39,942 markables and their contexts from the 10,361 animate and inanimate chains. The context of a markable is represented as a SkipB: it is simply the pair of the two words occurring to the left and right of the markable. The gold label of a markable and its SkipB is the animacy status of its chain: either animate or inanimate. We divide all SkipBs having received an embedding in the embedding learning phase into a training set of 11,301 (8097 animate, 3204 inanimate) and a balanced test set of 4036. We use LIBLINEAR (Fan et al., 2008) for classification, with penalty factors 3 and 1 for inanimate and animate classes, respectively, because the training data are unbalanced. 4.1.1 Experimental results We compare the following representations for animacy classification of markables. (i) Phrase embedding: Skip-bigram embeddings with skip distance k = 2 and 2 &lt; k &lt; 3; (ii) Word embedding: concatenation of the embeddings of the two enclosing words where the embeddings are either standard word2vec embeddings (see Section 2) or the embeddings published by (Collobert et al., 2011);6 (iii) the one-hot vector representation of a SkipB</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Discriminative improvements to distributional sentence similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>891--896</pages>
<contexts>
<context position="17800" citStr="Ji and Eisenstein, 2013" startWordPosition="2860" endWordPosition="2863">with a sentence like “· · · A B · · · A B · · · ” we only consider “A B” embedding once. The system “word embedding” is based on the embeddings of single words only. Subsequently, pair representation is derived by concatenating the two sentence vectors. This concatentation is then classified by LIBLINEAR as “paraphrase” or “no paraphrase”. 4.2.1 Experimental results and analysis Table 4 shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intricate features and achieves performance numbers higher than what we report here (Ji and Eisenstein, 2013; Madnani et al., 2012; Blacoe and Lapata, 2012). Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task. We are interested in how phrase embeddings make an impact on this task. To that end, we perform an analysis on test examples where word embeddings are better than phrase embeddings and vice versa. Table 5 shows four pairs, of which “phrase embedding” outperforms “word embedding” in the Table 4: Paraphrase task results. first two examples, “word embedding” defeats “phrase embedding” in the last two examples. In t</context>
</contexts>
<marker>Ji, Eisenstein, 2013</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2013. Discriminative improvements to distributional sentence similarity. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<pages>104--113</pages>
<contexts>
<context position="1342" citStr="Luong et al., 2013" startWordPosition="193" endWordPosition="197">uch embeddings perform better than word form embeddings. 1 Motivation One advantage of recent work in deep learning on natural language processing (NLP) is that linguistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013), phrases and word sequences (Socher et al., 2010; Mikolov et al., 2013).1 Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? Building on the prior work in (Socher et al., 2010; Mikolov et al., 2013), we generalize the notion of phrase to include skip-bigrams (SkipBs) and lexicon entries, 1Socher et al. use the term “word sequence”. Mikolov et al. use the term “phrase” for word sequences that are mostly frequent continuous collocations. where lexicon entries can be both “continuous” and “noncontinuous” lin</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Conference on Computational Natural Language Learning, pages 104–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17822" citStr="Madnani et al., 2012" startWordPosition="2864" endWordPosition="2867"> · A B · · · A B · · · ” we only consider “A B” embedding once. The system “word embedding” is based on the embeddings of single words only. Subsequently, pair representation is derived by concatenating the two sentence vectors. This concatentation is then classified by LIBLINEAR as “paraphrase” or “no paraphrase”. 4.2.1 Experimental results and analysis Table 4 shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intricate features and achieves performance numbers higher than what we report here (Ji and Eisenstein, 2013; Madnani et al., 2012; Blacoe and Lapata, 2012). Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task. We are interested in how phrase embeddings make an impact on this task. To that end, we perform an analysis on test examples where word embeddings are better than phrase embeddings and vice versa. Table 5 shows four pairs, of which “phrase embedding” outperforms “word embedding” in the Table 4: Paraphrase task results. first two examples, “word embedding” defeats “phrase embedding” in the last two examples. In the first pair, success</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546.</title>
<date>2013</date>
<contexts>
<context position="1414" citStr="Mikolov et al., 2013" startWordPosition="206" endWordPosition="209">One advantage of recent work in deep learning on natural language processing (NLP) is that linguistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013), phrases and word sequences (Socher et al., 2010; Mikolov et al., 2013).1 Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? Building on the prior work in (Socher et al., 2010; Mikolov et al., 2013), we generalize the notion of phrase to include skip-bigrams (SkipBs) and lexicon entries, 1Socher et al. use the term “word sequence”. Mikolov et al. use the term “phrase” for word sequences that are mostly frequent continuous collocations. where lexicon entries can be both “continuous” and “noncontinuous” linguistic phrases. Examples of skip-bigrams at distance 2 in the sentence </context>
<context position="5799" citStr="Mikolov et al., 2013" startWordPosition="932" endWordPosition="935">eference resolution classification task and for paraphrase identification. Another contribution lies in that the phrase embeddings we release2 could be a valuable resource for others. The remainder of this paper is organized as follows. Section 2 and Section 3 introduce how to learn embeddings for SkipBs and phrases, respectively. Experiments are provided in Section 4. Subsequently, we analyze related work in Section 5, and conclude our work in Section 6. 2 Embedding learning for SkipBs With English Gigaword Corpus (Parker et al., 2009), we use the skip-gram model as implemented in word2vec3 (Mikolov et al., 2013) to induce embeddings. Word2vec skip-gram scheme is a neural network language model, using a given word to predict its context words within a window size. To be able to use word2vec directly without code changes, we represent the corpus as a sequence of sentences, each consisting of two tokens: a SkipB and a word that occurs between the 2http://www.cis.lmu.de/pub/ phraseEmbedding.txt.bz2 3https://code.google.com/p/word2vec/ two enclosing words of the SkipB. The distance k between the two enclosing words can be varied. In our experiments, we use either distance k = 2 or distance 2 ≤ k ≤ 3. For </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Wordnet: An electronic lexical database.</title>
<date>1998</date>
<contexts>
<context position="7361" citStr="Miller and Fellbaum, 1998" startWordPosition="1195" endWordPosition="1198">), and the surrounding context of words of original sentences are also kept (i.e., the SkipB in the new sentences). We can run word2vec without any changes on the reformatted corpus to learn embeddings for SkipBs. As a baseline, we run word2vec on the original corpus to compute embeddings for words. Embedding size is set to 200. 3 Embedding learning for phrases 3.1 Phrase collection Phrases defined by a lexicon have not been deeply investigated before in deep learning. To collect canonical phrase set, we extract two-word phrases defined in Wiktionary4, and two-word phrases defined in Wordnet (Miller and Fellbaum, 1998) to form a collection of size 95218. This collection contains phrases whose parts always occur next to each other (e.g., “cold cuts”) and phrases whose parts more often occur separated from each other (e.g., “take (something) apart”). 3.2 Identification of phrase continuity Wiktionary and WordNet do not categorize phrases as continuous or discontinous. So we need a heuristic for determining this automatically. For each phrase “A B”, we compute [c1, c2, c3, c4, c5] where ci,1 ≤ i ≤ 5, indicates there are ci occurrences of A and B in that order with a distance of i. We compute these statistics f</context>
</contexts>
<marker>Miller, Fellbaum, 1998</marker>
<rawString>George Miller and Christiane Fellbaum. 1998. Wordnet: An electronic lexical database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="22285" citStr="Mitchell and Lapata, 2010" startWordPosition="3596" endWordPosition="3599">representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by methods other than deep learning of embeddings, e.g., as vector space representations (Turney, 2012; Turney, 2013; Dinu et al., 2013). The main point of this paper – generalizing phrases to discontinuous phrases and computing representations for them – is orthogonal to this issue. It would be interesting to evaluate other types of representations for generalized phrases. 6 Conclusion and Future Work We have argued that generalized phrases are</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
</authors>
<title>Linguistic Data Consortium, et al.</title>
<date>2009</date>
<marker>Parker, 2009</marker>
<rawString>Robert Parker, Linguistic Data Consortium, et al. 2009. English gigaword fourth edition. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="1391" citStr="Socher et al., 2010" startWordPosition="202" endWordPosition="205">ddings. 1 Motivation One advantage of recent work in deep learning on natural language processing (NLP) is that linguistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013), phrases and word sequences (Socher et al., 2010; Mikolov et al., 2013).1 Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? Building on the prior work in (Socher et al., 2010; Mikolov et al., 2013), we generalize the notion of phrase to include skip-bigrams (SkipBs) and lexicon entries, 1Socher et al. use the term “word sequence”. Mikolov et al. use the term “phrase” for word sequences that are mostly frequent continuous collocations. where lexicon entries can be both “continuous” and “noncontinuous” linguistic phrases. Examples of skip-bigrams at dist</context>
<context position="21820" citStr="Socher et al., 2010" startWordPosition="3523" endWordPosition="3526">order of sentence 2 of the third pair is “· · · in Camden County jail on $ 100,000 bail”. tences into better units, it weakens unexpectedly the expressiveness of subordinate context. This example demonstrates the difficulty of paraphrase identification. Differing from simple similarity tasks, two sentences are often not paraphrases even though they may contain very similar words. 5 Related work To date, approaches to extend embedding (or more generally “representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can </context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="21841" citStr="Socher et al., 2011" startWordPosition="3527" endWordPosition="3530">f the third pair is “· · · in Camden County jail on $ 100,000 bail”. tences into better units, it weakens unexpectedly the expressiveness of subordinate context. This example demonstrates the difficulty of paraphrase identification. Differing from simple similarity tasks, two sentences are often not paraphrases even though they may contain very similar words. 5 Related work To date, approaches to extend embedding (or more generally “representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by me</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="21862" citStr="Socher et al., 2012" startWordPosition="3531" endWordPosition="3534">· · · in Camden County jail on $ 100,000 bail”. tences into better units, it weakens unexpectedly the expressiveness of subordinate context. This example demonstrates the difficulty of paraphrase identification. Differing from simple similarity tasks, two sentences are often not paraphrases even though they may contain very similar words. 5 Related work To date, approaches to extend embedding (or more generally “representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by methods other than deep</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="21751" citStr="Turney, 2012" startWordPosition="3511" endWordPosition="3512"> The formatting used by the system is shown. The original word order of sentence 2 of the third pair is “· · · in Camden County jail on $ 100,000 bail”. tences into better units, it weakens unexpectedly the expressiveness of subordinate context. This example demonstrates the difficulty of paraphrase identification. Differing from simple similarity tasks, two sentences are often not paraphrases even though they may contain very similar words. 5 Related work To date, approaches to extend embedding (or more generally “representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) mak</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Distributional semantics beyond words: Supervised learning of analogy and paraphrase.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--353</pages>
<contexts>
<context position="22552" citStr="Turney, 2013" startWordPosition="3639" endWordPosition="3640"> even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by methods other than deep learning of embeddings, e.g., as vector space representations (Turney, 2012; Turney, 2013; Dinu et al., 2013). The main point of this paper – generalizing phrases to discontinuous phrases and computing representations for them – is orthogonal to this issue. It would be interesting to evaluate other types of representations for generalized phrases. 6 Conclusion and Future Work We have argued that generalized phrases are part of the inventory of linguistic units that we should compute embeddings for and we have shown that such embeddings are superior to word form embeddings in a coreference resolution task and standard paraphrase identification task. In this paper we have presented </context>
</contexts>
<marker>Turney, 2013</marker>
<rawString>Peter D Turney. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. Transactions of the Association for Computational Linguistics, 1:353–366.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>