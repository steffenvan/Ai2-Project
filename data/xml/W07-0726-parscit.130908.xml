<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.106134">
<title confidence="0.9985595">
Multi-Engine Machine Translation
with an Open-Source Decoder for Statistical Machine Translation
</title>
<author confidence="0.993333">
Yu Chen&apos;, Andreas Eisele&apos; 2, Christian Federmann2,
Eva Hasler3, Michael Jellinghaus&apos;, Silke Theison&apos;
</author>
<affiliation confidence="0.905035">
(authors listed in alphabetical order)
1: Saarland University, Saarbr ucken, Germany
2: DFKI GmbH, Saarbr ucken, Germany
3: University of Cologne, Germany
</affiliation>
<sectionHeader confidence="0.988106" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985214285714">
We describe an architecture that allows
to combine statistical machine translation
(SMT) with rule-based machine translation
(RBMT) in a multi-engine setup. We use a
variant of standard SMT technology to align
translations from one or more RBMT sys-
tems with the source text. We incorporate
phrases extracted from these alignments into
the phrase table of the SMT system and use
the open-source decoder Moses to find good
combinations of phrases from SMT training
data with the phrases derived from RBMT.
First experiments based on this hybrid archi-
tecture achieve promising results.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946317073171">
Recent work on statistical machine translation has
led to significant progress in coverage and quality of
translation technology, but so far, most of this work
focuses on translation into English, where relatively
simple morphological structure and abundance of
monolingual training data helped to compensate for
the relative lack of linguistic sophistication of the
underlying models. As SMT systems are trained on
massive amounts of data, they are typically quite
good at capturing implicit knowledge contained in
co-occurrence statistics, which can serve as a shal-
low replacement for the world knowledge that would
be required for the resolution of ambiguities and the
insertion of information that happens to be missing
in the source text but is required to generate well-
formed text in the target language.
Already before, decades of work went into the im-
plementation of MT systems (typically rule-based)
for frequently used language pairs1, and these sys-
tems quite often contain a wealth of linguistic
knowledge about the languages involved, such as
fairly complete mechanisms for morphological and
syntactic analysis and generation, as well as a large
number of bilingual lexical entries spanning many
application domains.
It is an interesting challenge to combine the differ-
ent types of knowledge into integrated systems that
could then exploit both explicit linguistic knowledge
contained in the rules of one or several conventional
MT system(s) and implicit knowledge that can be
extracted from large amounts of text.
The recently started EuroMatrix2 project will ex-
plore this integration of rule-based and statistical
knowledge sources, and one of the approaches to
be investigated is the combination of existing rule-
based MT systems into a multi-engine architecture.
The work described in this paper is one of the
first incarnations of such a multi-engine architec-
ture within the project, and a careful analysis of the
results will guide us in the choice of further steps
within the project.
</bodyText>
<sectionHeader confidence="0.991794" genericHeader="method">
2 Architectures for multi-engine MT
</sectionHeader>
<bodyText confidence="0.986989">
Combinations of MT systems into multi-engine ar-
chitectures have a long tradition, starting perhaps
with (Frederking and Nirenburg, 1994). Multi-
engine systems can be roughly divided into simple
</bodyText>
<footnote confidence="0.999290333333333">
1See (Hutchins et al., 2006) for a list of commercial MT
systems
2See http://www.euromatrix.net
</footnote>
<page confidence="0.958252">
193
</page>
<note confidence="0.808077">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 193–196,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.984129">
Figure 1: Architecture for multi-engine MT driven
by a SMT decoder
</figureCaption>
<bodyText confidence="0.999928222222222">
architectures that try to select the best output from a
number of systems, but leave the individual hypothe-
ses as is (Tidhar and K¨ussner, 2000; Akiba et al.,
2001; Callison-Burch and Flournoy, 2001; Akiba et
al., 2002; Nomoto, 2004; Eisele, 2005) and more so-
phisticated setups that try to recombine the best parts
from multiple hypotheses into a new utterance that
can be better than the best of the given candidates,
as described in (Rayner and Carter, 1997; Hogan and
Frederking, 1998; Bangalore et al., 2001; Jayaraman
and Lavie, 2005; Matusov et al., 2006; Rosti et al.,
2007).
Recombining multiple MT results requires find-
ing the correspondences between alternative render-
ings of a source-language expression proposed by
different MT systems. This is generally not straight-
forward, as different word order and errors in the
output can make it hard to identify the alignment.
Still, we assume that a good way to combine the var-
ious MT outcomes will need to involve word align-
ment between the MT output and the given source
text, and hence a specialized module for word align-
ment is a central component of our setup.
Additionally, a recombination system needs a way
to pick the best combination of alternative building
blocks; and when judging the quality of a particu-
lar configuration, both the plausibility of the build-
ing blocks as such and their relation to the context
need to be taken into account. The required opti-
mization process is very similar to the search in a
SMT decoder that looks for naturally sounding com-
binations of highly probable partial translations. In-
stead of implementing a special-purpose search pro-
cedure from scratch, we transform the information
contained in the MT output into a form that is suit-
able as input for an existing SMT decoder. This has
the additional advantage that resources used in stan-
dard phrase-based SMT can be flexibly combined
with the material extracted from the rule-based MT
results; the optimal combination can essentially be
reduced to the task of finding good relative weights
for the various phrase table entries.
A sketch of the overall architecture is given in
Fig. 1, where the blue (light) parts represent the
modules and data sets used in purely statistical MT,
and the red (dark) parts are the additional modules
and data sets derived from the rule-based engines. It
should be noted that this is by far not the only way
to combine systems. In particular, as this proposed
setup gives the last word to the SMT decoder, we
risk that linguistically well-formed constructs from
one of the rule-based engines will be deteriorated in
the final decoding step. Alternative architectures are
under exploration and will be described elsewhere.
</bodyText>
<sectionHeader confidence="0.951651" genericHeader="method">
3 MT systems and other knowledge
sources
</sectionHeader>
<bodyText confidence="0.999956473684211">
For the experiments, we used a set of six rule-based
MT engines that are partly available via web inter-
faces and partly installed locally. The web based
systems are provided by Google (based on Systran
for the relevant language pairs), SDL, and ProMT
which all deliver significantly different output. Lo-
cally installed systems are OpenLogos, Lucy (a re-
cent offspring of METAL), and translate pro by lin-
genio (only for German H English). In addition to
these engines, we also used the scripts included in
the Moses toolkit (Koehn et al., 2006)3 to generate
phrase tables from the training data. We enhanced
the phrase tables with information on whether a
given pair of phrases can also be derived via a third,
intermediate language. We assume that this can be
useful to distinguish different degrees of reliability,
but due to lack of time for fine-tuning we could not
yet show that it indeed helps in increasing the overall
quality of the output.
</bodyText>
<footnote confidence="0.997599">
3see http://www.statmt.org/moses/
</footnote>
<page confidence="0.996782">
194
</page>
<sectionHeader confidence="0.995477" genericHeader="method">
4 Implementation Details
</sectionHeader>
<subsectionHeader confidence="0.999261">
4.1 Alignment of MT output
</subsectionHeader>
<bodyText confidence="0.99999385">
The input text and the output text of the MT systems
was aligned by means of GIZA++ (Och and Ney,
2003), a tool with which statistical models for align-
ment of parallel texts can be trained. Since training
new models on merely short texts does not yield very
accurate results, we applied a method where text can
be aligned based on existing models that have been
trained on the Europarl Corpus (Koehn, 2005) be-
forehand. This was achieved by using a modified
version of GIZA++ that is able to load given mod-
els.
The modified version of GIZA++ is embedded
into a client-server setup. The user can send two
corresponding files to the server, and specify two
models for both translation directions from which
alignments should be generated. After generating
alignments in both directions (by running GIZA++
twice), the system also delivers a combination of
these alignments which then serves as input to the
following steps described below.
</bodyText>
<subsectionHeader confidence="0.995777">
4.2 Phrase tables from MT output
</subsectionHeader>
<bodyText confidence="0.999889">
We then concatenated the phrase tables from the
SMT baseline system and the phrase tables obtained
from the rule-based MT systems and augmented
them by additional columns, one for each system
used. With this additional information it is clear
which of the MT systems a phrase pair stems from,
enabling us to assign relative weights to the con-
tributions of the different systems. The optimal
weights for the different columns can then be as-
signed with the help of minimum error rate training
(Och, 2003).
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999422">
We compared the hybrid system to a purely statis-
tical baseline system as well as two rule-based sys-
tems. The only differences between the baseline sys-
tem and our hybrid system are the phrase table – the
hybrid system includes more lexical entries than the
baseline – and the weights obtained from minimum
error rate training.
For a statistical system, lexical coverage becomes
an obstacle – especially when the bilingual lexical
entries are trained on documents from different do-
mains. However, due to the distinct mechanisms
used to generate these entries, rule-based systems
and statistical systems usually differ in coverage.
Our system managed to utilize lexical entries from
various sources by integrating the phrase tables de-
rived from rule-based systems into the phrase table
trained on a large parallel corpus. Table 1 shows
</bodyText>
<table confidence="0.997954166666667">
Systems Token #
Ref. 2091 (4.21%)
R-I 3886 (7.02%)
R-II 3508 (6.30%)
SMT 3976 (7.91%)
Hybrid 2425 (5.59%)
</table>
<tableCaption confidence="0.998216">
Table 1: Untranslated tokens (excl. numbers and
</tableCaption>
<bodyText confidence="0.969799294117647">
punctuations) in output for news commentary task
(de-en) from different systems
a rough estimation of the number of untranslated
words in the respective output of different systems.
The estimation was done by counting “words” (i.e.
tokens excluding numbers and punctuations) that ap-
pear in both the source document and the outputs.
Note that, as we are investigating translations from
German to English, where the languages share a lot
of vocabulary, e.g. named entities such as “USA”,
there are around 4.21% of words that should stay the
same throughout the translation process. In the hy-
brid system, 5.59% of the words remain unchanged,
which is is the lowest percentage among all systems.
Our baseline system (SMT in Table 1), not compris-
ing additional phrase tables, was the one to produce
the highest number of such untranslated words.
</bodyText>
<table confidence="0.99549">
Baseline Hybrid
test 18.07 21.39
nc-test 21.17 22.86
</table>
<tableCaption confidence="0.643722666666667">
Table 2: Performance comparison (BLEU scores)
between baseline and hybrid systems, on in-domain
(test) and out-of-domain (nc-test) test data
</tableCaption>
<bodyText confidence="0.9958656">
Higher lexical coverage leads to better perfor-
mance as can be seen in Table 2, which compares
BLEU scores of the baseline and hybrid systems,
both measured on in-domain and out-of-domain test
data. Due to time constraints these numbers reflect
</bodyText>
<page confidence="0.997857">
195
</page>
<bodyText confidence="0.998853">
results from using a single RBMT system (Lucy);
using more systems would potentially further im-
prove results.
Andreas Eisele. 2005. First steps towards multi-engine
machine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, June.
</bodyText>
<sectionHeader confidence="0.996493" genericHeader="conclusions">
6 Outlook
</sectionHeader>
<bodyText confidence="0.999990058823529">
Due to lack of time for fine-tuning the parameters
and technical difficulties in the last days before de-
livery, the results submitted for the shared task do
not yet show the full potential of our architecture.
The architecture described here places a strong
emphasis on the statistical models and can be seen
as a variant of SMT where lexical information from
rule-based engines is used to increase lexical cover-
age. We are currently also exploring setups where
statistical alignments are fed into a rule-based sys-
tem, which has the advantage that well-formed syn-
tactic structures generated via linguistic rules can-
not be broken apart by the SMT components. But
as rule-based systems typically lack mechanisms for
ruling out implausible results, they cannot easily
cope with errors that creep into the lexicon due to
misalignments and similar problems.
</bodyText>
<sectionHeader confidence="0.998358" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99952425">
This research has been supported by the European
Commission in the FP6-IST project EuroMatrix. We
also want to thank Teresa Herrmann for helping us
with the Lucy system.
</bodyText>
<sectionHeader confidence="0.998496" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997128786885246">
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings ofMT
Summit VIII, Santiago de Compostela, Spain.
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to select
the best among outputs from multiple mt systems. In
COLING.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In ASRU, Italy.
Chris Callison-Burch and Raymond S. Flournoy. 2001.
A program for automatically selecting the best output
from multiple machine translation engines. In Proc. of
MT Summit VIII, Santiago de Compostela, Spain.
Robert E. Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In ANLP, pages 95–100.
Christopher Hogan and Robert E. Frederking. 1998. An
evaluation of the multi-engine MT architecture. In
Proceedings ofAMTA, pages 113–123.
John Hutchins, Walter Hartmann, and Etsuo Ito. 2006.
IAMT compendium of translation software. Twelfth
Edition, January.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. of EAMT, Budapest, Hungary.
P. Koehn, M. Federico, W. Shen, N. Bertoldi, O. Bo-
jar, C. Callison-Burch, B. Cowan, C. Dyer, H. Hoang,
R. Zens, A. Constantin, C. C. Moran, and E. Herbst.
2006. Open source toolkit for statistical machine trans-
lation: Factored translation models and confusion net-
work decoding. Final Report of the 2006 JHU Summer
Workshop.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the MT
Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In In Proc. EACL, pages 33–40.
Tadashi Nomoto. 2004. Multi-engine machine translation
with voted language model. In Proc. ofACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings ofACL,
Sapporo, Japan, July.
Manny Rayner and David M. Carter. 1997. Hybrid lan-
guage processing in the spoken language translator. In
Proc. ICASSP ’97, pages 107–110, Munich, Germany.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL’2007), pages 228–
235, Rochester, NY, April 22-27.
Dan Tidhar and Uwe K¨ussner. 2000. Learning to select a
good translation. In COLING, pages 843–849.
</reference>
<page confidence="0.998966">
196
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365772">
<title confidence="0.9998605">Multi-Engine Machine with an Open-Source Decoder for Statistical Machine Translation</title>
<author confidence="0.9989">Andreas Christian Michael Silke</author>
<note confidence="0.642941">(authors listed in alphabetical order) 1: Saarland University, Saarbr ucken, 2: DFKI GmbH, Saarbr ucken, 3: University of Cologne, Germany</note>
<abstract confidence="0.994418133333334">We describe an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup. We use a variant of standard SMT technology to align translations from one or more RBMT systems with the source text. We incorporate phrases extracted from these alignments into the phrase table of the SMT system and use the open-source decoder Moses to find good combinations of phrases from SMT training data with the phrases derived from RBMT. First experiments based on this hybrid architecture achieve promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasuhiro Akiba</author>
<author>Kenji Imamura</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Using multiple edit distances to automatically rank machine translation output.</title>
<date>2001</date>
<booktitle>In Proceedings ofMT Summit VIII, Santiago de Compostela,</booktitle>
<contexts>
<context position="3676" citStr="Akiba et al., 2001" startWordPosition="554" endWordPosition="557">have a long tradition, starting perhaps with (Frederking and Nirenburg, 1994). Multiengine systems can be roughly divided into simple 1See (Hutchins et al., 2006) for a list of commercial MT systems 2See http://www.euromatrix.net 193 Proceedings of the Second Workshop on Statistical Machine Translation, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is genera</context>
</contexts>
<marker>Akiba, Imamura, Sumita, 2001</marker>
<rawString>Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001. Using multiple edit distances to automatically rank machine translation output. In Proceedings ofMT Summit VIII, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhiro Akiba</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Using language and translation models to select the best among outputs from multiple mt systems.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="3731" citStr="Akiba et al., 2002" startWordPosition="562" endWordPosition="565">g and Nirenburg, 1994). Multiengine systems can be roughly divided into simple 1See (Hutchins et al., 2006) for a list of commercial MT systems 2See http://www.euromatrix.net 193 Proceedings of the Second Workshop on Statistical Machine Translation, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and er</context>
</contexts>
<marker>Akiba, Watanabe, Sumita, 2002</marker>
<rawString>Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita. 2002. Using language and translation models to select the best among outputs from multiple mt systems. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In ASRU,</booktitle>
<contexts>
<context position="4024" citStr="Bangalore et al., 2001" startWordPosition="611" endWordPosition="614">7 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is a central component of o</context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In ASRU, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Raymond S Flournoy</author>
</authors>
<title>A program for automatically selecting the best output from multiple machine translation engines.</title>
<date>2001</date>
<booktitle>In Proc. of MT Summit VIII, Santiago de Compostela,</booktitle>
<contexts>
<context position="3711" citStr="Callison-Burch and Flournoy, 2001" startWordPosition="558" endWordPosition="561">n, starting perhaps with (Frederking and Nirenburg, 1994). Multiengine systems can be roughly divided into simple 1See (Hutchins et al., 2006) for a list of commercial MT systems 2See http://www.euromatrix.net 193 Proceedings of the Second Workshop on Statistical Machine Translation, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as differe</context>
</contexts>
<marker>Callison-Burch, Flournoy, 2001</marker>
<rawString>Chris Callison-Burch and Raymond S. Flournoy. 2001. A program for automatically selecting the best output from multiple machine translation engines. In Proc. of MT Summit VIII, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In ANLP,</booktitle>
<pages>95--100</pages>
<contexts>
<context position="3135" citStr="Frederking and Nirenburg, 1994" startWordPosition="471" endWordPosition="474">ted EuroMatrix2 project will explore this integration of rule-based and statistical knowledge sources, and one of the approaches to be investigated is the combination of existing rulebased MT systems into a multi-engine architecture. The work described in this paper is one of the first incarnations of such a multi-engine architecture within the project, and a careful analysis of the results will guide us in the choice of further steps within the project. 2 Architectures for multi-engine MT Combinations of MT systems into multi-engine architectures have a long tradition, starting perhaps with (Frederking and Nirenburg, 1994). Multiengine systems can be roughly divided into simple 1See (Hutchins et al., 2006) for a list of commercial MT systems 2See http://www.euromatrix.net 193 Proceedings of the Second Workshop on Statistical Machine Translation, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; No</context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert E. Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In ANLP, pages 95–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Hogan</author>
<author>Robert E Frederking</author>
</authors>
<title>An evaluation of the multi-engine MT architecture.</title>
<date>1998</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>113--123</pages>
<contexts>
<context position="4000" citStr="Hogan and Frederking, 1998" startWordPosition="607" endWordPosition="610">96, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is </context>
</contexts>
<marker>Hogan, Frederking, 1998</marker>
<rawString>Christopher Hogan and Robert E. Frederking. 1998. An evaluation of the multi-engine MT architecture. In Proceedings ofAMTA, pages 113–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hutchins</author>
<author>Walter Hartmann</author>
<author>Etsuo Ito</author>
</authors>
<date>2006</date>
<booktitle>IAMT compendium of translation software. Twelfth Edition,</booktitle>
<contexts>
<context position="3220" citStr="Hutchins et al., 2006" startWordPosition="485" endWordPosition="488">sources, and one of the approaches to be investigated is the combination of existing rulebased MT systems into a multi-engine architecture. The work described in this paper is one of the first incarnations of such a multi-engine architecture within the project, and a careful analysis of the results will guide us in the choice of further steps within the project. 2 Architectures for multi-engine MT Combinations of MT systems into multi-engine architectures have a long tradition, starting perhaps with (Frederking and Nirenburg, 1994). Multiengine systems can be roughly divided into simple 1See (Hutchins et al., 2006) for a list of commercial MT systems 2See http://www.euromatrix.net 193 Proceedings of the Second Workshop on Statistical Machine Translation, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the bes</context>
</contexts>
<marker>Hutchins, Hartmann, Ito, 2006</marker>
<rawString>John Hutchins, Walter Hartmann, and Etsuo Ito. 2006. IAMT compendium of translation software. Twelfth Edition, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyamsundar Jayaraman</author>
<author>Alon Lavie</author>
</authors>
<title>Multiengine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proc. of EAMT,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="4051" citStr="Jayaraman and Lavie, 2005" startWordPosition="615" endWordPosition="618">ational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is a central component of our setup. Additionally, a r</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>Shyamsundar Jayaraman and Alon Lavie. 2005. Multiengine machine translation guided by explicit word matching. In Proc. of EAMT, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>M Federico</author>
<author>W Shen</author>
<author>N Bertoldi</author>
<author>O Bojar</author>
<author>C Callison-Burch</author>
<author>B Cowan</author>
<author>C Dyer</author>
<author>H Hoang</author>
<author>R Zens</author>
<author>A Constantin</author>
<author>C C Moran</author>
<author>E Herbst</author>
</authors>
<title>Open source toolkit for statistical machine translation: Factored translation models and confusion network decoding.</title>
<date>2006</date>
<booktitle>Final Report of the 2006 JHU Summer Workshop.</booktitle>
<contexts>
<context position="6798" citStr="Koehn et al., 2006" startWordPosition="1073" endWordPosition="1076">on and will be described elsewhere. 3 MT systems and other knowledge sources For the experiments, we used a set of six rule-based MT engines that are partly available via web interfaces and partly installed locally. The web based systems are provided by Google (based on Systran for the relevant language pairs), SDL, and ProMT which all deliver significantly different output. Locally installed systems are OpenLogos, Lucy (a recent offspring of METAL), and translate pro by lingenio (only for German H English). In addition to these engines, we also used the scripts included in the Moses toolkit (Koehn et al., 2006)3 to generate phrase tables from the training data. We enhanced the phrase tables with information on whether a given pair of phrases can also be derived via a third, intermediate language. We assume that this can be useful to distinguish different degrees of reliability, but due to lack of time for fine-tuning we could not yet show that it indeed helps in increasing the overall quality of the output. 3see http://www.statmt.org/moses/ 194 4 Implementation Details 4.1 Alignment of MT output The input text and the output text of the MT systems was aligned by means of GIZA++ (Och and Ney, 2003), </context>
</contexts>
<marker>Koehn, Federico, Shen, Bertoldi, Bojar, Callison-Burch, Cowan, Dyer, Hoang, Zens, Constantin, Moran, Herbst, 2006</marker>
<rawString>P. Koehn, M. Federico, W. Shen, N. Bertoldi, O. Bojar, C. Callison-Burch, B. Cowan, C. Dyer, H. Hoang, R. Zens, A. Constantin, C. C. Moran, and E. Herbst. 2006. Open source toolkit for statistical machine translation: Factored translation models and confusion network decoding. Final Report of the 2006 JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit.</booktitle>
<contexts>
<context position="7699" citStr="Koehn, 2005" startWordPosition="1228" endWordPosition="1229">of time for fine-tuning we could not yet show that it indeed helps in increasing the overall quality of the output. 3see http://www.statmt.org/moses/ 194 4 Implementation Details 4.1 Alignment of MT output The input text and the output text of the MT systems was aligned by means of GIZA++ (Och and Ney, 2003), a tool with which statistical models for alignment of parallel texts can be trained. Since training new models on merely short texts does not yield very accurate results, we applied a method where text can be aligned based on existing models that have been trained on the Europarl Corpus (Koehn, 2005) beforehand. This was achieved by using a modified version of GIZA++ that is able to load given models. The modified version of GIZA++ is embedded into a client-server setup. The user can send two corresponding files to the server, and specify two models for both translation directions from which alignments should be generated. After generating alignments in both directions (by running GIZA++ twice), the system also delivers a combination of these alignments which then serves as input to the following steps described below. 4.2 Phrase tables from MT output We then concatenated the phrase table</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In</title>
<date>2006</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="4073" citStr="Matusov et al., 2006" startWordPosition="619" endWordPosition="622">1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is a central component of our setup. Additionally, a recombination system ne</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In In Proc. EACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>Multi-engine machine translation with voted language model.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="3745" citStr="Nomoto, 2004" startWordPosition="566" endWordPosition="567">4). Multiengine systems can be roughly divided into simple 1See (Hutchins et al., 2006) for a list of commercial MT systems 2See http://www.euromatrix.net 193 Proceedings of the Second Workshop on Statistical Machine Translation, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the ou</context>
</contexts>
<marker>Nomoto, 2004</marker>
<rawString>Tadashi Nomoto. 2004. Multi-engine machine translation with voted language model. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7396" citStr="Och and Ney, 2003" startWordPosition="1174" endWordPosition="1177">(Koehn et al., 2006)3 to generate phrase tables from the training data. We enhanced the phrase tables with information on whether a given pair of phrases can also be derived via a third, intermediate language. We assume that this can be useful to distinguish different degrees of reliability, but due to lack of time for fine-tuning we could not yet show that it indeed helps in increasing the overall quality of the output. 3see http://www.statmt.org/moses/ 194 4 Implementation Details 4.1 Alignment of MT output The input text and the output text of the MT systems was aligned by means of GIZA++ (Och and Ney, 2003), a tool with which statistical models for alignment of parallel texts can be trained. Since training new models on merely short texts does not yield very accurate results, we applied a method where text can be aligned based on existing models that have been trained on the Europarl Corpus (Koehn, 2005) beforehand. This was achieved by using a modified version of GIZA++ that is able to load given models. The modified version of GIZA++ is embedded into a client-server setup. The user can send two corresponding files to the server, and specify two models for both translation directions from which</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="8764" citStr="Och, 2003" startWordPosition="1404" endWordPosition="1405">gnments which then serves as input to the following steps described below. 4.2 Phrase tables from MT output We then concatenated the phrase tables from the SMT baseline system and the phrase tables obtained from the rule-based MT systems and augmented them by additional columns, one for each system used. With this additional information it is clear which of the MT systems a phrase pair stems from, enabling us to assign relative weights to the contributions of the different systems. The optimal weights for the different columns can then be assigned with the help of minimum error rate training (Och, 2003). 5 Results We compared the hybrid system to a purely statistical baseline system as well as two rule-based systems. The only differences between the baseline system and our hybrid system are the phrase table – the hybrid system includes more lexical entries than the baseline – and the weights obtained from minimum error rate training. For a statistical system, lexical coverage becomes an obstacle – especially when the bilingual lexical entries are trained on documents from different domains. However, due to the distinct mechanisms used to generate these entries, rule-based systems and statist</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings ofACL, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>David M Carter</author>
</authors>
<title>Hybrid language processing in the spoken language translator.</title>
<date>1997</date>
<booktitle>In Proc. ICASSP ’97,</booktitle>
<pages>107--110</pages>
<location>Munich, Germany.</location>
<contexts>
<context position="3972" citStr="Rayner and Carter, 1997" startWordPosition="603" endWordPosition="606"> Translation, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized m</context>
</contexts>
<marker>Rayner, Carter, 1997</marker>
<rawString>Manny Rayner and David M. Carter. 1997. Hybrid language processing in the spoken language translator. In Proc. ICASSP ’97, pages 107–110, Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Combining translations from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and North American chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL’2007),</booktitle>
<pages>228--235</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="4094" citStr="Rosti et al., 2007" startWordPosition="623" endWordPosition="626">lti-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is a central component of our setup. Additionally, a recombination system needs a way to pick the</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie J. Dorr. 2007. Combining translations from multiple machine translation systems. In Proceedings of the Conference on Human Language Technology and North American chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL’2007), pages 228– 235, Rochester, NY, April 22-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tidhar</author>
<author>Uwe K¨ussner</author>
</authors>
<title>Learning to select a good translation. In</title>
<date>2000</date>
<booktitle>COLING,</booktitle>
<pages>843--849</pages>
<marker>Tidhar, K¨ussner, 2000</marker>
<rawString>Dan Tidhar and Uwe K¨ussner. 2000. Learning to select a good translation. In COLING, pages 843–849.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>