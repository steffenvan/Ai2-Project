<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021257">
<title confidence="0.9973445">
NCSU: Modeling Temporal Relations
with Markov Logic and Lexical Ontology
</title>
<author confidence="0.994865">
Eun Young Ha Alok Baikadi Carlyle Licata James C. Lester
</author>
<affiliation confidence="0.837712">
Department of Computer Science
North Carolina State University
</affiliation>
<address confidence="0.800193">
Raleigh, NC, USA
</address>
<email confidence="0.999497">
{eha,abaikad,cjlicata,lester}@ncsu.edu
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876">
As a participant in TempEval-2, we ad-
dress the temporal relations task consist-
ing of four related subtasks. We take a su-
pervised machine-learning technique us-
ing Markov Logic in combination with
rich lexical relations beyond basic and
syntactic features. One of our two submit-
ted systems achieved the highest score for
the Task F (66% precision), untied, and
the second highest score (63% precision)
for the Task C, which tied with three other
systems.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987781301587302">
Time plays a key role in narrative. However, cor-
rectly recognizing temporal order among events
is a challenging task. As a follow-up to the first
TempEval competition, TempEval-2 addresses
this challenge. Among the three proposed tasks
of TempEval-2, we address the temporal rela-
tions task consisting of four subtasks: predicting
temporal relations that hold between events and
time expressions in the same sentence (Task C),
events and the document creation time (Task D),
main events in adjacent sentences (Task E), and
main events and syntactically dominated events,
such as those in subordinated clauses (Task F).
We are primarily concerned with Task C, E, and
F, because D is not relevant to our application
domain.1 However, rather than eliminating Task
D altogether, we build a very simple model for
this task by using only those features that are
shared with other task models (i.e., the document
1 Our application domain concerns analysis of narrative
stories written by middle school students, with the analysis
being conducted a single story at a time.
creation time data are not used because none of
the other task models need them as features). It
was expected that this approach would support
more interesting comparisons with other systems
that take a more sophisticated approach to the
task. Further, we experiment with a joint model-
ing technique to examine if the communication
with other task models brings a boost to a per-
formance of the simple model.
Taking a supervised machine-learning ap-
proach with Markov Logic (ML) (Richardson and
Domingos, 2006), we constructed two systems,
NCSU-INDI and NCSU-JOINT. NCSU-INDI con-
sists of four independently trained classifiers,
one for each task, whereas NCSU-JOINT models
all four tasks jointly. The choice of ML as learn-
ing technique for temporal relations is motivated
both theoretically and practically. Theoretically,
it is a statistical relational learning framework
that does not make the i.i.d. assumption for the
data. This is a desirable characteristic for com-
plex problems such as temporal relation classifi-
cation, as well as many other natural language
problems, in which the features representing a
given problem are often correlated with one an-
other. Practically, ML allows us to build both
individual and joint models in a uniform frame-
work; individual models can be easily combined
together into a joint model with a set of global
formulae governing over them.
In previous work (Yoshikawa et al., 2009),
ML was successfully applied to temporal relation
classification task. Our approach is different
from this work in two primary respects. First, we
introduce new lexical relation features derived
from English lexical ontologies. Second, our
model addresses a new task introduced in Tem-
pEval-2, which is to identify temporal relations
between main and syntactically dominated
events in the same sentence. We also employ
phrase-based syntactic features (Bethard and
</bodyText>
<page confidence="0.981826">
341
</page>
<bodyText confidence="0.76077625">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 341–344,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Martin 2007) rather than dependency-based syn-
tactic features.
</bodyText>
<sectionHeader confidence="0.993173" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.9987647">
We consider three types of features: basic, syn-
tactic, and lexical relation features. Basic fea-
tures represent the information directly available
from the original data provided by the task orga-
nizer; syntactic features are extracted from syn-
tactic parses generated by Charniak parser
(Charniak, 2000); and lexical semantic relations
that are derived from two external lexical data-
bases, VERBOCEAN (Chklovski and Pantel,
2004) and WordNet (Fellbaum, 1998).
</bodyText>
<subsectionHeader confidence="0.992261">
2.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.9999846">
Basic features include the word tokens, stems of
the words, and the manually annotated attributes
of events and time expressions. In the TempEval-
2 data, an event always consists of a single word
token, but time expressions often consist of mul-
tiple tokens. We treat each word in time expres-
sions as a different feature. For example, two
word features, ‘this’ and ‘afternoon’, are ex-
tracted from a given time expression ‘this after-
noon’. Stemming is done with the Porter
Stemmer in NLTK (Loper and Bird, 2002). The
value attributes of time expressions are treated as
symbolic features, rather than being decomposed
into actual integer values representing dates and
times.
</bodyText>
<subsectionHeader confidence="0.998615">
2.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.99992875">
Our syntactic features draw upon the features
previously shown to be effective for temporal
relation classification (Bethard and Martin,
2007), including the following:
</bodyText>
<listItem confidence="0.998225181818182">
• pos: the part-of-speech (pos) tags of the
event and the time expression word to-
kens, assigned by Charniak parser.
• gov-prep: any prepositions governing
the event or time expression (e.g., ‘for’ in
‘for ten years’).
• gov-verb: the verb governing the
event or time expression, similar to gov-
prep.
• gov-verb-pos: the pos tag of the
governing verb.
</listItem>
<bodyText confidence="0.9995938">
We also investigate both full and partial syn-
tactic paths between a pair of event and time ex-
pressions, but including these features does not
improve the classification results on our devel-
opment data set.
</bodyText>
<subsectionHeader confidence="0.999039">
2.3 Lexical Relation Features
</subsectionHeader>
<bodyText confidence="0.999966512820513">
VERBOCEAN is a graph of semantic relations
between verbs. There are 22,306 relations be-
tween 3,477 verbs that have been mined using
Google searches for lexico-syntactic patterns.
VERBOCEAN contains five different types of re-
lations (Table 1). Verbs are stored in the lemma-
tized forms and senses are not disambiguated. A
connection between two verbs indicates that the
relation holds between some senses of the verbs.
VERBOCEAN’S database is presented as a list
of verb pair relations, along with a confidence
score. Both the transitive and symmetric closure
over the relations were taken before storage in a
SQLite database for queries. The transitive clo-
sure was calculated using the Warshall algorithm
(Agrawal and Jagadish, 1990). The confidence
score for the new arc was calculated as the aver-
age of the two constituents. The symmetric clo-
sure was calculated using a simple pass. The
confidence score is the same as the reflected
edge for symmetric relations. A set of VER-
BOCEAN features were calculated for each target
event pair within each of the temporal relations
tasks. Each verb was lemmatized using the
WordNet lemmatizer in NLTK before being
compared against the database. Rather than fo-
cusing only on HAPPENS-BEFORE relation as in
Mani et al. (2006), we consider all five verb rela-
tions in two different versions, unweighted and
weighted. The unweighted version is a binary
feature indicating the existence of an arc between
the two target verbs in VERBOCEAN. In the
weighted version, the existence of an arc is
weighted by the associated confidence score.
In addition to VerbOcean, WordNet was used
for its conceptual relations. WordNet is a large
lexical database, which contains information on
verbs, nouns, adjectives and adverbs, grouped
into hierarchically organized cognitive synonym
</bodyText>
<table confidence="0.999075833333333">
Relation Example
SIMILARITY à† produce :: create
STRENGTH † wound :: kill
ANTONYMY à open :: close
ENABLEMENT fight :: win
HAPPENS-BEFORE † buy :: own
</table>
<tableCaption confidence="0.997976">
Table 1: Semantic relations between verbs in
</tableCaption>
<footnote confidence="0.6198552">
VERBOCEAN (à and † denotes symmetric and
transitive closure, respectively, holds for the
given relation)2
2 Examples are taken from
http://demo.patrickpantel.com/Content/Verbocean/.
</footnote>
<page confidence="0.993487">
342
</page>
<bodyText confidence="0.999922">
sets (synsets). WordNet was accessed through
the WordNetCorpusReader module of NLTK.
For each target event pair within each of the
temporal relations tasks, a semantic distance be-
tween the associated tokens was computed using
the path-similarity metric present within the API.
The synset chosen was simply the first synset
returned by the reader. Similar to the VER-
BOCEAN features, we consider both unweighted
and weighted versions of the feature.
</bodyText>
<sectionHeader confidence="0.985395" genericHeader="method">
3 The Systems
</sectionHeader>
<bodyText confidence="0.999791">
ML is a probabilistic extension of first-order
logic that allows formulae to be violated. It as-
signs a weight to each formula, reflecting the
strength of the constraint represented by the for-
mula. A Markov logic network (MLN) is a set of
weighted first-order clauses, which, together
with constants, defines a Markov network. We
constructed two systems, NCSU-INDI and
NCSU-JOINT using an off-the-shelf tool for ML
(Riedel, 2008).
</bodyText>
<subsectionHeader confidence="0.992523">
3.1 NCSU-INDI
</subsectionHeader>
<bodyText confidence="0.9998916">
NCSU-INDI consists of four independently
trained MLNs, one for each task. Each MLN is
defined by a set of local formulae that are con-
junctions of predicates representing the features.
An example local formula used for Task C is
</bodyText>
<equation confidence="0.990868">
eventTimex(e, t) eventWord(e, w)
relEventTimex(e, t, r) (1)
</equation>
<bodyText confidence="0.999920833333333">
If a pair of event e and time expression t exists
and the event consists of a word token w, for-
mula (1) assigns a temporal relation t to the
given pair of e and t with some weights.
For each task, the features described in Sec-
tion 2 were examined on a held-out development
data set (about 10% of the training data) for their
effectiveness in predicting temporal relations and
removed if they do not improve the results. Ta-
ble 2 lists the features actually used for the tasks.
Interestingly, none of the time expression fea-
tures were effective on the development data.
</bodyText>
<subsectionHeader confidence="0.99432">
3.2 NCSU-JOINT
</subsectionHeader>
<bodyText confidence="0.999849">
As well as the local formulae from the four local
MLNs, a set of global formulae are added to
NCSU-JOINT as hard constraints to ensure the
consistency between the classification decisions
of local MLNs. For example, formula (2) ensures
that if an event e1 happens before the document
creation time (dct) and another event e2 happens
</bodyText>
<table confidence="0.999177076923077">
Feature Task
C D E F
Event event-word √ √ √e2 √e1,e2
event-stem √ √ √e1.e2 √e1,e2
Event event-polarity √ √ √e1.e2 √e1,e2
Attribute
event-modal √ √ √e1.e2 √e1,e2
event-pos √ √ √e1.e2 √e2
event-tense √ √e1.e2 √e1,e2
event-aspect √ √ √e1.e2 √e1,e2
event-class √ √ √e1.e2 √e1,e2
Timex timex-word
timex-stem
Timex timex-type
Attribute
timex-value
Syntactic pos √e √e1.e2
Parse
gov-prep √e,t √e √e1.e2 √e1,e2
gov-verb √e,t √e √e1.e2 √e1,e2
gov-verb-pos √e,t √e1.e2 √e1,e2
Verb- verb-rel √
Ocean
verb-rel-w √
WordNet word-dist √
word-dist-w
</table>
<tableCaption confidence="0.987074">
Table 2: Features used for each task (subscripts
</tableCaption>
<bodyText confidence="0.933947428571429">
e and t mean event and time expression, re-
spectively. Subscripts e1 and e2 mean the first
and the second main events for the Task E and
the main and the syntactically dominated
events for the Task F, respectively)
after dct, then e1 happens before e2 and vice ver-
sa.
</bodyText>
<equation confidence="0.9984245">
relDctEvent(e1,t,BEFORE) relDctEvent(e2,t, AFTER)
relEvents(e1, e2, BEFORE) (2)
</equation>
<bodyText confidence="0.999889666666667">
A set of global constraints is defined between
Tasks C and F, D and F, as well as D and E, re-
spectively.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99991975">
The predicted outputs from our systems exhibit
mixed results. NCSU-INDI achieves the highest
precision score on the test data for Task F by a
relatively large margin (6%) from the second-
place system, as well as the second highest preci-
sion score on Task C, tied with three other sys-
tems. Given the encouraging result for Task F,
we would preliminarily conclude that the VE-
BOCEAN relations are effective predictors of
temporal relations between main and syntacti-
cally dominated events. However, the same sys-
tem does not achieve the same level of accuracy
</bodyText>
<page confidence="0.997377">
343
</page>
<table confidence="0.99977">
System Precision / Recall (%)
Task C Task D Task E Task F
NCSU-INDI 63/63 68/68 48/48 66/66
NCSU-JOINT 62/62 21/21 51/51 25/25
</table>
<tableCaption confidence="0.999885">
Table 3: Accuracy of the systems on each task
</tableCaption>
<bodyText confidence="0.999907476190476">
for Task E, even though it is closely related to
Task F. The major difference between the mod-
els of Task E and F is that the Task E model uses
weighted VERBOCEAN relations along with a
WordNet feature, while the Task F model uses
unweighted VERBOCEAN relations without the
WordNet feature. We suspect these two features
might negatively impact the classification deci-
sions on the test data, even though they prelimi-
narily appeared to be effective predictors on the
development data.
NCSU-JOINT also yields mixed results. The
performance on both Task D and F dramatically
drops with the joint modeling approach, while
there is a modest improvement on Task E. Man-
ual examination of the results on the test data
revealed that the majority of the relations in Task
D and F were classified as OVERLAP, which may
be due to overly strict global constraints; rather
than violating global constraints, the system re-
sorted to rather neutral predictions.
</bodyText>
<sectionHeader confidence="0.999484" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999925285714286">
Temporal event order recognition is a challeng-
ing task. Using basic, syntactic, and lexical rela-
tion features, we built two systems with ML:
NCSU-INDI models each subtask independently,
and NCSU-JOINT models all four tasks jointly.
NCSU-INDI was most effective in predicting
temporal relations between main events and syn-
tactically dominated events (66% precision), as
well as temporal relations between time expres-
sions and events (63% precision). Future direc-
tions include conducting a more rigorous exami-
nation of the predictive power of the features, as
well as the impact of global formulae for the
joint model.
</bodyText>
<sectionHeader confidence="0.998322" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999116">
This research was supported by the National Sci-
ence Foundation under Grant IIS-0757535. Any
opinions, findings, and conclusions or recom-
mendations expressed in this material are those
of the authors and do not necessarily reflect the
views of the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.990297" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852803921569">
R. Agrawal, S. Dar, and H. V. Jagadish. 1990. Direct
transitive closure algorithms: design and perform-
ance evaluation. ACM Transactions on Database
Systems, 15(3): 427-458.
S. Bethard and J. H. Martin. 2007. CU-TMP: tempo-
ral relation classification using syntactic and se-
mantic features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages
129-132, Prague, Czech Republic.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Lin-
guistics conference, pages 132-139, Seattle, WA.
Y. Cheng, M. Asahara, and Y. Matsumoto. 2007.
NAIST.Japan: Temporal relation identification us-
ing dependency parsed tree. In Proceedings of the
4th International Workshop on Semantic Evalua-
tions, pages 245-248, Prague, Czech Republic.
T. Chklovski and P. Pantel. 2004.VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Rela-
tions. In Proceedings of Conference on Empirical
Methods in Natural Language Processing, pages
33-40, Barcelona, Spain.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA: MIT Press.
E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of ACL Workshop
on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational
Linguistics, pages 62–69, Philadelphia, PA.
I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J.
Pustejovsky. 2006. Machine learning of temporal
relations. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 753-760, Sydney, Aus-
tralia.
M. Richardson and P. Domingos. 2006. Markov Log-
ic Networks. Machine Learning, 62(1): 107-136.
S. Riedel. 2008. Improving the accuracy and effi-
ciency of MAP inference for Markov Logic. In
Proceedings of the 24th Conference in Uncertainty
in Artificial Intelligence, pages 468-475, Helsinki,
Finland.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsu-
moto. 2009. Jointly Identifying Temporal Relations
with Markov Logic. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
405-413, Suntec, Singapore.
</reference>
<page confidence="0.998983">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935054">
<title confidence="0.9813675">NCSU: Modeling Temporal Relations with Markov Logic and Lexical Ontology</title>
<author confidence="0.998941">Eun Young Ha Alok Baikadi Carlyle Licata James C Lester</author>
<affiliation confidence="0.9988525">Department of Computer Science North Carolina State University</affiliation>
<address confidence="0.996913">Raleigh, NC, USA</address>
<email confidence="0.99979">eha@ncsu.edu</email>
<email confidence="0.99979">abaikad@ncsu.edu</email>
<email confidence="0.99979">cjlicata@ncsu.edu</email>
<email confidence="0.99979">lester@ncsu.edu</email>
<abstract confidence="0.997901538461539">As a participant in TempEval-2, we address the temporal relations task consisting of four related subtasks. We take a supervised machine-learning technique using Markov Logic in combination with rich lexical relations beyond basic and syntactic features. One of our two submitted systems achieved the highest score for the Task F (66% precision), untied, and the second highest score (63% precision) for the Task C, which tied with three other systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>S Dar</author>
<author>H V Jagadish</author>
</authors>
<title>Direct transitive closure algorithms: design and performance evaluation.</title>
<date>1990</date>
<journal>ACM Transactions on Database Systems,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>427--458</pages>
<marker>Agrawal, Dar, Jagadish, 1990</marker>
<rawString>R. Agrawal, S. Dar, and H. V. Jagadish. 1990. Direct transitive closure algorithms: design and performance evaluation. ACM Transactions on Database Systems, 15(3): 427-458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bethard</author>
<author>J H Martin</author>
</authors>
<title>CU-TMP: temporal relation classification using syntactic and semantic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>129--132</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5236" citStr="Bethard and Martin, 2007" startWordPosition="805" endWordPosition="808">expressions often consist of multiple tokens. We treat each word in time expressions as a different feature. For example, two word features, ‘this’ and ‘afternoon’, are extracted from a given time expression ‘this afternoon’. Stemming is done with the Porter Stemmer in NLTK (Loper and Bird, 2002). The value attributes of time expressions are treated as symbolic features, rather than being decomposed into actual integer values representing dates and times. 2.2 Syntactic Features Our syntactic features draw upon the features previously shown to be effective for temporal relation classification (Bethard and Martin, 2007), including the following: • pos: the part-of-speech (pos) tags of the event and the time expression word tokens, assigned by Charniak parser. • gov-prep: any prepositions governing the event or time expression (e.g., ‘for’ in ‘for ten years’). • gov-verb: the verb governing the event or time expression, similar to govprep. • gov-verb-pos: the pos tag of the governing verb. We also investigate both full and partial syntactic paths between a pair of event and time expressions, but including these features does not improve the classification results on our development data set. 2.3 Lexical Relat</context>
</contexts>
<marker>Bethard, Martin, 2007</marker>
<rawString>S. Bethard and J. H. Martin. 2007. CU-TMP: temporal relation classification using syntactic and semantic features. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 129-132, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="4225" citStr="Charniak, 2000" startWordPosition="649" endWordPosition="650">e also employ phrase-based syntactic features (Bethard and 341 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 341–344, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Martin 2007) rather than dependency-based syntactic features. 2 Features We consider three types of features: basic, syntactic, and lexical relation features. Basic features represent the information directly available from the original data provided by the task organizer; syntactic features are extracted from syntactic parses generated by Charniak parser (Charniak, 2000); and lexical semantic relations that are derived from two external lexical databases, VERBOCEAN (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). 2.1 Basic Features Basic features include the word tokens, stems of the words, and the manually annotated attributes of events and time expressions. In the TempEval2 data, an event always consists of a single word token, but time expressions often consist of multiple tokens. We treat each word in time expressions as a different feature. For example, two word features, ‘this’ and ‘afternoon’, are extracted from a given time expression ‘this </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132-139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Cheng</author>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<title>NAIST.Japan: Temporal relation identification using dependency parsed tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>245--248</pages>
<location>Prague, Czech Republic.</location>
<marker>Cheng, Asahara, Matsumoto, 2007</marker>
<rawString>Y. Cheng, M. Asahara, and Y. Matsumoto. 2007. NAIST.Japan: Temporal relation identification using dependency parsed tree. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 245-248, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>P Pantel</author>
</authors>
<title>2004.VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.</title>
<date></date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>33--40</pages>
<location>Barcelona,</location>
<marker>Chklovski, Pantel, </marker>
<rawString>T. Chklovski and P. Pantel. 2004.VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 33-40, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="4379" citStr="Fellbaum, 1998" startWordPosition="671" endWordPosition="672">44, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Martin 2007) rather than dependency-based syntactic features. 2 Features We consider three types of features: basic, syntactic, and lexical relation features. Basic features represent the information directly available from the original data provided by the task organizer; syntactic features are extracted from syntactic parses generated by Charniak parser (Charniak, 2000); and lexical semantic relations that are derived from two external lexical databases, VERBOCEAN (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). 2.1 Basic Features Basic features include the word tokens, stems of the words, and the manually annotated attributes of events and time expressions. In the TempEval2 data, an event always consists of a single word token, but time expressions often consist of multiple tokens. We treat each word in time expressions as a different feature. For example, two word features, ‘this’ and ‘afternoon’, are extracted from a given time expression ‘this afternoon’. Stemming is done with the Porter Stemmer in NLTK (Loper and Bird, 2002). The value attributes of time expressions are treated as symbolic feat</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,</booktitle>
<pages>62--69</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4908" citStr="Loper and Bird, 2002" startWordPosition="759" endWordPosition="762">ernal lexical databases, VERBOCEAN (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). 2.1 Basic Features Basic features include the word tokens, stems of the words, and the manually annotated attributes of events and time expressions. In the TempEval2 data, an event always consists of a single word token, but time expressions often consist of multiple tokens. We treat each word in time expressions as a different feature. For example, two word features, ‘this’ and ‘afternoon’, are extracted from a given time expression ‘this afternoon’. Stemming is done with the Porter Stemmer in NLTK (Loper and Bird, 2002). The value attributes of time expressions are treated as symbolic features, rather than being decomposed into actual integer values representing dates and times. 2.2 Syntactic Features Our syntactic features draw upon the features previously shown to be effective for temporal relation classification (Bethard and Martin, 2007), including the following: • pos: the part-of-speech (pos) tags of the event and the time expression word tokens, assigned by Charniak parser. • gov-prep: any prepositions governing the event or time expression (e.g., ‘for’ in ‘for ten years’). • gov-verb: the verb govern</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>E. Loper and S. Bird. 2002. NLTK: The Natural Language Toolkit. In Proceedings of ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62–69, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>M Verhagen</author>
<author>B Wellner</author>
<author>C M Lee</author>
<author>J Pustejovsky</author>
</authors>
<title>Machine learning of temporal relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>753--760</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="7112" citStr="Mani et al. (2006)" startWordPosition="1111" endWordPosition="1114">sitive closure was calculated using the Warshall algorithm (Agrawal and Jagadish, 1990). The confidence score for the new arc was calculated as the average of the two constituents. The symmetric closure was calculated using a simple pass. The confidence score is the same as the reflected edge for symmetric relations. A set of VERBOCEAN features were calculated for each target event pair within each of the temporal relations tasks. Each verb was lemmatized using the WordNet lemmatizer in NLTK before being compared against the database. Rather than focusing only on HAPPENS-BEFORE relation as in Mani et al. (2006), we consider all five verb relations in two different versions, unweighted and weighted. The unweighted version is a binary feature indicating the existence of an arc between the two target verbs in VERBOCEAN. In the weighted version, the existence of an arc is weighted by the associated confidence score. In addition to VerbOcean, WordNet was used for its conceptual relations. WordNet is a large lexical database, which contains information on verbs, nouns, adjectives and adverbs, grouped into hierarchically organized cognitive synonym Relation Example SIMILARITY à† produce :: create STRENGTH </context>
</contexts>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J. Pustejovsky. 2006. Machine learning of temporal relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 753-760, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov Logic Networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<volume>62</volume>
<issue>1</issue>
<pages>107--136</pages>
<contexts>
<context position="2297" citStr="Richardson and Domingos, 2006" startWordPosition="359" endWordPosition="362">alysis of narrative stories written by middle school students, with the analysis being conducted a single story at a time. creation time data are not used because none of the other task models need them as features). It was expected that this approach would support more interesting comparisons with other systems that take a more sophisticated approach to the task. Further, we experiment with a joint modeling technique to examine if the communication with other task models brings a boost to a performance of the simple model. Taking a supervised machine-learning approach with Markov Logic (ML) (Richardson and Domingos, 2006), we constructed two systems, NCSU-INDI and NCSU-JOINT. NCSU-INDI consists of four independently trained classifiers, one for each task, whereas NCSU-JOINT models all four tasks jointly. The choice of ML as learning technique for temporal relations is motivated both theoretically and practically. Theoretically, it is a statistical relational learning framework that does not make the i.i.d. assumption for the data. This is a desirable characteristic for complex problems such as temporal relation classification, as well as many other natural language problems, in which the features representing </context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov Logic Networks. Machine Learning, 62(1): 107-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
</authors>
<title>Improving the accuracy and efficiency of MAP inference for Markov Logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence,</booktitle>
<pages>468--475</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="8926" citStr="Riedel, 2008" startWordPosition="1390" endWordPosition="1391">The synset chosen was simply the first synset returned by the reader. Similar to the VERBOCEAN features, we consider both unweighted and weighted versions of the feature. 3 The Systems ML is a probabilistic extension of first-order logic that allows formulae to be violated. It assigns a weight to each formula, reflecting the strength of the constraint represented by the formula. A Markov logic network (MLN) is a set of weighted first-order clauses, which, together with constants, defines a Markov network. We constructed two systems, NCSU-INDI and NCSU-JOINT using an off-the-shelf tool for ML (Riedel, 2008). 3.1 NCSU-INDI NCSU-INDI consists of four independently trained MLNs, one for each task. Each MLN is defined by a set of local formulae that are conjunctions of predicates representing the features. An example local formula used for Task C is eventTimex(e, t) eventWord(e, w) relEventTimex(e, t, r) (1) If a pair of event e and time expression t exists and the event consists of a word token w, formula (1) assigns a temporal relation t to the given pair of e and t with some weights. For each task, the features described in Section 2 were examined on a held-out development data set (about 10% of </context>
</contexts>
<marker>Riedel, 2008</marker>
<rawString>S. Riedel. 2008. Improving the accuracy and efficiency of MAP inference for Markov Logic. In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, pages 468-475, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yoshikawa</author>
<author>S Riedel</author>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<title>Jointly Identifying Temporal Relations with Markov Logic.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>405--413</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="3205" citStr="Yoshikawa et al., 2009" startWordPosition="501" endWordPosition="504">practically. Theoretically, it is a statistical relational learning framework that does not make the i.i.d. assumption for the data. This is a desirable characteristic for complex problems such as temporal relation classification, as well as many other natural language problems, in which the features representing a given problem are often correlated with one another. Practically, ML allows us to build both individual and joint models in a uniform framework; individual models can be easily combined together into a joint model with a set of global formulae governing over them. In previous work (Yoshikawa et al., 2009), ML was successfully applied to temporal relation classification task. Our approach is different from this work in two primary respects. First, we introduce new lexical relation features derived from English lexical ontologies. Second, our model addresses a new task introduced in TempEval-2, which is to identify temporal relations between main and syntactically dominated events in the same sentence. We also employ phrase-based syntactic features (Bethard and 341 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 341–344, Uppsala, Sweden, 15-16 July 2010. c�2</context>
</contexts>
<marker>Yoshikawa, Riedel, Asahara, Matsumoto, 2009</marker>
<rawString>K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsumoto. 2009. Jointly Identifying Temporal Relations with Markov Logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 405-413, Suntec, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>