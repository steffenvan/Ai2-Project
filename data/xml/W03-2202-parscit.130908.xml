<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000871">
<title confidence="0.9986065">
An Evaluation of a Lexicographer&apos;s Workbench: building lexicons for
Machine Translation
</title>
<author confidence="0.998235">
Rob Koeling Adam Kilgarriff, David Tugwell, Roger Evans
</author>
<affiliation confidence="0.988669">
COGS, University of Sussex ITRI, University of Brighton
</affiliation>
<bodyText confidence="0.810069">
robk@cogs . susx . ac . uk {adam, david, roger}@itri bton ac uk
</bodyText>
<sectionHeader confidence="0.979355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999800142857143">
NLP system developers and corpus lexicogra-
phers would both benefit from a tool for finding
and organizing the distinctive patterns of use of
words in texts. Such a tool would be an asset for
both language research and lexicon development,
particularly for lexicons for Machine Translation
(MT). We have developed the WASPBENCH, a
tool that (I) presents a &amp;quot;word sketch&amp;quot;, a sum-
mary of the corpus evidence for a word, to the
lexicographer; (2) supports the lexicographer in
analysing the word into its distinct meanings and
(3) uses the lexicographer&apos;s analysis as the input
to a state-of-the-art word sense disambiguation
algorithm, the output of which is a &amp;quot;word ex-
pert&amp;quot; for the word which can then disambiguate
new instances of the word. In this paper we de-
scribe a set of evaluation experiments, designed
to establish whether WASPBENCH can be used to
save time and improve performance in the devel-
opment of a lexicon for Machine Translation or
other NLP application.
</bodyText>
<sectionHeader confidence="0.991894" genericHeader="keywords">
1 Motivations
</sectionHeader>
<bodyText confidence="0.999967325581395">
On the one hand, Human Language Technologies
(HLT) need dictionaries, to tell them what words
mean and how they behave. On the other hand,
the people making dictionaries (herafter, lexicog-
raphers) need HLT, to help them identify how
words behave so they can make better dictionar-
ies. This potential for synergy exists across the
range of lexical data - in the construction of head-
word lists, for spelling correction, phonetics, mor-
phology and syntax, but nowhere is it truer than
for semantics, and in particular the vexed question
of how a word&apos;s meaning should be analysed into
distinct senses. HLT needs all the help it can get
from dictionaries, because it is a very hard prob-
lem to identify which meaning of a word applies,
and if the dictionary does not provide both a co-
herent and accurate analysis of what the meanings
are, and a good set of clues as to where each mean-
ing applies, then the enterprise is doomed. The
lexicographer needs all the help they can get be-
cause the analysis of meaning is the second hard-
est part of their job (Kilgarriff, 1998), it occupies
a large share of their working hours, and it is one
where, currently, they have very little to go on be-
yond intuition.
Synergy between HLT and lexicographer be-
comes a possibility with the advent of the cor-
pus. Lexicographers have long been aware of their
great need for evidence about how words behave.
The pioneering project was COBUILD (Sinclair,
1987) and its first offering to the world, the Collins
COBUILD English Dictionary came out in 1987.
The basic working methodology, in those early
days, was the &apos;coloured pens&apos; method. A lexicog-
rapher who was to write an entry for a word, say
pike, was given the corpus evidence for pike in the
form of a key-word-in-context printout. They then
read the corpus lines, identifying different mean-
ings as they went along, assigning a colour to each
meaning and marking each corpus line with the
appropriate colour. Once they had marked all (or
almost all - there are always anomalies) the corpus
lines, they could then go back to write a definition
</bodyText>
<page confidence="0.995494">
9
</page>
<bodyText confidence="0.999952">
for each sense, using, eg, the red corpus lines as
the evidence for the first meaning, the green as the
evidence for the second, and so on.
In this scenario, note that a meaning, or word
sense, corresponds to a cluster of corpus lines.
This is a representation that HLT can work with.
As corpus-based HLT took off, in the 1990s, re-
searchers such as (Gale et al., 1993) explored
corpus methods for word sense disambiguation
(WSD). Here the correspondence between word
senses and sets of corpus lines was taken at face
value, with a set of corpus lines which were known
to belong to a particular sense being used as a
training set. A machine-learning algorithm was
then able to use the training set to induce a word
expert which could decide which sense a new cor-
pus instance belonged to.
So the stage is set for software which both uses
HLT to support the corpus lexicographer in devel-
oping good meaning analyses, and uses the mean-
ing analysis, realised as corpus evidence, to sup-
port accurate WSD. This is what the WASPBENCH
aims to do.
</bodyText>
<subsectionHeader confidence="0.996959">
1.1 The WASPBENCH system
</subsectionHeader>
<bodyText confidence="0.990245753623188">
Behind the current implementation of the English
WASPBENCH lies a database of 70M instances of
grammatical relations for English. These are 5-
tuples:
&lt; gr amr el , w or dl , wor d2, particle pointer &gt;
gramrel can be any of a set of 27 core grammatical
relations for English (including subject, subject-
qt object, object-of modifier, and/or, PP-comp),
word] and word2 are words of English (nouns,
verbs or adjectives, lemmatized to give dictionary
headword form; word2 may be null), particle is
a particle or preposition, so that grammatical re-
lations involving prepositions as well as two fully
lexical arguments can be captured. For all rela-
tions except PP-comp it is null. Pointer points into
the corpus, so we can identify where the instance
occurs and retrieve its context if required. Exam-
ples of 5-tuples are
PP-comp,look,picture,at,1004683
object, sip, beer, -, 1005678
The database was prepared by parsing a lemma-
tised, part-of-speech-tagged version of the British
National Corpus, a 100M word corpus of recent
spoken and written British English.
Using this database, WASPBENCH prepares a
set of lists for each word] in which, for each
gramrel, the words which occur frequently and
with high mutual information as word2 are iden-
tified and sorted according to their lexicographic
salience. This set of lists is presented to the lexi-
cographer for whom it is a useful summary of the
word&apos;s behaviour. This is a word sketch (Kilgarriff
and Tugwell, 2001b).
The word sketch is a good starting point for
the lexicographer to analyse the different mean-
ings (step 1). They study it. All underlying corpus
evidence is available at a mouseclick, in case they
are unsure what contexts word] occurs in gram-
rel with word2 in. They reach preliminary opin-
ions about the different meanings the word has.
They assign a short mnemonic label to each sense,
and type the labels into a text-input box provided.
Hitting the &amp;quot;set senses&amp;quot; button updates the word
sketch, with each collocate now having a pull-
down menu through which it can be assigned to
one of the senses.
The lexicographer then spends some time —
typically some thirty minutes for a moderately
complicated word— assigning collocates to senses
(step 2). The majority of high-salience
&lt; collocate, g r amr el &gt; pairs relate to one sense
of a word only (in accordance with Yarowsky&apos;s
&amp;quot;one sense per collocation&amp;quot; dictum (Yarowsky,
1993)), and it is usually immediately evident
which sense is salient, so the task is not unduly
taxing. The lexicographer does not have to as-
sign all, or any particular, collocate, and any collo-
cate which is associated with more than one sense
should be left unassigned.
When the lexicographer has assigned a good
range of collocates, they press &amp;quot;submit&amp;quot;. The
WSD algorithm takes over, using the corpus in-
stances where the collocates assigned by the lex-
icographer apply as the clusters of instances cor-
responding to a sense, and bootstrapping further
evidence about how other corpus instances are as-
signed (step 3). The algorithm produces a word ex-
pert which can disambiguate new instances of the
http://info.ox.ac.uk/bnc
</bodyText>
<page confidence="0.993879">
10
</page>
<bodyText confidence="0.999542625">
provides even a remotely similar combina-
tion of inputs (corpus + human) and outputs
(meaning analysis + word expert). This
leaves us with no other products to compare
it with.
word. The algorithm currently in use is a reim-
plementation of Yarowski&apos;s decision list learner
(Yarowsky, 1995).
</bodyText>
<subsectionHeader confidence="0.872427">
1.2 WASPBENCH and Machine Translation
</subsectionHeader>
<bodyText confidence="0.977303">
WASPBENCH is designed particularly with the
needs of MT lexicography in mind. In that con-
text, the components of the problem take on a
slightly different form, sometimes with different
names. MT has long needed many rules of the
form,
in context C, translate source language
word S as target language word T
The problem has traditionally been that these rules
are hard for humans to identify, and, as there is a
large number of possible contexts for most words
and a large number of ambiguous words, a very
large number of rules is needed. In step (1), the
word sketch, WASPBENCH identifies and displays
to the user a good set of candidate rules but with
the target word T unspecified. In step (2), it sup-
ports the assignment of target words, by the lexi-
cographer, for a number of the rules. In step (3), it
takes this small set of rules and uses a bootstrap-
ping algorithm to automatically identify a very
large set of rules, so the word can be appropriately
translated wherever it occurs (Kilgarriff and Tug-
well, 2001a).
</bodyText>
<sectionHeader confidence="0.880007" genericHeader="introduction">
2 Evaluating WASPBENCH
</sectionHeader>
<bodyText confidence="0.990634333333333">
Evaluating how successful we have been in devel-
oping the WASPBENCH presents a number of chal-
lenges.
</bodyText>
<listItem confidence="0.89746871875">
• We straddle three communities - the (largely
commercial) dictionary-making world, the
(largely research) Human Language Technol-
ogy (and specifically, WSD) world, and the
(part commercial, part research) MT world,
all with very different ideas about what
makes a technology useful.
• There are no precedents. WASPBENCH
performs a function — corpus-based
disambiguating-lexicon development with
human input — which no other technology
performs. We believe no other technology
• On the lexicography front: human analysis
of meaning is decidedly &apos;craft&apos; (or even &apos;art&apos;)
rather than &apos;science&apos;. WASPBENCH is aiding
the practitioners of this craft in doing their
job better and faster. But, in the dictionary
world, even qualitative analyses of the rela-
tive merits of one meaning analysis as against
another are rare treats. Quantitative evalua-
tions are unheard of.
• A critical question for commercial MT would
be &amp;quot;does it take less time to produce a word
expert using WASPBENCH than using tradi-
tional methods, for the same quality of out-
put&amp;quot;. We are constrained in pursuing this
route because we do not have access to MT
companies&apos; lexicography budgets, and more-
over consider it unlikely that MT companies
would view the production of disambiguation
rules as a distinct function in the way that we
do.
</listItem>
<bodyText confidence="0.999956214285714">
In the light of these issues, we have adopted
a &apos;divide and rule&apos; strategy, setting up different
evaluation themes for different perspectives. We
have pursued five different evaluation strategies.
One of them is the subject of this paper.2 Of
the other strategies, we only mention the applica-
tion of word sketches within a large scale com-
mercial lexicography project here (the production
of Macmillan English Dictionary for Advanced
Learners) (Kilgarriff and Rundell, 2002). The set
of experiments that we report on in this paper
explored the performance of WASPBENCH-based
translations in comparison with translations pro-
duced by commercial MT systems.
</bodyText>
<sectionHeader confidence="0.997202" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.993671">
A group of twelve people were involved in the ex-
periment. All were students in translation studies
at the University of Leeds. None of them had a
</bodyText>
<footnote confidence="0.6952855">
2A report bringing together evidence from all evaluation
approaches is in preparation.
</footnote>
<page confidence="0.99209">
11
</page>
<figure confidence="0.455773">
Results for bank_n (eight) Enter yotu- own code: Subtak cheicei
No. Text Translation Which one is correct? Preference Alternative
OEM
</figure>
<bodyText confidence="0.982291625">
The region&apos;s earliest levees were built of sand dredged from
the river and piled high on the bank, where it would often melt
1. away with the next high flow Today&apos;s levees area Bank
patchwork of original, reinforced structures and newer •
carefully engineered with the finest design and materials
For the first lime since the San Joaquin River chewed through
the old levee on its north ....1 and sent its surging flood
waters his way, fanner Pete Andrew was ready to call it a
</bodyText>
<listItem confidence="0.641128">
2. Ufer
day. He had fought a maddening, 24—hour battle against a
</listItem>
<tableCaption confidence="0.632102636363636">
river that California agriculture had tamed for more than a half
century.
Contrary to his image back home in Gaza City ate wealthy
man about to invest half a million dollars, Abu Kernel&apos;s final
months were spent an meager surroundings. At the River
Oaks Motel on U S. 1 in Melbourne, Fla., he rented a
3. $150 —a—week room, and paid in $11:0 bills. Investgators said
they found no indication of the Swiss ,• accounts Abu
Kernel&apos;s family said he kept. The largest single amount of
money Abu Kamal appears to have spent since arraying in the
United States on Christmas Eve 1996 was $475.
</tableCaption>
<table confidence="0.9501752">
Bank
both neither WASPS
WASPS ( MT MT lifer
( unsure
( both ( neither ( WASPS
( WASPS ( MT ( MT I ???
( unsure
( both ( neither (WASPS
( WASPS ( MT ( MT [ ???
( unsure
</table>
<figureCaption confidence="0.999189">
Figure 1: Snapshot of the evaluation screen
</figureCaption>
<bodyText confidence="0.998965532258065">
specific background in lexicography. They were
all native or near-native speakers of both English
and the language they worked with for the ex-
periment. The students worked with Chinese (4),
French (3),German (2) and Italian (1).3
We asked the participants to work with the
WASPBENCH; creating word experts for the
selected words. This task gave us information
about how the users experienced using the work-
bench, either explicitly, by giving us feedback, or
implicitly by supplying us with data. This part
of the experiment created the word experts. The
other task was to evaluate the word experts. We
applied their word-experts to a set of previously
unseen test sentences and compared the output of
the WASPBENCH with the output of a commercial
MT system.
Creating the word experts The main task
for the participants was to use the WASPBENCH
to create word experts for a list of selected
ambiguous English words. The evaluation task
focussed on translation. The user was asked to
use the WASPBENCH in order to find out how the
word was used in English (i.e. as represented by
the BNC) and how the different uses of this word
would be translated into a target language of the
participant&apos;s choice. After the user has chosen
/Two more students worked with Japanese, but at the time
of the experiment we did not have the MT translations for
Japanese avaliable. Their word experts were evaluated in a
different way. We do not discuss these results in this paper.
the translations for the word and selected the
clues giving evidence for when the word should
receive a particular translation, the user submits
the data and the WASPBENCH infers further rules
to complete the word expert. The user is presented
the rule set and can manually inspect it. If they
are happy with the set, they can decide to submit
the word expert and continue with the next word.
If they are not happy with the rule set, they can
return to the wordsketch definition form and
add to or amend their input. After submitting,
the word expert is applied to a set of test sentences.
Assessing the results Evaluating a word ex-
pert is like evaluating the work of a translator.
The work of a translator can be judged by some-
one else, who can disagree on certain decisions
made by the translator. The disagreement can be
a matter of personal style. The assessment task
here involves the same kind of problem. In this
experimental paradigm we do not define before-
hand what the desired translation is. Every subject
may identify a different set of target translations
for each word and even if they work with the same
set, people might disagree on the preferred trans-
lation of a ord in a particular context. There is no
gold standard and thus we cannot evaluate the de-
cisions automatically. Therefore we asked the par-
ticipants to assess the word experts&apos; judgements.
The assessment task can best be introduced by
looking at a screenshot. In figure 1 we present
part of the evaluation screen with the results of ap-
</bodyText>
<page confidence="0.997195">
12
</page>
<bodyText confidence="0.999766375">
plying the word expert made by participant &apos;one&apos;
for the noun bank to the set of 45 test sentences.
The assessor is asked to enter their own number
for identification purposes. The second column
gives the test sentences with the word we are in-
terested in (here bank) highlighted. The third col-
umn presents the word expert&apos;s translation. The
assesser is asked to judge the correctness of the
translation in this particular context in the fourth
column. It was our intention to either include the
whole translated sentence as generated by the MT
system on the screen (with the target word high-
lighted) or just the translated target word. How-
ever, last minute technical problems made this im-
possible and we had to provide the MT system
output on paper. The assesser was asked to de-
cide which translation was correct in the given
context. The options given were &apos;WASPS&apos;, &apos;MT&apos;,
&apos;both&apos;, &apos;neither&apos;, &apos;unsure&apos; and combinations like
&apos;both correct, but WASPS preferable&apos;.
In case they disagree with the translation of-
fered, they can pick their preferred translation
from the pulldown menu in the fifth column (Al-
ternative). This pulldown menu offers all the
other suggested target translations for bank as de-
fined by participant &apos;one&apos;. In case the assesser
thinks the proper target translation is not avail-
able, their choice can be entered in the last column
(Other).
After judging all 45 test sentences, the assesser
is asked to submit the form by pressing the button
in the right upper corner.
</bodyText>
<subsectionHeader confidence="0.992383">
3.1 Instruction and Available Time
</subsectionHeader>
<bodyText confidence="0.999969363636364">
Most participants had not worked with the WASP-
BENCH before. They were given a theoretical in-
troduction and the opportunity afterwards to ex-
plore the user interface and its functionality by cre-
ating a word expert. The participants were allowed
plenty of time to create the word expert and play
with the WASPBENCH. They then applied the word
expert to a set of test sentences and inspected the
results, to conclude the introduction.
After the instruction session, approximately 4
days were allowed for working on the task: about
two days for creating word experts and two days
for assessment. The participants were instructed
to take their time to create the word experts, but to
keep in mind that we did not expect perfection. In
order to finish all 33 words in two working days,
only aproximately 30 minutes per word was avail-
able. We did not expect them to complete the full
list. To ensure that every word on the list would be
covered by equally many subjects, everyone was
asked to start at a different position in the list of
words.
</bodyText>
<subsectionHeader confidence="0.999357">
3.2 The Data
</subsectionHeader>
<bodyText confidence="0.998322055555556">
Words For the experiment we chose a set
of words that are clearly ambiguous in En-
glish. We only selected words that were fairly,
but not extremely, common (i.e. with 1,500 -
20,000 instances in the BNC). A total of 33
words were selected: 16 nouns, 10 verbs and
7 adjectives. Some of the words have just two
clearly distinct meanings in English, others have
more. There may of course also be further,
more subtle meaning distinctions. All of the
words were checked to confirm that the &apos;clearly
distinct meanings&apos; receive different translations
in at least one of the languages at our disposal
(Dutch, German and French). While we had
identified a set of meanings for the words in the
course of this process, this set was never shown
to the participants. They were asked to create
their own word expert with its own inventory
of meanings/translations. This might result in
different sets of target translation for different lan-
guages. In some languages two distinct different
meanings might be translated with the same word,
while subtle meaning differences might produce
different translations in the target language. It
is, of course, possible that, where more than one
participant was working on the same language,
they disagreed on the one set of target translations.
Test Data In order to test the performance of
the word experts, we selected for every word be-
tween 40 and 50 text fragments containing the tar-
get word. These fragments consisted of the com-
plete sentence in which the word occurred plus one
or two surrounding sentences. The test sentences
were selected from the North American News Text
Corpus.4 Random samples were taken from the
corpus and inspected for suitability. This was done
</bodyText>
<footnote confidence="0.867186">
4Available from the Linguistic Data Consortium.
</footnote>
<page confidence="0.994506">
13
</page>
<table confidence="0.999118">
Language Wasps MT both neither unsure
German 0.60 % (0.41) 0.28 % (0.09) 0.19 % 0.26 % 0.05 %
French 0.61 (0.24) 0.45 (0.07) 0.37 0.28 0.04
Chinese 0.68 (0.32) 0.42 (0.05) 0.37 0.23 0.03
Italian 0.67 (0.44) 0.29 (0.06) 0.23 0.22 0.05
All 0.64 (0.35) 0.36 (0.07) 0.29 0.25 0.04
</table>
<figureCaption confidence="0.974286">
Figure 2: WASPBENCH results compared with MT per language
</figureCaption>
<bodyText confidence="0.996316043478261">
to make sure that the samples were usable (some
samples, like words from headlines, did not have
much surrounding text) and to ensure that for ev-
ery identified distinct meaning there were at least
some test sentences available. If we had chosen
a large set of test sentences from the corpus, we
could have relied on pure random selection to take
care of the proper meaning distribution, but a con-
siderably larger sample than the 45 test sentences
taken here would be necessary to rely on that.
The fact that we used an American news corpus
for the test sentences and that the WASPBENCH
currently uses the BNC for creating the word
experts caused another problem: some words are
used differently in British and American English,
for example lot which has the &apos;parking space&apos;
meaning in American but not British English.
MT translation The MT translations were
produced with BabelFish from Systran.5 The in-
dividual fragments (i.e. the sentence wit the am-
biguous word in it plus 1 or 2 surrounding sen-
tences) were submitted as seperate paragraphs to
the translation engine.
</bodyText>
<sectionHeader confidence="0.865393" genericHeader="method">
4 Evaluation of the Results
</sectionHeader>
<bodyText confidence="0.999888857142857">
A total of 240 word experts were produced for
32 words.6 This means that an average of 7.5 word
experts per word are available. There were at least
5 different word experts for any word, the maxi-
mum number of word experts for one word is 10.
The results for the different words depend very
much on the perceived ambiguity of the word and
</bodyText>
<footnote confidence="0.79053775">
5 Available over the web via Altavista:
http://babelfish.altavista.com/
6We experienced problems with one of the nouns. The
data for this word (film&apos;) was discarded.
</footnote>
<bodyText confidence="0.999909516129032">
how closely related the different meanings for that
word are. For example, a noun like bank with
two clear and distinct meanings (&apos;financial insti-
tution&apos; and &apos;river bank&apos;) gave very good results,
while the results for very ambiguous words like
the noun line were quite poor. The table in figure
3 gives an overview of the results of applying the
word experts to the test sentences and comparing
the translation of the target word with the transla-
tion for that word given by the MT system. The
data is presented here per language. The figures
in bold face give the overall percentage of cases
were the WASPBENCH or the MT system was con-
sidered to be right. This number is the sum of the
percentage of cases were only WASPBENCH
/MT was right (percentage in brackets after the
bold face) and those cases where both were con-
sidered to have given the right translation.
The table in figure 3 presents the data per PoS
tag. This table shows that the WASPBENCH per-
forms slightly better on nouns (which is consistent
with the comments we got from the participants,
who thought that the nouns were less problematic
than the verbs and adjectives).
The data shows that the WASPBENCH results
consistently outperform the MT results by a con-
siderable margin. We do have to take into acccount
that the sample sentences in the test sets we used
here were not taken from one particular domain,
but a sample of general text. The gains for trans-
lating domain specific text might be less dramatic.
</bodyText>
<sectionHeader confidence="0.96547" genericHeader="method">
5 User Experience with the Workbench
</sectionHeader>
<bodyText confidence="0.999659666666667">
The evaluation task did not only provide data; it
also gave us feedback on working with the work-
bench. Many comments were given on the pre-
</bodyText>
<page confidence="0.994671">
14
</page>
<table confidence="0.99713125">
PoS Wasps MT both neither unsure
Noun 0.69 (0.34) 0.40 (0.06) 0.35 0.24 0.02
Verb 0.61 (0.29) 0.38 (0.05) 0.32 0.27 0.06
Adjective 0.63 (0.32) 0.41 (0.10) 0.31 0.24 0.04
</table>
<figureCaption confidence="0.965639">
Figure 3: WASPBENCH results compared with MT per Part of Speech
</figureCaption>
<bodyText confidence="0.999469666666667">
sentation of the data, missing navigation abilities,
buttons and correction facilities and other user-
interface issues. We will incorporate suggestions
into future releases of the workbench.
An important issue is that people have difficul-
ties with many of the grammatical relations, and
instead, focus on example sentences. This is time-
consuming and it would be better if we could clar-
ify the grammatical relations, either on the same
screen, or on demand (for example by making help
available).
A source of confusion and irritation is PoS tag-
ger errors and errors made in predicting the gram-
matical relations. It is clear that these components
are critical for the usability of the workbench.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="method">
6 Conclusions and Further Research
</sectionHeader>
<bodyText confidence="0.999995627450981">
We have already mentioned that the evaluation
experiment have provided us with valuable feed-
back on how people experience working with the
WASPBENCH, giving us the opportunity to further
develop the workbench. Several changes in the
user interface will be made and will improve the
usability of the tool. The main objective for this
particular experiment, however, was to investigate
how well the word-experts created with the WASP-
BENCH help to disambiguate words in a translation
task. These experiments show that with the WASP-
BENCH it is possible to create word sense disam-
biguation rules that help translation of ambiguous
words enormously without spending a whole lot of
time in creating these rules. The results show that
people, with no prior experience using the work-
bench, are able to create disambiguation rules that
outperformed a well-established MT system by a
great length, even though they had limited time to
spend on creating the rules and did not have the
opportunity to improve on their efforts.
While thinking about the WASPBENCH as a tool
for improving WSD for MT systems, one of the
questions we asked ourselves was: &amp;quot;does it take
less time to produce a word expert using WASP-
BENCH than using traditional methods, for the
same quality of output&amp;quot;. Even though we can&apos;t
answer this question, we do know now that we
can improve substantially upon the quality of the
output. We can also estimate the cost (in time or
money) to create disambiguation rules for all the
words and estimate the improvement in quality it
will give us.
Another important aspect of the evaluation re-
sults is the fact that the results for the different
languages are very similar. We feel that consis-
tency is important for a disambiguation tool. Even
though the word experts created by the partici-
pants will always be different, they should ideally
behave similarly. In another experiment (Koeling
and Kilgarriff, 2002) we looked explicitly at the
consistency of results by comparing word experts
(same word, same target language) made by sev-
eral people. In that experiment we found more ev-
idence for our consistency claim.
Even though we feel that these experiments
show that the WASPBENCH succesfully meets
many of the goals we had in mind when we de-
signed the workbench, there are still ways to im-
prove the current system. The fronts on which we
would like to develop the WASPBENCH include:
</bodyText>
<sectionHeader confidence="0.42691" genericHeader="method">
exploring alternative WSD algorithms
</sectionHeader>
<bodyText confidence="0.999763222222222">
(Yarowsky and Florian, 2002) show that
&amp;quot;winner-take-all&amp;quot; algorithms, are sometimes
preferable, but sometimes cumulative al-
gorithms, where evidence from different
clues is summed, perform better. We would
like to explore how we might match the
algorithm-type to the data instance.
interactivity Currently there is only minimal sup-
port for a &apos;second round&apos; of the lexicogra-
</bodyText>
<page confidence="0.992442">
15
</page>
<bodyText confidence="0.999992125">
pher revising their meaning analysis accord-
ing to the feedback provided by the WSD al-
gorithm. We would like the system to enter
a dialogue with the lexicographer, whereby it
identified anomalies and facilitated revisions
to the meaning analysis.
multiwords Although some fuctionality for mul-
tiwords is already supported, for phrasal
verbs and subcategorising nouns and adjec-
tives, through the three-argument RN:1)21, re-
lation, we would like to extend system func-
tionality by permitting the user to input mul-
tiwords, for which collocations would be
found.
thesaurus We have already produced a
thesaurus from the database (see
http://wasps.itri.bton.ac.uk), using Lin&apos;s
similarity measure (Lin, 1998). We would
like to use the thesaural classes in the word
sketches and elsewhere, so that evidence
from words in the same thesaural class could
be pooled, and inferences drawn where
two words were not encountered together,
but their thesaural classes had high mutual
information.
other languages Developments for a number of
languages other than English are under way.
Once we have two databases of grammati-
cal relations, based on comparable corpora,
for different languages, the potential for map-
ping tuples between the databases (using a
bilingual dictionary) arises.
new corpora there&apos;s no data like more data, and
both wordsketch production and the WSD
learning algorithm work better, the more they
are fed. Using the BNC, we have insuffi-
cient data to say much about words beyond
the commonest 20,000 in the language, and
miss many patterns. We are exploring using
the web (suitably filtered) as the input corpus.
</bodyText>
<sectionHeader confidence="0.995607" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99993475">
This work was supported by the UK EPSRC, un-
der the WASPS project, grant GR/M54971. We
would like to thank Prof. Tony Hartley from Leeds
University for organising the experiments.
</bodyText>
<sectionHeader confidence="0.98743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998637904761905">
COBUILD, 1987. The Collins COB UILD English
Language Dictionary. Edited by John McH. Sinclair
et at. London.
William Gale, Kenneth Church, and David Yarowsky.
1993. A method for disambiguating word senses
in a large corpus. Computers and the Humanities,
26(1-2):415-539.
Adam Kilgarriff and Michael Rundell. 2002. Lexical
profiling software and its lexicographical applica-
tions - a case study. In EURALEX 02, Copenhagen.
Adam Kilgarriff and David Tugwell. 2001a. Wasp-
bench: an MT lexicographer&apos;s workstation support-
ing state-of-the-art lexical disambiguation. In Proc.
MT Summit VIII, pages 187-190, Santiago de Com-
postela, Spain, September.
Adam Kilgarriff and David Tugwell. 2001b. Word
sketch: Extraction and display of significant colloca-
tions for lexicography. In Proc. Collocations work-
shop, ACL 2001, pages 32-38, Toulouse, France.
Adam Kilgarriff. 1998. The hard parts of lexi-
cography. International Journal of Lexicography,
11(1):51-54.
Rob Koeling and Adam Kilgarriff. 2002. Evaluat-
ing the waspbench, a lexicography tool incorporat-
ing word sense disambiguation. In Proceedings of
ICON 2002, Mumbai, India, December.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL, Montreal.
John M. Sinclair, editor. 1987. Looking Up: An Ac-
count of the COBUILD Project in Lexical Comput-
ing. Collins, London.
David Yarowsky and Radu Florian. 2002. Evalu-
ating sense disambiguation performance across di-
verse parameter spaces. Journal of Natural Lan-
guage Engineering, page In press. Special Issue on
Evaluating Word Sense Disambiguation Systems.
David Yarowsky. 1993. One sense per collocation.
In Proc. ARPA Human Language Technology Work-
shop, Princeton.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proc.
of ACL 1995, pages 189-196, Cambridge, MA.
</reference>
<page confidence="0.99867">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.094334">
<title confidence="0.9990665">An Evaluation of a Lexicographer&apos;s Workbench: building lexicons Machine Translation</title>
<author confidence="0.999876">Rob Koeling Adam Kilgarriff</author>
<author confidence="0.999876">David Tugwell</author>
<author confidence="0.999876">Roger Evans</author>
<affiliation confidence="0.986542">University of Sussex of</affiliation>
<abstract confidence="0.993080343589744">robk@cogs . susx . ac . uk {adam, david, roger}@itri bton ac uk Abstract developers and corpus lexicographers would both benefit from a tool for finding and organizing the distinctive patterns of use of words in texts. Such a tool would be an asset for both language research and lexicon development, particularly for lexicons for Machine Translation We have developed the tool that (I) presents a &amp;quot;word sketch&amp;quot;, a summary of the corpus evidence for a word, to the the lexicographer in analysing the word into its distinct meanings and (3) uses the lexicographer&apos;s analysis as the input to a state-of-the-art word sense disambiguation algorithm, the output of which is a &amp;quot;word expert&amp;quot; for the word which can then disambiguate new instances of the word. In this paper we describe a set of evaluation experiments, designed establish whether be used to save time and improve performance in the development of a lexicon for Machine Translation or other NLP application. 1 Motivations On the one hand, Human Language Technologies (HLT) need dictionaries, to tell them what words mean and how they behave. On the other hand, the people making dictionaries (herafter, lexicographers) need HLT, to help them identify how words behave so they can make better dictionaries. This potential for synergy exists across the range of lexical data in the construction of headlists, for spelling correction, phonetics, morphology and syntax, but nowhere is it truer than for semantics, and in particular the vexed question of how a word&apos;s meaning should be analysed into distinct senses. HLT needs all the help it can get from dictionaries, because it is a very hard problem to identify which meaning of a word applies, and if the dictionary does not provide both a coherent and accurate analysis of what the meanings are, and a good set of clues as to where each meaning applies, then the enterprise is doomed. The lexicographer needs all the help they can get because the analysis of meaning is the second hardest part of their job (Kilgarriff, 1998), it occupies a large share of their working hours, and it is one where, currently, they have very little to go on beyond intuition. Synergy between HLT and lexicographer becomes a possibility with the advent of the corpus. Lexicographers have long been aware of their great need for evidence about how words behave. The pioneering project was COBUILD (Sinclair, 1987) and its first offering to the world, the Collins COBUILD English Dictionary came out in 1987. The basic working methodology, in those early days, was the &apos;coloured pens&apos; method. A lexicographer who was to write an entry for a word, say given the corpus evidence for the form of a key-word-in-context printout. They then read the corpus lines, identifying different meanings as they went along, assigning a colour to each meaning and marking each corpus line with the appropriate colour. Once they had marked all (or almost all there are always anomalies) the corpus lines, they could then go back to write a definition 9 for each sense, using, eg, the red corpus lines as the evidence for the first meaning, the green as the evidence for the second, and so on. In this scenario, note that a meaning, or word sense, corresponds to a cluster of corpus lines. This is a representation that HLT can work with. As corpus-based HLT took off, in the 1990s, researchers such as (Gale et al., 1993) explored corpus methods for word sense disambiguation (WSD). Here the correspondence between word senses and sets of corpus lines was taken at face value, with a set of corpus lines which were known to belong to a particular sense being used as a training set. A machine-learning algorithm was then able to use the training set to induce a word expert which could decide which sense a new corpus instance belonged to. So the stage is set for software which both uses HLT to support the corpus lexicographer in developing good meaning analyses, and uses the meaning analysis, realised as corpus evidence, to supaccurate WSD. This is what the aims to do. The Behind the current implementation of the English a database of 70M instances of grammatical relations for English. These are 5tuples: &lt; gr amr el , w or dl , wor d2, particle pointer &gt; be any of a set of 27 core grammatical for English (including subjectqt object, object-of modifier, and/or, PP-comp), words of English (nouns, verbs or adjectives, lemmatized to give dictionary form; be null), a particle or preposition, so that grammatical relations involving prepositions as well as two fully lexical arguments can be captured. For all relaexcept is null. into the corpus, so we can identify where the instance occurs and retrieve its context if required. Examples of 5-tuples are PP-comp,look,picture,at,1004683 object, sip, beer, -, 1005678 The database was prepared by parsing a lemmatised, part-of-speech-tagged version of the British National Corpus, a 100M word corpus of recent spoken and written British English. this database, a of lists for each which, for each words which occur frequently and high mutual information as identified and sorted according to their lexicographic salience. This set of lists is presented to the lexicographer for whom it is a useful summary of the behaviour. This is a sketch and Tugwell, 2001b). The word sketch is a good starting point for the lexicographer to analyse the different meanings (step 1). They study it. All underlying corpus evidence is available at a mouseclick, in case they unsure what contexts in gram- They reach preliminary opinions about the different meanings the word has. They assign a short mnemonic label to each sense, and type the labels into a text-input box provided. Hitting the &amp;quot;set senses&amp;quot; button updates the word sketch, with each collocate now having a pulldown menu through which it can be assigned to one of the senses. The lexicographer then spends some time — typically some thirty minutes for a moderately complicated word— assigning collocates to senses (step 2). The majority of high-salience collocate, g r amr el &gt; relate to one sense of a word only (in accordance with Yarowsky&apos;s &amp;quot;one sense per collocation&amp;quot; dictum (Yarowsky, 1993)), and it is usually immediately evident which sense is salient, so the task is not unduly taxing. The lexicographer does not have to assign all, or any particular, collocate, and any collocate which is associated with more than one sense should be left unassigned. When the lexicographer has assigned a good range of collocates, they press &amp;quot;submit&amp;quot;. The WSD algorithm takes over, using the corpus instances where the collocates assigned by the lexicographer apply as the clusters of instances corresponding to a sense, and bootstrapping further evidence about how other corpus instances are as- (step 3). The algorithm produces a excan disambiguate new instances of the http://info.ox.ac.uk/bnc 10 provides even a remotely similar combination of inputs (corpus + human) and outputs (meaning analysis + word expert). This leaves us with no other products to compare it with. word. The algorithm currently in use is a reimplementation of Yarowski&apos;s decision list learner (Yarowsky, 1995). Machine Translation designed particularly with the needs of MT lexicography in mind. In that context, the components of the problem take on a slightly different form, sometimes with different names. MT has long needed many rules of the form, context source language target language word The problem has traditionally been that these rules are hard for humans to identify, and, as there is a large number of possible contexts for most words and a large number of ambiguous words, a very large number of rules is needed. In step (1), the sketch, and displays to the user a good set of candidate rules but with the target word T unspecified. In step (2), it supports the assignment of target words, by the lexicographer, for a number of the rules. In step (3), it takes this small set of rules and uses a bootstrapping algorithm to automatically identify a very large set of rules, so the word can be appropriately translated wherever it occurs (Kilgarriff and Tugwell, 2001a). Evaluating Evaluating how successful we have been in develthe a number of challenges. • We straddle three communities the (largely commercial) dictionary-making world, the (largely research) Human Language Technology (and specifically, WSD) world, and the (part commercial, part research) MT world, all with very different ideas about what makes a technology useful. There are no precedents. performs a function — corpus-based disambiguating-lexicon development with human input — which no other technology performs. We believe no other technology • On the lexicography front: human analysis of meaning is decidedly &apos;craft&apos; (or even &apos;art&apos;) than &apos;science&apos;. aiding the practitioners of this craft in doing their job better and faster. But, in the dictionary world, even qualitative analyses of the relative merits of one meaning analysis as against another are rare treats. Quantitative evaluations are unheard of. • A critical question for commercial MT would be &amp;quot;does it take less time to produce a word using using traditional methods, for the same quality of output&amp;quot;. We are constrained in pursuing this route because we do not have access to MT companies&apos; lexicography budgets, and moreover consider it unlikely that MT companies would view the production of disambiguation rules as a distinct function in the way that we do. In the light of these issues, we have adopted a &apos;divide and rule&apos; strategy, setting up different evaluation themes for different perspectives. We have pursued five different evaluation strategies. of them is the subject of this Of the other strategies, we only mention the application of word sketches within a large scale commercial lexicography project here (the production of Macmillan English Dictionary for Advanced Learners) (Kilgarriff and Rundell, 2002). The set of experiments that we report on in this paper explored the performance of WASPBENCH-based translations in comparison with translations produced by commercial MT systems. 3 Experimental setup A group of twelve people were involved in the experiment. All were students in translation studies at the University of Leeds. None of them had a report bringing together evidence from all evaluation approaches is in preparation. 11 for bank_n (eight) yotuown code:Subtak cheicei No. Text Translation Which one is correct? Preference Alternative OEM The region&apos;s earliest levees were built of sand dredged from the river and piled high on the bank, where it would often melt away with the next high flow Today&apos;s levees area of original, reinforced structures and newer• carefully engineered with the finest design and materials For the first lime since the San Joaquin River chewed through old levee on its north....1and sent its surging flood waters his way, fanner Pete Andrew was ready to call it a 2. day. He had fought a maddening, 24—hour battle against a river that California agriculture had tamed for more than a half century. to his image back home in City ate wealthy man about to invest half a million dollars, Abu Kernel&apos;s final months were spent an meager surroundings. At the River Oaks Motel on U S. 1 in Melbourne, Fla., he rented a 3. $150 —a—week room, and paid in $11:0 bills. Investgators said found no indication of the Swiss,•accounts Kernel&apos;s family said he kept. The largest single amount of money Abu Kamal appears to have spent since arraying in the States on Christmas Eve 1996 was$475. Bank both neither WASPS WASPS ( MT MT ( unsure ( both ( neither ( WASPS WASPS ( MT ( MTI ( unsure ( both ( neither (WASPS WASPS ( MT ( MT[ ( unsure Figure 1: Snapshot of the evaluation screen specific background in lexicography. They were all native or near-native speakers of both English and the language they worked with for the experiment. The students worked with Chinese (4), (3),German (2) and Italian We asked the participants to work with the word experts for the selected words. This task gave us information about how the users experienced using the workbench, either explicitly, by giving us feedback, or implicitly by supplying us with data. This part of the experiment created the word experts. The other task was to evaluate the word experts. We applied their word-experts to a set of previously unseen test sentences and compared the output of the output of a commercial MT system. Creating the word experts The main task the participants was to use the to create word experts for a list of selected ambiguous English words. The evaluation task focussed on translation. The user was asked to the order to find out how the word was used in English (i.e. as represented by the BNC) and how the different uses of this word would be translated into a target language of the participant&apos;s choice. After the user has chosen more students worked with Japanese, but at the time of the experiment we did not have the MT translations for Japanese avaliable. Their word experts were evaluated in a different way. We do not discuss these results in this paper. the translations for the word and selected the clues giving evidence for when the word should receive a particular translation, the user submits data and the further rules to complete the word expert. The user is presented the rule set and can manually inspect it. If they are happy with the set, they can decide to submit the word expert and continue with the next word. If they are not happy with the rule set, they can return to the wordsketch definition form and add to or amend their input. After submitting, the word expert is applied to a set of test sentences. the results Evaluating a word expert is like evaluating the work of a translator. The work of a translator can be judged by someone else, who can disagree on certain decisions made by the translator. The disagreement can be a matter of personal style. The assessment task here involves the same kind of problem. In this experimental paradigm we do not define beforehand what the desired translation is. Every subject may identify a different set of target translations for each word and even if they work with the same set, people might disagree on the preferred translation of a ord in a particular context. There is no gold standard and thus we cannot evaluate the decisions automatically. Therefore we asked the participants to assess the word experts&apos; judgements. The assessment task can best be introduced by looking at a screenshot. In figure 1 we present of the evaluation screen with the results of ap- 12 plying the word expert made by participant &apos;one&apos; the noun the set of 45 test sentences. The assessor is asked to enter their own number for identification purposes. The second column gives the test sentences with the word we are inin (here The third column presents the word expert&apos;s translation. The assesser is asked to judge the correctness of the translation in this particular context in the fourth column. It was our intention to either include the whole translated sentence as generated by the MT system on the screen (with the target word highlighted) or just the translated target word. However, last minute technical problems made this impossible and we had to provide the MT system output on paper. The assesser was asked to decide which translation was correct in the given context. The options given were &apos;WASPS&apos;, &apos;MT&apos;, &apos;both&apos;, &apos;neither&apos;, &apos;unsure&apos; and combinations like &apos;both correct, but WASPS preferable&apos;. In case they disagree with the translation offered, they can pick their preferred translation the pulldown menu in the fifth column (Alpulldown menu offers all the suggested target translations for defined by participant &apos;one&apos;. In case the assesser thinks the proper target translation is not available, their choice can be entered in the last column (Other). After judging all 45 test sentences, the assesser is asked to submit the form by pressing the button in the right upper corner. 3.1 Instruction and Available Time participants had not worked with the WASP- They were given a theoretical introduction and the opportunity afterwards to explore the user interface and its functionality by creating a word expert. The participants were allowed plenty of time to create the word expert and play the then applied the word expert to a set of test sentences and inspected the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>COBUILD</author>
</authors>
<title>The Collins COB UILD English Language Dictionary. Edited by John McH. Sinclair et at.</title>
<date>1987</date>
<location>London.</location>
<marker>COBUILD, 1987</marker>
<rawString>COBUILD, 1987. The Collins COB UILD English Language Dictionary. Edited by John McH. Sinclair et at. London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1993</date>
<pages>26--1</pages>
<contexts>
<context position="3694" citStr="Gale et al., 1993" startWordPosition="632" endWordPosition="635"> as they went along, assigning a colour to each meaning and marking each corpus line with the appropriate colour. Once they had marked all (or almost all - there are always anomalies) the corpus lines, they could then go back to write a definition 9 for each sense, using, eg, the red corpus lines as the evidence for the first meaning, the green as the evidence for the second, and so on. In this scenario, note that a meaning, or word sense, corresponds to a cluster of corpus lines. This is a representation that HLT can work with. As corpus-based HLT took off, in the 1990s, researchers such as (Gale et al., 1993) explored corpus methods for word sense disambiguation (WSD). Here the correspondence between word senses and sets of corpus lines was taken at face value, with a set of corpus lines which were known to belong to a particular sense being used as a training set. A machine-learning algorithm was then able to use the training set to induce a word expert which could decide which sense a new corpus instance belonged to. So the stage is set for software which both uses HLT to support the corpus lexicographer in developing good meaning analyses, and uses the meaning analysis, realised as corpus evide</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1993</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1993. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26(1-2):415-539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Michael Rundell</author>
</authors>
<title>Lexical profiling software and its lexicographical applications - a case study.</title>
<date>2002</date>
<booktitle>In EURALEX 02,</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="10736" citStr="Kilgarriff and Rundell, 2002" startWordPosition="1803" endWordPosition="1806">udgets, and moreover consider it unlikely that MT companies would view the production of disambiguation rules as a distinct function in the way that we do. In the light of these issues, we have adopted a &apos;divide and rule&apos; strategy, setting up different evaluation themes for different perspectives. We have pursued five different evaluation strategies. One of them is the subject of this paper.2 Of the other strategies, we only mention the application of word sketches within a large scale commercial lexicography project here (the production of Macmillan English Dictionary for Advanced Learners) (Kilgarriff and Rundell, 2002). The set of experiments that we report on in this paper explored the performance of WASPBENCH-based translations in comparison with translations produced by commercial MT systems. 3 Experimental setup A group of twelve people were involved in the experiment. All were students in translation studies at the University of Leeds. None of them had a 2A report bringing together evidence from all evaluation approaches is in preparation. 11 Results for bank_n (eight) Enter yotu- own code: Subtak cheicei No. Text Translation Which one is correct? Preference Alternative OEM The region&apos;s earliest levees</context>
</contexts>
<marker>Kilgarriff, Rundell, 2002</marker>
<rawString>Adam Kilgarriff and Michael Rundell. 2002. Lexical profiling software and its lexicographical applications - a case study. In EURALEX 02, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>David Tugwell</author>
</authors>
<title>Waspbench: an MT lexicographer&apos;s workstation supporting state-of-the-art lexical disambiguation.</title>
<date>2001</date>
<booktitle>In Proc. MT Summit VIII,</booktitle>
<pages>187--190</pages>
<location>Santiago de Compostela, Spain,</location>
<contexts>
<context position="5845" citStr="Kilgarriff and Tugwell, 2001" startWordPosition="993" endWordPosition="996">icture,at,1004683 object, sip, beer, -, 1005678 The database was prepared by parsing a lemmatised, part-of-speech-tagged version of the British National Corpus, a 100M word corpus of recent spoken and written British English. Using this database, WASPBENCH prepares a set of lists for each word] in which, for each gramrel, the words which occur frequently and with high mutual information as word2 are identified and sorted according to their lexicographic salience. This set of lists is presented to the lexicographer for whom it is a useful summary of the word&apos;s behaviour. This is a word sketch (Kilgarriff and Tugwell, 2001b). The word sketch is a good starting point for the lexicographer to analyse the different meanings (step 1). They study it. All underlying corpus evidence is available at a mouseclick, in case they are unsure what contexts word] occurs in gramrel with word2 in. They reach preliminary opinions about the different meanings the word has. They assign a short mnemonic label to each sense, and type the labels into a text-input box provided. Hitting the &amp;quot;set senses&amp;quot; button updates the word sketch, with each collocate now having a pulldown menu through which it can be assigned to one of the senses. </context>
<context position="8837" citStr="Kilgarriff and Tugwell, 2001" startWordPosition="1502" endWordPosition="1506">y, and, as there is a large number of possible contexts for most words and a large number of ambiguous words, a very large number of rules is needed. In step (1), the word sketch, WASPBENCH identifies and displays to the user a good set of candidate rules but with the target word T unspecified. In step (2), it supports the assignment of target words, by the lexicographer, for a number of the rules. In step (3), it takes this small set of rules and uses a bootstrapping algorithm to automatically identify a very large set of rules, so the word can be appropriately translated wherever it occurs (Kilgarriff and Tugwell, 2001a). 2 Evaluating WASPBENCH Evaluating how successful we have been in developing the WASPBENCH presents a number of challenges. • We straddle three communities - the (largely commercial) dictionary-making world, the (largely research) Human Language Technology (and specifically, WSD) world, and the (part commercial, part research) MT world, all with very different ideas about what makes a technology useful. • There are no precedents. WASPBENCH performs a function — corpus-based disambiguating-lexicon development with human input — which no other technology performs. We believe no other technolo</context>
</contexts>
<marker>Kilgarriff, Tugwell, 2001</marker>
<rawString>Adam Kilgarriff and David Tugwell. 2001a. Waspbench: an MT lexicographer&apos;s workstation supporting state-of-the-art lexical disambiguation. In Proc. MT Summit VIII, pages 187-190, Santiago de Compostela, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>David Tugwell</author>
</authors>
<title>Word sketch: Extraction and display of significant collocations for lexicography.</title>
<date>2001</date>
<booktitle>In Proc. Collocations workshop, ACL</booktitle>
<pages>32--38</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="5845" citStr="Kilgarriff and Tugwell, 2001" startWordPosition="993" endWordPosition="996">icture,at,1004683 object, sip, beer, -, 1005678 The database was prepared by parsing a lemmatised, part-of-speech-tagged version of the British National Corpus, a 100M word corpus of recent spoken and written British English. Using this database, WASPBENCH prepares a set of lists for each word] in which, for each gramrel, the words which occur frequently and with high mutual information as word2 are identified and sorted according to their lexicographic salience. This set of lists is presented to the lexicographer for whom it is a useful summary of the word&apos;s behaviour. This is a word sketch (Kilgarriff and Tugwell, 2001b). The word sketch is a good starting point for the lexicographer to analyse the different meanings (step 1). They study it. All underlying corpus evidence is available at a mouseclick, in case they are unsure what contexts word] occurs in gramrel with word2 in. They reach preliminary opinions about the different meanings the word has. They assign a short mnemonic label to each sense, and type the labels into a text-input box provided. Hitting the &amp;quot;set senses&amp;quot; button updates the word sketch, with each collocate now having a pulldown menu through which it can be assigned to one of the senses. </context>
<context position="8837" citStr="Kilgarriff and Tugwell, 2001" startWordPosition="1502" endWordPosition="1506">y, and, as there is a large number of possible contexts for most words and a large number of ambiguous words, a very large number of rules is needed. In step (1), the word sketch, WASPBENCH identifies and displays to the user a good set of candidate rules but with the target word T unspecified. In step (2), it supports the assignment of target words, by the lexicographer, for a number of the rules. In step (3), it takes this small set of rules and uses a bootstrapping algorithm to automatically identify a very large set of rules, so the word can be appropriately translated wherever it occurs (Kilgarriff and Tugwell, 2001a). 2 Evaluating WASPBENCH Evaluating how successful we have been in developing the WASPBENCH presents a number of challenges. • We straddle three communities - the (largely commercial) dictionary-making world, the (largely research) Human Language Technology (and specifically, WSD) world, and the (part commercial, part research) MT world, all with very different ideas about what makes a technology useful. • There are no precedents. WASPBENCH performs a function — corpus-based disambiguating-lexicon development with human input — which no other technology performs. We believe no other technolo</context>
</contexts>
<marker>Kilgarriff, Tugwell, 2001</marker>
<rawString>Adam Kilgarriff and David Tugwell. 2001b. Word sketch: Extraction and display of significant collocations for lexicography. In Proc. Collocations workshop, ACL 2001, pages 32-38, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>The hard parts of lexicography.</title>
<date>1998</date>
<journal>International Journal of Lexicography,</journal>
<pages>11--1</pages>
<contexts>
<context position="2316" citStr="Kilgarriff, 1998" startWordPosition="390" endWordPosition="391">yntax, but nowhere is it truer than for semantics, and in particular the vexed question of how a word&apos;s meaning should be analysed into distinct senses. HLT needs all the help it can get from dictionaries, because it is a very hard problem to identify which meaning of a word applies, and if the dictionary does not provide both a coherent and accurate analysis of what the meanings are, and a good set of clues as to where each meaning applies, then the enterprise is doomed. The lexicographer needs all the help they can get because the analysis of meaning is the second hardest part of their job (Kilgarriff, 1998), it occupies a large share of their working hours, and it is one where, currently, they have very little to go on beyond intuition. Synergy between HLT and lexicographer becomes a possibility with the advent of the corpus. Lexicographers have long been aware of their great need for evidence about how words behave. The pioneering project was COBUILD (Sinclair, 1987) and its first offering to the world, the Collins COBUILD English Dictionary came out in 1987. The basic working methodology, in those early days, was the &apos;coloured pens&apos; method. A lexicographer who was to write an entry for a word,</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. The hard parts of lexicography. International Journal of Lexicography, 11(1):51-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Evaluating the waspbench, a lexicography tool incorporating word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of ICON 2002,</booktitle>
<location>Mumbai, India,</location>
<contexts>
<context position="26417" citStr="Koeling and Kilgarriff, 2002" startWordPosition="4500" endWordPosition="4503"> can&apos;t answer this question, we do know now that we can improve substantially upon the quality of the output. We can also estimate the cost (in time or money) to create disambiguation rules for all the words and estimate the improvement in quality it will give us. Another important aspect of the evaluation results is the fact that the results for the different languages are very similar. We feel that consistency is important for a disambiguation tool. Even though the word experts created by the participants will always be different, they should ideally behave similarly. In another experiment (Koeling and Kilgarriff, 2002) we looked explicitly at the consistency of results by comparing word experts (same word, same target language) made by several people. In that experiment we found more evidence for our consistency claim. Even though we feel that these experiments show that the WASPBENCH succesfully meets many of the goals we had in mind when we designed the workbench, there are still ways to improve the current system. The fronts on which we would like to develop the WASPBENCH include: exploring alternative WSD algorithms (Yarowsky and Florian, 2002) show that &amp;quot;winner-take-all&amp;quot; algorithms, are sometimes prefe</context>
</contexts>
<marker>Koeling, Kilgarriff, 2002</marker>
<rawString>Rob Koeling and Adam Kilgarriff. 2002. Evaluating the waspbench, a lexicography tool incorporating word sense disambiguation. In Proceedings of ICON 2002, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="28001" citStr="Lin, 1998" startWordPosition="4749" endWordPosition="4750">ithm. We would like the system to enter a dialogue with the lexicographer, whereby it identified anomalies and facilitated revisions to the meaning analysis. multiwords Although some fuctionality for multiwords is already supported, for phrasal verbs and subcategorising nouns and adjectives, through the three-argument RN:1)21, relation, we would like to extend system functionality by permitting the user to input multiwords, for which collocations would be found. thesaurus We have already produced a thesaurus from the database (see http://wasps.itri.bton.ac.uk), using Lin&apos;s similarity measure (Lin, 1998). We would like to use the thesaural classes in the word sketches and elsewhere, so that evidence from words in the same thesaural class could be pooled, and inferences drawn where two words were not encountered together, but their thesaural classes had high mutual information. other languages Developments for a number of languages other than English are under way. Once we have two databases of grammatical relations, based on comparable corpora, for different languages, the potential for mapping tuples between the databases (using a bilingual dictionary) arises. new corpora there&apos;s no data lik</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of ACL, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Sinclair</author>
<author>editor</author>
</authors>
<date>1987</date>
<booktitle>Looking Up: An Account of the COBUILD Project in Lexical Computing.</booktitle>
<publisher>Collins,</publisher>
<location>London.</location>
<marker>Sinclair, editor, 1987</marker>
<rawString>John M. Sinclair, editor. 1987. Looking Up: An Account of the COBUILD Project in Lexical Computing. Collins, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation performance across diverse parameter spaces.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>page</pages>
<note>In press. Special Issue on</note>
<contexts>
<context position="26957" citStr="Yarowsky and Florian, 2002" startWordPosition="4591" endWordPosition="4594">y should ideally behave similarly. In another experiment (Koeling and Kilgarriff, 2002) we looked explicitly at the consistency of results by comparing word experts (same word, same target language) made by several people. In that experiment we found more evidence for our consistency claim. Even though we feel that these experiments show that the WASPBENCH succesfully meets many of the goals we had in mind when we designed the workbench, there are still ways to improve the current system. The fronts on which we would like to develop the WASPBENCH include: exploring alternative WSD algorithms (Yarowsky and Florian, 2002) show that &amp;quot;winner-take-all&amp;quot; algorithms, are sometimes preferable, but sometimes cumulative algorithms, where evidence from different clues is summed, perform better. We would like to explore how we might match the algorithm-type to the data instance. interactivity Currently there is only minimal support for a &apos;second round&apos; of the lexicogra15 pher revising their meaning analysis according to the feedback provided by the WSD algorithm. We would like the system to enter a dialogue with the lexicographer, whereby it identified anomalies and facilitated revisions to the meaning analysis. multiwor</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation performance across diverse parameter spaces. Journal of Natural Language Engineering, page In press. Special Issue on Evaluating Word Sense Disambiguation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proc. ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton.</location>
<contexts>
<context position="6772" citStr="Yarowsky, 1993" startWordPosition="1153" endWordPosition="1154">ent meanings the word has. They assign a short mnemonic label to each sense, and type the labels into a text-input box provided. Hitting the &amp;quot;set senses&amp;quot; button updates the word sketch, with each collocate now having a pulldown menu through which it can be assigned to one of the senses. The lexicographer then spends some time — typically some thirty minutes for a moderately complicated word— assigning collocates to senses (step 2). The majority of high-salience &lt; collocate, g r amr el &gt; pairs relate to one sense of a word only (in accordance with Yarowsky&apos;s &amp;quot;one sense per collocation&amp;quot; dictum (Yarowsky, 1993)), and it is usually immediately evident which sense is salient, so the task is not unduly taxing. The lexicographer does not have to assign all, or any particular, collocate, and any collocate which is associated with more than one sense should be left unassigned. When the lexicographer has assigned a good range of collocates, they press &amp;quot;submit&amp;quot;. The WSD algorithm takes over, using the corpus instances where the collocates assigned by the lexicographer apply as the clusters of instances corresponding to a sense, and bootstrapping further evidence about how other corpus instances are assigned</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proc. ARPA Human Language Technology Workshop, Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="7779" citStr="Yarowsky, 1995" startWordPosition="1318" endWordPosition="1319">orpus instances where the collocates assigned by the lexicographer apply as the clusters of instances corresponding to a sense, and bootstrapping further evidence about how other corpus instances are assigned (step 3). The algorithm produces a word expert which can disambiguate new instances of the http://info.ox.ac.uk/bnc 10 provides even a remotely similar combination of inputs (corpus + human) and outputs (meaning analysis + word expert). This leaves us with no other products to compare it with. word. The algorithm currently in use is a reimplementation of Yarowski&apos;s decision list learner (Yarowsky, 1995). 1.2 WASPBENCH and Machine Translation WASPBENCH is designed particularly with the needs of MT lexicography in mind. In that context, the components of the problem take on a slightly different form, sometimes with different names. MT has long needed many rules of the form, in context C, translate source language word S as target language word T The problem has traditionally been that these rules are hard for humans to identify, and, as there is a large number of possible contexts for most words and a large number of ambiguous words, a very large number of rules is needed. In step (1), the wor</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of ACL 1995, pages 189-196, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>