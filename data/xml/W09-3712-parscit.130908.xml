<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.985918">
An Extensible Toolkit for Computational Semantics
</title>
<author confidence="0.993548">
Dan Garrette Ewan Klein
</author>
<affiliation confidence="0.994895">
dhgarrette@gmail.com University of Edinburgh
</affiliation>
<email confidence="0.996866">
ewan@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998445" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798689655172">
In this paper we focus on the software for computational semantics pro-
vided by the Python-based Natural Language Toolkit (NLTK). The seman-
tics modules in NLTK are inspired in large part by the approach developed in
Blackburn and Bos (2005) (henceforth referred to as B&amp;B). Since Blackburn
and Bos have also provided a software suite to accompany their excellent
textbook, one might ask what the justification is for the NLTK offering, which
is similarly slanted towards teaching computational semantics.
This question can be answered in a number of ways. First, we believe
there is intrinsic merit in the availability of different software tools for se-
mantic analysis, even when there is some duplication of coverage; and this
will become more true as computational semantics starts to be as widely
studied as computational syntax. For example, one rarely hears the ob-
jection that too many implementations of syntactic parsers are available.
Moreover, the NLTK software significantly goes beyond B&amp;B in providing
an implementation of Glue Semantics.
Second, whatever the relative merits of Prolog vs. Python as program-
ming languages, there is surely an advantage in offering students and in-
structors a choice in this respect. Given that many students have either
already been exposed to Java, or else have had no programming experience
at all, Python offers them the option of accomplishing interesting results
with only a shallow learning curve.
Third, NLTK is a rapidly developing, open source project&apos; with a broad
coverage of natural language processing (NLP) tools; see Bird et al. (2008)
for a recent overview. This wide functionality has a number of benefits,
most notably that lexical, syntactic and semantic processing can be carried
out within a uniform computational framework. As a result, NLTK makes it
much easier to include some computational semantics subject matter in a
broad course on natural language analysis, rather than having to devote a
whole course exclusively to the topic.
</bodyText>
<footnote confidence="0.998604">
1See http://www.nltk.org
</footnote>
<page confidence="0.979912">
116
</page>
<bodyText confidence="0.9326845">
Proceedings of the 8th International Conference on Computational Semantics, pages 116–127,
Tilburg, January 2009. c�2009 International Conference on Computational Semantics
Fourth, NLTK is accompanied by a substantial collection of corpora, plus
easy-to-use corpus readers. This collection, which currently stands at over
50 corpora and trained models, includes parsed, POS-tagged, plain text, cat-
egorized text, and lexicons. The availability of corpora can help encourage
students to go beyond writing toy grammars, and instead to start grappling
with the complexities of semantically analysing realistic bodies of text.
Fifth, NLTK is not just for students. Although Python is slower than
languages like Java and C++, its suitability for rapid prototyping makes it
an attractive addition to the researcher’s inventory of resources. Building
an experimental set-up in NLTK to test a hypothesis or explore some data is
straightforward and quick, and the rich variety of existing NLP components in
the toolkit allows rapid assembly of quite sophisticated processing pipelines.
</bodyText>
<sectionHeader confidence="0.983099" genericHeader="keywords">
2 Overview
</sectionHeader>
<bodyText confidence="0.999929">
Like B&amp;B, we assume that
one of the most important
tasks for the teacher is to
ground students in the ba-
sic concepts of first order
logic and the lambda cal-
culus, model-theoretic in-
terpretation and inference.
This provides a basis for
exploring more modern ap-
proaches like Discourse Rep-
resentation Theory (DRT;
Kamp and Reyle (1993))
and underspecification.
In the accompanying fig-
ure, we give a diagram-
matic overview of the main
semantics-related function-
ality that is currently avail-
able in NLTK. Logical
forms (LFs) can be induced
as result of syntactic pars-
ing, using either feature-
based grammars that are
processed with an Earley
chart parser, or else by as-
sociating LFs with the output of a broad-coverage dependency parser. Our
basic LFs are expressions of first order logic, supplemented with the lambda
</bodyText>
<figure confidence="0.998296789473684">
Model Checking
Parsing
Feature-based CFG +
Earley parser
Statistical dependency
grammar parser
Underspecified LFs
Theorem Proving
Hole Semantics
Logical Forms
Tableau TP
Non-monotonic TP
FOL + λ
DRS + λ
Linear Logic Glue
Semantics
Prover9
Model Building
Mace
</figure>
<page confidence="0.992569">
117
</page>
<bodyText confidence="0.999100363636364">
operator. However, we also admit Discourse Representation Structures
(DRSs) as LFs, and underspecified LFs can be built using either Hole Se-
mantics (Blackburn and Bos, 2005) or Glue Semantics (Dalrymple et al.,
1999). Once we have constructed LFs, they can be evaluated in a first order
model (Klein, 2006), tested for equivalence and validity in a variety of the-
orem provers, or tested for consistency in a model builder. The latter two
tasks are aided by NLTK interfaces to third-party inference tools, currently
Prover9 and Mace4 (McCune, 2008).
We do not have space in this paper to discuss all of these components,
but will try to present some of the key aspects, and along the way noting
certain points of difference vis-`a-vis B&amp;B.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="introduction">
3 Logical Form
</sectionHeader>
<subsectionHeader confidence="0.999857">
3.1 First Order Predicate Logic with Lambda Calculus
</subsectionHeader>
<bodyText confidence="0.99986">
From a pedagogical point of view, it is usually important to ensure that stu-
dents have some grasp of the language of first order predicate logic (FOL),
and can also manipulate A-abstraction. The nltk.sem.logic module con-
tains an object-oriented approach to representing FOL plus A-abstraction.
Logical formulas are typically fed to the logic parser as strings, and then
represented as instances of various subclasses of Expression, as we will see
shortly.
An attractive feature of Python is its interactive interpreter, which allows
the user to enter Python expressions and statements for evaluation. In the
example below and subsequently, &gt;&gt;&gt; is the Python interpreter’s prompt.
</bodyText>
<equation confidence="0.9443056">
1 &gt;&gt;&gt; from nltk.sem import logic
2 &gt;&gt;&gt; lp = logic.LogicParser()
3 &gt;&gt;&gt; e = lp.parse(’all x.(girl(x) -&gt; exists y.(dog(y) &amp; chase(x,y)))’)
4 &gt;&gt;&gt; e
5 &lt;AllExpression all x.(girl(x) -&gt; exists y.(dog(y) &amp; chase(x,y)))&gt;
</equation>
<bodyText confidence="0.999849090909091">
As illustrated, the result of parsing the formula at line 3 is an object e be-
longing to the class AllExpression, itself a subclass of Expression. All
such subclasses have numerous methods that implement standard logical
operations. For example, the simplify() method carries out 0-conversion;
the free() method finds all the free variables in an expression; and for quan-
tified expressions (such as AllExpressions), there is an alpha convert()
method. The logic module will α-convert automatically when appropri-
ate to avoid name-clashes in the replace() method. Let’s illustrate these
methods with a formula involving A-abstraction, namely \x.P(x)(y); we
use \ to represent A. (Since \ is a special character in Python, we add the
r prefix to strings containing it to preclude additional escape characters.)
</bodyText>
<page confidence="0.982544">
118
</page>
<bodyText confidence="0.772591">
&gt;&gt;&gt; from nltk.sem import Variable
</bodyText>
<equation confidence="0.997604454545454">
&gt;&gt;&gt; e1 = lp.parse(r’\x.P(x)(y)’)
&gt;&gt;&gt; print e1.simplify()
P(y)
&gt;&gt;&gt; e2 = lp.parse(’all x.P(x,a,b)’)
&gt;&gt;&gt; print e2.free()
set([&lt;Variable(’a’), Variable(’b’)])
&gt;&gt;&gt; print e2.alpha_convert(Variable(’z’))
all z.P(z,a,b)
&gt;&gt;&gt; e3 = lp.parse(’x’)
&gt;&gt;&gt; print e2.replace(Variable(’b’), e3)
all z1.P(z1,a,x)
</equation>
<bodyText confidence="0.9996946">
Allowing students to build simple first order models, and evaluate expres-
sions in those models, can be useful for helping them clarify their intuitions
about quantification. In the next example, we show one of the available
methods in NLTK for specifying a model and using it to determine the set of
satisfiers of the open formula ∃x.(girl(y) ∧ chase(x, y)).2, 3
</bodyText>
<figure confidence="0.919344083333333">
&gt;&gt;&gt; from nltk.sem import parse_valuation, Model, Assignment
&gt;&gt;&gt; v = uuu
... suzie =&gt; s
... fido=&gt;f
... rover =&gt; r
... girl =&gt; {s}
... chase =&gt; {(f, s), (r, s), (s, f)}
uuu
...
&gt;&gt;&gt; val = parse_valuation(v) #create a Valuation
&gt;&gt;&gt; m = Model(val.domain, val) #initialize a Model
&gt;&gt;&gt; g = Assignment(val.domain) #initialize an Assignment
</figure>
<equation confidence="0.571369333333333">
&gt;&gt;&gt; e4 = lp.parser(’exists y. (girl(y) &amp; chase(x, y))’)
&gt;&gt;&gt; m.satisfiers(e4, ’x’, g) #check satisfiers of e4 wrt to x
set([’r’, ’f’])
</equation>
<bodyText confidence="0.999679375">
In B&amp;B, A-abstracts are second-class citizens, used exclusively as a ‘glue’
mechanism for composing meaning representations. Although we use A-
abstracts as glue too, abstracts over individual variables are semantically
interpreted in NLTK, namely as characteristic functions.
Expressions in NLTK can be optionally typed (using Montague-style
types) by passing the parameter type check=True to LogicParser. Apart
from allowing the user to display the Expression’s type with type, type
checking will raise an exception for non-well typed expressions:
</bodyText>
<footnote confidence="0.98205425">
2The triple quotes &amp;quot;&amp;quot;&amp;quot; in Python allow us to break a logical line across several physical
lines.
3Given a valuation val, the property val.domain returns the set of all domain indi-
viduals specified in the valuation.
</footnote>
<page confidence="0.985488">
119
</page>
<equation confidence="0.503031833333333">
&gt;&gt;&gt; tlp = logic.LogicParser(type_check=True)
&gt;&gt;&gt; a = tlp.parse(r’\x y.see(x,y)’)
&gt;&gt;&gt; b = tlp.parse(r’\x.man(x)’)
&gt;&gt;&gt; a.type, b.type
(&lt;e,&lt;e,t&gt;&gt;, &lt;e,t&gt;)
&gt;&gt;&gt; tlp.parse(r’\x y.see(x,y)(\x.man(x))’)
</equation>
<bodyText confidence="0.5298552">
Traceback (most recent call last):
. . .
TypeException: The function ’\x y.see(x,y)’ is of type ’&lt;e,&lt;e,t&gt;&gt;’
and cannot be applied to ’\x.man(x)’ of type ’&lt;e,t&gt;’. Its argument
must match type ’e’.
</bodyText>
<subsectionHeader confidence="0.968787">
3.2 Discourse Representation Theory
</subsectionHeader>
<bodyText confidence="0.9999185">
As mentioned earlier, NLTK contains an extension to the logic module for
working with Discourse Representation Theory (DRT) (Kamp and Reyle,
1993). The nltk.sem.drt module introduces a DRS() constructor which
takes lists of discourse referents and conditions as initialization parameters:
</bodyText>
<equation confidence="0.96206">
(1) DRS([j,d],[John(j), dog(d), sees(j,d)])
</equation>
<bodyText confidence="0.970548875">
On top of the functionality available for FOL expressions, DRT expres-
sions have a ‘DRS-concatenation’ operator, represented as the + symbol. The
concatenation of two DRSs is a single DRS containing the merged discourse
referents and the conditions from both arguments. DRS-concatenation auto-
matically α-converts bound variables to avoid name-clashes. The + symbol
is overloaded so that DRT expressions can be added together easily. The
nltk.sem.drt parser allows DRSs to be specified succinctly as strings.
&gt;&gt;&gt; from nltk.sem import drt
</bodyText>
<equation confidence="0.956175538461538">
&gt;&gt;&gt; dp = drt.DrtParser()
&gt;&gt;&gt; d1 = dp.parse(’([x],[walk(x)]) + ([y],[run(y)])’)
&gt;&gt;&gt; print d1
(([x],[walk(x)]) + ([y],[run(y)]))
&gt;&gt;&gt; print d1.simplify()
([x,y],[walk(x), run(y)])
&gt;&gt;&gt; d2 = dp.parse(’([x,y],[Bill(x), Fred(y)])’)
&gt;&gt;&gt; d3 = dp.parse(&amp;quot;&amp;quot;&amp;quot;([],[([u],[Porsche(u), own(x,u)])
... -&gt; ([v],[Ferrari(v), own(y,u)])])&amp;quot;&amp;quot;&amp;quot;)
&gt;&gt;&gt; d4 = d2 + d3
&gt;&gt;&gt; print d4.simplify()
([x,y],[Bill(x), Fred(y),
(([u],[Porsche(u), own(x,u)]) -&gt; ([v],[Ferrari(v), own(y,u)]))])
</equation>
<bodyText confidence="0.998985333333333">
DRT expressions can be converted to their first order predicate logic equiva-
lents using the toFol() method and can be graphically rendered on screen
with the draw() method.
</bodyText>
<page confidence="0.946963">
120
</page>
<table confidence="0.447906">
&gt;&gt;&gt; print d1.toFol()
(exists x.walk(x) &amp; exists y.run(y))
&gt;&gt;&gt; d4.simplify().draw()
</table>
<bodyText confidence="0.9819862">
Since the λ operator can be combined
with DRT expressions, the nltk.sem.drt mod-
ule can be used as a plug-in replacement for
nltk.sem.logic in building compositional se-
mantics.
</bodyText>
<figureCaption confidence="0.993913">
Figure 1: DRS Screenshot
</figureCaption>
<sectionHeader confidence="0.649488" genericHeader="method">
4 Scope Ambiguity and Underspecification
</sectionHeader>
<bodyText confidence="0.710361">
Two key questions in introducing students to computational semantics are:
</bodyText>
<listItem confidence="0.8643505">
Q1: How are semantic representations constructed from input sentences?
Q2: What is scope ambiguity and how is it captured?
</listItem>
<bodyText confidence="0.999800636363636">
A standard pedagogical approach is to address (Q1) with a simple syntax-
driven induction of logical forms which fails to deal with scope ambiguity,
while (Q2) is addressed by introducing underspecified representations which
are resolved to produce different readings of ambiguous sentences.
NLTK includes a suite of parsing tools, amongst which is a chart parser
for context free grammars augmented with feature structures. A ‘semantics’
feature sem allows us to compose the contributions of constituents to build
a logical form for a complete sentence. To illustrate, the following minimal
grammar sem1.fcfg handles quantification and intransitive verbs (where
values such as ?subj and ?vp are unification variables, while P and Q are
λ-bound object language variables):
</bodyText>
<equation confidence="0.999794833333333">
S[sem = &lt;?subj(?vp)&gt;] -&gt; NP[sem=?subj] VP[sem=?vp]
VP[sem=?v] -&gt; IV[sem=?v]
NP[sem=&lt;?det(?n)&gt;] -&gt; Det[sem=?det] N[sem=?n]
Det[sem=&lt;\P.\Q.exists x.(P(x) &amp; Q(x))&gt;] -&gt; ’a’
N[sem=&lt;\x.dog(x)&gt;] -&gt; ’dog’
IV[sem=&lt;\x.bark(x)&gt;] -&gt; ’barks’
</equation>
<bodyText confidence="0.960235333333333">
Using sem1.fcfg, we can parse A dog barks and view its semantics. The
load earley() method takes an optional parameter logic parser which
specifies the logic-parser for processing the value of the sem feature, thus
allowing different kinds of logical forms to be constructed.
&gt;&gt;&gt; from nltk.parse import load_earley
&gt;&gt;&gt; parser = load_earley(’grammars/sem1.fcfg’, trace=0)
&gt;&gt;&gt; trees = parser.nbest_parse(’a dog barks’.split())
&gt;&gt;&gt; print trees[0].node[’sem’].simplify()
exists x.(dog(x) &amp; bark(x))
</bodyText>
<page confidence="0.990087">
121
</page>
<bodyText confidence="0.9997532">
Underspecified logical forms allow us to loosen the relation between syn-
tactic and semantic representations. We consider two approaches to under-
specification, namely Hole Semantics and Glue Semantics. Since the former
will be familiar from B&amp;B, we devote most of our attention to presenting
Glue Semantics.
</bodyText>
<subsectionHeader confidence="0.996679">
4.1 Hole Semantics
</subsectionHeader>
<bodyText confidence="0.845387">
Hole Semantics in NLTK is handled by the nltk.sem.hole module, which
uses a context free grammar to generate an underspecified logical form.
Since the latter is itself a formula of first order logic, we can continue to use
the sem feature in the context free grammar:
N[sem=&lt;\x h l.(PRED(l,dog,x) &amp; LEQ(l,h) &amp; HOLE(h) &amp; LABEL(l))&gt;]
-&gt; ’dog’
The Hole Semantics module uses a standard plugging algorithm to derive
the sentence’s readings from the underspecified LF.
&gt;&gt;&gt; from nltk.sem import hole
&gt;&gt;&gt; readings = hole.hole_readings(’every girl chases a dog’)
&gt;&gt;&gt; for r in reading: print r
exists z1.(dog(z1) &amp; all z2.(girl(z2) -&gt; chase(z1,z2)))
all z2.(girl(z2) -&gt; exists z1.(dog(z1) &amp; chase(z1,z2)))
</bodyText>
<subsectionHeader confidence="0.975302">
4.2 Glue Semantics
</subsectionHeader>
<bodyText confidence="0.999905529411765">
Glue Semantics (Dalrymple et al., 1999), or Glue for short, is an approach to
compositionality that tries to handle semantic ambiguity by using resource-
sensitive logic to assemble meaning expressions. The approach builds proofs
over ‘meaning constructors’; these are of the form M : G, where M is a
meaning representation and G is a term of linear logic. The linear logic
term G dictates how the meaning expression M can be combined. Each
distinct proof that can be derived reflects a different semantic reading of the
entire sentence.
The variant of linear logic that we use has (linear) implication (i.e., �)
as its only operator, so the primary operation during the proof is Modus
Ponens. Linear logic is an appropriate logic to serve as ‘glue’ because it
is resource-sensitive. This means that when Modus Ponens combines two
terms to create a new one, the two original terms are ‘consumed’, and cannot
be used again in the proof; cf. (2) vs. (3). Additionally, every premise must
be used for the proof to be valid; cf. (4). This resource-sensitivity dictates
that each word contributes its meaning exactly once to the meaning of the
whole.
</bodyText>
<page confidence="0.991972">
122
</page>
<listItem confidence="0.999063">
(2) A, (A ( B) �- B
(3) A, (A ( B) Y A, B
(4) A, A, (A ( B) Y B
</listItem>
<bodyText confidence="0.992113222222222">
NLTK’s nltk.gluesemantics.linearlogic module contains an implemen-
tation of linear logic.
The primary rule for composing Glue formulas is (5). Function-argument
application of meaning expressions is reflected (via the Curry-Howard iso-
morphism) by the application of Modus Ponens in a linear logic proof. Note
that A and B are meta-variables over constants of linear logic; these con-
stants represent ‘attachment points’ for meaning expressions in some kind of
syntactically-derived representation (such as an LFG f-structure). It is (5)
which allows Glue to guide the construction of complex meaning expressions.
</bodyText>
<listItem confidence="0.809323">
(5) α : A, γ : (A ( B) �- γ(α) : B
</listItem>
<bodyText confidence="0.96586075">
The NLTK modules gluesemantics.glue and gluesemantics.drt glue
implement Glue for FOL and DRT meaning expressions, respectively.4 The
following example shows how Glue formulas are created and combined to
derive a logical form for John walks:
&gt;&gt;&gt; from nltk.gluesemantics.glue import GlueFormula
&gt;&gt;&gt; john = GlueFormula(’john’, ’g’)
&gt;&gt;&gt; walks = GlueFormula(r’\x.walk(x)’, ’(g -o f)’)
&gt;&gt;&gt; john_walks = walks.applyto(john)
&gt;&gt;&gt; print john_walks.meaning.simplify()
walk(john)
Thus, the non-logical constant john is associated with the Glue term g,
while the meaning expression λx.walk(x) is associated with (g ( f) since
it is a function that takes g as input and returns the meaning expression f,
corresponding to the whole sentence. Consequently, a proof of f from the
premises is a derivation of a meaning representation for the sentence.
Scope ambiguity, resulting, for example, from quantifiers, requires the
use of variables in the Glue terms. Such variables may be instantiated to any
linear logic constant, so long as this is carried out uniformly. Let’s assume
that the quantified noun phrase every girl has the meaning constructor (6)
(where G is a linear logic variable):
</bodyText>
<listItem confidence="0.914758">
(6) λQ.bx.(girl(x) → Q(x)) : ((g ( G) ( G)
</listItem>
<footnote confidence="0.999563">
4See http://nltk.googlecode.com/svn/trunk/doc/contrib/sem/index.html for
more details.
</footnote>
<page confidence="0.993895">
123
</page>
<table confidence="0.950447181818182">
Then the Glue derivation shown below correctly generates two readings for
the sentence Every girl chases a dog:
&gt;&gt;&gt; from nltk.gluesemantics.glue import GlueFormula, Glue
&gt;&gt;&gt; a = GlueFormula(r’\Q.all x.(girl(x) -&gt; Q(x))’, ’((g -o G) -o G)’)
&gt;&gt;&gt; b = GlueFormula(r’\x y.chase(x,y)’, ’(g -o (h -o f))’)
&gt;&gt;&gt; c = GlueFormula(r’\Q.exists x.(dog(x)&amp;Q(x))’, ’((h -o H) -o H)’)
&gt;&gt;&gt; glue = Glue()
&gt;&gt;&gt; for reading in glue.get_readings(glue.gfl_to_compiled([a,b,c])):
... print reading.simplify()
exists x.(dog(x) &amp; all z13.(girl(z13) -&gt; chase(z13,x)))
all x.(girl(x) -&gt; exists z14.(dog(z14) &amp; chase(x,z14)))
</table>
<sectionHeader confidence="0.968985" genericHeader="method">
5 Inference tools
</sectionHeader>
<bodyText confidence="0.968023272727273">
In order to perform inference over semantic representations, NLTK can call
both theorem provers and model builders. The library includes a pure
Python tableau-based first order theorem prover; this is intended to allow
students to study tableau methods for theorem proving, and provides an
opportunity for experimentation. In addition, NLTK provides interfaces to
two off-the-shelf tools, namely the theorem prover Prover9, and the model
builder Mace4 (McCune, 2008).
The get_prover(G, A) method by default calls Prover9, and takes as
parameters a proof goal G and a list A of assumptions. Here, we verify that
if every dog barks, and Rover is a dog, then it is true that Rover barks:
&gt;&gt;&gt; from nltk.inference import inference
</bodyText>
<equation confidence="0.512861666666667">
&gt;&gt;&gt; a = lp.parse(’all x.(dog(x) -&gt; bark(x))’)
&gt;&gt;&gt; b = lp.parse(’dog(rover)’)
&gt;&gt;&gt; c = lp.parse(’bark(rover)’)
&gt;&gt;&gt; prover = inference.get_prover(c, [a,b])
&gt;&gt;&gt; prover.prove()
True
</equation>
<bodyText confidence="0.9996829">
A theorem prover can also be used to check the logical equivalence of
expressions. For two expressions A and B, we can pass (A ⇐⇒ B) into
a theorem prover and know that the theorem will be proved if and only if
the expressions are logically equivalent. NLTK’s standard equality operator
for Expressions (==) is able to handle situations where two expressions are
identical up to α-conversion. However, it would be impractical for NLTK
to invoke a wider range of logic rules every time we checked for equality of
two expressions. Consequently, both the logic and drt modules in NLTK
have a separate method, tp equals, for checking ‘equality’ up to logical
equivalence.
</bodyText>
<page confidence="0.988878">
124
</page>
<figure confidence="0.9436054">
&gt;&gt;&gt; a = lp.parse(’all x.walk(x)’)
&gt;&gt;&gt; b = lp.parse(’all y.walk(y)’)
&gt;&gt;&gt; a == b
True
&gt;&gt;&gt; c = lp.parse(’-(P(x) &amp; Q(x))’)
&gt;&gt;&gt; d = lp.parse(’-P(x)  |-Q(x)’)
&gt;&gt;&gt; c == d
False
&gt;&gt;&gt; c.tp_equals(d)
True
</figure>
<sectionHeader confidence="0.819893" genericHeader="method">
6 Discourse Processing
</sectionHeader>
<bodyText confidence="0.993398428571428">
NLTK contains a discourse processing module, nltk.inference.discourse,
similar to the CURT program presented in B&amp;B. This module processes
sentences incrementally, keeping track of all possible threads when there is
ambiguity. For simplicity, the following example ignores scope ambiguity.
&gt;&gt;&gt; from nltk.inference.discourse import DiscourseTester as DT
&gt;&gt;&gt; dt = DT([’A student dances’, ’Every student is a person’])
&gt;&gt;&gt; dt.readings()
</bodyText>
<equation confidence="0.83712575">
s0 readings:
s0-r0: exists x.(student(x) &amp; dance(x))
s1 readings:
s1-r0: all x.(student(x) -&gt; person(x))
</equation>
<bodyText confidence="0.99857525">
When a new sentence is added to the current discourse, setting the parameter
consistchk=True causes consistency to be checked by invoking the model
checker for each ‘thread’, i.e., discourse sequence of admissible readings. In
this case, the user has the option of retracting the sentence in question.
</bodyText>
<figure confidence="0.395834666666667">
&gt;&gt;&gt; dt.add_sentence(’No person dances’, consistchk=True)
Inconsistent discourse d0 [’s0-r0’, ’s1-r0’, ’s2-r0’]:
s0-r0: exists x.(student(x) &amp; dance(x))
s1-r0: all x.(student(x) -&gt; person(x))
s2-r0: -exists x.(person(x) &amp; dance(x))
&gt;&gt;&gt; dt.retract_sentence(’No person dances’, quiet=False)
Current sentences are
s0: A student dances
s1: Every student is a person
</figure>
<bodyText confidence="0.999623666666667">
In a similar manner, we use informchk=True to check whether the new sen-
tence is informative relative to the current discourse (by asking the theorem
prover to derive it from the discourse).
</bodyText>
<page confidence="0.995676">
125
</page>
<bodyText confidence="0.917916266666667">
&gt;&gt;&gt; dt.add_sentence(’A person dances’, informchk=True)
Sentence ’A person dances’ under reading ’exists x.(person(x) &amp;
dance(x))’:
Not informative relative to thread ’d0’
It is also possible to pass in an additional set of assumptions as background
knowledge and use these to filter out inconsistent readings.
The discourse module can accommodate semantic ambiguity and filter
out readings that are not admissable. By invoking both Glue Semantics and
DRT, the following example processes the two-sentence discourse Every dog
chases a boy. He runs. As shown, the first sentence has two possible read-
ings, while the second sentence contains an anaphoric pronoun, indicated as
PRO(x).
&gt;&gt;&gt; from nltk.inference.discourse import DrtGlueReadingCommand as RC
&gt;&gt;&gt; dt = DT([’Every dog chases a boy’, ’He runs’], RC())
&gt;&gt;&gt; dt.readings()
</bodyText>
<equation confidence="0.9880248">
s0 readings:
s0-r0: ([],[(([x],[dog(x)]) -&gt; ([z15],[boy(z15), chase(x,z15)]))])
s0-r1: ([z16],[boy(z16), (([x],[dog(x)]) -&gt; ([],[chase(x,z16)]))])
s1 readings:
s1-r0: ([x],[PRO(x), run(x)])
</equation>
<bodyText confidence="0.763687777777778">
When we examine the two threads d0 and d1, we see that that reading
s0-r0, where every dog out-scopes a boy, is deemed inadmissable because
the pronoun in the second sentence cannot be resolved. By contrast, in
thread d1 the pronoun (relettered to z24) has been bound via the equation
(z24 = z20).
&gt;&gt;&gt; dt.readings(show_thread_readings=True)
d0: [’s0-r0’, ’s1-r0’] : INVALID: AnaphoraResolutionException
d1: [’s0-r1’, ’s1-r0’] : ([z20,z24],[boy(z20), (([x],[dog(x)]) -&gt;
([],[chase(x,z20)])), (z24 = z20), run(z24)])
</bodyText>
<sectionHeader confidence="0.991021" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999969142857143">
NLTK’s semantics functionality has been written with extensibility in mind.
The logic module’s LogicParser employs a basic parsing template and
contains hooks that an extending module can use to supplement or sub-
stitute functionality. Moreover, the base Expression class in logic, as
well as any derived classes, can be extended, allowing variants to reuse the
existing functionality. For example, the DRT and linear logic modules are
implemented as extensions to logic.py.
</bodyText>
<page confidence="0.994956">
126
</page>
<bodyText confidence="0.999891933333333">
The theorem prover and model builder code has also been carefully archi-
tected to allow extensions and the nltk.inference.api library exposes the
framework for the inference architecture. The library therefore provides a
good starting point for creating interfaces with other theorem provers and
model builders in addition to Prover9, Mace4, and the tableau prover.
NLTK already includes the beginnings of a framework for ‘recognizing
textual entailment’; access to the RTE data sets is provided and we are in the
course of developing a few simple modules to demonstrate RTE techniques.
For example, a Logical Entailment RTE tagger based on Bos and Markert
(2005) begins by building a semantic representation of both the text and
the hypothesis in DRT. It then runs a theorem prover with the text as the
assumption and the hypothesis as the goal in order to check whether the
text entails the hypothesis.The tagger is also capable of adding background
knowledge via an interface to the WordNet dictionary in nltk.wordnet as
a first step in making the entailment checking more robust.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993962434782608">
Steven Bird, Ewan Klein, Edward Loper, and Jason Baldridge. Multidisciplinary
instruction with the Natural Language Toolkit. In Proceedings of the Third
Workshop on Issues in Teaching Computational Linguistics, Columbus, Ohio,
USA, June 2008.
Patrick Blackburn and Johan Bos. Representation and Inference for Natural Lan-
guage: A First Course in Computational Semantics. CSLI Publications, New
York, 2005.
Johan Bos and Katja Markert. Recognising textual entailment with logical infer-
ence. In Proceedings of the conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, Vancouver, British Columbia,
Canada, 2005.
Mary Dalrymple, V. Gupta, John Lamping, and V. Saraswat. Relating resource-
based semantics to categorial semantics. In Mary Dalrymple, editor, Semantics
and syntax in Lexical Functional Grammar: the resource logic approach, pages
261–280. MIT Press, Cambridge, MA, 1999.
Hans Kamp and Uwe Reyle. From Discourse to the Lexicon: Introduction to Mod-
eltheoretic Semantics of Natural Language, Formal Logic and Discourse Repre-
sentation Theory. Kluwer Academic Publishers, 1993.
Ewan Klein. Computational semantics in the Natural Language Toolkit. In Pro-
ceedings of the Australasian Language Technology Workshop, pages 26–33, 2006.
William McCune. Prover9: Automated theorem prover for first-order
and equational logic, 2008. http://www.cs.unm.edu/~mccune/mace4/
manual-examples.html.
</reference>
<page confidence="0.996975">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912616">
<title confidence="0.999426">An Extensible Toolkit for Computational Semantics</title>
<author confidence="0.99998">Dan Garrette Ewan Klein</author>
<affiliation confidence="0.985211">of Edinburgh</affiliation>
<email confidence="0.926788">ewan@inf.ed.ac.uk</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
<author>Jason Baldridge</author>
</authors>
<title>Multidisciplinary instruction with the Natural Language Toolkit.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,</booktitle>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="1745" citStr="Bird et al. (2008)" startWordPosition="266" endWordPosition="269">ficantly goes beyond B&amp;B in providing an implementation of Glue Semantics. Second, whatever the relative merits of Prolog vs. Python as programming languages, there is surely an advantage in offering students and instructors a choice in this respect. Given that many students have either already been exposed to Java, or else have had no programming experience at all, Python offers them the option of accomplishing interesting results with only a shallow learning curve. Third, NLTK is a rapidly developing, open source project&apos; with a broad coverage of natural language processing (NLP) tools; see Bird et al. (2008) for a recent overview. This wide functionality has a number of benefits, most notably that lexical, syntactic and semantic processing can be carried out within a uniform computational framework. As a result, NLTK makes it much easier to include some computational semantics subject matter in a broad course on natural language analysis, rather than having to devote a whole course exclusively to the topic. 1See http://www.nltk.org 116 Proceedings of the 8th International Conference on Computational Semantics, pages 116–127, Tilburg, January 2009. c�2009 International Conference on Computational </context>
</contexts>
<marker>Bird, Klein, Loper, Baldridge, 2008</marker>
<rawString>Steven Bird, Ewan Klein, Edward Loper, and Jason Baldridge. Multidisciplinary instruction with the Natural Language Toolkit. In Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics, Columbus, Ohio, USA, June 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Johan Bos</author>
</authors>
<title>Representation and Inference for Natural Language: A First Course in Computational Semantics.</title>
<date>2005</date>
<publisher>CSLI Publications,</publisher>
<location>New York,</location>
<contexts>
<context position="4517" citStr="Blackburn and Bos, 2005" startWordPosition="690" endWordPosition="693"> an Earley chart parser, or else by associating LFs with the output of a broad-coverage dependency parser. Our basic LFs are expressions of first order logic, supplemented with the lambda Model Checking Parsing Feature-based CFG + Earley parser Statistical dependency grammar parser Underspecified LFs Theorem Proving Hole Semantics Logical Forms Tableau TP Non-monotonic TP FOL + λ DRS + λ Linear Logic Glue Semantics Prover9 Model Building Mace 117 operator. However, we also admit Discourse Representation Structures (DRSs) as LFs, and underspecified LFs can be built using either Hole Semantics (Blackburn and Bos, 2005) or Glue Semantics (Dalrymple et al., 1999). Once we have constructed LFs, they can be evaluated in a first order model (Klein, 2006), tested for equivalence and validity in a variety of theorem provers, or tested for consistency in a model builder. The latter two tasks are aided by NLTK interfaces to third-party inference tools, currently Prover9 and Mace4 (McCune, 2008). We do not have space in this paper to discuss all of these components, but will try to present some of the key aspects, and along the way noting certain points of difference vis-`a-vis B&amp;B. 3 Logical Form 3.1 First Order Pre</context>
</contexts>
<marker>Blackburn, Bos, 2005</marker>
<rawString>Patrick Blackburn and Johan Bos. Representation and Inference for Natural Language: A First Course in Computational Semantics. CSLI Publications, New York, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, British Columbia, Canada,</location>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. Recognising textual entailment with logical inference. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, Vancouver, British Columbia, Canada, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>V Gupta</author>
<author>John Lamping</author>
<author>V Saraswat</author>
</authors>
<title>Relating resourcebased semantics to categorial semantics.</title>
<date>1999</date>
<booktitle>Semantics and syntax in Lexical Functional Grammar: the resource logic approach,</booktitle>
<pages>261--280</pages>
<editor>In Mary Dalrymple, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="4560" citStr="Dalrymple et al., 1999" startWordPosition="697" endWordPosition="700">ing LFs with the output of a broad-coverage dependency parser. Our basic LFs are expressions of first order logic, supplemented with the lambda Model Checking Parsing Feature-based CFG + Earley parser Statistical dependency grammar parser Underspecified LFs Theorem Proving Hole Semantics Logical Forms Tableau TP Non-monotonic TP FOL + λ DRS + λ Linear Logic Glue Semantics Prover9 Model Building Mace 117 operator. However, we also admit Discourse Representation Structures (DRSs) as LFs, and underspecified LFs can be built using either Hole Semantics (Blackburn and Bos, 2005) or Glue Semantics (Dalrymple et al., 1999). Once we have constructed LFs, they can be evaluated in a first order model (Klein, 2006), tested for equivalence and validity in a variety of theorem provers, or tested for consistency in a model builder. The latter two tasks are aided by NLTK interfaces to third-party inference tools, currently Prover9 and Mace4 (McCune, 2008). We do not have space in this paper to discuss all of these components, but will try to present some of the key aspects, and along the way noting certain points of difference vis-`a-vis B&amp;B. 3 Logical Form 3.1 First Order Predicate Logic with Lambda Calculus From a pe</context>
<context position="13805" citStr="Dalrymple et al., 1999" startWordPosition="2067" endWordPosition="2070">al form. Since the latter is itself a formula of first order logic, we can continue to use the sem feature in the context free grammar: N[sem=&lt;\x h l.(PRED(l,dog,x) &amp; LEQ(l,h) &amp; HOLE(h) &amp; LABEL(l))&gt;] -&gt; ’dog’ The Hole Semantics module uses a standard plugging algorithm to derive the sentence’s readings from the underspecified LF. &gt;&gt;&gt; from nltk.sem import hole &gt;&gt;&gt; readings = hole.hole_readings(’every girl chases a dog’) &gt;&gt;&gt; for r in reading: print r exists z1.(dog(z1) &amp; all z2.(girl(z2) -&gt; chase(z1,z2))) all z2.(girl(z2) -&gt; exists z1.(dog(z1) &amp; chase(z1,z2))) 4.2 Glue Semantics Glue Semantics (Dalrymple et al., 1999), or Glue for short, is an approach to compositionality that tries to handle semantic ambiguity by using resourcesensitive logic to assemble meaning expressions. The approach builds proofs over ‘meaning constructors’; these are of the form M : G, where M is a meaning representation and G is a term of linear logic. The linear logic term G dictates how the meaning expression M can be combined. Each distinct proof that can be derived reflects a different semantic reading of the entire sentence. The variant of linear logic that we use has (linear) implication (i.e., �) as its only operator, so the</context>
</contexts>
<marker>Dalrymple, Gupta, Lamping, Saraswat, 1999</marker>
<rawString>Mary Dalrymple, V. Gupta, John Lamping, and V. Saraswat. Relating resourcebased semantics to categorial semantics. In Mary Dalrymple, editor, Semantics and syntax in Lexical Functional Grammar: the resource logic approach, pages 261–280. MIT Press, Cambridge, MA, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to the Lexicon: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="3601" citStr="Kamp and Reyle (1993)" startWordPosition="547" endWordPosition="550">esearcher’s inventory of resources. Building an experimental set-up in NLTK to test a hypothesis or explore some data is straightforward and quick, and the rich variety of existing NLP components in the toolkit allows rapid assembly of quite sophisticated processing pipelines. 2 Overview Like B&amp;B, we assume that one of the most important tasks for the teacher is to ground students in the basic concepts of first order logic and the lambda calculus, model-theoretic interpretation and inference. This provides a basis for exploring more modern approaches like Discourse Representation Theory (DRT; Kamp and Reyle (1993)) and underspecification. In the accompanying figure, we give a diagrammatic overview of the main semantics-related functionality that is currently available in NLTK. Logical forms (LFs) can be induced as result of syntactic parsing, using either featurebased grammars that are processed with an Earley chart parser, or else by associating LFs with the output of a broad-coverage dependency parser. Our basic LFs are expressions of first order logic, supplemented with the lambda Model Checking Parsing Feature-based CFG + Earley parser Statistical dependency grammar parser Underspecified LFs Theore</context>
<context position="9349" citStr="Kamp and Reyle, 1993" startWordPosition="1434" endWordPosition="1437">main individuals specified in the valuation. 119 &gt;&gt;&gt; tlp = logic.LogicParser(type_check=True) &gt;&gt;&gt; a = tlp.parse(r’\x y.see(x,y)’) &gt;&gt;&gt; b = tlp.parse(r’\x.man(x)’) &gt;&gt;&gt; a.type, b.type (&lt;e,&lt;e,t&gt;&gt;, &lt;e,t&gt;) &gt;&gt;&gt; tlp.parse(r’\x y.see(x,y)(\x.man(x))’) Traceback (most recent call last): . . . TypeException: The function ’\x y.see(x,y)’ is of type ’&lt;e,&lt;e,t&gt;&gt;’ and cannot be applied to ’\x.man(x)’ of type ’&lt;e,t&gt;’. Its argument must match type ’e’. 3.2 Discourse Representation Theory As mentioned earlier, NLTK contains an extension to the logic module for working with Discourse Representation Theory (DRT) (Kamp and Reyle, 1993). The nltk.sem.drt module introduces a DRS() constructor which takes lists of discourse referents and conditions as initialization parameters: (1) DRS([j,d],[John(j), dog(d), sees(j,d)]) On top of the functionality available for FOL expressions, DRT expressions have a ‘DRS-concatenation’ operator, represented as the + symbol. The concatenation of two DRSs is a single DRS containing the merged discourse referents and the conditions from both arguments. DRS-concatenation automatically α-converts bound variables to avoid name-clashes. The + symbol is overloaded so that DRT expressions can be adde</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. From Discourse to the Lexicon: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer Academic Publishers, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ewan Klein</author>
</authors>
<title>Computational semantics in the Natural Language Toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="4650" citStr="Klein, 2006" startWordPosition="715" endWordPosition="716"> order logic, supplemented with the lambda Model Checking Parsing Feature-based CFG + Earley parser Statistical dependency grammar parser Underspecified LFs Theorem Proving Hole Semantics Logical Forms Tableau TP Non-monotonic TP FOL + λ DRS + λ Linear Logic Glue Semantics Prover9 Model Building Mace 117 operator. However, we also admit Discourse Representation Structures (DRSs) as LFs, and underspecified LFs can be built using either Hole Semantics (Blackburn and Bos, 2005) or Glue Semantics (Dalrymple et al., 1999). Once we have constructed LFs, they can be evaluated in a first order model (Klein, 2006), tested for equivalence and validity in a variety of theorem provers, or tested for consistency in a model builder. The latter two tasks are aided by NLTK interfaces to third-party inference tools, currently Prover9 and Mace4 (McCune, 2008). We do not have space in this paper to discuss all of these components, but will try to present some of the key aspects, and along the way noting certain points of difference vis-`a-vis B&amp;B. 3 Logical Form 3.1 First Order Predicate Logic with Lambda Calculus From a pedagogical point of view, it is usually important to ensure that students have some grasp o</context>
</contexts>
<marker>Klein, 2006</marker>
<rawString>Ewan Klein. Computational semantics in the Natural Language Toolkit. In Proceedings of the Australasian Language Technology Workshop, pages 26–33, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William McCune</author>
</authors>
<title>Prover9: Automated theorem prover for first-order and equational logic,</title>
<date>2008</date>
<note>http://www.cs.unm.edu/~mccune/mace4/ manual-examples.html.</note>
<contexts>
<context position="4891" citStr="McCune, 2008" startWordPosition="754" endWordPosition="755">S + λ Linear Logic Glue Semantics Prover9 Model Building Mace 117 operator. However, we also admit Discourse Representation Structures (DRSs) as LFs, and underspecified LFs can be built using either Hole Semantics (Blackburn and Bos, 2005) or Glue Semantics (Dalrymple et al., 1999). Once we have constructed LFs, they can be evaluated in a first order model (Klein, 2006), tested for equivalence and validity in a variety of theorem provers, or tested for consistency in a model builder. The latter two tasks are aided by NLTK interfaces to third-party inference tools, currently Prover9 and Mace4 (McCune, 2008). We do not have space in this paper to discuss all of these components, but will try to present some of the key aspects, and along the way noting certain points of difference vis-`a-vis B&amp;B. 3 Logical Form 3.1 First Order Predicate Logic with Lambda Calculus From a pedagogical point of view, it is usually important to ensure that students have some grasp of the language of first order predicate logic (FOL), and can also manipulate A-abstraction. The nltk.sem.logic module contains an object-oriented approach to representing FOL plus A-abstraction. Logical formulas are typically fed to the logi</context>
<context position="18014" citStr="McCune, 2008" startWordPosition="2727" endWordPosition="2728">ading.simplify() exists x.(dog(x) &amp; all z13.(girl(z13) -&gt; chase(z13,x))) all x.(girl(x) -&gt; exists z14.(dog(z14) &amp; chase(x,z14))) 5 Inference tools In order to perform inference over semantic representations, NLTK can call both theorem provers and model builders. The library includes a pure Python tableau-based first order theorem prover; this is intended to allow students to study tableau methods for theorem proving, and provides an opportunity for experimentation. In addition, NLTK provides interfaces to two off-the-shelf tools, namely the theorem prover Prover9, and the model builder Mace4 (McCune, 2008). The get_prover(G, A) method by default calls Prover9, and takes as parameters a proof goal G and a list A of assumptions. Here, we verify that if every dog barks, and Rover is a dog, then it is true that Rover barks: &gt;&gt;&gt; from nltk.inference import inference &gt;&gt;&gt; a = lp.parse(’all x.(dog(x) -&gt; bark(x))’) &gt;&gt;&gt; b = lp.parse(’dog(rover)’) &gt;&gt;&gt; c = lp.parse(’bark(rover)’) &gt;&gt;&gt; prover = inference.get_prover(c, [a,b]) &gt;&gt;&gt; prover.prove() True A theorem prover can also be used to check the logical equivalence of expressions. For two expressions A and B, we can pass (A ⇐⇒ B) into a theorem prover and know</context>
</contexts>
<marker>McCune, 2008</marker>
<rawString>William McCune. Prover9: Automated theorem prover for first-order and equational logic, 2008. http://www.cs.unm.edu/~mccune/mace4/ manual-examples.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>