<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037759">
<title confidence="0.993358">
MELB-YB: Preposition Sense Disambiguation Using Rich Semantic
Features
</title>
<author confidence="0.998871">
Patrick Ye and Timothy Baldwin
</author>
<affiliation confidence="0.998833">
Computer Science and Software Engineering
University of Melbourne, Australia
</affiliation>
<email confidence="0.998668">
{jingy,tim}@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816">
This paper describes a maxent-based prepo-
sition sense disambiguation system entry to
the preposition sense disambiguation task
of the SemEval 2007. This system uses a
wide variety of semantic and syntactic fea-
tures to perform the disambiguation task and
achieves a precision of 69.3% over the test
data.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999759185185185">
Prepositional phrases (PPs) are both common and
semantically varied in open English text. While the
conventional view on prepositions from the com-
putational linguistics community has been that they
are semantically transient at best, and semantically-
vacuous at worst, a robust account of the semantics
of prepositions and disambiguation method can be
helpful in a range of NLP tasks including machine
translation, parsing (prepositional phrase attach-
ment) and semantic role labelling (Durand, 1993;
O’Hara and Wiebe, 2003; Ye and Baldwin, 2006a).
The SemEval 2007 preposition sense disambigua-
tion task provides a common test bed for the evalua-
tion of preposition sense disambiguation systems.
Our proposed method is maximum entropy based,
and combines features developed in the context of
preposition sense disambiguation for semantic role
labelling (Ye and Baldwin, 2006a), and verb sense
disambiguation (Ye and Baldwin, 2006b).
The remainder of this paper is structured as fol-
lows. We first discuss the pre-processing steps
used in our system (Section 2), and outline the fea-
tures our preposition disambiguation method uses
(Section 3) and our parameter tuning method (Sec-
tion 4). We then discuss and analyse the results of
our method (Section 5) and conclude the paper (Sec-
tion 6).
</bodyText>
<sectionHeader confidence="0.844782" genericHeader="introduction">
2 Pre-processing
</sectionHeader>
<bodyText confidence="0.9960971875">
The following list shows the pre-processing steps
that our system goes through and the tools used:
Part of speech tagging SVMTool version 1.2
(Gim´enez and M`arquez, 2004).
Chunking An in-house chunker implemented
with fnTBL, a transformation based learner (Ngai
and Florian, 2001), and trained on the British Na-
tional Corpus (BNC).1
Parsing Charniak’s re-ranking parser, version Au-
gust, 2006 (Charniak and Johnson, 2005).
Named entity extraction A statistical NER sys-
tem described in Cohn et al. (2005).
Supersense tagging A WordNet-based super-
sense tagger (Ciaramita and Altun, 2006).
Semantic role labeling ASSERT version 1.4
(Pradhan et al., 2004).
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.99990975">
The disambiguation features used by our system can
be divided into three categories: collocation fea-
tures, syntactic features and semantic-role based fea-
tures. We discuss each in turn below.
</bodyText>
<subsectionHeader confidence="0.999693">
3.1 Collocation Features
</subsectionHeader>
<bodyText confidence="0.999744142857143">
The collocation features were inspired by the
one-sense-per-collocation heuristic proposed by
Yarowsky (1995). These features were designed to
capture open class words that exhibit strong colloca-
tion properties with respect to the different senses of
the target preposition. Details of the features in this
category are listed below.
</bodyText>
<footnote confidence="0.998522666666667">
1This chunker is not exactly the same as Ngai and Florian’s
system, however it does use the default transformation tem-
plates supplied by fnTBL.
</footnote>
<page confidence="0.978844">
241
</page>
<bodyText confidence="0.980651">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 241–244,
Prague, June 2007. c�2007 Association for Computational Linguistics
Bag of open class words The part-of-speech
(POS) tags and lemmas of all the open class words
that occur in the same sentence as the target prepo-
sition.
Bag of WordNet synsets The WordNet (Miller,
1993) synonym sets and their hypernyms of all the
open class words that occur in the same sentence as
the target preposition.
Bag of named entities Each named entity in the
same sentence as the target preposition is treated as
a separate feature.
Surrounding words These features are the com-
binations of the lemma, POS tag and relative posi-
tion of the words surrounding the target preposition
within a window of 7 words.
Surrounding super senses These features are the
combinations of super-sense tag, POS tag and rel-
ative position of the words surrounding the target
preposition within a window of 7 words.
</bodyText>
<subsectionHeader confidence="0.999533">
3.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999914954545455">
The syntactic features were designed to capture both
the flat and recursive syntactic properties of the tar-
get preposition. The flat syntactic features were de-
rived from the surrounding POS tags and chunk tags
of the target preposition; the recursive syntactic fea-
tures were derived from the parse trees. The details
of these feature are given below.
Surrounding POS tags These features are the
combination of POS tag and relative position of the
words surrounding the target preposition within a
window of 7 words.
Surrounding chunk tags These features are the
combination of IOB style chunk tag and relative po-
sition of the words surrounding the target preposi-
tion within a window of 5 words.
Surrounding chunk types Instead of using only
the chunk tags themselves, we also extracted the ac-
tual chunk types (NP, VP, ADJP, etc) of the words
surrounding the target preposition within a window
of 5 words. Each chunk type is also combined with
its relative position to the target preposition as a sep-
arate feature.
</bodyText>
<figureCaption confidence="0.999162">
Figure 1: Parse tree examples
</figureCaption>
<bodyText confidence="0.9992191875">
Parse tree features Given the position of the tar-
get preposition p in the parse tree, the basic form of
the corresponding parse tree feature is just the list of
nodes of p’s siblings in the tree (the POS tags are
treated as part of the terminal). For example, sup-
pose the original parse tree for the sentence I live in
Melbourne is the left tree in Figure 1, for the target
preposition in, the basic form of the parse tree fea-
ture would be (1, NP). In order to gain more syn-
tactic information, we further annotated each non-
terminal of the parse tree with its parent node, and
used the new non-terminals as our features. The
right tree in Figure 1 shows the result of applying
this annotation once to the original parse tree. Two
levels of additional annotation were performed on
the original parse trees in our feature extraction.
</bodyText>
<subsectionHeader confidence="0.999509">
3.3 Semantic-Role Based Features
</subsectionHeader>
<bodyText confidence="0.999785095238095">
Finally, since prepositional phrases can often func-
tion as the temporal, location, and manner modifiers
for verbs, we designed semantic-role-based features
to specifically capture this type of verb-preposition
semantic information. The details of these features
are as follows:
Surrounding semantic role tags The semantic
role tags of the words surrounding the target preposi-
tion within a window of 5 words are combined with
their relative positions to the target preposition and
treated as separate features. For example, consider
the preposition on in the sentence The man who
stole my car on Sunday has apologised to me, the
semantic roles for the two verbs (stole and apolo-
gised) are shown in Table 1. The semantic roles for
stole would generate the following features: (-5, I-
A0), (-4, R-A0), (-3, TARGET), (-2, B-A1), (-1,
I-A1), (0, B-AM-TMP), (1, I-AM-TMP), (2, O), (3,
O), (4, O and (5, O).
Attached verbs This feature was designed to
capture the verb-particle and verb-preposition-
</bodyText>
<figure confidence="0.988453769230769">
S
S_NP S_VP
live VP_PP
in PP NP
Melbourne
I
S
I live
in
PP
NP
NP VP
Melbourne
</figure>
<page confidence="0.923243">
242
</page>
<table confidence="0.953222666666667">
The man who stole my car on Sunday has apologised to me
stole B-A0 I-A0 R-A0 TARGET B-A1 I-A1 B-AM-TMP I-AM-TMP O O O O
apologised B-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 O TARGET B-A2 I-A2
</table>
<tableCaption confidence="0.999889">
Table 1: Example semantic-role-labelled sentence
</tableCaption>
<bodyText confidence="0.9993685625">
attachment relationships between verbs and prepo-
sitions. There are two situations in which a preposi-
tion p is deemed to be attached to a verb v: (1) p has
a semantic role tag relative to v and this tag is a ’B’
tag, (2) p has no semantic role tag relative to v, but
the first token to the right of p has a ’B’ tag relative
to v. In the sentence shown in Table 1, stole would
be considered as the governor of on.
Verb’s relative position The lemma of each verb
in the same sentence as the target preposition is com-
bined with its relative position to the target preposi-
tion and treated as a separate feature. For example,
the sentence shown in Table 1 would generate the
two features: (-1, steal) and (1, apologize).
More detailed descriptions and examples for these
features may be found in Ye and Baldwin (2006b).
</bodyText>
<sectionHeader confidence="0.992783" genericHeader="method">
4 Parameter Tuning
</sectionHeader>
<bodyText confidence="0.999982818181818">
We used the ranking-based feature selection method
from Ye and Baldwin (2006b) to select the most rele-
vant feature based on our training data. This method
works in two steps. Firstly, we calculated the infor-
mation gain, gain ratio and Chi-squared statistics for
each feature, and used these values to generate 3 sets
of rankings for the features. We then summed up the
individual ranks, and used the sums to create a set of
final rankings for the features.
The feature selection process is based on 10-fold
cross validation: we divided our training data into
10 pairs of training-test datasets; then for each fold,
we extracted the top N% ranked features using our
feature selection heuristic from the cv-training set
(where N was set to values 5, 10, .., 100), and used
these features to test the held-out test set. The best
N as determined by the cross validation was then
applied to the entire training data set.
Additionally, since we used a maximum entropy-
based machine learning package,2 it was important
to determine the best Gaussian smoothing parameter
g for the probability distribution. The tuning of g
</bodyText>
<footnote confidence="0.7966445">
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
</footnote>
<bodyText confidence="0.998984666666667">
was incorporated into the cross validation process of
feature selection.
Given the possible combinations of parameter
tuning, we trained the following three classifiers for
the preposition sense disambiguation task:
Non-tuned Using all the original features and
10.0 for the Gaussian smoothing parameter.
Smoothing-tuned Using all the original features
but automatically tuned Gaussian smoothing param-
eter.
Fully-tuned Using both automatically tuned fea-
tures and Gaussian smoothing parameter.
</bodyText>
<sectionHeader confidence="0.997844" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.9991845">
The overall precision (%) obtained by the three clas-
sifiers for the fine-grained senses are as follows:
</bodyText>
<subsectionHeader confidence="0.532242">
Non-tuned Smoothing-tuned Fully tuned
</subsectionHeader>
<bodyText confidence="0.973197521739131">
67.9 68.0 69.3
The best overall results were achieved when both
the features and the Gaussian smoothing parameters
were automatically tuned, achieving a 1.4% absolute
precision gain over the non-tuned system. However,
such parameter tuning may not always be useful: the
same tuning process was found to be detrimental in a
Senseval-2 verb sense disambiguation task (Ye and
Baldwin, 2006b). Consistent with the findings of
Ye and Baldwin (2006b), the improvement caused
by the tuning of the Gaussian smoothing parame-
ter is only marginal compared with the improvement
caused by the tuning of the features.
We also evaluated our features based on their cate-
gories and types. Collocation features performed the
best among the three feature categories. Without any
parameter tuning, the collocation-feature-only clas-
sifier achieved an overall precision of 67.4% on the
test set; the semantic-role-feature-only classifier and
the syntactic-feature-only classifier achieved preci-
sion of 46.9% and 50.5% respectively.
The best-performing individual features are the
bag-of-words features and bag-of-synsets features.
</bodyText>
<page confidence="0.996729">
243
</page>
<bodyText confidence="0.947089">
their surrounding context, and not syntactic proper-
ties or verb-preposition semantics.
</bodyText>
<sectionHeader confidence="0.837183" genericHeader="evaluation">
Acknowledgements
</sectionHeader>
<table confidence="0.9854002">
The research in this paper has been supported by the Aus-
tralian Research Council through Discovery Project grant num-
ber DP0663879.
Feature type Feature type % in Overall
Bag of Words top N% features % of the
Bag of Synsets 10 20 30 feature type
Verb’s rel. positions 13.46 13.43 12.94 13.37
Surrounding POS tags 57.83 58.38 59.53 58.29
3.97 3.95 3.76 4.02
1.36 1.33 1.43 1.27
</table>
<tableCaption confidence="0.923367">
Table 2: Percentages of top-performing feature
types in the top N% ranked features
</tableCaption>
<bodyText confidence="0.959841103448276">
On the test set, the bag-of-words-only classifier and
the bag-of-synsets-only classifier achieved overall
precision of 63.2% and 61.9% respectively.
We also analysed the top ranking features as cal-
culated by our feature selection algorithm, as pre-
sented in Table 2. The results show the percentages
of the top-performing feature types of each feature
category in the top N% ranked features. It can be
observed that none of the top-performing features
seem to have a significantly disproportional repre-
sentation in the top-ranked features. This indicates
that the disambiguation power of a particular type
of features is determined mostly by the number of
features of that type.
On the other hand, the bag-of-words features ap-
pear to be the most effective, considering that they
account for only 13.4% of the total features, but
out-performed the bag-of-synsets features which ac-
count for nearly 60% of the total features.
It is also disappointing to see that the syntactic
and semantic-role based features had little positive
influence in the disambiguation process. However,
this is perhaps caused by the sparseness of these fea-
tures since they together only account for less than
10% of all the extracted features.
The overall finding from all this is that, similar
to nouns and verbs, preposition sense is determined
primarily by word context, and that syntactic and se-
mantic role-based features play only a minor role.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999455">
In this paper, we have described a maximum entropy
based preposition sense disambiguation system that
uses a rich set of features. We have shown that
this system performed well above the majority class
baseline of 39.6% precision. Our analysis showed
that the most important disambiguation features are
collocation-based features. This indicates that the
semantics of prepositions can be learnt mostly from
</bodyText>
<sectionHeader confidence="0.997399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997282173076923">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL’05), pages 173–180, Ann
Arbor, USA.
Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594–602, Sydney, Australia.
Trevor Cohn, Andrew Smith, and Miles Osborne. 2005. Scal-
ing conditional random fields using error-correcting codes.
In Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages 10–17,
Ann Arbor, USA.
Jacques Durand. 1993. On the translation of prepositions in
multilingual MT. In Frank Van Eynde, editor, Linguistic Is-
sues in Machine Translation, pages 138–159. Pinter Publish-
ers, London, UK.
Jes´us Gim´enez and Lluis M`arquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector machines.
In Proceedings of the 4th International Conference on Lan-
guage Resources and Evaluation, pages 43–46, Lisbon, Por-
tugal.
George A. Miller. 1993. Wordnet: a lexical database for en-
glish. In HLT ’93: Proceedings of the workshop on Human
Language Technology, pages 409–409, Princeton, USA.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40–7, Pittsburgh,
USA.
Tom O’Hara and Janyce Wiebe. 2003. Preposition semantic
classification via Treebank and FrameNet. In Proc. of the 7th
Conference on Natural Language Learning (CoNLL-2003),
pages 79–86, Edmonton, Canada.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2004. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1–3):11–39.
David Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Meeting of the Associ-
ation for Computational Linguistics, pages 189–196, Cam-
bridge, USA.
Patrick Ye and Timothy Baldwin. 2006a. Semantic role label-
ing of prepositional phrases. ACM Transactions on Asian
Language Information Processing (TALIP), 5(3):228–244.
Patrick Ye and Timothy Baldwin. 2006b. Verb sense dis-
ambiguation using selectional preferences extracted with a
state-of-the-art semantic role labeler. In Proceedings of the
Australasian Language Technology Workshop, pages 141–
148, Sydney, Australia.
</reference>
<page confidence="0.998535">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526433">
<title confidence="0.998881">MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features</title>
<author confidence="0.999666">Patrick Ye</author>
<author confidence="0.999666">Timothy Baldwin</author>
<affiliation confidence="0.774784">Computer Science and Software Engineering University of Melbourne, Australia</affiliation>
<email confidence="0.997391">jingy@csse.unimelb.edu.au</email>
<email confidence="0.997391">tim@csse.unimelb.edu.au</email>
<abstract confidence="0.995841444444444">This paper describes a maxent-based preposition sense disambiguation system entry to the preposition sense disambiguation task of the SemEval 2007. This system uses a wide variety of semantic and syntactic features to perform the disambiguation task and a precision of the test data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="2262" citStr="Charniak and Johnson, 2005" startWordPosition="333" endWordPosition="336">on disambiguation method uses (Section 3) and our parameter tuning method (Section 4). We then discuss and analyse the results of our method (Section 5) and conclude the paper (Section 6). 2 Pre-processing The following list shows the pre-processing steps that our system goes through and the tools used: Part of speech tagging SVMTool version 1.2 (Gim´enez and M`arquez, 2004). Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngai and Florian, 2001), and trained on the British National Corpus (BNC).1 Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling ASSERT version 1.4 (Pradhan et al., 2004). 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed t</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broadcoverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2426" citStr="Ciaramita and Altun, 2006" startWordPosition="358" endWordPosition="361"> the paper (Section 6). 2 Pre-processing The following list shows the pre-processing steps that our system goes through and the tools used: Part of speech tagging SVMTool version 1.2 (Gim´enez and M`arquez, 2004). Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngai and Florian, 2001), and trained on the British National Corpus (BNC).1 Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling ASSERT version 1.4 (Pradhan et al., 2004). 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed to capture open class words that exhibit strong collocation properties with respect to the different senses of the target preposition. Details of the features in thi</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broadcoverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 594–602, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Andrew Smith</author>
<author>Miles Osborne</author>
</authors>
<title>Scaling conditional random fields using error-correcting codes.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>10--17</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="2344" citStr="Cohn et al. (2005)" startWordPosition="347" endWordPosition="350">hen discuss and analyse the results of our method (Section 5) and conclude the paper (Section 6). 2 Pre-processing The following list shows the pre-processing steps that our system goes through and the tools used: Part of speech tagging SVMTool version 1.2 (Gim´enez and M`arquez, 2004). Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngai and Florian, 2001), and trained on the British National Corpus (BNC).1 Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling ASSERT version 1.4 (Pradhan et al., 2004). 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed to capture open class words that exhibit strong collocation properties with respect</context>
</contexts>
<marker>Cohn, Smith, Osborne, 2005</marker>
<rawString>Trevor Cohn, Andrew Smith, and Miles Osborne. 2005. Scaling conditional random fields using error-correcting codes. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 10–17, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Durand</author>
</authors>
<title>On the translation of prepositions</title>
<date>1993</date>
<booktitle>Linguistic Issues in Machine Translation,</booktitle>
<pages>138--159</pages>
<editor>in multilingual MT. In Frank Van Eynde, editor,</editor>
<publisher>Pinter Publishers,</publisher>
<location>London, UK.</location>
<contexts>
<context position="1036" citStr="Durand, 1993" startWordPosition="145" endWordPosition="146">c features to perform the disambiguation task and achieves a precision of 69.3% over the test data. 1 Introduction Prepositional phrases (PPs) are both common and semantically varied in open English text. While the conventional view on prepositions from the computational linguistics community has been that they are semantically transient at best, and semanticallyvacuous at worst, a robust account of the semantics of prepositions and disambiguation method can be helpful in a range of NLP tasks including machine translation, parsing (prepositional phrase attachment) and semantic role labelling (Durand, 1993; O’Hara and Wiebe, 2003; Ye and Baldwin, 2006a). The SemEval 2007 preposition sense disambiguation task provides a common test bed for the evaluation of preposition sense disambiguation systems. Our proposed method is maximum entropy based, and combines features developed in the context of preposition sense disambiguation for semantic role labelling (Ye and Baldwin, 2006a), and verb sense disambiguation (Ye and Baldwin, 2006b). The remainder of this paper is structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our prepositio</context>
</contexts>
<marker>Durand, 1993</marker>
<rawString>Jacques Durand. 1993. On the translation of prepositions in multilingual MT. In Frank Van Eynde, editor, Linguistic Issues in Machine Translation, pages 138–159. Pinter Publishers, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Svmtool: A general pos tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>43--46</pages>
<location>Lisbon, Portugal.</location>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2004. Svmtool: A general pos tagger generator based on support vector machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 43–46, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1993</date>
<booktitle>In HLT ’93: Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>409--409</pages>
<location>Princeton, USA.</location>
<contexts>
<context position="3573" citStr="Miller, 1993" startWordPosition="533" endWordPosition="534">t senses of the target preposition. Details of the features in this category are listed below. 1This chunker is not exactly the same as Ngai and Florian’s system, however it does use the default transformation templates supplied by fnTBL. 241 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 241–244, Prague, June 2007. c�2007 Association for Computational Linguistics Bag of open class words The part-of-speech (POS) tags and lemmas of all the open class words that occur in the same sentence as the target preposition. Bag of WordNet synsets The WordNet (Miller, 1993) synonym sets and their hypernyms of all the open class words that occur in the same sentence as the target preposition. Bag of named entities Each named entity in the same sentence as the target preposition is treated as a separate feature. Surrounding words These features are the combinations of the lemma, POS tag and relative position of the words surrounding the target preposition within a window of 7 words. Surrounding super senses These features are the combinations of super-sense tag, POS tag and relative position of the words surrounding the target preposition within a window of 7 word</context>
</contexts>
<marker>Miller, 1993</marker>
<rawString>George A. Miller. 1993. Wordnet: a lexical database for english. In HLT ’93: Proceedings of the workshop on Human Language Technology, pages 409–409, Princeton, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proc. of the 2nd Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL2001),</booktitle>
<pages>40--7</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="2122" citStr="Ngai and Florian, 2001" startWordPosition="312" endWordPosition="315"> structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our preposition disambiguation method uses (Section 3) and our parameter tuning method (Section 4). We then discuss and analyse the results of our method (Section 5) and conclude the paper (Section 6). 2 Pre-processing The following list shows the pre-processing steps that our system goes through and the tools used: Part of speech tagging SVMTool version 1.2 (Gim´enez and M`arquez, 2004). Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngai and Florian, 2001), and trained on the British National Corpus (BNC).1 Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling ASSERT version 1.4 (Pradhan et al., 2004). 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features T</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast lane. In Proc. of the 2nd Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL2001), pages 40–7, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Janyce Wiebe</author>
</authors>
<title>Preposition semantic classification via Treebank and FrameNet.</title>
<date>2003</date>
<booktitle>In Proc. of the 7th Conference on Natural Language Learning (CoNLL-2003),</booktitle>
<pages>79--86</pages>
<location>Edmonton, Canada.</location>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>Tom O’Hara and Janyce Wiebe. 2003. Preposition semantic classification via Treebank and FrameNet. In Proc. of the 7th Conference on Natural Language Learning (CoNLL-2003), pages 79–86, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="2492" citStr="Pradhan et al., 2004" startWordPosition="368" endWordPosition="371">re-processing steps that our system goes through and the tools used: Part of speech tagging SVMTool version 1.2 (Gim´enez and M`arquez, 2004). Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngai and Florian, 2001), and trained on the British National Corpus (BNC).1 Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling ASSERT version 1.4 (Pradhan et al., 2004). 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed to capture open class words that exhibit strong collocation properties with respect to the different senses of the target preposition. Details of the features in this category are listed below. 1This chunker is not exactly the same</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2004. Support vector learning for semantic argument classification. Machine Learning, 60(1–3):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, USA.</location>
<contexts>
<context position="2830" citStr="Yarowsky (1995)" startWordPosition="417" endWordPosition="418">rsion August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling ASSERT version 1.4 (Pradhan et al., 2004). 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed to capture open class words that exhibit strong collocation properties with respect to the different senses of the target preposition. Details of the features in this category are listed below. 1This chunker is not exactly the same as Ngai and Florian’s system, however it does use the default transformation templates supplied by fnTBL. 241 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 241–244, Prague, June 2007. c�2007 Association for Computational Linguistics Bag of open class words The part-of-speech (POS) tags and </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semantic role labeling of prepositional phrases.</title>
<date>2006</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="1082" citStr="Ye and Baldwin, 2006" startWordPosition="151" endWordPosition="154">on task and achieves a precision of 69.3% over the test data. 1 Introduction Prepositional phrases (PPs) are both common and semantically varied in open English text. While the conventional view on prepositions from the computational linguistics community has been that they are semantically transient at best, and semanticallyvacuous at worst, a robust account of the semantics of prepositions and disambiguation method can be helpful in a range of NLP tasks including machine translation, parsing (prepositional phrase attachment) and semantic role labelling (Durand, 1993; O’Hara and Wiebe, 2003; Ye and Baldwin, 2006a). The SemEval 2007 preposition sense disambiguation task provides a common test bed for the evaluation of preposition sense disambiguation systems. Our proposed method is maximum entropy based, and combines features developed in the context of preposition sense disambiguation for semantic role labelling (Ye and Baldwin, 2006a), and verb sense disambiguation (Ye and Baldwin, 2006b). The remainder of this paper is structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our preposition disambiguation method uses (Section 3) and o</context>
<context position="8227" citStr="Ye and Baldwin (2006" startWordPosition="1341" endWordPosition="1344">nd this tag is a ’B’ tag, (2) p has no semantic role tag relative to v, but the first token to the right of p has a ’B’ tag relative to v. In the sentence shown in Table 1, stole would be considered as the governor of on. Verb’s relative position The lemma of each verb in the same sentence as the target preposition is combined with its relative position to the target preposition and treated as a separate feature. For example, the sentence shown in Table 1 would generate the two features: (-1, steal) and (1, apologize). More detailed descriptions and examples for these features may be found in Ye and Baldwin (2006b). 4 Parameter Tuning We used the ranking-based feature selection method from Ye and Baldwin (2006b) to select the most relevant feature based on our training data. This method works in two steps. Firstly, we calculated the information gain, gain ratio and Chi-squared statistics for each feature, and used these values to generate 3 sets of rankings for the features. We then summed up the individual ranks, and used the sums to create a set of final rankings for the features. The feature selection process is based on 10-fold cross validation: we divided our training data into 10 pairs of traini</context>
<context position="10469" citStr="Ye and Baldwin, 2006" startWordPosition="1685" endWordPosition="1688"> both automatically tuned features and Gaussian smoothing parameter. 5 Results and Analysis The overall precision (%) obtained by the three classifiers for the fine-grained senses are as follows: Non-tuned Smoothing-tuned Fully tuned 67.9 68.0 69.3 The best overall results were achieved when both the features and the Gaussian smoothing parameters were automatically tuned, achieving a 1.4% absolute precision gain over the non-tuned system. However, such parameter tuning may not always be useful: the same tuning process was found to be detrimental in a Senseval-2 verb sense disambiguation task (Ye and Baldwin, 2006b). Consistent with the findings of Ye and Baldwin (2006b), the improvement caused by the tuning of the Gaussian smoothing parameter is only marginal compared with the improvement caused by the tuning of the features. We also evaluated our features based on their categories and types. Collocation features performed the best among the three feature categories. Without any parameter tuning, the collocation-feature-only classifier achieved an overall precision of 67.4% on the test set; the semantic-role-feature-only classifier and the syntactic-feature-only classifier achieved precision of 46.9% </context>
</contexts>
<marker>Ye, Baldwin, 2006</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2006a. Semantic role labeling of prepositional phrases. ACM Transactions on Asian Language Information Processing (TALIP), 5(3):228–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>Verb sense disambiguation using selectional preferences extracted with a state-of-the-art semantic role labeler.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>141--148</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1082" citStr="Ye and Baldwin, 2006" startWordPosition="151" endWordPosition="154">on task and achieves a precision of 69.3% over the test data. 1 Introduction Prepositional phrases (PPs) are both common and semantically varied in open English text. While the conventional view on prepositions from the computational linguistics community has been that they are semantically transient at best, and semanticallyvacuous at worst, a robust account of the semantics of prepositions and disambiguation method can be helpful in a range of NLP tasks including machine translation, parsing (prepositional phrase attachment) and semantic role labelling (Durand, 1993; O’Hara and Wiebe, 2003; Ye and Baldwin, 2006a). The SemEval 2007 preposition sense disambiguation task provides a common test bed for the evaluation of preposition sense disambiguation systems. Our proposed method is maximum entropy based, and combines features developed in the context of preposition sense disambiguation for semantic role labelling (Ye and Baldwin, 2006a), and verb sense disambiguation (Ye and Baldwin, 2006b). The remainder of this paper is structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our preposition disambiguation method uses (Section 3) and o</context>
<context position="8227" citStr="Ye and Baldwin (2006" startWordPosition="1341" endWordPosition="1344">nd this tag is a ’B’ tag, (2) p has no semantic role tag relative to v, but the first token to the right of p has a ’B’ tag relative to v. In the sentence shown in Table 1, stole would be considered as the governor of on. Verb’s relative position The lemma of each verb in the same sentence as the target preposition is combined with its relative position to the target preposition and treated as a separate feature. For example, the sentence shown in Table 1 would generate the two features: (-1, steal) and (1, apologize). More detailed descriptions and examples for these features may be found in Ye and Baldwin (2006b). 4 Parameter Tuning We used the ranking-based feature selection method from Ye and Baldwin (2006b) to select the most relevant feature based on our training data. This method works in two steps. Firstly, we calculated the information gain, gain ratio and Chi-squared statistics for each feature, and used these values to generate 3 sets of rankings for the features. We then summed up the individual ranks, and used the sums to create a set of final rankings for the features. The feature selection process is based on 10-fold cross validation: we divided our training data into 10 pairs of traini</context>
<context position="10469" citStr="Ye and Baldwin, 2006" startWordPosition="1685" endWordPosition="1688"> both automatically tuned features and Gaussian smoothing parameter. 5 Results and Analysis The overall precision (%) obtained by the three classifiers for the fine-grained senses are as follows: Non-tuned Smoothing-tuned Fully tuned 67.9 68.0 69.3 The best overall results were achieved when both the features and the Gaussian smoothing parameters were automatically tuned, achieving a 1.4% absolute precision gain over the non-tuned system. However, such parameter tuning may not always be useful: the same tuning process was found to be detrimental in a Senseval-2 verb sense disambiguation task (Ye and Baldwin, 2006b). Consistent with the findings of Ye and Baldwin (2006b), the improvement caused by the tuning of the Gaussian smoothing parameter is only marginal compared with the improvement caused by the tuning of the features. We also evaluated our features based on their categories and types. Collocation features performed the best among the three feature categories. Without any parameter tuning, the collocation-feature-only classifier achieved an overall precision of 67.4% on the test set; the semantic-role-feature-only classifier and the syntactic-feature-only classifier achieved precision of 46.9% </context>
</contexts>
<marker>Ye, Baldwin, 2006</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2006b. Verb sense disambiguation using selectional preferences extracted with a state-of-the-art semantic role labeler. In Proceedings of the Australasian Language Technology Workshop, pages 141– 148, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>