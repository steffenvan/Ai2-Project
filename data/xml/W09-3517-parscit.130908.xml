<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000896">
<title confidence="0.677004">
English to Hindi Machine Transliteration System at NEWS 2009
</title>
<author confidence="0.517282">
Amitava Das, Asif Ekbal, Tapabrata Mandal and Sivaji Bandyopadhyay
</author>
<affiliation confidence="0.4132695">
Computer Science and Engineering Department
Jadavpur University, Kolkata-700032, India
</affiliation>
<email confidence="0.762323">
amitava.research@gmail.com, asif.ekbal@gmail.com, ta-
pabratamondal@gmail.com, sivaji_cse_ju@yahoo.com
</email>
<sectionHeader confidence="0.993613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9922876">
This paper reports about our work in the
NEWS 2009 Machine Transliteration Shared
Task held as part of ACL-IJCNLP 2009. We
submitted one standard run and two non-
standard runs for English to Hindi translitera-
tion. The modified joint source-channel model
has been used along with a number of alterna-
tives. The system has been trained on the
NEWS 2009 Machine Transliteration Shared
Task datasets. For standard run, the system
demonstrated an accuracy of 0.471 and the
mean F-Score of 0.861. The non-standard runs
yielded the accuracy and mean F-scores of
0.389 and 0.831 respectively in the first one
and 0.384 and 0.828 respectively in the second
one. The non-standard runs resulted in sub-
stantially worse performance than the standard
run. The reasons for this are the ranking algo-
rithm used for the output and the types of to-
kens present in the test set.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999638054054054">
Technical terms and named entities (NEs) consti-
tute the bulk of the Out Of Vocabulary (OOV)
words. Named entities are usually not found in
bilingual dictionaries and are very generative in
nature. Proper identification, classification and
translation of Named entities (NEs) are very im-
portant in many Natural Language Processing
(NLP) applications. Translation of NEs involves
both translation and transliteration. Translitera-
tion is the method of translating into another lan-
guage by expressing the original foreign word
using characters of the target language preserv-
ing the pronunciation in their source language.
Thus, the central problem in transliteration is
predicting the pronunciation of the original word.
Transliteration between two languages that use
the same set of alphabets is trivial: the word is
left as it is. However, for languages those use
different alphabet sets the names must be transli-
terated or rendered in the target language alpha-
bets. Transliteration of NEs is necessary in many
applications, such as machine translation, corpus
alignment, cross-language Information Retrieval,
information extraction and automatic lexicon
acquisition. In the literature, a number of transli-
teration algorithms are available involving Eng-
lish (Li et al., 2004; Vigra and Khudanpur, 2003;
Goto et al., 2003), European languages (Marino
et al., 2005) and some of the Asian languages,
namely Chinese (Li et al., 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al., 2003;
Knight and Graehl, 1998), Korean (Jung et al.,
2000) and Arabic (Al-Onaizan and Knight,
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving
Indian languages (Ekbal et al., 2006; Ekbal et al.,
2007; Surana and Singh, 2008).
</bodyText>
<sectionHeader confidence="0.984125" genericHeader="method">
2 Machine Transliteration Systems
</sectionHeader>
<bodyText confidence="0.999756904761905">
Three transliteration models have been used that
can generate the Hindi transliteration from an
English named entity (NE). An English NE is
divided into Transliteration Units (TUs) with
patterns C*V*, where C represents a consonant
and V represents a vowel. The Hindi NE is di-
vided into TUs with patterns C+M?, where C
represents a consonant or a vowel or a conjunct
and M represents the vowel modifier or matra.
The TUs are the lexical units for machine transli-
teration. The system considers the English and
Hindi contextual information in the form of col-
located TUs simultaneously to calculate the plau-
sibility of transliteration from each English TU
to various Hindi candidate TUs and chooses the
one with maximum probability. This is equiva-
lent to choosing the most appropriate sense of a
word in the source language to identify its repre-
sentation in the target language. The system
learns the mappings automatically from the bi-
lingual NEWS training set being guided by lin-
</bodyText>
<page confidence="0.979782">
80
</page>
<note confidence="0.983677">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 80–83,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999192125">
guistic features/knowledge. The system consid-
ers the linguistic knowledge in the form of con-
juncts and/or diphthongs in English and their
possible transliteration in Hindi. The output of
the mapping process is a decision-list classifier
with collocated TUs in the source language and
their equivalent TUs in collocation in the target
language along with the probability of each deci-
sion obtained from the training set. Linguistic
knowledge is used in order to make the number
of TUs in both the source and target sides equal.
A Direct example base has been maintained that
contains the bilingual training examples that do
not result in the equal number of TUs in both the
source and target sides during alignment. The
Direct example base is checked first during ma-
chine transliteration of the input English word. If
no match is obtained, the system uses direct or-
thographic mapping by identifying the equivalent
Hindi TU for each English TU in the input and
then placing the Hindi TUs in order. The transli-
teration models are described below in which S
and T denotes the source and the target words
respectively:
</bodyText>
<listItem confidence="0.97896">
• Model A
</listItem>
<bodyText confidence="0.9998364">
This is essentially the joint source-channel model
(Hazhiou et al., 2004) where the previous TUs
with reference to the current TUs in both the
source (s) and the target sides (t) are considered
as the context.
</bodyText>
<listItem confidence="0.394074">
S →T(S) = arg max T{P(T)×P(S  |T)}
• Model B
</listItem>
<bodyText confidence="0.999831666666667">
This is basically the trigram model where the
previous and the next source TUs are considered
as the context.
</bodyText>
<equation confidence="0.4357865">
k 1
S →T(S) = arg max T{P(T)×P(S  |T)}
</equation>
<listItem confidence="0.795113">
• Model C
</listItem>
<bodyText confidence="0.999959">
In this model, the previous and the next TUs in
the source and the previous target TU are
considered as the context. This is the improved
modified joint source-channel model.
</bodyText>
<subsectionHeader confidence="0.716311">
S →T(S) = arg max T{P(T)×P(S  |T)}
</subsectionHeader>
<bodyText confidence="0.999613222222222">
For NE transliteration, P(T), i.e., the
probability of transliteration in the target
language, is calculated from a English-Hindi
bilingual database of approximately 961,890
English person names, collected from the web1.
If, T is not found in the dictionary, then a very
small value is assigned to P(T). These models
have been desribed in details in Ekbal et al.
(2007).
</bodyText>
<listItem confidence="0.886967">
• Post-Processing
</listItem>
<bodyText confidence="0.932649142857143">
Depending upon the nature of errors involved in
the results, we have devised a set of translitera-
tion rules. A few rules have been devised to pro-
duce more spelling variations. Some examples
are given below.
Spelling variation rules
Badlapur ~~~~~~Z 1 Z
</bodyText>
<sectionHeader confidence="0.810204" genericHeader="method">
Shree I Shri J)ft
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999095769230769">
We have trained our transliteration models using
the English-Hindi datasets obtained from the
NEWS 2009 Machine Transliteration Shared
Task (Li et al., 2009). A brief statistics of the
datasets are presented in Table 1. Out of 9975
English-Hindi parallel examples in the training
set, 4009 are multi-words. During training, we
have split these multi-words into collections of
single word transliterations. It was observed that
the number of tokens in the source and target
sides mismatched in 22 multi-words and these
cases were not considered further. Following are
some examples:
</bodyText>
<subsectionHeader confidence="0.855891">
Paris Charles de Gaulle �����
</subsectionHeader>
<bodyText confidence="0.862493">
���� ����� �� �����
South Arlington Church of
Christ u113$ 3iWwr
In the training set, some multi-words were partly
translated and not transliterated. Such examples
were dropped from the training set. Finally, the
training set consists of 15905 single word Eng-
lish-Hindi parallel examples.
</bodyText>
<equation confidence="0.998058125">
=1
k
K
P
(  |)
S T = ∏ P(&lt; s, t &gt;k k s, t &gt;k−1)
K
P
(  |)
S T = ∏ P(&lt; s, t &gt;k  |sk−1,sk+1)
=1
k
K
P
(  |)
S T = ∏ P(&lt; s, t &gt;k k s, t &gt;k−1, sk+1)
</equation>
<footnote confidence="0.943966">
1http://www.eci.gov.in/DevForum/Fullname.asp
</footnote>
<page confidence="0.990199">
81
</page>
<table confidence="0.9930025">
Set Number of examples
Training 9975
Development 974
Test 1000
</table>
<tableCaption confidence="0.999791">
Table 1. Statistics of Dataset
</tableCaption>
<bodyText confidence="0.99981309375">
The output of the modified joint source-
channel model is given more priority during out-
put ranking followed by the trigram and the joint
source-channel model. During testing, the Direct
example base is searched first to find the transli-
teration. Experimental results on the develop-
ment set yielded the accuracy of 0.442 and mean
F-score of 0.829. Depending upon the nature of
errors involved in the results, we have devised a
set of transliteration rules. The use of these trans-
literation rules increased the accuracy and mean
F-score values up to 0.489 and 0.881 respective-
ly.
The system has been evaluated for the test set
and the detailed reports are available in Li et al.
(2009). There are 88.88% unknown examples in
the test set. We submitted one standard run in
which the outputs are provided for the modified
joint source-channel model (Model C), trigram
model (Model B) and joint source-channel model
(Model A). The same ranking procedure (i.e.,
Model C, Model B and Model A) has been fol-
lowed as that of the development set. The output
of each transliteration model has been post-
processed with the set of transliteration rules. For
each word, three different outputs are provided in
a ranked order. If the outputs of any two models
are same for any word then only two outputs are
provided for that particular word. Post-
processing rules generate more number of possi-
ble transliteration output. Evaluation results of
the standard run are shown in Table 2.
</bodyText>
<table confidence="0.996968777777778">
Parameters Accuracy
Accuracy in top-1 0.471
Mean F-score 0.861
Mean Reciprocal Rank 0.519
(MRR)
Mean Average Preci- 0.463
sion (MAP)ref
MAP10 0.162
MAPsys 0.383
</table>
<tableCaption confidence="0.999905">
Table 2. Results of the standard run
</tableCaption>
<bodyText confidence="0.7521075">
The results of the two non-standard runs are
presented in Table 3 and Table 4 respectively.
</bodyText>
<table confidence="0.998858333333333">
Parameters Accuracy
Accuracy in top-1 0.389
Mean F-score 0.831
Mean Reciprocal Rank 0.487
(MRR)
Mean Average Preci- 0.385
sion (MAP)ref
MAP10 0.16
MAPsys 0.328
</table>
<tableCaption confidence="0.997083">
Table 3. Results of the non-standard run 1
</tableCaption>
<table confidence="0.999639555555556">
Parameters Accuracy
Accuracy in top-1 0.384
Mean F-score 0.823
Mean Reciprocal Rank 0.485
(MRR)
Mean Average Precision 0.380
(MAP)ref
MAP10 0.16
MAPsys 0.325
</table>
<tableCaption confidence="0.999762">
Table 4. Results of the non-standard run2
</tableCaption>
<bodyText confidence="0.99952880952381">
In both the non-standard runs, we have used
an English-Hindi bilingual database of approx-
imately 961, 890 examples that have been col-
lected from the web2. This database contains the
(frequency) of the corresponding English-Hindi
name pair. Along with the outputs of three mod-
els, the output obtained from this bilingual data-
base has been also provided for each English
word. In the first non-standard run, only the most
frequent transliteration has been considered. But,
in the second non-standard run all the possible
transliteration have been considered. It is to be
noted that in these two non-standard runs, the
transliterations obtained from the bilingual data-
base have been kept first in the ranking. Results
of the tables show quite similar performance in
both the runs. But the non-standard runs resulted
in substantially worse performance than the stan-
dard run. The reasons for this are the ranking
algorithm used for the output and the types of
tokens present in the test set. The additional da-
</bodyText>
<footnote confidence="0.9215">
2http://www.eci.gov.in/DevForum/Fullname.asp
</footnote>
<page confidence="0.99859">
82
</page>
<bodyText confidence="0.999898944444445">
taset used for the non-standard runs is mainly
census data consisting of only Indian person
names. The NEWS 2009 Machine Transliteration
Shared Task training set is well distributed with
foreign names (Ex. Sweden, Warren), common
nouns (Mahfuz, Darshanaa) and a few non
named entities. Hence the training set for the
non-standard runs was biased towards the Indian
person name transliteration pattern. Additional
training set was quite larger (961, 890) than the
shared task training set (9,975). Actually outputs
of non-standard runs have more alternative trans-
literation outputs than the standard set. That
means non-standard sets are superset of standard
set. Our observation is that the ranking algorithm
used for the output and biased training are the
main reasons for the worse performance of the
non-standard runs.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9997609375">
This paper reports about our works as part of the
NEWS 2009 Machine Transliteration Shared
Task. We have used the modified joint source-
channel model along with two other alternatives
to generate the Hindi transliteration from an Eng-
lish word (to generate more spelling variations of
Hindi names). We have also devised some post-
processing rules to remove the errors. During
standard run, we have obtained the word accura-
cy of 0.471 and mean F-score of 0.831. In non-
standard rune, we have used a bilingual database
obtained from the web. The non-standard runs
yielded the word accuracy and mean F-score
values of 0.389 and 0.831 respectively in the first
run and 0.384 and 0.823 respectively in the
second run.
</bodyText>
<sectionHeader confidence="0.998587" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999806768115942">
Al-Onaizan, Y. and Knight, K. 2002a. Named
Entity Translation: Extended Abstract. In
Proceedings of the Human Language Tech-
nology Conference, 122– 124.
Al-Onaizan, Y. and Knight, K. 2002b. Translat-
ing Named Entities using Monolingual and
Bilingual Resources. In Proceedings of the
40th Annual Meeting of the ACL, 400–408,
USA.
Ekbal, A. Naskar, S. and Bandyopadhyay, S.
2007. Named Entity Transliteration. Interna-
tional Journal of Computer Processing of
Oriental Languages (IJCPOL), Volume
(20:4), 289-310, World Scientific Publishing
Company, Singapore.
Ekbal, A., Naskar, S. and Bandyopadhyay, S.
2006. A Modified Joint Source Channel
Model for Transliteration. In Proceedings of
the COLING-ACL 2006, 191-198, Australia.
Goto, I., Kato, N., Uratani, N. and Ehara, T.
2003. Transliteration Considering Context
Information based on the Maximum Entropy
Method. In Proceeding of the MT-Summit
IX, 125–132, New Orleans, USA.
Jung, Sung Young , Sung Lim Hong and Eunok
Paek. 2000. An English to Korean Translite-
ration Model of Extended Markov Window.
In Proceedings of International Conference
on Computational Linguistics (COLING
2000), 383-389.
Knight, K. and Graehl, J. 1998. Machine Transli-
teration, Computational Linguistics, Volume
(24:4), 599–612.
Kumaran, A. and Tobias Kellner. 2007. A gener-
ic framework for machine transliteration. In
Proc. of the 30th SIGIR.
Li, Haizhou, A Kumaran, Min Zhang and Vla-
dimir Pervouchine. 2009. Whitepaper of
NEWS 2009 Machine Transliteration Shared
Task. In Proceedings of ACL-IJCNLP 2009
Named Entities Workshop (NEWS 2009), Sin-
gapore.
Li, Haizhou, A Kumaran, Vladimir Pervouchine
and Min Zhang. 2009. Report on NEWS 2009
Machine Transliteration Shared Task. In Pro-
ceedings of ACL-IJCNLP 2009 amed Entities
Workshop (NEWS 2009), Singapore.
Li, Haizhou, Min Zhang and Su Jian. 2004. A
Joint Source-Channel Model for Machine
Transliteration. In Proceedings of the 42nd
Annual Meeting of the ACL, 159-166. Spain.
Marino, J. B., R. Banchs, J. M. Crego, A. de
Gispert, P. Lambert, J. A. Fonollosa and M.
Ruiz. 2005. Bilingual n-gram Statistical
Machine Translation. In Proceedings of the
MT-Summit X, 275–282.
Surana, Harshit, and Singh, Anil Kumar. 2008. A
More Discerning and Adaptable Multilingual
Transliteration Mechanism for Indian Lan-
guages. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Lan-
guage Processing (IJCNLP-08), 64-71, In-
dia.
Vigra, Paola and Khudanpur, S. 2003. Translite-
ration of Proper Names in Cross-Lingual In-
formation Retrieval. In Proceedings of the
ACL 2003 Workshop on Multilingual and
Mixed-Language Named Entity Recognition,
57–60.
</reference>
<page confidence="0.999311">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476132">
<note confidence="0.837616">English to Hindi Machine Transliteration System at NEWS 2009</note>
<author confidence="0.674428">Amitava Das</author>
<author confidence="0.674428">Asif Ekbal</author>
<author confidence="0.674428">Tapabrata Mandal</author>
<author confidence="0.674428">Sivaji</author>
<affiliation confidence="0.883597">Computer Science and Engineering</affiliation>
<address confidence="0.901061">Jadavpur University, Kolkata-700032, India</address>
<email confidence="0.97759">asif.ekbal@gmail.com,pabratamondal@gmail.com,sivaji_cse_ju@yahoo.com</email>
<abstract confidence="0.998972">This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard run and two nonstandard runs for English to Hindi transliteration. The modified joint source-channel model has been used along with a number of alternatives. The system has been trained on the NEWS 2009 Machine Transliteration Shared Task datasets. For standard run, the system demonstrated an accuracy of 0.471 and the mean F-Score of 0.861. The non-standard runs yielded the accuracy and mean F-scores of 0.389 and 0.831 respectively in the first one and 0.384 and 0.828 respectively in the second one. The non-standard runs resulted in substantially worse performance than the standard run. The reasons for this are the ranking algorithm used for the output and the types of tokens present in the test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Named Entity Translation: Extended Abstract.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<volume>122</volume>
<pages>124</pages>
<contexts>
<context position="2773" citStr="Al-Onaizan and Knight, 2002" startWordPosition="415" endWordPosition="418">age alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE). An English NE is divided into Transliteration Units (TUs) with patterns C*V*, where C represents a consonant and V represents a vowel. The Hindi NE is divided into TUs with patterns C+M?, where C represents a consonant or a vowel or a conjunct and M represents the vowel modi</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and Knight, K. 2002a. Named Entity Translation: Extended Abstract. In Proceedings of the Human Language Technology Conference, 122– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Translating Named Entities using Monolingual and Bilingual Resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<location>400–408, USA.</location>
<contexts>
<context position="2773" citStr="Al-Onaizan and Knight, 2002" startWordPosition="415" endWordPosition="418">age alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE). An English NE is divided into Transliteration Units (TUs) with patterns C*V*, where C represents a consonant and V represents a vowel. The Hindi NE is divided into TUs with patterns C+M?, where C represents a consonant or a vowel or a conjunct and M represents the vowel modi</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and Knight, K. 2002b. Translating Named Entities using Monolingual and Bilingual Resources. In Proceedings of the 40th Annual Meeting of the ACL, 400–408, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Naskar Ekbal</author>
<author>S</author>
<author>S Bandyopadhyay</author>
</authors>
<title>Named Entity Transliteration.</title>
<date>2007</date>
<journal>International Journal of Computer Processing of Oriental Languages (IJCPOL), Volume</journal>
<volume>20</volume>
<pages>289--310</pages>
<institution>World Scientific Publishing Company, Singapore.</institution>
<contexts>
<context position="2914" citStr="Ekbal et al., 2007" startWordPosition="437" endWordPosition="440">rieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE). An English NE is divided into Transliteration Units (TUs) with patterns C*V*, where C represents a consonant and V represents a vowel. The Hindi NE is divided into TUs with patterns C+M?, where C represents a consonant or a vowel or a conjunct and M represents the vowel modifier or matra. The TUs are the lexical units for machine transliteration. The system considers the English and Hindi contextual information i</context>
<context position="6202" citStr="Ekbal et al. (2007)" startWordPosition="991" endWordPosition="994">T(S) = arg max T{P(T)×P(S |T)} • Model C In this model, the previous and the next TUs in the source and the previous target TU are considered as the context. This is the improved modified joint source-channel model. S →T(S) = arg max T{P(T)×P(S |T)} For NE transliteration, P(T), i.e., the probability of transliteration in the target language, is calculated from a English-Hindi bilingual database of approximately 961,890 English person names, collected from the web1. If, T is not found in the dictionary, then a very small value is assigned to P(T). These models have been desribed in details in Ekbal et al. (2007). • Post-Processing Depending upon the nature of errors involved in the results, we have devised a set of transliteration rules. A few rules have been devised to produce more spelling variations. Some examples are given below. Spelling variation rules Badlapur ~~~~~~Z 1 Z Shree I Shri J)ft 3 Experimental Results We have trained our transliteration models using the English-Hindi datasets obtained from the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). A brief statistics of the datasets are presented in Table 1. Out of 9975 English-Hindi parallel examples in the training set, 4</context>
</contexts>
<marker>Ekbal, S, Bandyopadhyay, 2007</marker>
<rawString>Ekbal, A. Naskar, S. and Bandyopadhyay, S. 2007. Named Entity Transliteration. International Journal of Computer Processing of Oriental Languages (IJCPOL), Volume (20:4), 289-310, World Scientific Publishing Company, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ekbal</author>
<author>S Naskar</author>
<author>S Bandyopadhyay</author>
</authors>
<title>A Modified Joint Source Channel Model for Transliteration.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL</booktitle>
<pages>191--198</pages>
<contexts>
<context position="2894" citStr="Ekbal et al., 2006" startWordPosition="433" endWordPosition="436">uage Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE). An English NE is divided into Transliteration Units (TUs) with patterns C*V*, where C represents a consonant and V represents a vowel. The Hindi NE is divided into TUs with patterns C+M?, where C represents a consonant or a vowel or a conjunct and M represents the vowel modifier or matra. The TUs are the lexical units for machine transliteration. The system considers the English and Hindi cont</context>
</contexts>
<marker>Ekbal, Naskar, Bandyopadhyay, 2006</marker>
<rawString>Ekbal, A., Naskar, S. and Bandyopadhyay, S. 2006. A Modified Joint Source Channel Model for Transliteration. In Proceedings of the COLING-ACL 2006, 191-198, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Goto</author>
<author>N Kato</author>
<author>N Uratani</author>
<author>T Ehara</author>
</authors>
<title>Transliteration Considering Context Information based on the Maximum Entropy Method.</title>
<date>2003</date>
<booktitle>In Proceeding of the MT-Summit IX,</booktitle>
<pages>125--132</pages>
<location>New Orleans, USA.</location>
<contexts>
<context position="2515" citStr="Goto et al., 2003" startWordPosition="372" endWordPosition="375"> original word. Transliteration between two languages that use the same set of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE). An English NE is </context>
</contexts>
<marker>Goto, Kato, Uratani, Ehara, 2003</marker>
<rawString>Goto, I., Kato, N., Uratani, N. and Ehara, T. 2003. Transliteration Considering Context Information based on the Maximum Entropy Method. In Proceeding of the MT-Summit IX, 125–132, New Orleans, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Lim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An English to Korean Transliteration Model of Extended Markov Window.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING</booktitle>
<pages>383--389</pages>
<marker>Hong, Paek, 2000</marker>
<rawString>Jung, Sung Young , Sung Lim Hong and Eunok Paek. 2000. An English to Korean Transliteration Model of Extended Markov Window. In Proceedings of International Conference on Computational Linguistics (COLING 2000), 383-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration, Computational Linguistics, Volume</journal>
<volume>24</volume>
<pages>599--612</pages>
<contexts>
<context position="2705" citStr="Knight and Graehl, 1998" startWordPosition="404" endWordPosition="407"> the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE). An English NE is divided into Transliteration Units (TUs) with patterns C*V*, where C represents a consonant and V represents a vowel. The Hindi NE is divided into TUs with patterns C+M?, where C represents </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Knight, K. and Graehl, J. 1998. Machine Transliteration, Computational Linguistics, Volume (24:4), 599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proc. of the 30th SIGIR.</booktitle>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>Kumaran, A. and Tobias Kellner. 2007. A generic framework for machine transliteration. In Proc. of the 30th SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<title>Machine Transliteration Shared Task.</title>
<date>2009</date>
<journal>Whitepaper of NEWS</journal>
<booktitle>In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="6673" citStr="Li et al., 2009" startWordPosition="1066" endWordPosition="1069"> T is not found in the dictionary, then a very small value is assigned to P(T). These models have been desribed in details in Ekbal et al. (2007). • Post-Processing Depending upon the nature of errors involved in the results, we have devised a set of transliteration rules. A few rules have been devised to produce more spelling variations. Some examples are given below. Spelling variation rules Badlapur ~~~~~~Z 1 Z Shree I Shri J)ft 3 Experimental Results We have trained our transliteration models using the English-Hindi datasets obtained from the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). A brief statistics of the datasets are presented in Table 1. Out of 9975 English-Hindi parallel examples in the training set, 4009 are multi-words. During training, we have split these multi-words into collections of single word transliterations. It was observed that the number of tokens in the source and target sides mismatched in 22 multi-words and these cases were not considered further. Following are some examples: Paris Charles de Gaulle ����� ���� ����� �� ����� South Arlington Church of Christ u113$ 3iWwr In the training set, some multi-words were partly translated and not translitera</context>
<context position="8382" citStr="Li et al. (2009)" startWordPosition="1364" endWordPosition="1367"> priority during output ranking followed by the trigram and the joint source-channel model. During testing, the Direct example base is searched first to find the transliteration. Experimental results on the development set yielded the accuracy of 0.442 and mean F-score of 0.829. Depending upon the nature of errors involved in the results, we have devised a set of transliteration rules. The use of these transliteration rules increased the accuracy and mean F-score values up to 0.489 and 0.881 respectively. The system has been evaluated for the test set and the detailed reports are available in Li et al. (2009). There are 88.88% unknown examples in the test set. We submitted one standard run in which the outputs are provided for the modified joint source-channel model (Model C), trigram model (Model B) and joint source-channel model (Model A). The same ranking procedure (i.e., Model C, Model B and Model A) has been followed as that of the development set. The output of each transliteration model has been postprocessed with the set of transliteration rules. For each word, three different outputs are provided in a ranked order. If the outputs of any two models are same for any word then only two outpu</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Li, Haizhou, A Kumaran, Min Zhang and Vladimir Pervouchine. 2009. Whitepaper of NEWS 2009 Machine Transliteration Shared Task. In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<date>2009</date>
<booktitle>Report on NEWS 2009 Machine Transliteration Shared Task. In Proceedings of ACL-IJCNLP 2009 amed Entities Workshop (NEWS</booktitle>
<contexts>
<context position="6673" citStr="Li et al., 2009" startWordPosition="1066" endWordPosition="1069"> T is not found in the dictionary, then a very small value is assigned to P(T). These models have been desribed in details in Ekbal et al. (2007). • Post-Processing Depending upon the nature of errors involved in the results, we have devised a set of transliteration rules. A few rules have been devised to produce more spelling variations. Some examples are given below. Spelling variation rules Badlapur ~~~~~~Z 1 Z Shree I Shri J)ft 3 Experimental Results We have trained our transliteration models using the English-Hindi datasets obtained from the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). A brief statistics of the datasets are presented in Table 1. Out of 9975 English-Hindi parallel examples in the training set, 4009 are multi-words. During training, we have split these multi-words into collections of single word transliterations. It was observed that the number of tokens in the source and target sides mismatched in 22 multi-words and these cases were not considered further. Following are some examples: Paris Charles de Gaulle ����� ���� ����� �� ����� South Arlington Church of Christ u113$ 3iWwr In the training set, some multi-words were partly translated and not translitera</context>
<context position="8382" citStr="Li et al. (2009)" startWordPosition="1364" endWordPosition="1367"> priority during output ranking followed by the trigram and the joint source-channel model. During testing, the Direct example base is searched first to find the transliteration. Experimental results on the development set yielded the accuracy of 0.442 and mean F-score of 0.829. Depending upon the nature of errors involved in the results, we have devised a set of transliteration rules. The use of these transliteration rules increased the accuracy and mean F-score values up to 0.489 and 0.881 respectively. The system has been evaluated for the test set and the detailed reports are available in Li et al. (2009). There are 88.88% unknown examples in the test set. We submitted one standard run in which the outputs are provided for the modified joint source-channel model (Model C), trigram model (Model B) and joint source-channel model (Model A). The same ranking procedure (i.e., Model C, Model B and Model A) has been followed as that of the development set. The output of each transliteration model has been postprocessed with the set of transliteration rules. For each word, three different outputs are provided in a ranked order. If the outputs of any two models are same for any word then only two outpu</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Li, Haizhou, A Kumaran, Vladimir Pervouchine and Min Zhang. 2009. Report on NEWS 2009 Machine Transliteration Shared Task. In Proceedings of ACL-IJCNLP 2009 amed Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Su Jian</author>
</authors>
<title>A Joint Source-Channel Model for Machine Transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="2468" citStr="Li et al., 2004" startWordPosition="364" endWordPosition="367">ation is predicting the pronunciation of the original word. Transliteration between two languages that use the same set of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from </context>
</contexts>
<marker>Li, Zhang, Jian, 2004</marker>
<rawString>Li, Haizhou, Min Zhang and Su Jian. 2004. A Joint Source-Channel Model for Machine Transliteration. In Proceedings of the 42nd Annual Meeting of the ACL, 159-166. Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Marino</author>
<author>R Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A Fonollosa</author>
<author>M Ruiz</author>
</authors>
<title>Bilingual n-gram Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT-Summit X,</booktitle>
<pages>275--282</pages>
<marker>Marino, Banchs, Crego, de Gispert, Lambert, Fonollosa, Ruiz, 2005</marker>
<rawString>Marino, J. B., R. Banchs, J. M. Crego, A. de Gispert, P. Lambert, J. A. Fonollosa and M. Ruiz. 2005. Bilingual n-gram Statistical Machine Translation. In Proceedings of the MT-Summit X, 275–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harshit Surana</author>
<author>Anil Kumar Singh</author>
</authors>
<title>A More Discerning and Adaptable Multilingual Transliteration Mechanism for Indian Languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP-08),</booktitle>
<pages>64--71</pages>
<contexts>
<context position="2939" citStr="Surana and Singh, 2008" startWordPosition="441" endWordPosition="444">extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE). An English NE is divided into Transliteration Units (TUs) with patterns C*V*, where C represents a consonant and V represents a vowel. The Hindi NE is divided into TUs with patterns C+M?, where C represents a consonant or a vowel or a conjunct and M represents the vowel modifier or matra. The TUs are the lexical units for machine transliteration. The system considers the English and Hindi contextual information in the form of collocated </context>
</contexts>
<marker>Surana, Singh, 2008</marker>
<rawString>Surana, Harshit, and Singh, Anil Kumar. 2008. A More Discerning and Adaptable Multilingual Transliteration Mechanism for Indian Languages. In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP-08), 64-71, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Vigra</author>
<author>S Khudanpur</author>
</authors>
<title>Transliteration of Proper Names in Cross-Lingual Information Retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-Language Named Entity Recognition,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="2495" citStr="Vigra and Khudanpur, 2003" startWordPosition="368" endWordPosition="371">ng the pronunciation of the original word. Transliteration between two languages that use the same set of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). 2 Machine Transliteration Systems Three transliteration models have been used that can generate the Hindi transliteration from an English named entity (NE</context>
</contexts>
<marker>Vigra, Khudanpur, 2003</marker>
<rawString>Vigra, Paola and Khudanpur, S. 2003. Transliteration of Proper Names in Cross-Lingual Information Retrieval. In Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-Language Named Entity Recognition, 57–60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>