<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.068256">
<title confidence="0.99393">
Revisiting Word Embedding for Contrasting Meaning
</title>
<author confidence="0.993973">
Zhigang Chen†, Wei Lin‡, Qian Chen∗,
Xiaoping Chen†, Si Wei‡, Hui Jiang§ and Xiaodan Zhu§
</author>
<affiliation confidence="0.99704875">
†School of Computer Science and Technology, ∗NELSLIP,
University of Science and Technology of China, Hefei, China
‡ iFLYTEK Research, Hefei, China
§Department of EECS, York University, Toronto, Canada
</affiliation>
<email confidence="0.883057">
emails: zgchen9517017@gmail.com, weilin2@iflytek.com, cq1231@mail.ustc.edu.cn,
xpchen@ustc.edu.cn, siwei@iflytek.com, hj@cse.yorku.ca, zhu2048@gmail.com
</email>
<sectionHeader confidence="0.997052" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928076923077">
Contrasting meaning is a basic aspect of
semantics. Recent word-embedding mod-
els based on distributional semantics hy-
pothesis are known to be weak for mod-
eling lexical contrast. We present in this
paper the embedding models that achieve
an F-score of 92% on the widely-used,
publicly available dataset, the GRE “most
contrasting word” questions (Mohammad
et al., 2008). This is the highest perfor-
mance seen so far on this dataset. Sur-
prisingly at the first glance, unlike what
was suggested in most previous work,
where relatedness statistics learned from
corpora is claimed to yield extra gains
over lexicon-based models, we obtained
our best result relying solely on lexical re-
sources (Roget’s and WordNet)—corpora
statistics did not lead to further improve-
ment. However, this should not be sim-
ply taken as that distributional statistics is
not useful. We examine several basic con-
cerns in modeling contrasting meaning to
provide detailed analysis, with the aim to
shed some light on the future directions for
this basic semantics modeling problem.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865333333334">
Learning good representations of meaning for dif-
ferent granularities of texts is core to human lan-
guage understanding, where a basic problem is
representing the meanings of words. Distributed
representations learned with neural networks have
recently showed to result in significant improve-
ment of performance on a number of language
understanding problems (e.g., speech recognition
and automatic machine translation) and on many
non-language problems (e.g., image recognition).
Distributed representations have been leveraged
to represent words as in (Collobert et al., 2011;
Mikolov et al., 2013).
Contrasting meaning is a basic aspect of seman-
tics, but it is widely known that word embedding
models based on distributional semantics hypoth-
esis are weak in modeling this—contrasting mean-
ing is often lost in the low-dimensional spaces
based on such a hypothesis, and better models
would be desirable.
Lexical contrast has been modeled in (Lin and
Zhao, 2003; Mohammad et al., 2008; Moham-
mad et al., 2013). The recent literature has also
included research efforts of modeling contrasting
meaning in embedding spaces, leading to state-
of-the-art performances. For example, Yih et al.
(2012) proposed to use polarity-primed latent se-
mantic analysis (LSA), called PILSA, to capture
contrast, which was further used to initialize a neu-
ral network and achieved an F-score of 81% on
the same GRE “most contrasting word” questions
(Mohammad et al., 2008). More recently, Zhang
et al. (2014) proposed a tensor factorization ap-
proach to solving the problem, resulting in a 82%
F-score.
In this paper, we present embedding models that
achieve an F-score of 92% on the GRE dataset,
which outperforms the previous best result (82%)
by a large margin. Unlike what was suggested in
previous work, where relatedness statistics learned
from corpora is often claimed to yield extra gains
over lexicon-based models, we obtained this new
state-of-the-art result relying solely on lexical re-
sources (Roget’s and WordNet), and corpus statis-
tics does not seem to bring further improvement.
To provide a comprehensive understanding, we
constructed our study in a framework that exam-
ines a number of basic concerns in modeling con-
trasting meaning. We hope our efforts would help
shed some light on future directions for this basic
semantic modeling problem.
</bodyText>
<page confidence="0.973507">
106
</page>
<note confidence="0.983710333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 106–115,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.99913" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999869961538462">
The terms contrasting, opposite, and antonym
have different definitions in the literature, while
sometimes they are used interchangeably. Follow-
ing (Mohammad et al., 2013), in this paper we re-
fer to opposites as word pairs that “have a strong
binary incompatibility relation with each other or
that are saliently different across a dimension of
meaning”, e.g., day and night. Antonyms are a sub-
set of opposites that are also gradable adjectives,
with same definition as in (Cruse, 1986) as well.
Contrasting word pairs have the broadest mean-
ing among them, referring to word pairs having
“some non-zero degree of binary incompatibility
and/or have some non-zero difference across a di-
mension of meaning.” Therefore by definition, op-
posites are a subset of contrasting word pairs (refer
to (Mohammad et al., 2013) for detailed discus-
sions).
Word Embedding Word embedding models learn
continuous representations for words in a low di-
mensional space (Turney and Pantel, 2010; Hin-
ton and Roweis, 2002; Collobert et al., 2011;
Mikolov et al., 2013; Liu et al., 2015), which is not
new. Linear dimension reduction such as Latent
Semantic Analysis (LSA) has been extensively
used in lexical semantics (see (Turney and Pantel,
2010) for good discussions in vector space mod-
els.) Non-linear models such as those described
in (Roweis and Saul, 2000) and (Tenenbaum et
al., 2000), among many others, can also be ap-
plied to learn word embeddings. A particularly in-
teresting model is stochastic neighbor embedding
(SNE) (Hinton and Roweis, 2002), which explic-
itly enforces that in the embedding space, the dis-
tribution of neighbors of a given word to be similar
to that in the original, uncompressed space. SNE
can learn multiple senses of a word with a mix-
ture component. Recently, neural-network based
model such as those proposed by (Collobert et al.,
2011) and (Mikolov et al., 2013) have attracted ex-
tensive attention; particularly the latter, which can
scale up to handle large corpora efficiently.
Although word embeddings have recently
showed to be superior in some NLP tasks, they
are very weak in distinguishing contrasting mean-
ing, as the models are often based on the
well-known distributional semantics hypothesis—
words in similar context have similar meanings.
Contrasting words have similar context too, so
contrasting meaning is not distinguished well in
such representations. Better models for contrast-
ing meaning is fundamentally interesting.
</bodyText>
<subsectionHeader confidence="0.79137">
Modeling Contrasting Meaning Automatically
</subsectionHeader>
<bodyText confidence="0.999784676470589">
detecting contrasting meaning has been studied in
earlier work such as (Lin and Zhao, 2003; Mo-
hammad et al., 2008; Mohammad et al., 2013).
Specifically, as far as the embedding-based meth-
ods are concerned, PILSA (Yih et al., 2012) made
a progress in achieving one of the best results, by
priming LSA to encode contrasting meaning. In
addition, PILSA was also used to initialize a neu-
ral network to get a further improvement on the
GRE benchmark, where an F-score of 81% was
obtained. Another recent method was proposed
by (Zhang et al., 2014), called Bayesian proba-
bilistic tensor factorization. It considered multi-
dimensional semantic information, relations, un-
supervised data structure information in tensor
factorization, and achieved an F-score of 82% on
the GRE questions. These methods employed both
lexical resources and corpora statistics to achieve
their best results. In this paper, we show that us-
ing only lexical resources to construct embedding
systems can achieve significantly better results (an
F-score of 92%). To provide a more comprehen-
sive understanding, we constructed our study in a
framework that examines a number of basic con-
cerns in modeling contrasting meaning within em-
bedding.
Note that sentiment contrast may be viewed as
a specific case of more general semantic contrast
or semantic differentials (Osgood et al., 1957).
Tang et al. (2014) learned sentiment-specific em-
bedding and applied it to sentiment analysis of
tweets, which was often solved with more conven-
tional methods (Zhu et al., 2014b; Kiritchenko et
al., 2014a; Kiritchenko et al., 2014b).
</bodyText>
<sectionHeader confidence="0.996079" genericHeader="method">
3 The Models
</sectionHeader>
<bodyText confidence="0.99989275">
We described in this section the framework in
which we study word embedding for contrasting
meaning. The general aim of the models is to en-
force that in the embedding space, the word pairs
with higher degrees of contrast will be put farther
from each other than those of less contrast. How
to learn this is critical. Figure 1 describes a very
high-level view of the framework.
</bodyText>
<page confidence="0.998529">
107
</page>
<figureCaption confidence="0.9947615">
Figure 1: A high-level view of the contrasting em-
bedding framework.
</figureCaption>
<subsectionHeader confidence="0.998139">
3.1 Top Hidden Layer(s)
</subsectionHeader>
<bodyText confidence="0.999990586206896">
It is widely recognized that contrasting words,
e.g., good and bad, also intend to appear in sim-
ilar context or co-occur with each other. For ex-
ample, opposite pairs, special cases of contrasting
words, tend to co-occur more often than chance
(Charles and Miller, 1989; Fellbaum, 1995; Mur-
phy and Andrew, 1993). Mohammad et al. (2013),
in addition, proposed a degree of contrast hypoth-
esis, stating that “if a pair of words, A and B, are
contrasting, then their degree of contrast is pro-
portional to their tendency to co-occur in a large
corpus.”
These suggest some non-linear interaction be-
tween distributional relatedness and the degree of
contrast: the increase of relatedness correspond
to the increase of both semantic contrast and se-
mantic closeness; for example, they can form a
U-shaped curve if one plots the word pairs on a
two dimensional plane with y-axis denoting relat-
edness scores, while the most contrasting and (se-
mantically) close pairs lie on the two side of the
x-axis, respectively. In this paper, when combin-
ing word-pair distances learned by different com-
ponents of the contrasting inference layer, we use
some top hidden layer(s) to provide a non-linear
combination. Specifically, we use two hidden lay-
ers, which is able to express complicated func-
tions (Bishop, 2006). We use ten hidden units in
each hidden layer.
</bodyText>
<subsectionHeader confidence="0.997807">
3.2 Stochastic Contrast Embedding (SCE)
</subsectionHeader>
<bodyText confidence="0.999284978723404">
Hinton and Roweis (2002) proposed a stochas-
tic neighbor embedding (SNE) framework. Infor-
mally, the objective is to explicitly enforce that in
the learned embedding space, the distribution of
neighbors of a given word w to be similar to the
distribution of its neighbors in the original, un-
compressed space.
In our study, we instead use the concept of
“neighbors” to encode the contrasting pairs, and
we call the model stochastic contrasting embed-
ding (SCE), depicted by the left component of the
contrast inference layer in Figure 1. The model
is different from SNE in three respects. First,
as mentioned above, “neighbors” here are actu-
ally contrasting pairs—we enforce that in the em-
bedding space, the distribution of the contrasting
“neighbors” to be close to the distribution of the
“neighbors” in the original, higher-dimensional
space. The probability of word wk being contrast-
ing neighbor of the given word wi can be com-
puted as:
where d is some distance metric between wi and
wk, and v is the size of a vocabulary.
Second, we train SCE using only lexical re-
sources but not corpus statistics, so as to explore
the behavior of lexical resources separately (we
will use the relatedness modeling component be-
low to model distributional semantics). Specifi-
cally, we use antonym pairs in lexical resources to
learn contrasting neighbors. Hence in the original
high-dimensional space, all antonyms of a given
word wi have the same probability to be its con-
trasting neighbors. That is, d in Equation (1) takes
a binary score, with value 1 indicating an antonym
pair and 0 not. In the embedding space, the cor-
responding probability of wk to be the contrast-
ing neighbor of wi , denoted as q1(wk|wi), can be
computed similarly with Equation (1). But since
the embedding is in a continuous space, d is not
binary but can be computed with regular distance
metric such as euclidean and cosine. The objective
is minimizing the KL divergence between p(.) and
q(.).
Third, semantic closeness or contrast are not in-
dependent. For example, if a pair of words, A and
B, are synonyms, and if the pair of words, A and
C, are contrasting, then A and C is likely to be
</bodyText>
<equation confidence="0.999069666666667">
p1(wk|wi) =
�vm i exp(−d2i,m) (1)
exp(−d2i,k)
</equation>
<page confidence="0.988153">
108
</page>
<bodyText confidence="0.999969846153846">
contrasting than a random chance. SCE considers
both semantic contrast and closeness. That is, for
a given word wi, we jointly force that in the em-
bedding space, its contrasting neighbors and se-
mantically close neighbors to be similar to those
in the original uncompressed space. These two
objective functions are linearly combined with a
parameter A and are jointly optimized to learn one
embedding. The value of A is determined on the
development questions of the GRE data. Later in
Section 4, we will discuss how the training pairs of
semantic contrast and closeness are obtained from
lexical resources.
</bodyText>
<subsectionHeader confidence="0.988345">
3.3 Marginal Contrast Embedding (MCE) 1
</subsectionHeader>
<bodyText confidence="0.9997612">
In this paper, we use also another training criteria,
motivated by the pairwise ranking approach (Co-
hen et al., 1998). The motivation is to explicitly
enforce the distances between contrasting pairs to
be larger than distances between unrelated word
pairs by a margin, and enforce the distances be-
tween semantically close pairs to be smaller than
unrelated word pairs by another margin. More
specifically, we minimize the following objective
functions:
</bodyText>
<equation confidence="0.99851975">
Objs(mce) _ � max{0,α−di,r+di,j} (2)
(wi,wj)∈S
Obja(mce) _ � max{0, Q − di,k + di,r}
(wi,wk)∈A
</equation>
<bodyText confidence="0.975342333333333">
(3)
where A and S are the set of contrasting pairs and
semantically close pairs in lexicons respectively;
d denotes distance function between two words in
the embedding space. The subscript r indicates a
randomly sampled unrelated word. We call this
model Marginal Contrasting Embedding (MCE).
Intuitively, if two words wi and wj are seman-
tically close, the model maximizes Equation (2),
which attempts to force the di,j (distance between
wi and wj) in the embedding space to be differ-
ent from that of two unrelated words di,r by a
margin α. For each given word pair, we sample
100 random words during training. Similarly, if
two words wi and wk are contrasting, the model
</bodyText>
<footnote confidence="0.99359675">
1We made the code of MCE available at
https://github.com/lukecq1231/mce, as MCE achieved
the best performance according to the experimental results
described later in this paper.
</footnote>
<bodyText confidence="0.999516">
maximizes Equation (3), which attempts to force
the distance between wi and wk to be different
from that of two unrelated words di,r by a mar-
gin Q. Same as in SCE, these two objective func-
tions are linearly combined with a parameter A and
are jointly optimized to learn one embedding for
each word. This joint objective function attempts
to force the values of di,r (distances of unrelated
pairs) to be in between di,k (distances of contrast-
ing pairs) and di,j (distances of semantically close
pairs) by two margins.
</bodyText>
<subsectionHeader confidence="0.961941">
3.4 Corpus Relatedness Modeling (CRM)
</subsectionHeader>
<bodyText confidence="0.99999052631579">
As discussed in previous work and above as well,
relatedness obtained with corpora based on dis-
tributional hypothesis interplays with semantic
closeness and contrast. Mohammad et al. (2013)
proposed a degree of contrast hypothesis, stating
that “if a pair of words, A and B, are contrast-
ing, then their degree of contrast is proportional
to their tendency to co-occur in a large corpus.” In
embedding, such dependency can be used to help
measure the degree of contrast. Specifically, we
use the skip-gram model (Mikolov et al., 2013) to
learn the relatedness embedding.
As discussed above, through the top hidden lay-
ers, the word embedding and distances learned in
SCE/MCE and CRM, together with that learned
with SDR below, can be used to predict the GRE
“most contrasting word”’ questions. With enough
GRE data, the prediction error may be backpropa-
gated to directly adjust or learn embedding in the
look-up tables. However, given the limited size of
the GRE data, we only employed the top hidden
layers to non-linearly merge the distances between
a word pair that are obtained within each of the
modules in the Contrast Inference Layer. We did
not backpropagate the errors to fine-tune already
learned word embeddings.
Note that embeddings in the look-up tables were
learned independently in different modules in the
contrast inference layer, e.g., in SCE, MCE and
CRM, respectively. And in each module, given the
corresponding objective functions, unconstrained
optimization (e.g., in the paper SGD) was used
to find embeddings that optimize the correspond-
ing objectives. The embeddings were then used
out-of-box and not further fine-tuned. Depend-
ing on experiment settings, embeddings learned in
each module are either used separately or jointly
(through the top hidden lay) to predict test cases.
</bodyText>
<page confidence="0.99646">
109
</page>
<bodyText confidence="0.990086">
More details will be discussed in the experiment
section below.
</bodyText>
<sectionHeader confidence="0.769263" genericHeader="method">
3.5 Semantic Differential Reconstruction
(SDR)
</sectionHeader>
<bodyText confidence="0.998103263157895">
Using factor analysis, Osgood et al. (1957) identi-
fied three dimensions of semantics that account for
most of the variation in the connotative meaning
of adjectives. These three dimensions are evalu-
ative (good-bad), potency (strong-weak), and ac-
tivity(active-passive). We hypothesize that such
information should help reconstruct contrasting
meaning.
The General Inquirer lexicon (Stone1966) rep-
resents these three factors but has a limited cov-
erage. We used the algorithm of (Turney and
Littman, 2003) to extend the labels to more words
with Google one billion words corpus (refer to
Section 4 for details). For example, to obtain the
evaluative score for a candidate word w, the point-
wise mutual information (PMI) between w and a
set of seed words eval+ and eval− are computed
respectively, and the evaluative value for w is cal-
culated with:
</bodyText>
<equation confidence="0.993131">
eval(w) = PMI(w, eval+) − PMI(w, eval−)
</equation>
<bodyText confidence="0.968648272727273">
(4)
where eval+ contains predefined positive evalua-
tive words, e.g., good, positive, fortunate, and su-
perior, while eval− includes negative evaluative
words like passive, slow, treble, and old. The seed
words were selected as described in (Turney and
Littman, 2003) to have a good coverage and to
avoid redundancy at the same time. Similarly, the
potency and activity scores of a word can be ob-
tained. The distances of a word pair on these three
dimensions can therefore be obtained.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="method">
4 Experiment Set-Up
</sectionHeader>
<bodyText confidence="0.999607305084746">
Data Our experiment uses the “most contrast-
ing word” questions collected by Mohammad
et al. (2008) from Graduate Record Examination
(GRE), which was originally created by Educa-
tional Testing Service (ETS). Each GRE question
has a target word and five candidate choices; the
task is to identify among the choices the most con-
trasting word with regard to the given target word.
The dataset consists of a development set and a
test set, with 162 and 950 questions, respectively.
As an example from (Mohammad et al., 2013),
one of the questions has the target word adulter-
ate and the five candidate choices: (A) renounce,
(B) forbid, (C) purify, (D) criticize, and (E) cor-
rect. While in this example the choice correct has
a meaning that is contrasting with that of adulter-
ate, the word purify is the gold answer as it has the
greatest degree of contrast with adulterate.
Lexical Resources In our work, we use two
publicly available lexical resources, WordNet
(Miller, 1995) (version 3.0) and the Roget’s The-
saurus (Kipfer, 2009). We utilized the labeled
antonym relations to obtain more contrasting pairs
under the contrast hypothesis (Mohammad et al.,
2013), by assuming a contrasting pair is related
to a pair of opposites (antonyms here). Specif-
ically in WordNet, we consider the word pairs
with relations other than antonym as semantically
close. In this way, we obtained a thesaurus con-
taining 83,118 words, 494,579 contrasting pairs,
and 368,209 close pairs. Note that we did not only
use synonyms to expand the contrasting pairs. We
will discuss how this affects the performance in
the experiment section.
In the Roget’s Thesaurus, every word or entry
has its synonyms and/or antonyms. We obtained
35,717 antonym pairs and 346,619 synonym pairs,
which consist of 43,409 word types. The antonym
and synonym pairs in Roget’s were combined with
contrasting pairs and semantically close pairs in
WordNet, respectively. And in total, we have
92,339 word types, 520,734 antonym pairs, and
646,433 close pairs.
Google Billion-Word Corpus The corpus used in
our experiment for modeling lexical relatedness in
the CRM component was Google one billion word
corpus (Chelba et al., 2013). Normalization and
tokenization were performed using the scripts dis-
tributed from https://code.google.com/p/1-billion-
word-language-modeling-benchmark/, and sen-
tences were shuffled randomly. We computed em-
bedding for a word if its count in the corpus is
equal to or larger than five, with the method de-
scribed in Section 3.4. Words with counts lower
than five were discarded.
Evaluation Metric Same as in previous work, the
evaluation metric is F-score, where precision is
the percentage of the questions answered correctly
over the questions the models attempt to answer,
</bodyText>
<page confidence="0.995062">
110
</page>
<bodyText confidence="0.9990575">
and recall is the percentage of the questions that
are answered correctly among all questions.
</bodyText>
<sectionHeader confidence="0.990915" genericHeader="evaluation">
5 Experiment Results
</sectionHeader>
<bodyText confidence="0.999935823529412">
In training, we used stochastic gradient descent
(SGD) to optimize the objective function, and the
dimension of embedding was set to be 200. In
MCE (Equation 2 and 3) the margins α and β are
both set to be 0.4. During testing, when using SCE
or MCE embedding to answer the GRE questions,
we directly calculated distances for a pair between
a question word and a candidate choice in these
two corresponding embedding spaces to report
their performances. We also combined SCE/MCE
with other components in the contrast inference
layer, for which we used ten-fold cross validation
to tune the weights of the top hidden layers on nine
fold and test on the rest and repeated this for ten
times to report the results. As discussed above, er-
rors were not backpropagated to modify word em-
bedding.
</bodyText>
<subsectionHeader confidence="0.987474">
5.1 General Performance of the Models
</subsectionHeader>
<bodyText confidence="0.999979357142857">
The performance of the models are showed in Ta-
ble 1. For comparison, we list the results reported
in (Yih et al., 2012) and (Zhang et al., 2014). The
table shows that on the GRE dataset, both SCE (a
90% F-score) and MCE (92%) significantly out-
perform the previous best results reported in (Yih
et al., 2012) (81%) and (Zhang et al., 2014) (82%).
The F-score of MCE outperforms that of SCE by
2%, which suggests the ranking criterion fits the
dataset better. In our experiment, we found that
the MCE model achieved robust performances on
different distance metrics, e.g., the cosine simi-
larity and Euclidean distance. In the paper, we
present the results with cosine similarity. SCE is
slightly more sensitive to distance metrics, and the
best performing metric on the development set is
inner product, so we chose that for testing.
Unlike what was suggested in the previous
work, where semantics learned from corpus is
claimed to yield extra gains in performance, we
obtained this result by using solely lexical re-
sources (Roget’s and WordNet) with SCE and
MCE. Using corpus statistics that model distri-
butional hypothesis (MCE+CRM) and utilize se-
mantic differential categories (MCE+CRM+SDR)
does not bring further improvement here (they are
useful in the experiments discussed below in Sec-
tion 5.3).
</bodyText>
<subsectionHeader confidence="0.999544">
5.2 Roles of Lexical Resources
</subsectionHeader>
<bodyText confidence="0.999956020408163">
To provide a more detailed comparison, we also
present lexicon lookup results, together with those
reported in (Zhang et al., 2014) and (Yih et al.,
2012). For our lookup results and those copied
here from (Zhang et al., 2014), the methods do not
randomly guess an answer if the target word is in
the vocabulary but none of the choices are, while
the results of (Yih et al., 2012) randomly guess
an answer in this situation. The Encarta thesaurus
used in (Yih et al., 2012) is not publicly available,
so we did not use it in our experiments. We due
the differences among the lookup results on Word-
Net (WordNet lookup) to the differences in prepro-
cessing as well as the way we expanded indirect
contrasting word pairs. As described in Section 4,
we utilized all relations other than antonym pairs
to expand our indirect antonym pairs. These also
have impact on the W&amp;R lookup results (WordNet
and Roget’s pairs are combined). For both set-
tings, our expansion resulted in much better per-
formances.
Whether the differences between the F-scores
of MCE/SCE and that reported in (Zhang et al.,
2014) and (Yih et al., 2012) are also due to the
differences in expanding indirect pairs? To answer
this, we downloaded the word pairs that Zhang et
al. (2014) used to train their models,2 but we used
them to train our MCE. The result are presented in
Table 1 and the F-score on test set is 91%, which
is only slightly lower than MCE using our lexicon.
So the extension is very helpful for lookup meth-
ods, but the MCE appears to be able to cover such
information by itself.
SCE and MCE learn contrasting meaning that
is not explicitly encoded in lexical resources. The
experiment results show that such implicit contrast
can be recovered by jointly learning the embed-
ding by using contrasting words and other seman-
tically close words.
To help better understand why corpus statis-
tics does not further help SCE and MCE, we
further demonstrate that most of the target-gold-
answer pairs in the GRE test set are connected
by short paths (with length between 1 to 3).
More specifically, based on breadth-first search,
we found the nearest paths that connect target-
gold-answer pairs, in the graph formed by Word-
Net and Roget’s—each word is a vertex, and con-
trasting words and semantically close words are
</bodyText>
<footnote confidence="0.96553">
2https://github.com/iceboal/word-representations-bptf
</footnote>
<page confidence="0.990084">
111
</page>
<table confidence="0.999964409090909">
Development Set Test Set Fi
Prec. Rec. Fi Prec. Rec.
WordNet PILSA (Yih et al., 2012) 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA (Yih et al., 2012) 0.66 0.65 0.65 0.61 0.59 0.60
Encarta lookup (Yih et al., 2012) 0.65 0.61 0.63 0.61 0.56 0.59
Encarta PILSA (Yih et al., 2012) 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA (Yih et al., 2012) 0.87 0.82 0.84 0.82 0.74 0.78
WordNet lookup (Yih et al., 2012) 0.40 0.40 0.40 0.42 0.41 0.42
WordNet lookup (Zhang et al., 2014) 0.93 0.32 0.48 0.95 0.33 0.49
WordNet lookup 0.97 0.37 0.54 0.97 0.41 0.58
Roget lookup (Zhang et al., 2014) 1.00 0.35 0.52 0.99 0.31 0.47
Roget lookup 1.00 0.32 0.49 0.97 0.29 0.44
W&amp;R lookup (Zhang et al., 2014) 1.00 0.48 0.64 0.98 0.45 0.62
W&amp;R lookup 0.98 0.52 0.68 0.97 0.52 0.68
(Mohammad et al., 2008) Best 0.76 0.66 0.70 0.76 0.64 0.70
(Yih et al., 2012) Best 0.88 0.87 0.87 0.81 0.80 0.81
(Zhang et al., 2014) Best 0.88 0.88 0.88 0.82 0.82 0.82
SCE 0.94 0.93 0.93 0.90 0.90 0.90
MCE (using zhang et al. lex.) 0.94 0.93 0.94 0.92 0.91 0.91
MCE 0.96 0.94 0.95 0.92 0.92 0.92
MCE+CRM 0.94 0.93 0.93 0.90 0.90 0.90
MCE+CRM+SDR 0.04 0.94 0.94 0.90 0.90 0.90
</table>
<tableCaption confidence="0.999957">
Table 1: Results on the GRE “most contrasting words” questions.
</tableCaption>
<bodyText confidence="0.983105551724138">
connected with these two types of edges respec-
tively. Then we require the shortest path must have
one and only one contrasting edge. Word pairs that
cannot be connected by such paths are regarded to
have an infinite length of distance.
Figure 2: Percentages of target-gold-answer word
pairs, categorized by the shortest lengths of paths
connecting them.
The pie graph in Figure 2 shows the percentages
of target-gold-answer word pairs, categorized by
the lengths of shortest paths defined above. We
can see that in the GRE data, the percentage of
paths with a length larger than three is very small
(1%). It seems that SCE and MCE can learn this
very well. Again, they force semantically close
pairs to be close in the embedding spaces which
“share” similar contrasting pairs.
Figure 3 draws the envelope of histogram of
cosine distance between all target-choice word
pairs in the GRE test set, calculated in the em-
bedding space learned with MCE. The figure in-
tuitively shows how the target-gold-answer pairs
(most contrasting pairs) are discriminated from the
other target-choice pairs. We also plot the MCE
results without using the random sampling de-
picted in Equation (2) and Equation (3), showing
that discriminative power dramatically dropped.
Without the sampling, the F-score achieved on the
test data is 83%.
</bodyText>
<subsectionHeader confidence="0.999208">
5.3 Roles of Corpus-based Embedding
</subsectionHeader>
<bodyText confidence="0.9999445">
However, the findings presented above should not
be simply taken as that distributional hypothesis
is not useful for learning lexical contrast. Our re-
sults and detailed analysis has showed it is due to
the good coverage of the manually created lexi-
cal resources and the capability of the SCE and
</bodyText>
<page confidence="0.995577">
112
</page>
<figureCaption confidence="0.999882">
Figure 4: The effect of removing lexicon items.
</figureCaption>
<figure confidence="0.875294">
Cosine similarity
</figure>
<figureCaption confidence="0.888492">
Figure 3: The envelope of histogram of cosine dis-
tance between word pair embeddings in GRE test
set.
</figureCaption>
<bodyText confidence="0.999869102564102">
MCE models in capturing indirect semantic rela-
tions. There may exist circumstances where the
coverage is be lower, e.g., for resource-poor lan-
guages or social media text where (indirect) out-
of-vocabulary pairs may be frequent.
To simulate the situations, we randomly re-
moved different percentages of words from the
combined thesaurus used above in our experi-
ments, and removed all the corresponding word
pairs. The performances of different models are
showed in Figure 4. It is observed that as the
out of vocabulary (OOV) becomes more serious,
the MCE suffered the most. Using the seman-
tic differential (MCE+SDR) showed to be help-
ful as 50% to 70% lexicon entries are kept. Con-
sidering relatedness learned from corpus together
with MCE (MCE+CRM), i.e., combining MCE
distances with CRM distances for target-choice
pairs, yielded robust performance—the F-score of
MCE+CRM drops significantly slower than that
of MCE, as we removed lexical entries. We also
combined MCE distances and CRM distances lin-
early (MCE+CRM (linear)), with a coefficient de-
termined with the development set. It showed a
performance worse than that of MCE+CRM when
50%–80% entries kept, while as discussed above,
MCE+CRM combines the two parts with the non-
linear top layers. In general, using corpora statis-
tics make the models more robust as OOV be-
comes more serious. It deserves to note that the
use of corpora here is rather straightforward; more
patterns may be learned from corpora to capture
contrasting expressions as discussed in (Moham-
mad et al., 2013). Also, context such as nega-
tion may change contrasting meaning, e.g., sen-
timent contrast (Kiritchenko et al., 2014b; Zhu et
al., 2014a), in a dramatic and complicated manner,
which has been considered in learning sentiment
contrast (Kiritchenko et al., 2014b).
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9998535">
Contrasting meaning is a basic aspect of seman-
tics. In this paper, we present a new state-of-the-
art result, a 92% F-score, on the GRE dataset cre-
ated by (Mohammad et al., 2008), which is widely
used as the benchmark for modeling lexical con-
trast. The result reported here outperforms the
best reported in previous work (82%) by a large
margin. Unlike what was suggested in most pre-
vious work, we show that this performance can be
achieved without relying on corpora statistics. To
provide a more comprehensive understanding, we
constructed our study in a framework that exam-
</bodyText>
<figure confidence="0.988974166666667">
Frequency
0.25
0.15
0.05
0.2
0.1
0
−1 −0.5 0 0.5 1
MCE on most contrasting pairs
MCE on others
MCE w/o negative sampling on most contrasting pairs
MCE w/o negative sampling on others
</figure>
<page confidence="0.99707">
113
</page>
<bodyText confidence="0.999910083333333">
ines a number of concerns in modeling contrast-
ing meaning. We hope our work could help shed
some light on future directions on this basic se-
mantic problem.
From our own viewpoints, creating more eval-
uation data for measuring further progress in
contrasting-meaning modeling, e.g., handling real
OOV issues, is interesting to us. Also, the de-
gree of contrast may be better formulated as a re-
gression problem rather than a classification prob-
lem, in which finer or even real-valued annotation
would be desirable.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955685393259">
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer-Verlag New York,
Inc., Secaucus, NJ, USA.
Walter G. Charles and George A. Miller. 1989. Con-
texts of antonymous adjectives. Applied Psychol-
ogy, 10:357–375.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for mea-
suring progress in statistical language modeling.
arXiv:1312.3005.
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1998. Learning to order things. Journal of
Articial Intelligence Research (JAIR), 10:243–270.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
David A. Cruse. 1986. Lexical semantics. Cambridge
University Press.
Christiane Fellbaum. 1995. Co-occurrence and
antonymy. International Journal of Lexicography,
8:281–303.
Geoffrey Hinton and Sam Roweis. 2002. Stochastic
neighbor embedding. In Advances in Neural Infor-
mation Processing Systems 15, pages 833–840. MIT
Press.
Barbara Ann Kipfer. 2009. Rogets 21st Century The-
saurus. Philip Lief Group, third edition edition edi-
tion.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-
mad. 2014a. Nrc-canada-2014: Detecting aspects
and sentiment in customer reviews. In Proceedings
of International Workshop on Semantic Evaluation,
Dublin, Ireland.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-
mad. 2014b. Sentiment analysis of short informal
texts. Journal of Artificial Intelligence Research,
50:723–762.
Dekang Lin and Shaojun Zhao. 2003. Identifying syn-
onyms among distributionally similar words. In In
Proceedings of IJCAI-03, pages 1492–1493.
Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embed-
dings based on ordinal knowledge constraints. In
Proceedings ofACL, Beijing, China.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
George A Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 982–991. Associ-
ation for Computational Linguistics.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics, 39(3):555–590.
Gregory L. Murphy and Jane M. Andrew. 1993.
The conceptual basis of antonymy and synonymy
in adjectives. Journal of Memory and Language,
32(3):1–19.
Charles E Osgood, George J Suci, and Percy Tannen-
baum. 1957. The measurement of meaning. Univer-
sity of Illinois Press.
Sam T. Roweis and Lawrence K. Saul. 2000. Nonlin-
ear dimensionality reduction by locally linear em-
bedding. Science, 290:2323–2326.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings ofACL, Baltimore, Mary-
land, USA, June.
Joshua B. Tenenbaum, Vin de Silva, and John C.
Langford. 2000. A global geometric framework
for nonlinear dimensionality reduction. Science,
290(5500):2319.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems (TOIS), 21(4):315–346.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141–188, January.
</reference>
<page confidence="0.988274">
114
</page>
<reference confidence="0.999566130434783">
Wen-tau Yih, Geoffrey Zweig, and John C Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1212–1222. Association for Computational Linguis-
tics.
Jingwei Zhang, Jeremy Salwen, Michael Glass, and
Alfio Gliozzo. 2014. Word semantic representa-
tions using bayesian probabilistic tensor factoriza-
tion. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1522–1531, Doha, Qatar, October.
Association for Computational Linguistics.
Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and
Svetlana Kiritchenko. 2014a. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings ofACL, Baltimore, Maryland, USA, June.
Xiaodan Zhu, Svetlana Kiritchenko, and Saif Moham-
mad. 2014b. Nrc-canada-2014: Recent improve-
ments in the sentiment analysis of tweets. In Pro-
ceedings of International Workshop on Semantic
Evaluation, Dublin, Ireland.
</reference>
<page confidence="0.999026">
115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.477560">
<title confidence="0.999963">Revisiting Word Embedding for Contrasting Meaning</title>
<author confidence="0.9859485">Wei Qian Si Hui Xiaodan</author>
<affiliation confidence="0.979035">of Computer Science and Technology, University of Science and Technology of China, Hefei,</affiliation>
<address confidence="0.827646666666667">Research, Hefei, of EECS, York University, Toronto, emails: zgchen9517017@gmail.com, weilin2@iflytek.com,</address>
<email confidence="0.991288">xpchen@ustc.edu.cn,siwei@iflytek.com,hj@cse.yorku.ca,zhu2048@gmail.com</email>
<abstract confidence="0.99975">Contrasting meaning is a basic aspect of semantics. Recent word-embedding modbased on semantics hyknown to be weak for modeling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest performance seen so far on this dataset. Surprisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget’s and WordNet)—corpora statistics did not lead to further improvement. However, this should not be simply taken as that distributional statistics is not useful. We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern Recognition and Machine Learning.</title>
<date>2006</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="10037" citStr="Bishop, 2006" startWordPosition="1568" endWordPosition="1569"> correspond to the increase of both semantic contrast and semantic closeness; for example, they can form a U-shaped curve if one plots the word pairs on a two dimensional plane with y-axis denoting relatedness scores, while the most contrasting and (semantically) close pairs lie on the two side of the x-axis, respectively. In this paper, when combining word-pair distances learned by different components of the contrasting inference layer, we use some top hidden layer(s) to provide a non-linear combination. Specifically, we use two hidden layers, which is able to express complicated functions (Bishop, 2006). We use ten hidden units in each hidden layer. 3.2 Stochastic Contrast Embedding (SCE) Hinton and Roweis (2002) proposed a stochastic neighbor embedding (SNE) framework. Informally, the objective is to explicitly enforce that in the learned embedding space, the distribution of neighbors of a given word w to be similar to the distribution of its neighbors in the original, uncompressed space. In our study, we instead use the concept of “neighbors” to encode the contrasting pairs, and we call the model stochastic contrasting embedding (SCE), depicted by the left component of the contrast inferen</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter G Charles</author>
<author>George A Miller</author>
</authors>
<title>Contexts of antonymous adjectives.</title>
<date>1989</date>
<pages>10--357</pages>
<publisher>Applied Psychology,</publisher>
<contexts>
<context position="9013" citStr="Charles and Miller, 1989" startWordPosition="1398" endWordPosition="1401">of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., good and bad, also intend to appear in similar context or co-occur with each other. For example, opposite pairs, special cases of contrasting words, tend to co-occur more often than chance (Charles and Miller, 1989; Fellbaum, 1995; Murphy and Andrew, 1993). Mohammad et al. (2013), in addition, proposed a degree of contrast hypothesis, stating that “if a pair of words, A and B, are contrasting, then their degree of contrast is proportional to their tendency to co-occur in a large corpus.” These suggest some non-linear interaction between distributional relatedness and the degree of contrast: the increase of relatedness correspond to the increase of both semantic contrast and semantic closeness; for example, they can form a U-shaped curve if one plots the word pairs on a two dimensional plane with y-axis </context>
</contexts>
<marker>Charles, Miller, 1989</marker>
<rawString>Walter G. Charles and George A. Miller. 1989. Contexts of antonymous adjectives. Applied Psychology, 10:357–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Tomas Mikolov</author>
<author>Mike Schuster</author>
<author>Qi Ge</author>
<author>Thorsten Brants</author>
<author>Phillipp Koehn</author>
<author>Tony Robinson</author>
</authors>
<title>One billion word benchmark for measuring progress in statistical language modeling.</title>
<date>2013</date>
<pages>1312--3005</pages>
<contexts>
<context position="20415" citStr="Chelba et al., 2013" startWordPosition="3267" endWordPosition="3270">affects the performance in the experiment section. In the Roget’s Thesaurus, every word or entry has its synonyms and/or antonyms. We obtained 35,717 antonym pairs and 346,619 synonym pairs, which consist of 43,409 word types. The antonym and synonym pairs in Roget’s were combined with contrasting pairs and semantically close pairs in WordNet, respectively. And in total, we have 92,339 word types, 520,734 antonym pairs, and 646,433 close pairs. Google Billion-Word Corpus The corpus used in our experiment for modeling lexical relatedness in the CRM component was Google one billion word corpus (Chelba et al., 2013). Normalization and tokenization were performed using the scripts distributed from https://code.google.com/p/1-billionword-language-modeling-benchmark/, and sentences were shuffled randomly. We computed embedding for a word if its count in the corpus is equal to or larger than five, with the method described in Section 3.4. Words with counts lower than five were discarded. Evaluation Metric Same as in previous work, the evaluation metric is F-score, where precision is the percentage of the questions answered correctly over the questions the models attempt to answer, 110 and recall is the perce</context>
</contexts>
<marker>Chelba, Mikolov, Schuster, Ge, Brants, Koehn, Robinson, 2013</marker>
<rawString>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv:1312.3005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Learning to order things.</title>
<date>1998</date>
<journal>Journal of Articial Intelligence Research (JAIR),</journal>
<pages>10--243</pages>
<contexts>
<context position="13092" citStr="Cohen et al., 1998" startWordPosition="2083" endWordPosition="2087">embedding space, its contrasting neighbors and semantically close neighbors to be similar to those in the original uncompressed space. These two objective functions are linearly combined with a parameter A and are jointly optimized to learn one embedding. The value of A is determined on the development questions of the GRE data. Later in Section 4, we will discuss how the training pairs of semantic contrast and closeness are obtained from lexical resources. 3.3 Marginal Contrast Embedding (MCE) 1 In this paper, we use also another training criteria, motivated by the pairwise ranking approach (Cohen et al., 1998). The motivation is to explicitly enforce the distances between contrasting pairs to be larger than distances between unrelated word pairs by a margin, and enforce the distances between semantically close pairs to be smaller than unrelated word pairs by another margin. More specifically, we minimize the following objective functions: Objs(mce) _ � max{0,α−di,r+di,j} (2) (wi,wj)∈S Obja(mce) _ � max{0, Q − di,k + di,r} (wi,wk)∈A (3) where A and S are the set of contrasting pairs and semantically close pairs in lexicons respectively; d denotes distance function between two words in the embedding </context>
</contexts>
<marker>Cohen, Schapire, Singer, 1998</marker>
<rawString>William W. Cohen, Robert E. Schapire, and Yoram Singer. 1998. Learning to order things. Journal of Articial Intelligence Research (JAIR), 10:243–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2144" citStr="Collobert et al., 2011" startWordPosition="304" endWordPosition="307">sic semantics modeling problem. 1 Introduction Learning good representations of meaning for different granularities of texts is core to human language understanding, where a basic problem is representing the meanings of words. Distributed representations learned with neural networks have recently showed to result in significant improvement of performance on a number of language understanding problems (e.g., speech recognition and automatic machine translation) and on many non-language problems (e.g., image recognition). Distributed representations have been leveraged to represent words as in (Collobert et al., 2011; Mikolov et al., 2013). Contrasting meaning is a basic aspect of semantics, but it is widely known that word embedding models based on distributional semantics hypothesis are weak in modeling this—contrasting meaning is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable. Lexical contrast has been modeled in (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances. For example, Yih</context>
<context position="5203" citStr="Collobert et al., 2011" startWordPosition="786" endWordPosition="789">of opposites that are also gradable adjectives, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting word pairs (refer to (Mohammad et al., 2013) for detailed discussions). Word Embedding Word embedding models learn continuous representations for words in a low dimensional space (Turney and Pantel, 2010; Hinton and Roweis, 2002; Collobert et al., 2011; Mikolov et al., 2013; Liu et al., 2015), which is not new. Linear dimension reduction such as Latent Semantic Analysis (LSA) has been extensively used in lexical semantics (see (Turney and Pantel, 2010) for good discussions in vector space models.) Non-linear models such as those described in (Roweis and Saul, 2000) and (Tenenbaum et al., 2000), among many others, can also be applied to learn word embeddings. A particularly interesting model is stochastic neighbor embedding (SNE) (Hinton and Roweis, 2002), which explicitly enforces that in the embedding space, the distribution of neighbors o</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cruse</author>
</authors>
<title>Lexical semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4669" citStr="Cruse, 1986" startWordPosition="703" endWordPosition="704"> Processing, pages 106–115, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work The terms contrasting, opposite, and antonym have different definitions in the literature, while sometimes they are used interchangeably. Following (Mohammad et al., 2013), in this paper we refer to opposites as word pairs that “have a strong binary incompatibility relation with each other or that are saliently different across a dimension of meaning”, e.g., day and night. Antonyms are a subset of opposites that are also gradable adjectives, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting word pairs (refer to (Mohammad et al., 2013) for detailed discussions). Word Embedding Word embedding models learn continuous representations for words in a low dimensional space (Turney and Pantel, 2010; Hinton and Roweis, 2002; Collobert et al., 2011; Mikolov et al., 2013; Liu et al., 2015), which is not new. Linea</context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>David A. Cruse. 1986. Lexical semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Co-occurrence and antonymy.</title>
<date>1995</date>
<journal>International Journal of Lexicography,</journal>
<pages>8--281</pages>
<contexts>
<context position="9029" citStr="Fellbaum, 1995" startWordPosition="1402" endWordPosition="1403">e that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., good and bad, also intend to appear in similar context or co-occur with each other. For example, opposite pairs, special cases of contrasting words, tend to co-occur more often than chance (Charles and Miller, 1989; Fellbaum, 1995; Murphy and Andrew, 1993). Mohammad et al. (2013), in addition, proposed a degree of contrast hypothesis, stating that “if a pair of words, A and B, are contrasting, then their degree of contrast is proportional to their tendency to co-occur in a large corpus.” These suggest some non-linear interaction between distributional relatedness and the degree of contrast: the increase of relatedness correspond to the increase of both semantic contrast and semantic closeness; for example, they can form a U-shaped curve if one plots the word pairs on a two dimensional plane with y-axis denoting related</context>
</contexts>
<marker>Fellbaum, 1995</marker>
<rawString>Christiane Fellbaum. 1995. Co-occurrence and antonymy. International Journal of Lexicography, 8:281–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Sam Roweis</author>
</authors>
<title>Stochastic neighbor embedding.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 15,</booktitle>
<pages>833--840</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5179" citStr="Hinton and Roweis, 2002" startWordPosition="781" endWordPosition="785">t. Antonyms are a subset of opposites that are also gradable adjectives, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting word pairs (refer to (Mohammad et al., 2013) for detailed discussions). Word Embedding Word embedding models learn continuous representations for words in a low dimensional space (Turney and Pantel, 2010; Hinton and Roweis, 2002; Collobert et al., 2011; Mikolov et al., 2013; Liu et al., 2015), which is not new. Linear dimension reduction such as Latent Semantic Analysis (LSA) has been extensively used in lexical semantics (see (Turney and Pantel, 2010) for good discussions in vector space models.) Non-linear models such as those described in (Roweis and Saul, 2000) and (Tenenbaum et al., 2000), among many others, can also be applied to learn word embeddings. A particularly interesting model is stochastic neighbor embedding (SNE) (Hinton and Roweis, 2002), which explicitly enforces that in the embedding space, the dis</context>
<context position="10149" citStr="Hinton and Roweis (2002)" startWordPosition="1584" endWordPosition="1587">orm a U-shaped curve if one plots the word pairs on a two dimensional plane with y-axis denoting relatedness scores, while the most contrasting and (semantically) close pairs lie on the two side of the x-axis, respectively. In this paper, when combining word-pair distances learned by different components of the contrasting inference layer, we use some top hidden layer(s) to provide a non-linear combination. Specifically, we use two hidden layers, which is able to express complicated functions (Bishop, 2006). We use ten hidden units in each hidden layer. 3.2 Stochastic Contrast Embedding (SCE) Hinton and Roweis (2002) proposed a stochastic neighbor embedding (SNE) framework. Informally, the objective is to explicitly enforce that in the learned embedding space, the distribution of neighbors of a given word w to be similar to the distribution of its neighbors in the original, uncompressed space. In our study, we instead use the concept of “neighbors” to encode the contrasting pairs, and we call the model stochastic contrasting embedding (SCE), depicted by the left component of the contrast inference layer in Figure 1. The model is different from SNE in three respects. First, as mentioned above, “neighbors” </context>
</contexts>
<marker>Hinton, Roweis, 2002</marker>
<rawString>Geoffrey Hinton and Sam Roweis. 2002. Stochastic neighbor embedding. In Advances in Neural Information Processing Systems 15, pages 833–840. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Ann Kipfer</author>
</authors>
<title>Rogets 21st Century Thesaurus. Philip Lief Group, third edition edition edition.</title>
<date>2009</date>
<contexts>
<context position="19261" citStr="Kipfer, 2009" startWordPosition="3087" endWordPosition="3088">t set and a test set, with 162 and 950 questions, respectively. As an example from (Mohammad et al., 2013), one of the questions has the target word adulterate and the five candidate choices: (A) renounce, (B) forbid, (C) purify, (D) criticize, and (E) correct. While in this example the choice correct has a meaning that is contrasting with that of adulterate, the word purify is the gold answer as it has the greatest degree of contrast with adulterate. Lexical Resources In our work, we use two publicly available lexical resources, WordNet (Miller, 1995) (version 3.0) and the Roget’s Thesaurus (Kipfer, 2009). We utilized the labeled antonym relations to obtain more contrasting pairs under the contrast hypothesis (Mohammad et al., 2013), by assuming a contrasting pair is related to a pair of opposites (antonyms here). Specifically in WordNet, we consider the word pairs with relations other than antonym as semantically close. In this way, we obtained a thesaurus containing 83,118 words, 494,579 contrasting pairs, and 368,209 close pairs. Note that we did not only use synonyms to expand the contrasting pairs. We will discuss how this affects the performance in the experiment section. In the Roget’s </context>
</contexts>
<marker>Kipfer, 2009</marker>
<rawString>Barbara Ann Kipfer. 2009. Rogets 21st Century Thesaurus. Philip Lief Group, third edition edition edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif Mohammad</author>
</authors>
<title>Nrc-canada-2014: Detecting aspects and sentiment in customer reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of International Workshop on Semantic Evaluation,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="8228" citStr="Kiritchenko et al., 2014" startWordPosition="1263" endWordPosition="1266">urces to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehensive understanding, we constructed our study in a framework that examines a number of basic concerns in modeling contrasting meaning within embedding. Note that sentiment contrast may be viewed as a specific case of more general semantic contrast or semantic differentials (Osgood et al., 1957). Tang et al. (2014) learned sentiment-specific embedding and applied it to sentiment analysis of tweets, which was often solved with more conventional methods (Zhu et al., 2014b; Kiritchenko et al., 2014a; Kiritchenko et al., 2014b). 3 The Models We described in this section the framework in which we study word embedding for contrasting meaning. The general aim of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., good and bad, also intend to </context>
<context position="30274" citStr="Kiritchenko et al., 2014" startWordPosition="4924" endWordPosition="4927"> with a coefficient determined with the development set. It showed a performance worse than that of MCE+CRM when 50%–80% entries kept, while as discussed above, MCE+CRM combines the two parts with the nonlinear top layers. In general, using corpora statistics make the models more robust as OOV becomes more serious. It deserves to note that the use of corpora here is rather straightforward; more patterns may be learned from corpora to capture contrasting expressions as discussed in (Mohammad et al., 2013). Also, context such as negation may change contrasting meaning, e.g., sentiment contrast (Kiritchenko et al., 2014b; Zhu et al., 2014a), in a dramatic and complicated manner, which has been considered in learning sentiment contrast (Kiritchenko et al., 2014b). 6 Conclusions Contrasting meaning is a basic aspect of semantics. In this paper, we present a new state-of-theart result, a 92% F-score, on the GRE dataset created by (Mohammad et al., 2008), which is widely used as the benchmark for modeling lexical contrast. The result reported here outperforms the best reported in previous work (82%) by a large margin. Unlike what was suggested in most previous work, we show that this performance can be achieved </context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif Mohammad. 2014a. Nrc-canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of International Workshop on Semantic Evaluation, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>50--723</pages>
<contexts>
<context position="8228" citStr="Kiritchenko et al., 2014" startWordPosition="1263" endWordPosition="1266">urces to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehensive understanding, we constructed our study in a framework that examines a number of basic concerns in modeling contrasting meaning within embedding. Note that sentiment contrast may be viewed as a specific case of more general semantic contrast or semantic differentials (Osgood et al., 1957). Tang et al. (2014) learned sentiment-specific embedding and applied it to sentiment analysis of tweets, which was often solved with more conventional methods (Zhu et al., 2014b; Kiritchenko et al., 2014a; Kiritchenko et al., 2014b). 3 The Models We described in this section the framework in which we study word embedding for contrasting meaning. The general aim of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., good and bad, also intend to </context>
<context position="30274" citStr="Kiritchenko et al., 2014" startWordPosition="4924" endWordPosition="4927"> with a coefficient determined with the development set. It showed a performance worse than that of MCE+CRM when 50%–80% entries kept, while as discussed above, MCE+CRM combines the two parts with the nonlinear top layers. In general, using corpora statistics make the models more robust as OOV becomes more serious. It deserves to note that the use of corpora here is rather straightforward; more patterns may be learned from corpora to capture contrasting expressions as discussed in (Mohammad et al., 2013). Also, context such as negation may change contrasting meaning, e.g., sentiment contrast (Kiritchenko et al., 2014b; Zhu et al., 2014a), in a dramatic and complicated manner, which has been considered in learning sentiment contrast (Kiritchenko et al., 2014b). 6 Conclusions Contrasting meaning is a basic aspect of semantics. In this paper, we present a new state-of-theart result, a 92% F-score, on the GRE dataset created by (Mohammad et al., 2008), which is widely used as the benchmark for modeling lexical contrast. The result reported here outperforms the best reported in previous work (82%) by a large margin. Unlike what was suggested in most previous work, we show that this performance can be achieved </context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif Mohammad. 2014b. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research, 50:723–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
</authors>
<title>Identifying synonyms among distributionally similar words. In</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI-03,</booktitle>
<pages>1492--1493</pages>
<contexts>
<context position="2528" citStr="Lin and Zhao, 2003" startWordPosition="367" endWordPosition="370">anding problems (e.g., speech recognition and automatic machine translation) and on many non-language problems (e.g., image recognition). Distributed representations have been leveraged to represent words as in (Collobert et al., 2011; Mikolov et al., 2013). Contrasting meaning is a basic aspect of semantics, but it is widely known that word embedding models based on distributional semantics hypothesis are weak in modeling this—contrasting meaning is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable. Lexical contrast has been modeled in (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances. For example, Yih et al. (2012) proposed to use polarity-primed latent semantic analysis (LSA), called PILSA, to capture contrast, which was further used to initialize a neural network and achieved an F-score of 81% on the same GRE “most contrasting word” questions (Mohammad et al., 2008). More recently, Zhang et al. (2014) proposed a tensor factorization approach to solving the problem, resulting </context>
<context position="6765" citStr="Lin and Zhao, 2003" startWordPosition="1030" endWordPosition="1033"> large corpora efficiently. Although word embeddings have recently showed to be superior in some NLP tasks, they are very weak in distinguishing contrasting meaning, as the models are often based on the well-known distributional semantics hypothesis— words in similar context have similar meanings. Contrasting words have similar context too, so contrasting meaning is not distinguished well in such representations. Better models for contrasting meaning is fundamentally interesting. Modeling Contrasting Meaning Automatically detecting contrasting meaning has been studied in earlier work such as (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). Specifically, as far as the embedding-based methods are concerned, PILSA (Yih et al., 2012) made a progress in achieving one of the best results, by priming LSA to encode contrasting meaning. In addition, PILSA was also used to initialize a neural network to get a further improvement on the GRE benchmark, where an F-score of 81% was obtained. Another recent method was proposed by (Zhang et al., 2014), called Bayesian probabilistic tensor factorization. It considered multidimensional semantic information, relations, unsupervised data structure in</context>
</contexts>
<marker>Lin, Zhao, 2003</marker>
<rawString>Dekang Lin and Shaojun Zhao. 2003. Identifying synonyms among distributionally similar words. In In Proceedings of IJCAI-03, pages 1492–1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quan Liu</author>
<author>Hui Jiang</author>
<author>Si Wei</author>
<author>Zhen-Hua Ling</author>
<author>Yu Hu</author>
</authors>
<title>Learning semantic word embeddings based on ordinal knowledge constraints.</title>
<date>2015</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="5244" citStr="Liu et al., 2015" startWordPosition="794" endWordPosition="797">, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting word pairs (refer to (Mohammad et al., 2013) for detailed discussions). Word Embedding Word embedding models learn continuous representations for words in a low dimensional space (Turney and Pantel, 2010; Hinton and Roweis, 2002; Collobert et al., 2011; Mikolov et al., 2013; Liu et al., 2015), which is not new. Linear dimension reduction such as Latent Semantic Analysis (LSA) has been extensively used in lexical semantics (see (Turney and Pantel, 2010) for good discussions in vector space models.) Non-linear models such as those described in (Roweis and Saul, 2000) and (Tenenbaum et al., 2000), among many others, can also be applied to learn word embeddings. A particularly interesting model is stochastic neighbor embedding (SNE) (Hinton and Roweis, 2002), which explicitly enforces that in the embedding space, the distribution of neighbors of a given word to be similar to that in t</context>
</contexts>
<marker>Liu, Jiang, Wei, Ling, Hu, 2015</marker>
<rawString>Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and Yu Hu. 2015. Learning semantic word embeddings based on ordinal knowledge constraints. In Proceedings ofACL, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2167" citStr="Mikolov et al., 2013" startWordPosition="308" endWordPosition="311">roblem. 1 Introduction Learning good representations of meaning for different granularities of texts is core to human language understanding, where a basic problem is representing the meanings of words. Distributed representations learned with neural networks have recently showed to result in significant improvement of performance on a number of language understanding problems (e.g., speech recognition and automatic machine translation) and on many non-language problems (e.g., image recognition). Distributed representations have been leveraged to represent words as in (Collobert et al., 2011; Mikolov et al., 2013). Contrasting meaning is a basic aspect of semantics, but it is widely known that word embedding models based on distributional semantics hypothesis are weak in modeling this—contrasting meaning is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable. Lexical contrast has been modeled in (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances. For example, Yih et al. (2012) proposed</context>
<context position="5225" citStr="Mikolov et al., 2013" startWordPosition="790" endWordPosition="793">so gradable adjectives, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting word pairs (refer to (Mohammad et al., 2013) for detailed discussions). Word Embedding Word embedding models learn continuous representations for words in a low dimensional space (Turney and Pantel, 2010; Hinton and Roweis, 2002; Collobert et al., 2011; Mikolov et al., 2013; Liu et al., 2015), which is not new. Linear dimension reduction such as Latent Semantic Analysis (LSA) has been extensively used in lexical semantics (see (Turney and Pantel, 2010) for good discussions in vector space models.) Non-linear models such as those described in (Roweis and Saul, 2000) and (Tenenbaum et al., 2000), among many others, can also be applied to learn word embeddings. A particularly interesting model is stochastic neighbor embedding (SNE) (Hinton and Roweis, 2002), which explicitly enforces that in the embedding space, the distribution of neighbors of a given word to be s</context>
<context position="15463" citStr="Mikolov et al., 2013" startWordPosition="2472" endWordPosition="2475">nces of semantically close pairs) by two margins. 3.4 Corpus Relatedness Modeling (CRM) As discussed in previous work and above as well, relatedness obtained with corpora based on distributional hypothesis interplays with semantic closeness and contrast. Mohammad et al. (2013) proposed a degree of contrast hypothesis, stating that “if a pair of words, A and B, are contrasting, then their degree of contrast is proportional to their tendency to co-occur in a large corpus.” In embedding, such dependency can be used to help measure the degree of contrast. Specifically, we use the skip-gram model (Mikolov et al., 2013) to learn the relatedness embedding. As discussed above, through the top hidden layers, the word embedding and distances learned in SCE/MCE and CRM, together with that learned with SDR below, can be used to predict the GRE “most contrasting word”’ questions. With enough GRE data, the prediction error may be backpropagated to directly adjust or learn embedding in the look-up tables. However, given the limited size of the GRE data, we only employed the top hidden layers to non-linearly merge the distances between a word pair that are obtained within each of the modules in the Contrast Inference </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="19206" citStr="Miller, 1995" startWordPosition="3078" endWordPosition="3079">given target word. The dataset consists of a development set and a test set, with 162 and 950 questions, respectively. As an example from (Mohammad et al., 2013), one of the questions has the target word adulterate and the five candidate choices: (A) renounce, (B) forbid, (C) purify, (D) criticize, and (E) correct. While in this example the choice correct has a meaning that is contrasting with that of adulterate, the word purify is the gold answer as it has the greatest degree of contrast with adulterate. Lexical Resources In our work, we use two publicly available lexical resources, WordNet (Miller, 1995) (version 3.0) and the Roget’s Thesaurus (Kipfer, 2009). We utilized the labeled antonym relations to obtain more contrasting pairs under the contrast hypothesis (Mohammad et al., 2013), by assuming a contrasting pair is related to a pair of opposites (antonyms here). Specifically in WordNet, we consider the word pairs with relations other than antonym as semantically close. In this way, we obtained a thesaurus containing 83,118 words, 494,579 contrasting pairs, and 368,209 close pairs. Note that we did not only use synonyms to expand the contrasting pairs. We will discuss how this affects the</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word-pair antonymy.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>982--991</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="871" citStr="Mohammad et al., 2008" startWordPosition="112" endWordPosition="115">hina ‡ iFLYTEK Research, Hefei, China §Department of EECS, York University, Toronto, Canada emails: zgchen9517017@gmail.com, weilin2@iflytek.com, cq1231@mail.ustc.edu.cn, xpchen@ustc.edu.cn, siwei@iflytek.com, hj@cse.yorku.ca, zhu2048@gmail.com Abstract Contrasting meaning is a basic aspect of semantics. Recent word-embedding models based on distributional semantics hypothesis are known to be weak for modeling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest performance seen so far on this dataset. Surprisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget’s and WordNet)—corpora statistics did not lead to further improvement. However, this should not be simply taken as that distributional statistics is not useful. We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to s</context>
<context position="2551" citStr="Mohammad et al., 2008" startWordPosition="371" endWordPosition="374">., speech recognition and automatic machine translation) and on many non-language problems (e.g., image recognition). Distributed representations have been leveraged to represent words as in (Collobert et al., 2011; Mikolov et al., 2013). Contrasting meaning is a basic aspect of semantics, but it is widely known that word embedding models based on distributional semantics hypothesis are weak in modeling this—contrasting meaning is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable. Lexical contrast has been modeled in (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances. For example, Yih et al. (2012) proposed to use polarity-primed latent semantic analysis (LSA), called PILSA, to capture contrast, which was further used to initialize a neural network and achieved an F-score of 81% on the same GRE “most contrasting word” questions (Mohammad et al., 2008). More recently, Zhang et al. (2014) proposed a tensor factorization approach to solving the problem, resulting in a 82% F-score. In th</context>
<context position="6788" citStr="Mohammad et al., 2008" startWordPosition="1034" endWordPosition="1038">iently. Although word embeddings have recently showed to be superior in some NLP tasks, they are very weak in distinguishing contrasting meaning, as the models are often based on the well-known distributional semantics hypothesis— words in similar context have similar meanings. Contrasting words have similar context too, so contrasting meaning is not distinguished well in such representations. Better models for contrasting meaning is fundamentally interesting. Modeling Contrasting Meaning Automatically detecting contrasting meaning has been studied in earlier work such as (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). Specifically, as far as the embedding-based methods are concerned, PILSA (Yih et al., 2012) made a progress in achieving one of the best results, by priming LSA to encode contrasting meaning. In addition, PILSA was also used to initialize a neural network to get a further improvement on the GRE benchmark, where an F-score of 81% was obtained. Another recent method was proposed by (Zhang et al., 2014), called Bayesian probabilistic tensor factorization. It considered multidimensional semantic information, relations, unsupervised data structure information in tensor fac</context>
<context position="18334" citStr="Mohammad et al. (2008)" startWordPosition="2928" endWordPosition="2931">l+) − PMI(w, eval−) (4) where eval+ contains predefined positive evaluative words, e.g., good, positive, fortunate, and superior, while eval− includes negative evaluative words like passive, slow, treble, and old. The seed words were selected as described in (Turney and Littman, 2003) to have a good coverage and to avoid redundancy at the same time. Similarly, the potency and activity scores of a word can be obtained. The distances of a word pair on these three dimensions can therefore be obtained. 4 Experiment Set-Up Data Our experiment uses the “most contrasting word” questions collected by Mohammad et al. (2008) from Graduate Record Examination (GRE), which was originally created by Educational Testing Service (ETS). Each GRE question has a target word and five candidate choices; the task is to identify among the choices the most contrasting word with regard to the given target word. The dataset consists of a development set and a test set, with 162 and 950 questions, respectively. As an example from (Mohammad et al., 2013), one of the questions has the target word adulterate and the five candidate choices: (A) renounce, (B) forbid, (C) purify, (D) criticize, and (E) correct. While in this example th</context>
<context position="26386" citStr="Mohammad et al., 2008" startWordPosition="4281" endWordPosition="4284">.65 0.65 0.61 0.59 0.60 Encarta lookup (Yih et al., 2012) 0.65 0.61 0.63 0.61 0.56 0.59 Encarta PILSA (Yih et al., 2012) 0.86 0.81 0.84 0.81 0.74 0.77 Encarta MRLSA (Yih et al., 2012) 0.87 0.82 0.84 0.82 0.74 0.78 WordNet lookup (Yih et al., 2012) 0.40 0.40 0.40 0.42 0.41 0.42 WordNet lookup (Zhang et al., 2014) 0.93 0.32 0.48 0.95 0.33 0.49 WordNet lookup 0.97 0.37 0.54 0.97 0.41 0.58 Roget lookup (Zhang et al., 2014) 1.00 0.35 0.52 0.99 0.31 0.47 Roget lookup 1.00 0.32 0.49 0.97 0.29 0.44 W&amp;R lookup (Zhang et al., 2014) 1.00 0.48 0.64 0.98 0.45 0.62 W&amp;R lookup 0.98 0.52 0.68 0.97 0.52 0.68 (Mohammad et al., 2008) Best 0.76 0.66 0.70 0.76 0.64 0.70 (Yih et al., 2012) Best 0.88 0.87 0.87 0.81 0.80 0.81 (Zhang et al., 2014) Best 0.88 0.88 0.88 0.82 0.82 0.82 SCE 0.94 0.93 0.93 0.90 0.90 0.90 MCE (using zhang et al. lex.) 0.94 0.93 0.94 0.92 0.91 0.91 MCE 0.96 0.94 0.95 0.92 0.92 0.92 MCE+CRM 0.94 0.93 0.93 0.90 0.90 0.90 MCE+CRM+SDR 0.04 0.94 0.94 0.90 0.90 0.90 Table 1: Results on the GRE “most contrasting words” questions. connected with these two types of edges respectively. Then we require the shortest path must have one and only one contrasting edge. Word pairs that cannot be connected by such paths</context>
<context position="30611" citStr="Mohammad et al., 2008" startWordPosition="4981" endWordPosition="4984">at the use of corpora here is rather straightforward; more patterns may be learned from corpora to capture contrasting expressions as discussed in (Mohammad et al., 2013). Also, context such as negation may change contrasting meaning, e.g., sentiment contrast (Kiritchenko et al., 2014b; Zhu et al., 2014a), in a dramatic and complicated manner, which has been considered in learning sentiment contrast (Kiritchenko et al., 2014b). 6 Conclusions Contrasting meaning is a basic aspect of semantics. In this paper, we present a new state-of-theart result, a 92% F-score, on the GRE dataset created by (Mohammad et al., 2008), which is widely used as the benchmark for modeling lexical contrast. The result reported here outperforms the best reported in previous work (82%) by a large margin. Unlike what was suggested in most previous work, we show that this performance can be achieved without relying on corpora statistics. To provide a more comprehensive understanding, we constructed our study in a framework that examFrequency 0.25 0.15 0.05 0.2 0.1 0 −1 −0.5 0 0.5 1 MCE on most contrasting pairs MCE on others MCE w/o negative sampling on most contrasting pairs MCE w/o negative sampling on others 113 ines a number o</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008. Computing word-pair antonymy. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 982–991. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Bonnie J Dorr</author>
<author>Graeme Hirst</author>
<author>Peter D Turney</author>
</authors>
<title>Computing lexical contrast.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="2575" citStr="Mohammad et al., 2013" startWordPosition="375" endWordPosition="379">nd automatic machine translation) and on many non-language problems (e.g., image recognition). Distributed representations have been leveraged to represent words as in (Collobert et al., 2011; Mikolov et al., 2013). Contrasting meaning is a basic aspect of semantics, but it is widely known that word embedding models based on distributional semantics hypothesis are weak in modeling this—contrasting meaning is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable. Lexical contrast has been modeled in (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances. For example, Yih et al. (2012) proposed to use polarity-primed latent semantic analysis (LSA), called PILSA, to capture contrast, which was further used to initialize a neural network and achieved an F-score of 81% on the same GRE “most contrasting word” questions (Mohammad et al., 2008). More recently, Zhang et al. (2014) proposed a tensor factorization approach to solving the problem, resulting in a 82% F-score. In this paper, we present emb</context>
<context position="4354" citStr="Mohammad et al., 2013" startWordPosition="647" endWordPosition="650"> a number of basic concerns in modeling contrasting meaning. We hope our efforts would help shed some light on future directions for this basic semantic modeling problem. 106 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 106–115, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work The terms contrasting, opposite, and antonym have different definitions in the literature, while sometimes they are used interchangeably. Following (Mohammad et al., 2013), in this paper we refer to opposites as word pairs that “have a strong binary incompatibility relation with each other or that are saliently different across a dimension of meaning”, e.g., day and night. Antonyms are a subset of opposites that are also gradable adjectives, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting wor</context>
<context position="6812" citStr="Mohammad et al., 2013" startWordPosition="1039" endWordPosition="1042">mbeddings have recently showed to be superior in some NLP tasks, they are very weak in distinguishing contrasting meaning, as the models are often based on the well-known distributional semantics hypothesis— words in similar context have similar meanings. Contrasting words have similar context too, so contrasting meaning is not distinguished well in such representations. Better models for contrasting meaning is fundamentally interesting. Modeling Contrasting Meaning Automatically detecting contrasting meaning has been studied in earlier work such as (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). Specifically, as far as the embedding-based methods are concerned, PILSA (Yih et al., 2012) made a progress in achieving one of the best results, by priming LSA to encode contrasting meaning. In addition, PILSA was also used to initialize a neural network to get a further improvement on the GRE benchmark, where an F-score of 81% was obtained. Another recent method was proposed by (Zhang et al., 2014), called Bayesian probabilistic tensor factorization. It considered multidimensional semantic information, relations, unsupervised data structure information in tensor factorization, and achieved</context>
<context position="9079" citStr="Mohammad et al. (2013)" startWordPosition="1409" endWordPosition="1412">irs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., good and bad, also intend to appear in similar context or co-occur with each other. For example, opposite pairs, special cases of contrasting words, tend to co-occur more often than chance (Charles and Miller, 1989; Fellbaum, 1995; Murphy and Andrew, 1993). Mohammad et al. (2013), in addition, proposed a degree of contrast hypothesis, stating that “if a pair of words, A and B, are contrasting, then their degree of contrast is proportional to their tendency to co-occur in a large corpus.” These suggest some non-linear interaction between distributional relatedness and the degree of contrast: the increase of relatedness correspond to the increase of both semantic contrast and semantic closeness; for example, they can form a U-shaped curve if one plots the word pairs on a two dimensional plane with y-axis denoting relatedness scores, while the most contrasting and (seman</context>
<context position="15119" citStr="Mohammad et al. (2013)" startWordPosition="2413" endWordPosition="2416">words di,r by a margin Q. Same as in SCE, these two objective functions are linearly combined with a parameter A and are jointly optimized to learn one embedding for each word. This joint objective function attempts to force the values of di,r (distances of unrelated pairs) to be in between di,k (distances of contrasting pairs) and di,j (distances of semantically close pairs) by two margins. 3.4 Corpus Relatedness Modeling (CRM) As discussed in previous work and above as well, relatedness obtained with corpora based on distributional hypothesis interplays with semantic closeness and contrast. Mohammad et al. (2013) proposed a degree of contrast hypothesis, stating that “if a pair of words, A and B, are contrasting, then their degree of contrast is proportional to their tendency to co-occur in a large corpus.” In embedding, such dependency can be used to help measure the degree of contrast. Specifically, we use the skip-gram model (Mikolov et al., 2013) to learn the relatedness embedding. As discussed above, through the top hidden layers, the word embedding and distances learned in SCE/MCE and CRM, together with that learned with SDR below, can be used to predict the GRE “most contrasting word”’ question</context>
<context position="18754" citStr="Mohammad et al., 2013" startWordPosition="2999" endWordPosition="3002">. The distances of a word pair on these three dimensions can therefore be obtained. 4 Experiment Set-Up Data Our experiment uses the “most contrasting word” questions collected by Mohammad et al. (2008) from Graduate Record Examination (GRE), which was originally created by Educational Testing Service (ETS). Each GRE question has a target word and five candidate choices; the task is to identify among the choices the most contrasting word with regard to the given target word. The dataset consists of a development set and a test set, with 162 and 950 questions, respectively. As an example from (Mohammad et al., 2013), one of the questions has the target word adulterate and the five candidate choices: (A) renounce, (B) forbid, (C) purify, (D) criticize, and (E) correct. While in this example the choice correct has a meaning that is contrasting with that of adulterate, the word purify is the gold answer as it has the greatest degree of contrast with adulterate. Lexical Resources In our work, we use two publicly available lexical resources, WordNet (Miller, 1995) (version 3.0) and the Roget’s Thesaurus (Kipfer, 2009). We utilized the labeled antonym relations to obtain more contrasting pairs under the contra</context>
<context position="30159" citStr="Mohammad et al., 2013" startWordPosition="4905" endWordPosition="4909">MCE, as we removed lexical entries. We also combined MCE distances and CRM distances linearly (MCE+CRM (linear)), with a coefficient determined with the development set. It showed a performance worse than that of MCE+CRM when 50%–80% entries kept, while as discussed above, MCE+CRM combines the two parts with the nonlinear top layers. In general, using corpora statistics make the models more robust as OOV becomes more serious. It deserves to note that the use of corpora here is rather straightforward; more patterns may be learned from corpora to capture contrasting expressions as discussed in (Mohammad et al., 2013). Also, context such as negation may change contrasting meaning, e.g., sentiment contrast (Kiritchenko et al., 2014b; Zhu et al., 2014a), in a dramatic and complicated manner, which has been considered in learning sentiment contrast (Kiritchenko et al., 2014b). 6 Conclusions Contrasting meaning is a basic aspect of semantics. In this paper, we present a new state-of-theart result, a 92% F-score, on the GRE dataset created by (Mohammad et al., 2008), which is widely used as the benchmark for modeling lexical contrast. The result reported here outperforms the best reported in previous work (82%)</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, Turney, 2013</marker>
<rawString>Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney. 2013. Computing lexical contrast. Computational Linguistics, 39(3):555–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory L Murphy</author>
<author>Jane M Andrew</author>
</authors>
<title>The conceptual basis of antonymy and synonymy in adjectives.</title>
<date>1993</date>
<journal>Journal of Memory and Language,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="9055" citStr="Murphy and Andrew, 1993" startWordPosition="1404" endWordPosition="1408">bedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., good and bad, also intend to appear in similar context or co-occur with each other. For example, opposite pairs, special cases of contrasting words, tend to co-occur more often than chance (Charles and Miller, 1989; Fellbaum, 1995; Murphy and Andrew, 1993). Mohammad et al. (2013), in addition, proposed a degree of contrast hypothesis, stating that “if a pair of words, A and B, are contrasting, then their degree of contrast is proportional to their tendency to co-occur in a large corpus.” These suggest some non-linear interaction between distributional relatedness and the degree of contrast: the increase of relatedness correspond to the increase of both semantic contrast and semantic closeness; for example, they can form a U-shaped curve if one plots the word pairs on a two dimensional plane with y-axis denoting relatedness scores, while the mos</context>
</contexts>
<marker>Murphy, Andrew, 1993</marker>
<rawString>Gregory L. Murphy and Jane M. Andrew. 1993. The conceptual basis of antonymy and synonymy in adjectives. Journal of Memory and Language, 32(3):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles E Osgood</author>
<author>George J Suci</author>
<author>Percy Tannenbaum</author>
</authors>
<title>The measurement of meaning.</title>
<date>1957</date>
<publisher>University of Illinois Press.</publisher>
<contexts>
<context position="8024" citStr="Osgood et al., 1957" startWordPosition="1230" endWordPosition="1233">achieved an F-score of 82% on the GRE questions. These methods employed both lexical resources and corpora statistics to achieve their best results. In this paper, we show that using only lexical resources to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehensive understanding, we constructed our study in a framework that examines a number of basic concerns in modeling contrasting meaning within embedding. Note that sentiment contrast may be viewed as a specific case of more general semantic contrast or semantic differentials (Osgood et al., 1957). Tang et al. (2014) learned sentiment-specific embedding and applied it to sentiment analysis of tweets, which was often solved with more conventional methods (Zhu et al., 2014b; Kiritchenko et al., 2014a; Kiritchenko et al., 2014b). 3 The Models We described in this section the framework in which we study word embedding for contrasting meaning. The general aim of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-leve</context>
<context position="16891" citStr="Osgood et al. (1957)" startWordPosition="2695" endWordPosition="2698">yer, e.g., in SCE, MCE and CRM, respectively. And in each module, given the corresponding objective functions, unconstrained optimization (e.g., in the paper SGD) was used to find embeddings that optimize the corresponding objectives. The embeddings were then used out-of-box and not further fine-tuned. Depending on experiment settings, embeddings learned in each module are either used separately or jointly (through the top hidden lay) to predict test cases. 109 More details will be discussed in the experiment section below. 3.5 Semantic Differential Reconstruction (SDR) Using factor analysis, Osgood et al. (1957) identified three dimensions of semantics that account for most of the variation in the connotative meaning of adjectives. These three dimensions are evaluative (good-bad), potency (strong-weak), and activity(active-passive). We hypothesize that such information should help reconstruct contrasting meaning. The General Inquirer lexicon (Stone1966) represents these three factors but has a limited coverage. We used the algorithm of (Turney and Littman, 2003) to extend the labels to more words with Google one billion words corpus (refer to Section 4 for details). For example, to obtain the evaluat</context>
</contexts>
<marker>Osgood, Suci, Tannenbaum, 1957</marker>
<rawString>Charles E Osgood, George J Suci, and Percy Tannenbaum. 1957. The measurement of meaning. University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam T Roweis</author>
<author>Lawrence K Saul</author>
</authors>
<title>Nonlinear dimensionality reduction by locally linear embedding.</title>
<date>2000</date>
<journal>Science,</journal>
<pages>290--2323</pages>
<contexts>
<context position="5522" citStr="Roweis and Saul, 2000" startWordPosition="838" endWordPosition="841"> definition, opposites are a subset of contrasting word pairs (refer to (Mohammad et al., 2013) for detailed discussions). Word Embedding Word embedding models learn continuous representations for words in a low dimensional space (Turney and Pantel, 2010; Hinton and Roweis, 2002; Collobert et al., 2011; Mikolov et al., 2013; Liu et al., 2015), which is not new. Linear dimension reduction such as Latent Semantic Analysis (LSA) has been extensively used in lexical semantics (see (Turney and Pantel, 2010) for good discussions in vector space models.) Non-linear models such as those described in (Roweis and Saul, 2000) and (Tenenbaum et al., 2000), among many others, can also be applied to learn word embeddings. A particularly interesting model is stochastic neighbor embedding (SNE) (Hinton and Roweis, 2002), which explicitly enforces that in the embedding space, the distribution of neighbors of a given word to be similar to that in the original, uncompressed space. SNE can learn multiple senses of a word with a mixture component. Recently, neural-network based model such as those proposed by (Collobert et al., 2011) and (Mikolov et al., 2013) have attracted extensive attention; particularly the latter, whi</context>
</contexts>
<marker>Roweis, Saul, 2000</marker>
<rawString>Sam T. Roweis and Lawrence K. Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentimentspecific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="8044" citStr="Tang et al. (2014)" startWordPosition="1234" endWordPosition="1237"> 82% on the GRE questions. These methods employed both lexical resources and corpora statistics to achieve their best results. In this paper, we show that using only lexical resources to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehensive understanding, we constructed our study in a framework that examines a number of basic concerns in modeling contrasting meaning within embedding. Note that sentiment contrast may be viewed as a specific case of more general semantic contrast or semantic differentials (Osgood et al., 1957). Tang et al. (2014) learned sentiment-specific embedding and applied it to sentiment analysis of tweets, which was often solved with more conventional methods (Zhu et al., 2014b; Kiritchenko et al., 2014a; Kiritchenko et al., 2014b). 3 The Models We described in this section the framework in which we study word embedding for contrasting meaning. The general aim of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framew</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentimentspecific word embedding for twitter sentiment classification. In Proceedings ofACL, Baltimore, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua B Tenenbaum</author>
<author>Vin de Silva</author>
<author>John C Langford</author>
</authors>
<title>A global geometric framework for nonlinear dimensionality reduction.</title>
<date>2000</date>
<journal>Science,</journal>
<volume>290</volume>
<issue>5500</issue>
<marker>Tenenbaum, de Silva, Langford, 2000</marker>
<rawString>Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. 2000. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="17350" citStr="Turney and Littman, 2003" startWordPosition="2761" endWordPosition="2764"> cases. 109 More details will be discussed in the experiment section below. 3.5 Semantic Differential Reconstruction (SDR) Using factor analysis, Osgood et al. (1957) identified three dimensions of semantics that account for most of the variation in the connotative meaning of adjectives. These three dimensions are evaluative (good-bad), potency (strong-weak), and activity(active-passive). We hypothesize that such information should help reconstruct contrasting meaning. The General Inquirer lexicon (Stone1966) represents these three factors but has a limited coverage. We used the algorithm of (Turney and Littman, 2003) to extend the labels to more words with Google one billion words corpus (refer to Section 4 for details). For example, to obtain the evaluative score for a candidate word w, the pointwise mutual information (PMI) between w and a set of seed words eval+ and eval− are computed respectively, and the evaluative value for w is calculated with: eval(w) = PMI(w, eval+) − PMI(w, eval−) (4) where eval+ contains predefined positive evaluative words, e.g., good, positive, fortunate, and superior, while eval− includes negative evaluative words like passive, slow, treble, and old. The seed words were sele</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems (TOIS), 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="5154" citStr="Turney and Pantel, 2010" startWordPosition="777" endWordPosition="780">ning”, e.g., day and night. Antonyms are a subset of opposites that are also gradable adjectives, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting word pairs (refer to (Mohammad et al., 2013) for detailed discussions). Word Embedding Word embedding models learn continuous representations for words in a low dimensional space (Turney and Pantel, 2010; Hinton and Roweis, 2002; Collobert et al., 2011; Mikolov et al., 2013; Liu et al., 2015), which is not new. Linear dimension reduction such as Latent Semantic Analysis (LSA) has been extensively used in lexical semantics (see (Turney and Pantel, 2010) for good discussions in vector space models.) Non-linear models such as those described in (Roweis and Saul, 2000) and (Tenenbaum et al., 2000), among many others, can also be applied to learn word embeddings. A particularly interesting model is stochastic neighbor embedding (SNE) (Hinton and Roweis, 2002), which explicitly enforces that in the</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. J. Artif. Int. Res., 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John C Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1212--1222</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2758" citStr="Yih et al. (2012)" startWordPosition="402" endWordPosition="405">011; Mikolov et al., 2013). Contrasting meaning is a basic aspect of semantics, but it is widely known that word embedding models based on distributional semantics hypothesis are weak in modeling this—contrasting meaning is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable. Lexical contrast has been modeled in (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances. For example, Yih et al. (2012) proposed to use polarity-primed latent semantic analysis (LSA), called PILSA, to capture contrast, which was further used to initialize a neural network and achieved an F-score of 81% on the same GRE “most contrasting word” questions (Mohammad et al., 2008). More recently, Zhang et al. (2014) proposed a tensor factorization approach to solving the problem, resulting in a 82% F-score. In this paper, we present embedding models that achieve an F-score of 92% on the GRE dataset, which outperforms the previous best result (82%) by a large margin. Unlike what was suggested in previous work, where </context>
<context position="6905" citStr="Yih et al., 2012" startWordPosition="1054" endWordPosition="1057">ing contrasting meaning, as the models are often based on the well-known distributional semantics hypothesis— words in similar context have similar meanings. Contrasting words have similar context too, so contrasting meaning is not distinguished well in such representations. Better models for contrasting meaning is fundamentally interesting. Modeling Contrasting Meaning Automatically detecting contrasting meaning has been studied in earlier work such as (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). Specifically, as far as the embedding-based methods are concerned, PILSA (Yih et al., 2012) made a progress in achieving one of the best results, by priming LSA to encode contrasting meaning. In addition, PILSA was also used to initialize a neural network to get a further improvement on the GRE benchmark, where an F-score of 81% was obtained. Another recent method was proposed by (Zhang et al., 2014), called Bayesian probabilistic tensor factorization. It considered multidimensional semantic information, relations, unsupervised data structure information in tensor factorization, and achieved an F-score of 82% on the GRE questions. These methods employed both lexical resources and co</context>
<context position="22053" citStr="Yih et al., 2012" startWordPosition="3537" endWordPosition="3540">air between a question word and a candidate choice in these two corresponding embedding spaces to report their performances. We also combined SCE/MCE with other components in the contrast inference layer, for which we used ten-fold cross validation to tune the weights of the top hidden layers on nine fold and test on the rest and repeated this for ten times to report the results. As discussed above, errors were not backpropagated to modify word embedding. 5.1 General Performance of the Models The performance of the models are showed in Table 1. For comparison, we list the results reported in (Yih et al., 2012) and (Zhang et al., 2014). The table shows that on the GRE dataset, both SCE (a 90% F-score) and MCE (92%) significantly outperform the previous best results reported in (Yih et al., 2012) (81%) and (Zhang et al., 2014) (82%). The F-score of MCE outperforms that of SCE by 2%, which suggests the ranking criterion fits the dataset better. In our experiment, we found that the MCE model achieved robust performances on different distance metrics, e.g., the cosine similarity and Euclidean distance. In the paper, we present the results with cosine similarity. SCE is slightly more sensitive to distanc</context>
<context position="23420" citStr="Yih et al., 2012" startWordPosition="3760" endWordPosition="3763">ious work, where semantics learned from corpus is claimed to yield extra gains in performance, we obtained this result by using solely lexical resources (Roget’s and WordNet) with SCE and MCE. Using corpus statistics that model distributional hypothesis (MCE+CRM) and utilize semantic differential categories (MCE+CRM+SDR) does not bring further improvement here (they are useful in the experiments discussed below in Section 5.3). 5.2 Roles of Lexical Resources To provide a more detailed comparison, we also present lexicon lookup results, together with those reported in (Zhang et al., 2014) and (Yih et al., 2012). For our lookup results and those copied here from (Zhang et al., 2014), the methods do not randomly guess an answer if the target word is in the vocabulary but none of the choices are, while the results of (Yih et al., 2012) randomly guess an answer in this situation. The Encarta thesaurus used in (Yih et al., 2012) is not publicly available, so we did not use it in our experiments. We due the differences among the lookup results on WordNet (WordNet lookup) to the differences in preprocessing as well as the way we expanded indirect contrasting word pairs. As described in Section 4, we utiliz</context>
<context position="25694" citStr="Yih et al., 2012" startWordPosition="4151" endWordPosition="4154">lose words. To help better understand why corpus statistics does not further help SCE and MCE, we further demonstrate that most of the target-goldanswer pairs in the GRE test set are connected by short paths (with length between 1 to 3). More specifically, based on breadth-first search, we found the nearest paths that connect targetgold-answer pairs, in the graph formed by WordNet and Roget’s—each word is a vertex, and contrasting words and semantically close words are 2https://github.com/iceboal/word-representations-bptf 111 Development Set Test Set Fi Prec. Rec. Fi Prec. Rec. WordNet PILSA (Yih et al., 2012) 0.63 0.62 0.62 0.60 0.60 0.60 WordNet MRLSA (Yih et al., 2012) 0.66 0.65 0.65 0.61 0.59 0.60 Encarta lookup (Yih et al., 2012) 0.65 0.61 0.63 0.61 0.56 0.59 Encarta PILSA (Yih et al., 2012) 0.86 0.81 0.84 0.81 0.74 0.77 Encarta MRLSA (Yih et al., 2012) 0.87 0.82 0.84 0.82 0.74 0.78 WordNet lookup (Yih et al., 2012) 0.40 0.40 0.40 0.42 0.41 0.42 WordNet lookup (Zhang et al., 2014) 0.93 0.32 0.48 0.95 0.33 0.49 WordNet lookup 0.97 0.37 0.54 0.97 0.41 0.58 Roget lookup (Zhang et al., 2014) 1.00 0.35 0.52 0.99 0.31 0.47 Roget lookup 1.00 0.32 0.49 0.97 0.29 0.44 W&amp;R lookup (Zhang et al., 2014) 1.</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John C Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingwei Zhang</author>
<author>Jeremy Salwen</author>
<author>Michael Glass</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Word semantic representations using bayesian probabilistic tensor factorization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1522--1531</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="3052" citStr="Zhang et al. (2014)" startWordPosition="450" endWordPosition="453"> and better models would be desirable. Lexical contrast has been modeled in (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to stateof-the-art performances. For example, Yih et al. (2012) proposed to use polarity-primed latent semantic analysis (LSA), called PILSA, to capture contrast, which was further used to initialize a neural network and achieved an F-score of 81% on the same GRE “most contrasting word” questions (Mohammad et al., 2008). More recently, Zhang et al. (2014) proposed a tensor factorization approach to solving the problem, resulting in a 82% F-score. In this paper, we present embedding models that achieve an F-score of 92% on the GRE dataset, which outperforms the previous best result (82%) by a large margin. Unlike what was suggested in previous work, where relatedness statistics learned from corpora is often claimed to yield extra gains over lexicon-based models, we obtained this new state-of-the-art result relying solely on lexical resources (Roget’s and WordNet), and corpus statistics does not seem to bring further improvement. To provide a co</context>
<context position="7217" citStr="Zhang et al., 2014" startWordPosition="1109" endWordPosition="1112">ing is fundamentally interesting. Modeling Contrasting Meaning Automatically detecting contrasting meaning has been studied in earlier work such as (Lin and Zhao, 2003; Mohammad et al., 2008; Mohammad et al., 2013). Specifically, as far as the embedding-based methods are concerned, PILSA (Yih et al., 2012) made a progress in achieving one of the best results, by priming LSA to encode contrasting meaning. In addition, PILSA was also used to initialize a neural network to get a further improvement on the GRE benchmark, where an F-score of 81% was obtained. Another recent method was proposed by (Zhang et al., 2014), called Bayesian probabilistic tensor factorization. It considered multidimensional semantic information, relations, unsupervised data structure information in tensor factorization, and achieved an F-score of 82% on the GRE questions. These methods employed both lexical resources and corpora statistics to achieve their best results. In this paper, we show that using only lexical resources to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehensive understanding, we constructed our study in a framework that examines a number of b</context>
<context position="22078" citStr="Zhang et al., 2014" startWordPosition="3542" endWordPosition="3545">word and a candidate choice in these two corresponding embedding spaces to report their performances. We also combined SCE/MCE with other components in the contrast inference layer, for which we used ten-fold cross validation to tune the weights of the top hidden layers on nine fold and test on the rest and repeated this for ten times to report the results. As discussed above, errors were not backpropagated to modify word embedding. 5.1 General Performance of the Models The performance of the models are showed in Table 1. For comparison, we list the results reported in (Yih et al., 2012) and (Zhang et al., 2014). The table shows that on the GRE dataset, both SCE (a 90% F-score) and MCE (92%) significantly outperform the previous best results reported in (Yih et al., 2012) (81%) and (Zhang et al., 2014) (82%). The F-score of MCE outperforms that of SCE by 2%, which suggests the ranking criterion fits the dataset better. In our experiment, we found that the MCE model achieved robust performances on different distance metrics, e.g., the cosine similarity and Euclidean distance. In the paper, we present the results with cosine similarity. SCE is slightly more sensitive to distance metrics, and the best p</context>
<context position="23397" citStr="Zhang et al., 2014" startWordPosition="3755" endWordPosition="3758">was suggested in the previous work, where semantics learned from corpus is claimed to yield extra gains in performance, we obtained this result by using solely lexical resources (Roget’s and WordNet) with SCE and MCE. Using corpus statistics that model distributional hypothesis (MCE+CRM) and utilize semantic differential categories (MCE+CRM+SDR) does not bring further improvement here (they are useful in the experiments discussed below in Section 5.3). 5.2 Roles of Lexical Resources To provide a more detailed comparison, we also present lexicon lookup results, together with those reported in (Zhang et al., 2014) and (Yih et al., 2012). For our lookup results and those copied here from (Zhang et al., 2014), the methods do not randomly guess an answer if the target word is in the vocabulary but none of the choices are, while the results of (Yih et al., 2012) randomly guess an answer in this situation. The Encarta thesaurus used in (Yih et al., 2012) is not publicly available, so we did not use it in our experiments. We due the differences among the lookup results on WordNet (WordNet lookup) to the differences in preprocessing as well as the way we expanded indirect contrasting word pairs. As described </context>
<context position="26077" citStr="Zhang et al., 2014" startWordPosition="4223" endWordPosition="4226">et and Roget’s—each word is a vertex, and contrasting words and semantically close words are 2https://github.com/iceboal/word-representations-bptf 111 Development Set Test Set Fi Prec. Rec. Fi Prec. Rec. WordNet PILSA (Yih et al., 2012) 0.63 0.62 0.62 0.60 0.60 0.60 WordNet MRLSA (Yih et al., 2012) 0.66 0.65 0.65 0.61 0.59 0.60 Encarta lookup (Yih et al., 2012) 0.65 0.61 0.63 0.61 0.56 0.59 Encarta PILSA (Yih et al., 2012) 0.86 0.81 0.84 0.81 0.74 0.77 Encarta MRLSA (Yih et al., 2012) 0.87 0.82 0.84 0.82 0.74 0.78 WordNet lookup (Yih et al., 2012) 0.40 0.40 0.40 0.42 0.41 0.42 WordNet lookup (Zhang et al., 2014) 0.93 0.32 0.48 0.95 0.33 0.49 WordNet lookup 0.97 0.37 0.54 0.97 0.41 0.58 Roget lookup (Zhang et al., 2014) 1.00 0.35 0.52 0.99 0.31 0.47 Roget lookup 1.00 0.32 0.49 0.97 0.29 0.44 W&amp;R lookup (Zhang et al., 2014) 1.00 0.48 0.64 0.98 0.45 0.62 W&amp;R lookup 0.98 0.52 0.68 0.97 0.52 0.68 (Mohammad et al., 2008) Best 0.76 0.66 0.70 0.76 0.64 0.70 (Yih et al., 2012) Best 0.88 0.87 0.87 0.81 0.80 0.81 (Zhang et al., 2014) Best 0.88 0.88 0.88 0.82 0.82 0.82 SCE 0.94 0.93 0.93 0.90 0.90 0.90 MCE (using zhang et al. lex.) 0.94 0.93 0.94 0.92 0.91 0.91 MCE 0.96 0.94 0.95 0.92 0.92 0.92 MCE+CRM 0.94 0.93</context>
</contexts>
<marker>Zhang, Salwen, Glass, Gliozzo, 2014</marker>
<rawString>Jingwei Zhang, Jeremy Salwen, Michael Glass, and Alfio Gliozzo. 2014. Word semantic representations using bayesian probabilistic tensor factorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1522–1531, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Hongyu Guo</author>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
</authors>
<title>An empirical study on the effect of negation words on sentiment.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="8201" citStr="Zhu et al., 2014" startWordPosition="1259" endWordPosition="1262">g only lexical resources to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehensive understanding, we constructed our study in a framework that examines a number of basic concerns in modeling contrasting meaning within embedding. Note that sentiment contrast may be viewed as a specific case of more general semantic contrast or semantic differentials (Osgood et al., 1957). Tang et al. (2014) learned sentiment-specific embedding and applied it to sentiment analysis of tweets, which was often solved with more conventional methods (Zhu et al., 2014b; Kiritchenko et al., 2014a; Kiritchenko et al., 2014b). 3 The Models We described in this section the framework in which we study word embedding for contrasting meaning. The general aim of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., go</context>
<context position="30293" citStr="Zhu et al., 2014" startWordPosition="4928" endWordPosition="4931">ned with the development set. It showed a performance worse than that of MCE+CRM when 50%–80% entries kept, while as discussed above, MCE+CRM combines the two parts with the nonlinear top layers. In general, using corpora statistics make the models more robust as OOV becomes more serious. It deserves to note that the use of corpora here is rather straightforward; more patterns may be learned from corpora to capture contrasting expressions as discussed in (Mohammad et al., 2013). Also, context such as negation may change contrasting meaning, e.g., sentiment contrast (Kiritchenko et al., 2014b; Zhu et al., 2014a), in a dramatic and complicated manner, which has been considered in learning sentiment contrast (Kiritchenko et al., 2014b). 6 Conclusions Contrasting meaning is a basic aspect of semantics. In this paper, we present a new state-of-theart result, a 92% F-score, on the GRE dataset created by (Mohammad et al., 2008), which is widely used as the benchmark for modeling lexical contrast. The result reported here outperforms the best reported in previous work (82%) by a large margin. Unlike what was suggested in most previous work, we show that this performance can be achieved without relying on </context>
</contexts>
<marker>Zhu, Guo, Mohammad, Kiritchenko, 2014</marker>
<rawString>Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svetlana Kiritchenko. 2014a. An empirical study on the effect of negation words on sentiment. In Proceedings ofACL, Baltimore, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Svetlana Kiritchenko</author>
<author>Saif Mohammad</author>
</authors>
<title>Nrc-canada-2014: Recent improvements in the sentiment analysis of tweets.</title>
<date>2014</date>
<booktitle>In Proceedings of International Workshop on Semantic Evaluation,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="8201" citStr="Zhu et al., 2014" startWordPosition="1259" endWordPosition="1262">g only lexical resources to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehensive understanding, we constructed our study in a framework that examines a number of basic concerns in modeling contrasting meaning within embedding. Note that sentiment contrast may be viewed as a specific case of more general semantic contrast or semantic differentials (Osgood et al., 1957). Tang et al. (2014) learned sentiment-specific embedding and applied it to sentiment analysis of tweets, which was often solved with more conventional methods (Zhu et al., 2014b; Kiritchenko et al., 2014a; Kiritchenko et al., 2014b). 3 The Models We described in this section the framework in which we study word embedding for contrasting meaning. The general aim of the models is to enforce that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. Figure 1 describes a very high-level view of the framework. 107 Figure 1: A high-level view of the contrasting embedding framework. 3.1 Top Hidden Layer(s) It is widely recognized that contrasting words, e.g., go</context>
<context position="30293" citStr="Zhu et al., 2014" startWordPosition="4928" endWordPosition="4931">ned with the development set. It showed a performance worse than that of MCE+CRM when 50%–80% entries kept, while as discussed above, MCE+CRM combines the two parts with the nonlinear top layers. In general, using corpora statistics make the models more robust as OOV becomes more serious. It deserves to note that the use of corpora here is rather straightforward; more patterns may be learned from corpora to capture contrasting expressions as discussed in (Mohammad et al., 2013). Also, context such as negation may change contrasting meaning, e.g., sentiment contrast (Kiritchenko et al., 2014b; Zhu et al., 2014a), in a dramatic and complicated manner, which has been considered in learning sentiment contrast (Kiritchenko et al., 2014b). 6 Conclusions Contrasting meaning is a basic aspect of semantics. In this paper, we present a new state-of-theart result, a 92% F-score, on the GRE dataset created by (Mohammad et al., 2008), which is widely used as the benchmark for modeling lexical contrast. The result reported here outperforms the best reported in previous work (82%) by a large margin. Unlike what was suggested in most previous work, we show that this performance can be achieved without relying on </context>
</contexts>
<marker>Zhu, Kiritchenko, Mohammad, 2014</marker>
<rawString>Xiaodan Zhu, Svetlana Kiritchenko, and Saif Mohammad. 2014b. Nrc-canada-2014: Recent improvements in the sentiment analysis of tweets. In Proceedings of International Workshop on Semantic Evaluation, Dublin, Ireland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>