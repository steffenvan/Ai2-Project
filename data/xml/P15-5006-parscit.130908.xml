<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.234077">
<title confidence="0.992429">
Scalable Large-Margin Structured Learning:
Theory and Algorithms
</title>
<author confidence="0.998029">
Liang Huang Kai Zhao
</author>
<affiliation confidence="0.982232">
The City University of New York
</affiliation>
<email confidence="0.987568">
{liang.huang.sh, kzhao.hf, lemaoliu}@gmail.com
</email>
<sectionHeader confidence="0.996923" genericHeader="abstract">
1 Motivations
</sectionHeader>
<bodyText confidence="0.999823666666667">
Much of NLP tries to map structured input (sen-
tences) to some form of structured output (tag se-
quences, parse trees, semantic graphs, or trans-
lated/paraphrased/compressed sentences). Thus
structured prediction and its learning algorithm
are of central importance to us NLP researchers.
However, when applying machine learning to
structured domains, we often face scalability is-
sues for two reasons:
</bodyText>
<listItem confidence="0.939344416666666">
1. Even the fastest exact search algorithms for
most NLP problems (such as parsing and
translation) is too slow for repeated use on the
training data, but approximate search (such
as beam search) unfortunately breaks down
the nice theoretical properties (such as con-
vergence) of existing machine learning algo-
rithms.
2. Even with inexact search, the scale of the
training data in NLP still makes pure online
learning (such as perceptron and MIRA) too
slow on a single CPU.
</listItem>
<bodyText confidence="0.9998643">
This tutorial reviews recent advances that ad-
dress these two challenges. In particular, we will
cover principled machine learning methods that
are designed to work under vastly inexact search,
and parallelization algorithms that speed up learn-
ing on multiple CPUs. We will also extend struc-
tured learning to the latent variable setting, where
in many NLP applications such as translation and
semantic parsing the gold-standard derivation is
hidden.
</bodyText>
<sectionHeader confidence="0.862926" genericHeader="keywords">
2 Contents
</sectionHeader>
<listItem confidence="0.916054909090909">
1. Overview of Structured Learning
(a) key challenge 1: search efficiency
(b) key challenge 2: interactions between
search and learning
2. Structured Perceptron
(a) the basic algorithm
(b) convergence proof – a purely geometric
approach (updated in 2015)
(c) voted and averaged perceptrons, and ef-
ficient implementation tricks
(d) applications in tagging, parsing, etc.
(e) inseparability and generalization
bounds (new in 2015)
3. Structured Perceptron under Inexact Search
(a) convergence theory breaks under inex-
act search
(b) early update
(c) violation-fixing perceptron
(d) applications in tagging, parsing, etc.
—coffee break—
4. Large-Margin Structured Learning with La-
tent Variables
(a) examples: machine translation, seman-
tic parsing, transliteration
(b) separability condition and convergence
proof (updated in 2015)
(c) latent-variable perceptron under inexact
search
(d) applications in machine translation
5. Parallelizing Large-Margin Structured
Learning
(a) iterative parameter mixing (IPM)
(b) minibatch perceptron and MIRA
</listItem>
<page confidence="0.989052">
19
</page>
<note confidence="0.979993">
Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 19–20,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.989218" genericHeader="introduction">
3 Instructor Biographies
</sectionHeader>
<bodyText confidence="0.99949008">
Liang Huang is an Assistant Professor at the City
University of New York (CUNY). He received
his Ph.D. in 2008 from Penn and has worked
as a Research Scientist at Google and a Re-
search Assistant Professor at USC/ISI. His work
is mainly on the theoretical aspects (algorithms
and formalisms) of computational linguistics, as
well as theory and algorithms of structured learn-
ing. He has received a Best Paper Award at ACL
2008, several best paper nominations (ACL 2007,
EMNLP 2008, and ACL 2010), two Google Fac-
ulty Research Awards (2010 and 2013), and a Uni-
versity Graduate Teaching Prize at Penn (2005).
He has given three tutorials at COLING 2008,
NAACL 2009 and ACL 2014.
Kai Zhao is a Ph.D. candidate at the City Univer-
sity of New York (CUNY), working with Liang
Huang. He received his B.S. from the Univer-
sity of Science and Technology in China (USTC).
He has published on structured prediction, online
learning, machine translation, and parsing algo-
rithms. He was a summer intern with IBM TJ Wat-
son Research Center in 2013, Microsoft Research
Redmond in 2014, and Google Research NYC in
2015.
</bodyText>
<page confidence="0.991299">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.009728">
<title confidence="0.999866">Scalable Large-Margin Structured Theory and Algorithms</title>
<author confidence="0.9996">Liang Huang Kai Zhao</author>
<affiliation confidence="0.445577">The City University of New York</affiliation>
<abstract confidence="0.919901806451613">kzhao.hf, 1 Motivations Much of NLP tries to map structured input (sentences) to some form of structured output (tag sequences, parse trees, semantic graphs, or translated/paraphrased/compressed sentences). Thus structured prediction and its learning algorithm are of central importance to us NLP researchers. However, when applying machine learning to structured domains, we often face scalability issues for two reasons: 1. Even the fastest exact search algorithms for most NLP problems (such as parsing and translation) is too slow for repeated use on the training data, but approximate search (such as beam search) unfortunately breaks down the nice theoretical properties (such as convergence) of existing machine learning algorithms. 2. Even with inexact search, the scale of the training data in NLP still makes pure online learning (such as perceptron and MIRA) too slow on a single CPU. This tutorial reviews recent advances that address these two challenges. In particular, we will cover principled machine learning methods that are designed to work under vastly inexact search, and parallelization algorithms that speed up learning on multiple CPUs. We will also extend structured learning to the latent variable setting, where in many NLP applications such as translation and semantic parsing the gold-standard derivation is hidden. 2 Contents 1. Overview of Structured Learning (a) key challenge 1: search efficiency (b) key challenge 2: interactions between search and learning 2. Structured Perceptron (a) the basic algorithm (b) convergence proof – a purely geometric in 2015) (c) voted and averaged perceptrons, and efficient implementation tricks (d) applications in tagging, parsing, etc. (e) inseparability and generalization in 2015) 3. Structured Perceptron under Inexact Search (a) convergence theory breaks under inexact search (b) early update (c) violation-fixing perceptron (d) applications in tagging, parsing, etc. —coffee break— 4. Large-Margin Structured Learning with Latent Variables (a) examples: machine translation, semantic parsing, transliteration (b) separability condition and convergence in 2015) (c) latent-variable perceptron under inexact search (d) applications in machine translation 5. Parallelizing Large-Margin Structured Learning (a) iterative parameter mixing (IPM) (b) minibatch perceptron and MIRA 19 of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th pages China, July 26-31, 2015. Association for Computational Linguistics 3 Instructor Biographies Huang an Assistant Professor at the City University of New York (CUNY). He received his Ph.D. in 2008 from Penn and has worked as a Research Scientist at Google and a Research Assistant Professor at USC/ISI. His work is mainly on the theoretical aspects (algorithms and formalisms) of computational linguistics, as well as theory and algorithms of structured learning. He has received a Best Paper Award at ACL 2008, several best paper nominations (ACL 2007, EMNLP 2008, and ACL 2010), two Google Fac- Research Awards (2010 and 2013), and a University Graduate Teaching Prize at Penn (2005). He has given three tutorials at COLING 2008, NAACL 2009 and ACL 2014. Zhao a Ph.D. candidate at the City University of New York (CUNY), working with Liang Huang. He received his B.S. from the University of Science and Technology in China (USTC). He has published on structured prediction, online learning, machine translation, and parsing algorithms. He was a summer intern with IBM TJ Wat-</abstract>
<note confidence="0.817392666666667">son Research Center in 2013, Microsoft Research Redmond in 2014, and Google Research NYC in 2015.</note>
<intro confidence="0.507053">20</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>