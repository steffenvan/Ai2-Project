<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012837">
<title confidence="0.9957155">
Lexical Comparison Between Wikipedia and Twitter Corpora by Using
Word Embeddings
</title>
<author confidence="0.94419">
Luchen Tan1, Haotian Zhang1∗, Charles L.A. Clarke1, and Mark D. Smucker2
</author>
<affiliation confidence="0.67871">
1David R. Cheriton School of Computer Science, University of Waterloo, Canada
</affiliation>
<email confidence="0.955358">
{luchen.tan, haotian.zhang, claclark}@uwaterloo.ca
</email>
<affiliation confidence="0.931988">
2Department of Management Sciences, University of Waterloo, Canada
</affiliation>
<email confidence="0.995842">
mark.smucker@uwaterloo.ca
</email>
<sectionHeader confidence="0.993823" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755117647059">
Compared with carefully edited prose, the
language of social media is informal in the
extreme. The application of NLP tech-
niques in this context may require a better
understanding of word usage within social
media. In this paper, we compute a word
embedding for a corpus of tweets, compar-
ing it to a word embedding for Wikipedia.
After learning a transformation of one vec-
tor space to the other, and adjusting simi-
larity values according to term frequency,
we identify words whose usage differs
greatly between the two corpora. For any
given word, the set of words closest to it in
a particular embedding provides a charac-
terization for that word’s usage within the
corresponding corpora.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989132194444444">
Users of social media typically employ highly
informal language, including slang, acronyms,
typos, deliberate misspellings, and interjec-
tions (Han and Baldwin, 2011). This heavy use
of nonstandard language, as well as the overall
level of noise on social media, creates substantial
problems when applying standard NLP tools and
techniques (Eisenstein, 2013). For example, Kauf-
mann and Kalita (2010) apply machine translation
methods to convert tweets to standard English in
an attempt to ameliorate this problem. Similarly,
Baldwin et al. (2013) and Han et al. (2012) address
this problem by generating corrections for irregu-
larly spelled words in social media.
In this short paper, we continue this line of re-
search, applying word embedding to the problem
of translating between the informal English of so-
cial media, specifically Twitter, and the formal En-
glish of carefully edited texts, such as those found
∗Luchen Tan and Haotian Zhang contributed equally to
this work.
in Wikipedia. Starting with a large collection of
tweets and a copy of Wikipedia, we construct word
embeddings for both corpora. We then gener-
ate a transformation matrix, mapping one vector
space into another. After applying a normalization
based on term frequency, we use distances in the
transformed space as an indicator of differences in
word usage between the two corpora. The method
identifies differences in usage due to jargon, con-
tractions, abbreviations, hashtags, and the influ-
ence of popular culture, as well as other factors.
As a method of validation, we examine the over-
lap in closely related words, showing that distance
after transformation and normalization correlates
with the degree of overlap.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999921708333333">
Mikolov et al. (2013b) proposed a novel neural
network model to train continuous vector repre-
sentation for words. The high-quality word vec-
tors obtained from large data sets achieve high
accuracy in both semantic and syntactic relation-
ships (Goldberg and Levy, 2014).
Some probabilistic similarity measures, based
on Kullback-Leibler (KL) divergence (or relative
entropy), give an inspection of relative divergence
between two probability distributions of corpus
(Kullback and Leibler, 1951; Tan and Clarke,
2014). For a given token, KL divergence measures
the distribution divergence of this word in different
corpora according to its corresponding probability.
Intuitively, the value for KL divergence increases
as two distributions become more different. Ver-
spoor et al. (2009) found that KL divergence could
be applied to analyze text in terms of two charac-
teristics: the magnitude of the differences, and the
semantic nature of the characteristic words.
Subaˇsi´c and Berendt (2011) applied a sym-
metrical variant of KL divergence, the Jensen-
Shannon (JS) divergence (Lin, 1991), to compare
various aspects of the corpora such as language
</bodyText>
<page confidence="0.910736">
657
</page>
<bodyText confidence="0.904223857142857">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 657–661,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
divergence, headline divergence, named-entity di-
vergence and sentiment divergence. As for the ap-
plications derived from above methods, Tang et al.
(2011) studied the lexical semantics and sentiment
tendency of high frequency terms in each corpus
by comparing microblog texts with general arti-
cles. Baldwin et al. (2013) analyzed non-standard
language on social media in the aspects of lexi-
cal variants, acronyms, grammaticality and corpus
similarity. Their results revealed that social media
text is less grammatical than edited text.
</bodyText>
<sectionHeader confidence="0.869297" genericHeader="method">
3 Methods of Lexical Comparison
</sectionHeader>
<bodyText confidence="0.9997344375">
Mikolov et al. (2013a) construct vector spaces for
various languages, including English and Spanish,
finding that the relative positions of semantically
related words are preserved across languages. We
adapt this result to explore differences between
corpora written in a single language, specifically
to explore the contrast between the highly in-
formal language used in English-language social
media with the more formal language used in
Wikipedia. We assume that there exists a lin-
ear transformation relationship between the vec-
tors for the most frequent words from each cor-
pus. Working with these frequent terms, we learn
a linear projection matrix that maps source to tar-
get spaces. We hypothesize that usage of those
words appearing far apart after this transformation
differs substantially between the two corpora.
Let a E R1×d and b E R1×d be the corre-
sponding source and target word vector represen-
tation with dimension d. We construct a source
matrix A = [aT1 , aT2 , ..., aTc ]T and a target matrix
B = [bT1 , bT2 , ..., bTc ]T, composed of vector pairs
{ai, bi}ci=1, where c is the size of the vocabulary
common between the source and target corpora.
We order these vectors according to frequency in
the target corpus, so that ai and bi correspond to
the i-th most common word in the target corpus.
These vectors are used to learn a linear transfor-
mation matrix M E Rd×d. Once this transforma-
tion matrix M is obtained, we can transform any
ai to a0i = aiM in order to approximate bi. The
linear transformation can be depicted as:
</bodyText>
<equation confidence="0.797489666666667">
AM=B (1)
using stochastic gradient descent:
11 aiM − bi 112 (2)
</equation>
<bodyText confidence="0.941774272727273">
where we limit the training process to the top n
terms.
After the generation of M, we calculate a0i =
aiM for each word. For each ai where i &gt; n, we
determine the distance between a0i and bi:
Sim(a0i, bi), n &lt; i &lt; c. (3)
Let Z be the set of these words ordered by dis-
tance, so that zj is the word with the j-th greatest
distance between the corresponding a0 and b vec-
tors. For the experiments reported in this paper, we
used cosine distance to calculate this Sim metric.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9997435">
In this section, we describe the results of applying
our method to Twitter and Wikipedia.
</bodyText>
<subsectionHeader confidence="0.992874">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999883631578947">
The Wikipedia dataset for our experiments con-
sists of all English Wikipedia articles downloaded
from MediaWiki data dumps1. The Twitter dataset
was collected through the Twitter Streaming API
from November 2013 to March 2015. We re-
stricted the dataset to English-language tweets on
the basis of the language field contained in each
tweet. To obtain distributed word representation
for both corpora, we trained word vectors sep-
arately by applying the word2vec2 tool, a well-
known implementation of word embedding.
Before applying the tool, we cleaned Wikipedia
and Twitter corpora. The clean version of
Wikipedia retains only normally visible article
text on Wikipedia web pages. The Twitter clean
version removes HTML code, URLs, user men-
tions(@), the # symbol of hashtags, and all the
retweeted tweets. The sizes of document and vo-
cabulary in both corpora are listed in Table 1.
</bodyText>
<table confidence="0.928801833333333">
Corpora # Documents # Vocabulary
Wikipedia 3,776,418 7,267,802
Twitter 263,572,856 13,622,411
n
min
M i=1
</table>
<tableCaption confidence="0.99916">
Table 1: Corpora sizes
</tableCaption>
<bodyText confidence="0.812099">
Following the solution provided by (Mikolov et
al., 2013a), M can be approximately computed by
</bodyText>
<footnote confidence="0.999972">
1https://dumps.wikimedia.org/enwiki/20150304/
2https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.992675">
658
</page>
<bodyText confidence="0.99435225">
There are two major parameters that affect
word2vec training quality: the dimensionality of
word vectors, and the size of the surrounding
words window. We choose 300 for our word vec-
tor dimensionality, which is typical for training
large dataset with word2vec. We choose 10 words
for the window, since tweet sentence length is
9.2 + 6.4 (Baldwin et al., 2013).
</bodyText>
<subsectionHeader confidence="0.976937">
4.2 Visualization
</subsectionHeader>
<bodyText confidence="0.9967135">
In Figure 1, we visualize the vectors of some
most common English words by applying prin-
cipal component analysis (PCA) to the vector
spaces. The words “and”, “is”, “was” and “by”
have similar geometric arrangements in Wikipedia
and in Twitter, since these common words are not
key differentiators for these corpora. On the other
hand, the pronouns “I” and “you”, are heavily used
in Twitter but rarely used in Wikipedia. Despite
this difference in term frequency, after transfor-
mation, the vectors for these terms appear close
together.
</bodyText>
<figureCaption confidence="0.910792">
Figure 1: Word representations in Wikipedia,
Twitter and transformed vectors after mapping
from Wikipedia to Twitter.
</figureCaption>
<subsectionHeader confidence="0.855343">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999845846153846">
As our primary goal, we hope to demonstrate
that our transformation method reflects meaning-
ful lexical usage differences between Wikipedia
and Twitter. To train our space transformation ma-
trix, we used the top n = 1, 000 most frequent
words from the 505,121 words that appear in both
corpora. The transformation can be either from
Twitter to Wikipedia (T2W) or the opposite direc-
tion W2T. We observed that the two transforma-
tion matrices are not exactly the same, but they
produce similar results. Mikolov et al. (2013c)
suggest that a simple vector offset method based
on cosine distance was remarkably effective to
search both syntactic and semantic similar words.
They also report that cosine similarity preformed
well, given that the embedding vectors are all nor-
malized to unit norm.
Figure 2 illustrates how T2W word vectors are
similar to their original word vectors. For the
purpose of explaining Figure 2, we define new
notation as follows: Let T and W be the word
sets of Twitter and Wikipedia respectively, and let
C = T n W. Denote the document frequency of
a word t in the Twitter corpus as df(t). Sorting
the whole set C by df(t) in an ascending order,
we obtain a sequence S¯ = {c0, &apos; &apos; &apos; , cm−1}, where
ci E C; m = 505,121; and df(ci) &lt; df(cj),
Vi &lt; j. We partition the sequence S¯ into 506
buckets, with a bucket size b = 1000. Bi =
{ci*b, &apos; &apos; &apos; , c(i+1)*b−1} represents the i-th bucket.
We number the curves in Figure 2 from the top to
the bottom. The points on the i-th curve demon-
strates the cosine similarity of the (i − 1) * 100-th
word in each bucket. From this figure, it is appar-
ent that words with higher frequencies have higher
average cosine similarity than those words with
lower frequencies. Since our goal is to find words
with lower than average similar, we apply the me-
dian curve of Figure 2 to adjust word distances.
</bodyText>
<figureCaption confidence="0.99653">
Figure 2: T2W transformated similarity curves.
</figureCaption>
<bodyText confidence="0.998818">
Defining adjusted distance as Dadjusted(t) of
a given word t, we calculate the cosine distance
between t and the median point cmedian from its
corresponding bucket Bi.
</bodyText>
<equation confidence="0.9985">
Dadjusted(t) = Sim(cmedian) − Sim(t) (4)
</equation>
<bodyText confidence="0.998952666666667">
where the index of median point should be i * b +
b/2. A negative adjusted distance value means
the word is more similar than at least half of
</bodyText>
<figure confidence="0.996508421052632">
1.0
0.5
0.0
0.5
Highest
Top 10%
Top 20%
Top 30%
1.0
0 100 200 300 400 500
Twitter Frequency Rank (* 1000)
Top 40%
Median
Top 60%
Top 70%
Top 80%
Top 90%
Lowest
Twitter to Wikipedia Cosine Similarity
</figure>
<page confidence="0.992641">
659
</page>
<table confidence="0.995694625">
Word Twitter Most Similar Wikipedia Most Similar
bc because bcus bcuz cuz cos bce macedon hellenistic euthydemus ptolemaic
ill ll imma ima will youll unwell sick frail fated bedridden
cameron cam nash followmecam camerons callmecam gillies duncan mckay mitchell bryce
mentions unfollow reply respond strangerswelcomed offend mentions mentioned mentioning reference attested
miss misss love missss missssss imiss pageant pageants titlehoder titlehoders pageantopolis
yup yep yupp yeah yea yepp chevak yupik gwaii tlingit nunivak
taurus capricorn sagittarius pisces gemini scorpio poniatovii scorpio subcompact sagittarius chevette
</table>
<tableCaption confidence="0.993924">
Table 2: Characteristic Words in Twitter Corpora
</tableCaption>
<bodyText confidence="0.9983002">
words in its bucket. On the other hand, the words
that are less similar than at least half of words in
their buckets have positive adjusted distance val-
ues. The larger an adjusted distance, the less sim-
ilar the word is between the corpora.
</bodyText>
<subsectionHeader confidence="0.993162">
4.4 Examples
</subsectionHeader>
<bodyText confidence="0.999261333333334">
Table 2 provides some examples of common
words with large adjusted distance, suggesting that
their usage in the two corpora are quite differ-
ent. For each of these words, the example shows
the closest terms to that word in the two corpora.
In Twitter, “bc” is frequently an abbreviation for
“because”, while in Wikipedia “bc” is more com-
monly used as part of dates, e.g. 900 BC. Simi-
larly, in Twitter “ill” is often a misspelling of the
contraction “I’ll”, rather than a synonym for sick-
ness, as in Wikipedia. In Twitter, the most similar
words to “cameron” relate to a YouTube person-
ality, whereas in Wikipedia they relate to notable
Scotish persons. In Wikipedia, “miss” is related
to beauty pageants, while in Twitter it is related
to expressions of affection (“I misssss you”). The
other examples also have explanations related to
popular culture, jargon, slang, and other factors.
</bodyText>
<sectionHeader confidence="0.985298" genericHeader="evaluation">
5 Validation
</sectionHeader>
<bodyText confidence="0.99926876">
To validate our method of comparing lexical dis-
tinctions in the two corpora, we employ a ranking
similarity measurement. Within a single corpus,
the most similar words to a word t can be gen-
erated by ranking cosine distance to t. We then
determine the overlap between the most similar
words to t from Twitter and Wikipedia. The more
the two lists overlap, the greater the similarity be-
tween the words in the two corpora. Our hypoth-
esis is that larger rank similarity correlates with
smaller adjusted distance.
Rank biased overlap (RBO) provides a rank
similarity measure designed for comparisons be-
tween top-weighted, incomplete and indefinite
rankings. Given two ranked lists, A and B, let
A1:k and B1:k denote the top k items in A and
B (Webber et al., 2010). RBO defines the overlap
between A and B at depth k as the size of the inter-
section between these lists at depth k and defines
the agreement between A and B at depth k as the
overlap divided by the depth. Webber et al. (2010)
define RBO as a weighted average of agreement
across depths, where the weights decay geometri-
cally with depth, reflecting the requirement for top
weighting:
</bodyText>
<equation confidence="0.996669">
ϕk−1 |A1:k ∩ B1:k |(5)
k
</equation>
<bodyText confidence="0.999144">
Here, ϕ is a persistence parameter. As suggested
by Webber et al., we set ϕ = 0.9. In practice, RBO
is computed down to some fixed depth K. We se-
lect K = 50 for our experiments. For a word t,
we compute RBO value between its top 50 simi-
lar words in Wikipedia and top 50 similar words
in Twitter.
In Figure 3, we validate consistency between
results of our space transformation method and
RBO. For the top 5,000 terms in the Twitter cor-
pus, we sort them by their adjusted distance value.
Due to properties of RBO, there are many zero
RBO values. To illustrate the density of these zero
overlaps, we smooth our plot by sliding a 100-
word window with a step of 10 words. As shown
sharply in the figure, RBO and adjusted distance
is negatively correlated.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999968222222222">
This paper analyzed the lexical usage difference
between Twitter microblog corpus and Wikipedia
corpus. A word-level comparison method based
on word embedding is employed to find the char-
acterisic words that particularly discriminating
corpora. In future work, we plan to introduce this
method to normalize the nonstandard language
used in Twitter, applying the methods to problems
in search and other areas.
</bodyText>
<equation confidence="0.8077005">
RBO = (1 − ϕ) �∞
k=1
</equation>
<page confidence="0.816504">
660
</page>
<figure confidence="0.9967489">
Average RBO
Average RBO
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.000.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.5
Average T2W adjusted distance(Top 5000 in Twitter)
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.000.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4
Average W2T adjusted distance(Top 5000 in Twitter)
</figure>
<figureCaption confidence="0.999786">
Figure 3: T2W and W2T negative correlation between adjusted distance and RBO.
</figureCaption>
<sectionHeader confidence="0.9943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969443342857143">
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources. In Pro-
ceedings of the Sixth International Joint Conference
on Natural Language Processing, pages 356–364.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In HLT-NAACL, pages 359–
369.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 368–378.
Association for Computational Linguistics.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
joint conference on empirical methods in natural
language processing and computational natural lan-
guage learning, pages 421–432. Association for
Computational Linguistics.
Max Kaufmann and Jugal Kalita. 2010. Syntac-
tic normalization of twitter messages. In Interna-
tional conference on natural language processing,
Kharagpur, India.
Solomon Kullback and Richard A Leibler. 1951. On
information and sufficiency. The annals of mathe-
matical statistics, pages 79–86.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. Information Theory, IEEE Trans-
actions on, 37(1):145–151.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.
Ilija Subaˇsi´c and Bettina Berendt. 2011. Peddling or
creating? investigating the role of twitter in news
reporting. In Advances in Information Retrieval,
pages 207–213. Springer.
Luchen Tan and Charles L.A. Clarke. 2014. Suc-
cinct queries for linking and tracking news in so-
cial media. In Proceedings of the 23rd ACM In-
ternational Conference on Conference on Informa-
tion and Knowledge Management, CIKM ’14, pages
1883–1886, New York, NY, USA. ACM.
Yi-jie Tang, Chang-Ye Li, and Hsin-Hsi Chen. 2011.
A comparison between microblog corpus and bal-
anced corpus from linguistic and sentimental per-
spectives. In Analyzing Microtext.
Karin Verspoor, K Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of tradi-
tional and open access scientific journals are similar.
BMC bioinformatics, 10(1):183.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
</reference>
<page confidence="0.997745">
661
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.749160">
<title confidence="0.9949435">Lexical Comparison Between Wikipedia and Twitter Corpora by Word Embeddings</title>
<author confidence="0.956692">Haotian Charles L A</author>
<author confidence="0.956692">D Mark</author>
<affiliation confidence="0.95445">R. Cheriton School of Computer Science, University of Waterloo,</affiliation>
<email confidence="0.877782">haotian.zhang,</email>
<affiliation confidence="0.964702">of Management Sciences, University of Waterloo,</affiliation>
<email confidence="0.989988">mark.smucker@uwaterloo.ca</email>
<abstract confidence="0.9980885">Compared with carefully edited prose, the language of social media is informal in the extreme. The application of NLP techniques in this context may require a better understanding of word usage within social media. In this paper, we compute a word embedding for a corpus of tweets, comparing it to a word embedding for Wikipedia. After learning a transformation of one vector space to the other, and adjusting similarity values according to term frequency, we identify words whose usage differs greatly between the two corpora. For any given word, the set of words closest to it in a particular embedding provides a characterization for that word’s usage within the corresponding corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Paul Cook</author>
<author>Marco Lui</author>
<author>Andrew MacKinlay</author>
<author>Li Wang</author>
</authors>
<title>How noisy social media text, how diffrnt social media sources.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>356--364</pages>
<contexts>
<context position="1635" citStr="Baldwin et al. (2013)" startWordPosition="240" endWordPosition="243">tion for that word’s usage within the corresponding corpora. 1 Introduction Users of social media typically employ highly informal language, including slang, acronyms, typos, deliberate misspellings, and interjections (Han and Baldwin, 2011). This heavy use of nonstandard language, as well as the overall level of noise on social media, creates substantial problems when applying standard NLP tools and techniques (Eisenstein, 2013). For example, Kaufmann and Kalita (2010) apply machine translation methods to convert tweets to standard English in an attempt to ameliorate this problem. Similarly, Baldwin et al. (2013) and Han et al. (2012) address this problem by generating corrections for irregularly spelled words in social media. In this short paper, we continue this line of research, applying word embedding to the problem of translating between the informal English of social media, specifically Twitter, and the formal English of carefully edited texts, such as those found ∗Luchen Tan and Haotian Zhang contributed equally to this work. in Wikipedia. Starting with a large collection of tweets and a copy of Wikipedia, we construct word embeddings for both corpora. We then generate a transformation matrix, </context>
<context position="4535" citStr="Baldwin et al. (2013)" startWordPosition="683" endWordPosition="686">pora such as language 657 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 657–661, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics divergence, headline divergence, named-entity divergence and sentiment divergence. As for the applications derived from above methods, Tang et al. (2011) studied the lexical semantics and sentiment tendency of high frequency terms in each corpus by comparing microblog texts with general articles. Baldwin et al. (2013) analyzed non-standard language on social media in the aspects of lexical variants, acronyms, grammaticality and corpus similarity. Their results revealed that social media text is less grammatical than edited text. 3 Methods of Lexical Comparison Mikolov et al. (2013a) construct vector spaces for various languages, including English and Spanish, finding that the relative positions of semantically related words are preserved across languages. We adapt this result to explore differences between corpora written in a single language, specifically to explore the contrast between the highly informa</context>
<context position="8538" citStr="Baldwin et al., 2013" startWordPosition="1347" endWordPosition="1350">418 7,267,802 Twitter 263,572,856 13,622,411 n min M i=1 Table 1: Corpora sizes Following the solution provided by (Mikolov et al., 2013a), M can be approximately computed by 1https://dumps.wikimedia.org/enwiki/20150304/ 2https://code.google.com/p/word2vec/ 658 There are two major parameters that affect word2vec training quality: the dimensionality of word vectors, and the size of the surrounding words window. We choose 300 for our word vector dimensionality, which is typical for training large dataset with word2vec. We choose 10 words for the window, since tweet sentence length is 9.2 + 6.4 (Baldwin et al., 2013). 4.2 Visualization In Figure 1, we visualize the vectors of some most common English words by applying principal component analysis (PCA) to the vector spaces. The words “and”, “is”, “was” and “by” have similar geometric arrangements in Wikipedia and in Twitter, since these common words are not key differentiators for these corpora. On the other hand, the pronouns “I” and “you”, are heavily used in Twitter but rarely used in Wikipedia. Despite this difference in term frequency, after transformation, the vectors for these terms appear close together. Figure 1: Word representations in Wikipedia</context>
</contexts>
<marker>Baldwin, Cook, Lui, MacKinlay, Wang, 2013</marker>
<rawString>Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text, how diffrnt social media sources. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 356–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>359--369</pages>
<contexts>
<context position="1447" citStr="Eisenstein, 2013" startWordPosition="213" endWordPosition="214">m frequency, we identify words whose usage differs greatly between the two corpora. For any given word, the set of words closest to it in a particular embedding provides a characterization for that word’s usage within the corresponding corpora. 1 Introduction Users of social media typically employ highly informal language, including slang, acronyms, typos, deliberate misspellings, and interjections (Han and Baldwin, 2011). This heavy use of nonstandard language, as well as the overall level of noise on social media, creates substantial problems when applying standard NLP tools and techniques (Eisenstein, 2013). For example, Kaufmann and Kalita (2010) apply machine translation methods to convert tweets to standard English in an attempt to ameliorate this problem. Similarly, Baldwin et al. (2013) and Han et al. (2012) address this problem by generating corrections for irregularly spelled words in social media. In this short paper, we continue this line of research, applying word embedding to the problem of translating between the informal English of social media, specifically Twitter, and the formal English of carefully edited texts, such as those found ∗Luchen Tan and Haotian Zhang contributed equal</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. What to do about bad language on the internet. In HLT-NAACL, pages 359– 369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="3060" citStr="Goldberg and Levy, 2014" startWordPosition="469" endWordPosition="472">pora. The method identifies differences in usage due to jargon, contractions, abbreviations, hashtags, and the influence of popular culture, as well as other factors. As a method of validation, we examine the overlap in closely related words, showing that distance after transformation and normalization correlates with the degree of overlap. 2 Related Work Mikolov et al. (2013b) proposed a novel neural network model to train continuous vector representation for words. The high-quality word vectors obtained from large data sets achieve high accuracy in both semantic and syntactic relationships (Goldberg and Levy, 2014). Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus (Kullback and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora according to its corresponding probability. Intuitively, the value for KL divergence increases as two distributions become more different. Verspoor et al. (2009) found that KL divergence could be applied to analyze text in terms of two characteris</context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a# twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>368--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1255" citStr="Han and Baldwin, 2011" startWordPosition="182" endWordPosition="185"> embedding for a corpus of tweets, comparing it to a word embedding for Wikipedia. After learning a transformation of one vector space to the other, and adjusting similarity values according to term frequency, we identify words whose usage differs greatly between the two corpora. For any given word, the set of words closest to it in a particular embedding provides a characterization for that word’s usage within the corresponding corpora. 1 Introduction Users of social media typically employ highly informal language, including slang, acronyms, typos, deliberate misspellings, and interjections (Han and Baldwin, 2011). This heavy use of nonstandard language, as well as the overall level of noise on social media, creates substantial problems when applying standard NLP tools and techniques (Eisenstein, 2013). For example, Kaufmann and Kalita (2010) apply machine translation methods to convert tweets to standard English in an attempt to ameliorate this problem. Similarly, Baldwin et al. (2013) and Han et al. (2012) address this problem by generating corrections for irregularly spelled words in social media. In this short paper, we continue this line of research, applying word embedding to the problem of trans</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 368–378. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning,</booktitle>
<pages>421--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1657" citStr="Han et al. (2012)" startWordPosition="245" endWordPosition="248"> within the corresponding corpora. 1 Introduction Users of social media typically employ highly informal language, including slang, acronyms, typos, deliberate misspellings, and interjections (Han and Baldwin, 2011). This heavy use of nonstandard language, as well as the overall level of noise on social media, creates substantial problems when applying standard NLP tools and techniques (Eisenstein, 2013). For example, Kaufmann and Kalita (2010) apply machine translation methods to convert tweets to standard English in an attempt to ameliorate this problem. Similarly, Baldwin et al. (2013) and Han et al. (2012) address this problem by generating corrections for irregularly spelled words in social media. In this short paper, we continue this line of research, applying word embedding to the problem of translating between the informal English of social media, specifically Twitter, and the formal English of carefully edited texts, such as those found ∗Luchen Tan and Haotian Zhang contributed equally to this work. in Wikipedia. Starting with a large collection of tweets and a copy of Wikipedia, we construct word embeddings for both corpora. We then generate a transformation matrix, mapping one vector spa</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pages 421–432. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Kaufmann</author>
<author>Jugal Kalita</author>
</authors>
<title>Syntactic normalization of twitter messages.</title>
<date>2010</date>
<booktitle>In International conference on natural language processing,</booktitle>
<location>Kharagpur, India.</location>
<contexts>
<context position="1488" citStr="Kaufmann and Kalita (2010)" startWordPosition="217" endWordPosition="221">hose usage differs greatly between the two corpora. For any given word, the set of words closest to it in a particular embedding provides a characterization for that word’s usage within the corresponding corpora. 1 Introduction Users of social media typically employ highly informal language, including slang, acronyms, typos, deliberate misspellings, and interjections (Han and Baldwin, 2011). This heavy use of nonstandard language, as well as the overall level of noise on social media, creates substantial problems when applying standard NLP tools and techniques (Eisenstein, 2013). For example, Kaufmann and Kalita (2010) apply machine translation methods to convert tweets to standard English in an attempt to ameliorate this problem. Similarly, Baldwin et al. (2013) and Han et al. (2012) address this problem by generating corrections for irregularly spelled words in social media. In this short paper, we continue this line of research, applying word embedding to the problem of translating between the informal English of social media, specifically Twitter, and the formal English of carefully edited texts, such as those found ∗Luchen Tan and Haotian Zhang contributed equally to this work. in Wikipedia. Starting w</context>
</contexts>
<marker>Kaufmann, Kalita, 2010</marker>
<rawString>Max Kaufmann and Jugal Kalita. 2010. Syntactic normalization of twitter messages. In International conference on natural language processing, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard A Leibler</author>
</authors>
<title>On information and sufficiency. The annals of mathematical statistics,</title>
<date>1951</date>
<pages>79--86</pages>
<contexts>
<context position="3284" citStr="Kullback and Leibler, 1951" startWordPosition="497" endWordPosition="500">osely related words, showing that distance after transformation and normalization correlates with the degree of overlap. 2 Related Work Mikolov et al. (2013b) proposed a novel neural network model to train continuous vector representation for words. The high-quality word vectors obtained from large data sets achieve high accuracy in both semantic and syntactic relationships (Goldberg and Levy, 2014). Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus (Kullback and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora according to its corresponding probability. Intuitively, the value for KL divergence increases as two distributions become more different. Verspoor et al. (2009) found that KL divergence could be applied to analyze text in terms of two characteristics: the magnitude of the differences, and the semantic nature of the characteristic words. Subaˇsi´c and Berendt (2011) applied a symmetrical variant of KL divergence, the JensenShannon (JS) divergence (Lin, 1991), to comp</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the shannon entropy. Information Theory,</title>
<date>1991</date>
<journal>IEEE Transactions on,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="3875" citStr="Lin, 1991" startWordPosition="590" endWordPosition="591">k and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora according to its corresponding probability. Intuitively, the value for KL divergence increases as two distributions become more different. Verspoor et al. (2009) found that KL divergence could be applied to analyze text in terms of two characteristics: the magnitude of the differences, and the semantic nature of the characteristic words. Subaˇsi´c and Berendt (2011) applied a symmetrical variant of KL divergence, the JensenShannon (JS) divergence (Lin, 1991), to compare various aspects of the corpora such as language 657 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 657–661, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics divergence, headline divergence, named-entity divergence and sentiment divergence. As for the applications derived from above methods, Tang et al. (2011) studied the lexical semantics and sentiment tendency of high frequency terms in each corpus by comparing </context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the shannon entropy. Information Theory, IEEE Transactions on, 37(1):145–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="2814" citStr="Mikolov et al. (2013" startWordPosition="431" endWordPosition="434">e then generate a transformation matrix, mapping one vector space into another. After applying a normalization based on term frequency, we use distances in the transformed space as an indicator of differences in word usage between the two corpora. The method identifies differences in usage due to jargon, contractions, abbreviations, hashtags, and the influence of popular culture, as well as other factors. As a method of validation, we examine the overlap in closely related words, showing that distance after transformation and normalization correlates with the degree of overlap. 2 Related Work Mikolov et al. (2013b) proposed a novel neural network model to train continuous vector representation for words. The high-quality word vectors obtained from large data sets achieve high accuracy in both semantic and syntactic relationships (Goldberg and Levy, 2014). Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus (Kullback and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora a</context>
<context position="4803" citStr="Mikolov et al. (2013" startWordPosition="723" endWordPosition="726">on for Computational Linguistics divergence, headline divergence, named-entity divergence and sentiment divergence. As for the applications derived from above methods, Tang et al. (2011) studied the lexical semantics and sentiment tendency of high frequency terms in each corpus by comparing microblog texts with general articles. Baldwin et al. (2013) analyzed non-standard language on social media in the aspects of lexical variants, acronyms, grammaticality and corpus similarity. Their results revealed that social media text is less grammatical than edited text. 3 Methods of Lexical Comparison Mikolov et al. (2013a) construct vector spaces for various languages, including English and Spanish, finding that the relative positions of semantically related words are preserved across languages. We adapt this result to explore differences between corpora written in a single language, specifically to explore the contrast between the highly informal language used in English-language social media with the more formal language used in Wikipedia. We assume that there exists a linear transformation relationship between the vectors for the most frequent words from each corpus. Working with these frequent terms, we l</context>
<context position="8053" citStr="Mikolov et al., 2013" startWordPosition="1278" endWordPosition="1281">g the word2vec2 tool, a wellknown implementation of word embedding. Before applying the tool, we cleaned Wikipedia and Twitter corpora. The clean version of Wikipedia retains only normally visible article text on Wikipedia web pages. The Twitter clean version removes HTML code, URLs, user mentions(@), the # symbol of hashtags, and all the retweeted tweets. The sizes of document and vocabulary in both corpora are listed in Table 1. Corpora # Documents # Vocabulary Wikipedia 3,776,418 7,267,802 Twitter 263,572,856 13,622,411 n min M i=1 Table 1: Corpora sizes Following the solution provided by (Mikolov et al., 2013a), M can be approximately computed by 1https://dumps.wikimedia.org/enwiki/20150304/ 2https://code.google.com/p/word2vec/ 658 There are two major parameters that affect word2vec training quality: the dimensionality of word vectors, and the size of the surrounding words window. We choose 300 for our word vector dimensionality, which is typical for training large dataset with word2vec. We choose 10 words for the window, since tweet sentence length is 9.2 + 6.4 (Baldwin et al., 2013). 4.2 Visualization In Figure 1, we visualize the vectors of some most common English words by applying principal c</context>
<context position="9743" citStr="Mikolov et al. (2013" startWordPosition="1541" endWordPosition="1544">ns in Wikipedia, Twitter and transformed vectors after mapping from Wikipedia to Twitter. 4.3 Results As our primary goal, we hope to demonstrate that our transformation method reflects meaningful lexical usage differences between Wikipedia and Twitter. To train our space transformation matrix, we used the top n = 1, 000 most frequent words from the 505,121 words that appear in both corpora. The transformation can be either from Twitter to Wikipedia (T2W) or the opposite direction W2T. We observed that the two transformation matrices are not exactly the same, but they produce similar results. Mikolov et al. (2013c) suggest that a simple vector offset method based on cosine distance was remarkably effective to search both syntactic and semantic similar words. They also report that cosine similarity preformed well, given that the embedding vectors are all normalized to unit norm. Figure 2 illustrates how T2W word vectors are similar to their original word vectors. For the purpose of explaining Figure 2, we define new notation as follows: Let T and W be the word sets of Twitter and Wikipedia respectively, and let C = T n W. Denote the document frequency of a word t in the Twitter corpus as df(t). Sorting</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2814" citStr="Mikolov et al. (2013" startWordPosition="431" endWordPosition="434">e then generate a transformation matrix, mapping one vector space into another. After applying a normalization based on term frequency, we use distances in the transformed space as an indicator of differences in word usage between the two corpora. The method identifies differences in usage due to jargon, contractions, abbreviations, hashtags, and the influence of popular culture, as well as other factors. As a method of validation, we examine the overlap in closely related words, showing that distance after transformation and normalization correlates with the degree of overlap. 2 Related Work Mikolov et al. (2013b) proposed a novel neural network model to train continuous vector representation for words. The high-quality word vectors obtained from large data sets achieve high accuracy in both semantic and syntactic relationships (Goldberg and Levy, 2014). Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus (Kullback and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora a</context>
<context position="4803" citStr="Mikolov et al. (2013" startWordPosition="723" endWordPosition="726">on for Computational Linguistics divergence, headline divergence, named-entity divergence and sentiment divergence. As for the applications derived from above methods, Tang et al. (2011) studied the lexical semantics and sentiment tendency of high frequency terms in each corpus by comparing microblog texts with general articles. Baldwin et al. (2013) analyzed non-standard language on social media in the aspects of lexical variants, acronyms, grammaticality and corpus similarity. Their results revealed that social media text is less grammatical than edited text. 3 Methods of Lexical Comparison Mikolov et al. (2013a) construct vector spaces for various languages, including English and Spanish, finding that the relative positions of semantically related words are preserved across languages. We adapt this result to explore differences between corpora written in a single language, specifically to explore the contrast between the highly informal language used in English-language social media with the more formal language used in Wikipedia. We assume that there exists a linear transformation relationship between the vectors for the most frequent words from each corpus. Working with these frequent terms, we l</context>
<context position="8053" citStr="Mikolov et al., 2013" startWordPosition="1278" endWordPosition="1281">g the word2vec2 tool, a wellknown implementation of word embedding. Before applying the tool, we cleaned Wikipedia and Twitter corpora. The clean version of Wikipedia retains only normally visible article text on Wikipedia web pages. The Twitter clean version removes HTML code, URLs, user mentions(@), the # symbol of hashtags, and all the retweeted tweets. The sizes of document and vocabulary in both corpora are listed in Table 1. Corpora # Documents # Vocabulary Wikipedia 3,776,418 7,267,802 Twitter 263,572,856 13,622,411 n min M i=1 Table 1: Corpora sizes Following the solution provided by (Mikolov et al., 2013a), M can be approximately computed by 1https://dumps.wikimedia.org/enwiki/20150304/ 2https://code.google.com/p/word2vec/ 658 There are two major parameters that affect word2vec training quality: the dimensionality of word vectors, and the size of the surrounding words window. We choose 300 for our word vector dimensionality, which is typical for training large dataset with word2vec. We choose 10 words for the window, since tweet sentence length is 9.2 + 6.4 (Baldwin et al., 2013). 4.2 Visualization In Figure 1, we visualize the vectors of some most common English words by applying principal c</context>
<context position="9743" citStr="Mikolov et al. (2013" startWordPosition="1541" endWordPosition="1544">ns in Wikipedia, Twitter and transformed vectors after mapping from Wikipedia to Twitter. 4.3 Results As our primary goal, we hope to demonstrate that our transformation method reflects meaningful lexical usage differences between Wikipedia and Twitter. To train our space transformation matrix, we used the top n = 1, 000 most frequent words from the 505,121 words that appear in both corpora. The transformation can be either from Twitter to Wikipedia (T2W) or the opposite direction W2T. We observed that the two transformation matrices are not exactly the same, but they produce similar results. Mikolov et al. (2013c) suggest that a simple vector offset method based on cosine distance was remarkably effective to search both syntactic and semantic similar words. They also report that cosine similarity preformed well, given that the embedding vectors are all normalized to unit norm. Figure 2 illustrates how T2W word vectors are similar to their original word vectors. For the purpose of explaining Figure 2, we define new notation as follows: Let T and W be the word sets of Twitter and Wikipedia respectively, and let C = T n W. Denote the document frequency of a word t in the Twitter corpus as df(t). Sorting</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="2814" citStr="Mikolov et al. (2013" startWordPosition="431" endWordPosition="434">e then generate a transformation matrix, mapping one vector space into another. After applying a normalization based on term frequency, we use distances in the transformed space as an indicator of differences in word usage between the two corpora. The method identifies differences in usage due to jargon, contractions, abbreviations, hashtags, and the influence of popular culture, as well as other factors. As a method of validation, we examine the overlap in closely related words, showing that distance after transformation and normalization correlates with the degree of overlap. 2 Related Work Mikolov et al. (2013b) proposed a novel neural network model to train continuous vector representation for words. The high-quality word vectors obtained from large data sets achieve high accuracy in both semantic and syntactic relationships (Goldberg and Levy, 2014). Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus (Kullback and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora a</context>
<context position="4803" citStr="Mikolov et al. (2013" startWordPosition="723" endWordPosition="726">on for Computational Linguistics divergence, headline divergence, named-entity divergence and sentiment divergence. As for the applications derived from above methods, Tang et al. (2011) studied the lexical semantics and sentiment tendency of high frequency terms in each corpus by comparing microblog texts with general articles. Baldwin et al. (2013) analyzed non-standard language on social media in the aspects of lexical variants, acronyms, grammaticality and corpus similarity. Their results revealed that social media text is less grammatical than edited text. 3 Methods of Lexical Comparison Mikolov et al. (2013a) construct vector spaces for various languages, including English and Spanish, finding that the relative positions of semantically related words are preserved across languages. We adapt this result to explore differences between corpora written in a single language, specifically to explore the contrast between the highly informal language used in English-language social media with the more formal language used in Wikipedia. We assume that there exists a linear transformation relationship between the vectors for the most frequent words from each corpus. Working with these frequent terms, we l</context>
<context position="8053" citStr="Mikolov et al., 2013" startWordPosition="1278" endWordPosition="1281">g the word2vec2 tool, a wellknown implementation of word embedding. Before applying the tool, we cleaned Wikipedia and Twitter corpora. The clean version of Wikipedia retains only normally visible article text on Wikipedia web pages. The Twitter clean version removes HTML code, URLs, user mentions(@), the # symbol of hashtags, and all the retweeted tweets. The sizes of document and vocabulary in both corpora are listed in Table 1. Corpora # Documents # Vocabulary Wikipedia 3,776,418 7,267,802 Twitter 263,572,856 13,622,411 n min M i=1 Table 1: Corpora sizes Following the solution provided by (Mikolov et al., 2013a), M can be approximately computed by 1https://dumps.wikimedia.org/enwiki/20150304/ 2https://code.google.com/p/word2vec/ 658 There are two major parameters that affect word2vec training quality: the dimensionality of word vectors, and the size of the surrounding words window. We choose 300 for our word vector dimensionality, which is typical for training large dataset with word2vec. We choose 10 words for the window, since tweet sentence length is 9.2 + 6.4 (Baldwin et al., 2013). 4.2 Visualization In Figure 1, we visualize the vectors of some most common English words by applying principal c</context>
<context position="9743" citStr="Mikolov et al. (2013" startWordPosition="1541" endWordPosition="1544">ns in Wikipedia, Twitter and transformed vectors after mapping from Wikipedia to Twitter. 4.3 Results As our primary goal, we hope to demonstrate that our transformation method reflects meaningful lexical usage differences between Wikipedia and Twitter. To train our space transformation matrix, we used the top n = 1, 000 most frequent words from the 505,121 words that appear in both corpora. The transformation can be either from Twitter to Wikipedia (T2W) or the opposite direction W2T. We observed that the two transformation matrices are not exactly the same, but they produce similar results. Mikolov et al. (2013c) suggest that a simple vector offset method based on cosine distance was remarkably effective to search both syntactic and semantic similar words. They also report that cosine similarity preformed well, given that the embedding vectors are all normalized to unit norm. Figure 2 illustrates how T2W word vectors are similar to their original word vectors. For the purpose of explaining Figure 2, we define new notation as follows: Let T and W be the word sets of Twitter and Wikipedia respectively, and let C = T n W. Denote the document frequency of a word t in the Twitter corpus as df(t). Sorting</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilija Subaˇsi´c</author>
<author>Bettina Berendt</author>
</authors>
<title>Peddling or creating? investigating the role of twitter in news reporting.</title>
<date>2011</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>207--213</pages>
<publisher>Springer.</publisher>
<marker>Subaˇsi´c, Berendt, 2011</marker>
<rawString>Ilija Subaˇsi´c and Bettina Berendt. 2011. Peddling or creating? investigating the role of twitter in news reporting. In Advances in Information Retrieval, pages 207–213. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luchen Tan</author>
<author>Charles L A Clarke</author>
</authors>
<title>Succinct queries for linking and tracking news in social media.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM ’14,</booktitle>
<pages>1883--1886</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3307" citStr="Tan and Clarke, 2014" startWordPosition="501" endWordPosition="504"> that distance after transformation and normalization correlates with the degree of overlap. 2 Related Work Mikolov et al. (2013b) proposed a novel neural network model to train continuous vector representation for words. The high-quality word vectors obtained from large data sets achieve high accuracy in both semantic and syntactic relationships (Goldberg and Levy, 2014). Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus (Kullback and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora according to its corresponding probability. Intuitively, the value for KL divergence increases as two distributions become more different. Verspoor et al. (2009) found that KL divergence could be applied to analyze text in terms of two characteristics: the magnitude of the differences, and the semantic nature of the characteristic words. Subaˇsi´c and Berendt (2011) applied a symmetrical variant of KL divergence, the JensenShannon (JS) divergence (Lin, 1991), to compare various aspects of </context>
</contexts>
<marker>Tan, Clarke, 2014</marker>
<rawString>Luchen Tan and Charles L.A. Clarke. 2014. Succinct queries for linking and tracking news in social media. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM ’14, pages 1883–1886, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-jie Tang</author>
<author>Chang-Ye Li</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>A comparison between microblog corpus and balanced corpus from linguistic and sentimental perspectives. In Analyzing Microtext.</title>
<date>2011</date>
<contexts>
<context position="4369" citStr="Tang et al. (2011)" startWordPosition="657" endWordPosition="660">. Subaˇsi´c and Berendt (2011) applied a symmetrical variant of KL divergence, the JensenShannon (JS) divergence (Lin, 1991), to compare various aspects of the corpora such as language 657 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 657–661, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics divergence, headline divergence, named-entity divergence and sentiment divergence. As for the applications derived from above methods, Tang et al. (2011) studied the lexical semantics and sentiment tendency of high frequency terms in each corpus by comparing microblog texts with general articles. Baldwin et al. (2013) analyzed non-standard language on social media in the aspects of lexical variants, acronyms, grammaticality and corpus similarity. Their results revealed that social media text is less grammatical than edited text. 3 Methods of Lexical Comparison Mikolov et al. (2013a) construct vector spaces for various languages, including English and Spanish, finding that the relative positions of semantically related words are preserved acros</context>
</contexts>
<marker>Tang, Li, Chen, 2011</marker>
<rawString>Yi-jie Tang, Chang-Ye Li, and Hsin-Hsi Chen. 2011. A comparison between microblog corpus and balanced corpus from linguistic and sentimental perspectives. In Analyzing Microtext.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Verspoor</author>
<author>K Bretonnel Cohen</author>
<author>Lawrence Hunter</author>
</authors>
<title>The textual characteristics of traditional and open access scientific journals are similar.</title>
<date>2009</date>
<journal>BMC bioinformatics,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="3574" citStr="Verspoor et al. (2009)" startWordPosition="539" endWordPosition="543">large data sets achieve high accuracy in both semantic and syntactic relationships (Goldberg and Levy, 2014). Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus (Kullback and Leibler, 1951; Tan and Clarke, 2014). For a given token, KL divergence measures the distribution divergence of this word in different corpora according to its corresponding probability. Intuitively, the value for KL divergence increases as two distributions become more different. Verspoor et al. (2009) found that KL divergence could be applied to analyze text in terms of two characteristics: the magnitude of the differences, and the semantic nature of the characteristic words. Subaˇsi´c and Berendt (2011) applied a symmetrical variant of KL divergence, the JensenShannon (JS) divergence (Lin, 1991), to compare various aspects of the corpora such as language 657 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 657–661, Beijing, China, July 26-31, 2015. c�2015 </context>
</contexts>
<marker>Verspoor, Cohen, Hunter, 2009</marker>
<rawString>Karin Verspoor, K Bretonnel Cohen, and Lawrence Hunter. 2009. The textual characteristics of traditional and open access scientific journals are similar. BMC bioinformatics, 10(1):183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Webber</author>
<author>Alistair Moffat</author>
<author>Justin Zobel</author>
</authors>
<title>A similarity measure for indefinite rankings.</title>
<date>2010</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="14268" citStr="Webber et al., 2010" startWordPosition="2319" endWordPosition="2322">pus, the most similar words to a word t can be generated by ranking cosine distance to t. We then determine the overlap between the most similar words to t from Twitter and Wikipedia. The more the two lists overlap, the greater the similarity between the words in the two corpora. Our hypothesis is that larger rank similarity correlates with smaller adjusted distance. Rank biased overlap (RBO) provides a rank similarity measure designed for comparisons between top-weighted, incomplete and indefinite rankings. Given two ranked lists, A and B, let A1:k and B1:k denote the top k items in A and B (Webber et al., 2010). RBO defines the overlap between A and B at depth k as the size of the intersection between these lists at depth k and defines the agreement between A and B at depth k as the overlap divided by the depth. Webber et al. (2010) define RBO as a weighted average of agreement across depths, where the weights decay geometrically with depth, reflecting the requirement for top weighting: ϕk−1 |A1:k ∩ B1:k |(5) k Here, ϕ is a persistence parameter. As suggested by Webber et al., we set ϕ = 0.9. In practice, RBO is computed down to some fixed depth K. We select K = 50 for our experiments. For a word t,</context>
</contexts>
<marker>Webber, Moffat, Zobel, 2010</marker>
<rawString>William Webber, Alistair Moffat, and Justin Zobel. 2010. A similarity measure for indefinite rankings. ACM Transactions on Information Systems (TOIS), 28(4):20.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>