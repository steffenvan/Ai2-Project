<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.99926">
Japanese Dependency Structure Analysis
Based on Support Vector Machines
</title>
<author confidence="0.994022">
Taku Kudo and Yuji Matsumoto
</author>
<affiliation confidence="0.997866">
Graduate School of Information Science,
Nara Institute of Science and Technology
</affiliation>
<email confidence="0.997098">
Itaku-ku, matsul@is.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.997357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978333333333">
This paper presents a method of Japanese
dependency structure analysis based on Sup-
port Vector Machines (SVMs). Conventional
parsing techniques based on Machine Learn-
ing framework, such as Decision Trees and
Maximum Entropy Models, have difficulty
in selecting useful features as well as find-
ing appropriate combination of selected fea-
tures. On the other hand, it is well-known
that SVMs achieve high generalization per-
formance even with input data of very high
dimensional feature space. Furthermore, by
introducing the Kernel principle, SVMs can
carry out the training in high-dimensional
spaces with a smaller computational cost in-
dependent of their dimensionality. We apply
SVMs to Japanese dependency structure iden-
tification problem. Experimental results on
Kyoto University corpus show that our sys-
tem achieves the accuracy of 89.09% even with
small training data (7958 sentences).
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995270967742">
Dependency structure analysis has been rec-
ognized as a basic technique in Japanese
sentence analysis, and a number of stud-
ies have been proposed for years. Japanese
dependency structure is usually defined in
terms of the relationship between phrasal
units called &apos;bunsetsu&apos; segments (hereafter
&amp;quot;chunks&amp;quot;). Generally, dependency structure
analysis consists of two steps. In the first
step, dependency matrix is constructed, in
which each element corresponds to a pair of
chunks and represents the probability of a de-
pendency relation between them. The second
step is to find the optimal combination of de-
pendencies to form the entire sentence.
In previous approaches, these probabilites
of dependencies are given by manually con-
structed rules. However, rule-based ap-
proaches have problems in coverage and con-
sistency, since there are a number of features
that affect the accuracy of the final results,
and these features usually relate to one an-
other.
On the other hand, as large-scale tagged
corpora have become available these days,
a number of statistical parsing techniques
which estimate the dependency probabilities
using such tagged corpora have been devel-
oped(Collins, 1996; Fujio and Matsumoto,
1998). These approaches have overcome the
systems based on the rule-based approaches.
Decision Trees(Haruno et al., 1998) and Max-
imum Entropy models(Ratnaparkhi, 1997;
Uchimoto et al., 1999; Charniak, 2000) have
been applied to dependency or syntactic struc-
ture analysis. However, these models require
an appropriate feature selection in order to
achieve a high performance. In addition, ac-
quisition of an efficient combination of fea-
tures is difficult in these models.
In recent years, new statistical learning
techniques such as Support Vector Machines
(SVMs) (Cortes and Vapnik, 1995; Vap-
nik, 1998) and Boosting(Freund and Schapire,
1996) are proposed. These techniques take a
strategy that maximize the margin between
critical examples and the separating hyper-
plane. In particular, compared with other
conventional statistical learning algorithms,
SVMs achieve high generalization even with
training data of a very high dimension. Fur-
thermore, by optimizing the Kernel function,
SVMs can handle non-linear feature spaces,
and carry out the training with considering
combinations of more than one feature.
Thanks to such predominant nature, SVMs
deliver state-of-the-art performance in real-
world applications such as recognition of
hand-written letters, or of three dimensional
images. In the field of natural language pro-
cessing, SVMs are also applied to text cate-
gorization, and are reported to have achieved
</bodyText>
<page confidence="0.985434">
18
</page>
<equation confidence="0.7966686">
I w • xi + bl w. I • xi + 1)1
= min + mm
xi;yi =1 IINV II x1 ;y=-1 liwI
2
iiwii •
</equation>
<bodyText confidence="0.8792525">
To maximize this margin, we should minimize
In other words, this problem becomes
equivalent to solving the following optimiza-
tion problem:
</bodyText>
<equation confidence="0.701803">
Minimize: -L(w) = ilw112
Subject to: yi[(w - xi) + &gt; 1 (i = 1, , /).
</equation>
<bodyText confidence="0.93210975">
Furthermore, this optimization problem can
be rewritten into the dual form problem: Find
the Lagrange multipliers ai &gt; 0(i = 1, , /)
so that:
</bodyText>
<equation confidence="0.955874875">
Maximize:
aiajyiyi(xi • xj) (5)
2
i=1 Jo . .
,
Subject to:
ai ?_ 0, Eaiyi = 0 (i = 1,...,/)
/=1
</equation>
<bodyText confidence="0.948948684210526">
In this dual form problem, xi with non-zero ai
is called a Support Vector. For the Support
Vectors, w and b can thus be expressed as
follows
w E aiyi xi b = w • xi — yi.
/;xiEsvs
The elements of the set SVs are the Support
Vectors that lie on the separating hyperplanes.
Finally, the decision function f : —&gt; {±1}
can be written as:
high accuracy without falling into over-fitting
even with a large number of words taken as the
features (Joachims, 1998; Taira and Haruno,
1999).
In this paper, we propose an application
of SVMs to Japanese dependency structure
analysis. We use the features that have been
studied in conventional statistical dependency
analysis with a little modification on them.
</bodyText>
<sectionHeader confidence="0.560693" genericHeader="method">
2 Support Vector Machines
</sectionHeader>
<subsectionHeader confidence="0.930771">
2.1 Optimal Hyperplane
</subsectionHeader>
<bodyText confidence="0.999446">
Let us define the training data which belong
either to positive or negative class as follows.
</bodyText>
<equation confidence="0.822586">
Y1), , (xi, 1/i), (x/,Y/)
xi E Rn, y E {+1,-1}
</equation>
<bodyText confidence="0.478698">
xi is a feature vector of i-th sample, which is
represented by an n dimensional vector (xi =
(f1, fn) E Rn). yi is a scalar value that
specifies the class (positive(+1) or negative(-
1) class) of i-th data. Formally, we can define
the pattern recognition problem as a learning
and building process of the decision function
</bodyText>
<equation confidence="0.884661">
f : Rn {±1}.
</equation>
<bodyText confidence="0.9601395">
In basic SVMs framework, we try to sepa-
rate the positive and negative examples in the
training data by a linear hyperplane written
as:
(w-x)+b= 0 wERn,bert. (1)
It is -supposed that the farther the positive
and negative examples are separated by the
discrimination function, the more accurately
we could separate unseen test examples with
high generalization performance. Let us con-
sider two hyperplanes called separating hyper-
planes:
</bodyText>
<equation confidence="0.99637075">
(w xi) + b 1 if (yi = 1) (2)
(w xi) + b &lt; —1 if (yi = —1). (3) (x)
(2) (3) can be written in one formula as:
Rw xi) + bl 1 (i = 1, , /). (4)
</equation>
<bodyText confidence="0.978339">
Distance from the separating hyperplane to
the point xi can be written as:
</bodyText>
<equation confidence="0.9643404">
lw bl
d(w, b; xi) =
mm d(w, , b; xi) + min d(w, , b; xi)
x; y=1
iiwii •
Thus, the margin between two separating hy-
perplanes can be written as:
sgn E aiyi (xi - x) +1(6)
i;xi ESVs )
sgn(w -x+b).
</equation>
<subsectionHeader confidence="0.974926">
2.2 Soft Margin
</subsectionHeader>
<bodyText confidence="0.999564">
In the case where we cannot separate train-
ing examples linearly, &amp;quot;Soft Margin&amp;quot; method
forgives some classification errors that may be
caused by some noise in the training examples.
First, we introduce non-negative slack vari-
ables, and (2),(3) are rewritten as:
</bodyText>
<equation confidence="0.990033">
(w•xi)+b&gt; 1-6 if (yi = 1)
(w-xi)+b?-_-1“i if (yi = —1).
</equation>
<page confidence="0.966226">
19
</page>
<bodyText confidence="0.9555865">
In this case, we minimize the following value
instead of 111w112•
</bodyText>
<equation confidence="0.996419">
1
+ C eii (7)
</equation>
<bodyText confidence="0.999881090909091">
The first term in (7) specifies the size of mar-
gin and the second term evaluates how far the
training data are away from the optimal sep-
arating hyperpla,ne. C is the parameter that
defines the balance of two quantities. If we
make C larger, the more classification errors
are neglected.
Though we omit the details here, minimiza-
tion of (7) is reduced to the problem to mini-
mize the objective function (5) under the fol-
lowing constraints.
</bodyText>
<equation confidence="0.816165">
E aiyi = 0 (i = 1, , /)
</equation>
<bodyText confidence="0.999325">
Usually, the value of C is estimated experi-
mentally.
</bodyText>
<subsectionHeader confidence="0.999176">
2.3 Kernel Function
</subsectionHeader>
<bodyText confidence="0.990941071428571">
In general classification problems, there are
cases in which it is unable to separate the
training data linearly. In such cases, the train-
ing data could be separated linearly by ex-
panding all combinations of features as new
ones, and projecting them onto a higher-
dimensional space. However, such a naive ap-
proach requires enormous computational over-
head.
Let us consider the case where we project
the training data x onto a higher-dimensional
space by using projection function cio 1. As
we pay attention to the objective function (5)
and the decision function (6), these functions
depend only on the dot products of the in-
put training vectors. If we could calculate the
dot products from xi and x2 directly without
considering the vectors (I)(xi) and (I, (x2) pro-
jected onto the higher-dimensional space, we
can reduce the computational complexity con-
siderably. Namely, we can reduce the compu-
tational overhead if we could find the function
K that satisfies:
- 4)(x2) = K(xi, x2). (8)
On the other hand, since we do not need
itself for actual learning and classification,
&apos;In general, (1,(x) is a mapping into Hilbert space.
all we have to do is to prove the existence of
cl• that satisfies (8) provided the function K is
selected properly. It is known that (8) holds if
and only if the function K satisfies the Mercer
condition (Vapnik, 1998).
In this way, instead of projecting the train-
ing data onto the high-dimensional space, we
can decrease the computational overhead by
replacing the dot products, which is calculated
in optimization and classification steps, with
the function K.
Such a function K is called a Kernel func-
tion. Among the many kinds of Kernel func-
tions available, we will focus on the d-th poly-
nomial kernel:
</bodyText>
<equation confidence="0.976211">
K(xi, x2) = (x1 • x2 + 1)d. (9)
</equation>
<bodyText confidence="0.999729666666667">
Use of d-th polynomial kernel function allows
us to build an optimal separating hyperplane
which takes into account all combination of
features up to d.
Using a Kernel function, we can rewrite the
decision function as:
</bodyText>
<equation confidence="0.8326795">
y = sgn( E aiyiK(xi, x) + b . (10)
i;xi E SVs
</equation>
<sectionHeader confidence="0.9939065" genericHeader="method">
3 Dependency Analysis using
SVMs
</sectionHeader>
<subsectionHeader confidence="0.999555">
3.1 The Probability Model
</subsectionHeader>
<bodyText confidence="0.9719775">
This section describes a general formulation of
the probability model and parsing techniques
for Japanese statistical dependency analysis.
First of all, we let a sequence of
chunks be {b1, b2 , bni} by B, and
the sequence dependency pattern be
{Dep(1), Dep(2), , Dep(rn. — 1)} by D,
where Dep(i) = j means that the chunk bi
depends on (modifies) the chunk bi.
In this framework, we suppose that the de-
pendency sequence D satisfies the following
constraints.
</bodyText>
<listItem confidence="0.991631">
1. Except for the rightmost one, each chunk
depends on (modifies) exactly one of the
chunks appearing to the right.
2. Dependencies do not cross each other.
</listItem>
<bodyText confidence="0.999588">
Statistical dependency structure analysis
is defined as a searching problem for the
dependency pattern D that maximizes the
conditional probability P(D1./3) of the in-
</bodyText>
<page confidence="0.967331">
20
</page>
<bodyText confidence="0.997533">
put sequence under the above-mentioned con-
straints.
</bodyText>
<subsubsectionHeader confidence="0.429658">
Dbest --= argmax P(DIB)
</subsubsectionHeader>
<bodyText confidence="0.998762333333333">
If we assume that the dependency probabil-
ities are mutually independent, P(DIB) could
be rewritten as:
</bodyText>
<equation confidence="0.97790875">
m-i
P(DIB) =II P(Dep(i)= j I fij)
fii = Ifi,- • • , fn.} E R71.
P(Dep(i) = j I fij) represents the probability
</equation>
<bodyText confidence="0.997874916666667">
that bi depends on (modifies) bi. fij is an n di-
mensional feature vector that represents var-
ious kinds of linguistic features related with
the chunks bi and bj.
We obtain Db„t taking into all the combina-
tion of these probabilities. Generally, the op-
timal solution Db„t can be identified by using
bottom-up algorithm such as CYK algorithm.
Sekine suggests an efficient parsing technique
for Japanese sentences that parses from the
end of a sentence(Sekine et al., 2000). We ap-
ply Sekine&apos;s technique in our experiments.
</bodyText>
<subsectionHeader confidence="0.999944">
3.2 Training with SVMs
</subsectionHeader>
<bodyText confidence="0.9999399">
In order to use SVMs for dependency analysis,
we need to prepare positive and negative ex-
amples since SVMs is a binary classifier. We
adopt a simple and effective method for our
purpose: Out of all combination of two chunks
in the training data, we take a pair of chunks
that are in a dependency relation as a positive
example, and two chunks that appear in a sen-
tence but are not in a dependency relation as
a negative example.
</bodyText>
<equation confidence="0.972109111111111">
U (fii)Yii)= {(f127Y12), (f233 Y23))
1&lt;i&lt;m-1
i+1&lt;j&lt; m
• • • 7 ( frrt-1 M 7 Y772-1 Tri)
fii = fh, • • • , Rn
yii E {Depend(+1), Not-Depend(-1)}
Then, we define the dependency probability
P(Dep(i) =j I r ij):
P(Dep(i) =j If&apos; ij) =
</equation>
<bodyText confidence="0.888693333333333">
(tank E aklYkIK Via, flij) ± b (11)
k,t;fkiEsvs
(11) shows that the distance between test data
and the separating hyperplane is put into
the sigmoid function, assuming it represents
the probability value of the dependency rela-
tion.
We adopt this method in our experiment
to transform the distance measure obtained
in SVMs into a probability function and an-
alyze dependency structure with a framework
of conventional probability model 2.
</bodyText>
<subsectionHeader confidence="0.998669">
3.3 Static and Dynamic Features
</subsectionHeader>
<bodyText confidence="0.997818111111111">
Features that are supposed to be effective
in Japanese dependency analysis are: head
words and their parts-of-speech, particles and
inflection forms of the words that appear
at the end of chunks, distance between two
chunks, existence of punctuation marks. As
those are solely defined by the pair of chunks,
we refer to them as static features.
Japanese dependency relations are heavily
constrained by such static features since the
inflection forms and postpositional particles
constrain the dependency relation. However,
when a sentence is long and there are more
than one possible dependents, static features,
by themselves cannot determine the correct
dependency. Let us look at the following ex-
ample.
watashi-ha kono-hon-wo motteiru josei-wo sagasiteiru
I-top, this book-acc, have, lady-acc, be looking for
In this example, &amp;quot;kono-hon-wo(this book-
acc)&amp;quot; may modify either of &amp;quot;motteiru(have)&amp;quot;
or &amp;quot;sagasiteiru(be looking for)&amp;quot; and cannot
be determined only with the static features.
However, &amp;quot;josei-wo (lady-am)&amp;quot; can modify
the only the verb &amp;quot;sagasiteiru,&amp;quot;. Knowing
such information is quite useful for resolv-
ing syntactic ambiguity, since two accusative
noun phrses hardly modify the same verb. It
is possible to use such information if we add
new features related to other modifiers. In
the above case, the chunk &amp;quot;sagasiteiru&amp;quot; can
receive a new feature of accusative modifica-
tion (by &amp;quot;josei-wo&amp;quot;) during the parsing pro-
cess, which precludes the chunk &amp;quot;kono-hon-
wo&amp;quot; from modifying &amp;quot;sagasiteiru&amp;quot; since there
is a strict constraint about double-accusative
</bodyText>
<footnote confidence="0.560463">
2Experimentally, it is shown that the sigmoid func-
tion gives a good approximation of probability func-
tion from the decision function of SVMs(Platt, 1999).
</footnote>
<page confidence="0.997667">
21
</page>
<bodyText confidence="0.999464904761905">
modification that will be learned from train-
ing examples. We decided to take into consid-
eration all such modification information by
using functional words or inflection forms of
modifiers.
Using such information about modifiers in
the training phase has no difficulty since they
are clearly available in a tree-bank. On the
other hand, they are not known in the parsing
phase of the test data. This problem can be
easily solved if we adopt a bottom-up parsing
algorithm and attach the modification infor-
mation dynamically to the newly constructed
phrases (the chunks that become the head of
the phrases). As we describe later we apply a
beam search for parsing, and it is possible to
keep several intermediate solutions while sup-
pressing the combinatorial explosion.
We refer to the features that are added in-
crementally during the parsing process as dy-
namic features.
</bodyText>
<sectionHeader confidence="0.99521" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.685712">
4.1 Experiments Setting
</subsectionHeader>
<bodyText confidence="0.999872741935484">
We use Kyoto University text corpus (Ver-
sion 2.0) consisting of articles of Mainichi
newspaper annotated with dependency struc-
ture(Kurohashi and Nagao, 1997). 7,958 sen-
tences from the articles on January 1st to Jan-
uary 7th are used for the training data, and
1,246 sentences from the articles on January
9th are used for the test data. For the kernel
function, we used the polynomial function (9).
We set the soft margin parameter C to be 1.
The feature set used in the experiments are
shown in Table 1. The static features are ba-
sically taken from Uchimoto&apos;s list(Uchimoto
et al., 1999) with little modification. In Table
1, &apos;Head&apos; means the rightmost content word
in a chunk whose part-of-speech is not a func-
tional category. &apos;Type&apos; mewls the rightmost
functional word or the inflectional form of the
rightmost predicate if there is no functional
word in the chunk. The static features in-
clude the information on existence of brack-
ets, question marks and punctuation marks
etc. Besides, there are features that show
the relative relation of two chunks, such as
distance, and existence of brackets, quotation
marks and punctuation marks between them.
For dynamic features, we selected func-
tional words or inflection forms of the right-
most predicates in the chunks that appear be-
tween two chunks and depend on the modi-
fiee. Considering data sparseness problem, we
</bodyText>
<table confidence="0.999671304347826">
Static Left/ Head
Features Right (surface-form, POS,
Chunks POS-subcategory,
inflection-type,
inflection-form),
Type
(surface-form, POS,
POS-subcategory,
inflection-type,
inflection-form),
brackets,
quotation-marks,
punctuation-marks,
position in sentence
(beginning, end)
Between distance(1,2-5,6-),
Chunks case-particles,
brackets,
quotation-marks,
punctuation-marks
Dynamic Form of functional words or in-
Features flection that modifies the right
chunk
</table>
<tableCaption confidence="0.999635">
Table 1: Features used in experiments
</tableCaption>
<bodyText confidence="0.999684166666667">
apply a simple filtering based on the part-of-
speech of functional words: We use the lexical
form if the word&apos;s POS is particle, adverb, ad-
nominal or conjunction. We use the inflection
form if the word has inflection. We use the
POS tags for others.
</bodyText>
<subsectionHeader confidence="0.977421">
4.2 Results of Experiments
</subsectionHeader>
<bodyText confidence="0.99992275">
Table 2 shows the result of passing accuracy
under the condition k = 5 (beam width), and
d = 3 (dimension of the polynomial functions
used for the kernel function).
This table shows two types of dependency
accuracy, A and B. The training data size is
measured by the number of sentences. The ac-
curacy A means the accuracy of the entire de-
pendency relations. Since Japanese is a head-
final language, the second chunk from the end
of a sentence always modifies the last chunk.
The accuracy B is calculated by excluding this
dependency relation. Hereafter, we use the ac-
curacy A, if it is not explicitly specified, since
this measure is usually used in other litera-
ture.
</bodyText>
<subsectionHeader confidence="0.997087">
4.3 Effects of Dynamic Features
</subsectionHeader>
<bodyText confidence="0.974855">
Table3 shows the accuracy when only static
features are used. Generally, the results with
</bodyText>
<page confidence="0.995644">
22
</page>
<table confidence="0.9998523">
Training Dependency Accuracy Sentence
data Accuracy
A B
1172 86.52% 84.86% 39.31%
1917 87.21% 85.62% 40.06%
3032 87.67% 86.14% 42.94%
4318 88.35% 86.91% 44.15%
5540 88.66% 87.26% 45.20%
6756 88.77% 87.38% 45.36%
7958 89.09% 87.74% 46.17%
</table>
<tableCaption confidence="0.945495">
Table 2: Result (d = 3, k = 5)
</tableCaption>
<table confidence="0.9999419">
Training Dependency Accuracy Sentence
data Accuracy
A B
1172 86.12% 84.41% 38.50%
1917 86.81% 85.18% 39.80%
3032 87.62% 86.10% 42.45%
4318 88.33% 86.89% 44.47%
5540 88.40% 86.96% 43.66%
6756 88.55% 87.13% 45.04%
7958 88.77% 87.38% 45.04%
</table>
<tableCaption confidence="0.999914">
Table 3: Result without dynamic features
</tableCaption>
<equation confidence="0.648218">
(d = 3, k = 5)
</equation>
<bodyText confidence="0.999914">
dynamic feature set is better than the results
without them. The results with dynamic fea-
tures constantly outperform that with static
features only. In most of cases, the improve-
ments is significant. In the experiments, we
restrict the features only from the chunks that
appear between two chunks being in consider-
ation, however, dynamic features could be also
taken from the chunks that appear not be-
tween the two chunks. For example, we could
also take into consideration the chunk that is
modified by the right chunk, or the chunks
</bodyText>
<footnote confidence="0.786302">
3X0 4000 5000 5000 7000 WOO
Nt.,0lT40orç Data (sonlenees)
</footnote>
<figureCaption confidence="0.997612">
Figure 1: Training Data vs Accuracy
</figureCaption>
<table confidence="0.999849833333333">
Dimension Dependency Sentence
of Kernel Accuracy Accuracy
1 N/A N/A
2 86.87% 40.60%
3 87.67% 42.94%
4 87.72% 42.78%
</table>
<tableCaption confidence="0.7448425">
Table 4: Dimension vs. Accuracy (3032 sen-
tences, k = 5)
</tableCaption>
<bodyText confidence="0.99403">
that modify the left chunk. We leave experi-
ment in such a setting for the future work.
</bodyText>
<subsectionHeader confidence="0.9993">
4.4 Training data vs. Accuracy
</subsectionHeader>
<bodyText confidence="0.999965272727273">
Figure 1 shows the relationship between the
size of the training data and the parsing accu-
racy. This figure shows the accuracy of with
and without the dynamic features.
The parser achieves 86.52% accuracy for
test data even with small training data (1172
sentences). This is due to a good character-
istic of SVMs to cope with the data sparse-
ness problem. Furthermore, it achieves almost
100% accuracy for the training data, showing
that the training data are completely sepa-
rated by appropriate combination of features.
Generally, selecting those specific features of
the training data tends to cause overfitting,
and accuracy for test data may fall. However,
the SVMs method achieve a high accuracy not
only on the training data but also on the test
data. We claim that this is due to the high
generalization ability of SVMs. In addition,
observing at the learning curve, further im-
provement will be possible if we increase the
size of the training data.
</bodyText>
<subsectionHeader confidence="0.995199">
4.5 Kernel Function vs. Accuracy
</subsectionHeader>
<bodyText confidence="0.9999089375">
Table 4 shows the relationship between the di-
mension of the kernel function and the parsing
accuracy under the condition k = 5.
As a result, the case of d = 4 gives the best
accuracy. We could not carry out the training
in realistic time for the case of d = 1.
This result supports our intuition that we
need a combination of at least two features.
In other words, it will be hard to confirm a
dependency relation with only the features of
the modifier or the modfiee. It is natural that
a dependency relation is decided by at least
the information from both of two chunks. In
addition, further improvement has been pos-
sible by considering combinations of three or
more features.
</bodyText>
<page confidence="0.754162">
23 L
</page>
<table confidence="0.999625625">
Beam Dependency Sentence
Width Accuracy Accuracy
1 88.66% 45.76%
3 88.74% 45.20%
5 88.77% 45.36%
7 88.76% 45.36%
10 88.67% 45.28%
15 88.65% 45.28%
</table>
<tableCaption confidence="0.86479">
Table 5: Beam width vs. Accuracy (6756 sen-
tences, d = 3)
</tableCaption>
<subsectionHeader confidence="0.985781">
4.6 Beam width vs. Accuracy
</subsectionHeader>
<bodyText confidence="0.999980178571429">
Sekine (Sekine et al., 2000) gives an interest-
ing report about the relationship between the
beam width and the parsing accuracy. Gener-
ally, high parsing accuracy is expected when
a large beam width is employed in the depen-
dency structure analysis. However, the result
is against our intuition. They report that a
beam width between 3 and 10 gives the best
parsing accuracy, and parsing accuracy falls
down with a width larger than 10. This result
suggests that Japanese dependency structures
may consist of a series of local optimization
processes.
We evaluate the relationship between the
beam width and the parsing accuracy. Table 5
shows their relationships under the condition
d = 3, along with the changes of the beam
width from k = 1 to 15. The best parsing
accuracy is achieved at k = 5 and the best
sentence accuracy is achieved at k = 5 and
k = 7.
We have to consider how we should set the
beam width that gives the best parsing accu-
racy. We believe that the beam width that
gives the best passing accuracy is related not
only with the length of the sentence, but also
with the lexical entries and parts-of-speech
that comprise the chunks.
</bodyText>
<subsectionHeader confidence="0.617394">
4.17 Committee based approach
</subsectionHeader>
<bodyText confidence="0.999979">
Instead of learning a single classifier using all
training data, we can make n classifiers di-
viding all training data by n, and the final
result is decided by their voting. This ap-
proach would reduce computational overhead.
The use of multi-processing computer would
help to reduce their training time considerably
since all individual training can be carried out
in parallel.
To investigate the effectiveness of this
method, we perform a simple experiment: Di-
viding all training data (7958 sentences) by
4, the final dependency score is given by a
weighted average of each scores. This simple
voting approach is shown to achieve the ac-
curacy of 88.66%, which is nearly the same
accuracy achieved 5540 training sentences.
In this experiment, we simply give an equal
weight to each classifier. However, if we op-
timized the voting weight more carefully, the
further improvements would be achieved (Inui
and Inui, 2000).
</bodyText>
<subsectionHeader confidence="0.997603">
4.8 Comparison with Related Work
</subsectionHeader>
<bodyText confidence="0.999954935483871">
Uchimoto (Uchimoto et al., 1999) and Sekine
(Sekine et al., 2000) report that using Kyoto
University Corpus for their training and test-
ing, they achieve around 87.2% accuracy by
building statistical model based on Maximum
Entropy framework. For the training data, we
used exactly the same data that they used in
order to make a fair comparison. In our ex-
periments, the accuracy of 89.09% is achieved
using same training data. Our model outper-
forms Uchimoto&apos;s model as far as the accura-
cies are compared.
Although Uchimoto suggests that the im-
portance of considering combination of fea-
tures, in ME framework we must expand
these combination by introducing new fea-
ture set. Uchimoto heuristically selects &amp;quot;effec-
tive&amp;quot; combination of features. However, such
a manual selection does not always cover all
relevant combinations that are important in
the determination of dependency relation.
We believe that our model is better than
others from the viewpoints of coverage and
consistency, since our model learns the combi-
nation of features without increasing the com-
putational complexity. If we want to recon-
sider them, all we have to do is just to change
the Kernel function. The computational com-
plexity depends on the number of support vec-
tors not on the dimension of the Kernel func-
tion.
</bodyText>
<subsectionHeader confidence="0.671739">
4.9 Future Work
</subsectionHeader>
<bodyText confidence="0.999990625">
The simplest and most effective way to achieve
better accuracy is to increase the training
data. However, the proposed method that
uses all candidates that form dependency re-
lation requires a great amount of time to com-
pute the separating hyperplane as the size of
the training data increases. The experiments
given in this paper have actually taken long
</bodyText>
<page confidence="0.995437">
24
</page>
<bodyText confidence="0.999547636363636">
training time 3.
To handle large size of training data, we
have to select only the related portion of ex-
amples that are effective for the analysis. This
will reduce the training overhead as well as
the analysis time. The committee-based ap-
proach discussed section 4.7 is one method of
coping with this problem. For future research,
to reduce the computational overhead, we will
work on methods for sample selection as fol-
lows:
</bodyText>
<listItem confidence="0.977548">
• Introduction of constraints on non-
dependency
</listItem>
<bodyText confidence="0.9949095">
Some pairs of chunks need not consider
since there is no possibility of depen-
dency between them from grammatical
constraints. Such pairs of chunks are not
necessary to use as negative examples in
the training phase. For example, a chunk
within quotation marks may not modify
a chunk that locates outside of the quo-
tation marks. Of course, we have to be
careful in introducing such constraints,
and they should be learned from existing
corpus.
• Integration with other simple models
Suppose that a computationally light and
moderately accuracy learning model is
obtainable (there are actually such sys-
tems based on probabilistic parsing mod-
els). We can use the system to output
some redundant parsing results and use
only those results for the positive and
negative examples. This is another way
to reduce the size of training data.
</bodyText>
<listItem confidence="0.942875">
• Error-driven data selection
</listItem>
<bodyText confidence="0.999922625">
We can start with a small size of train-
ing data with a small size of feature
set. Then, by analyzing held-out training
data and selecting the features that affect
the passing accuracy. This kind of grad-
ual increase of training data and feature
set will be another method for reducing
the computational overhead.
</bodyText>
<sectionHeader confidence="0.997793" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.975473923076923">
This paper proposes Japanese dependency
analysis based on Support Vector Machines.
Through the experiments with Japanese
bracketed corpus, the proposed method
achieves a high accuracy even with a small
3With AlphaServer 8400 (617Mhz), it took 15 days
to train with 7958 sentences.
training data and outperforms existing meth-
ods based on Maximum Entropy Models. The
result shows that Japanese dependency anal-
ysis can be effectively performed by use of
SVMs due to its good generalization and non-
overfitting characteristics.
</bodyText>
<sectionHeader confidence="0.991017" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992201923077">
Eugene Charniak. 2000. A maximum-entropy-
inspired passer. In Processing of the NAACL
2000, pages 132-139.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Pro-
ceedings of the ACL &apos;96, pages 184-191.
C. Cortes and Vladimir N. Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273-
297.
Y. Freund and Schapire. 1996. Experiments with
a new Boosting algoritm. In 13th International
Conference on Machine Learning.
Masakazu Fujio and Yuji Matsumoto. 1998.
Japanese Dependency Structure Analysis based
on Lexicalized Statistics. In Proceedings of
EMNLP &apos;98, pages 87-96.
Msahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1998. Using Decision Trees to Con-
struct a Partial Parser. In Proceedings of the
COLING &apos;98, pages 505-511.
Takashi Inui and Kentaro Inui. 2000. Committe-
based Decision Making in Probabilistic Partial
Parsing. In Proceedings of the COLING 2000,
pages 348-354.
Thorsten Joachims. 1998. Text Categorization
with Support Vector Machines: Learning with
Many Relevant Features. In European Confer-
ence on Machine Learning (ECML).
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto
University text corpus project. In Proceedings
of the ANLP, Japan, pages 115-118.
John C. Platt. 1999. Probabilistic Outputs for
Support Vector Machines and Comparisons to
Regularized Likelihood Methods. In Advances
in Large Margin Classifiers. MIT Press.
Adwait Ratnaparkhi. 1997. A Liner Observed
Time Statistical Parser Based on Maximum En-
tropy Models. In Proceedings of EMNLP &apos;97.
Satoshi Seldne, Kiyotaka Uchimoto, and Hitoshi
Isahara. 2000. Backward Beam Search Algo-
rithm for Dependency Analysis of Japanese. In
Proceedings of the COLING 2000, pages 754-
760.
Hirotoshi Taira and Masahiko Haruno. 1999. Fea-
ture Selection in SVM Text Categorization. In
AAAI-99.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 1999. Japanese Dependency Structure
Analysis Based on Maximum Entropy Models.
In Proceedings of the EACL, pages 196-203.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. Wiley-Interscience.
</reference>
<page confidence="0.998735">
25
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955591">
<title confidence="0.998067">Japanese Dependency Structure Based on Support Vector Machines</title>
<author confidence="0.997744">Taku Kudo</author>
<author confidence="0.997744">Yuji</author>
<affiliation confidence="0.9992735">Graduate School of Information Nara Institute of Science and</affiliation>
<abstract confidence="0.998154090909091">This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs). Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space. Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality. We apply SVMs to Japanese dependency structure identification problem. Experimental results on Kyoto University corpus show that our sysachieves the 89.09% even with small training data (7958 sentences).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired passer.</title>
<date>2000</date>
<booktitle>In Processing of the NAACL</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2539" citStr="Charniak, 2000" startWordPosition="374" endWordPosition="375">erage and consistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane. In particular, compared with other </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired passer. In Processing of the NAACL 2000, pages 132-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL &apos;96,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="2310" citStr="Collins, 1996" startWordPosition="341" endWordPosition="343">o find the optimal combination of dependencies to form the entire sentence. In previous approaches, these probabilites of dependencies are given by manually constructed rules. However, rule-based approaches have problems in coverage and consistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vap</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the ACL &apos;96, pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>Support Vector Networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="2919" citStr="Cortes and Vapnik, 1995" startWordPosition="430" endWordPosition="433">(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane. In particular, compared with other conventional statistical learning algorithms, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by optimizing the Kernel function, SVMs can handle non-linear feature spaces, and carry out the training with considering combinations of more than one feature. Thanks to such predominant nature, SVMs deliver state-of-the-art performance </context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and Vladimir N. Vapnik. 1995. Support Vector Networks. Machine Learning, 20:273-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>Schapire</author>
</authors>
<title>Experiments with a new Boosting algoritm.</title>
<date>1996</date>
<booktitle>In 13th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2974" citStr="Freund and Schapire, 1996" startWordPosition="438" endWordPosition="441">proaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane. In particular, compared with other conventional statistical learning algorithms, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by optimizing the Kernel function, SVMs can handle non-linear feature spaces, and carry out the training with considering combinations of more than one feature. Thanks to such predominant nature, SVMs deliver state-of-the-art performance in realworld applications such as recognition of hand-w</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and Schapire. 1996. Experiments with a new Boosting algoritm. In 13th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Fujio</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Structure Analysis based on Lexicalized Statistics.</title>
<date>1998</date>
<booktitle>In Proceedings of EMNLP &apos;98,</booktitle>
<pages>87--96</pages>
<contexts>
<context position="2338" citStr="Fujio and Matsumoto, 1998" startWordPosition="344" endWordPosition="347">mal combination of dependencies to form the entire sentence. In previous approaches, these probabilites of dependencies are given by manually constructed rules. However, rule-based approaches have problems in coverage and consistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and</context>
</contexts>
<marker>Fujio, Matsumoto, 1998</marker>
<rawString>Masakazu Fujio and Yuji Matsumoto. 1998. Japanese Dependency Structure Analysis based on Lexicalized Statistics. In Proceedings of EMNLP &apos;98, pages 87-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Msahiko Haruno</author>
<author>Satoshi Shirai</author>
<author>Yoshifumi Ooyama</author>
</authors>
<title>Using Decision Trees to Construct a Partial Parser.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING &apos;98,</booktitle>
<pages>505--511</pages>
<contexts>
<context position="2454" citStr="Haruno et al., 1998" startWordPosition="360" endWordPosition="363">e given by manually constructed rules. However, rule-based approaches have problems in coverage and consistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between</context>
</contexts>
<marker>Haruno, Shirai, Ooyama, 1998</marker>
<rawString>Msahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama. 1998. Using Decision Trees to Construct a Partial Parser. In Proceedings of the COLING &apos;98, pages 505-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Inui</author>
<author>Kentaro Inui</author>
</authors>
<title>Committebased Decision Making in Probabilistic Partial Parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING</booktitle>
<pages>348--354</pages>
<contexts>
<context position="23171" citStr="Inui and Inui, 2000" startWordPosition="3872" endWordPosition="3875"> time considerably since all individual training can be carried out in parallel. To investigate the effectiveness of this method, we perform a simple experiment: Dividing all training data (7958 sentences) by 4, the final dependency score is given by a weighted average of each scores. This simple voting approach is shown to achieve the accuracy of 88.66%, which is nearly the same accuracy achieved 5540 training sentences. In this experiment, we simply give an equal weight to each classifier. However, if we optimized the voting weight more carefully, the further improvements would be achieved (Inui and Inui, 2000). 4.8 Comparison with Related Work Uchimoto (Uchimoto et al., 1999) and Sekine (Sekine et al., 2000) report that using Kyoto University Corpus for their training and testing, they achieve around 87.2% accuracy by building statistical model based on Maximum Entropy framework. For the training data, we used exactly the same data that they used in order to make a fair comparison. In our experiments, the accuracy of 89.09% is achieved using same training data. Our model outperforms Uchimoto&apos;s model as far as the accuracies are compared. Although Uchimoto suggests that the importance of considering</context>
</contexts>
<marker>Inui, Inui, 2000</marker>
<rawString>Takashi Inui and Kentaro Inui. 2000. Committebased Decision Making in Probabilistic Partial Parsing. In Proceedings of the COLING 2000, pages 348-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="4730" citStr="Joachims, 1998" startWordPosition="752" endWordPosition="753">roblem: Find the Lagrange multipliers ai &gt; 0(i = 1, , /) so that: Maximize: aiajyiyi(xi • xj) (5) 2 i=1 Jo . . , Subject to: ai ?_ 0, Eaiyi = 0 (i = 1,...,/) /=1 In this dual form problem, xi with non-zero ai is called a Support Vector. For the Support Vectors, w and b can thus be expressed as follows w E aiyi xi b = w • xi — yi. /;xiEsvs The elements of the set SVs are the Support Vectors that lie on the separating hyperplanes. Finally, the decision function f : —&gt; {±1} can be written as: high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999). In this paper, we propose an application of SVMs to Japanese dependency structure analysis. We use the features that have been studied in conventional statistical dependency analysis with a little modification on them. 2 Support Vector Machines 2.1 Optimal Hyperplane Let us define the training data which belong either to positive or negative class as follows. Y1), , (xi, 1/i), (x/,Y/) xi E Rn, y E {+1,-1} xi is a feature vector of i-th sample, which is represented by an n dimensional vector (xi = (f1, fn) E Rn). yi is a scalar value that specifies the class (positive</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Kyoto University text corpus project.</title>
<date>1997</date>
<booktitle>In Proceedings of the ANLP, Japan,</booktitle>
<pages>115--118</pages>
<contexts>
<context position="14921" citStr="Kurohashi and Nagao, 1997" startWordPosition="2484" endWordPosition="2488">arsing algorithm and attach the modification information dynamically to the newly constructed phrases (the chunks that become the head of the phrases). As we describe later we apply a beam search for parsing, and it is possible to keep several intermediate solutions while suppressing the combinatorial explosion. We refer to the features that are added incrementally during the parsing process as dynamic features. 4 Experiments and Discussion 4.1 Experiments Setting We use Kyoto University text corpus (Version 2.0) consisting of articles of Mainichi newspaper annotated with dependency structure(Kurohashi and Nagao, 1997). 7,958 sentences from the articles on January 1st to January 7th are used for the training data, and 1,246 sentences from the articles on January 9th are used for the test data. For the kernel function, we used the polynomial function (9). We set the soft margin parameter C to be 1. The feature set used in the experiments are shown in Table 1. The static features are basically taken from Uchimoto&apos;s list(Uchimoto et al., 1999) with little modification. In Table 1, &apos;Head&apos; means the rightmost content word in a chunk whose part-of-speech is not a functional category. &apos;Type&apos; mewls the rightmost fu</context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1997. Kyoto University text corpus project. In Proceedings of the ANLP, Japan, pages 115-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.</title>
<date>1999</date>
<booktitle>In Advances in Large Margin Classifiers.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13836" citStr="Platt, 1999" startWordPosition="2313" endWordPosition="2314">eful for resolving syntactic ambiguity, since two accusative noun phrses hardly modify the same verb. It is possible to use such information if we add new features related to other modifiers. In the above case, the chunk &amp;quot;sagasiteiru&amp;quot; can receive a new feature of accusative modification (by &amp;quot;josei-wo&amp;quot;) during the parsing process, which precludes the chunk &amp;quot;kono-honwo&amp;quot; from modifying &amp;quot;sagasiteiru&amp;quot; since there is a strict constraint about double-accusative 2Experimentally, it is shown that the sigmoid function gives a good approximation of probability function from the decision function of SVMs(Platt, 1999). 21 modification that will be learned from training examples. We decided to take into consideration all such modification information by using functional words or inflection forms of modifiers. Using such information about modifiers in the training phase has no difficulty since they are clearly available in a tree-bank. On the other hand, they are not known in the parsing phase of the test data. This problem can be easily solved if we adopt a bottom-up parsing algorithm and attach the modification information dynamically to the newly constructed phrases (the chunks that become the head of the</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods. In Advances in Large Margin Classifiers. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Liner Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP &apos;97.</booktitle>
<contexts>
<context position="2499" citStr="Ratnaparkhi, 1997" startWordPosition="368" endWordPosition="369">rule-based approaches have problems in coverage and consistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperpl</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A Liner Observed Time Statistical Parser Based on Maximum Entropy Models. In Proceedings of EMNLP &apos;97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Seldne</author>
<author>Kiyotaka Uchimoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Backward Beam Search Algorithm for Dependency Analysis of Japanese.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING</booktitle>
<pages>754--760</pages>
<marker>Seldne, Uchimoto, Isahara, 2000</marker>
<rawString>Satoshi Seldne, Kiyotaka Uchimoto, and Hitoshi Isahara. 2000. Backward Beam Search Algorithm for Dependency Analysis of Japanese. In Proceedings of the COLING 2000, pages 754-760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirotoshi Taira</author>
<author>Masahiko Haruno</author>
</authors>
<title>Feature Selection in SVM Text Categorization.</title>
<date>1999</date>
<booktitle>In AAAI-99.</booktitle>
<contexts>
<context position="4755" citStr="Taira and Haruno, 1999" startWordPosition="754" endWordPosition="757"> Lagrange multipliers ai &gt; 0(i = 1, , /) so that: Maximize: aiajyiyi(xi • xj) (5) 2 i=1 Jo . . , Subject to: ai ?_ 0, Eaiyi = 0 (i = 1,...,/) /=1 In this dual form problem, xi with non-zero ai is called a Support Vector. For the Support Vectors, w and b can thus be expressed as follows w E aiyi xi b = w • xi — yi. /;xiEsvs The elements of the set SVs are the Support Vectors that lie on the separating hyperplanes. Finally, the decision function f : —&gt; {±1} can be written as: high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999). In this paper, we propose an application of SVMs to Japanese dependency structure analysis. We use the features that have been studied in conventional statistical dependency analysis with a little modification on them. 2 Support Vector Machines 2.1 Optimal Hyperplane Let us define the training data which belong either to positive or negative class as follows. Y1), , (xi, 1/i), (x/,Y/) xi E Rn, y E {+1,-1} xi is a feature vector of i-th sample, which is represented by an n dimensional vector (xi = (f1, fn) E Rn). yi is a scalar value that specifies the class (positive(+1) or negative(- 1) cla</context>
</contexts>
<marker>Taira, Haruno, 1999</marker>
<rawString>Hirotoshi Taira and Masahiko Haruno. 1999. Feature Selection in SVM Text Categorization. In AAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese Dependency Structure Analysis Based on Maximum Entropy Models.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>196--203</pages>
<contexts>
<context position="2522" citStr="Uchimoto et al., 1999" startWordPosition="370" endWordPosition="373">es have problems in coverage and consistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane. In particular, com</context>
<context position="15351" citStr="Uchimoto et al., 1999" startWordPosition="2564" endWordPosition="2567">Discussion 4.1 Experiments Setting We use Kyoto University text corpus (Version 2.0) consisting of articles of Mainichi newspaper annotated with dependency structure(Kurohashi and Nagao, 1997). 7,958 sentences from the articles on January 1st to January 7th are used for the training data, and 1,246 sentences from the articles on January 9th are used for the test data. For the kernel function, we used the polynomial function (9). We set the soft margin parameter C to be 1. The feature set used in the experiments are shown in Table 1. The static features are basically taken from Uchimoto&apos;s list(Uchimoto et al., 1999) with little modification. In Table 1, &apos;Head&apos; means the rightmost content word in a chunk whose part-of-speech is not a functional category. &apos;Type&apos; mewls the rightmost functional word or the inflectional form of the rightmost predicate if there is no functional word in the chunk. The static features include the information on existence of brackets, question marks and punctuation marks etc. Besides, there are features that show the relative relation of two chunks, such as distance, and existence of brackets, quotation marks and punctuation marks between them. For dynamic features, we selected f</context>
<context position="23238" citStr="Uchimoto et al., 1999" startWordPosition="3882" endWordPosition="3885">ut in parallel. To investigate the effectiveness of this method, we perform a simple experiment: Dividing all training data (7958 sentences) by 4, the final dependency score is given by a weighted average of each scores. This simple voting approach is shown to achieve the accuracy of 88.66%, which is nearly the same accuracy achieved 5540 training sentences. In this experiment, we simply give an equal weight to each classifier. However, if we optimized the voting weight more carefully, the further improvements would be achieved (Inui and Inui, 2000). 4.8 Comparison with Related Work Uchimoto (Uchimoto et al., 1999) and Sekine (Sekine et al., 2000) report that using Kyoto University Corpus for their training and testing, they achieve around 87.2% accuracy by building statistical model based on Maximum Entropy framework. For the training data, we used exactly the same data that they used in order to make a fair comparison. In our experiments, the accuracy of 89.09% is achieved using same training data. Our model outperforms Uchimoto&apos;s model as far as the accuracies are compared. Although Uchimoto suggests that the importance of considering combination of features, in ME framework we must expand these comb</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese Dependency Structure Analysis Based on Maximum Entropy Models. In Proceedings of the EACL, pages 196-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="2934" citStr="Vapnik, 1998" startWordPosition="434" endWordPosition="436"> Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane. In particular, compared with other conventional statistical learning algorithms, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by optimizing the Kernel function, SVMs can handle non-linear feature spaces, and carry out the training with considering combinations of more than one feature. Thanks to such predominant nature, SVMs deliver state-of-the-art performance in realworld ap</context>
<context position="8642" citStr="Vapnik, 1998" startWordPosition="1453" endWordPosition="1454"> vectors (I)(xi) and (I, (x2) projected onto the higher-dimensional space, we can reduce the computational complexity considerably. Namely, we can reduce the computational overhead if we could find the function K that satisfies: - 4)(x2) = K(xi, x2). (8) On the other hand, since we do not need itself for actual learning and classification, &apos;In general, (1,(x) is a mapping into Hilbert space. all we have to do is to prove the existence of cl• that satisfies (8) provided the function K is selected properly. It is known that (8) holds if and only if the function K satisfies the Mercer condition (Vapnik, 1998). In this way, instead of projecting the training data onto the high-dimensional space, we can decrease the computational overhead by replacing the dot products, which is calculated in optimization and classification steps, with the function K. Such a function K is called a Kernel function. Among the many kinds of Kernel functions available, we will focus on the d-th polynomial kernel: K(xi, x2) = (x1 • x2 + 1)d. (9) Use of d-th polynomial kernel function allows us to build an optimal separating hyperplane which takes into account all combination of features up to d. Using a Kernel function, w</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>