<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010096">
<title confidence="0.995463">
EBL-Hope: Multilingual Word Sense Disambiguation Using A Hybrid
Knowledge-Based Technique
</title>
<author confidence="0.897292">
Eniafe Festus Ayetiran
</author>
<affiliation confidence="0.97994">
CIRSFID, University of Bologna
</affiliation>
<address confidence="0.925402">
Via Galliera, 3 - 40121
Bologna, Italy
</address>
<email confidence="0.996801">
eniafe.ayetiran2@unibo.it
</email>
<author confidence="0.997116">
Guido Boella
</author>
<affiliation confidence="0.965469333333333">
Department of Computer Science
University of Turin
Turin, Italy
</affiliation>
<email confidence="0.994643">
boella@di.unito.it
</email>
<sectionHeader confidence="0.995584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.979158833333333">
We present a hybrid knowledge-based ap-
proach to multilingual word sense disam-
biguation using BabelNet. Our approach is
based on a hybrid technique derived from the
modified version of the Lesk algorithm and
the Jiang &amp; Conrath similarity measure. We
present our system&apos;s runs for the word sense
disambiguation subtask of the Multilingual
Word Sense Disambiguation and Entity Link-
ing task of SemEval 2015. Our system ranked
9th among the participating systems for En-
glish.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999650607142857">
The computational identification of the meaning of
words in context is called Word Sense Disambigua-
tion (WSD), also known as Lexical Disambigua-
tion. There have been a significant amount of re-
search on WSD over the years with numerous differ-
ent approaches being explored. Multilingual word
sense disambiguation aims to disambiguate the tar-
get word in different languages. This, however, in-
volves a different scenario compared to monolingual
WSD in the sense that a single word in a language
might have varying number of senses in other lan-
guages with significant differences in the semantics
of some of the available senses.
Approaches to word sense disambiguation may
be: (1) knowledge-based which depends on some
knowledge dictionary or lexicon (2) supervised ma-
chine learning techniques which train systems from
labelled training sets and (3) unsupervised which
is based on unlabelled corpora, and do not exploit
any manually sense-tagged corpus to provide a sense
choice for a word in context.
We present a hybrid knowledge-based approach
based on the Modified Lesk algorithm and the Jiang
&amp; Conrath similarity measure using BabelNet (Nav-
igli and Ponzetto, 2012). The system presented here
is an adaptation of our earlier work on monolingual
word sense disambiguation in English (Ayetiran et
al., 2014).
</bodyText>
<sectionHeader confidence="0.988221" genericHeader="introduction">
2 Methodology
</sectionHeader>
<figureCaption confidence="0.9892908">
Figure 1 illustrates the general architecture of our
hybrid disambiguation system.
Figure 1: The Hybrid Word Sense Disambiguation Sys-
tem - A system that combines two distinct disambigua-
tion submodules.
</figureCaption>
<page confidence="0.959074">
340
</page>
<note confidence="0.509366">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 340–344,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.994963">
2.1 The Lesk Algorithm
</subsectionHeader>
<bodyText confidence="0.997196848484848">
Micheal Lesk (1986) invented this approach named
gloss overlap or the Lesk algorithm. It is one of the
first algorithms developed for the semantic disam-
biguation of all words in unrestricted texts. The only
resource required by the algorithm is a set of dictio-
nary entries, one for each possible word sense, and
knowledge about the immediate context where the
sense disambiguation is performed. The idea behind
the Lesk algorithm represents the seed for today&apos;s
corpus-based algorithms. Almost every supervised
WSD system relies one way or the other on some
form of contextual overlap, with the overlap being
typically measured between the context of an am-
biguous word and contexts specific to various mean-
ings of that word, as learned from previously anno-
tated data.
The main idea behind the original definition of
the algorithm is to disambiguate words by finding
the overlap among their sense definitions. Namely,
given two words, W1 and W2, each with NW1 and
NW2 senses defined in a dictionary, for each pos-
sible sense pair W1i and W2,, i = 1, , NW1,
j = 1, , NW2, we first determine the over-
lap of the corresponding definitions by counting the
number of words they have in common. Next, the
sense pair with the maximum overlap is selected,
and therefore the sense is assigned to each word in
the text as the appropriate sense. Several variations
of the algorithm have been proposed after the initial
work of Lesk. Ours follow the work of Banerjee and
Pedersen (2002) who adapted the algorithm using
WordNet (Miller, 1990) and the semantic relations
in it.
</bodyText>
<subsectionHeader confidence="0.999199">
2.2 Jiang &amp; Conrath Similarity Measure
</subsectionHeader>
<bodyText confidence="0.996334095238095">
Jiang &amp; Conrath similarity (Jiang &amp; Conrath, 1997)
is a similarity metric derived from corpus statistics
and the WordNet lexical taxonomy. The method
makes use of information content (IC) scores de-
rived from corpus statistics (Reisnik 1995) to weight
edges in the taxonomy. Edge weights are set to the
difference in IC of the concepts represented by the
two connected notes.
For this algorithm, Reisnik (1995)’s IC measure
is augmented with the notion of path length between
concepts. This approach includes the information
content of the concepts themselves along with the
information content of their lowest common sub-
sumer. A lowest common subsumer is a concept
in a lexical taxonomy which has the shortest dis-
tance from the two concepts compared. They argue
that the strength of a child link is proportional to the
conditional probability of encountering an instance
of the child sense sz given an instance of its parent
sense. The resulting formula can be expressed in
Equation (1) below:
</bodyText>
<equation confidence="0.999954">
Dist(w1, w2) = IC(s1) + IC(s2) (1)
IC(c) = log−1P(s) (2)
</equation>
<bodyText confidence="0.8388455">
P(s) is the probability of encountering an instance of
sense s.
</bodyText>
<sectionHeader confidence="0.97678" genericHeader="method">
3 The Hybrid WSD System
</sectionHeader>
<bodyText confidence="0.999992086956522">
For monosemous words, the sense is returned as dis-
ambiguated based on the part of speech. For poly-
semous words, we followed the Adapted Lesk ap-
proach of Banerjee and Pederson (2002) but instead
of a limited window size used by Banerjee and Ped-
erson, we used all context words as the window size.
Most prior work has not made use of the
antonymy relation for WSD. But according to Ji
(2010), if two context words are antonyms and be-
long to the same semantic cluster, they tend to rep-
resent the alternative attributes for the target word.
Furthermore, if two words are antonymous, the gloss
and examples of the opposing senses often con-
tain many words that are mutually useful for dis-
ambiguating both the original sense and its oppo-
site. Therefore, we added the glosses of antonyms
in addition to hypernyms, hyponyms, meronyms etc.
used by Banerjee and Pedersen (2002). Also, for
verbs we have added the glosses of entailment and
causes relations of each word sense to their vectors.
For adjectives and adverbs, we added the morpho-
logically related nouns to the vectors of each word
sense in computing the similarity score.
</bodyText>
<equation confidence="0.981612">
−2 × IC(Lsuper(s1, s2))
</equation>
<bodyText confidence="0.99959925">
Where s1 and s2 are the first and second senses
respectively and LSuper (lowest common subsumer)
is the lowest super-ordinate of s1 and s2. IC is the
information content given by equation (2):
</bodyText>
<page confidence="0.993879">
341
</page>
<bodyText confidence="0.999285466666667">
The similarity score for the Modified Lesk al-
gorithm is computed using the Cosine similarity.
The vectors are composed using the glosses of the
word senses, that of their hypernyms, hyponyms,
and antonyms. We then compute the cosine of the
angle between the two vectors. This metric is a
measurement of orientation and not magnitude. The
magnitude of the score for each word is normalized
by the magnitude of the scores for all words within
the vector. The resulting normalized scores reflect
the degree the sense is characterized by each of the
component words.
Cosine similarity can be trivially computed as the
dot product of vectors normalized by their Euclidean
length:
</bodyText>
<equation confidence="0.877613">
a~ = (a1, a2, a3, ....an) and b~ = (b1, b2, b3, ....bn)
</equation>
<bodyText confidence="0.9608568">
Here an and bn are the components of vectors con-
taining length normalized TF-IDF scores for either
the words in a context window or the words within
the glosses associated with a sense being scored.
The dot product is then computed as follows:
~a. ~b = Eni=1 aibi = a1b1 + a2b2 + + anbn
The dot product is a simple multiplication of each
component from the both vectors added together.
The geometric definition of the dot product given by
equation (3):
</bodyText>
<equation confidence="0.70111">
~a.~b =k~ak ||~b||cosθ (3)
</equation>
<bodyText confidence="0.9562305">
Using the the cummutative property, we have equa-
tion (4):
</bodyText>
<equation confidence="0.694349">
~b||k~ak cosθ (4)
</equation>
<bodyText confidence="0.982025380952381">
wherek~ak cosθ is the projection of a~ into~b in which
solving the dot product equation for cosθ gives the
cosine similarity in equation (5):
where a.b is the dot product andkak and ||b ||are the
vector lengths of a and b, respectively.
We disambiguated each target word in a sentence
using the Jiang &amp; Conrath similarity measure using
all the context words as the window size. We did this
by computing Jiang &amp; Conrath similarity score for
each candidate senses of the target word and select
the sense that has the highest sum total similarity
score to all the words in the context window.
For each context word w and candidate word
senses ceval, we compute individual similarity
scores using equation (6):
sim(w, ceval) = maxcEsen(w)[sim(c, ceval)] (6)
where sim(w,ceval) function computes the maxi-
mum similarity score obtained by computing Jiang
&amp; Conrath similarity for all the candidate senses in
a context word. The aggregate summation of the in-
dividual similarity scores is given in equation (7):
</bodyText>
<equation confidence="0.9995155">
argmaxcevalEsen(W) = � maxcEsen(w)
wEcontext(W)
[sim(c, ceval)]
(7)
</equation>
<bodyText confidence="0.999803347826087">
An agreement between the results produced by
each of the two algorithms means the word un-
der consideration has been likely correctly disam-
biguated and the sense on which they agreed is re-
turned as the correct sense. Whenever one module
fails to produce any sense that can be applied to a
word but the other succeeds, we just return the sense
computed by the successful module. Module fail-
ures occur when all of the available senses receive
a score of 0 according to the module’s underlying
similarity algorithm (e.g., due to lack of overlapping
words for Modified Lesk).
Finally, in a situation where the two modules
select different senses, we heuristically resolved
the disagreement. Our heuristic first computes the
derivationally related forms of all of the words in
the context window and adds each of them the vec-
tor representation of the word being assessed. Then
for the senses produced by the Modified Lesk and
Jiang &amp; Conrath algorithms, we obtain the similar-
ity score between the vector representations of the
two competing senses and the new expanded con-
text vector. The algorithm returns the sense selected
</bodyText>
<figure confidence="0.99236875">
~a.
~b = ||
~
cosθ =
(5)
k~ak ||~b||
b
~a.
</figure>
<page confidence="0.995235">
342
</page>
<bodyText confidence="0.999928615384615">
by the module whose winning vector is most similar
to the augmented context vector.
The intuition behind this notion of validation is
that the glosses of a word sense, and that of their se-
mantically related ones in the WordNet lexical tax-
onomy should share words in common as much as
possible with words in context with the target word.
Adding the derivationally related forms of the words
in the context window increases the chances of over-
lap when there are mismatches caused by changes in
word morphology. When both modules fail to iden-
tify a sense, the Most Frequent Sense (MFS) in the
Semcor corpus is used as the appropriate sense.
</bodyText>
<sectionHeader confidence="0.995836" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<bodyText confidence="0.999941038461538">
The SemEval 2015 Multilingual Word Sense Disam-
biguation and Entity Linking task provides datasets
in English, Spanish and Italian. BabelNet (Navigli
and Ponzetto, 2012) which provides automatic trans-
lation of each word sense in other languages have
been employed. To enrich the glosses used by the
Modified Lesk algorithm, the glosses provided by
BabelNet from Wikipedia in the 3 subtask languages
have been used to extend the initial glosses available
in WordNet (Miller, 1990).
Furthermore, BabelNet contains some word
senses which are not available in WordNet. These
senses and their glosses were used directly with-
out any reference to WordNet translation since it
does not have any. For English, we disambiguate
all the open target words while for Spanish and Ital-
ian, we disambiguate all noun target words. Due
to some challenges we faced close to our task’s
evaluation deadline, we were unable to obtain Ba-
belNet 2.5 which is the official resource for the
task. Instead, we used BabelNet 1.1.1 from the Se-
mEval 2013 Multilingual Word Sense Disambigua-
tion Task, which we initially used to develop our
system but unfortunately contains only noun words
for Spanish and Italian and does not include some
English words found in the test set.
</bodyText>
<sectionHeader confidence="0.998398" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.626100333333333">
Table 1 compares the performance of our system
with other participating systems on the English sub-
task. Table 2 shows the result of our system for the
</bodyText>
<table confidence="0.999895636363636">
System Precision Recall F1
LIMSI 68.7 63.1 65.8
SUDOKU-Run2 62.9 60.4 61.6
SUDOKU-Run3 61.9 59.4 60.6
vua-background 67.5 51.4 58.4
SUDOKU-Run1 60.1 52.1 55.8
WSD-games-Run2 58.8 50.0 54.0
WSD-games-Run1 57.4 48.8 52.8
WSD-games Run3 53.5 45.4 49.1
EBL-Hope 48.4 44.4 46.3
TeamUFAL 40.4 36.5 38.3
</table>
<tableCaption confidence="0.8635104">
Table 1: Performance of All Participating Systems for
English Subtask. Our EBL-Hope System ranked 9th out
of the submitted systems.
Spanish and Italian subtask where we submitted a
run for only nouns and named enitities.
</tableCaption>
<table confidence="0.999781">
Subtask Precision Recall F1
Spanish 52.5 44.6 48.2
Italian 43.1 35.3 38.8
</table>
<tableCaption confidence="0.992812">
Table 2: EBL-Hope’s hybrid system performance on the
Spanish and Italian subtasks.
</tableCaption>
<bodyText confidence="0.999365666666666">
Our system performs noticeably better in Spanish
than Italian. Further analysis shows that the weak-
est area of our system for the English subtask are
the verbs, which achieve 35.8 F1 score. We achieve
high scores on named-entities with an F1 scores of
80.2 in English, 48.5 in Italian and the highest F1
score across all participating systems on Spanish
with 70.8.
Table 3 and Table 4 give the performance obtained
when using the Modified Lesk and Jiang &amp; Con-
rath modules independently. Our hybrid system out-
performs the individual component modules on both
English and Spanish. On Italian, the Hybrid system
performs comparably to Jiang &amp; Conrath, which is
the best individual module.
</bodyText>
<table confidence="0.9912455">
Subtask Precision Recall F1
English 43.6 41.3 42.4
Spanish 48.1 41.2 44.3
Italian 46.3 33.5 38.9
</table>
<tableCaption confidence="0.9918395">
Table 4: Performance of the Jiang &amp; Conrath module in
isolation on the 3 subtasks.
</tableCaption>
<page confidence="0.964199">
343
</page>
<table confidence="0.999777">
Subtask Precision Recall F1
English 44.2 40.6 42.3
Spanish 47.6 40.1 43.5
Italian 40.3 31.7 35.4
</table>
<tableCaption confidence="0.981008">
Table 3: Performance of the Modified Lesk module in
isolation on the 3 subtasks.
</tableCaption>
<sectionHeader confidence="0.996936" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999962538461538">
In this work, we have combined two algorithms for
word sense disambiguation, Modified Lesk and an
approach based on Jiang &amp; Conrath similarity. The
resulting hybrid system improves performance by
heuristically resolving disagreements in the word
sense assigned by the individual algorithms. We
observe the results of the combined algorithm do
consistently outperform each of the individual algo-
rithms used in isolation. However, our poor perfor-
mance on the official evaluation could likely have
been improved by making use of the more recent 2.5
version of BabelNet as recommended by the task or-
ganizers.
</bodyText>
<sectionHeader confidence="0.976708" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999921">
This work has been supported by European Com-
mission scholarship under the Erasmus+ doctoral
scholarship programmes. We would like to thank
the anonymous reviewers for their helpful sugges-
tions and comments. Special thanks to Daniel Cer
for his great and useful editorial input on the final
manuscript.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999722368421053">
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
Lesk Algorithm for Word Sense Disambiguation us-
ing WordNet. In Proceedings of the 3rd International
Conference on Computational Linguistics and Intelli-
gent Text Processing (CICLING), Mexico City, Mex-
ico, 17 - 23 February, 2002, pp. 136 - 145.
George Miller. 1990. An Online Lexical Database. In-
ternational Journal of Lexicography, 3(4): 235 - 244.
Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity Based on Corpus Statistics and Lexical
Taxonomy. In Proceedings of the 10th International
Conference on Research in Computational Linguistics,
Taipei, Taiwan, 2 - 4 August 1998, pp. 19 - 33.
Michael E. Lesk. 1986. Automatic Sense Disambigua-
tion Using Machine Readable Dictionaries: How to
Tell a Pine Cone from an Ice Cream Cone. In Proceed-
ings of the 5th ACM-SIGDOC Conference, Toronto,
Canada, 8 - 11 June 1986, pp. 24 - 26.
Eniafe F. Ayetiran, Guido Boella, Luigi Di Caro, Livio
Robaldo. 2014. Enhancing Word Sense Disambigua-
tion Using A Hybrid Knowledge-Based Technique. In
Proceedings of 11th international workshop on natu-
ral language processing and cognitive science, Venice,
Italy 27 - 29, October, pp. 15 - 26.
Heng Ji. 2010. One Sense per Context Cluster: Im-
proving Word Sense Disambiguation Using Web-Scale
Phrase Clustering. In Proceedings of the 4th Universal
Communication Symposium (IUCS), Beijing, China,
18 -19 October 2010, pp. 181 -184.
Roberto Navigli and Simone P. Ponzetto. 2012. Babel-
Net: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic net-
work. In Artificial Intelligence, 193(2012) 217-250.
Philip Reisnik. 1995. One Sense per Context Cluster:
Using Information Content to Evaluate Semantic Sim-
ilarity. In Proceedings of the 14th International Joint
Conference on Artificial Intelligence (IJCAI), Mon-
treal, Canada, 20 25 August 1995, pp. 448453.
</reference>
<page confidence="0.998978">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.073639">
<title confidence="0.9154965">EBL-Hope: Multilingual Word Sense Disambiguation Using A Knowledge-Based Technique</title>
<affiliation confidence="0.670957333333333">Eniafe Festus CIRSFID, University of Via Galliera, 3 -</affiliation>
<address confidence="0.738955">Bologna,</address>
<email confidence="0.996778">eniafe.ayetiran2@unibo.it</email>
<author confidence="0.990758">Guido</author>
<affiliation confidence="0.999929">Department of Computer University of</affiliation>
<address confidence="0.719373">Turin,</address>
<email confidence="0.999386">boella@di.unito.it</email>
<abstract confidence="0.966999923076923">We present a hybrid knowledge-based approach to multilingual word sense disambiguation using BabelNet. Our approach is based on a hybrid technique derived from the modified version of the Lesk algorithm and the Jiang &amp; Conrath similarity measure. We present our system&apos;s runs for the word sense disambiguation subtask of the Multilingual Word Sense Disambiguation and Entity Linking task of SemEval 2015. Our system ranked 9th among the participating systems for English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted Lesk Algorithm for Word Sense Disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Computational Linguistics and Intelligent Text Processing (CICLING), Mexico City,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="4005" citStr="Banerjee and Pedersen (2002)" startWordPosition="629" endWordPosition="632">guate words by finding the overlap among their sense definitions. Namely, given two words, W1 and W2, each with NW1 and NW2 senses defined in a dictionary, for each possible sense pair W1i and W2,, i = 1, , NW1, j = 1, , NW2, we first determine the overlap of the corresponding definitions by counting the number of words they have in common. Next, the sense pair with the maximum overlap is selected, and therefore the sense is assigned to each word in the text as the appropriate sense. Several variations of the algorithm have been proposed after the initial work of Lesk. Ours follow the work of Banerjee and Pedersen (2002) who adapted the algorithm using WordNet (Miller, 1990) and the semantic relations in it. 2.2 Jiang &amp; Conrath Similarity Measure Jiang &amp; Conrath similarity (Jiang &amp; Conrath, 1997) is a similarity metric derived from corpus statistics and the WordNet lexical taxonomy. The method makes use of information content (IC) scores derived from corpus statistics (Reisnik 1995) to weight edges in the taxonomy. Edge weights are set to the difference in IC of the concepts represented by the two connected notes. For this algorithm, Reisnik (1995)’s IC measure is augmented with the notion of path length betw</context>
<context position="6139" citStr="Banerjee and Pedersen (2002)" startWordPosition="990" endWordPosition="993">d Pederson, we used all context words as the window size. Most prior work has not made use of the antonymy relation for WSD. But according to Ji (2010), if two context words are antonyms and belong to the same semantic cluster, they tend to represent the alternative attributes for the target word. Furthermore, if two words are antonymous, the gloss and examples of the opposing senses often contain many words that are mutually useful for disambiguating both the original sense and its opposite. Therefore, we added the glosses of antonyms in addition to hypernyms, hyponyms, meronyms etc. used by Banerjee and Pedersen (2002). Also, for verbs we have added the glosses of entailment and causes relations of each word sense to their vectors. For adjectives and adverbs, we added the morphologically related nouns to the vectors of each word sense in computing the similarity score. −2 × IC(Lsuper(s1, s2)) Where s1 and s2 are the first and second senses respectively and LSuper (lowest common subsumer) is the lowest super-ordinate of s1 and s2. IC is the information content given by equation (2): 341 The similarity score for the Modified Lesk algorithm is computed using the Cosine similarity. The vectors are composed usin</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk Algorithm for Word Sense Disambiguation using WordNet. In Proceedings of the 3rd International Conference on Computational Linguistics and Intelligent Text Processing (CICLING), Mexico City, Mexico, 17 - 23 February, 2002, pp. 136 - 145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>An Online Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>235--244</pages>
<contexts>
<context position="4060" citStr="Miller, 1990" startWordPosition="639" endWordPosition="640">ly, given two words, W1 and W2, each with NW1 and NW2 senses defined in a dictionary, for each possible sense pair W1i and W2,, i = 1, , NW1, j = 1, , NW2, we first determine the overlap of the corresponding definitions by counting the number of words they have in common. Next, the sense pair with the maximum overlap is selected, and therefore the sense is assigned to each word in the text as the appropriate sense. Several variations of the algorithm have been proposed after the initial work of Lesk. Ours follow the work of Banerjee and Pedersen (2002) who adapted the algorithm using WordNet (Miller, 1990) and the semantic relations in it. 2.2 Jiang &amp; Conrath Similarity Measure Jiang &amp; Conrath similarity (Jiang &amp; Conrath, 1997) is a similarity metric derived from corpus statistics and the WordNet lexical taxonomy. The method makes use of information content (IC) scores derived from corpus statistics (Reisnik 1995) to weight edges in the taxonomy. Edge weights are set to the difference in IC of the concepts represented by the two connected notes. For this algorithm, Reisnik (1995)’s IC measure is augmented with the notion of path length between concepts. This approach includes the information co</context>
<context position="11280" citStr="Miller, 1990" startWordPosition="1861" endWordPosition="1862"> fail to identify a sense, the Most Frequent Sense (MFS) in the Semcor corpus is used as the appropriate sense. 4 Experimental Setting The SemEval 2015 Multilingual Word Sense Disambiguation and Entity Linking task provides datasets in English, Spanish and Italian. BabelNet (Navigli and Ponzetto, 2012) which provides automatic translation of each word sense in other languages have been employed. To enrich the glosses used by the Modified Lesk algorithm, the glosses provided by BabelNet from Wikipedia in the 3 subtask languages have been used to extend the initial glosses available in WordNet (Miller, 1990). Furthermore, BabelNet contains some word senses which are not available in WordNet. These senses and their glosses were used directly without any reference to WordNet translation since it does not have any. For English, we disambiguate all the open target words while for Spanish and Italian, we disambiguate all noun target words. Due to some challenges we faced close to our task’s evaluation deadline, we were unable to obtain BabelNet 2.5 which is the official resource for the task. Instead, we used BabelNet 1.1.1 from the SemEval 2013 Multilingual Word Sense Disambiguation Task, which we in</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George Miller. 1990. An Online Lexical Database. International Journal of Lexicography, 3(4): 235 - 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity Based on Corpus Statistics and Lexical Taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the 10th International Conference on Research in Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>19--33</pages>
<location>Taipei,</location>
<contexts>
<context position="4184" citStr="Jiang &amp; Conrath, 1997" startWordPosition="657" endWordPosition="660">W1i and W2,, i = 1, , NW1, j = 1, , NW2, we first determine the overlap of the corresponding definitions by counting the number of words they have in common. Next, the sense pair with the maximum overlap is selected, and therefore the sense is assigned to each word in the text as the appropriate sense. Several variations of the algorithm have been proposed after the initial work of Lesk. Ours follow the work of Banerjee and Pedersen (2002) who adapted the algorithm using WordNet (Miller, 1990) and the semantic relations in it. 2.2 Jiang &amp; Conrath Similarity Measure Jiang &amp; Conrath similarity (Jiang &amp; Conrath, 1997) is a similarity metric derived from corpus statistics and the WordNet lexical taxonomy. The method makes use of information content (IC) scores derived from corpus statistics (Reisnik 1995) to weight edges in the taxonomy. Edge weights are set to the difference in IC of the concepts represented by the two connected notes. For this algorithm, Reisnik (1995)’s IC measure is augmented with the notion of path length between concepts. This approach includes the information content of the concepts themselves along with the information content of their lowest common subsumer. A lowest common subsume</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity Based on Corpus Statistics and Lexical Taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics, Taipei, Taiwan, 2 - 4 August 1998, pp. 19 - 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Lesk</author>
</authors>
<title>Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th ACM-SIGDOC Conference, Toronto, Canada, 8 - 11</booktitle>
<pages>24--26</pages>
<contexts>
<context position="2553" citStr="Lesk (1986)" startWordPosition="382" endWordPosition="383">igli and Ponzetto, 2012). The system presented here is an adaptation of our earlier work on monolingual word sense disambiguation in English (Ayetiran et al., 2014). 2 Methodology Figure 1 illustrates the general architecture of our hybrid disambiguation system. Figure 1: The Hybrid Word Sense Disambiguation System - A system that combines two distinct disambiguation submodules. 340 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 340–344, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2.1 The Lesk Algorithm Micheal Lesk (1986) invented this approach named gloss overlap or the Lesk algorithm. It is one of the first algorithms developed for the semantic disambiguation of all words in unrestricted texts. The only resource required by the algorithm is a set of dictionary entries, one for each possible word sense, and knowledge about the immediate context where the sense disambiguation is performed. The idea behind the Lesk algorithm represents the seed for today&apos;s corpus-based algorithms. Almost every supervised WSD system relies one way or the other on some form of contextual overlap, with the overlap being typically </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael E. Lesk. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. In Proceedings of the 5th ACM-SIGDOC Conference, Toronto, Canada, 8 - 11 June 1986, pp. 24 - 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eniafe F Ayetiran</author>
<author>Guido Boella</author>
<author>Luigi Di Caro</author>
<author>Livio Robaldo</author>
</authors>
<title>Enhancing Word Sense Disambiguation Using A Hybrid Knowledge-Based Technique.</title>
<date>2014</date>
<journal></journal>
<booktitle>In Proceedings of 11th international workshop on natural language processing and cognitive science,</booktitle>
<volume>29</volume>
<pages>15--26</pages>
<location>Venice,</location>
<marker>Ayetiran, Boella, Di Caro, Robaldo, 2014</marker>
<rawString>Eniafe F. Ayetiran, Guido Boella, Luigi Di Caro, Livio Robaldo. 2014. Enhancing Word Sense Disambiguation Using A Hybrid Knowledge-Based Technique. In Proceedings of 11th international workshop on natural language processing and cognitive science, Venice, Italy 27 - 29, October, pp. 15 - 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
</authors>
<title>One Sense per Context Cluster: Improving Word Sense Disambiguation Using Web-Scale Phrase Clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th Universal Communication Symposium (IUCS),</booktitle>
<pages>181--184</pages>
<location>Beijing,</location>
<contexts>
<context position="5662" citStr="Ji (2010)" startWordPosition="912" endWordPosition="913">arent sense. The resulting formula can be expressed in Equation (1) below: Dist(w1, w2) = IC(s1) + IC(s2) (1) IC(c) = log−1P(s) (2) P(s) is the probability of encountering an instance of sense s. 3 The Hybrid WSD System For monosemous words, the sense is returned as disambiguated based on the part of speech. For polysemous words, we followed the Adapted Lesk approach of Banerjee and Pederson (2002) but instead of a limited window size used by Banerjee and Pederson, we used all context words as the window size. Most prior work has not made use of the antonymy relation for WSD. But according to Ji (2010), if two context words are antonyms and belong to the same semantic cluster, they tend to represent the alternative attributes for the target word. Furthermore, if two words are antonymous, the gloss and examples of the opposing senses often contain many words that are mutually useful for disambiguating both the original sense and its opposite. Therefore, we added the glosses of antonyms in addition to hypernyms, hyponyms, meronyms etc. used by Banerjee and Pedersen (2002). Also, for verbs we have added the glosses of entailment and causes relations of each word sense to their vectors. For adj</context>
</contexts>
<marker>Ji, 2010</marker>
<rawString>Heng Ji. 2010. One Sense per Context Cluster: Improving Word Sense Disambiguation Using Web-Scale Phrase Clustering. In Proceedings of the 4th Universal Communication Symposium (IUCS), Beijing, China, 18 -19 October 2010, pp. 181 -184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone P Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>In Artificial Intelligence,</journal>
<volume>193</volume>
<issue>2012</issue>
<pages>217--250</pages>
<contexts>
<context position="1966" citStr="Navigli and Ponzetto, 2012" startWordPosition="294" endWordPosition="298">guages with significant differences in the semantics of some of the available senses. Approaches to word sense disambiguation may be: (1) knowledge-based which depends on some knowledge dictionary or lexicon (2) supervised machine learning techniques which train systems from labelled training sets and (3) unsupervised which is based on unlabelled corpora, and do not exploit any manually sense-tagged corpus to provide a sense choice for a word in context. We present a hybrid knowledge-based approach based on the Modified Lesk algorithm and the Jiang &amp; Conrath similarity measure using BabelNet (Navigli and Ponzetto, 2012). The system presented here is an adaptation of our earlier work on monolingual word sense disambiguation in English (Ayetiran et al., 2014). 2 Methodology Figure 1 illustrates the general architecture of our hybrid disambiguation system. Figure 1: The Hybrid Word Sense Disambiguation System - A system that combines two distinct disambiguation submodules. 340 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 340–344, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2.1 The Lesk Algorithm Micheal Lesk (1986) invented thi</context>
<context position="10970" citStr="Navigli and Ponzetto, 2012" startWordPosition="1809" endWordPosition="1812">ated ones in the WordNet lexical taxonomy should share words in common as much as possible with words in context with the target word. Adding the derivationally related forms of the words in the context window increases the chances of overlap when there are mismatches caused by changes in word morphology. When both modules fail to identify a sense, the Most Frequent Sense (MFS) in the Semcor corpus is used as the appropriate sense. 4 Experimental Setting The SemEval 2015 Multilingual Word Sense Disambiguation and Entity Linking task provides datasets in English, Spanish and Italian. BabelNet (Navigli and Ponzetto, 2012) which provides automatic translation of each word sense in other languages have been employed. To enrich the glosses used by the Modified Lesk algorithm, the glosses provided by BabelNet from Wikipedia in the 3 subtask languages have been used to extend the initial glosses available in WordNet (Miller, 1990). Furthermore, BabelNet contains some word senses which are not available in WordNet. These senses and their glosses were used directly without any reference to WordNet translation since it does not have any. For English, we disambiguate all the open target words while for Spanish and Ital</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone P. Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. In Artificial Intelligence, 193(2012) 217-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Reisnik</author>
</authors>
<title>One Sense per Context Cluster: Using Information Content to Evaluate Semantic Similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<volume>20</volume>
<pages>448453</pages>
<location>Montreal,</location>
<contexts>
<context position="4374" citStr="Reisnik 1995" startWordPosition="688" endWordPosition="689">rlap is selected, and therefore the sense is assigned to each word in the text as the appropriate sense. Several variations of the algorithm have been proposed after the initial work of Lesk. Ours follow the work of Banerjee and Pedersen (2002) who adapted the algorithm using WordNet (Miller, 1990) and the semantic relations in it. 2.2 Jiang &amp; Conrath Similarity Measure Jiang &amp; Conrath similarity (Jiang &amp; Conrath, 1997) is a similarity metric derived from corpus statistics and the WordNet lexical taxonomy. The method makes use of information content (IC) scores derived from corpus statistics (Reisnik 1995) to weight edges in the taxonomy. Edge weights are set to the difference in IC of the concepts represented by the two connected notes. For this algorithm, Reisnik (1995)’s IC measure is augmented with the notion of path length between concepts. This approach includes the information content of the concepts themselves along with the information content of their lowest common subsumer. A lowest common subsumer is a concept in a lexical taxonomy which has the shortest distance from the two concepts compared. They argue that the strength of a child link is proportional to the conditional probabili</context>
</contexts>
<marker>Reisnik, 1995</marker>
<rawString>Philip Reisnik. 1995. One Sense per Context Cluster: Using Information Content to Evaluate Semantic Similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), Montreal, Canada, 20 25 August 1995, pp. 448453.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>