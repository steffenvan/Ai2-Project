<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.990258">
Competitive Grammar Writing*
</title>
<author confidence="0.99923">
Jason Eisner
</author>
<affiliation confidence="0.941585">
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.668168">
Baltimore, MD 21218, USA
</address>
<email confidence="0.998844">
jason@cs.jhu.edu
</email>
<sectionHeader confidence="0.993892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873666666667">
Just as programming is the traditional introduction to
computer science, writing grammars by hand is an ex-
cellent introduction to many topics in computational lin-
guistics. We present and justify a well-tested introductory
activity in which teams of mixed background compete
to write probabilistic context-free grammars of English.
The exercise brings together symbolic, probabilistic, al-
gorithmic, and experimental issues in a way that is acces-
sible to novices and enjoyable.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926583333333">
We describe a hands-on group activity for novices
that introduces several central topics in computa-
tional linguistics (CL). While the task is intellec-
tually challenging, it requires no background other
than linguistic intuitions, no programming,1 and
only a very basic understanding of probability.
The activity is especially appropriate for mixed
groups of linguists, computer scientists, and others,
letting them collaborate effectively on small teams
and learn from one another. A friendly competition
among the teams makes the activity intense and en-
joyable and introduces quantitative evaluation.
</bodyText>
<subsectionHeader confidence="0.996739">
1.1 Task Overview
</subsectionHeader>
<bodyText confidence="0.9982435">
Each 3-person team is asked to write a generative
context-free grammar that generates as much of En-
</bodyText>
<footnote confidence="0.705332">
* This work was supported by NSF award 0121285,
“ITR/IM+PE+SY: Summer Workshops on Human Language
Technology: Integrating Research and Education,” and by a
Fannie and John Hertz Foundation Fellowship to the second
author. We thank David A. Smith and Markus Dreyer for co-
leading the lab in 2004–2007 and for implementing various im-
provements in 2004–2007 and for providing us with data from
those years. The lab has benefited over the years from feedback
from the participants, many of whom attended the JHU sum-
mer school thanks to the generous support of NAACL. We also
thank the anonymous reviewers for helpful comments.
1In our setup, students do need the ability to invoke scripts
and edit files in a shared directory, e.g., on a Unix system.
</footnote>
<author confidence="0.63299">
Noah A. Smith
</author>
<affiliation confidence="0.767764333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.987947">
nasmith@cs.cmu.edu
</email>
<bodyText confidence="0.999989863636364">
glish as possible (over a small fixed vocabulary).
Obviously, writing a full English grammar would
take years even for experienced linguists. Thus each
team will only manage to cover a few phenomena,
and imperfectly.
To encourage precision but also recall and lin-
guistic creativity, teams are rewarded for generating
sentences that are (prescriptively) grammatical but
are not anticipated by other teams’ grammars. This
somewhat resembles scoring in the Boggle word
game, where players are rewarded for finding valid
words in a grid that are not found by other players.
A final twist is that the exercise uses probabilistic
context-free grammars (PCFGs); the actual scoring
methods are based on sampling and cross-entropy.
Each team must therefore decide how to allocate
probability mass among sentences. To avoid assign-
ing probability of zero when attempting to parse an-
other team’s sentences, a team is allowed to “back
off” (when parsing) to a simpler probability model,
such as a part-of-speech bigram model, also ex-
pressed as a PCFG.
</bodyText>
<subsectionHeader confidence="0.981716">
1.2 Setting
</subsectionHeader>
<bodyText confidence="0.9999332">
We have run this activity for six consecutive years,
as a laboratory exercise on the very first afternoon
of an intensive 2-week summer school on various
topics in human language technology.2 We allot 3.5
hours in this setting, including about 15 minutes for
setup, 30 minutes for instructions, 15 minutes for
evaluation, and 30 minutes for final discussion.
The remaining 2 hours is barely enough time for
team members to get acquainted, understand the re-
quirements, plan a strategy, and make a small dent in
</bodyText>
<footnote confidence="0.9964808">
2This 2-week course is offered as a prelude to the Johns
Hopkins University summer research workshops, sponsored by
the National Science Foundation and the Department of De-
fense. In recent years the course has been co-sponsored by the
North American ACL.
</footnote>
<page confidence="0.992312">
97
</page>
<note confidence="0.7370685">
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 97–105,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999780083333333">
the problem. Nonetheless, participants consistently
tell us that the exercise is enjoyable and pedagogi-
cally effective, almost always voting to stay an extra
hour to make further progress.
Our 3-person teams have consisted of approxi-
mately one undergraduate, one junior graduate stu-
dent, and one more senior graduate student. If pos-
sible, each team should include at least one member
who has basic familiarity with some syntactic phe-
nomena and phrasal categories. Teams that wholly
lack this experience have been at a disadvantage in
the time-limited setting.
</bodyText>
<subsectionHeader confidence="0.641422">
1.3 Resources for Instructors
</subsectionHeader>
<bodyText confidence="0.999603555555555">
We will maintain teaching materials at http:
//www.clsp.jhu.edu/grammar-writing,
for both the laboratory exercise version and for
homework versions: scripts, data, instructions for
participants, and tips for instructors. While our
materials are designed for participants who are
fluent in English, we would gladly host translations
or adaptations into other languages, as well as other
variants and similar assignments.
</bodyText>
<sectionHeader confidence="0.810624" genericHeader="method">
2 Why Grammar Writing?
</sectionHeader>
<bodyText confidence="0.95526348">
A computer science curriculum traditionally starts
with programming, because programming is acces-
sible, hands-on, and necessary to motivate or under-
stand most other topics in computer science. We be-
lieve that grammar writing should play the same role
in computational linguistics—as it often did before
the statistical revolution3—and for similar reasons.
Grammar writing remains central because many
theoretical and applied CL topics center around
grammar formalisms. Much of the field tries to de-
sign expressive formalisms (akin to programming
languages); solve linguistic problems within them
(akin to programming); enrich them with probabil-
ities; process them with efficient algorithms; learn
them from data; and connect them to other modules
in the linguistic processing pipeline.
3The first author was specifically inspired by his experience
writing a grammar in Bill Woods’s NLP course at Harvard in
1987. An anonymous reviewer remarks that such assignments
were common at the time. Our contributions are to introduce
statistical and finite-state elements, to make the exercise into a
game, and to provide reusable instructional materials.
Of course, there are interesting grammar for-
malisms at all levels of language processing. One
might ask why syntax is a good level at which to be-
gin education in computational linguistics.
First, starting with syntax establishes at the start
that there are formal and computational methods
specific to natural language. Computational linguis-
tics is not merely a set of applied tasks to be solved
with methods already standardly taught in courses
on machine learning, theory of computation,4 or
knowledge representation.
Second, we have found that syntax captures stu-
dents’ interest rapidly. They quickly appreciate the
linguistic phenomena, see that they are non-trivial,
and have little trouble with the CFG formalism.
Third, beginning specifically with PCFGs pays
technical dividends in a CL course. Once one un-
derstands PCFG models, it is easy to understand the
simpler finite-state models (including n-gram mod-
els, HMMs, etc.) and their associated algorithms, ei-
ther by analogy or by explicit reduction to special
cases of PCFGs. CFGs are also a good starting point
for more complex syntactic formalisms (BNF, cate-
gorial grammars, TAG, LFG, HPSG, etc.) and for
compositional semantics. Indeed, our exercise mo-
tivates these more complex formalisms by forcing
students to work with the more impoverished PCFG
formalism and experience its limitations.
</bodyText>
<sectionHeader confidence="0.984581" genericHeader="method">
3 Educational Goals of the Exercise
</sectionHeader>
<bodyText confidence="0.999952818181818">
Our grammar-writing exercise is intended to serve
as a touchstone for discussion of many subsequent
topics in NLP and CL (which are italicized below).
As an instructor, one can often refer back later to the
exercise to remind students of their concrete experi-
ence with a given concept.
Generative probabilistic models. The first set of
concepts concerns language models. These are eas-
iest to understand as processes for generating text.
Thus, we give our teams a script for generating ran-
dom sentences from their grammar and their backoff
</bodyText>
<footnote confidence="0.9889618">
4Courses on theory of computation do teach pushdown au-
tomata and CFGs, of course, but they rarely touch on parsing
or probabilistic grammars, as this exercise does. Courses on
compilers may cover parsing algorithms, but only for restricted
grammar families such as unambiguous LR(1) grammars.
</footnote>
<page confidence="0.995708">
98
</page>
<bodyText confidence="0.999837752688172">
model—a helpful way to observe the generative ca-
pacity and qualitative behavior of any model.
Of course, in practice a generative grammar is
most often run “backwards” to parse an observed
sentence or score its inside probability, and we also
give the teams a script to do that. Most teams do ac-
tually run these scripts repeatedly to test their gram-
mars, since both scripts will be central in the evalua-
tion (where sentences are randomly generated from
one grammar and scored with another grammar).
It is common for instructors of NLP to show ex-
amples of randomly-generated text from an n-gram
model (e.g., Jurafsky and Martin, 2000, pp. 202–
203), yet this amusing demonstration may be misin-
terpreted as merely illustrating the inadequacy of n-
gram models. Our use of a hand-crafted PCFG com-
bined with a bigram-based (HMM) backoff grammar
demonstrates that although the HMM is much worse
at generating valid English sentences (precision), it
is much better at robustly assigning nonzero proba-
bility when analyzing English sentences (recall).
Finally, generative models do more than assign
probability. They often involve linguistically mean-
ingful latent variables, which can be recovered
given the observed data. Parsing with an appropri-
ate PCFG thus yields a intuitive and useful analy-
sis (a syntactic parse tree), although only for the
sentences that the PCFG covers. Even parsing with
the simple backoff grammar that we initially provide
yields some coarser analysis, in the form of a part-
of-speech tagging, since this backoff grammar is a
right-branching PCFG that captures part-of-speech
bigrams (for details see §1.1, §4.1, and Table 2). In
fact, parsing with the backoff PCFG is isomorphic to
Viterbi decoding in an HMM part-of-speech tagger,
a topic that is commonly covered in NLP courses.
Modeling grammaticality. The next set of con-
cepts concerns linguistic grammaticality. During the
evaluation phase of our exercise (see below), stu-
dents must make grammaticality judgments on other
teams’ randomly generated sentences—which are
usually nonsensical, frequently hard for humans to
parse, and sometimes ungrammatical. This concrete
task usually prompts questions from students about
how grammaticality ought to be defined, both for
purposes of the task and in principle. It could also
be used to discuss why some of the sentences are
so hard for humans to understand (e.g., garden-path
and frequency effects) and what parsing strategies
humans or machines might use.
The exercise of modeling grammaticality with
the CFG formalism, a formalism that appears else-
where in the computer science curriculum, high-
lights some important differences between natural
languages and formal languages. A natural lan-
guage’s true grammar is unknown (and may not even
exist: perhaps the CFG formalism is inadequate).
Rather, a grammar must be induced or constructed
as an approximate model of corpus data and/or cer-
tain native-speaker intuitions. A natural language
also differs from a programming language in includ-
ing ambiguous sentences. Students observe that the
parser uses probabilities to resolve ambiguity.
Linguistic analysis. Grammar writing is an ex-
cellent way to get students thinking about linguis-
tic phenomena (e.g., adjuncts, embedded sentences,
wh-questions, clefts, point absorption of punctuation
marks). It also forces students to think about appro-
priate linguistic formalisms. Many phenomena are
tedious to describe within CFGs (e.g., agreement,
movement, subcategorization, selectional restric-
tions, morphological inflection, and phonologically-
conditioned allomorphy such as a vs. an). They
can be treated in CFGs only with a large number of
repetitive rules. Students appreciate these problems
by grappling with them, and become very receptive
to designing expressive improvements such as fea-
ture structures and slashed categories.
Parameter tuning. Students observe the effects
of changing the rule probabilities by running the
scripts. For example, teams often find themselves
generating unreasonably long (or even infinite) sen-
tences, and must damp down the probabilities of
their recursive rules. Adjusting the rule probabilities
can also change the score and optimal tree that are
returned by the parser, and can make a big difference
in the final evaluation (see §5). This appreciation for
the role of numerical parameters helps motivate fu-
ture study of machine learning in NLP.
Quantitative evaluation. As an engineering pur-
suit, NLP research requires objective evaluation
measures to know how well systems work.
Our first measure is the precision of each team’s
</bodyText>
<page confidence="0.992812">
99
</page>
<bodyText confidence="0.999985166666667">
probabilistic grammar: how much of its probability
mass is devoted to sentences that are truly grammat-
ical? Estimating this requires human grammaticality
judgments on a random sample C of sentences gen-
erated from all teams’ grammars. These binary judg-
ments are provided by the participants themselves,
introducing the notion of linguistic annotation (al-
beit of a very simple kind). Details are in §4.3.3.
Our second measure is an upper-bound approx-
imation to cross-entropy (or log-perplexity—in ef-
fect, the recall of a probability model): how well
does each team’s probabilistic model (this time in-
cluding the backoff model of §1.1) anticipate unseen
data that are truly grammatical? (Details in §4.3.3.)
Note that in contrast to parsing competitions, we
do not evaluate the quality of the parse trees (e.g.,
PARSEVAL). Our cross-entropy measure evaluates
only the grammars’ ability to predict word strings
(language modeling). That is because we impose no
annotation standard for parse trees: each team is free
to develop its own theory of syntax. Furthermore,
many sentences will only be parsable by the backoff
grammar (e.g., a bigram model), which is not ex-
pected to produce a full syntactic analysis.
The lesson about cross-entropy evaluation is
slightly distorted by our peculiar choice of test data.
In principle, the instructors might prepare a batch
of grammatical sentences ahead of time and split
them into a test set (used to evaluate cross-entropy
at the end) and a development set (provided to the
students at the start, so that they know which gram-
matical phenomena are important to handle). The
activity could certainly be run in this way to demon-
strate proper experimental design for evaluating a
language model (discussed further in §5 and §6).
We have opted for the more entertaining “Boggle-
style” evaluation described in §1.1, where teams try
to stump one another by generating difficult test
data, using the fixed vocabulary. Thus, we evaluate
each team’s cross-entropy on all grammatical sen-
tences in the collection C, which was generated ex
post facto from all teams’ grammars.
</bodyText>
<sectionHeader confidence="0.997612" genericHeader="method">
4 Important Details
</sectionHeader>
<subsectionHeader confidence="0.868769">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.947520489361702">
A few elements are provided to participants to get
them started on their grammars.
Vocabulary. The terminal vocabulary E consists
of words from early scenes of the film Monty Python
and the Holy Grail along with some inflected forms
and function words, for a total vocabulary of 220
words. For simplicity, only 3rd-person pronouns,
nouns, and verbs are included. All words are case-
sensitive for readability (as are the grammar nonter-
minals), but we do not require or expect sentence-
initial capitalization.
All teams are restricted to this vocabulary, so that
the sentences that they generate will not frustrate
other teams’ parsers with out-of-vocabulary words.
However, they are free to use words in unexpected
ways (e.g., using castle in its verbal sense from
chess, or building up unusual constructions with the
available function words).
Initial lexicon. The initial lexical rules take the
form T —* w, where w E E+ and T E T , with
T being a set of six coarse part-of-speech tags:
Noun: 21 singular nouns starting with consonants
Det: 9 singular determiners
Prep: 14 prepositions
Proper: 8 singular proper nouns denoting people
(including multiwords such as Sir Lancelot)
VerbT: 6 3rd-person singular present transitive
verbs
Misc: 183 other words, divided into several com-
mented sections in the grammar file
Students are free to change this tagset. They
are especially encouraged to refine the Misc tag,
which includes 3rd-person plural nouns (including
some proper nouns), 3rd-person pronouns (nomina-
tive, accusative, and genitive), additional 3rd-person
verb forms (plural present, past, stem, and partici-
ples), verbs that cannot be used transitively, modals,
adverbs, numbers, adjectives (including some com-
parative and superlative forms), punctuation, coor-
dinating and subordinating conjunctions, wh-words,
and a few miscellaneous function words (to, not, ’s).
The initial lexicon is ambiguous: some words are
associated with more than one tag. Each rule has
weight 1, meaning that a tag T is equally likely to
rewrite as any of its allowed nonterminals.
Initial grammar. We provide the “S1” rules in Ta-
ble 1, so that students can try generating and parsing
</bodyText>
<table confidence="0.937521125">
100
1 S1 → NP VP .
1 VP → VerbT NP
20 NP → Det Nbar
1 NP → Proper
20 Nbar → Noun
1 Nbar → Nbar PP
1 PP → Prep NP
</table>
<tableCaption confidence="0.9824992">
Table 1: The S1 rules: a starting point for building an En-
glish grammar. The start symbol is S1. The weights in
the first column will be normalized into generative proba-
bilities; for example, the probability of expanding a given
NP with NP → Det Nbar is actually 20/(20 + 1).
</tableCaption>
<table confidence="0.999877">
1 S2 →
1 S2 → Noun
1 S2 → Misc
1 Noun → Noun
1 Noun → Noun Noun
1 Noun → Noun Misc
1 Misc → Misc
1 Misc → Misc Noun
1 Misc → Misc Misc
</table>
<tableCaption confidence="0.8475445">
Table 2: The S2 rules (simplified here where T =
{Noun, Misc}): a starting point for a backoff grammar.
</tableCaption>
<bodyText confidence="0.816479365853658">
The start symbol is S2. The Noun nonterminal gener-
ates those phrases that start with Nouns. Its 3 rules mean
that following a Noun, there is 1/3 probability each of
stopping, continuing with another Noun (via Noun), or
continuing with a Misc word (via Misc).
sentences right away. The S1 and lexical rules to-
gether implement a very small CFG. Note that no
Misc words can yet be generated. Indeed, this ini-
tial grammar will only generate some simple gram-
matical SVO sentences in singular present tense, al-
though they may be unboundedly long and ambigu-
ous because of recursion through Nbar and PP.
Initial backoff grammar. The provided “S2”
grammar is designed to assign positive probability
to any string in E* (see §1.1). At least initially, this
PCFG generates only right-branching structures. Its
nonterminals correspond to the states of a weighted
finite-state machine, with start state S2 and one state
per element of T (the coarse parts of speech listed
above). Table 2 shows a simplified version.
From each state, transition into any state except
the start state S2 is permitted, and so is stopping.
These rules can be seen as specifying the transitions
Arthur is the king .
Arthur rides the horse near the castle .
riding to Camelot is hard .
do coconuts speak ?
what does Arthur ride ?
who does Arthur suggest she carry ?
are they suggesting Arthur ride to Camelot ?
Guinevere might have known.
it is Sir Lancelot who knows Zoot !
neither Sir Lancelot nor Guinevere will speak of it .
the Holy Grail was covered by a yellow fruit.
do not speak!
Arthur will have been riding for eight nights.
Arthur, sixty inches , is a tiny king .
Arthur and Guinevere migrate frequently .
he knows what they are covering with that story.
the king drank to the castle that was his home .
when the king drinks , Patsy drinks.
</bodyText>
<tableCaption confidence="0.992514">
Table 3: Example sentences. Only the first two can be
parsed by the initial S1 and lexical rules.
</tableCaption>
<bodyText confidence="0.999699555555556">
in a bigram hidden Markov model (HMM) on part-
of-speech tags, whose emissions are specified by the
lexical rules. Since each rule initially has weight 1,
all part-of-speech sequences of a given length are
equally likely, but these weights could be changed
to arbitrary transition probabilities.
Start rules. The initial grammar S1 and the ini-
tial backoff grammar S2 are tied together by a single
symbol START, which has two production rules:
</bodyText>
<equation confidence="0.514926">
99 START → S1
1 START → S2
</equation>
<bodyText confidence="0.994386125">
These two rules are obligatory, but their weights
may be changed. The resulting model, rooted at
START, is a mixture of the S1 and S2 grammars,
where the weights of these two rules implement
the mixture coefficients. This is a simple form of
backoff smoothing by linear interpolation (Jelinek
and Mercer, 1980). The teams are warned to pay
special attention to these rules. If the weight of
START → S1 is decreased relative to START →
S2, then the model relies more heavily on the back-
off model—perhaps a wise choice for keeping cross-
entropy small, if the team has little faith in S1’s abil-
ity to parse the forthcoming data.
Sample sentences. A set of 27 example sentences
in E+ (subset shown in Table 3) is provided for lin-
guistic inspiration and as practice data on which to
</bodyText>
<page confidence="0.997855">
101
</page>
<bodyText confidence="0.9999162">
run the parser. Since only 2 of these sentences can
be parsed with the initial S1 and lexical rules, there
is plenty of room for improvement. A further devel-
opment set is provided midway through the exercise
(§4.3.2).
</bodyText>
<subsectionHeader confidence="0.99822">
4.2 Computing Environment
</subsectionHeader>
<bodyText confidence="0.999978">
We now describe how the above data are made avail-
able to students along with some software.
</bodyText>
<subsectionHeader confidence="0.864044">
4.2.1 Scripts
</subsectionHeader>
<bodyText confidence="0.999763434782609">
We provide scripts that implement two important
capabilities for PCFG development. Both scripts
are invoked with a set of grammar files specified on
the command line (typically all of them, “*.gr”).
A PCFG is obtained by concatenating these files
and stripping their comments, then normalizing their
rule weights into probabilities (see Table 1), and
finally checking that all terminal symbols of this
PCFG are legal words of the vocabulary E.
The random generation script prints a sample of
n sentences from this PCFG. The generator can op-
tionally print trees or flat word sequences. A start
symbol other than the default S1 may be specified
(e.g., NP, S2, START, etc.), to allow participants to
test subgrammars or the backoff grammar.5
The parsing script prints the most probable parse
tree for each sentence read from a file (or from the
standard input). A start symbol may again be speci-
fied; this time the default is START. The parser also
prints each sentence’s probability, total number of
parses, and the fraction of the probability that goes
to the best parse.
Tree outputs can be pretty-printed for readability.
</bodyText>
<subsectionHeader confidence="0.881282">
4.2.2 Collaborative Setup
</subsectionHeader>
<bodyText confidence="0.999609111111111">
Teams of three or four sit at adjacent workstations
with a shared filesystem. The scripts above are pub-
licly installed; a handout gives brief usage instruc-
tions. The instructor and teaching assistant roam the
room and offer assistance as needed.
Each team works in its own shared directory. The
Emacs editor will warn users who are simultane-
ously editing the same file. Individual participants
tend to work on different sub-grammar files; all of
</bodyText>
<footnote confidence="0.954804666666667">
5For some PCFGs, the stochastic process implemented by
the script has a nonzero probability of failing to terminate. This
has not caused problems to date.
</footnote>
<bodyText confidence="0.999242">
a team’s files can be concatenated (as *.gr) when
the scripts are run. (The directory initially includes
separate files for the S1 rules, S2 rules, and lexi-
cal rules.) To avoid unexpected interactions among
these grammar fragments, students are advised to di-
vide work based on nonterminals; e.g., one member
of a team may claim jurisdiction over all rules of the
form VP plural —* · · ·.
</bodyText>
<subsectionHeader confidence="0.9790555">
4.3 Activities
4.3.1 Introductory Lecture
</subsectionHeader>
<bodyText confidence="0.999994826086957">
Once students have formed themselves into teams
and managed to log in at adjacent computers, we
begin with an 30-minute introductory lecture. No
background is assumed. We explain PCFGs simply
by showing the S1 grammar and hand-simulating the
action of the random sentence generator.
We explain the goal of extending the S1 gram-
mar to cover more of English. We explain how each
team’s precision will be evaluated by human judg-
ments on a sample, but point out that this measure
gives no incentive to increase coverage (recall). This
motivates the “Boggle” aspect of the game, where
teams must also be able to parse one another’s gram-
matical sentences, and indeed assign them as high
a probability as possible. We demonstrate how the
parser assigns a probability by running it on the sen-
tence that we earlier generated by hand.6
We describe how the parser’s probabilities are
turned into a cross-entropy measure, and discuss
strategy. Finally, we show that parsing a sentence
that is not covered by the S1 grammar will lead to
infinite cross-entropy, and we motivate the S2 back-
off grammar as an escape hatch.
</bodyText>
<subsectionHeader confidence="0.966619">
4.3.2 Midpoint: Development data
</subsectionHeader>
<bodyText confidence="0.999977625">
Once or more during the course of the exercise,
we take a snapshot of all teams’ S1 grammars and
sample 50 sentences from each. The resulting col-
lection of sentences, in random order, is made avail-
able to all teams as a kind of development data.
While we do not filter for grammaticality as in the
final evaluation, this gives all participants an idea
of what they will be up against when it comes time
</bodyText>
<footnote confidence="0.98968225">
6The probability will be tiny, as a product of many rule prob-
abilities. But it may be higher than expected, and students are
challenged to guess why: there are additional parses beyond the
one we hand-generated, and the parser sums over all of them.
</footnote>
<page confidence="0.997729">
102
</page>
<bodyText confidence="0.998931333333333">
to parse other teams’ sentences. Teams are on their
honor not to disguise the true state of their grammar
at the time of the snapshot.
</bodyText>
<subsectionHeader confidence="0.921939">
4.3.3 Evaluation procedure
</subsectionHeader>
<bodyText confidence="0.999994">
Grammar development ends at an announced
deadline. The grammars are now evaluated on the
two measures discussed in §3. The instructors run a
few scripts that handle most of this work.
First, we generate a collection C by sampling 20
sentences from each team’s probabilistic grammar,
using S1 as the start symbol. (Thus, the backoff S2
grammar is not used for generation.)
We now determine, for each team, what fraction
of its 20-sentence sample was grammatical. The par-
ticipants play the role of grammaticality judges. In
our randomized double-blind procedure, each indi-
vidual judge receives (in his or her team directory)
a file of about 20 sentences from C, with instruc-
tions to delete the ungrammatical ones and save the
file, implying coarse Boolean grammaticality judg-
ments.7 The files are constructed so that each sen-
tence in C is judged by 3 different participants; a
sentence is considered grammatical if &gt; 2 judges
thinks that it is.
We define the test corpus C to consist of all sen-
tences in C that were judged grammatical. Each
team’s full grammar (using START as the start sym-
bol to allow backoff) is used to parse C. This
gives us the lo92-probability of each sentence in C;
the cross-entropy score is the sum of these lo92-
probabilities divided by the length of C.
</bodyText>
<subsectionHeader confidence="0.487085">
4.3.4 Group discussion
</subsectionHeader>
<bodyText confidence="0.9998836">
While the teaching assistant is running the evalua-
tion scripts and compiling the results, the instructor
leads a general discussion. Many topics are possi-
ble, according to the interests of the instructor and
participants. For example: What linguistic phenom-
ena did the teams handle, and how? Was the CFG
formalism adequately expressive? How well would
it work for languages other than English?
What strategies did the teams adopt, based on the
evaluation criteria? How were the weights chosen?
</bodyText>
<footnote confidence="0.6121956">
7Judges are on their honor to make fair judgments rather
than attempt to judge other teams’ sentences ungrammatical.
Moreover, such an attempt might be self-defeating, as they
might unknowingly be judging some of their own team’s sen-
tences ungrammatical.
</footnote>
<table confidence="0.999642166666667">
team precision cross-entropy new rules
(bits/sent.) lex. other
A 0.30 35.57 202 111
B 0.00 54.01 304 80
C 0.80 38.48 179 48
D 0.25 49.37 254 186
E 0.55 39.59 198 114
F 0.00 39.56 193 37
G 0.65 40.97 71 15
H 0.30 36.53 176 9
I 0.70 36.17 181 54
J 0.00 00 193 29
</table>
<tableCaption confidence="0.995559">
Table 4: Teams’ evaluation scores in one year, and the
</tableCaption>
<bodyText confidence="0.941181">
number of new rules (not including weight changes) that
they wrote. Only teams A and H modified the relative
weights of the START rules (they used 80/20 and 75/25,
respectively), giving them competitive perplexity scores.
(Cross-entropy in this year was approximated by an upper
bound that uses only the probability of each sentence’s
single best parse.)
How would you build a better backoff grammar?8
How would you organize a real long-term effort
to build a full English grammar? What would such a
grammar be good for? Would you use any additional
tools, data, or evaluation criteria?
</bodyText>
<sectionHeader confidence="0.999496" genericHeader="method">
5 Outcomes
</sectionHeader>
<bodyText confidence="0.99731255">
Table 4 shows scores achieved in one year (2002).
A valuable lesson for the students was the impor-
tance of backoff. None but the first two of the exam-
ple sentences (Table 3) are parseable with the small
S1 grammar. Thus, the best way to reduce perplexity
was to upweight the S2 grammar and perhaps spend
a little time improving its rules or weights. Teams
that spent all of their time on the S1 grammar may
have learned a lot about linguistics, but tended to
score poorly on perplexity.
Indeed, the winning team in a later year spent
nearly all of their effort on the S2 grammar. They
placed almost all their weight on the S2 grammar,
whose rules they edited and whose parameters they
estimated from the example sentences and develop-
ment data. As for their S1 grammar, it generated
only a small set of grammatical sentences with ob-
8E.g., training the model weights, extending it to trigrams,
or introducing syntax into the S2 model by allowing it to invoke
nonterminals of the S1 grammar.
</bodyText>
<page confidence="0.998032">
103
</page>
<bodyText confidence="0.999966025641026">
scure constructions that other teams were unlikely to
model well in their S1 grammars. This gave them a
100% precision score on grammaticality while pre-
senting a difficult parsing challenge to other teams.
This team gamed our scoring system, exploiting the
idiosyncrasy that S2 would be used to parse but not
to generate. (See §3 for an alternative system.)
We conducted a post hoc qualitative survey of the
grammars from teams in 2002. Teams were not
asked to provide comments, and nonterminal nam-
ing conventions often tend to be inscrutable, but the
intentions are mostly understandable. All 10 teams
developed more fine-grained parts of speech, includ-
ing coordinating conjunctions, modal verbs, number
words, adverbs. 9 teams implemented singular and
plural features on nouns and/or verbs, and 9 imple-
mented the distinction between base, past, present,
and gerund forms of verbs (or a subset of those). 7
teams brought in other features like comparative and
superlative adjectives and personal vs. possessive
pronouns. 4 teams modeled pronoun case. Team
C created a “location” category.
7 teams explicitly tried to model questions, of-
ten including rules for do-support; 3 of those teams
also modeled negation with do-insertion. 2 teams
used gapped categories (team D used them exten-
sively), and 7 teams used explicit X nonterminals,
most commonly within noun phrases (following the
initial grammar). Three teams used a rudimentary
subcategorization frame model, distinguishing be-
tween sentence-taking, transitive, and intransitive
verbs, with an exploded set of production rules as
a result. Team D modeled appositives.
The amount of effort teams put into weights var-
ied, as well. Team A used 11 distinct weight values
from 1 to 80, giving 79 rules weights &gt; 1(next clos-
est was team 10, with 7 weight values in [1, 99] and
only 43 up-weighted rules). Most teams set fewer
than 25 rules’ weights to something other than 1.
</bodyText>
<sectionHeader confidence="0.802018" genericHeader="method">
6 Use as a Homework Assignment
</sectionHeader>
<bodyText confidence="0.999932413043479">
Two hours is not enough time to complete a good
grammar. Our participants are ambitious but never
come close to finishing what they undertake; Table 4
reflects incomplete work. Nonetheless, we believe
that the experience still successfully fulfills many of
the goals of §2–3 in a short time, and the participants
enjoy the energy in a roomful of peers racing toward
a deadline. The fact that the task is open-ended and
clearly impossible keeps the competition friendly.
An alternative would be to allot 2 weeks or more
as a homework assignment, allowing teams to go
more deeply into linguistic issues and/or backoff
modeling techniques. A team’s grade could be
linked to its performance. In this setting, we recom-
mend limiting the team size to 1 or 2 people each,
since larger teams may not be able to find time or
facilities to work side-by-side for long.
This homework version of our exercise might
helpfully be split into two assignments:
Part 1 (non-competitive, smaller vocabulary).
“Extend the initial S1 grammar to cover a certain
small set of linguistic phenomena, as illustrated by a
development set [e.g., Table 3]. You will be eval-
uated on the cross-entropy of your grammar on a
test set that closely resembles the development set
[see §3], and perhaps also on the acceptability of
sentences sampled from your grammar (as judged
by you, your classmates, or the instructor). You
will also receive qualitative feedback on how cor-
rectly and elegantly your grammar solves the lin-
guistic problems posed by the development set.”
Part 2 (competitive, full 220-word vocabulary).
“Extend your S1 grammar from Part 1 to generate
phenomena that stump other teams, and add an S2
grammar to avoid being stumped by them. You will
be evaluated as follows ... [see §4.3.3].”
We have already experimented with simpler non-
competitive grammar-writing exercises (similar to
Part 1) in our undergraduate NLP courses. Given
two weeks, even without teammates, many students
do a fine job of covering several non-trivial syntactic
phenomena. These assignments are available for use
by others (see §1.3). In some versions, students were
asked to write their own random generator, judge
their own sentences, explain how to evaluate per-
plexity, or guess why the S2 grammar was used.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998984">
We hope that other instructors can make use of these
materials or ideas. Our competitive PCFG-writing
game touches upon many core CL concepts, is chal-
lenging and enjoyable, allows collaboration, and is
suitable for cross-disciplinary and intro courses.
</bodyText>
<page confidence="0.998669">
104
</page>
<sectionHeader confidence="0.995888" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999636">
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data. In
Proc. of Workshop on Pattern Recognition in Practice.
D. Jurafsky and J.H. Martin. 2000. Speech and Lan-
guage Processing. Prentice Hall.
</reference>
<page confidence="0.999016">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.269023">
<title confidence="0.994298">Grammar</title>
<author confidence="0.997754">Jason</author>
<affiliation confidence="0.7770625">Department of Computer Johns Hopkins</affiliation>
<address confidence="0.995665">Baltimore, MD 21218,</address>
<email confidence="0.999807">jason@cs.jhu.edu</email>
<intro confidence="0.491973">Abstract</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proc. of Workshop on Pattern Recognition in Practice.</booktitle>
<contexts>
<context position="20844" citStr="Jelinek and Mercer, 1980" startWordPosition="3322" endWordPosition="3325">all part-of-speech sequences of a given length are equally likely, but these weights could be changed to arbitrary transition probabilities. Start rules. The initial grammar S1 and the initial backoff grammar S2 are tied together by a single symbol START, which has two production rules: 99 START → S1 1 START → S2 These two rules are obligatory, but their weights may be changed. The resulting model, rooted at START, is a mixture of the S1 and S2 grammars, where the weights of these two rules implement the mixture coefficients. This is a simple form of backoff smoothing by linear interpolation (Jelinek and Mercer, 1980). The teams are warned to pay special attention to these rules. If the weight of START → S1 is decreased relative to START → S2, then the model relies more heavily on the backoff model—perhaps a wise choice for keeping crossentropy small, if the team has little faith in S1’s ability to parse the forthcoming data. Sample sentences. A set of 27 example sentences in E+ (subset shown in Table 3) is provided for linguistic inspiration and as practice data on which to 101 run the parser. Since only 2 of these sentences can be parsed with the initial S1 and lexical rules, there is plenty of room for </context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proc. of Workshop on Pattern Recognition in Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2000</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="9269" citStr="Jurafsky and Martin, 2000" startWordPosition="1425" endWordPosition="1428"> helpful way to observe the generative capacity and qualitative behavior of any model. Of course, in practice a generative grammar is most often run “backwards” to parse an observed sentence or score its inside probability, and we also give the teams a script to do that. Most teams do actually run these scripts repeatedly to test their grammars, since both scripts will be central in the evaluation (where sentences are randomly generated from one grammar and scored with another grammar). It is common for instructors of NLP to show examples of randomly-generated text from an n-gram model (e.g., Jurafsky and Martin, 2000, pp. 202– 203), yet this amusing demonstration may be misinterpreted as merely illustrating the inadequacy of ngram models. Our use of a hand-crafted PCFG combined with a bigram-based (HMM) backoff grammar demonstrates that although the HMM is much worse at generating valid English sentences (precision), it is much better at robustly assigning nonzero probability when analyzing English sentences (recall). Finally, generative models do more than assign probability. They often involve linguistically meaningful latent variables, which can be recovered given the observed data. Parsing with an app</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky and J.H. Martin. 2000. Speech and Language Processing. Prentice Hall.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>