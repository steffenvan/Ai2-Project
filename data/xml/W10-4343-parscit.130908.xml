<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.432612">
<title confidence="0.986395">
Cross-Domain Speech Disfluency Detection
</title>
<author confidence="0.997324">
Kallirroi Georgila, Ning Wang, Jonathan Gratch
</author>
<affiliation confidence="0.987739">
Institute for Creative Technologies, University of Southern California
</affiliation>
<address confidence="0.939738">
12015 Waterfront Drive, Playa Vista, CA 90094, USA
</address>
<email confidence="0.999824">
{kgeorgila,nwang,gratch}@ict.usc.edu
</email>
<sectionHeader confidence="0.994825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996955">
We build a model for speech disfluency
detection based on conditional random
fields (CRFs) using the Switchboard cor-
pus. This model is then applied to a
new domain without any adaptation. We
show that a technique for detecting speech
disfluencies based on Integer Linear Pro-
gramming (ILP) (Georgila, 2009) signifi-
cantly outperforms CRFs. In particular, in
terms of F-score and NIST Error Rate the
absolute improvement of ILP over CRFs
exceeds 20% and 25% respectively. We
conclude that ILP is an approach with
great potential for speech disfluency detec-
tion when there is a lack or shortage of in-
domain data for training.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998698118644068">
Speech disfluencies (also known as speech re-
pairs) occur frequently in spontaneous speech and
can pose difficulties to natural language process-
ing (NLP) since most NLP tools (e.g. parsers and
part-of-speech taggers) are traditionally trained on
written language. However, speech disfluencies
are not noise. They are an integral part of how
humans speak, may provide valuable information
about the speaker’s cognitive state, and can be crit-
ical for successful turn-taking (Shriberg, 2005).
Speech disfluencies have been the subject of much
research in the field of spoken language process-
ing, e.g. (Ginzburg et al., 2007).
Speech disfluencies can be divided into three
intervals, the reparandum, the editing term, and
the correction (Heeman and Allen, 1999; Liu et
al., 2006). In the example below, “it left” is the
reparandum (the part that will be repaired), “I
mean” is the editing term, and “it came” is the cor-
rection:
(it left) * (I mean) it came
The asterisk marks the interruption point at
which the speaker halts the original utterance in
order to start the repair. The editing term is op-
tional and consists of one or more filled pauses
(e.g. uh, um) or discourse markers (e.g. you know,
well). Our goal here is to automatically detect rep-
etitions (the speaker repeats some part of the ut-
terance), revisions (the speaker modifies the orig-
inal utterance), or restarts (the speaker abandons
an utterance and starts over). We also deal with
complex disfluencies, i.e. a series of disfluencies
in succession (“it it was it is sounds great”).
In previous work many different approaches to
detecting speech disfluencies have been proposed.
Different types of features have been used, e.g.
lexical features only, acoustic and prosodic fea-
tures only, or a combination of both (Liu et al.,
2006). Furthermore, a number of studies have
been conducted on human transcriptions while
other efforts have focused on detecting disfluen-
cies from the speech recognition output.
In our previous work (Georgila, 2009), we pro-
posed a novel two-stage technique for speech dis-
fluency detection based on Integer Linear Pro-
gramming (ILP). ILP has been applied success-
fully to several NLP problems, e.g. (Clarke
and Lapata, 2008). In the first stage of our
method, we trained state-of-the-art classifiers for
speech disfluency detection, in particular, Hidden-
Event Language Models (HELMs) (Stolcke and
Shriberg, 1996), Maximum Entropy (ME) mod-
els (Ratnaparkhi, 1998), and Conditional Random
Fields (CRFs) (Lafferty et al., 2001). Then in
the second stage and during testing, each classifier
proposed possible labels which were then assessed
in the presence of local and global constraints us-
ing ILP. These constraints are hand-crafted and en-
code common disfluency patterns. ILP makes the
</bodyText>
<subsubsectionHeader confidence="0.677648">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237–240,
</subsubsectionHeader>
<affiliation confidence="0.890803">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999068">
237
</page>
<bodyText confidence="0.999993094339623">
final decision taking into account both the output
of the classifier and the constraints. Our approach
is similar to the work of (Germesin et al., 2008) in
the sense that they also combine machine learning
with hand-crafted rules. However, we use differ-
ent machine learning techniques and ILP.
When we evaluated this approach on the
Switchboard corpus (available from LDC and
manually annotated with disfluencies) using lex-
ical features, we found that ILP significantly im-
proves the performance of HELMs and ME mod-
els with negligible cost in processing time. How-
ever, the improvement of ILP over CRFs was only
marginal. These results were achieved when each
classifier was trained on approx. 35,000 occur-
rences of disfluencies. Then we experimented
with varying training set sizes in Switchboard. As
soon as we started reducing the amount of data for
training the classifiers, the improvement of ILP
over CRFs rose and became very significant, ap-
prox. 4% absolute reduction of error rate with 25%
of the training set (approx. 9,000 occurrences of
disfluencies) (Georgila, 2009). This result showed
that ILP is particularly helpful when there is no
much training data available.
However, Switchboard is a unique corpus be-
cause the amount of disfluencies that it contains
is very large. Thus even 25% of our training set
contains more disfluencies than a typical corpus
of human-human or human-machine interactions.
In this paper, we investigate what happens when
we move to a new domain when there is no in-
domain data annotated with disfluencies to be used
for training. This is usually the case when we start
developing a dialogue system in a new domain,
when the system has not been fully implemented
yet, and thus no data from users interacting with
the system has been collected. Since the improve-
ment of ILP over HELMs and ME models was
very large even when the models were both trained
and tested on Switchboard (approx. 15% and 20%
absolute reduction of error rate when 100% and
25% of the training set was used for training the
classifiers respectively (Georgila, 2009)), in this
paper we focus only on comparing CRFs versus
CRFs+ILP. Our goal is to evaluate if and how
much ILP improves CRFs in the case that no train-
ing data is available at all.
The structure of the paper is as follows: In sec-
tion 2 we describe our data sets. In section 3 we
concisely describe our approach. Then in section 4
we present our experiments. Finally in section 5
we present our conclusion.
</bodyText>
<sectionHeader confidence="0.934421" genericHeader="method">
2 Data Sets
</sectionHeader>
<bodyText confidence="0.998863274509804">
To train our classifiers we use Switchboard (avail-
able from LDC), which is manually annotated
with disfluencies, and is traditionally used for
speech disfluency experiments. We transformed
the Switchboard annotations into the following
format:
it BE was IE a IP it was good
BE (beginning of edit) is the point where the
reparandum starts and IP is the interruption point
(the point before the repair starts). In the above
example the beginning of the reparandum is the
first occurrence of “it”, the interruption point ap-
pears after “a”, and every word between BE and
IP is tagged as IE (inside edit). Sometimes BE
and IP occur at the same point, e.g. “it BE-IP it
was”. In (Georgila, 2009) we divided Switchboard
into training, development, and test sets. Here we
use the same training and development sets as in
(Georgila, 2009) containing 34,387 occurrences of
BE labels and 39,031 occurrences of IP labels, and
3,146 occurrences of BE labels and 3,499 occur-
rences of IP labels, respectively.
We test our approach on a smaller corpus col-
lected in the framework of the Rapport project
(Gratch et al., 2007). The goal of the Rap-
port project is to study how rapport is achieved
in human-human and human-machine interaction.
By rapport we mean the harmony, fluidity, syn-
chrony and flow that someone feels when they are
engaged in a good conversation.
The Rapport agent is a virtual human designed
to elicit rapport from human participants within
the confines of a dyadic narrative task (Gratch et
al., 2007). In this setting, a speaker narrates some
previously observed series of events, i.e. the events
in a sexual harassment awareness and prevention
video, and the events in a video of the Tweety
cartoon. The central challenge for the Rapport
agent is to provide the non-verbal listening feed-
back associated with rapportful interaction (e.g.
head nods, postural mirroring, gaze shifts, etc.).
Our ultimate goal is to investigate possible cor-
relations between disfluencies and these types of
feedback.
We manually annotated 70 sessions of the Rap-
port corpus with disfluencies using the labels de-
scribed above (BE, IP, IE and BE-IP). In each ses-
sion the speaker narrates the events of one video.
These annotated sessions served as our reference
data set (gold-standard), which contained 738 and
865 occurrences of BE and IP labels respectively.
</bodyText>
<page confidence="0.995313">
238
</page>
<sectionHeader confidence="0.998983" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99998605">
In the first stage we train our classifier. Any clas-
sifier can be used as long as it provides more than
one possible answer (i.e. tag) for each word in the
utterance. Valid tags are BE, BE-IP, IP, IE or O.
The O tag indicates that the word is outside the
disfluent part of the utterance. ILP will be applied
to the output of the classifier during testing.
Let N be the number of words of each utter-
ance and i the location of the word in the utterance
(i=1,...,N). Also, let CBE(i) be a binary variable
(1 or 0) for the BE tag. Its value will be determined
by ILP. If it is 1 then the word will be tagged as
BE. In the same way, we use CBE−IP(i), CIP(i),
CIE(i), CO(i) for tags BE-IP, IP, IE and O re-
spectively. Let PBE(i) be the probability given by
the classifier that the word is tagged as BE. In the
same way, let PBE−IP(i), PIP (i), PIE(i), PO(i)
be the probabilities for tags BE-IP, IP, IE and O
respectively. Given the above definitions, the ILP
problem formulation can be as follows:
</bodyText>
<equation confidence="0.9989515">
max[E&apos;&apos; ��1[PBE(i)CBE(i) + PBE−IP (i)CBE−IP (i)
+PIP (i)CIP (i) + PIE(i)CIE(i) + PO(i)CO(i)11
(1)
subject to constraints, e.g.:
CBE(i) +CBE−IP(i) + CIP(i) + CIE(i) (2)
+CO(i) = 1 `di E (1 ... N)
</equation>
<bodyText confidence="0.999620693877551">
Equation 1 is the linear objective function that
we want to maximize, i.e. the overall probability
of the utterance. Equation 2 says that each word
can have one tag only. In the same way, we can
define constraints on which labels are allowed at
the start and end of an utterance. There are also
some constraints that define the transitions that are
allowed between tags. For example, IP cannot
follow an O directly, which means that we can-
not start a disfluency with an IP. There has to be
a BE after O and before IP. Details are given in
(Georgila, 2009).
We also formulate some additional rules that
encode common disfluency patterns. The idea
here is to generalize from these patterns. Be-
low is an example of a long-context rule. If we
have the sequence of words “she was trying to
well um she was talking to a coworker”, we ex-
pect this to be tagged as “she BE was IE try-
ing IE to IP well O um O she O was O talk-
ing O to O a O coworker O”, if we do not take
into account the context in which this pattern oc-
curs. Basically the pattern here is that two se-
quences of four words separated by a discourse
marker (“well”) and a filled pause (“um”) differ
only in their third word. That is, “trying” and
“talking” are different words but have the same
part-of-speech tag (gerund). We incorporate this
rule into our ILP problem formulation as follows:
Let (w1,...,wN) be a sequence of N words where
both w3 and wN−3 are verbs (gerund), the word
sequence w1,w2,w4 is the same as the sequence
wN−5,wN−4,wN−2, and all the words in between
(w5,...,wN−6) are filled pauses or discourse mark-
ers. Then the probabilities given by the classi-
fier are modified as follows: PBE(1)=PBE(1)+b1,
PIE(2)=PIE(2)+b2, PIE(3)=PIE(3)+b3, and
PIP(4)=PIP(4)+b4, where b1, b2, b3 and b4 are
empirically set boosting paremeters with values
between 0.5 and 1 computed using our Switch-
board development set. We use more complex
rules to cover cases such as “she makes he doesn’t
make”, and boost the probabilities that this is
tagged as “she BE makes IP he O doesn’t O make
O”.
In total we apply 17 rules and each rule can have
up to 5 more specific sub-rules. The largest con-
text that we take into account is 10 words, not in-
cluding filled pauses and discourse markers.
</bodyText>
<sectionHeader confidence="0.998673" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99966362962963">
For building the CRF model we use the CRF++
toolkit (available from sourceforge). We
used only lexical features, i.e. words and part-of-
speech (POS) tags. Switchboard includes POS
information but to annotate the Rapport corpus
with POS labels we used the Stanford POS tag-
ger (Toutanova and Manning, 2000). We ex-
perimented with different sets of features and
we achieved the best results with the follow-
ing setup (i is the location of the word or POS
in the sentence): Our word features are hwii,
hwi+1i, hwi−1, wii, hwi, wi+1i, hwi−2, wi−1, wii,
hwi, wi+1, wi+2i. Our POS features have the
same structure as the word features. For ILP we
use the lp solve software also available from
sourceforge. We train on Switchboard and
test on the Rapport corpus.
For evaluating the performance of our models
we use standard metrics proposed in the litera-
ture, i.e. Precision, Recall, F-score, and NIST Er-
ror Rate. We report results for BE and IP. F-score
is the harmonic mean of Precision and Recall (we
equally weight Precision and Recall). Precision is
the ratio of the correctly identified tags X to all the
tags X detected by the model (where X is BE or
IP). Recall is the ratio of the correctly identified
tags X to all the tags X that appear in the reference
</bodyText>
<page confidence="0.995053">
239
</page>
<table confidence="0.998404625">
BE
Prec Rec F-score Error
CRF 74.52 36.45 48.95 73.44
CRF+ILP 77.44 64.63 70.46 47.56
IP
Prec Rec F-score Error
CRF 86.36 41.73 56.27 64.62
CRF+ILP 88.75 72.95 80.08 35.61
</table>
<tableCaption confidence="0.999867">
Table 1: Comparative results between our models.
</tableCaption>
<bodyText confidence="0.999439386363636">
utterance. The NIST Error Rate is the sum of in-
sertions, deletions and substitutions divided by the
total number of reference tags (Liu et al., 2006).
Table 1 presents comparative results between
our models. As we can see, now the improve-
ment of ILP over CRFs is not marginal as in
Switchboard. In fact, in terms of F-score and
NIST Error Rate the absolute improvement of ILP
over CRFs exceeds 20% and 25% respectively.
The results are statistically significant (p&lt;10−8,
Wilcoxon signed-rank test). The main gain of ILP
comes from the large improvement in Recall. This
result shows that using ILP has great potential for
speech disfluency detection when there is a lack of
in-domain data for training, and when we use lex-
ical features and human transcriptions. Further-
more, the cost of applying ILP is negligible since
the process is fast and applied during testing.
Note that the improvement of ILP over CRFs is
significant even though the two corpora, Switch-
board and Rapport, differ in genre (conversation
versus narrative).
The reason for the large improvement of ILP
over CRFs is the fact that as explained above
ILP takes into account common disfluency pat-
terns and generalizes from them. CRFs can po-
tentially learn similar patterns from the data but
do not generalize that well. For example, if the
CRF model learns that “she she” is a repetition it
will not necessarily infer that any sequence of the
same two words is a repetition (e.g. “and and”).
Of course here, since we deal with human tran-
scriptions we do not worry about speech recogni-
tion errors. Preliminary results with speech recog-
nition output showed that ILP retains its advan-
tages but more modestly. In this case, when decid-
ing which boosting rules to apply, it makes sense
to consider speech recognition confidence scores
per word. For example, a possible repetition “to
to” could be the result of a misrecognition of “to
do”. But these types of problems also affect plain
CRFs, so in the end ILP is expected to continue
outperforming CRFs. This is one of the issues for
future work together with using prosodic features.
</bodyText>
<sectionHeader confidence="0.997707" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999987">
We built a model for speech disfluency detec-
tion based on CRFs using the Switchboard cor-
pus. This model was then applied to a new do-
main without any adaptation. We showed that a
technique for detecting speech disfluencies based
on ILP significantly outperforms CRFs. In partic-
ular, in terms of F-score and NIST Error Rate the
absolute improvement of ILP over CRFs exceeds
20% and 25% respectively. We conclude that ILP
is an approach with great potential for speech dis-
fluency detection when there is a lack or shortage
of in-domain data for training.
</bodyText>
<sectionHeader confidence="0.999037" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98145">
This work was sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM). The con-
tent does not necessarily reflect the position or the policy of
the Government, and no official endorsement should be in-
ferred.
</bodyText>
<sectionHeader confidence="0.997612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945142857143">
J. Clarke and M. Lapata. 2008. Global inference for sentence
compression: An integer linear programming approach.
Journal of Artificial Intelligence Research, 31:399–429.
K. Georgila. 2009. Using integer linear programming for
detecting speech disfluencies. In Proc. of NAACL.
S. Germesin, T. Becker, and P. Poller. 2008. Hybrid multi-
step disfluency detection. In Proc. of MLMI.
J. Ginzburg, R. Fern´andez, and D. Schlangen. 2007. Unify-
ing self- and other-repair. In Proc. of DECALOG.
J. Gratch, N. Wang, J. Gerten, E. Fast, and R. Duffy. 2007.
Creating rapport with virtual agents. In Proc. of Interna-
tional Conference on Intelligent Virtual Agents (IVA).
P. Heeman and J. Allen. 1999. Speech repairs, intonational
phrases and discourse markers: Modeling speakers’ ut-
terances in spoken dialogue. Computational Linguistics,
25:527–571.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. of ICML.
Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf,
and M. Harper. 2006. Enriching speech recognition with
automatic detection of sentence boundaries and disfluen-
cies. IEEE Trans. Audio, Speech and Language Process-
ing, 14(5):1526–1540.
A. Ratnaparkhi. 1998. Maximum Entropy Models for natural
language ambiguity resolution. Ph.D. thesis, University of
Pennsylvania.
E. Shriberg. 2005. Spontaneous speech: How people really
talk, and why engineers should care. In Proc. of Inter-
speech.
A. Stolcke and E. Shriberg. 1996. Statistical language mod-
eling for speech disfluencies. In Proc. of ICASSP.
K. Toutanova and C.D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-of-
speech tagger. In Proc. of EMNLP/VLC.
</reference>
<page confidence="0.997032">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928926">
<title confidence="0.99977">Cross-Domain Speech Disfluency Detection</title>
<author confidence="0.999451">Kallirroi Georgila</author>
<author confidence="0.999451">Ning Wang</author>
<author confidence="0.999451">Jonathan</author>
<affiliation confidence="0.999953">Institute for Creative Technologies, University of Southern</affiliation>
<address confidence="0.999358">12015 Waterfront Drive, Playa Vista, CA 90094,</address>
<abstract confidence="0.995871294117647">We build a model for speech disfluency detection based on conditional random fields (CRFs) using the Switchboard corpus. This model is then applied to a new domain without any adaptation. We show that a technique for detecting speech disfluencies based on Integer Linear Programming (ILP) (Georgila, 2009) significantly outperforms CRFs. In particular, in terms of F-score and NIST Error Rate the absolute improvement of ILP over CRFs exceeds 20% and 25% respectively. We conclude that ILP is an approach with great potential for speech disfluency detection when there is a lack or shortage of indomain data for training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>31--399</pages>
<contexts>
<context position="3104" citStr="Clarke and Lapata, 2008" startWordPosition="486" endWordPosition="489">s to detecting speech disfluencies have been proposed. Different types of features have been used, e.g. lexical features only, acoustic and prosodic features only, or a combination of both (Liu et al., 2006). Furthermore, a number of studies have been conducted on human transcriptions while other efforts have focused on detecting disfluencies from the speech recognition output. In our previous work (Georgila, 2009), we proposed a novel two-stage technique for speech disfluency detection based on Integer Linear Programming (ILP). ILP has been applied successfully to several NLP problems, e.g. (Clarke and Lapata, 2008). In the first stage of our method, we trained state-of-the-art classifiers for speech disfluency detection, in particular, HiddenEvent Language Models (HELMs) (Stolcke and Shriberg, 1996), Maximum Entropy (ME) models (Ratnaparkhi, 1998), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Then in the second stage and during testing, each classifier proposed possible labels which were then assessed in the presence of local and global constraints using ILP. These constraints are hand-crafted and encode common disfluency patterns. ILP makes the Proceedings of SIGDIAL 2010: the 11th Ann</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>J. Clarke and M. Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research, 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
</authors>
<title>Using integer linear programming for detecting speech disfluencies.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="2898" citStr="Georgila, 2009" startWordPosition="454" endWordPosition="455">andons an utterance and starts over). We also deal with complex disfluencies, i.e. a series of disfluencies in succession (“it it was it is sounds great”). In previous work many different approaches to detecting speech disfluencies have been proposed. Different types of features have been used, e.g. lexical features only, acoustic and prosodic features only, or a combination of both (Liu et al., 2006). Furthermore, a number of studies have been conducted on human transcriptions while other efforts have focused on detecting disfluencies from the speech recognition output. In our previous work (Georgila, 2009), we proposed a novel two-stage technique for speech disfluency detection based on Integer Linear Programming (ILP). ILP has been applied successfully to several NLP problems, e.g. (Clarke and Lapata, 2008). In the first stage of our method, we trained state-of-the-art classifiers for speech disfluency detection, in particular, HiddenEvent Language Models (HELMs) (Stolcke and Shriberg, 1996), Maximum Entropy (ME) models (Ratnaparkhi, 1998), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Then in the second stage and during testing, each classifier proposed possible labels which w</context>
<context position="4966" citStr="Georgila, 2009" startWordPosition="776" endWordPosition="777">icantly improves the performance of HELMs and ME models with negligible cost in processing time. However, the improvement of ILP over CRFs was only marginal. These results were achieved when each classifier was trained on approx. 35,000 occurrences of disfluencies. Then we experimented with varying training set sizes in Switchboard. As soon as we started reducing the amount of data for training the classifiers, the improvement of ILP over CRFs rose and became very significant, approx. 4% absolute reduction of error rate with 25% of the training set (approx. 9,000 occurrences of disfluencies) (Georgila, 2009). This result showed that ILP is particularly helpful when there is no much training data available. However, Switchboard is a unique corpus because the amount of disfluencies that it contains is very large. Thus even 25% of our training set contains more disfluencies than a typical corpus of human-human or human-machine interactions. In this paper, we investigate what happens when we move to a new domain when there is no indomain data annotated with disfluencies to be used for training. This is usually the case when we start developing a dialogue system in a new domain, when the system has no</context>
<context position="7070" citStr="Georgila, 2009" startWordPosition="1144" endWordPosition="1145"> manually annotated with disfluencies, and is traditionally used for speech disfluency experiments. We transformed the Switchboard annotations into the following format: it BE was IE a IP it was good BE (beginning of edit) is the point where the reparandum starts and IP is the interruption point (the point before the repair starts). In the above example the beginning of the reparandum is the first occurrence of “it”, the interruption point appears after “a”, and every word between BE and IP is tagged as IE (inside edit). Sometimes BE and IP occur at the same point, e.g. “it BE-IP it was”. In (Georgila, 2009) we divided Switchboard into training, development, and test sets. Here we use the same training and development sets as in (Georgila, 2009) containing 34,387 occurrences of BE labels and 39,031 occurrences of IP labels, and 3,146 occurrences of BE labels and 3,499 occurrences of IP labels, respectively. We test our approach on a smaller corpus collected in the framework of the Rapport project (Gratch et al., 2007). The goal of the Rapport project is to study how rapport is achieved in human-human and human-machine interaction. By rapport we mean the harmony, fluidity, synchrony and flow that </context>
<context position="10484" citStr="Georgila, 2009" startWordPosition="1742" endWordPosition="1743">+CBE−IP(i) + CIP(i) + CIE(i) (2) +CO(i) = 1 `di E (1 ... N) Equation 1 is the linear objective function that we want to maximize, i.e. the overall probability of the utterance. Equation 2 says that each word can have one tag only. In the same way, we can define constraints on which labels are allowed at the start and end of an utterance. There are also some constraints that define the transitions that are allowed between tags. For example, IP cannot follow an O directly, which means that we cannot start a disfluency with an IP. There has to be a BE after O and before IP. Details are given in (Georgila, 2009). We also formulate some additional rules that encode common disfluency patterns. The idea here is to generalize from these patterns. Below is an example of a long-context rule. If we have the sequence of words “she was trying to well um she was talking to a coworker”, we expect this to be tagged as “she BE was IE trying IE to IP well O um O she O was O talking O to O a O coworker O”, if we do not take into account the context in which this pattern occurs. Basically the pattern here is that two sequences of four words separated by a discourse marker (“well”) and a filled pause (“um”) differ on</context>
</contexts>
<marker>Georgila, 2009</marker>
<rawString>K. Georgila. 2009. Using integer linear programming for detecting speech disfluencies. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Germesin</author>
<author>T Becker</author>
<author>P Poller</author>
</authors>
<title>Hybrid multistep disfluency detection.</title>
<date>2008</date>
<booktitle>In Proc. of MLMI.</booktitle>
<contexts>
<context position="4041" citStr="Germesin et al., 2008" startWordPosition="627" endWordPosition="630">cond stage and during testing, each classifier proposed possible labels which were then assessed in the presence of local and global constraints using ILP. These constraints are hand-crafted and encode common disfluency patterns. ILP makes the Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237–240, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 237 final decision taking into account both the output of the classifier and the constraints. Our approach is similar to the work of (Germesin et al., 2008) in the sense that they also combine machine learning with hand-crafted rules. However, we use different machine learning techniques and ILP. When we evaluated this approach on the Switchboard corpus (available from LDC and manually annotated with disfluencies) using lexical features, we found that ILP significantly improves the performance of HELMs and ME models with negligible cost in processing time. However, the improvement of ILP over CRFs was only marginal. These results were achieved when each classifier was trained on approx. 35,000 occurrences of disfluencies. Then we experimented wit</context>
</contexts>
<marker>Germesin, Becker, Poller, 2008</marker>
<rawString>S. Germesin, T. Becker, and P. Poller. 2008. Hybrid multistep disfluency detection. In Proc. of MLMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ginzburg</author>
<author>R Fern´andez</author>
<author>D Schlangen</author>
</authors>
<title>Unifying self- and other-repair.</title>
<date>2007</date>
<booktitle>In Proc. of DECALOG.</booktitle>
<marker>Ginzburg, Fern´andez, Schlangen, 2007</marker>
<rawString>J. Ginzburg, R. Fern´andez, and D. Schlangen. 2007. Unifying self- and other-repair. In Proc. of DECALOG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gratch</author>
<author>N Wang</author>
<author>J Gerten</author>
<author>E Fast</author>
<author>R Duffy</author>
</authors>
<title>Creating rapport with virtual agents.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Intelligent Virtual Agents (IVA).</booktitle>
<contexts>
<context position="7488" citStr="Gratch et al., 2007" startWordPosition="1211" endWordPosition="1214">f “it”, the interruption point appears after “a”, and every word between BE and IP is tagged as IE (inside edit). Sometimes BE and IP occur at the same point, e.g. “it BE-IP it was”. In (Georgila, 2009) we divided Switchboard into training, development, and test sets. Here we use the same training and development sets as in (Georgila, 2009) containing 34,387 occurrences of BE labels and 39,031 occurrences of IP labels, and 3,146 occurrences of BE labels and 3,499 occurrences of IP labels, respectively. We test our approach on a smaller corpus collected in the framework of the Rapport project (Gratch et al., 2007). The goal of the Rapport project is to study how rapport is achieved in human-human and human-machine interaction. By rapport we mean the harmony, fluidity, synchrony and flow that someone feels when they are engaged in a good conversation. The Rapport agent is a virtual human designed to elicit rapport from human participants within the confines of a dyadic narrative task (Gratch et al., 2007). In this setting, a speaker narrates some previously observed series of events, i.e. the events in a sexual harassment awareness and prevention video, and the events in a video of the Tweety cartoon. T</context>
</contexts>
<marker>Gratch, Wang, Gerten, Fast, Duffy, 2007</marker>
<rawString>J. Gratch, N. Wang, J. Gerten, E. Fast, and R. Duffy. 2007. Creating rapport with virtual agents. In Proc. of International Conference on Intelligent Virtual Agents (IVA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heeman</author>
<author>J Allen</author>
</authors>
<title>Speech repairs, intonational phrases and discourse markers: Modeling speakers’ utterances in spoken dialogue.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--527</pages>
<contexts>
<context position="1648" citStr="Heeman and Allen, 1999" startWordPosition="243" endWordPosition="246">ge processing (NLP) since most NLP tools (e.g. parsers and part-of-speech taggers) are traditionally trained on written language. However, speech disfluencies are not noise. They are an integral part of how humans speak, may provide valuable information about the speaker’s cognitive state, and can be critical for successful turn-taking (Shriberg, 2005). Speech disfluencies have been the subject of much research in the field of spoken language processing, e.g. (Ginzburg et al., 2007). Speech disfluencies can be divided into three intervals, the reparandum, the editing term, and the correction (Heeman and Allen, 1999; Liu et al., 2006). In the example below, “it left” is the reparandum (the part that will be repaired), “I mean” is the editing term, and “it came” is the correction: (it left) * (I mean) it came The asterisk marks the interruption point at which the speaker halts the original utterance in order to start the repair. The editing term is optional and consists of one or more filled pauses (e.g. uh, um) or discourse markers (e.g. you know, well). Our goal here is to automatically detect repetitions (the speaker repeats some part of the utterance), revisions (the speaker modifies the original utte</context>
</contexts>
<marker>Heeman, Allen, 1999</marker>
<rawString>P. Heeman and J. Allen. 1999. Speech repairs, intonational phrases and discourse markers: Modeling speakers’ utterances in spoken dialogue. Computational Linguistics, 25:527–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="3403" citStr="Lafferty et al., 2001" startWordPosition="528" endWordPosition="531">orts have focused on detecting disfluencies from the speech recognition output. In our previous work (Georgila, 2009), we proposed a novel two-stage technique for speech disfluency detection based on Integer Linear Programming (ILP). ILP has been applied successfully to several NLP problems, e.g. (Clarke and Lapata, 2008). In the first stage of our method, we trained state-of-the-art classifiers for speech disfluency detection, in particular, HiddenEvent Language Models (HELMs) (Stolcke and Shriberg, 1996), Maximum Entropy (ME) models (Ratnaparkhi, 1998), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Then in the second stage and during testing, each classifier proposed possible labels which were then assessed in the presence of local and global constraints using ILP. These constraints are hand-crafted and encode common disfluency patterns. ILP makes the Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237–240, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 237 final decision taking into account both the output of the classifier and the constraints. Our approach is similar </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>M Harper</author>
</authors>
<title>Enriching speech recognition with automatic detection of sentence boundaries and disfluencies.</title>
<date>2006</date>
<journal>IEEE Trans. Audio, Speech and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1667" citStr="Liu et al., 2006" startWordPosition="247" endWordPosition="250">e most NLP tools (e.g. parsers and part-of-speech taggers) are traditionally trained on written language. However, speech disfluencies are not noise. They are an integral part of how humans speak, may provide valuable information about the speaker’s cognitive state, and can be critical for successful turn-taking (Shriberg, 2005). Speech disfluencies have been the subject of much research in the field of spoken language processing, e.g. (Ginzburg et al., 2007). Speech disfluencies can be divided into three intervals, the reparandum, the editing term, and the correction (Heeman and Allen, 1999; Liu et al., 2006). In the example below, “it left” is the reparandum (the part that will be repaired), “I mean” is the editing term, and “it came” is the correction: (it left) * (I mean) it came The asterisk marks the interruption point at which the speaker halts the original utterance in order to start the repair. The editing term is optional and consists of one or more filled pauses (e.g. uh, um) or discourse markers (e.g. you know, well). Our goal here is to automatically detect repetitions (the speaker repeats some part of the utterance), revisions (the speaker modifies the original utterance), or restarts</context>
<context position="13810" citStr="Liu et al., 2006" startWordPosition="2332" endWordPosition="2335">cision and Recall). Precision is the ratio of the correctly identified tags X to all the tags X detected by the model (where X is BE or IP). Recall is the ratio of the correctly identified tags X to all the tags X that appear in the reference 239 BE Prec Rec F-score Error CRF 74.52 36.45 48.95 73.44 CRF+ILP 77.44 64.63 70.46 47.56 IP Prec Rec F-score Error CRF 86.36 41.73 56.27 64.62 CRF+ILP 88.75 72.95 80.08 35.61 Table 1: Comparative results between our models. utterance. The NIST Error Rate is the sum of insertions, deletions and substitutions divided by the total number of reference tags (Liu et al., 2006). Table 1 presents comparative results between our models. As we can see, now the improvement of ILP over CRFs is not marginal as in Switchboard. In fact, in terms of F-score and NIST Error Rate the absolute improvement of ILP over CRFs exceeds 20% and 25% respectively. The results are statistically significant (p&lt;10−8, Wilcoxon signed-rank test). The main gain of ILP comes from the large improvement in Recall. This result shows that using ILP has great potential for speech disfluency detection when there is a lack of in-domain data for training, and when we use lexical features and human tran</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Hillard, Ostendorf, Harper, 2006</marker>
<rawString>Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf, and M. Harper. 2006. Enriching speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Trans. Audio, Speech and Language Processing, 14(5):1526–1540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for natural language ambiguity resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3341" citStr="Ratnaparkhi, 1998" startWordPosition="521" endWordPosition="522">ave been conducted on human transcriptions while other efforts have focused on detecting disfluencies from the speech recognition output. In our previous work (Georgila, 2009), we proposed a novel two-stage technique for speech disfluency detection based on Integer Linear Programming (ILP). ILP has been applied successfully to several NLP problems, e.g. (Clarke and Lapata, 2008). In the first stage of our method, we trained state-of-the-art classifiers for speech disfluency detection, in particular, HiddenEvent Language Models (HELMs) (Stolcke and Shriberg, 1996), Maximum Entropy (ME) models (Ratnaparkhi, 1998), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Then in the second stage and during testing, each classifier proposed possible labels which were then assessed in the presence of local and global constraints using ILP. These constraints are hand-crafted and encode common disfluency patterns. ILP makes the Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237–240, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 237 final decision taking into account both the output o</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>A. Ratnaparkhi. 1998. Maximum Entropy Models for natural language ambiguity resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
</authors>
<title>Spontaneous speech: How people really talk, and why engineers should care.</title>
<date>2005</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="1380" citStr="Shriberg, 2005" startWordPosition="203" endWordPosition="204"> great potential for speech disfluency detection when there is a lack or shortage of indomain data for training. 1 Introduction Speech disfluencies (also known as speech repairs) occur frequently in spontaneous speech and can pose difficulties to natural language processing (NLP) since most NLP tools (e.g. parsers and part-of-speech taggers) are traditionally trained on written language. However, speech disfluencies are not noise. They are an integral part of how humans speak, may provide valuable information about the speaker’s cognitive state, and can be critical for successful turn-taking (Shriberg, 2005). Speech disfluencies have been the subject of much research in the field of spoken language processing, e.g. (Ginzburg et al., 2007). Speech disfluencies can be divided into three intervals, the reparandum, the editing term, and the correction (Heeman and Allen, 1999; Liu et al., 2006). In the example below, “it left” is the reparandum (the part that will be repaired), “I mean” is the editing term, and “it came” is the correction: (it left) * (I mean) it came The asterisk marks the interruption point at which the speaker halts the original utterance in order to start the repair. The editing t</context>
</contexts>
<marker>Shriberg, 2005</marker>
<rawString>E. Shriberg. 2005. Spontaneous speech: How people really talk, and why engineers should care. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Statistical language modeling for speech disfluencies.</title>
<date>1996</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="3292" citStr="Stolcke and Shriberg, 1996" startWordPosition="512" endWordPosition="515">oth (Liu et al., 2006). Furthermore, a number of studies have been conducted on human transcriptions while other efforts have focused on detecting disfluencies from the speech recognition output. In our previous work (Georgila, 2009), we proposed a novel two-stage technique for speech disfluency detection based on Integer Linear Programming (ILP). ILP has been applied successfully to several NLP problems, e.g. (Clarke and Lapata, 2008). In the first stage of our method, we trained state-of-the-art classifiers for speech disfluency detection, in particular, HiddenEvent Language Models (HELMs) (Stolcke and Shriberg, 1996), Maximum Entropy (ME) models (Ratnaparkhi, 1998), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Then in the second stage and during testing, each classifier proposed possible labels which were then assessed in the presence of local and global constraints using ILP. These constraints are hand-crafted and encode common disfluency patterns. ILP makes the Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237–240, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 237 fin</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>A. Stolcke and E. Shriberg. 1996. Statistical language modeling for speech disfluencies. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proc. of EMNLP/VLC.</booktitle>
<contexts>
<context position="12485" citStr="Toutanova and Manning, 2000" startWordPosition="2096" endWordPosition="2099">e makes he doesn’t make”, and boost the probabilities that this is tagged as “she BE makes IP he O doesn’t O make O”. In total we apply 17 rules and each rule can have up to 5 more specific sub-rules. The largest context that we take into account is 10 words, not including filled pauses and discourse markers. 4 Experiments For building the CRF model we use the CRF++ toolkit (available from sourceforge). We used only lexical features, i.e. words and part-ofspeech (POS) tags. Switchboard includes POS information but to annotate the Rapport corpus with POS labels we used the Stanford POS tagger (Toutanova and Manning, 2000). We experimented with different sets of features and we achieved the best results with the following setup (i is the location of the word or POS in the sentence): Our word features are hwii, hwi+1i, hwi−1, wii, hwi, wi+1i, hwi−2, wi−1, wii, hwi, wi+1, wi+2i. Our POS features have the same structure as the word features. For ILP we use the lp solve software also available from sourceforge. We train on Switchboard and test on the Rapport corpus. For evaluating the performance of our models we use standard metrics proposed in the literature, i.e. Precision, Recall, F-score, and NIST Error Rate. </context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova and C.D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-ofspeech tagger. In Proc. of EMNLP/VLC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>