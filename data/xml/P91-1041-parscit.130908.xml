<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.986531">
Quasi-Destructive Graph Unification
</title>
<author confidence="0.80751">
Hideto Tomabechi
</author>
<affiliation confidence="0.759377">
Carnegie Mellon University
</affiliation>
<address confidence="0.661818">
109 EDSH, Pittsburgh, PA 15213-3890
</address>
<email confidence="0.950687">
tomabech+@cs.cmu.edu
</email>
<note confidence="0.978949">
ATR Interpreting Telephony
Research Laboratories*
Seika-cho, Soralcugun, Kyoto 619-02 JAPAN
</note>
<sectionHeader confidence="0.958621" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999880444444444">
Graph unification is the most expensive part
of unification-based grammar parsing. It of-
ten takes over 90% of the total parsing time
of a sentence. We focus on two speed-up
elements in the design of unification algo-
rithms: 1) elimination of excessive copying
by only copying successful unifications, 2)
Finding unification failures as soon as possi-
ble. We have developed a scheme to attain
these two elements without expensive over-
head through temporarily modifying graphs
during unification to eliminate copying dur-
ing unification. We found that parsing rel-
atively long sentences (requiring about 500
top-level unifications during a parse) using
our algorithm is approximately twice as fast
as parsing the same sentences using Wrob-
lewski&apos;s algorithm.
</bodyText>
<sectionHeader confidence="0.986285" genericHeader="keywords">
1. Motivation
</sectionHeader>
<bodyText confidence="0.999953722222222">
Graph unification is the most expensive part of
unification-based grammar parsing systems. For ex-
ample, in the three types of parsing systems currently
used at ATR&apos;, all of which use graph unification algo-
rithms based on [Wroblewski, 1987], unification oper-
ations consume 85 to 90 percent of the total cpu time
devoted to a parse.2 The number of unification opera-
tions per sentence tends to grow as the grammar gets
larger and more complicated. An unavoidable paradox
is that when the natural language system gets larger
and the coverage of linguistic phenomena increases
the writers of natural language grammars tend to rely
more on deeper and more complex path equations (cy-
cles and frequent reentrancy) to lessen the complexity
of writing the grammar. As a result, we have seen that
the number of unification operations increases rapidly
as the coverage of the grammar grows in contrast to
the parsing algorithm itself which does not seem to
</bodyText>
<footnote confidence="0.981401714285714">
*Visiting Research Scientist. Local email address:
tomabech%atz-la.atr.co.jp@ uunet.UU.NET.
1The three parsing systems are based on: 1. Earley&apos;s
algorithm, 2. active chart parsing, 3. generalized LR parsing.
21n the large-scale HPSG-based spoken Japanese analy-
sis system developed at ATR, sometimes 98 percent of the
elapsed time is devoted to graph unification ([Kogure, 1990J).
</footnote>
<bodyText confidence="0.98848258974359">
grow so quickly. Thus, it makes sense to speed up
the unification operations to improve the total speed
performance of the natural language systems.
Our original unification algorithm was based on
[Wroblewski, 19871 which was chosen in 1988 as
the then fastest algorithm available for our applica-
tion (I-LPSG based unification grammar, three types of
parsers (Earley, Tomita-LR, and active chart), unifica-
tion with variables and cycles3 combined with &apos;Casper &apos;s
([Kasper, 19871) scheme for handling disjunctions. In
designing the graph unification algorithm, we have
made the following observation which influenced the
basic design of the new algorithm described in this
paper:
Unification does not always succeed.
As we will see from the data presented in a later section,
when our parsing system operates with a relatively
small grammar, about 60 percent of the unifications
attempted during a successful parse result in failure.
If a unification fails, any computation performed and
memory consumed during the unification is wasted. As
the grammar size increases, the number of unification
failures for each successful parse increases&apos;s. Without
completely rewriting the grammar and the parser, it
seems difficult to shift any significant amount of the
computational burden to the parser in order to reduce
the number of unification failuress.
Another problem that we would like to address in
our design, which seems to be well documented in the
existing literature is that:
Copying is an expensive operation.
The copying of a node is a heavy burden to the pars-
ing system. [Wroblewski, 1987] calls it a &amp;quot;computa-
tional sink&amp;quot;. Copying is expensive in two ways: 1) it
takes time; 2) it takes space. Copying takes time and
space essentially because the area in the random access
memory needs to be dynamically allocated which is an
expensive operation. [Godden, 19901 calculates the
computation time cost of copying to be about 67 per-
</bodyText>
<footnote confidence="0.992938285714286">
3Please refer to [Kogure, 1989] for trivial time modifica-
tion of Wroblewsld&apos;s algorithm to handle cycles.
4We estimate over 80% of unifications to be failures in
our large-scale speech-to-speech translation system under
development.
50f course, whether that will improve the overall perfor-
mance is another question.
</footnote>
<page confidence="0.998411">
315
</page>
<bodyText confidence="0.9999403125">
cent of the total parsing time in his TIME parsing sys-
tem. This time/space burden of copying is non-trivial
when we consider the fact that creation of unneces-
sary copies will eventually trigger garbage collections
more often (in a Lisp environment) which will also
slow down the overall performance of the parsing sys-
tem. In general, parsing systems are always short of
memory space (such as large LR tables of Tomita-LR
parsers and expanding tables and charts of Earley and
active chart parsers6), and the marginal addition or sub-
traction of the amount of memory space consumed by
other parts of the system often has critical effects on
the performance of these systems.
Considering the aforementioned problems, we pro-
pose the following principles to be the desirable con-
ditions for a fast graph unification algorithm:
</bodyText>
<listItem confidence="0.866986666666667">
• Copying should be performed only for success-
ful unifications.
• Unification failures should be found as soon as
possible.
By way of definition we would like to categorize ex-
cessive copying of dags into Over Copying and Early
Copying. Our definition of over copying is the same as
Wroblewski&apos;s; however, our definition of early copying
is slightly different.
• Over Copying: Two dags are created in order
to create one new dag. — This typically happens
when copies of two input dags are created prior
to a destructive unification operation to build one
new dag. aGodden, 19901 calls such a unifica-
tion: Eager Unification.). When two arcs point to
the same node, over copying is often unavoidable
with incremental copying schemes.
• Early Copying: Copies are created prior to the
failure of unification so that copies created since
the beginning of the unification up to the point of
failure are wasted.
</listItem>
<bodyText confidence="0.968347333333333">
Wroblewski defines Early Copying as follows: &amp;quot;The
argument dags are copied before unification started. If
the unification fails then some of the copying is wasted
effort&amp;quot; and restricts early copying to cases that only
apply to copies that are created prior to a unification.
Restricting early copying to copies that are made prior
to a unification leaves a number of wasted copies that
are created during a unification up to the point of failure
to be uncovered by either of the above definitions for
excessive copying. We would like Early Copying to
mean all copies that are wasted due to a unification fail-
ure whether these copies are created before or during
the actual unification operations.
Incremental copying has been accepted as an effec-
tive method of minimizing over copying and eliminat-
</bodyText>
<footnote confidence="0.68113">
6For example, our phoneme-based generalized LR parser
for speech input is always running on a swapping space be-
cause the LR table is too big.
</footnote>
<bodyText confidence="0.999955146341464">
ing early copying as defined by Wroblewski. How-
ever, while being effective in minimizing over copying
(it over copies only in some cases of convergent arcs
into one node), incremental copying is ineffective in
eliminating early copying as we define it.7 Incremen-
tal copying is ineffective in eliminating early copying
because when a graph unification algorithm recurses
for shared arcs (i.e. the arcs with labels that exist in
both input graphs), each created unification operation
recursing into each shared arc is independent of other
recursive calls into other arcs. In other words, the re-
cursive calls into shared arcs are non-deterministic and
there is no way for one particular recursion into a shared
arc to know the result of future recursions into other
shared arcs. Thus even if a particular recursion into
one arc succeeds (with minimum over copying and no
early copying in Wroblewski&apos;s sense), other arcs may
eventually fail and thus the copies that are created in
the successful arcs are all wasted. We consider it a
drawback of incremental copying schemes that copies
that are incrementally created up to the point of fail-
ure get wasted. This problem will be particularly felt
when we consider parallel implementations of incre-
mental copying algorithms. Because each recursion
into shared arcs is non-deterministic, parallel processes
can be created to work concurrently on all arcs. In each
of the parallelly created processes for each shared arc,
another recursion may take place creating more paral-
lel processes. While some parallel recursive call into
some arc may take time (due to a large number of sub-
arcs, etc.) another non-deterministic call to other arcs
may proceed deeper and deeper creating a large num-
ber of parallel processes. In the meantime, copies are
incrementally created at different depths of subgraphs
as long as the subgraphs of each of them are unified
successfully. This way, when a failure is finally de-
tected at some deep location in some subgraph, other
numerous processes may have created a large number
of copies that are wasted. Thus, early copying will be
a significant problem when we consider the possibility
of parallelizing the unification algorithms as well.
</bodyText>
<sectionHeader confidence="0.920738" genericHeader="introduction">
2. Our Scheme
</sectionHeader>
<bodyText confidence="0.997490769230769">
We would like to introduce an algorithm which ad-
dresses the criteria for fast unification discussed in the
previous sections. It also handles cycles without over
copying (without any additional schemes such as those
introduced by [Kogure, 1989]).
As a data structure, a node is represented with eight
fields: type, arc-list, comp-arc-list, forward, copy,
comp-arc-mark, forward-mark, and copy-mark. Al-
though this number may seem high for a graph node
data structure, the amount of memory consumed is
not significantly different from that consumed by other
7&apos;Early copying&apos; will henceforth be used to refer to early
copying as defined by us.
</bodyText>
<page confidence="0.996147">
316
</page>
<bodyText confidence="0.954705875">
algorithms. Type can be represented by three bits;
comp-arc-mark, forward-mark, and copy-mark can be
represented by short integers (i.e. fixnums); and comp-
arc-list (just like arc-list) is a mere collection of pointers
to memory locations. Thus this additional information
is trivial in terms of memory cells consumed and be-
cause of this data structure the unification algorithm
itself can remain simple.
</bodyText>
<table confidence="0.996423">
NODE 1
1 type
1 arc-list
comp-arc-list 1
1 forward 1
1 copy 1
1 comp-arc-mark 1 ARC
1 forward-mark 1 label 1
1 copy-mark 1 1 value 1
</table>
<figureCaption confidence="0.999117">
Figure 1: Node and Arc Structures
</figureCaption>
<bodyText confidence="0.996957620253164">
The representation for an arc is no different from that
of other unification algorithms. Each arc has two fields
for &apos;label&apos; and &apos;value&apos;. &apos;Label&apos; is an atomic symbol
which labels the arc, and &apos;value&apos; is a pointer to a node.
The central notion of our algorithm is the depen-
dency of the representational content on the global
timing clock (or the global counter for the current
generation of unification algorithms). This scheme
was used in [Wroblewski, 1987] to invalidate the copy
field of a node after one unification by incrementing a
global counter. This is an extremely cheap operation
but has the power to invalidate the copy fields of all
nodes in the system simultaneously. In our algorithm,
this dependency of the content of fields on global tim-
ing is adopted for arc lists, forwarding pointers, and
copy pointers. Thus any modification made, such as
adding forwarding links, copy links or arcs during one
top-level unification (unify0) to any node in memory
can be invalidated by one increment operation on the
global timing counter. During unification (in unify 1)
and copying after a successful unification, the global
timing ID fora specific field can be checked by compar-
ing the content of mark fields with the global counter
value and if they match then the content is respected;
if not it is simply ignored. Thus the whole operation is
a trivial addition to the original destructive unification
algorithm (Pereira&apos;s and Wroblewski&apos;s unify 1).
We have two kinds of arc lists 1) arc-list and comp-
arc-list. Arc-list contains the arcs that are permanent
(i.e., usual graph arcs) and comp-arc-list contains arcs
that are only valid during one graph unification oper-
ation. We also have two kinds of forwarding links,
i.e., permanent and temporary. A permanent forward-
ing link is the usual forwarding link found in other
algorithms ([Pereira, 1985], [Wroblewski, 1987], etc).
Temporary forwarding links are links that are only valid
during one unification. The currency of the temporary
links is determined by matching the content of the mark
field for the links with the global counter and if they
match then the content of this field is respected8. As
in [Pereira, 1985], we have three types of nodes: 1)
:atomic, 2) :bottom9, and 3) :complex. :atomic type
nodes represent atomic symbol values (such as Noun),
:bottom type nodes are variables and :complex type
nodes are nodes that have arcs coming out of them.
Arcs are stored in the arc-list field. The atomic value
is also stored in the arc-list if the node type is :atomic.
:bottom nodes succeed in unifying with any nodes and
the result of unification takes the type and the value
of the node that the :bottom node was unified with.
:atomic nodes succeed in unifying with :bottom nodes
or :atomic nodes with the same value (stored in the
arc-list). Unification of an :atomic node with a :com-
plex node immediately fails. :complex nodes succeed
in unifying with :bottom nodes or with :complex nodes
whose subgraphs all unify. Arc values are always nodes
and never symbolic values because the :atomic and
:bottom nodes may be pointed to by multiple arcs (just
as in structure sharing of :complex nodes) depending
on grammar constraints, and we do not want arcs to
contain terminal atomic values. Figure 2 is the cen-
tral quasi-destructive graph unification algorithm and
Figure 3 shows the algorithm for copying nodes and
arcs (called by unify0) while respecting the contents of
comp-arc-lists.
The functions Complementarcs(dg 1 ,dg2) and Inter-
sectarcs(dgl ,dg2) are similar to Wroblewski&apos;s algo-
rithm and return the set-difference (the arcs with la-
bels that exist in dg 1 but not in dg2) and intersec-
tion (the arcs with labels that exist both in dg 1 and
dg2) respectively. During the set-difference and set-
intersection operations, the content of comp-arc-lists
are respected as parts of arc lists if the comp-arc-
marks match the current value of the global timing
counter. Dereference-dg(dg) recursively traverses the
forwarding link to return the forwarded node. In do-
ing so, it checks the forward-mark of the node and
if the forward-mark value is 9 (9 represents a perma-
nent forwarding link) or its value matches the current
</bodyText>
<tableCaption confidence="0.625181571428571">
8We do not have a separate field for temporary forwarding
links; instead, we designate the integer value 9 to represent a
permanent forwarding link. We start incrementing the global
counter from 10 so whenever the forward-mark is not 9 the
integer value must equal the global counter value to respect
the forwarding link.
813ottom is called leaf in Pereira&apos;s algorithm.
</tableCaption>
<page confidence="0.995475">
317
</page>
<bodyText confidence="0.99900379661017">
value of *unify-global-counter*, then the function re-
turns the forwarded node; otherwise it simply returns
the input node. Forward(dgl, dg2, :forward-type) puts
(the pointer to) dg2 in the forward field of dgl. If
the keyword in the function call is :temporary, the cur-
rent value of the *unify-global-counter* is written in
the forward-mark field of dgl. If the keyword is :per-
manent, 9 is written in the forward-mark field of dgl.
Our algorithm itself does not require any permanent
forwarding; however, the functionality is added be-
cause the grammar reader module that reads the path
equation specifications into dg feature-structures uses
permanent forwarding to merge the additional gram-
matical specifications into a graph structure10. The
temporary forwarding links are necessary to handle
reentrancy and cycles. As soon as unification (at any
level of recursion through shared arcs) succeeds, a tem-
porary forwarding link is made from dg2 to dgl (dgl
to dg2 if dgl is of type :bottom). Thus, during unifi-
cation, a node already unified by other recursive calls
to unify I within the same unify() call has a temporary
forwarding link from dg2 to dgl (or dgl to dg2). As
a result, if this node becomes an input argument node,
dereferencing the node causes dgl and dg2 to become
the same node and unification immediately succeeds.
Thus a subgraph below an already unified node will not
be checked more than once even if an argument graph
has a cycle. Also, during copying done subsequently to
a successful unification, two arcs converging into the
same node will not cause over copying simply because
if a node already has a copy then the copy is returned.
For example, as a case that may cause over copies in
other schemes for dg2 convergent arcs, let us consider
the case when the destination node has a corresponding
node in dgl and only one of the convergent arcs has a
corresponding arc in dgl. This destination node is al-
ready temporarily forwarded to the node in dgl (since
the unification check was successful prior to copying).
Once a copy is created for the corresponding dgl node
and recorded in the copy field of dgl, every time a
convergent arc in dg2 that needs to be copied points
to its destination node, dereferencing the node returns
the corresponding node in dgl and since a copy of it
already exists, this copy is returned. Thus no duplicate
copy is created&amp;quot;.
10We have been using Wroblewski&apos;s algorithm for the uni-
fication part of the parser and thus usage of (permanent)
forwarding links is adopted by the grammar reader module
to convert path equations to graphs. For example, permanent
forwarding is done when a :bottom node is to be merged with
other nodes.
11Copying of dg2 arcs happens for arcs that exist in dg2
but not in dgl (i.e., Complementarcs(dg2,dg1)). Such arcs
are pushed to the comp-arc-list of dgl during unifyl and
are copied into the arc-list of the copy during subsequent
copying. If there is a cycle or a convergence in arcs in dgl or
in arcs in dg2 that do not have corresponding arcs in dgl, then
the mechanism is even simpler than the one discussed here.
A copy is made once, and the same copy is simply returned
</bodyText>
<equation confidence="0.669888704545455">
QUASI-DESTRUC11VE GRAPH UNIFICATION
FUNCTION unify -dg (dg 1,dg2);
result 4- catch with tag &apos;unify-fail
calling unify0(dgl,dg2);
increment *unify-global-counter*; ;; starts from 10 12
return(result);
END;
FUNCTION unify0(dg 1,dg2);
if &apos;*T* = unifyl(dgl,dg2); THEN
copy 4-- copy-dg-with-comp-arcs(dg1);
return(copy);
END;
FUNCTION unifyl (dgl-underef,dg2-underef);
dgl dereference-dg(dgl-underef);
dg2 4— dereference-dg(dg2-underef);
IF (dgl = dg2)13THEN
return(&apos;*T*);
ELSE IF (dgl.type = :bottom) THEN
forward-dg(dg 1 ,dg2,:temporazy);
retum(&apos;*T*);
ELSE IF (dg2.type = :bottom) THEN
forward-dg (dg2,dg 1 ,:temporary);
return(&apos;*T*);
ELSE IF (dgl.type = :atomic AND
dg2.type = :atomic) THEN
IF (dgl.arc-list = dg2.arc-fist)14THEN
forward-dg(dg2,dgl,:temporary);
retum(&apos;*T*);
ELSE throw15 with keyword &apos;unify-fail;
ELSE IF (dgl.type = :atomic OR
dg2.type = :atomic) THEN
throw with keyword &apos;unify-fail;
ELSE new 4— complementarcs(dg2,dg1);
shared intersectarcs(dgl,dg2);
FOR EACH arc IN shared DO
unifyl(destination of
the shared arc for dgl,
destination of
the shared arc for dg2);
forward-dg(dg2,dgl,:temporary);16
dgl.comp-arc-mark 4- * u nify -g lob al-cou nter* ;
dgl.comp-arc-list 4- new;
return (&apos;*T*);
END;
</equation>
<figureCaption confidence="0.999478">
Figure 2: The Q.D. Unification Functions
</figureCaption>
<bodyText confidence="0.675822">
every time another convergent arc points to the original node.
It is because arcs are copied only from either dgl or dg2.
</bodyText>
<footnote confidence="0.961104444444445">
129 indicates a permanent forwarding link.
13rq— ual in the &apos;eq&apos; sense. Because of forwarding and
cycles, it is possible that dgl and dg2 are &apos;eq&apos;.
14Arc-list contains atomic value if the node is of type
:atomic.
15Catch/throw construct; i.e., immediately return to unify-
dg.
16This will be executed only when all recursive calls into
unifyl succeeded. Otherwise, a failure would have caused
</footnote>
<page confidence="0.991151">
318
</page>
<table confidence="0.80738241025641">
QUASI-DESTRUCTIVE COPYING
FUNCTION copy-dg-with-comp-arcs(dg-underef);
dg 4— dereference-dg(dg-underef);
IF (dg.copy is non-empty AND
dg.copy-mark = *unify-global-counter*) THEN
retum(dg.copy);17
ELSE IF (dg.type = :atomic) THEN
copy 4— create-node0;18
copy.type 4— :atomic;
copy.arc-list dg.arc-list;
dg.copy 4— copy;
dg.copy-mark *unify-global-counter*;
retum(copy);
ELSE IF (dg.type = :bottom) THEN
copy create-node();
copy.type :bottom;
dg.copy 4— copy;
dg.copy-mark 4— *unify-global-counter*;
return(copy);
ELSE
copy 4.... create-node();
copy.type 4— :complex;
FOR ALL arc IN dg.arc-list DO
newarc 4— copy-arc-and-comp-arc(arc);
push newarc into copy.arc-list;
IF (dg.comp-arc-list is non-empty AND
dg.comp-arc-mark = *unify-global-counter*) THEN
FOR ALL comp-arc IN dg.comp-arc-list DO
newarc copy-arc-and-comp-arc(comp-arc);
push newarc into copy.arc-list;
dg.copy copy;
dg.copy-mark 4— *unify-global-counter*;
return (copy);
END;
FUNCTION copy-arc-and-comp-arcs(input-arc);
label 4— input-arc.label;
value copy-dg-with-comp-arcs(input-arc.value);
return a new arc with label and value;
END;
</table>
<figureCaption confidence="0.990561">
Figure 3: Node and Arc Copying Functions
</figureCaption>
<bodyText confidence="0.9663179375">
Figure 4 shows a simple example of quasi-
destructive graph unification with dg2 convergent arcs.
The round nodes indicate atomic nodes and the rect-
angular nodes indicate bottom (variable) nodes. First,
top-level unify! finds that each of the input graphs has
arc-a and arc-b (shared). Then unify 1 is recursively
called. At step two, the recursion into arc-a locally
succeeds, and a temporary forwarding link with time-
stamp(n) is made from node [12 to nodes. At the third
step (recursion into arc-b), by the previous forwarding,
node 02 already has the value s (by dereferencing).
Then this unification returns a success and a tempo-
rary forwarding link with time-stamp(n) is created from
an immediate return to unib-dg.
17I.e., the existing copy of the node.
18Creates an empty node structure.
node 111 to node s. At the fourth step, since all recur-
sive unifications (unify 1 s) into shared arcs succeeded,
top-level unify 1 creates a temporary forwarding link
with time-stamp(n) from dag2&apos;s root node to dag 1 &apos;s
root node, and sets arc-c (new) into comp-arc-list of
clag 1 and returns success (&apos;*T*). At the fifth step, a
copy of dagl is created respecting the content of comp-
arc-list and dereferencing the valid forward links. This
copy is returned as a result of unification. At the last
step (step six), the global timing counter is incremented
(n n+1). After this operation, temporary forwarding
links and comp-arc-lists with time-stamp (&lt; n+1) will
be ignored. Therefore, the original dag 1 and dag2 are
recovered in a constant time without a costly reversing
operations. (Also, note that recursions into shared-arcs
can be done in any order producing the same result).
</bodyText>
<figureCaption confidence="0.9020485">
Figure 4: A Simple Example of Quasi-Destructive
Graph Unification
</figureCaption>
<figure confidence="0.999033">
dag2
1
For each node with arc-a.
unifyl( s, [ ]2)
dag I
a
comp-arc-
b &apos;list(n)=(c)
t
dag2
forward(n)
copy-comp-arc-list(dag 1 )
copy of dagl (n) dag forward(n)
1
forward(n)
For each node with arc-b,
unifyl( [11, [ )2)
dag
1
fonprdW
forward(a)
dagl forward(n)
2
comp-arc-
b Nist(n)=( c)
1 6t
fonvard(a)
forward(n)
copy of dag I (a) dag I
t s
dag2
unify 1 (dag 1 ,dag 2) SHARED= (a , b)
da Al NEW=(c) Jag2
a
</figure>
<page confidence="0.997607">
319
</page>
<bodyText confidence="0.999711954545454">
As we just saw, the algorithm itself is simple. The &apos;Unifs&apos; represents the total number of unifications dur-
basic control structure of the unification is similar to ing a parse (the number of calls to the top-level &apos;unify-
Pereira&apos;s and Wroblewski&apos;s unify 1. The essential dif- dg&apos;, and not &apos;unify1&apos;). `I.JSrate&apos; represents the ratio
ference between our unifyl and the previous ones is of successful unifications to the total number of uni-
that our unify 1 is non-destructive. It is because the fications. We parsed each sentence three times on a
complementarcs(dg2,dg 1) are set to the comp-arc-list Symbolics 3620 using both unification methods and
of dg 1 and not into the arc-list of dgl • Thus, as soon took the shortest elapsed time for both methods (7&apos;
as we increment the global counter, the changes made represents our scheme, &apos;W&apos; represents Wroblewski&apos;s
to dg 1 (i.e., addition of complement arcs into comp- algorithm with a modification to handle cycles and
arc-list) vanish. As long as the comp-arc-mark value variables&amp;quot;). Data structures are the same for both uni-
matches that of the global counter the content of the fication algorithms (except for additional fields for a
comp-arc-list can be considered a part of arc-list and node in our algorithm, i.e., comp-arc-list, comp-arc-
therefore, dg 1 is the result of unification. Hence the mark, and forward-mark). Same functions are used to
name quasi-destructive graph unification. In order to interface with Earley&apos;s parser and the same subfunc-
create a copy for subsequent use we only need to make lions are used wherever possible (such as creation and
a copy of dg 1 before we increment the global counter access of arcs) to minimize the differences that are not
while respecting the content of the comp-arc-list of purely algorithmic. &apos;Number of copies&apos; represents the
dgl. number of nodes created during each parse (and does
Thus instead of calling other unification functions not include the number of arc structures that are cre-
(such as unify2 of Wroblewski) for incrementally cre- ated during a parse). &apos;Number of conses&apos; represents the
ating a copy node during a unification, we only need amount of structure words consed during a parse. This
to create a copy after unification. Thus, if unifica- number represents the real comparison of the amount
tion fails no copies are made at all (as in [ICarttunen, of space being consumed by each unification algorithm
1986]&apos;s scheme). Because unification that recurses (including added fields for nodes in our algorithm and
into shared arcs carries no burden of incremental copy- arcs that are created in both algorithms).
ing (i.e., it simply checks if nodes are compatible), as We used Earley&apos;s parsing algorithm for the experi-
the depth of unification increases (i.e., the graph gets ment. The Japanese grammar is based on HPSG anal-
larger) the speed-up of our method should get conspic- ysis ([Pollard and Sag, 1987]) covering phenomena
uous if a unification eventually fails. If all unifica- such as coordination, case adjunction, adjuncts, con-
tions during a parse are going to be successful, our trol, slash categories, zero-pronouns, interrogatives,
algorithm should be as fast as or slightly slower than WH constructs, and some pragmatics (speaker, hearer
Wroblewski&apos;s algorithm19. Since a parse that does not relations, politeness, etc.) ([Yoshimoto and Kogure,
fail on a single unification is unrealistic, the gain from 1989]). The grammar covers many of the important
our scheme should depend on the amount of unification linguistic phenomena in conversational Japanese. The
failures that occur during a unification. As the number grammar graphs which are converted from the path
of failures per parse increases and the graphs that failed equations contain 2324 nodes. We used 16 sentences
get larger, the speed-up from our algorithm should be- from a sample telephone conversation dialog which
come more apparent. Therefore, the characteristics of range from very short sentences (one word, i.e., iie
our algorithm seem desirable. In the next section, we `no&apos;) to relatively long ones (such as soredehakochi-
will see the actual results of experiments which com- rakarasochiranitourokuyoushiw000kuriitashimasu&apos;In
pare our unification algorithm to Wroblewski&apos;s algo- that case, we [speaker] will send you [hearer] the reg-
rithm (slightly modified to handle variables and cycles istration form.&apos;). Thus, the number of (top-level) uni-
that are required by our HPSG based grammar). fications per sentence varied widely (from 6 to over
500).
</bodyText>
<sectionHeader confidence="0.993953" genericHeader="method">
3. Experiments
</sectionHeader>
<bodyText confidence="0.916298333333333">
Table 1 shows the results of our experiments using an
HPSG-based Japanese grammar developed at AIR for
a conference registration telephone dialogue domain.
</bodyText>
<footnote confidence="0.829737">
191t may be slightly slower because our unification recurses
twice on a graph: once to unify and once to copy, whereas in
incremental unification schemes copying is performed dur-
ing the same recursion as unifying. Additional bookkeeping
for incremental copying and an additional set-difference op-
eration (i.e, complementarcs(dgl,dg2)) during unify2 may
offset this, however.
</footnote>
<bodyText confidence="0.931820230769231">
&amp;quot;Cycles can be handled in Wroblewski&apos;s algorithm by
checking whether an arc with the same label already exists
when arcs are added to a node. And if such an arc already
exists, we destructively unify the node which is the destina-
tion of the existing arc with the node which is the destination
of the arc being added. If such an arc does not exist, we
simply add the arc. ([Kogure, 1989]). Thus, cycles can be
handled very cheaply in Wroblewski&apos;s algorithm. Handling
variables in Wroblewski&apos;s algorithm is basically the same as
in our algorithm (i.e.. Pereira&apos;s scheme), and the addition of
this functionality can be ignored in terms of comparison to
our algorithm. Our algorithm does not require any additional
scheme to handle cycles in input dgs.
</bodyText>
<page confidence="0.988798">
320
</page>
<table confidence="0.999508111111111">
sent# Unifs USrate Elapsed time(sec) Num of Copies Num of Conses
T w T w T w
1 6 0.5 1.066 1.113 85 107 1231 1451
2 101 0.35 1.897 2.899 1418 2285 15166 23836
3 24 0.33 1.206 1.290 129 220 1734 2644
4 71 0.41 3.349 4.102 1635 2151 17133 22943
5 305 0.39 12.151 17.309 5529 9092 57405 93035
6 59 0.38 1.254 1.601 608 997 6873 10763
7 6 0.38 1.016 1.030 85 107 1175 1395
8 81 0.39 3.499 4.452 1780 2406 18718 24978
9 480 0.38 18.402 34.653 9466 15756 96985 167211
10 555 0.39 26.933 47.224 11789 18822 119629 189997
11 109 0.40 4.592 5.433 2047 2913 21871 30531
12 428 0.38 13.728 24.350 7933 13363 81536 135808
13 559 0.38 15.480 42.357 9976 17741 102489 180169
14 52 0.38 1.977 2.410 745 941 8272 10292
15 77 0.39 3.574 4.688 1590 2137 16946 22416
16 77 0.39 3.658 4.431 1590 2137 16943 22413
</table>
<tableCaption confidence="0.999978">
Table 1: Comparison of our algorithm with Wroblewski&apos;s
</tableCaption>
<sectionHeader confidence="0.996497" genericHeader="method">
4. Discussion: Comparison to Other
Approaches
</sectionHeader>
<bodyText confidence="0.998879586206897">
The control structure of our algorithm is identical to
that of [Pereira, 1985]. However, instead of stor-
ing changes to the argument dags in the environment
we store the changes in the dags themselves non-
destructively. Because we do not use the environment,
the log(d) overhead (where d is the number of nodes
in a dag) associated with Pereira&apos;s scheme that is re-
quired during node access (to assemble the whole dag
from the skeleton and the updates in the environment)
is avoided in our scheme. We share the principle of
storing changes in a restorable way with [Karttunen,
19861&apos;s reversible unification and copy graphs only
after a successful unification. Karttunen originally
introduced this scheme in order to replace the less
efficient structure-sharing implementations ([Pereira,
1985], [Karttunen and Kay, 1985]). In Karttunen&apos;s
method21, whenever a destructive change is about to
be made, the attribute value pairs22 stored in the body
of the node are saved into an array. The dag node struc-
ture itself is also saved in another array. These values
are restored after the top level unification is completed.
(A copy is made prior to the restoration operation if
the unification was a successful one.) The difference
between Karttunen&apos;s method and ours is that in our al-
gorithm, one increment to the global counter can invali-
date all the changes made to nodes, while in Karttunen&apos;s
algorithm each node in the entire argument graph that
has been destructively modified must be restored sep-
arately by retrieving the attribute-values saved in an
</bodyText>
<reference confidence="0.2314212">
2IThe discussion of Kartunnen&apos;s method is based on the D-
PATR implementation on Xerox 1100 machines ([Karttunen,
1986]).
221.e., arc structures: &apos;label&apos; and &apos;value&apos; pairs in our
vocabulary.
</reference>
<bodyText confidence="0.997732594594595">
array and resetting the values into the dag structure
skeletons saved in another array. In both Karttunen&apos;s
and our algorithm, there will be a non-destructive (re-
versible, and quasi-destructive) saving of intersection
arcs that may be wasted when a subgraph of a partic-
ular node successfully unifies but the final unification
fails due to a failure in some other part of the argument
graphs. This is nota problem in our method because the
temporary change made to a node is performed as push-
ing pointers into already existing structures (nodes) and
it does not require entirely new structures to be created
and dynamically allocated memory (which was neces-
sary for the copy (create-node) operation). 23 [Godden,
19901 presents a method of using lazy evaluation in
unification which seems to be one successful actual-
ization of [Karttunen and Kay, 19851&apos;s lazy evaluation
idea. One question about lazy evaluation is that the ef-
ficiency of lazy evaluation varies depending upon the
particular hardware and programming language envi-
ronment. For example, in CommonLisp, to attain a
lazy evaluation, as soon as a function is delayed, a clo-
sure (or a structure) needs to be created receiving a dy-
namic allocation of memory (just as in creating a copy
node). Thus, there is a shift of memory and associated
computation consumed from making copies to making
closures. In terms of memory cells saved, although
the lazy scheme may reduce the total number of copies
created, if we consider the memory consumed to create
closures, the saving may be significantly canceled. In
terms of speed, since delayed evaluation requires addi-
tional bookkeeping, how schemes such as the one in-
troduced by [Godden, 1990] would compare with non-
lazy incremental copying schemes is an open question.
Unfortunately Godden offers a comparison of his algo-
&amp;quot;Although, in Karttunen&apos;s method it may become rather
expensive if the arrays require resizing during the saving
operation of the subgraphs.
</bodyText>
<page confidence="0.995752">
321
</page>
<bodyText confidence="0.99989">
rithm with one that uses a full copying method (i.e. his
Eager Copying) which is already significantly slower
than Wroblewski&apos;s algorithm. However, no compari-
son is offered with prevailing unification schemes such
as Wroblewski&apos;s. With the complexity for lazy evalu-
ation and the memory consumed for delayed closures
added, it is hard to estimate whether lazy unification
runs considerably faster than Wroblewski&apos;s incremen-
tal copying scheme.24
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="method">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.970529936170213">
The algorithm introduced in this paper runs signifi-
cantly faster than Wroblewski&apos;s algorithm using Ear-
ley&apos;s parser and an HPSG based grammar developed
at ATR. The gain comes from the fact that our algo-
rithm does not create any over copies or early copies.
In Wroblewski&apos;s algorithm, although over copies are
essentially avoided, early copies (by our definition)
are a significant problem because about 60 percent of
unifications result in failure in a successful parse in
our sample parses. The additional set-difference oper-
ation required for incremental copying during unify2
may also be contributing to the slower speed of Wrob-
lewski&apos;s algorithm. Given that our sample grammar is
relatively small, we would expect that the difference
in the performance between the incremental copying
schemes and ours will expand as the grammar size
increases and both the number of failures 25 and the
size of the wasted subgraphs of failed unifications be-
come larger. Since our algorithm is essentially paral-
lel, parallelization is one logical choice to pursue fur-
ther speedup. Parallel processes can be continuously
created as unifyl recurses deeper and deeper without
creating any copies by simply looking for a possible
failure of the unification (and preparing for successive
copying in case unification succeeds). So far, we have
completed a preliminary implementation on a shared
memory parallel hardware with about 75 percent of
effective parallelization rate. With the simplicity of
our algorithm and the ease of implementing it (com-
pared to both incremental copying schemes and lazy
schemes), combined with the demonstrated speed of
the algorithm, the algorithm could be a viable alterna-
tive to existing unification algorithms used in current
2&apos;That is, unless some new scheme for reducing exces-
sive copying is introduced such as structure-sharing of an
unchanged shared-forest ([Kogure, 1990]). Even then, our
criticism of the cost of delaying evaluation would still be
valid. Also, although different in methodology from the way
suggested by Kogure for Wroblewski&apos;s algorithm, it is possi-
ble to attain structure-sharing of an unchanged forest in our
scheme as well. We have already developed a preliminary
version of such a scheme which is not discussed in this paper.
25For example, in our large-scale speech-to-speech trans-
lation system under development, the USrate is estimated to
be under 20%, i.e., over 80% of unifications are estimated to
be failures.
natural language systems.
</bodyText>
<sectionHeader confidence="0.999321" genericHeader="evaluation">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.997928125">
The author would like to thank Akira Kurematsu,
Tsuyoshi Morimoto, Hitoshi Iida, Osamu Furuse,
Masaald Nagata, Toshiyuki Takezawa and other mem-
bers of ATR and Masaru Tomita and Jaime Carbonell
at CMU. Thanks are also due to Margalit Zabludowski
and Hiroald Kitano for comments on the final version
of this paper and Takako Fujiolca for assistance in im-
plementing the parallel version of the algorithm.
</bodyText>
<sectionHeader confidence="0.985196" genericHeader="conclusions">
Appendix: Implementation
</sectionHeader>
<bodyText confidence="0.9998515">
The unification algorithms, Earley parser and the
HPSG path equation to graph converter programs are
implemented in CommonLisp on a Symbolics ma-
chine. The preliminary parallel version of our uni-
fication algorithm is currently implemented on a Se-
quent/Symmetry closely-coupled shared-memory par-
allel machine running Allegro CLiP parallel Common-
Lisp.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997735">
[Godden, 1990] Godden, K. &amp;quot;Lazy Unification&amp;quot; In Proceed-
ings of ACL-90, 1990.
[Karttunen, 1986] Karttunen, L. &amp;quot;D-PATR: A Development
Environment for Unification-Based Grammars&amp;quot;. In Pro-
ceedings ofCOLING-86, 1986. (Also, Report CSLI-86-61
Stanford University).
[Karttunen and Kay, 1985] Karttunen, L. and M. Kay
&amp;quot;Structure Sharing with Binary Trees&amp;quot;. In Proceedings
of ACL-85, 1985.
[Kasper, 1987] Kasper, R. &amp;quot;A Unification Method for Dis-
junctive Feature Descriptions&amp;quot;. In Proceedings of ACL-87,
1987.
[Kogure, 1989] Kogure, K. A Study on Feature Structures
and Unification. ATR Technical Report. TR-1-0032, 1988.
[Kogure, 1990] Kogure, K. &amp;quot;Strategic Lazy Incremental
Copy Graph Unification&amp;quot;. In Proceedings of COLING-90,
1990.
[Pereira, 1985] Pereira, F. &amp;quot;A Structure-Sharing Represen-
tation for Unification-Based Grammar Formalisms&amp;quot;. In
Proceedings of ACL-85, 1985.
[Pollard and Sag, 1987] Pollard, C. and!. Sag Information-
based Syntax and Semantics. Vol 1, CSLI, 1987.
[Yoshimoto and Kogure, 1989] Yoshimoto, K. and K.
Kogure Japanese Sentence Analysis by means of Phrase
Structure Grammar. ATR Technical Report. TR-1-0049,
1989.
[Wroblewski, 1987] Wroblewski, D.&amp;quot;Nondestructive Graph
Unification&amp;quot; In Proceedings of AAAI87 , 1987.
</reference>
<page confidence="0.998343">
322
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.999635">Quasi-Destructive Graph Unification</title>
<author confidence="0.996161">Hideto Tomabechi</author>
<affiliation confidence="0.999991">Carnegie Mellon University</affiliation>
<address confidence="0.999564">Pittsburgh, PA 15213-3890</address>
<email confidence="0.998861">tomabech+@cs.cmu.edu</email>
<affiliation confidence="0.9169035">ATR Interpreting Telephony Research Laboratories*</affiliation>
<address confidence="0.956293">Seika-cho, Soralcugun, Kyoto 619-02 JAPAN</address>
<abstract confidence="0.997580642086332">Graph unification is the most expensive part of unification-based grammar parsing. It often takes over 90% of the total parsing time of a sentence. We focus on two speed-up elements in the design of unification algorithms: 1) elimination of excessive copying by only copying successful unifications, 2) Finding unification failures as soon as possible. We have developed a scheme to attain these two elements without expensive overhead through temporarily modifying graphs during unification to eliminate copying during unification. We found that parsing relatively long sentences (requiring about 500 top-level unifications during a parse) using our algorithm is approximately twice as fast as parsing the same sentences using Wroblewski&apos;s algorithm. 1. Motivation Graph unification is the most expensive part of unification-based grammar parsing systems. For example, in the three types of parsing systems currently used at ATR&apos;, all of which use graph unification algorithms based on [Wroblewski, 1987], unification operations consume 85 to 90 percent of the total cpu time to a The number of unification operations per sentence tends to grow as the grammar gets larger and more complicated. An unavoidable paradox is that when the natural language system gets larger and the coverage of linguistic phenomena increases the writers of natural language grammars tend to rely more on deeper and more complex path equations (cycles and frequent reentrancy) to lessen the complexity of writing the grammar. As a result, we have seen that the number of unification operations increases rapidly as the coverage of the grammar grows in contrast to the parsing algorithm itself which does not seem to *Visiting Research Scientist. Local email address: tomabech%atz-la.atr.co.jp@ uunet.UU.NET. three parsing systems are based on: 1. Earley&apos;s algorithm, 2. active chart parsing, 3. generalized LR parsing. the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990J). grow so quickly. Thus, it makes sense to speed up the unification operations to improve the total speed performance of the natural language systems. Our original unification algorithm was based on [Wroblewski, 19871 which was chosen in 1988 as the then fastest algorithm available for our applicabased unification grammar, three types of parsers (Earley, Tomita-LR, and active chart), unificawith variables and combined with &apos;Casper &apos;s ([Kasper, 19871) scheme for handling disjunctions. In designing the graph unification algorithm, we have made the following observation which influenced the basic design of the new algorithm described in this paper: Unification does not always succeed. As we will see from the data presented in a later section, when our parsing system operates with a relatively small grammar, about 60 percent of the unifications attempted during a successful parse result in failure. If a unification fails, any computation performed and memory consumed during the unification is wasted. As the grammar size increases, the number of unification for each successful parse Without completely rewriting the grammar and the parser, it seems difficult to shift any significant amount of the computational burden to the parser in order to reduce number of unification Another problem that we would like to address in our design, which seems to be well documented in the existing literature is that: Copying is an expensive operation. The copying of a node is a heavy burden to the parsing system. [Wroblewski, 1987] calls it a &amp;quot;computational sink&amp;quot;. Copying is expensive in two ways: 1) it takes time; 2) it takes space. Copying takes time and space essentially because the area in the random access memory needs to be dynamically allocated which is an expensive operation. [Godden, 19901 calculates the time cost of copying to be about 67 perrefer to [Kogure, 1989] for trivial time modification of Wroblewsld&apos;s algorithm to handle cycles. estimate over 80% of unifications to be failures in our large-scale speech-to-speech translation system under development. course, whether that will improve the overall performance is another question. 315 cent of the total parsing time in his TIME parsing system. This time/space burden of copying is non-trivial when we consider the fact that creation of unnecessary copies will eventually trigger garbage collections more often (in a Lisp environment) which will also slow down the overall performance of the parsing system. In general, parsing systems are always short of memory space (such as large LR tables of Tomita-LR parsers and expanding tables and charts of Earley and chart and the marginal addition or subtraction of the amount of memory space consumed by other parts of the system often has critical effects on the performance of these systems. Considering the aforementioned problems, we propose the following principles to be the desirable conditions for a fast graph unification algorithm: • Copying should be performed only for successful unifications. • Unification failures should be found as soon as possible. By way of definition we would like to categorize excessive copying of dags into Over Copying and Early Copying. Our definition of over copying is the same as Wroblewski&apos;s; however, our definition of early copying is slightly different. • Over Copying: Two dags are created in order to create one new dag. — This typically happens when copies of two input dags are created prior to a destructive unification operation to build one new dag. aGodden, 19901 calls such a unification: Eager Unification.). When two arcs point to same node, over copying is often with incremental copying schemes. • Early Copying: Copies are created prior to the failure of unification so that copies created since the beginning of the unification up to the point of failure are wasted. Wroblewski defines Early Copying as follows: &amp;quot;The dags are copied started. If the unification fails then some of the copying is wasted effort&amp;quot; and restricts early copying to cases that only apply to copies that are created prior to a unification. Restricting early copying to copies that are made prior to a unification leaves a number of wasted copies that are created during a unification up to the point of failure to be uncovered by either of the above definitions for excessive copying. We would like Early Copying to mean all copies that are wasted due to a unification failure whether these copies are created before or during the actual unification operations. Incremental copying has been accepted as an effecmethod of minimizing over copying and eliminatexample, our phoneme-based generalized LR parser for speech input is always running on a swapping space because the LR table is too big. ing early copying as defined by Wroblewski. However, while being effective in minimizing over copying (it over copies only in some cases of convergent arcs into one node), incremental copying is ineffective in early copying as we define Incremental copying is ineffective in eliminating early copying because when a graph unification algorithm recurses for shared arcs (i.e. the arcs with labels that exist in both input graphs), each created unification operation recursing into each shared arc is independent of other recursive calls into other arcs. In other words, the recursive calls into shared arcs are non-deterministic and there is no way for one particular recursion into a shared arc to know the result of future recursions into other shared arcs. Thus even if a particular recursion into one arc succeeds (with minimum over copying and no early copying in Wroblewski&apos;s sense), other arcs may eventually fail and thus the copies that are created in the successful arcs are all wasted. We consider it a drawback of incremental copying schemes that copies that are incrementally created up to the point of failure get wasted. This problem will be particularly felt when we consider parallel implementations of incremental copying algorithms. Because each recursion into shared arcs is non-deterministic, parallel processes can be created to work concurrently on all arcs. In each of the parallelly created processes for each shared arc, another recursion may take place creating more parallel processes. While some parallel recursive call into some arc may take time (due to a large number of subarcs, etc.) another non-deterministic call to other arcs may proceed deeper and deeper creating a large number of parallel processes. In the meantime, copies are incrementally created at different depths of subgraphs as long as the subgraphs of each of them are unified successfully. This way, when a failure is finally detected at some deep location in some subgraph, other numerous processes may have created a large number of copies that are wasted. Thus, early copying will be a significant problem when we consider the possibility of parallelizing the unification algorithms as well. 2. Our Scheme We would like to introduce an algorithm which addresses the criteria for fast unification discussed in the previous sections. It also handles cycles without over copying (without any additional schemes such as those introduced by [Kogure, 1989]). As a data structure, a node is represented with eight fields: type, arc-list, comp-arc-list, forward, copy, comp-arc-mark, forward-mark, and copy-mark. Although this number may seem high for a graph node data structure, the amount of memory consumed is not significantly different from that consumed by other copying&apos; will henceforth be used to refer to early copying as defined by us. 316 algorithms. Type can be represented by three bits; comp-arc-mark, forward-mark, and copy-mark can be represented by short integers (i.e. fixnums); and comparc-list (just like arc-list) is a mere collection of pointers to memory locations. Thus this additional information is trivial in terms of memory cells consumed and because of this data structure the unification algorithm itself can remain simple. NODE 1 comp-arc-list 1 1 1 1 ARC 1 label 1 1 1 value 1 1: and Arc Structures The representation for an arc is no different from that of other unification algorithms. Each arc has two fields for &apos;label&apos; and &apos;value&apos;. &apos;Label&apos; is an atomic symbol which labels the arc, and &apos;value&apos; is a pointer to a node. The central notion of our algorithm is the dependency of the representational content on the global timing clock (or the global counter for the current generation of unification algorithms). This scheme was used in [Wroblewski, 1987] to invalidate the copy field of a node after one unification by incrementing a global counter. This is an extremely cheap operation but has the power to invalidate the copy fields of all nodes in the system simultaneously. In our algorithm, this dependency of the content of fields on global timing is adopted for arc lists, forwarding pointers, and copy pointers. Thus any modification made, such as adding forwarding links, copy links or arcs during one top-level unification (unify0) to any node in memory can be invalidated by one increment operation on the global timing counter. During unification (in unify 1) and copying after a successful unification, the global timing ID fora specific field can be checked by comparing the content of mark fields with the global counter value and if they match then the content is respected; if not it is simply ignored. Thus the whole operation is a trivial addition to the original destructive unification algorithm (Pereira&apos;s and Wroblewski&apos;s unify 1). We have two kinds of arc lists 1) arc-list and comparc-list. Arc-list contains the arcs that are permanent (i.e., usual graph arcs) and comp-arc-list contains arcs are only valid unification operalso have two kinds of forwarding links, i.e., permanent and temporary. A permanent forwarding link is the usual forwarding link found in other algorithms ([Pereira, 1985], [Wroblewski, 1987], etc). Temporary forwarding links are links that are only valid during one unification. The currency of the temporary links is determined by matching the content of the mark field for the links with the global counter and if they then the content of this field is As [Pereira, 1985], we have three nodes: 1) 2) and :atomic type nodes represent atomic symbol values (such as Noun), :bottom type nodes are variables and :complex type nodes are nodes that have arcs coming out of them. Arcs are stored in the arc-list field. The atomic value is also stored in the arc-list if the node type is :atomic. :bottom nodes succeed in unifying with any nodes and the result of unification takes the type and the value of the node that the :bottom node was unified with. :atomic nodes succeed in unifying with :bottom nodes or :atomic nodes with the same value (stored in the arc-list). Unification of an :atomic node with a :complex node immediately fails. :complex nodes succeed in unifying with :bottom nodes or with :complex nodes whose subgraphs all unify. Arc values are always nodes and never symbolic values because the :atomic and :bottom nodes may be pointed to by multiple arcs (just as in structure sharing of :complex nodes) depending on grammar constraints, and we do not want arcs to contain terminal atomic values. Figure 2 is the central quasi-destructive graph unification algorithm and shows the algorithm for copying and arcs (called by unify0) while respecting the contents of comp-arc-lists. The functions Complementarcs(dg 1 ,dg2) and Intersectarcs(dgl ,dg2) are similar to Wroblewski&apos;s algorithm and return the set-difference (the arcs with labels that exist in dg 1 but not in dg2) and intersection (the arcs with labels that exist both in dg 1 and dg2) respectively. During the set-difference and setintersection operations, the content of comp-arc-lists are respected as parts of arc lists if the comp-arcmarks match the current value of the global timing counter. Dereference-dg(dg) recursively traverses the forwarding link to return the forwarded node. In doing so, it checks the forward-mark of the node and if the forward-mark value is 9 (9 represents a permanent forwarding link) or its value matches the current do not have a separate field for temporary forwarding links; instead, we designate the integer value 9 to represent a permanent forwarding link. We start incrementing the global counter from 10 so whenever the forward-mark is not 9 the integer value must equal the global counter value to respect the forwarding link. is called leaf in Pereira&apos;s algorithm. 317 value of *unify-global-counter*, then the function returns the forwarded node; otherwise it simply returns the input node. Forward(dgl, dg2, :forward-type) puts (the pointer to) dg2 in the forward field of dgl. If the keyword in the function call is :temporary, the current value of the *unify-global-counter* is written in the forward-mark field of dgl. If the keyword is :permanent, 9 is written in the forward-mark field of dgl. Our algorithm itself does not require any permanent forwarding; however, the functionality is added because the grammar reader module that reads the path equation specifications into dg feature-structures uses permanent forwarding to merge the additional gramspecifications into a graph The temporary forwarding links are necessary to handle reentrancy and cycles. As soon as unification (at any level of recursion through shared arcs) succeeds, a temporary forwarding link is made from dg2 to dgl (dgl to dg2 if dgl is of type :bottom). Thus, during unification, a node already unified by other recursive calls to unify I within the same unify() call has a temporary forwarding link from dg2 to dgl (or dgl to dg2). As a result, if this node becomes an input argument node, dereferencing the node causes dgl and dg2 to become the same node and unification immediately succeeds. Thus a subgraph below an already unified node will not be checked more than once even if an argument graph has a cycle. Also, during copying done subsequently to a successful unification, two arcs converging into the same node will not cause over copying simply because if a node already has a copy then the copy is returned. For example, as a case that may cause over copies in other schemes for dg2 convergent arcs, let us consider the case when the destination node has a corresponding node in dgl and only one of the convergent arcs has a corresponding arc in dgl. This destination node is already temporarily forwarded to the node in dgl (since the unification check was successful prior to copying). Once a copy is created for the corresponding dgl node and recorded in the copy field of dgl, every time a convergent arc in dg2 that needs to be copied points to its destination node, dereferencing the node returns the corresponding node in dgl and since a copy of it already exists, this copy is returned. Thus no duplicate copy is created&amp;quot;. have been using Wroblewski&apos;s algorithm for the unification part of the parser and thus usage of (permanent) forwarding links is adopted by the grammar reader module to convert path equations to graphs. For example, permanent forwarding is done when a :bottom node is to be merged with other nodes. of dg2 arcs happens for arcs that exist in dg2 but not in dgl (i.e., Complementarcs(dg2,dg1)). Such arcs are pushed to the comp-arc-list of dgl during unifyl and are copied into the arc-list of the copy during subsequent copying. If there is a cycle or a convergence in arcs in dgl or in arcs in dg2 that do not have corresponding arcs in dgl, then the mechanism is even simpler than the one discussed here. A copy is made once, and the same copy is simply returned GRAPH UNIFICATION -dg (dg 1,dg2); 4-catch with tag &apos;unify-fail calling unify0(dgl,dg2); *unify-global-counter*; ;; starts from 10 12 return(result); END; 1,dg2); if &apos;*T* = unifyl(dgl,dg2); THEN 4--copy-dg-with-comp-arcs(dg1); return(copy); END; (dgl-underef,dg2-underef); dgl dereference-dg(dgl-underef); dg2 4— dereference-dg(dg2-underef); = return(&apos;*T*); = :bottom) forward-dg(dg 1 ,dg2,:temporazy); retum(&apos;*T*); = :bottom) THEN forward-dg (dg2,dg 1 ,:temporary); return(&apos;*T*); = :atomic AND dg2.type = :atomic) THEN = forward-dg(dg2,dgl,:temporary); retum(&apos;*T*); with keyword &apos;unify-fail; = :atomic dg2.type = :atomic) THEN throw with keyword &apos;unify-fail; ELSE new 4— complementarcs(dg2,dg1); shared intersectarcs(dgl,dg2); arc IN shared unifyl(destination of the shared arc for dgl, destination of the shared arc for dg2); * nify -g lob al-cou nter* ; 4-new; return (&apos;*T*); END; 2: Q.D. Unification Functions every time another convergent arc points to the original node. It is because arcs are copied only from either dgl or dg2. a permanent forwarding link. ual in the &apos;eq&apos; sense. Because of forwarding and cycles, it is possible that dgl and dg2 are &apos;eq&apos;. contains atomic value if the node is of type :atomic. construct; i.e., immediately return to unifydg. be executed only when all recursive calls into unifyl succeeded. Otherwise, a failure would have caused 318 COPYING FUNCTION copy-dg-with-comp-arcs(dg-underef); 4—dereference-dg(dg-underef); IF (dg.copy is non-empty AND dg.copy-mark = *unify-global-counter*) THEN ELSE IF (dg.type = :atomic) THEN 4— copy.type 4— :atomic; copy.arc-list dg.arc-list; dg.copy 4— copy; dg.copy-mark *unify-global-counter*; retum(copy); ELSE IF (dg.type = :bottom) THEN copy create-node(); copy.type :bottom; dg.copy 4— copy; 4—*unify-global-counter*; return(copy); ELSE 4....create-node(); copy.type 4— :complex; FOR ALL arc IN dg.arc-list DO newarc 4— copy-arc-and-comp-arc(arc); push newarc into copy.arc-list; IF (dg.comp-arc-list is non-empty AND dg.comp-arc-mark = *unify-global-counter*) THEN FOR ALL comp-arc IN dg.comp-arc-list DO newarc copy-arc-and-comp-arc(comp-arc); push newarc into copy.arc-list; dg.copy copy; dg.copy-mark 4— *unify-global-counter*; return (copy); END; FUNCTION copy-arc-and-comp-arcs(input-arc); label 4— input-arc.label; value copy-dg-with-comp-arcs(input-arc.value); return a new arc with label and value; END; 3: and Arc Copying Functions a simple example of quasidestructive graph unification with dg2 convergent arcs. The round nodes indicate atomic nodes and the rectangular nodes indicate bottom (variable) nodes. First, top-level unify! finds that each of the input graphs has and arc-b unify 1 is recursively called. At step two, the recursion into arc-a locally succeeds, and a temporary forwarding link with timeis made from node [12 to the third step (recursion into arc-b), by the previous forwarding, 02 already has the value dereferencing). Then this unification returns a success and a temporary forwarding link with time-stamp(n) is created from immediate return to the existing copy of the node. an empty node structure. 111 to node the fourth step, since all recursive unifications (unify 1 s) into shared arcs succeeded, top-level unify 1 creates a temporary forwarding link with time-stamp(n) from dag2&apos;s root node to dag 1 &apos;s node, and sets arc-c comp-arc-list of clag 1 and returns success (&apos;*T*). At the fifth step, a copy of dagl is created respecting the content of comparc-list and dereferencing the valid forward links. This copy is returned as a result of unification. At the last step (step six), the global timing counter is incremented (n n+1). After this operation, temporary forwarding links and comp-arc-lists with time-stamp (&lt; n+1) will be ignored. Therefore, the original dag 1 and dag2 are recovered in a constant time without a costly reversing operations. (Also, note that recursions into shared-arcs can be done in any order producing the same result). A Simple Example of Quasi-Destructive Graph Unification 1 For each node with arc-a. unifyl( s, [ ]2) dag I a comp-arct dag2 forward(n) 1 ) of dagl forward(n) 1 forward(n) For each node with arc-b, [11,[ )2) dag 1 fonprdW forward(a) 2 comp-arcb Nist(n)=( c) 1 6t fonvard(a) forward(n) of dag I I t s dag2 1 (dag 1 ,dag 2) (a , Al NEW=(c) a 319 As we just saw, the algorithm itself is simple. The basic control structure of the unification is similar to Pereira&apos;s and Wroblewski&apos;s unify 1. The essential dif-ference between our unifyl and the previous ones is that our unify 1 is non-destructive. It is because the complementarcs(dg2,dg 1) are set to the comp-arc-list of dg 1 and not into the arc-list of dgl • Thus, as soon as we increment the global counter, the changes made to dg 1 (i.e., addition of complement arcs into comp-arc-list) vanish. As long as the comp-arc-mark value matches that of the global counter the content of the comp-arc-list can be considered a part of arc-list and therefore, dg 1 is the result of unification. Hence the name quasi-destructive graph unification. In order to create a copy for subsequent use we only need to make a copy of dg 1 before we increment the global counter while respecting the content of the comp-arc-list of dgl. &apos;Unifs&apos; represents the total number of unifications dur-ing a parse (the number of calls to the top-level &apos;unify-dg&apos;, and not &apos;unify1&apos;). `I.JSrate&apos; represents the ratio of successful unifications to the total number of uni-fications. We parsed each sentence three times on a Symbolics 3620 using both unification methods and took the shortest elapsed time for both methods (7&apos; represents our scheme, &apos;W&apos; represents Wroblewski&apos;s algorithm with a modification to handle cycles and variables&amp;quot;). Data structures are the same for both uni-fication algorithms (except for additional fields for a node in our algorithm, i.e., comp-arc-list, comp-arc-mark, and forward-mark). Same functions are used to interface with Earley&apos;s parser and the same subfunc-lions are used wherever possible (such as creation and access of arcs) to minimize the differences that are not purely algorithmic. &apos;Number of copies&apos; represents the number of nodes created during each parse (and does not include the number of arc structures that are cre-ated during a parse). &apos;Number of conses&apos; represents the amount of structure words consed during a parse. This number represents the real comparison of the amount of space being consumed by each unification algorithm (including added fields for nodes in our algorithm and arcs that are created in both algorithms). Thus instead of calling other unification functions (such as unify2 of Wroblewski) for incrementally cre-ating a copy node during a unification, we only need to create a copy after unification. Thus, if unifica-tion fails no copies are made at all (as in [ICarttunen, 1986]&apos;s scheme). Because unification that recurses into shared arcs carries no burden of incremental copy-ing (i.e., it simply checks if nodes are compatible), as the depth of unification increases (i.e., the graph gets larger) the speed-up of our method should get conspic-uous if a unification eventually fails. If all unifica-tions during a parse are going to be successful, our algorithm should be as fast as or slightly slower than Since a parse that does not fail on a single unification is unrealistic, the gain from our scheme should depend on the amount of unification failures that occur during a unification. As the number of failures per parse increases and the graphs that failed get larger, the speed-up from our algorithm should be-come more apparent. Therefore, the characteristics of our algorithm seem desirable. In the next section, we will see the actual results of experiments which com-pare our unification algorithm to Wroblewski&apos;s algo-rithm (slightly modified to handle variables and cycles that are required by our HPSG based grammar). We used Earley&apos;s parsing algorithm for the experi-ment. The Japanese grammar is based on HPSG anal-ysis ([Pollard and Sag, 1987]) covering phenomena such as coordination, case adjunction, adjuncts, con-trol, slash categories, zero-pronouns, interrogatives, WH constructs, and some pragmatics (speaker, hearer relations, politeness, etc.) ([Yoshimoto and Kogure, 1989]). The grammar covers many of the important linguistic phenomena in conversational Japanese. The grammar graphs which are converted from the path equations contain 2324 nodes. We used 16 sentences from a sample telephone conversation dialog which from very short sentences (one word, i.e., to relatively long ones (such as soredehakochirakarasochiranitourokuyoushiw000kuriitashimasu&apos;In that case, we [speaker] will send you [hearer] the reg-istration form.&apos;). Thus, the number of (top-level) uni-fications per sentence varied widely (from 6 to over 500). 3. Experiments Table 1 shows the results of our experiments using an HPSG-based Japanese grammar developed at AIR for a conference registration telephone dialogue domain. may be slightly slower because our unification recurses twice on a graph: once to unify and once to copy, whereas in incremental unification schemes copying is performed during the same recursion as unifying. Additional bookkeeping for incremental copying and an additional set-difference operation (i.e, complementarcs(dgl,dg2)) during unify2 may offset this, however. &amp;quot;Cycles can be handled in Wroblewski&apos;s algorithm by checking whether an arc with the same label already exists when arcs are added to a node. And if such an arc already exists, we destructively unify the node which is the destination of the existing arc with the node which is the destination of the arc being added. If such an arc does not exist, we simply add the arc. ([Kogure, 1989]). Thus, cycles can be handled very cheaply in Wroblewski&apos;s algorithm. Handling variables in Wroblewski&apos;s algorithm is basically the same as in our algorithm (i.e.. Pereira&apos;s scheme), and the addition of this functionality can be ignored in terms of comparison to our algorithm. Our algorithm does not require any additional scheme to handle cycles in input dgs. 320 sent# Unifs USrate Elapsed time(sec) Num of Copies Num of Conses</abstract>
<phone confidence="0.556837">1 6 0.5 1.066 1.113 85 107 1231 1451 2 101 0.35 1.897 2.899 1418 2285 15166 23836 3 24 0.33 1.206 1.290 129 220 1734 2644</phone>
<address confidence="0.640389333333333">4 71 0.41 3.349 4.102 1635 2151 17133 22943 5 305 0.39 12.151 17.309 5529 9092 57405 93035 6 59 0.38 1.254 1.601 608 997 6873 10763</address>
<phone confidence="0.5520144">7 6 0.38 1.016 1.030 85 107 1175 1395 8 81 0.39 3.499 4.452 1780 2406 18718 24978 9 480 0.38 18.402 34.653 9466 15756 96985 167211 10 555 0.39 26.933 47.224 11789 18822 119629 189997 11 109 0.40 4.592 5.433 2047 2913 21871 30531 12 428 0.38 13.728 24.350 7933 13363 81536 135808 13 559 0.38 15.480 42.357 9976 17741 102489 180169 14 52 0.38 1.977 2.410 745 941 8272 10292 15 77 0.39 3.574 4.688 1590 2137 16946 22416 16 77 0.39 3.658 4.431 1590 2137 16943 22413</phone>
<abstract confidence="0.96687076">Table 1: Comparison of our algorithm with Wroblewski&apos;s 4. Discussion: Comparison to Other Approaches The control structure of our algorithm is identical to that of [Pereira, 1985]. However, instead of storing changes to the argument dags in the environment we store the changes in the dags themselves nondestructively. Because we do not use the environment, the log(d) overhead (where d is the number of nodes in a dag) associated with Pereira&apos;s scheme that is required during node access (to assemble the whole dag from the skeleton and the updates in the environment) is avoided in our scheme. We share the principle of storing changes in a restorable way with [Karttunen, 19861&apos;s reversible unification and copy graphs only after a successful unification. Karttunen originally introduced this scheme in order to replace the less efficient structure-sharing implementations ([Pereira, 1985], [Karttunen and Kay, 1985]). In Karttunen&apos;s whenever a destructive change is about to made, the attribute value stored in the body of the node are saved into an array. The dag node structure itself is also saved in another array. These values are restored after the top level unification is completed. (A copy is made prior to the restoration operation if unification successful one.) The difference between Karttunen&apos;s method and ours is that in our algorithm, one increment to the global counter can invalidate all the changes made to nodes, while in Karttunen&apos;s algorithm each node in the entire argument graph that has been destructively modified must be restored separately by retrieving the attribute-values saved in an discussion of Kartunnen&apos;s method is based on the D- PATR implementation on Xerox 1100 machines ([Karttunen, 1986]). structures: &apos;label&apos; and &apos;value&apos; pairs in our vocabulary. array and resetting the values into the dag structure skeletons saved in another array. In both Karttunen&apos;s and our algorithm, there will be a non-destructive (reversible, and quasi-destructive) saving of intersection arcs that may be wasted when a subgraph of a particular node successfully unifies but the final unification fails due to a failure in some other part of the argument graphs. This is nota problem in our method because the temporary change made to a node is performed as pushing pointers into already existing structures (nodes) and it does not require entirely new structures to be created and dynamically allocated memory (which was necesfor the copy (create-node) operation). 23[Godden, 19901 presents a method of using lazy evaluation in unification which seems to be one successful actualization of [Karttunen and Kay, 19851&apos;s lazy evaluation idea. One question about lazy evaluation is that the efficiency of lazy evaluation varies depending upon the particular hardware and programming language environment. For example, in CommonLisp, to attain a lazy evaluation, as soon as a function is delayed, a closure (or a structure) needs to be created receiving a dynamic allocation of memory (just as in creating a copy node). Thus, there is a shift of memory and associated computation consumed from making copies to making closures. In terms of memory cells saved, although the lazy scheme may reduce the total number of copies created, if we consider the memory consumed to create closures, the saving may be significantly canceled. In terms of speed, since delayed evaluation requires additional bookkeeping, how schemes such as the one introduced by [Godden, 1990] would compare with nonlazy incremental copying schemes is an open question. Godden offers a comparison of his algo- &amp;quot;Although, in Karttunen&apos;s method it may become rather expensive if the arrays require resizing during the saving operation of the subgraphs. 321 rithm with one that uses a full copying method (i.e. his Eager Copying) which is already significantly slower than Wroblewski&apos;s algorithm. However, no comparison is offered with prevailing unification schemes such as Wroblewski&apos;s. With the complexity for lazy evaluation and the memory consumed for delayed closures added, it is hard to estimate whether lazy unification runs considerably faster than Wroblewski&apos;s incremencopying 5. Conclusion The algorithm introduced in this paper runs significantly faster than Wroblewski&apos;s algorithm using Earley&apos;s parser and an HPSG based grammar developed at ATR. The gain comes from the fact that our algorithm does not create any over copies or early copies. In Wroblewski&apos;s algorithm, although over copies are essentially avoided, early copies (by our definition) are a significant problem because about 60 percent of unifications result in failure in a successful parse in our sample parses. The additional set-difference operation required for incremental copying during unify2 may also be contributing to the slower speed of Wroblewski&apos;s algorithm. Given that our sample grammar is relatively small, we would expect that the difference in the performance between the incremental copying schemes and ours will expand as the grammar size and both the number of failures 25and the size of the wasted subgraphs of failed unifications become larger. Since our algorithm is essentially parallel, parallelization is one logical choice to pursue further speedup. Parallel processes can be continuously created as unifyl recurses deeper and deeper without creating any copies by simply looking for a possible failure of the unification (and preparing for successive copying in case unification succeeds). So far, we have completed a preliminary implementation on a shared memory parallel hardware with about 75 percent of effective parallelization rate. With the simplicity of our algorithm and the ease of implementing it (compared to both incremental copying schemes and lazy schemes), combined with the demonstrated speed of the algorithm, the algorithm could be a viable alternative to existing unification algorithms used in current is, unless some new scheme for reducing excessive copying is introduced such as structure-sharing of an unchanged shared-forest ([Kogure, 1990]). Even then, our criticism of the cost of delaying evaluation would still be valid. Also, although different in methodology from the way suggested by Kogure for Wroblewski&apos;s algorithm, it is possible to attain structure-sharing of an unchanged forest in our scheme as well. We have already developed a preliminary version of such a scheme which is not discussed in this paper. in our large-scale speech-to-speech translation system under development, the USrate is estimated to be under 20%, i.e., over 80% of unifications are estimated to be failures. natural language systems. ACKNOWLEDGMENTS The author would like to thank Akira Kurematsu, Tsuyoshi Morimoto, Hitoshi Iida, Osamu Furuse, Masaald Nagata, Toshiyuki Takezawa and other members of ATR and Masaru Tomita and Jaime Carbonell at CMU. Thanks are also due to Margalit Zabludowski and Hiroald Kitano for comments on the final version of this paper and Takako Fujiolca for assistance in implementing the parallel version of the algorithm. Appendix: Implementation The unification algorithms, Earley parser and the HPSG path equation to graph converter programs are implemented in CommonLisp on a Symbolics machine. The preliminary parallel version of our unification algorithm is currently implemented on a Sequent/Symmetry closely-coupled shared-memory parallel machine running Allegro CLiP parallel Common- Lisp.</abstract>
<note confidence="0.910648066666667">References 1990] Godden, K. &amp;quot;Lazy Unification&amp;quot; In Proceedof ACL-90, [Karttunen, 1986] Karttunen, L. &amp;quot;D-PATR: A Development for Unification-Based Grammars&amp;quot;. In ProofCOLING-86, (Also, Report CSLI-86-61 Stanford University). [Karttunen and Kay, 1985] Karttunen, L. and M. Kay Sharing with Binary Trees&amp;quot;. In ACL-85, [Kasper, 1987] Kasper, R. &amp;quot;A Unification Method for Dis- Feature Descriptions&amp;quot;. In of ACL-87, 1987. 1989] Kogure, K. A on Feature Structures Unification. Technical Report. TR-1-0032, 1988. [Kogure, 1990] Kogure, K. &amp;quot;Strategic Lazy Incremental Graph Unification&amp;quot;. In of COLING-90, 1990. [Pereira, 1985] Pereira, F. &amp;quot;A Structure-Sharing Representation for Unification-Based Grammar Formalisms&amp;quot;. In of ACL-85, and Sag, 1987] Pollard, C. and!. Sag Information- Syntax and Semantics. 1, CSLI, 1987. [Yoshimoto and Kogure, 1989] Yoshimoto, K. and K. Sentence Analysis by means of Phrase Grammar. Technical Report. TR-1-0049, 1989. [Wroblewski, 1987] Wroblewski, D.&amp;quot;Nondestructive Graph In of AAAI87 , 322</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>2IThe discussion of Kartunnen&apos;s method is based on the DPATR implementation on Xerox 1100 machines ([Karttunen,</title>
<date>1986</date>
<marker>1986</marker>
<rawString> 2IThe discussion of Kartunnen&apos;s method is based on the DPATR implementation on Xerox 1100 machines ([Karttunen, 1986]). 221.e., arc structures: &apos;label&apos; and &apos;value&apos; pairs in our vocabulary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Godden</author>
</authors>
<title>Lazy Unification&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90,</booktitle>
<marker>[Godden, 1990]</marker>
<rawString>Godden, K. &amp;quot;Lazy Unification&amp;quot; In Proceedings of ACL-90, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>D-PATR: A Development Environment for Unification-Based Grammars&amp;quot;.</title>
<date>1986</date>
<booktitle>In Proceedings ofCOLING-86,</booktitle>
<tech>Also, Report CSLI-86-61</tech>
<institution>Stanford University).</institution>
<marker>[Karttunen, 1986]</marker>
<rawString>Karttunen, L. &amp;quot;D-PATR: A Development Environment for Unification-Based Grammars&amp;quot;. In Proceedings ofCOLING-86, 1986. (Also, Report CSLI-86-61 Stanford University).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
<author>M Kay</author>
</authors>
<title>Structure Sharing with Binary Trees&amp;quot;.</title>
<date>1985</date>
<booktitle>In Proceedings of ACL-85,</booktitle>
<marker>[Karttunen and Kay, 1985]</marker>
<rawString>Karttunen, L. and M. Kay &amp;quot;Structure Sharing with Binary Trees&amp;quot;. In Proceedings of ACL-85, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kasper</author>
</authors>
<title>A Unification Method for Disjunctive Feature Descriptions&amp;quot;.</title>
<date>1987</date>
<booktitle>In Proceedings of ACL-87,</booktitle>
<marker>[Kasper, 1987]</marker>
<rawString>Kasper, R. &amp;quot;A Unification Method for Disjunctive Feature Descriptions&amp;quot;. In Proceedings of ACL-87, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kogure</author>
</authors>
<title>A Study on Feature Structures and Unification.</title>
<date>1988</date>
<tech>ATR Technical Report. TR-1-0032,</tech>
<marker>[Kogure, 1989]</marker>
<rawString>Kogure, K. A Study on Feature Structures and Unification. ATR Technical Report. TR-1-0032, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kogure</author>
</authors>
<title>Strategic Lazy Incremental Copy Graph Unification&amp;quot;.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING-90,</booktitle>
<marker>[Kogure, 1990]</marker>
<rawString>Kogure, K. &amp;quot;Strategic Lazy Incremental Copy Graph Unification&amp;quot;. In Proceedings of COLING-90, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>A Structure-Sharing Representation for Unification-Based Grammar Formalisms&amp;quot;.</title>
<date>1985</date>
<booktitle>In Proceedings of ACL-85,</booktitle>
<marker>[Pereira, 1985]</marker>
<rawString>Pereira, F. &amp;quot;A Structure-Sharing Representation for Unification-Based Grammar Formalisms&amp;quot;. In Proceedings of ACL-85, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C and Pollard</author>
</authors>
<title>Sag Informationbased Syntax and Semantics. Vol 1, CSLI,</title>
<date>1987</date>
<marker>[Pollard and Sag, 1987]</marker>
<rawString>Pollard, C. and!. Sag Informationbased Syntax and Semantics. Vol 1, CSLI, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yoshimoto</author>
<author>K</author>
</authors>
<title>Kogure Japanese Sentence Analysis by means of Phrase Structure Grammar.</title>
<date>1989</date>
<tech>ATR Technical Report. TR-1-0049,</tech>
<marker>[Yoshimoto and Kogure, 1989]</marker>
<rawString>Yoshimoto, K. and K. Kogure Japanese Sentence Analysis by means of Phrase Structure Grammar. ATR Technical Report. TR-1-0049, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wroblewski</author>
</authors>
<title>D.&amp;quot;Nondestructive Graph Unification&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings of AAAI87 ,</booktitle>
<marker>[Wroblewski, 1987]</marker>
<rawString>Wroblewski, D.&amp;quot;Nondestructive Graph Unification&amp;quot; In Proceedings of AAAI87 , 1987.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>