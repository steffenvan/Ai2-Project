<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98069">
Improvement of a Naive Bayes Sentiment Classifier Using MRS-Based
Features
</title>
<author confidence="0.998599">
Jared Kramer
</author>
<affiliation confidence="0.99885">
University of Washington
</affiliation>
<address confidence="0.793957">
Seattle, WA
</address>
<email confidence="0.998961">
jaredkk@uw.edu
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982375">
This study explores the potential of us-
ing deep semantic features to improve bi-
nary sentiment classification of paragraph-
length movie reviews from the IMBD
website. Using a Naive Bayes classifier as
a baseline, we show that features extracted
from Minimal Recursion Semantics repre-
sentations in conjunction with back-off re-
placement of sentiment terms is effective
in obtaining moderate increases in accu-
racy over the baseline’s n-gram features.
Although our results are mixed, our most
successful feature combination achieves
an accuracy of 89.09%, which represents
an increase of 0.76% over the baseline per-
formance and a 6.48% reduction in error.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99949819047619">
Text-based sentiment analysis offers valuable in-
sight into the opinions of large communities of re-
viewers, commenters and customers. In their sur-
vey of the field, Pang and Lee (2008) highlight the
importance of sentiment analysis across a range
of industries, including review aggregation web-
sites, business intelligence, and reputation man-
agement. Detection and classification of sentiment
can improve downstream performance in applica-
tions sensitive to user opinions, such as question-
answering, automatic product recommendations,
and social network analysis (ibid., p. 12).
While previous research in sentiment analysis
has investigated the extraction of features from
syntactic dependency trees, semantic representa-
tions appear to be underused as a resource for
modeling opinion in text. Indeed, to our knowl-
edge, there has been no research using seman-
tic dependencies created by a precision grammar
for sentiment analysis. The goal of the present
research is to address this gap by augmenting a
</bodyText>
<author confidence="0.304523">
Clara Gordon
</author>
<affiliation confidence="0.638139">
University of Washington
</affiliation>
<address confidence="0.405886">
Seattle, WA
</address>
<email confidence="0.866145">
cgordon1@uw.edu
</email>
<bodyText confidence="0.999885909090909">
baseline classifier with features based on Min-
imal Resursion Semantics (MRS; Copestake et
al., 2005), a formal semantic representation pro-
vided by the English Resource Grammar (ERG;
Flickinger, 2000). An MRS is a connected graph
in which semantic entities may be linked directly
through shared arguments or indirectly through
handle or qeq constraints, which denote equal-
ity modulo quantifier insertion (Copestake et al.,
2005). This schema allows for underspecification
of quantifier scope.
Using Narayanan et al.’s (2013) Naive Bayes
sentiment classifier as a baseline, we test the effec-
tiveness of eight feature types derived from MRS.
Our feature pipeline crawls various links in the
MRS representations of sentences in our corpus
of paragraph-length movie reviews and outputs
simple, human-readable features based on various
types of semantic relationships. This improved
system achieves modest increases in binary senti-
ment classification accuracy for several of the fea-
ture combinations tested.1
In the following sections, we summarize previ-
ous research in MRS feature extraction and senti-
ment classification, describe the baseline system
and our modifications to it, and outline our ap-
proach to parsing our data, constructing features,
and integrating them into the existing system. Fi-
nally, we report our findings, examine in more
detail where our improved system succeeded and
failed in relation to the baseline, and suggest av-
enues for further research in sentiment analysis
with MRS-based features.
</bodyText>
<sectionHeader confidence="0.961498" genericHeader="introduction">
2 Context and Related Work
</sectionHeader>
<bodyText confidence="0.9853865">
Current approaches to sentiment analysis tasks
typically use supervised machine learning meth-
</bodyText>
<footnote confidence="0.75835225">
1Because this task consists of binary classification on an
evenly split dataset and every test document is assigned a
class, simple accuracy is the most appropriate measure of per-
formance.
</footnote>
<page confidence="0.982964">
22
</page>
<note confidence="0.9809095">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 22–29,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.999933611111111">
ods with bag-of-words features as a baseline, and
for classification of longer documents like the ones
in our dataset, such features remain a powerful
tool of analysis. Wang and Manning (2012) com-
pare the performance of several machine learning
algorithms using uni- and bigram features from a
variety of common sentiment datasets, including
the IMDB set used in this project. They report that
that SVM classifiers generally perform better sen-
timent classification on paragraph-length reviews,
while Native Bayes classifiers produce better re-
sults for “snippets,” or short phrases (ibid., p. 91).
For our dataset, they obtain the highest accuracies
using a hybrid approach, SVM with Naive Bayes
features, which results in 91.22% accuracy (ibid.,
p. 93). This appears to be the best test result to
date on this dataset. Although we use a Naive
Bayes classifier in our project, alternative machine
learning algorithms are a promising topic of fur-
ther future investigation (see §6).
Two existing areas of research have direct rele-
vance to this project: MRS feature extraction, and
sentiment analysis using features based on deep
linguistic representations of data. In their work on
machine translation, Oepen et al. (2007) define a
type of MRS triple based on elementary dependen-
cies, a simplified “variable-free” representation of
predicate-argument relations in MRS (p. 5). Fu-
jita et al. (2007) and Pozen (2013) develop simi-
lar features for HPSG parse selection, and Pozen
experiments with replacing segments of predicate
values in triple features with WordNet sense, POS,
and lemma information (2013, p. 32).
While there has not yet been any research on us-
ing MRS features in sentiment analysis, there has
been work on extracting features from deep repre-
sentations of data for sentiment analysis. In work-
ing with deep representations such as MRSes or
dependency parses, there are myriad sub-graphs
that can be used as features. However these fea-
tures are often quite sparse and do not general-
ize well. Joshi &amp; Rose (2009) improve perfor-
mance of a sentiment classifier by incorporating
triples consisting of words and grammatical rela-
tions extracted from dependency parses. To in-
crease the generalizability of these triples, they
perform back-off by replacing words with part-of-
speech tags. Similarly, Arora et al. (2010) extract
features from dependency parses by using senti-
ment back-off to identify potentially meaningful
portions of the dependency graph. Given this suc-
cess combining back-off with sub-graph features,
we design several feature types following a similar
methodology.
</bodyText>
<subsectionHeader confidence="0.969205">
2.1 The IMBD Dataset
</subsectionHeader>
<bodyText confidence="0.999919789473684">
We use a dataset of 50,000 movie reviews crawled
from the IMDB website, originally developed by
Maas et al. (2011). The dataset is split equally
between training and test sets. Both training and
test sets contain equal numbers of positive and
negative reviews, which are defined according to
the number of stars assigned by the author on
the IMBD website: one to four stars for nega-
tive reviews, and seven to ten stars for positive
reivews. The reviews vary in length but gener-
ally contain between five and fifteen sentences.
The Natural Language ToolKit’s (NLTK; Loper
and Bird, 2002) sentence tokenizer distinguishes
616,995 sentences in the dataset.
Unlike previous research over this dataset, we
divide the 25,000 reviews of the test set into two
development sets and a final test set. As such,
our results are not directly comparable to those of
Wang &amp; Manning (2012).
</bodyText>
<subsectionHeader confidence="0.999866">
2.2 The Baseline System
</subsectionHeader>
<bodyText confidence="0.999972269230769">
The system we use as a baseline, created by
Narayanan et al. (2013), implements several small
but innovative improvements to a simple Naive
Bayes classifier. In the training phase, the base-
line performs simple scope of negation annota-
tion on the surface string tokens. Any word con-
taining the characters not, n’t or no triggers a
“negated” state, in which all following n-grams are
prepended with not . This continues until either
a punctuation delimiter (?.,!:;) or another negation
trigger is encountered.
During training, when an n-gram feature is read
into the classifier, it is counted toward P(f|c), and
the same feature with not prepended is counted
toward P(f|ˆc), where c is the document class
and cˆ is the opposite class. Singleton features
are then pruned away. Finally, the system runs a
set of feature-filtering trials, in which the pruned
features are ranked by mutual information score.
These trials start at a base threshold of 100,000
features, and the number of features is increased
stepwise in increments of 50,000. The feature set
that produces the highest accuracy in trials over a
development data set is then retained and used to
classify the test data. Table 1 shows the ten most
informative features, ranked by mutual informa-
</bodyText>
<page confidence="0.998874">
23
</page>
<tableCaption confidence="0.8583485">
Top N-Grams
Table 1: Top MI-ranked baseline n-gram Features.
</tableCaption>
<bodyText confidence="0.996660142857143">
tion score, out of the 12.1 million n-gram features
generated by our baseline.
Before modifying the baseline system’s code,
we reproduced their reported accuracy figure of
88.80% over the entire 25,000 review test set.
However, it appears the baseline system used the
test data as development data. In order to address
this, we split the data as into development sets as
described above. When we ran the baseline sys-
tem over our final test set, we obtained accuracies
of 88.34% pre-feature filtering and 88.29% post-
feature filtering; our division of the original test
set into development and test sets accounts for this
discrepancy.
</bodyText>
<sectionHeader confidence="0.998013" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999800333333333">
Our approach to this task consisted of three gen-
eral stages: obtaining MRSes for the dataset,
implementing a feature pipeline to process the
MRSes, and integrating the new features into the
classifier. In this section we will describe each of
these processes in turn.
</bodyText>
<subsectionHeader confidence="0.999959">
3.1 Parsing with the ERG
</subsectionHeader>
<bodyText confidence="0.999940642857143">
Because most of the reviews in our data set ap-
pear to be written in Standard English, we per-
form minimal pre-processing before parsing the
dataset with the ERG. We use NLTK’s sentence
tokenization function in our pipeline, along with
their HTML-cleaning function to remove some
stray HTML-style tags we encountered in the data.
To obtain MRS parses of the data, we use
ACE version 0.9.17, an “efficient processor
for DELPH-IN HPSG grammars.”2 ACE’s sim-
ple command line interface allows the parsing
pipeline to output MRS data in a single line to
a separate directory of MRS data files. We used
the 1212 ERG grammar image3 and specified root
</bodyText>
<footnote confidence="0.999794333333333">
2Available at http://sweaglesw.org/linguistics/ace/. Ac-
cessed January 15, 2014.
3Available at http://www.delph-in.net/erg/. Accessed Jan-
</footnote>
<bodyText confidence="0.999838083333333">
conditions that would allow for parses of the infor-
mal and fragmented sentences sometimes found
in our dataset: namely, the root informal,
root frag and root inffrag ERG root
nodes.
Parsing with these conditions resulted in
81.11% coverage over the entire dataset. After
manual inspection of sentences that failed to parse,
we found that irregularities in spelling and punc-
tuation accounted for the majority of these failures
and further cleaning of the data would yield higher
coverage.
</bodyText>
<subsectionHeader confidence="0.999159">
3.2 Feature Design
</subsectionHeader>
<bodyText confidence="0.999967657142857">
Our main focus in feature design is capturing rel-
evant semantic relationships between sentiment
terms that extend beyond the trigram boundary.
Our entry point into the MRS is the elementary
predication (EP), and our pipeline algorithm ex-
plores the three main EP components: arguments
and associated variables, label, and predicate sym-
bol. We also use the set of handle constraints in
crawling the links between EPs.
We use two main categories of crawled MRS
features: Predicate-Relation-Predicate (PRP)
triples, a term borrowed from (Pozen, 2013), and
Shared-Label (SL) features. Our feature template
consists of eight feature subtypes, including plain
EP symbols (type 1), five PRP features (types 2
through 6) and two SL features (types 7 and 8).
Table 2 gives examples of each type, along with
the unpruned counts of distinct features gathered
from our training data. The examples for types
1 through 6 are taken from the abridged MRS
example in Figure 1. Note that an &amp; character
separates predicate and argument components in
the feature strings. The type 7 and 8 examples
are taken from MRS of sentences featuring the
phrases successfully explores and didn’t flow well,
respectively.
In our feature extraction pipeline, we use Good-
man’s pyDelphin4 tool, a Python module that al-
lows for easy manipulation and querying of MRS
constituents. This tool allows our pipeline to
quickly process the ERG output files, obtain argu-
ment and handle constraint information, and out-
put the features for each MRS into a feature file to
be read by our classifier. If the grammar has not
returned an analysis for a particular sentence, the
</bodyText>
<footnote confidence="0.871917666666667">
uary 15, 2014.
4Available at https://github.com/goodmami/pydelphin.
Accessed January 20, 2014.
</footnote>
<listItem confidence="0.6628288">
1. worst 6. awful
2. bad 7. great
3. not the worst 8. waste
4. the worst 9. excellent
5. not worst 10. not not even
</listItem>
<page confidence="0.836333">
24
</page>
<figure confidence="0.61330925">
There is nothing redeeming about this trash.
[LTOP: h0
INDEX:e2 [e SF:prop TENSE:pres MOOD:indicative PROG:- PERF:-]
&lt;[ be v there rel&lt;6:8&gt; LBL:h1 ARG0:e2 ARG1:x4] [thing rel&lt;9:16&gt; LBL:h5 ARG0:x4] [ no q rel&lt;9:16&gt;
LBL:h6 ARG0:x4 RSTR:h7 BODY:h8] [&amp;quot; redeem v for rel&amp;quot;&lt;17:26&gt; LBL:h5 ARG0:e9 ARG1:x4 ARG2:x10]
[&amp;quot; about x deg rel&amp;quot;&lt;27:32&gt; LBL:h11 ARG0:e12 ARG1:u13] [ this q dem rel&lt;33:37&gt; LBL:h11 ARG0:x10 RSTR:h14
BODY:h15] [&amp;quot; trash n 1 rel&amp;quot;&lt;38:44&gt; LBL:h16 ARG0:x10]&gt;
HCONS: &lt;h0 qeq h1 h7 qeq h5 h14 qeq h16&gt;]
</figure>
<figureCaption confidence="0.9843085">
Figure 1: Sample abridged MRS, with mood, tense, and other morphosemantic features removed. Each
EP is enclosed in square brackets, bold type denotes predicate values.
</figureCaption>
<table confidence="0.689606">
Type Description Example Count
</table>
<tableCaption confidence="0.906607">
Table 2: Sample features (Note: Types 1 - 6 are taken from the MRS in Figure 1)
</tableCaption>
<table confidence="0.95819375">
Pred value
PRP: all
PRP: string preds only
PRP: first pred back-off
PRP: seond pred back-off
PRP: double back-off
SL: handle not a neg rel arg
SL: handle a neg rel arg
no q rel
no q rel&amp;RSTR&amp;&amp;quot; redeem v for rel&amp;quot;
&amp;quot; redeem v for rel&amp;quot;&amp;ARG2&amp;&amp;quot; trash n 1 rel&amp;quot;
&amp;quot; POS v rel&amp;quot;&amp;ARG2&amp;&amp;quot; trash n 1 rel&amp;quot;
&amp;quot; redeem v for rel&amp;quot;&amp;ARG2&amp;&amp;quot; NEG n rel&amp;quot;
&amp;quot; POS v rel&amp;quot;&amp;ARG2&amp;&amp;quot; NEG n rel&amp;quot;
&amp;quot; successful a 1 rel&amp;quot;&amp;&amp;quot; explore v 1 rel&amp;quot;
neg rel&amp;&amp;quot; flow v 1 rel&amp;quot;&amp;&amp;quot; well a 1 rel&amp;quot;
</table>
<figure confidence="0.936711125">
4,505,389
10,255,021
941,831
635,047
621,929
20,962
589,887
43,427
1
2
3
4
5
6
7
8
</figure>
<bodyText confidence="0.410173">
pipeline simply does not output any features for
that sentence.
</bodyText>
<subsectionHeader confidence="0.471028">
3.2.1 MRS Crawling
</subsectionHeader>
<bodyText confidence="0.999981321428572">
In their revisiting of the 2012 SEM scope of nega-
tion shared task, Packard et al. (2014) improve on
the previous best performance using a relatively
simple set of MRS crawling techniques. We make
use of two of these techniques, “argument crawl-
ing” and “label crawling” in extracting our PRP
and SL features (ibid., p. 3). Both include select-
ing an “active EP” and adding to its scope all EPs
that conform to certain specifications. Argument
crawling selects all EPs whose distinguished vari-
able or label is an argument of the active EP, while
label crawling adds EPs that share a label with the
active EP (ibid., p. 3).
Our features are constructed in a similar fash-
ion; for every EP in an MRS, the pipeline se-
lects all EPs linked to the current EP and con-
structs features from this group of “in-scope”
EPs. PRP and SL features are obtained through
one “layer” of argument and label crawling, re-
spectively. After observing a number of noisy
and uninformative features in our preliminary
feature vectors, we excluded a small number
of EPs from being considered as the “active
EP” in our pipeline algorithm: udef q rel,
proper q rel, named rel, pron rel, and
pronoun q rel. More information about what
exactly these EPs represent can be found in Copes-
take et al. (2005).
</bodyText>
<subsubsectionHeader confidence="0.697291">
3.2.2 PRP Features
</subsubsectionHeader>
<bodyText confidence="0.999958111111111">
These feature types are a version of the depen-
dency triple features used in Oepen et al. (2007)
and Fujita et al. (2007). We define the linking re-
lation as one in which the value of any argument of
the first EP matches the distinguished variable or
label of the second EP. For handle variables, we
count any targets of a qeq constraint headed by
that variable as equivalent. We use the same set of
EP arguments as Pozen (2013) to link predicates
in our PRP features: ARG, ARG1-N, L-INDEX,
R-INDEX, L-HANDL, R-HANDL, and RESTR (p.
31).
We also use a set of negative and positive word
lists from the social media domain, developed by
Hu and Liu (2004), for back-off replacement in
PRP features. Our pipeline algorithm attempts
back-off replacement for all EPs in all PRP triples.
If the surface string portion of the predicate value
</bodyText>
<page confidence="0.992438">
25
</page>
<table confidence="0.829510294117647">
Pre-Feature Post-Feature
Filtering Filtering
88.337 88.289
87.293 87.503
88.253 87.977
88.709 88.781
88.961 88.853
88.637 88.865
88.853 88.961
88.889 88.973
88.793 89.021
88.865 89.093
Feature Types Pre-Feature Post-Feature
Filtering Filtering
baseline (n-grams only) 88.337 88.289
1 88.289 88.517
2 87.857 87.809
</table>
<figure confidence="0.976755555555556">
3 88.589 88.757
4 88.673 88.757
5 88.709 88.817
6 88.337 88.301
7 88.193 88.205
8 88.361 88.265
Feature Types
baseline (n-grams only)
n-grams with back-off
</figure>
<bodyText confidence="0.944869625">
MRS only (all types)
n-grams, 4, 5
n-grams, 3, 4, 5, 7, 8
n-grams, 1, 4, 5
n-grams, 3, 4, 5 8
n-grams, 3, 4, 5, 7
n-grams, 1, 3, 4, 5
n-grams, 3, 4, 5
</bodyText>
<tableCaption confidence="0.9999145">
Table 3: Individual MRS feature trial results
Table 4: Combination feature results
</tableCaption>
<bodyText confidence="0.999748333333333">
matches any of the entries in the lexicon, the
pipeline produces a back-off predicate value by re-
placing that portion with NEG or POS and strip-
ping the sense category marker. These replace-
ments appear in various positions in feature types
4, 5, and 6 (see Table 2).
</bodyText>
<subsectionHeader confidence="0.847567">
3.2.3 SL Features
</subsectionHeader>
<bodyText confidence="0.999909">
To further explore the relationships in the MRS,
we include this second feature category in our fea-
ture template, which links together EPs that share
a handle variable. We limit SL features to groups
of EPs linked by a handle variable that is also an
argument of another EP, or the target of a qeq con-
straint of such a variable. Our pipeline is therefore
able to extract both PRP and SL features in a sin-
gle pass through the arguments of each EP. Feature
type 7 consists of shared-label groupings of two
or more EPs, where the handle is not the ARG1
of a neg rel EP. Type 8 includes groups of one
or more EPs where the handle is a neg rel ar-
gument, with neg rel prepended to the feature
string.
Features of type 7 tend to capture relationships
between modifiers, such as adverbs and adjectives,
and modified entities. Features of type 8 were
intended to provide some negation information,
though our goals of more fully analyzing scope
of negation in our dataset remain unrealized at
this point. We reasoned that the lemmatization of
string predicate values might provide some useful
back-off for the semantic entities involved in nega-
tion and modification.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999994342105263">
To test our MRS features, we adapted our base-
line to treat them much like the n-gram features.
As with n-grams, each MRS feature is counted to-
ward the probability of the class of its source doc-
ument, and a negated version of that feature, with
not prepended, is counted toward the opposite
class. We ran our feature filtering trials using the
first development set, then obtained preliminary
accuracy figures from our second development set.
We began with each feature type in isolation and
used these results to inform later experiments us-
ing combinations of feature types. The numbers
reported here are the results over the final, held-
out test set.
Our final test accuracies indicate that three fea-
ture types produce the best gains in accuracy:
back-off PRPs with first- and second-predicate re-
placement (types 4 and 5), and PRPs with string
predicates only (type 3). Table 3 displays isolated
feature test results, while Table 4 ranks the top
seven feature combinations in ascending order by
post-feature filtering accuracies. The bolded fea-
ture types show that all of the best combination
runs include one or more of the top three features
mentioned above. Notable also are the accura-
cies for MRS-based features alone, which fall very
close to the baseline. The best accuracies for pre-
and post-feature filtering tests appear in bold.
The highest accuracy, achieved by running a
feature-filtered combination of the baseline’s n-
gram features and feature types 3, 4, and 5, re-
sulted in a 0.80% increase over the baseline per-
formance with feature filtering, and a 0.76% in-
crease in the best baseline accuracy overall (ob-
tained without feature filtering). The experimental
best run successfully categorizes 63 more of the
8333 test documents than the baseline best run.
Although these gains are small, they account for
</bodyText>
<page confidence="0.995498">
26
</page>
<tableCaption confidence="0.531287">
a 6.48% reduction in error.
</tableCaption>
<table confidence="0.603429">
Most Informative MRS Features
</table>
<tableCaption confidence="0.969815">
Table 5: Most informative MRS features
</tableCaption>
<sectionHeader confidence="0.998557" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.998068">
5.1 The Most Successful Experiments
</subsectionHeader>
<bodyText confidence="0.987661157894737">
The test accuracies indicate that our back-off re-
placement method, in combination with the simple
predicate-argument relationships captured in PRP
triples, is the most successful aspect of feature de-
sign in this project. However, as our error analysis
indicates, back-off is the likely source of many of
our system’s errors (see §5.2). Table 5 lists the 15
most informative MRS features from our best run
based on mutual information score, all of which
are of feature type 4 or 5. Note that the not
prepended to some features is a function of way
our classifier reads in binary features (as described
in §2.2), not an indication of grammatical nega-
tion. The success of these partial back-off fea-
tures confirms our intuition that the semantic rela-
tionships between sentiment-laden terms and other
entities in the sentence offer a reliable indicator
of author sentiment. When we performed back-
off replacement directly on the surface strings
and ran our classifier with n-grams only, we ob-
tained accuracies of 87.29% pre-feature filtering
and 87.50% post-feature filtering, a small decrease
from the baseline performance (see Table 4). This
lends additional support to the idea that the com-
bination of sentiment back-off and semantic de-
pendencies is significant. These results also fit
with the findings of of Joshi and Rose (2009), who
determined that back-off triple features provide
“more generalizable and useful patterns” in sen-
timent data than lexical dependency features alone
(p. 316).
Despite these promising results, we found that
the separate EP values (type 1), PRP triples with-
out replacement (type 2), PRPs with double re-
placement (type 6) and SL features (types 7 and
8) have very little effect on accuracy by them-
selves. For type 1, we suspect that EP values alone
don’t contribute enough information beyond ba-
sic n-gram features. We had hypothesized that the
lemmatization in these values might provide some
helpful back-off. However, this effect is likely
drowned out by the lack of any scope of negation
handling in the MRS features.
We attribute the failure of the SL features to the
fact that they often capture EPs originating in ad-
jacent tokens in the surface string, which does not
improve on the n-gram features. Lastly, we be-
lieve the relative sparsity of double back-off fea-
tures was the primary reason they did not produce
meaningful results.
These results also call into question the use-
fulness of the feature filtering trials in our base-
line. By design, these trials produce performance
increases on the dataset on which they are run.
However, filtering produces small and inconsistent
gains for the final held-out test set.
Error Types
</bodyText>
<table confidence="0.999520833333333">
Misleading back-off 31
Plot summary / Noise 20
Obscure Words / Data Sparsity 7
Data Error 3
Nonsensical Review 3
Reason Unsure 40
</table>
<tableCaption confidence="0.997555">
Table 6: Error types from top MRS experiment
</tableCaption>
<subsectionHeader confidence="0.99119">
5.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999958222222222">
We manually inspected the 104 reviews from the
final test set that were correctly classified by the
best run of the baseline system but incorrectly
classified by the best run of our improved sys-
tem. This set contains 50 false negatives, and 54
false positives. We classified them according to
five subjective categories: misleading back-off, in
which many of the sentiment terms have a polarity
opposite to the overall review; excess plot sum-
</bodyText>
<figure confidence="0.937001833333333">
not &amp;quot; NEG a rel&amp;quot;&amp;ARG1&amp; the q rel
&amp;quot; NEG a rel&amp;quot;&amp;ARG1&amp; the q rel
&amp;quot; POS a rel&amp;quot;&amp;ARG1&amp; the q rel
not &amp;quot; POS a rel&amp;quot;&amp;ARG1&amp; the q rel
&amp;quot; POS a rel&amp;quot;&amp;ARG1&amp; a q rel
not &amp;quot; POS a rel&amp;quot;&amp;ARG1&amp; a q rel
not &amp;quot; NEG a rel&amp;quot;&amp;ARG1&amp;&amp;quot; movie n of rel&amp;quot;
&amp;quot; NEG a rel&amp;quot;&amp;ARG1&amp;&amp;quot; movie n of rel&amp;quot;
a q rel&amp;RSTR&amp;&amp;quot; POS a rel&amp;quot;
not a q rel&amp;RSTR&amp;&amp;quot; POS a rel&amp;quot;
not &amp;quot; NEG a rel&amp;quot;&amp;ARG1&amp;udef q rel
&amp;quot; NEG a rel&amp;quot;&amp;ARG1&amp;udef q rel
superl rel&amp;ARG1&amp;&amp;quot; POS a rel&amp;quot;
not superl rel&amp;ARG1&amp;&amp;quot; POS a rel&amp;quot;
and c rel&amp;LHNDL&amp;&amp;quot; POS a rel&amp;quot;
27
Negative docs
Positive docs
Incorrectly classified
&amp;quot;_POS_a__rel&amp;quot;&amp;ARG1&amp;_the_c_rel
&amp;quot;_POS_a__rel&amp;quot;&amp;ARG1&amp;_a_c_rel
&amp;quot;_POS_a__rel&amp;quot;&amp;ARG1&amp;udef_c_rel
&amp;quot;_NEG_n__rel&amp;quot;&amp;ARG0&amp;udef_c_rel
_a_c_rel&amp;RSTR&amp;&amp;quot;_POS_a__rel&amp;quot;
&amp;quot;_NEG_n__rel&amp;quot;&amp;ARG0&amp;udef_c_rel
&amp;quot;_NEG_v__rel&amp;quot;&amp;ARG1&amp;pronoun_c_rel
&amp;quot;_NEG_v__rel&amp;quot;&amp;ARG1&amp;pron_rel
&amp;quot;_NEG_a__rel&amp;quot;&amp;ARG1&amp;udef_c_rel
&amp;quot;_NEG_a__rel&amp;quot;&amp;ARG1&amp;_the_c_rel
Correctly classified
</figure>
<bodyText confidence="0.7040366">
&amp;quot;_NEG_n__rel&amp;quot;&amp;ARG0&amp;udef_c_rel
&amp;quot;_NEG_a__rel&amp;quot;&amp;ARG1&amp;_the_c_rel
&amp;quot;_NEG_a__rel&amp;quot;&amp;ARG1&amp;udef_c_rel
&amp;quot;_POS_a__rel&amp;quot;&amp;ARG1&amp;_a_c_rel
_the_c_rel&amp;RSTR&amp;&amp;quot;_NEG_n__rel&amp;quot;
&amp;quot;_POS_a__rel&amp;quot;&amp;ARG1&amp;_a_c_rel
&amp;quot;_POS_a__rel&amp;quot;&amp;ARG1&amp;_the_c_rel
_a_c_rel&amp;RSTR&amp;&amp;quot;_POS_a__rel&amp;quot;
&amp;quot;_NEG_n__rel&amp;quot;&amp;ARG0&amp;udef_c_rel
&amp;quot;_POS_a__rel&amp;quot;&amp;ARG1&amp;udef_c_rel
</bodyText>
<tableCaption confidence="0.914888">
Table 7: Most frequent features in test data by polarity and classification result
</tableCaption>
<bodyText confidence="0.999963619047619">
mary or off-topic language; use of obscure words
not likely to occur frequently in the data; miscat-
egorization in the dataset; and confusing or non-
sensical language. The counts for these categories
appear in Table 6.
The prevalence of errors in the first category
is revealing, and relates to certain subcategories
of review that confound our sentiment back-off
features. For horror films in particular, words
that would generally convey negative sentiment
(creepy, horrible, gruesome) are instead used pos-
itively. This presents an obvious problem for senti-
ment back-off, which relies on the assumption that
words are generally used with the same intent.
To explore this further, we collected counts of
the most frequent features in these 104 reviews,
and compared them to feature counts for correctly
classified documents of the same class. The stark
contrast between the back-off polarities of the fea-
tures extracted and the polarity of the documents
suggests that these feature types are overgener-
alizing and misleading the classifier (see Table
7). While the course-grained polarity of sentiment
terms is often a good indicator of overall review
polarity, our system has difficulty with cases in
which many sentiment terms do not align with the
review sentiment. Our back-off PRP features do
not include scope of negation handling, so even if
these terms are negated, our classifier in its current
form is unable to take advantage of that informa-
tion.
Further manual observation of the feature vec-
tors from these documents suggests that the senti-
ment lexicon contains elements that are not suited
to the movie review domain; plot, for example is
classified as a negative term. These results point
to the need for a more domain-specific sentiment
lexicon, and perhaps additional features that look
at the combination of sentiment terms present in a
review. LDA models could provide some guidance
in capturing and analyzing co-occurring groups of
sentiment terms.
</bodyText>
<sectionHeader confidence="0.998494" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999966038461539">
Our attempt to improve binary sentiment classifi-
cation with MRS-based features is motivated by a
desire to move beyond shallow approaches and ex-
plore the potential for features based on semantic
dependencies. Our preliminary results are promis-
ing, if modest, and point to back-off replacement
as a useful tool in combination with the relation-
ships captured by predicate triples.
There are a number of potential areas for im-
provement and further development of our ap-
proach. In light of Wang and Manning’s (2012) re-
sults using an SVM classifier on the same dataset,
one obvious direction would be to experiment with
this and other machine learning algorithms. Addi-
tionally, the ability to account for negation in the
MRS features types as in Packard et al. (2014)
would likely mitigate some of the errors caused
by the back-off PRP features
Another possibility for expansion would be the
development of features using larger feature sub-
graphs. Because of concerns about runtime and
data sparsity, we crawl only one level of the MRS
and examine a limited set of relationships. The
success of Socher et al.’s (2013) Recursive Neural
Tensor Network suggest that with enough data, it
is possible to capture the complex compositional
</bodyText>
<page confidence="0.99462">
28
</page>
<bodyText confidence="0.999940428571429">
effects of various sub-components. Given their
success with syntactic dependencies, and the re-
search presented here, we believe semantic de-
pendencies will be a fruitful avenue for future re-
search in sentiment analysis. This project has been
an exciting first step into uncharted territory, and
suggests the potential to further exploit the MRS
in sentiment analysis applications. Nonetheless,
the performance gains we were able to observe
demonstrate the power of using semantic repre-
sentations produced by a linguistically motivated,
broad-coverage parser as an information source
in a semantically sensitive task such as sentiment
analysis.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999743">
Thanks to our professor, Emily Bender, for pro-
viding her expertise and guidance at all stages of
this project.
We’re grateful to Michael Goodman for making
his pyDelphin module freely available, guiding
us in using it, and providing timely troubleshoot-
ing and support via email for the duration of this
project. Thanks to Woodley Packard, who pro-
vided helpful advice on getting the best use out
of ACE.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999559329113924">
S. Arora, E. Mayfield, C Penstein-Ros´e, and E Nyberg.
2010. Sentiment Classification using Automatically
Extracted Subgraph Features. In Proceedings of the
NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in
Text, pp. 131 - 139. Los Angeles, CA.
A. Copestake, D. Flickinger, C. Pollard, and I. A. Sag.
2005. Minimal Recursion Semantics: An Introduc-
tion. Research on Natural Language and Computa-
tion, 3(4), pp. 281 - 332.
D. Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering,, 6(1), pp. 15 - 28.
S. Fujita, F. Bond, S. Oepen, T. Tanaka. 2010. Exploit-
ing semantic information for HPSG parse selection.
Research on Language and Computation. 8(1): 1-22
M. Hu and B. Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proceedings of the ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD-2004). Seattle,
WA.
M. Joshi and C Penstein Ros´e. 2009. Generalizing
Dependency Features for Opinion Mining. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pp. 313 - 316. Suntec, Singapore.
E. Loper, and S., Bird. 2002. NLTK: The Natu-
ral Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics. Philadelphia: Association for
Computational Linguistics
L. Jia, C. Yu, and W. Meng. 2009. The effect of nega-
tion on sentiment analysis and retrieval effective-
ness. In Proceedings of the 18th ACM conference
on Information and knowledge management (CIKM
’09), pp. 1827 - 1830. Hong Kong, China.
A. L. Maas, R. E. Daly, Peter T. Pham, Dan Huang,
Andrew Y. Ng, and C. Potts. 2011. Learning word
vectors for sentiment analysis. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pp. 142 - 150. Portland, Oregon.
V. Narayanan, I. Arora, and A. Bhatia. 2013. Fast and
accurate sentiment classification using an enhanced
Naive Bayes’model. Intelligent Data Engineering
and Automated Learning IDEAL function Lecture
Notes in Computer Science, 8206:194 - 201.
S. Oepen, E. Velldal, J. Lonning, P. Meurer, V. Rosn,
and D. Flickinger. 2007. Towards Hybrid Qual-
ity Oriented Machine Translation. In Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation.
W. Packard, E. M. Bender, J. Read, S. Oepen and R.
Dridan. 2014. Simple Negation Scope Resolution
Through Deep Parsing: A Semantic Solution to a
Semantic Problem. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics. Baltimore, MD.
B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1 - 135.
Z. Pozen. 2013. Using Lexical and Compositional Se-
mantics to Improve HPSG Parse Selection. Master’s
Thesis, University of Washington.
R. Socher, A. Perelygin, J. Wu, J. Chuang. C. Man-
ning, A. Ng, and C. Potts 2013. Recursive Deep
Models for Semantic Compositionality Over a Sen-
timent Treebank In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pp. 1631-1642. Seattle, WA.
S. Wang and C. D. Manning. 2012. Baselines and Bi-
grams: Simple, Good Sentiment and Topic Classica-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, pp.
90 94. Jeju, Republic of Korea.
A Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceedings of
the 18th Conference on Computational Linguistics
(COLING), pp. 147 - 153.
</reference>
<page confidence="0.999105">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.498370">
<title confidence="0.9983895">Improvement of a Naive Bayes Sentiment Classifier Using MRS-Based Features</title>
<author confidence="0.995194">Jared</author>
<affiliation confidence="0.999583">University of</affiliation>
<address confidence="0.534812">Seattle,</address>
<email confidence="0.999949">jaredkk@uw.edu</email>
<abstract confidence="0.996374352941176">This study explores the potential of using deep semantic features to improve binary sentiment classification of paragraphlength movie reviews from the IMBD website. Using a Naive Bayes classifier as a baseline, we show that features extracted from Minimal Recursion Semantics representations in conjunction with back-off replacement of sentiment terms is effective in obtaining moderate increases in accuracy over the baseline’s n-gram features. Although our results are mixed, our most successful feature combination achieves an accuracy of 89.09%, which represents an increase of 0.76% over the baseline performance and a 6.48% reduction in error.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Arora</author>
<author>E Mayfield</author>
<author>C Penstein-Ros´e</author>
<author>E Nyberg</author>
</authors>
<title>Sentiment Classification using Automatically Extracted Subgraph Features.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>131--139</pages>
<location>Los Angeles, CA.</location>
<marker>Arora, Mayfield, Penstein-Ros´e, Nyberg, 2010</marker>
<rawString>S. Arora, E. Mayfield, C Penstein-Ros´e, and E Nyberg. 2010. Sentiment Classification using Automatically Extracted Subgraph Features. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pp. 131 - 139. Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Minimal Recursion Semantics: An Introduction.</title>
<date>2005</date>
<journal>Research on Natural Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>281--332</pages>
<contexts>
<context position="1979" citStr="Copestake et al., 2005" startWordPosition="289" endWordPosition="292"> network analysis (ibid., p. 12). While previous research in sentiment analysis has investigated the extraction of features from syntactic dependency trees, semantic representations appear to be underused as a resource for modeling opinion in text. Indeed, to our knowledge, there has been no research using semantic dependencies created by a precision grammar for sentiment analysis. The goal of the present research is to address this gap by augmenting a Clara Gordon University of Washington Seattle, WA cgordon1@uw.edu baseline classifier with features based on Minimal Resursion Semantics (MRS; Copestake et al., 2005), a formal semantic representation provided by the English Resource Grammar (ERG; Flickinger, 2000). An MRS is a connected graph in which semantic entities may be linked directly through shared arguments or indirectly through handle or qeq constraints, which denote equality modulo quantifier insertion (Copestake et al., 2005). This schema allows for underspecification of quantifier scope. Using Narayanan et al.’s (2013) Naive Bayes sentiment classifier as a baseline, we test the effectiveness of eight feature types derived from MRS. Our feature pipeline crawls various links in the MRS represen</context>
<context position="15390" citStr="Copestake et al. (2005)" startWordPosition="2470" endWordPosition="2474">cted in a similar fashion; for every EP in an MRS, the pipeline selects all EPs linked to the current EP and constructs features from this group of “in-scope” EPs. PRP and SL features are obtained through one “layer” of argument and label crawling, respectively. After observing a number of noisy and uninformative features in our preliminary feature vectors, we excluded a small number of EPs from being considered as the “active EP” in our pipeline algorithm: udef q rel, proper q rel, named rel, pron rel, and pronoun q rel. More information about what exactly these EPs represent can be found in Copestake et al. (2005). 3.2.2 PRP Features These feature types are a version of the dependency triple features used in Oepen et al. (2007) and Fujita et al. (2007). We define the linking relation as one in which the value of any argument of the first EP matches the distinguished variable or label of the second EP. For handle variables, we count any targets of a qeq constraint headed by that variable as equivalent. We use the same set of EP arguments as Pozen (2013) to link predicates in our PRP features: ARG, ARG1-N, L-INDEX, R-INDEX, L-HANDL, R-HANDL, and RESTR (p. 31). We also use a set of negative and positive w</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>A. Copestake, D. Flickinger, C. Pollard, and I. A. Sag. 2005. Minimal Recursion Semantics: An Introduction. Research on Natural Language and Computation, 3(4), pp. 281 - 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>15--28</pages>
<contexts>
<context position="2078" citStr="Flickinger, 2000" startWordPosition="305" endWordPosition="306">action of features from syntactic dependency trees, semantic representations appear to be underused as a resource for modeling opinion in text. Indeed, to our knowledge, there has been no research using semantic dependencies created by a precision grammar for sentiment analysis. The goal of the present research is to address this gap by augmenting a Clara Gordon University of Washington Seattle, WA cgordon1@uw.edu baseline classifier with features based on Minimal Resursion Semantics (MRS; Copestake et al., 2005), a formal semantic representation provided by the English Resource Grammar (ERG; Flickinger, 2000). An MRS is a connected graph in which semantic entities may be linked directly through shared arguments or indirectly through handle or qeq constraints, which denote equality modulo quantifier insertion (Copestake et al., 2005). This schema allows for underspecification of quantifier scope. Using Narayanan et al.’s (2013) Naive Bayes sentiment classifier as a baseline, we test the effectiveness of eight feature types derived from MRS. Our feature pipeline crawls various links in the MRS representations of sentences in our corpus of paragraph-length movie reviews and outputs simple, human-read</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>D. Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering,, 6(1), pp. 15 - 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fujita</author>
<author>F Bond</author>
<author>S Oepen</author>
<author>T Tanaka</author>
</authors>
<title>Exploiting semantic information for HPSG parse selection.</title>
<date>2010</date>
<journal>Research on Language and Computation.</journal>
<volume>8</volume>
<issue>1</issue>
<pages>1--22</pages>
<marker>Fujita, Bond, Oepen, Tanaka, 2010</marker>
<rawString>S. Fujita, F. Bond, S. Oepen, T. Tanaka. 2010. Exploiting semantic information for HPSG parse selection. Research on Language and Computation. 8(1): 1-22</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004).</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="16060" citStr="Hu and Liu (2004)" startWordPosition="2593" endWordPosition="2596">n of the dependency triple features used in Oepen et al. (2007) and Fujita et al. (2007). We define the linking relation as one in which the value of any argument of the first EP matches the distinguished variable or label of the second EP. For handle variables, we count any targets of a qeq constraint headed by that variable as equivalent. We use the same set of EP arguments as Pozen (2013) to link predicates in our PRP features: ARG, ARG1-N, L-INDEX, R-INDEX, L-HANDL, R-HANDL, and RESTR (p. 31). We also use a set of negative and positive word lists from the social media domain, developed by Hu and Liu (2004), for back-off replacement in PRP features. Our pipeline algorithm attempts back-off replacement for all EPs in all PRP triples. If the surface string portion of the predicate value 25 Pre-Feature Post-Feature Filtering Filtering 88.337 88.289 87.293 87.503 88.253 87.977 88.709 88.781 88.961 88.853 88.637 88.865 88.853 88.961 88.889 88.973 88.793 89.021 88.865 89.093 Feature Types Pre-Feature Post-Feature Filtering Filtering baseline (n-grams only) 88.337 88.289 1 88.289 88.517 2 87.857 87.809 3 88.589 88.757 4 88.673 88.757 5 88.709 88.817 6 88.337 88.301 7 88.193 88.205 8 88.361 88.265 Featu</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004). Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Joshi</author>
<author>C Penstein Ros´e</author>
</authors>
<title>Generalizing Dependency Features for Opinion Mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>313--316</pages>
<publisher>Suntec,</publisher>
<marker>Joshi, Ros´e, 2009</marker>
<rawString>M. Joshi and C Penstein Ros´e. 2009. Generalizing Dependency Features for Opinion Mining. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pp. 313 - 316. Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics</booktitle>
<contexts>
<context position="7037" citStr="Loper and Bird, 2002" startWordPosition="1075" endWordPosition="1078">r methodology. 2.1 The IMBD Dataset We use a dataset of 50,000 movie reviews crawled from the IMDB website, originally developed by Maas et al. (2011). The dataset is split equally between training and test sets. Both training and test sets contain equal numbers of positive and negative reviews, which are defined according to the number of stars assigned by the author on the IMBD website: one to four stars for negative reviews, and seven to ten stars for positive reivews. The reviews vary in length but generally contain between five and fifteen sentences. The Natural Language ToolKit’s (NLTK; Loper and Bird, 2002) sentence tokenizer distinguishes 616,995 sentences in the dataset. Unlike previous research over this dataset, we divide the 25,000 reviews of the test set into two development sets and a final test set. As such, our results are not directly comparable to those of Wang &amp; Manning (2012). 2.2 The Baseline System The system we use as a baseline, created by Narayanan et al. (2013), implements several small but innovative improvements to a simple Naive Bayes classifier. In the training phase, the baseline performs simple scope of negation annotation on the surface string tokens. Any word containin</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>E. Loper, and S., Bird. 2002. NLTK: The Natural Language Toolkit. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Jia</author>
<author>C Yu</author>
<author>W Meng</author>
</authors>
<title>The effect of negation on sentiment analysis and retrieval effectiveness.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management (CIKM ’09),</booktitle>
<pages>1827--1830</pages>
<location>Hong Kong, China.</location>
<marker>Jia, Yu, Meng, 2009</marker>
<rawString>L. Jia, C. Yu, and W. Meng. 2009. The effect of negation on sentiment analysis and retrieval effectiveness. In Proceedings of the 18th ACM conference on Information and knowledge management (CIKM ’09), pp. 1827 - 1830. Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Maas</author>
<author>R E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>C Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--150</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="6566" citStr="Maas et al. (2011)" startWordPosition="996" endWordPosition="999"> of words and grammatical relations extracted from dependency parses. To increase the generalizability of these triples, they perform back-off by replacing words with part-ofspeech tags. Similarly, Arora et al. (2010) extract features from dependency parses by using sentiment back-off to identify potentially meaningful portions of the dependency graph. Given this success combining back-off with sub-graph features, we design several feature types following a similar methodology. 2.1 The IMBD Dataset We use a dataset of 50,000 movie reviews crawled from the IMDB website, originally developed by Maas et al. (2011). The dataset is split equally between training and test sets. Both training and test sets contain equal numbers of positive and negative reviews, which are defined according to the number of stars assigned by the author on the IMBD website: one to four stars for negative reviews, and seven to ten stars for positive reivews. The reviews vary in length but generally contain between five and fifteen sentences. The Natural Language ToolKit’s (NLTK; Loper and Bird, 2002) sentence tokenizer distinguishes 616,995 sentences in the dataset. Unlike previous research over this dataset, we divide the 25,</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A. L. Maas, R. E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and C. Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142 - 150. Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Narayanan</author>
<author>I Arora</author>
<author>A Bhatia</author>
</authors>
<title>Fast and accurate sentiment classification using an enhanced Naive Bayes’model.</title>
<date>2013</date>
<booktitle>Intelligent Data Engineering and Automated Learning IDEAL function Lecture Notes in Computer Science,</booktitle>
<pages>8206--194</pages>
<contexts>
<context position="7417" citStr="Narayanan et al. (2013)" startWordPosition="1139" endWordPosition="1142">IMBD website: one to four stars for negative reviews, and seven to ten stars for positive reivews. The reviews vary in length but generally contain between five and fifteen sentences. The Natural Language ToolKit’s (NLTK; Loper and Bird, 2002) sentence tokenizer distinguishes 616,995 sentences in the dataset. Unlike previous research over this dataset, we divide the 25,000 reviews of the test set into two development sets and a final test set. As such, our results are not directly comparable to those of Wang &amp; Manning (2012). 2.2 The Baseline System The system we use as a baseline, created by Narayanan et al. (2013), implements several small but innovative improvements to a simple Naive Bayes classifier. In the training phase, the baseline performs simple scope of negation annotation on the surface string tokens. Any word containing the characters not, n’t or no triggers a “negated” state, in which all following n-grams are prepended with not . This continues until either a punctuation delimiter (?.,!:;) or another negation trigger is encountered. During training, when an n-gram feature is read into the classifier, it is counted toward P(f|c), and the same feature with not prepended is counted toward P(f</context>
</contexts>
<marker>Narayanan, Arora, Bhatia, 2013</marker>
<rawString>V. Narayanan, I. Arora, and A. Bhatia. 2013. Fast and accurate sentiment classification using an enhanced Naive Bayes’model. Intelligent Data Engineering and Automated Learning IDEAL function Lecture Notes in Computer Science, 8206:194 - 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>E Velldal</author>
<author>J Lonning</author>
<author>P Meurer</author>
<author>V Rosn</author>
<author>D Flickinger</author>
</authors>
<title>Towards Hybrid Quality Oriented Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="5064" citStr="Oepen et al. (2007)" startWordPosition="759" endWordPosition="762"> our dataset, they obtain the highest accuracies using a hybrid approach, SVM with Naive Bayes features, which results in 91.22% accuracy (ibid., p. 93). This appears to be the best test result to date on this dataset. Although we use a Naive Bayes classifier in our project, alternative machine learning algorithms are a promising topic of further future investigation (see §6). Two existing areas of research have direct relevance to this project: MRS feature extraction, and sentiment analysis using features based on deep linguistic representations of data. In their work on machine translation, Oepen et al. (2007) define a type of MRS triple based on elementary dependencies, a simplified “variable-free” representation of predicate-argument relations in MRS (p. 5). Fujita et al. (2007) and Pozen (2013) develop similar features for HPSG parse selection, and Pozen experiments with replacing segments of predicate values in triple features with WordNet sense, POS, and lemma information (2013, p. 32). While there has not yet been any research on using MRS features in sentiment analysis, there has been work on extracting features from deep representations of data for sentiment analysis. In working with deep r</context>
<context position="15506" citStr="Oepen et al. (2007)" startWordPosition="2492" endWordPosition="2495">features from this group of “in-scope” EPs. PRP and SL features are obtained through one “layer” of argument and label crawling, respectively. After observing a number of noisy and uninformative features in our preliminary feature vectors, we excluded a small number of EPs from being considered as the “active EP” in our pipeline algorithm: udef q rel, proper q rel, named rel, pron rel, and pronoun q rel. More information about what exactly these EPs represent can be found in Copestake et al. (2005). 3.2.2 PRP Features These feature types are a version of the dependency triple features used in Oepen et al. (2007) and Fujita et al. (2007). We define the linking relation as one in which the value of any argument of the first EP matches the distinguished variable or label of the second EP. For handle variables, we count any targets of a qeq constraint headed by that variable as equivalent. We use the same set of EP arguments as Pozen (2013) to link predicates in our PRP features: ARG, ARG1-N, L-INDEX, R-INDEX, L-HANDL, R-HANDL, and RESTR (p. 31). We also use a set of negative and positive word lists from the social media domain, developed by Hu and Liu (2004), for back-off replacement in PRP features. Ou</context>
</contexts>
<marker>Oepen, Velldal, Lonning, Meurer, Rosn, Flickinger, 2007</marker>
<rawString>S. Oepen, E. Velldal, J. Lonning, P. Meurer, V. Rosn, and D. Flickinger. 2007. Towards Hybrid Quality Oriented Machine Translation. In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Packard</author>
<author>E M Bender</author>
<author>J Read</author>
<author>S Oepen</author>
<author>R Dridan</author>
</authors>
<title>Simple Negation Scope Resolution Through Deep Parsing: A Semantic Solution to a Semantic Problem.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="14211" citStr="Packard et al. (2014)" startWordPosition="2264" endWordPosition="2267">: seond pred back-off PRP: double back-off SL: handle not a neg rel arg SL: handle a neg rel arg no q rel no q rel&amp;RSTR&amp;&amp;quot; redeem v for rel&amp;quot; &amp;quot; redeem v for rel&amp;quot;&amp;ARG2&amp;&amp;quot; trash n 1 rel&amp;quot; &amp;quot; POS v rel&amp;quot;&amp;ARG2&amp;&amp;quot; trash n 1 rel&amp;quot; &amp;quot; redeem v for rel&amp;quot;&amp;ARG2&amp;&amp;quot; NEG n rel&amp;quot; &amp;quot; POS v rel&amp;quot;&amp;ARG2&amp;&amp;quot; NEG n rel&amp;quot; &amp;quot; successful a 1 rel&amp;quot;&amp;&amp;quot; explore v 1 rel&amp;quot; neg rel&amp;&amp;quot; flow v 1 rel&amp;quot;&amp;&amp;quot; well a 1 rel&amp;quot; 4,505,389 10,255,021 941,831 635,047 621,929 20,962 589,887 43,427 1 2 3 4 5 6 7 8 pipeline simply does not output any features for that sentence. 3.2.1 MRS Crawling In their revisiting of the 2012 SEM scope of negation shared task, Packard et al. (2014) improve on the previous best performance using a relatively simple set of MRS crawling techniques. We make use of two of these techniques, “argument crawling” and “label crawling” in extracting our PRP and SL features (ibid., p. 3). Both include selecting an “active EP” and adding to its scope all EPs that conform to certain specifications. Argument crawling selects all EPs whose distinguished variable or label is an argument of the active EP, while label crawling adds EPs that share a label with the active EP (ibid., p. 3). Our features are constructed in a similar fashion; for every EP in a</context>
<context position="27645" citStr="Packard et al. (2014)" startWordPosition="4452" endWordPosition="4455"> explore the potential for features based on semantic dependencies. Our preliminary results are promising, if modest, and point to back-off replacement as a useful tool in combination with the relationships captured by predicate triples. There are a number of potential areas for improvement and further development of our approach. In light of Wang and Manning’s (2012) results using an SVM classifier on the same dataset, one obvious direction would be to experiment with this and other machine learning algorithms. Additionally, the ability to account for negation in the MRS features types as in Packard et al. (2014) would likely mitigate some of the errors caused by the back-off PRP features Another possibility for expansion would be the development of features using larger feature subgraphs. Because of concerns about runtime and data sparsity, we crawl only one level of the MRS and examine a limited set of relationships. The success of Socher et al.’s (2013) Recursive Neural Tensor Network suggest that with enough data, it is possible to capture the complex compositional 28 effects of various sub-components. Given their success with syntactic dependencies, and the research presented here, we believe sem</context>
</contexts>
<marker>Packard, Bender, Read, Oepen, Dridan, 2014</marker>
<rawString>W. Packard, E. M. Bender, J. Read, S. Oepen and R. Dridan. 2014. Simple Negation Scope Resolution Through Deep Parsing: A Semantic Solution to a Semantic Problem. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="996" citStr="Pang and Lee (2008)" startWordPosition="147" endWordPosition="150">ures extracted from Minimal Recursion Semantics representations in conjunction with back-off replacement of sentiment terms is effective in obtaining moderate increases in accuracy over the baseline’s n-gram features. Although our results are mixed, our most successful feature combination achieves an accuracy of 89.09%, which represents an increase of 0.76% over the baseline performance and a 6.48% reduction in error. 1 Introduction Text-based sentiment analysis offers valuable insight into the opinions of large communities of reviewers, commenters and customers. In their survey of the field, Pang and Lee (2008) highlight the importance of sentiment analysis across a range of industries, including review aggregation websites, business intelligence, and reputation management. Detection and classification of sentiment can improve downstream performance in applications sensitive to user opinions, such as questionanswering, automatic product recommendations, and social network analysis (ibid., p. 12). While previous research in sentiment analysis has investigated the extraction of features from syntactic dependency trees, semantic representations appear to be underused as a resource for modeling opinion </context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2):1 - 135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Pozen</author>
</authors>
<title>Using Lexical and Compositional Semantics to Improve HPSG Parse Selection. Master’s Thesis,</title>
<date>2013</date>
<institution>University of Washington.</institution>
<contexts>
<context position="5255" citStr="Pozen (2013)" startWordPosition="791" endWordPosition="792">ate on this dataset. Although we use a Naive Bayes classifier in our project, alternative machine learning algorithms are a promising topic of further future investigation (see §6). Two existing areas of research have direct relevance to this project: MRS feature extraction, and sentiment analysis using features based on deep linguistic representations of data. In their work on machine translation, Oepen et al. (2007) define a type of MRS triple based on elementary dependencies, a simplified “variable-free” representation of predicate-argument relations in MRS (p. 5). Fujita et al. (2007) and Pozen (2013) develop similar features for HPSG parse selection, and Pozen experiments with replacing segments of predicate values in triple features with WordNet sense, POS, and lemma information (2013, p. 32). While there has not yet been any research on using MRS features in sentiment analysis, there has been work on extracting features from deep representations of data for sentiment analysis. In working with deep representations such as MRSes or dependency parses, there are myriad sub-graphs that can be used as features. However these features are often quite sparse and do not generalize well. Joshi &amp; </context>
<context position="11437" citStr="Pozen, 2013" startWordPosition="1784" endWordPosition="1785">aning of the data would yield higher coverage. 3.2 Feature Design Our main focus in feature design is capturing relevant semantic relationships between sentiment terms that extend beyond the trigram boundary. Our entry point into the MRS is the elementary predication (EP), and our pipeline algorithm explores the three main EP components: arguments and associated variables, label, and predicate symbol. We also use the set of handle constraints in crawling the links between EPs. We use two main categories of crawled MRS features: Predicate-Relation-Predicate (PRP) triples, a term borrowed from (Pozen, 2013), and Shared-Label (SL) features. Our feature template consists of eight feature subtypes, including plain EP symbols (type 1), five PRP features (types 2 through 6) and two SL features (types 7 and 8). Table 2 gives examples of each type, along with the unpruned counts of distinct features gathered from our training data. The examples for types 1 through 6 are taken from the abridged MRS example in Figure 1. Note that an &amp; character separates predicate and argument components in the feature strings. The type 7 and 8 examples are taken from MRS of sentences featuring the phrases successfully e</context>
<context position="15837" citStr="Pozen (2013)" startWordPosition="2556" endWordPosition="2557">hm: udef q rel, proper q rel, named rel, pron rel, and pronoun q rel. More information about what exactly these EPs represent can be found in Copestake et al. (2005). 3.2.2 PRP Features These feature types are a version of the dependency triple features used in Oepen et al. (2007) and Fujita et al. (2007). We define the linking relation as one in which the value of any argument of the first EP matches the distinguished variable or label of the second EP. For handle variables, we count any targets of a qeq constraint headed by that variable as equivalent. We use the same set of EP arguments as Pozen (2013) to link predicates in our PRP features: ARG, ARG1-N, L-INDEX, R-INDEX, L-HANDL, R-HANDL, and RESTR (p. 31). We also use a set of negative and positive word lists from the social media domain, developed by Hu and Liu (2004), for back-off replacement in PRP features. Our pipeline algorithm attempts back-off replacement for all EPs in all PRP triples. If the surface string portion of the predicate value 25 Pre-Feature Post-Feature Filtering Filtering 88.337 88.289 87.293 87.503 88.253 87.977 88.709 88.781 88.961 88.853 88.637 88.865 88.853 88.961 88.889 88.973 88.793 89.021 88.865 89.093 Feature</context>
</contexts>
<marker>Pozen, 2013</marker>
<rawString>Z. Pozen. 2013. Using Lexical and Compositional Semantics to Improve HPSG Parse Selection. Master’s Thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>A Ng</author>
<author>C Potts</author>
</authors>
<title>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<location>Seattle, WA.</location>
<marker>Manning, Ng, Potts, 2013</marker>
<rawString>R. Socher, A. Perelygin, J. Wu, J. Chuang. C. Manning, A. Ng, and C. Potts 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>C D Manning</author>
</authors>
<title>Baselines and Bigrams: Simple, Good Sentiment and Topic Classication.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>90--94</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="4037" citStr="Wang and Manning (2012)" startWordPosition="599" endWordPosition="602">rk Current approaches to sentiment analysis tasks typically use supervised machine learning meth1Because this task consists of binary classification on an evenly split dataset and every test document is assigned a class, simple accuracy is the most appropriate measure of performance. 22 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 22–29, Dublin, Ireland, August 23-24 2014. ods with bag-of-words features as a baseline, and for classification of longer documents like the ones in our dataset, such features remain a powerful tool of analysis. Wang and Manning (2012) compare the performance of several machine learning algorithms using uni- and bigram features from a variety of common sentiment datasets, including the IMDB set used in this project. They report that that SVM classifiers generally perform better sentiment classification on paragraph-length reviews, while Native Bayes classifiers produce better results for “snippets,” or short phrases (ibid., p. 91). For our dataset, they obtain the highest accuracies using a hybrid approach, SVM with Naive Bayes features, which results in 91.22% accuracy (ibid., p. 93). This appears to be the best test resul</context>
<context position="7324" citStr="Wang &amp; Manning (2012)" startWordPosition="1122" endWordPosition="1125"> reviews, which are defined according to the number of stars assigned by the author on the IMBD website: one to four stars for negative reviews, and seven to ten stars for positive reivews. The reviews vary in length but generally contain between five and fifteen sentences. The Natural Language ToolKit’s (NLTK; Loper and Bird, 2002) sentence tokenizer distinguishes 616,995 sentences in the dataset. Unlike previous research over this dataset, we divide the 25,000 reviews of the test set into two development sets and a final test set. As such, our results are not directly comparable to those of Wang &amp; Manning (2012). 2.2 The Baseline System The system we use as a baseline, created by Narayanan et al. (2013), implements several small but innovative improvements to a simple Naive Bayes classifier. In the training phase, the baseline performs simple scope of negation annotation on the surface string tokens. Any word containing the characters not, n’t or no triggers a “negated” state, in which all following n-grams are prepended with not . This continues until either a punctuation delimiter (?.,!:;) or another negation trigger is encountered. During training, when an n-gram feature is read into the classifie</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>S. Wang and C. D. Manning. 2012. Baselines and Bigrams: Simple, Good Sentiment and Topic Classication. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pp. 90 94. Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics (COLING),</booktitle>
<pages>147--153</pages>
<marker>Yeh, 2000</marker>
<rawString>A Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th Conference on Computational Linguistics (COLING), pp. 147 - 153.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>