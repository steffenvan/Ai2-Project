<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.5094415">
Modeling Spoken Decision Making Dialogue
and Optimization of its Dialogue Strategy
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai and Satoshi Nakamura
</note>
<address confidence="0.4523345">
MASTAR Project, NICT
Kyoto, Japan.
</address>
<email confidence="0.995685">
teruhisa.misu@nict.go.jp
</email>
<sectionHeader confidence="0.99474" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999">
This paper presents a spoken dialogue frame-
work that helps users in making decisions.
Users often do not have a definite goal or cri-
teria for selecting from a list of alternatives.
Thus the system has to bridge this knowledge
gap and also provide the users with an appro-
priate alternative together with the reason for
this recommendation through dialogue. We
present a dialogue state model for such deci-
sion making dialogue. To evaluate this model,
we implement a trial sightseeing guidance sys-
tem and collect dialogue data. Then, we opti-
mize the dialogue strategy based on the state
model through reinforcement learning with a
natural policy gradient approach using a user
simulator trained on the collected dialogue
corpus.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971076923077">
In many situations where spoken dialogue interfaces
are used, information access by the user is not a goal in
itself, but a means for decision making (Polifroni and
Walker, 2008). For example, in a restaurant retrieval
system, the user’s goal may not be the extraction of
price information but to make a decision on candidate
restaurants based on the retrieved information.
This work focuses on how to assist a user who is
using the system for his/her decision making, when
he/she does not have enough knowledge about the tar-
get domain. In such a situation, users are often un-
aware of not only what kind of information the sys-
tem can provide but also their own preference or fac-
tors that they should emphasize. The system, too, has
little knowledge about the user, or where his/her inter-
ests lie. Thus, the system has to bridge such gaps by
sensing (potential) preferences of the user and recom-
mend information that the user would be interested in,
considering a trade-off with the length of the dialogue.
We propose a model of dialogue state that consid-
ers the user’s preferences as well as his/her knowledge
about the domain changing through a decision making
dialogue. A user simulator is trained on data collected
with a trial sightseeing system. Next, we optimize
the dialogue strategy of the system via reinforcement
learning (RL) with a natural policy gradient approach.
</bodyText>
<sectionHeader confidence="0.891286" genericHeader="introduction">
2 Spoken decision making dialogue
</sectionHeader>
<bodyText confidence="0.999115">
We assume a situation where a user selects from a given
set of alternatives. This is highly likely in real world
situations; for example, the situation wherein a user se-
lects one restaurant from a list of candidates presented
</bodyText>
<figureCaption confidence="0.8817035">
Figure 1: Hierarchy structure for sightseeing guidance
dialogue
</figureCaption>
<bodyText confidence="0.9998308">
by a car navigation system. In this work, we deal with
a sightseeing planning task where the user determines
the sightseeing spot to visit, with little prior knowledge
about the target domain. The study of (Ohtake et al.,
2009), which investigated human-human dialogue in
such a task, reported that such consulting usually con-
sists of a sequence of information requests from the
user, presentation and elaboration of information about
certain spots by the guide followed by the user’s evalu-
ation. We thus focus on these interactions.
Several studies have featured decision support sys-
tems in the operations research field, and the typical
method that has been employed is the Analytic Hierar-
chy Process (Saaty, 1980) (AHP). In AHP, the problem
is modeled as a hierarchy that consists of the decision
goal, the alternatives for achieving it, and the criteria
for evaluating these alternatives. An example hierarchy
using these criteria is shown in Figure 1.
For the user, the problem of making an optimal de-
cision can be solved by fixing a weight vector Puser =
(p1, p2, ... , pM) for criteria and local weight matrix
Vuser = (v11,v12,...,v1M,...,vNM) for alterna-
tives in terms of the criteria. The optimal alternative
is then identified by selecting the spot k with the maxi-
mum priority of EM 1 p,,,,vk,,,,,. In typical AHP meth-
ods, the procedure Mixing these weights is often con-
ducted through pairwise comparisons for all the possi-
ble combinations of criteria and spots in terms of the
criteria, followed by weight tuning based on the re-
sults of these comparisons (Saaty, 1980). However, this
methodology cannot be directly applied to spoken dia-
logue systems. The information about the spot in terms
of the criteria is not known to the users, but is obtained
only via navigating through the system’s information.
In addition, spoken dialogue systems usually handle
several candidates and criteria, making pairwise com-
parison a costly affair.
We thus consider a spoken dialogue framework that
estimates the weights for the user’s preference (po-
tential preferences) as well as the user’s knowledge
</bodyText>
<figure confidence="0.998932210526316">
Alternatives
(choices)
Goal
Criteria
V11 ... V12 ... V13...
Kinkakuji-
Temple
1. Cherry
Blossoms
P1 P2 P3
Ryoanji-
Temple
2. Japanese
Garden
Choose the optimal spot
Nanzenji-
Temple
3. Easy
Access
</figure>
<affiliation confidence="0.650366">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 221–224,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999542">
221
</page>
<bodyText confidence="0.987824">
about the domain through interactions of information
retrieval and navigation.
</bodyText>
<sectionHeader confidence="0.694805" genericHeader="method">
3 Decision support system with spoken
dialogue interface
</sectionHeader>
<bodyText confidence="0.999993363636364">
The dialogue system we built has two functions: an-
swering users’ information requests and recommend-
ing information to them. When the system is requested
to explain about the spots or their determinants, it ex-
plains the sightseeing spots in terms of the requested
determinant. After satisfying the user’s request, the
system then provides information that would be helpful
in making a decision (e.g., instructing what the system
can explain, recommending detailed information of the
current topic that the user might be interested in, etc.).
Note that the latter is optimized via RL (see Section 4).
</bodyText>
<subsectionHeader confidence="0.999252">
3.1 Knowledge base
</subsectionHeader>
<bodyText confidence="0.99998725">
Our back-end DB consists of 15 sightseeing spots as al-
ternatives and 10 determinants described for each spot.
We select determinants that frequently appear in the di-
alogue corpus of (Ohtake et al., 2009) (e.g. cherry blos-
soms, fall foliage). The spots are annotated in terms of
these determinants if they apply to them. The value of
the evaluation enm is “1” when the spot n applies to the
determinant m and “0” when it does not.
</bodyText>
<subsectionHeader confidence="0.999348">
3.2 System initiative recommendation
</subsectionHeader>
<bodyText confidence="0.999966">
The content of the recommendation is determined
based on one of the following six methods:
</bodyText>
<listItem confidence="0.9929285">
1. Recommendation of determinants based on the
currently focused spot (Method 1)
</listItem>
<bodyText confidence="0.9953584">
This method is structured on the basis of the user’s
current focus on a particular spot. Specifically, the
system selects several determinants related to the
current spot whose evaluation is “1” and presents
them to the user.
</bodyText>
<listItem confidence="0.828642">
2. Recommendation of spots based on the cur-
rently focused determinant (Method 2)
</listItem>
<bodyText confidence="0.957237">
This method functions on the basis of the focus on
a certain specific determinant.
</bodyText>
<sectionHeader confidence="0.726572" genericHeader="method">
3. Open prompt (Method 3)
</sectionHeader>
<bodyText confidence="0.999819">
The system does not make a recommendation, and
presents an open prompt.
</bodyText>
<subsectionHeader confidence="0.463578">
4. Listing of determinants 1 (Method 4)
</subsectionHeader>
<bodyText confidence="0.9936904">
This method lists several determinants to the user in
ascending order from the low level user knowledge
Ksys (that the system estimates). (Ksys, Psys, pm
and Pr(pm = 1) are defined and explained in Sec-
tion 4.2.)
</bodyText>
<sectionHeader confidence="0.440928" genericHeader="method">
5. Listing of determinants 2 (Method 5)
</sectionHeader>
<bodyText confidence="0.990476333333333">
This method also lists the determinants, but the or-
der is based on the user’s high preference Psys (that
the system estimates).
</bodyText>
<listItem confidence="0.4104075">
6. Recommendation of user’s possibly preferred
spot (Method 6)
</listItem>
<bodyText confidence="0.9999565">
The system recommends a spot as well as the de-
terminants that the users would be interested in
based on the estimated preference Psys. The sys-
tem selects one spot k with a maximum value of
</bodyText>
<equation confidence="0.50813">
�M m�1 Pr(pm = 1) · ek,m. This idea is based
</equation>
<bodyText confidence="0.896231181818182">
on collaborative filtering which is often used for
recommender systems (Breese et al., 1998). This
method will be helpful to users if the system suc-
cessfully estimates the user’s preference; however,
it will be irrelevant if the system does not.
We will represent these recommendations
through a dialogue act expression, (casys{scsys}),
which consists of a communicative act casys
and the semantic content scsys. (For exam-
ple Method1{(Spot5), (Det3, Det4, Det5)},
Method3{NULL, NULL}, etc.)
</bodyText>
<sectionHeader confidence="0.669155" genericHeader="method">
4 Optimization of dialogue strategy
</sectionHeader>
<subsectionHeader confidence="0.965098">
4.1 Models for simulating a user
</subsectionHeader>
<bodyText confidence="0.999614838709677">
We introduce a user model that consists of a tuple of
knowledge vector Kuser, preference vector Puser, and
local weight matrix Vuser. In this paper, for simplic-
ity, a user’s preference vector or weight for determi-
nants Puser = (p1, p2, ... , pM) is assumed to con-
sist of binary parameters. That is, if the user is in-
terested in (or potentially interested in) the determi-
nant m and emphasizes it when making a decision,
the preference pm is set to “1”. Otherwise, it is set
to “0”. In order to represent a state that the user has
potential preference, we introduce a knowledge param-
eter Kuser = (k1, k2, . . . , kM) that shows if the user
has the perception that the system is able to handle or
he/she is interested in the determinants. km is set to
“1” if the user knows (or is listed by system’s recom-
mendations) that the system can handle determinant m
and “0” when he/she does not. For example, the state
that the determinant m is the potential preference of a
user (but he/she is unaware of that) is represented by
(km = 0, pm = 1). This idea is in contrast to previous
research which assumes some fixed goal observable by
the user from the beginning of the dialogue (Schatz-
mann et al., 2007). A user’s local weight vnm for spot
n in terms of determinant m is set to “1”, when the
system lets the user know that the evaluation of spots is
“1” through recommendation Methods 1, 2 and 6.
We constructed a user simulator that is based on
the statistics calculated through an experiment with the
trial system (Misu et al., 2010) as well as the knowl-
edge and preference of the user. That is, the user’s com-
municative act catuser and the semantic content sct
</bodyText>
<subsubsectionHeader confidence="0.484357">
user
</subsubsectionHeader>
<bodyText confidence="0.99983">
for the system’s recommendation at sys are generated
based on the following equation:
</bodyText>
<equation confidence="0.616223">
Pr(catuser, sctuser|catsys, sctsys, Kuser, Puser)
= Pr(catuser|catsys)
·Pr(sctuser|Kuser, Puser, cat user, catsys,sctsys)
</equation>
<bodyText confidence="0.999710272727273">
This means that the user’s communicative act causer
is sampled based on the conditional probability of
Pr(cat user|catsys) in (Misu et al., 2010). The seman-
tic content scuser is selected based on the user’s pref-
erence Puser under current knowledge about the de-
terminants Kuser. That is, the sc is sampled from the
determinants within the user’s knowledge (km = 1)
based on the probability that the user requests the de-
terminant of his/her preference/non-preference, which
is also calculated from the dialogue data of the trial sys-
tem.
</bodyText>
<subsectionHeader confidence="0.981006">
4.2 Dialogue state expression
</subsectionHeader>
<bodyText confidence="0.999971571428572">
We defined the state expression of the user in the pre-
vious section. However the problem is that for the
system, the state (Puser, Kuser, Vuser) is not observ-
able, but is only estimated from the interactions with
the user. Thus, this model is a partially observable
Markov decision process (POMDP) problem. In or-
der to estimate unobservable properties of a POMDP
</bodyText>
<page confidence="0.983959">
222
</page>
<figure confidence="0.4761054375">
� �
Priors of the estimated state:
- Knowledge: Ksrs = (0.22, 0.01, 0.02, 0.18,... )
-Preference: Psrs = (0.37, 0.19, 0.48, 0.38,... )
Interactions (observation):
- System recommendation:
asrs = Method1{(Spot5), (Det1, Det3, Det4)}
- User query:
auser = Accept{(Spot5), (Det3)}
Posterior of the estimated state:
- Knowledge: Ksrs = (1.00, 0.01, 1.00, 1.00,... )
-Preference: Psrs = (0.26, 0.19, 0.65, 0.22,... )
User’s knowledge acquisition:
- Knowledge: Kuser ← {k1 = 1, k3 = 1, k4 = 1}
- Local weight: Vuser ← {v51 = 1, v53 = 1, v54 =
�1}
</figure>
<figureCaption confidence="0.99982">
Figure 2: Example of state update
</figureCaption>
<bodyText confidence="0.994087666666667">
and handle the problem as an MDP, we introduce
the system’s inferential user knowledge vector Ksys
or probability distribution (estimate value) Ksys =
</bodyText>
<equation confidence="0.784852333333333">
(Pr(k1 = 1), Pr(k2 = 1), ... , Pr(kM = 1)) and
that of preference Psys = (Pr(p1 = 1), Pr(p2 =
1), ... , Pr(pM = 1)).
</equation>
<bodyText confidence="0.997821846153846">
The dialogue state DSt+1 or estimated user’s dia-
logue state of the step t + 1 is assumed to be dependent
only on the previous state DSt, as well as the interac-
tions It = (atsys, atuser).
The estimated user’s state is represented as a prob-
ability distribution and is updated by each interac-
tion. This corresponds to representing the user types
as a probability distribution, whereas the work of (Ko-
matani et al., 2005) classifies users to several discrete
user types. The estimated user’s preference Psys is up-
dated when the system observes the interaction It. The
update is conducted based on the following Bayes’ the-
orem using the previous state DSt as a prior.
</bodyText>
<equation confidence="0.999159">
Pr(p. = 1|P) =
Pr(It|pm=1)Pr(pm=1)
Pr(It|pm=1)Pr(pm=1)+Pr(It|(pm=0))Pr(1−Pr(pm=1))
</equation>
<bodyText confidence="0.995958">
Here, Pr(It|pm = 1),Pr(It|(pm = 0) to the right
side was obtained from the dialogue corpus of (Misu et
al., 2010). This posterior is then used as a prior in the
next state update using interaction It+1. An example
of this update is illustrated in Figure 2.
</bodyText>
<subsectionHeader confidence="0.998022">
4.3 Reward function
</subsectionHeader>
<bodyText confidence="0.999978071428571">
The reward function that we use is based on the num-
ber of agreed attributes between the user preference
and the decided spot. Users are assumed to determine
the spot based on their preference Puser under their
knowledge Kuser (and local weight for spots Vuser)
at that time, and select the spot k with the maximum
priority of Em kk · pk · vkm. The reward R is then
calculated based on the improvement in the number of
agreed attributes between the user’s actual (potential)
preferences and the decided spot k over the expected
agreement by random spot selection.
For example, if the decided spot satisfies three prefer-
ences and the average agreement of the agreement by
random selection is 1.3, then the reward is 1.7.
</bodyText>
<subsectionHeader confidence="0.988401">
4.4 Optimization by reinforcement learning
</subsectionHeader>
<bodyText confidence="0.936837826086957">
The problem of system recommendation generation is
optimized through RL. The MDP (S, A, R) is defined
as follows. The state parameter S = (s1,s2, ... , sI) is
generated by extracting the features of the current dia-
logue state DSt. We use the following 29 features 1.
1. Parameters that indicate the # of interactions from
the beginning of the dialogue. This is approximated by
five parameters using triangular functions. 2. User’s
previous communicative act (1 if at�1
user = xi, other-
wise 0). 3. System’s previous communicative act (1 if
at�1 = yj, otherwise 0). 4. Sum of the estimated user
sys
knowledge about determinants (ENn=1 Pr(kn = 1)).
5. Number of presented spot information. 6. Expecta-
tion of the probability that the user emphasizes the de-
terminant in the current state (Pr(kn = 1) × Pr(pn =
1)) (10 parameters). The action set A consists of the
six recommendation methods shown in subsection 3.2.
Reward R is given by the reward function of subsection
4.3.
A system action asys (casys) is sampled based on the
following soft-max (Boltzmann) policy.
</bodyText>
<equation confidence="0.999559">
π(asys = k|S) = Pr(asys = k|S, O)
exp( I i=1 si · Oki)
EJj=1 exp(EIi=1 si · Oji)
</equation>
<bodyText confidence="0.88779">
Here, O = (O11, O12,... O1I, ... , OJI) consists of J (#
actions) × I (# features) parameters. The parameter
Oji works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. This O is the target of optimization by RL.
We adopt the Natural Actor Critic (NAC) (Peters and
Schaal, 2008), which adopts a natural policy gradient
method as the policy optimization method.
</bodyText>
<subsectionHeader confidence="0.993777">
4.5 Experiment by dialogue simulation
</subsectionHeader>
<bodyText confidence="0.999968958333333">
For each simulated dialogue session, a simulated user
(Puser, Kuser, Vuser) is sampled. A preference vector
Puser of the user is generated so that he/she has four
preferences. As a result, four parameters in Puser are
“1” and the others are “0”. This vector is fixed through-
out the dialogue episode. This sampling is conducted
based on the rate proportional to the percentage of users
who emphasize it for making decisions (Misu et al.,
2010). The user’s knowledge Kuser is also set based
on the statistics of the “percentage of users who stated
the determinants before system recommendation”. For
each determinant, we sample a random valuable r that
ranges from “0” to “1”, and km is set to “1” if r is
smaller than the percentage. All the parameters of
local weights Vuser are initialized to “0”, assuming
that users have no prior knowledge about the candi-
date spots. As for system parameters, the estimated
user’s preference Psys and knowledge Ksys are ini-
tialized based on the statistics of our trial system (Misu
et al., 2010).
We assumed that the system does not misunderstand
the user’s action. Users are assumed to continue a di-
alogue session for 20 turns2, and episodes are sampled
using the policy π at that time and the user simulator
</bodyText>
<footnote confidence="0.9970488">
1Note that about half of them are continuous variables and
that the value function cannot be denoted by a lookup table.
2In practice, users may make a decision at any point once
they are satisfied collecting information. And this is the rea-
son why we list the rewards in the early dialogue stage in
</footnote>
<equation confidence="0.9118558">
M
E
m=1
1 N
E
n=1
M
E
m=1
R=
pm · ek,m −
N
pm · en,m
�
=
</equation>
<page confidence="0.999398">
223
</page>
<tableCaption confidence="0.999953">
Table 1: Comparison of reward with baseline methods
</tableCaption>
<table confidence="0.9937444">
Reward (±std)
Policy T = 5 T = 10 T = 15 T = 20
NAC 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
B1 0.02 (0.42) 0.13 (0.54) 0.29 (0.59) 0.34 (0.59)
B2 0.46 (0.67) 0.68 (0.65) 0.80 (0.61) 0.92 (0.56)
</table>
<tableCaption confidence="0.966359">
Table 2: Comparison of reward with discrete dialogue
state expression
</tableCaption>
<bodyText confidence="0.877148833333333">
Reward (±std)
State T = 5 T = 10 T = 15 T = 20
of subsection 4.1. In each turn, the system is rewarded
using the reward function of subsection 4.3. The pol-
icy (parameter Θ) is updated using NAC in every 2,000
dialogues.
</bodyText>
<subsectionHeader confidence="0.907248">
4.6 Experimental result
</subsectionHeader>
<bodyText confidence="0.9966964">
The policy was fixed at about 30,000 dialogue
episodes. We analyzed the learned dialogue policy by
examining the value of weight parameter Θ. We com-
pared the parameters of the trained policy between ac-
tions3. The weight of the parameters that represent the
early stage of the dialogue was large in Methods 4 and
5. On the other hand, the weight of the parameters that
represent the latter stage of the dialogue was large in
Methods 2 and 6. This suggests that in the trained pol-
icy, the system first bridges the knowledge gap between
the user, estimates the user’s preference, and then, rec-
ommends specific information that would be useful to
the user.
Next, we compared the trained policy with the fol-
lowing baseline methods.
</bodyText>
<listItem confidence="0.647534">
1. No recommendation (B1)
</listItem>
<bodyText confidence="0.9977795">
The system only provides the requested informa-
tion and does not generate any recommendations.
</bodyText>
<sectionHeader confidence="0.590205" genericHeader="method">
2. Random recommendation (B2)
</sectionHeader>
<bodyText confidence="0.983593395348838">
The system randomly chooses a recommendation
from six methods.
The comparison of the average reward between the
baseline methods is listed in Table 1. Note that the ora-
cle average reward that can be obtained only when the
user knows all knowledge about the knowledge base
(it requires at least 50 turns) was 1.45. The reward by
the strategy optimized by NAC was significantly better
than that of baseline methods (n = 500, p &lt; .01).
We then compared the proposed method with the
case where estimated user’s knowledge and preference
are represented as discrete binary parameters instead of
probability distributions (PDs). That is, the estimated
user’s preference p,,,, of determinant m is set to “1”
when the user requested the determinant, otherwise it
is “0”. The estimated user’s knowledge k,,,, is set to
the following subsections. In our trial system, the dialogue
length was 16.3 turns with a standard deviation of 7.0 turns.
3The parameters can be interpreted as the size of the con-
tribution for selecting the action.
“1” when the system lets the user know the determi-
nant, otherwise it is “0”. Another dialogue strategy was
trained using this dialogue state expression. This result
is shown in Table 2. The proposed method that rep-
resents the dialogue state as a probability distribution
outperformed (p &lt; .01 (T=15,20)) the method using a
discrete state expression.
We also compared the proposed method with the
case where either one of estimated preference or
knowledge was used as a feature for dialogue state in
order to carefully investigate the effect of these factors.
In the proposed method, expectation of the probabil-
ity that the user emphasizes the determinant (Pr(kn =
1) x Pr(pn = 1)) was used as a feature of dialogue
state. We evaluated the performance of the cases where
the estimated knowledge Pr(kn = 1) or estimated
preference Pr(pn = 1) was used instead of the expec-
tation of the probability that the user emphasizes the
determinant. We also compared with the case where
no preference/knowledge feature was used. This result
is shown in Table 3. We confirmed that significant im-
provement (p &lt; .01 (T=15,20)) was obtained by taking
into account the estimated knowledge of the user.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978266666667">
In this paper, we presented a spoken dialogue frame-
work that helps users select an alternative from a list of
alternatives. We proposed a model of dialogue state for
spoken decision making dialogue that considers knowl-
edge as well as preference of the user and the system,
and its dialogue strategy was trained by RL. We con-
firmed that the learned policy achieved a better recom-
mendation strategy over several baseline methods.
Although we dealt with a simple recommendation
strategy with a fixed number of recommendation com-
ponents, there are many possible extensions to this
model. The system is expected to handle a more com-
plex planning of natural language generation. We also
need to consider errors in speech recognition and un-
derstanding when simulating dialogue.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999360307692308">
J. Breese, D. Heckerman, and C. Kadie. 1998. ”empirical
analysis of predictive algorithms for collaborative filter-
ing”. In ”Proc. the 14th Annual Conference on Uncer-
tainty in Artificial Intelligence”, pages 43–52.
K. Komatani, S. Ueno, T. Kawahara, and H. Okuno. 2005.
User Modeling in Spoken Dialogue Systems to Generate
Flexible Guidance. User Modeling and User-Adapted In-
teraction, 15(1):169–183.
T. Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, and
S. Nakamura. 2010. Construction and Experiment of a
Spoken Consulting Dialogue System. In Proc. IWSDS.
K. Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura.
2009. Annotating Dialogue Acts to Construct Dialogue
Systems for Consulting. In Proc. The 7th Workshop on
Asian Language Resources, pages 32–39.
J. Peters and S. Schaal. 2008. Natural Actor-Critic. Neuro-
computing, 71(7-9):1180–1190.
J. Polifroni and M. Walker. 2008. Intensional Summaries
as Cooperative Responses in Dialogue: Automation and
Evaluation. In Proc. ACL/HLT, pages 479–487.
T. Saaty. 1980. The Analytic Hierarchy Process: Planning,
Priority Setting, Resource Allocation. Mcgraw-Hill.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based User Simulation for
Bootstrapping a POMDP Dialogue System. In Proc.
HLT/NAACL.
</reference>
<table confidence="0.437905">
PDs 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Discrete 0.89 (0.60) 0.97 (0.56) 1.03 (0.54) 1.10 (0.52)
</table>
<tableCaption confidence="0.6246495">
Table 3: Effect of estimated preference and knowledge
Reward (±std)
</tableCaption>
<bodyText confidence="0.795576166666667">
Policy T = 5 T = 10 T = 15 T = 20
Pref+Know0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Pref only 0.94 (0.57) 0.96 (0.55) 1.02 (0.55) 1.09 (0.53)
Know only 0.96 (0.59) 1.00 (0.56) 1.08 (0.53) 1.15 (0.51)
No Pref or 0.93 (0.57) 0.96 (0.55) 1.02 (0.53) 1.08 (0.52)
Know
</bodyText>
<page confidence="0.996353">
224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.509941">
<title confidence="0.9716985">Modeling Spoken Decision Making and Optimization of its Dialogue Strategy</title>
<author confidence="0.877567">Teruhisa Misu</author>
<author confidence="0.877567">Komei Sugiura</author>
<author confidence="0.877567">Kiyonori Chiori Hori</author>
<author confidence="0.877567">Hideki Kashioka</author>
<author confidence="0.877567">Hisashi Kawai</author>
<author confidence="0.877567">Satoshi</author>
<affiliation confidence="0.928259">MASTAR Project,</affiliation>
<address confidence="0.737852">Kyoto,</address>
<email confidence="0.972082">teruhisa.misu@nict.go.jp</email>
<abstract confidence="0.997402166666667">This paper presents a spoken dialogue framework that helps users in making decisions. Users often do not have a definite goal or criteria for selecting from a list of alternatives. Thus the system has to bridge this knowledge gap and also provide the users with an appropriate alternative together with the reason for this recommendation through dialogue. We present a dialogue state model for such decision making dialogue. To evaluate this model, we implement a trial sightseeing guidance system and collect dialogue data. Then, we optimize the dialogue strategy based on the state model through reinforcement learning with a natural policy gradient approach using a user simulator trained on the collected dialogue corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Breese</author>
<author>D Heckerman</author>
<author>C Kadie</author>
</authors>
<title>empirical analysis of predictive algorithms for collaborative filtering”.</title>
<date>1998</date>
<booktitle>In ”Proc. the 14th Annual Conference on Uncertainty in Artificial Intelligence”,</booktitle>
<pages>43--52</pages>
<contexts>
<context position="7912" citStr="Breese et al., 1998" startWordPosition="1289" endWordPosition="1292">, pm and Pr(pm = 1) are defined and explained in Section 4.2.) 5. Listing of determinants 2 (Method 5) This method also lists the determinants, but the order is based on the user’s high preference Psys (that the system estimates). 6. Recommendation of user’s possibly preferred spot (Method 6) The system recommends a spot as well as the determinants that the users would be interested in based on the estimated preference Psys. The system selects one spot k with a maximum value of �M m�1 Pr(pm = 1) · ek,m. This idea is based on collaborative filtering which is often used for recommender systems (Breese et al., 1998). This method will be helpful to users if the system successfully estimates the user’s preference; however, it will be irrelevant if the system does not. We will represent these recommendations through a dialogue act expression, (casys{scsys}), which consists of a communicative act casys and the semantic content scsys. (For example Method1{(Spot5), (Det3, Det4, Det5)}, Method3{NULL, NULL}, etc.) 4 Optimization of dialogue strategy 4.1 Models for simulating a user We introduce a user model that consists of a tuple of knowledge vector Kuser, preference vector Puser, and local weight matrix Vuser</context>
</contexts>
<marker>Breese, Heckerman, Kadie, 1998</marker>
<rawString>J. Breese, D. Heckerman, and C. Kadie. 1998. ”empirical analysis of predictive algorithms for collaborative filtering”. In ”Proc. the 14th Annual Conference on Uncertainty in Artificial Intelligence”, pages 43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Komatani</author>
<author>S Ueno</author>
<author>T Kawahara</author>
<author>H Okuno</author>
</authors>
<date>2005</date>
<booktitle>User Modeling in Spoken Dialogue Systems to Generate Flexible Guidance. User Modeling and User-Adapted Interaction,</booktitle>
<pages>15--1</pages>
<contexts>
<context position="12442" citStr="Komatani et al., 2005" startWordPosition="2075" endWordPosition="2079">erential user knowledge vector Ksys or probability distribution (estimate value) Ksys = (Pr(k1 = 1), Pr(k2 = 1), ... , Pr(kM = 1)) and that of preference Psys = (Pr(p1 = 1), Pr(p2 = 1), ... , Pr(pM = 1)). The dialogue state DSt+1 or estimated user’s dialogue state of the step t + 1 is assumed to be dependent only on the previous state DSt, as well as the interactions It = (atsys, atuser). The estimated user’s state is represented as a probability distribution and is updated by each interaction. This corresponds to representing the user types as a probability distribution, whereas the work of (Komatani et al., 2005) classifies users to several discrete user types. The estimated user’s preference Psys is updated when the system observes the interaction It. The update is conducted based on the following Bayes’ theorem using the previous state DSt as a prior. Pr(p. = 1|P) = Pr(It|pm=1)Pr(pm=1) Pr(It|pm=1)Pr(pm=1)+Pr(It|(pm=0))Pr(1−Pr(pm=1)) Here, Pr(It|pm = 1),Pr(It|(pm = 0) to the right side was obtained from the dialogue corpus of (Misu et al., 2010). This posterior is then used as a prior in the next state update using interaction It+1. An example of this update is illustrated in Figure 2. 4.3 Reward fun</context>
</contexts>
<marker>Komatani, Ueno, Kawahara, Okuno, 2005</marker>
<rawString>K. Komatani, S. Ueno, T. Kawahara, and H. Okuno. 2005. User Modeling in Spoken Dialogue Systems to Generate Flexible Guidance. User Modeling and User-Adapted Interaction, 15(1):169–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Misu</author>
<author>K Ohtake</author>
<author>C Hori</author>
<author>H Kashioka</author>
<author>H Kawai</author>
<author>S Nakamura</author>
</authors>
<title>Construction and Experiment of a Spoken Consulting Dialogue System. In</title>
<date>2010</date>
<booktitle>Proc. IWSDS.</booktitle>
<contexts>
<context position="9908" citStr="Misu et al., 2010" startWordPosition="1645" endWordPosition="1648">at the determinant m is the potential preference of a user (but he/she is unaware of that) is represented by (km = 0, pm = 1). This idea is in contrast to previous research which assumes some fixed goal observable by the user from the beginning of the dialogue (Schatzmann et al., 2007). A user’s local weight vnm for spot n in terms of determinant m is set to “1”, when the system lets the user know that the evaluation of spots is “1” through recommendation Methods 1, 2 and 6. We constructed a user simulator that is based on the statistics calculated through an experiment with the trial system (Misu et al., 2010) as well as the knowledge and preference of the user. That is, the user’s communicative act catuser and the semantic content sct user for the system’s recommendation at sys are generated based on the following equation: Pr(catuser, sctuser|catsys, sctsys, Kuser, Puser) = Pr(catuser|catsys) ·Pr(sctuser|Kuser, Puser, cat user, catsys,sctsys) This means that the user’s communicative act causer is sampled based on the conditional probability of Pr(cat user|catsys) in (Misu et al., 2010). The semantic content scuser is selected based on the user’s preference Puser under current knowledge about the </context>
<context position="12884" citStr="Misu et al., 2010" startWordPosition="2144" endWordPosition="2147">lity distribution and is updated by each interaction. This corresponds to representing the user types as a probability distribution, whereas the work of (Komatani et al., 2005) classifies users to several discrete user types. The estimated user’s preference Psys is updated when the system observes the interaction It. The update is conducted based on the following Bayes’ theorem using the previous state DSt as a prior. Pr(p. = 1|P) = Pr(It|pm=1)Pr(pm=1) Pr(It|pm=1)Pr(pm=1)+Pr(It|(pm=0))Pr(1−Pr(pm=1)) Here, Pr(It|pm = 1),Pr(It|(pm = 0) to the right side was obtained from the dialogue corpus of (Misu et al., 2010). This posterior is then used as a prior in the next state update using interaction It+1. An example of this update is illustrated in Figure 2. 4.3 Reward function The reward function that we use is based on the number of agreed attributes between the user preference and the decided spot. Users are assumed to determine the spot based on their preference Puser under their knowledge Kuser (and local weight for spots Vuser) at that time, and select the spot k with the maximum priority of Em kk · pk · vkm. The reward R is then calculated based on the improvement in the number of agreed attributes </context>
<context position="15855" citStr="Misu et al., 2010" startWordPosition="2662" endWordPosition="2665">he Natural Actor Critic (NAC) (Peters and Schaal, 2008), which adopts a natural policy gradient method as the policy optimization method. 4.5 Experiment by dialogue simulation For each simulated dialogue session, a simulated user (Puser, Kuser, Vuser) is sampled. A preference vector Puser of the user is generated so that he/she has four preferences. As a result, four parameters in Puser are “1” and the others are “0”. This vector is fixed throughout the dialogue episode. This sampling is conducted based on the rate proportional to the percentage of users who emphasize it for making decisions (Misu et al., 2010). The user’s knowledge Kuser is also set based on the statistics of the “percentage of users who stated the determinants before system recommendation”. For each determinant, we sample a random valuable r that ranges from “0” to “1”, and km is set to “1” if r is smaller than the percentage. All the parameters of local weights Vuser are initialized to “0”, assuming that users have no prior knowledge about the candidate spots. As for system parameters, the estimated user’s preference Psys and knowledge Ksys are initialized based on the statistics of our trial system (Misu et al., 2010). We assume</context>
</contexts>
<marker>Misu, Ohtake, Hori, Kashioka, Kawai, Nakamura, 2010</marker>
<rawString>T. Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, and S. Nakamura. 2010. Construction and Experiment of a Spoken Consulting Dialogue System. In Proc. IWSDS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ohtake</author>
<author>T Misu</author>
<author>C Hori</author>
<author>H Kashioka</author>
<author>S Nakamura</author>
</authors>
<title>Annotating Dialogue Acts to Construct Dialogue Systems for Consulting.</title>
<date>2009</date>
<booktitle>In Proc. The 7th Workshop on Asian Language Resources,</booktitle>
<pages>32--39</pages>
<contexts>
<context position="2934" citStr="Ohtake et al., 2009" startWordPosition="474" endWordPosition="477">reinforcement learning (RL) with a natural policy gradient approach. 2 Spoken decision making dialogue We assume a situation where a user selects from a given set of alternatives. This is highly likely in real world situations; for example, the situation wherein a user selects one restaurant from a list of candidates presented Figure 1: Hierarchy structure for sightseeing guidance dialogue by a car navigation system. In this work, we deal with a sightseeing planning task where the user determines the sightseeing spot to visit, with little prior knowledge about the target domain. The study of (Ohtake et al., 2009), which investigated human-human dialogue in such a task, reported that such consulting usually consists of a sequence of information requests from the user, presentation and elaboration of information about certain spots by the guide followed by the user’s evaluation. We thus focus on these interactions. Several studies have featured decision support systems in the operations research field, and the typical method that has been employed is the Analytic Hierarchy Process (Saaty, 1980) (AHP). In AHP, the problem is modeled as a hierarchy that consists of the decision goal, the alternatives for </context>
<context position="6179" citStr="Ohtake et al., 2009" startWordPosition="991" endWordPosition="994">, it explains the sightseeing spots in terms of the requested determinant. After satisfying the user’s request, the system then provides information that would be helpful in making a decision (e.g., instructing what the system can explain, recommending detailed information of the current topic that the user might be interested in, etc.). Note that the latter is optimized via RL (see Section 4). 3.1 Knowledge base Our back-end DB consists of 15 sightseeing spots as alternatives and 10 determinants described for each spot. We select determinants that frequently appear in the dialogue corpus of (Ohtake et al., 2009) (e.g. cherry blossoms, fall foliage). The spots are annotated in terms of these determinants if they apply to them. The value of the evaluation enm is “1” when the spot n applies to the determinant m and “0” when it does not. 3.2 System initiative recommendation The content of the recommendation is determined based on one of the following six methods: 1. Recommendation of determinants based on the currently focused spot (Method 1) This method is structured on the basis of the user’s current focus on a particular spot. Specifically, the system selects several determinants related to the curren</context>
</contexts>
<marker>Ohtake, Misu, Hori, Kashioka, Nakamura, 2009</marker>
<rawString>K. Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura. 2009. Annotating Dialogue Acts to Construct Dialogue Systems for Consulting. In Proc. The 7th Workshop on Asian Language Resources, pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peters</author>
<author>S Schaal</author>
</authors>
<date>2008</date>
<journal>Natural Actor-Critic. Neurocomputing,</journal>
<pages>71--7</pages>
<contexts>
<context position="15292" citStr="Peters and Schaal, 2008" startWordPosition="2570" endWordPosition="2573"> six recommendation methods shown in subsection 3.2. Reward R is given by the reward function of subsection 4.3. A system action asys (casys) is sampled based on the following soft-max (Boltzmann) policy. π(asys = k|S) = Pr(asys = k|S, O) exp( I i=1 si · Oki) EJj=1 exp(EIi=1 si · Oji) Here, O = (O11, O12,... O1I, ... , OJI) consists of J (# actions) × I (# features) parameters. The parameter Oji works as a weight for the i-th feature of the action j and determines the likelihood that the action j is selected. This O is the target of optimization by RL. We adopt the Natural Actor Critic (NAC) (Peters and Schaal, 2008), which adopts a natural policy gradient method as the policy optimization method. 4.5 Experiment by dialogue simulation For each simulated dialogue session, a simulated user (Puser, Kuser, Vuser) is sampled. A preference vector Puser of the user is generated so that he/she has four preferences. As a result, four parameters in Puser are “1” and the others are “0”. This vector is fixed throughout the dialogue episode. This sampling is conducted based on the rate proportional to the percentage of users who emphasize it for making decisions (Misu et al., 2010). The user’s knowledge Kuser is also </context>
</contexts>
<marker>Peters, Schaal, 2008</marker>
<rawString>J. Peters and S. Schaal. 2008. Natural Actor-Critic. Neurocomputing, 71(7-9):1180–1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Polifroni</author>
<author>M Walker</author>
</authors>
<title>Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation.</title>
<date>2008</date>
<booktitle>In Proc. ACL/HLT,</booktitle>
<pages>479--487</pages>
<contexts>
<context position="1183" citStr="Polifroni and Walker, 2008" startWordPosition="180" endWordPosition="183">ether with the reason for this recommendation through dialogue. We present a dialogue state model for such decision making dialogue. To evaluate this model, we implement a trial sightseeing guidance system and collect dialogue data. Then, we optimize the dialogue strategy based on the state model through reinforcement learning with a natural policy gradient approach using a user simulator trained on the collected dialogue corpus. 1 Introduction In many situations where spoken dialogue interfaces are used, information access by the user is not a goal in itself, but a means for decision making (Polifroni and Walker, 2008). For example, in a restaurant retrieval system, the user’s goal may not be the extraction of price information but to make a decision on candidate restaurants based on the retrieved information. This work focuses on how to assist a user who is using the system for his/her decision making, when he/she does not have enough knowledge about the target domain. In such a situation, users are often unaware of not only what kind of information the system can provide but also their own preference or factors that they should emphasize. The system, too, has little knowledge about the user, or where his/</context>
</contexts>
<marker>Polifroni, Walker, 2008</marker>
<rawString>J. Polifroni and M. Walker. 2008. Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation. In Proc. ACL/HLT, pages 479–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Saaty</author>
</authors>
<title>The Analytic Hierarchy Process: Planning, Priority Setting, Resource Allocation.</title>
<date>1980</date>
<publisher>Mcgraw-Hill.</publisher>
<contexts>
<context position="3423" citStr="Saaty, 1980" startWordPosition="552" endWordPosition="553">ines the sightseeing spot to visit, with little prior knowledge about the target domain. The study of (Ohtake et al., 2009), which investigated human-human dialogue in such a task, reported that such consulting usually consists of a sequence of information requests from the user, presentation and elaboration of information about certain spots by the guide followed by the user’s evaluation. We thus focus on these interactions. Several studies have featured decision support systems in the operations research field, and the typical method that has been employed is the Analytic Hierarchy Process (Saaty, 1980) (AHP). In AHP, the problem is modeled as a hierarchy that consists of the decision goal, the alternatives for achieving it, and the criteria for evaluating these alternatives. An example hierarchy using these criteria is shown in Figure 1. For the user, the problem of making an optimal decision can be solved by fixing a weight vector Puser = (p1, p2, ... , pM) for criteria and local weight matrix Vuser = (v11,v12,...,v1M,...,vNM) for alternatives in terms of the criteria. The optimal alternative is then identified by selecting the spot k with the maximum priority of EM 1 p,,,,vk,,,,,. In typi</context>
</contexts>
<marker>Saaty, 1980</marker>
<rawString>T. Saaty. 1980. The Analytic Hierarchy Process: Planning, Priority Setting, Resource Allocation. Mcgraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S Young</author>
</authors>
<title>Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In</title>
<date>2007</date>
<booktitle>Proc. HLT/NAACL.</booktitle>
<contexts>
<context position="9576" citStr="Schatzmann et al., 2007" startWordPosition="1583" endWordPosition="1587">dge parameter Kuser = (k1, k2, . . . , kM) that shows if the user has the perception that the system is able to handle or he/she is interested in the determinants. km is set to “1” if the user knows (or is listed by system’s recommendations) that the system can handle determinant m and “0” when he/she does not. For example, the state that the determinant m is the potential preference of a user (but he/she is unaware of that) is represented by (km = 0, pm = 1). This idea is in contrast to previous research which assumes some fixed goal observable by the user from the beginning of the dialogue (Schatzmann et al., 2007). A user’s local weight vnm for spot n in terms of determinant m is set to “1”, when the system lets the user know that the evaluation of spots is “1” through recommendation Methods 1, 2 and 6. We constructed a user simulator that is based on the statistics calculated through an experiment with the trial system (Misu et al., 2010) as well as the knowledge and preference of the user. That is, the user’s communicative act catuser and the semantic content sct user for the system’s recommendation at sys are generated based on the following equation: Pr(catuser, sctuser|catsys, sctsys, Kuser, Puser</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. Young. 2007. Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In Proc. HLT/NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>