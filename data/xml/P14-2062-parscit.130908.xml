<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000783">
<title confidence="0.990147">
Experiments with crowdsourced re-annotation of a POS tagging data set
</title>
<author confidence="0.999884">
Dirk Hovy, Barbara Plank, and Anders Søgaard
</author>
<affiliation confidence="0.9972645">
Center for Language Technology
University of Copenhagen
</affiliation>
<address confidence="0.714327">
Njalsgade 140, 2300 Copenhagen
</address>
<email confidence="0.998095">
{dirkJbplank}@cst.dk, soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846133333333">
Crowdsourcing lets us collect multiple an-
notations for an item from several annota-
tors. Typically, these are annotations for
non-sequential classification tasks. While
there has been some work on crowdsourc-
ing named entity annotations, researchers
have largely assumed that syntactic tasks
such as part-of-speech (POS) tagging can-
not be crowdsourced. This paper shows
that workers can actually annotate sequen-
tial data almost as well as experts. Fur-
ther, we show that the models learned from
crowdsourced annotations fare as well as
the models learned from expert annota-
tions in downstream tasks.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958966101695">
Training good predictive NLP models typically re-
quires annotated data, but getting professional an-
notators to build useful data sets is often time-
consuming and expensive. Snow et al. (2008)
showed, however, that crowdsourced annotations
can produce similar results to annotations made
by experts. Crowdsourcing services such as Ama-
zon’s Mechanical Turk has since been successfully
used for various annotation tasks in NLP (Jha et
al., 2010; Callison-Burch and Dredze, 2010).
However, most applications of crowdsourcing
in NLP have been concerned with classification
problems, such as document classification and
constructing lexica (Callison-Burch and Dredze,
2010). A large part of NLP problems, however, are
structured prediction tasks. Typically, sequence
labeling tasks employ a larger set of labels than
classification problems, as well as complex inter-
actions between the annotations. Disagreement
among annotators is therefore potentially higher,
and the task of annotating structured data thus
harder.
Only a few recent studies have investi-
gated crowdsourcing sequential tasks; specifically,
named entity recognition (Finin et al., 2010; Ro-
drigues et al., 2013). Results for this are good.
However, named entities typically use only few la-
bels (LOC, ORG, and PER), and the data contains
mostly non-entities, so the complexity is manage-
able. The question of whether a more linguisti-
cally involved structured task like part-of-speech
(POS) tagging can be crowdsourced has remained
largely unaddressed.1
In this paper, we investigate how well lay anno-
tators can produce POS labels for Twitter data. In
our setup, we present annotators with one word at
a time, with a minimal surrounding context (two
words to each side). Our choice of annotating
Twitter data is not coincidental: with the short-
lived nature of Twitter messages, models quickly
lose predictive power (Eisenstein, 2013), and re-
training models on new samples of more represen-
tative data becomes necessary. Expensive profes-
sional annotation may be prohibitive for keeping
NLP models up-to-date with linguistic and topical
changes on Twitter. We use a minimum of instruc-
tions and require few qualifications.
Obviously, lay annotation is generally less re-
liable than professional annotation. It is there-
fore common to aggregate over multiple annota-
tions for the same item to get more robust anno-
tations. In this paper we compare two aggrega-
tion schemes, namely majority voting (MV) and
MACE (Hovy et al., 2013). We also show how we
can use Wiktionary, a crowdsourced lexicon, to fil-
ter crowdsourced annotations. We evaluate the an-
notations in several ways: (a) by testing their ac-
curacy with respect to a gold standard, (b) by eval-
uating the performance of POS models trained on
</bodyText>
<footnote confidence="0.99665125">
1One of the reviewers alerted us to an unpublished mas-
ters thesis, which uses pre-annotation to reduce tagging to
fewer multiple-choice questions. See Related Work section
for details.
</footnote>
<page confidence="0.978952">
377
</page>
<bodyText confidence="0.970743">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
the annotations across several existing data sets,
as well as (c) by applying our models in down-
stream tasks. We show that with minimal context
and annotation effort, we can produce structured
annotations of near-expert quality. We also show
that these annotations lead to better POS tagging
models than previous models learned from crowd-
sourced lexicons (Li et al., 2012). Finally, we
show that models learned from these annotations
are competitive with models learned from expert
annotations on various downstream tasks.
</bodyText>
<sectionHeader confidence="0.930516" genericHeader="introduction">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.998703409090909">
We crowdsource the training section of the data
from Gimpel et al. (2011)2 with POS tags. We use
Crowdflower,3 to collect five annotations for each
word, and then find the most likely label for each
word among the possible annotations. See Figure
1 for an example. If the correct label is not among
the annotations, we are unable to recover the cor-
rect answer. This was the case for 1497 instances
in our data (cf. the token “:” in the example).
We thus report on oracle score, i.e., the best label
sequence that could possibly be found, which is
correct except for the missing tokens. Note that
while we report agreement between the crowd-
sourced annotations and the crowdsourced anno-
tations, our main evaluations are based on models
learned from expert vs. crowdsourced annotations
and downstream applications thereof (chunking
and NER). We take care in evaluating our models
across different data sets to avoid biasing our
evaluations to particular annotations. All the data
sets used in our experiments are publicly available
athttp://lowlands.ku.dk/results/.
</bodyText>
<equation confidence="0.995035">
x Z y
@USER NOUN,NOUN,X,NOUN,-,NOUN NOUN
: .,.,-,.,.,. X
I PRON,NOUN,PRON,NOUN,PRON,- PRON
owe VERB,VERB,-,VERB,VERB,VERB VERB
U PRON,X,-,NOUN,NOUN,PRON PRON
0 = 0.9, 0.4, 0.2, 0.8, 0.8, 0.9
</equation>
<figureCaption confidence="0.733272">
Figure 1: Five annotations per token, supplied by 6
</figureCaption>
<bodyText confidence="0.698146">
different annotators (- = missing annotation), gold
label y. 0 = competence values for each annotator.
</bodyText>
<footnote confidence="0.9998385">
2http://www.ark.cs.cmu.edu/TweetNLP/
3http://crowdflower.com
</footnote>
<sectionHeader confidence="0.824606" genericHeader="method">
3 Crowdsourcing Sequential Annotation
</sectionHeader>
<bodyText confidence="0.972098071428572">
In order to use the annotations to train models that
can be applied across various data sets, i.e., mak-
ing out-of-sample evaluation possible (see Section
5), we follow Hovy et al. (2014) in using the uni-
versal tag set (Petrov et al., 2012) with 12 labels.
Figure 2: Screen shot of the annotation interface
on Crowdflower
Annotators were given a bold-faced word with
two words on either side and asked to select the
most appropriate tag from a drop down menu. For
each tag, we spell out the name of the syntactic
category, and provide a few example words.
See Figure 2 for a screenshot of the interface.
Annotators were also told that words can belong
to several classes, depending on the context. No
additional guidelines were given.
Only trusted annotators (in Crowdflower:
Bronze skills) that had answered correctly on 4
gold tokens (randomly chosen from a set of 20
gold tokens provided by the authors) were allowed
to submit annotations. In total, 177 individual
annotators supplied answers. We paid annotators
a reward of $0.05 for 10 tokens. The full data set
contains 14,619 tokens. Completion of the task
took slightly less than 10 days. Contributors were
very satisfied with the task (4.5 on a scale from 1
to 5). In particular, they felt instructions were clear
(4.4/5), and that the pay was reasonable (4.1/5).
</bodyText>
<sectionHeader confidence="0.985711" genericHeader="method">
4 Label Aggregation
</sectionHeader>
<bodyText confidence="0.9999512">
After collecting the annotations, we need to aggre-
gate the annotations to derive a single answer for
each token. In the simplest scheme, we choose the
majority label, i.e., the label picked by most an-
notators. In case of ties, we select the final label
at random. Since this is a stochastic process, we
average results over 100 runs. We refer to this as
MAJORITY VOTING (MV). Note that in MV we
trust all annotators to the same degree. However,
crowdsourcing attracts people with different mo-
</bodyText>
<page confidence="0.993691">
378
</page>
<bodyText confidence="0.9999875">
tives, and not all of them are equally reliable—
even the ones with Bronze level. Ideally, we would
like to factor this into our decision process.
We use MACE4 (Hovy et al., 2013) as our sec-
ond scheme to learn both the most likely answer
and a competence estimate for each of the annota-
tors. MACE treats annotator competence and the
correct answer as hidden variables and estimates
their parameters via EM (Dempster et al., 1977).
We use MACE with default parameter settings to
give us the weighted average for each annotated
example.
Finally, we also tried applying the joint learn-
ing scheme in Rodrigues et al. (2013), but their
scheme requires that entire sequences are anno-
tated by the same annotators, which we don’t have,
and it expects BIO sequences, rather than POS
tags.
Dictionaries Decoding tasks profit from the use
of dictionaries (Merialdo, 1994; Johnson, 2007;
Ravi and Knight, 2009) by restricting the number
of tags that need to be considered for each word,
also known as type constraints (T¨ackstr¨om et al.,
2013). We follow Li et al. (2012) in including
Wiktionary information as type constraints into
our decoding: if a word is found in Wiktionary,
we disregard all annotations that are not licensed
by the dictionary entry. If the word is not found in
Wiktionary, or if none of its annotations is licensed
by Wiktionary, we keep the original annotations.
Since we aggregate annotations independently
(unlike Viterbi decoding), we basically use Wik-
tionary as a pre-filtering step, such that MV and
MACE only operate on the reduced annotations.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.996719">
Each of the two aggregation schemes above pro-
duces a final label sequence yˆ for our training cor-
pus. We evaluate the resulting annotated data in
three ways.
</bodyText>
<listItem confidence="0.972117666666667">
1. We compare yˆ to the available expert annota-
tion on the training data. This tells us how similar
lay annotation is to professional annotation.
2. Ultimately, we want to use structured anno-
tations for supervised training, where annotation
quality influences model performance on held-out
test data. To test this, we train a CRF model
(Lafferty et al., 2001) with simple orthographic
features and word clusters (Owoputi et al., 2013)
</listItem>
<footnote confidence="0.913367">
4http://www.isi.edu/publications/
licensed-sw/mace/
</footnote>
<bodyText confidence="0.996675744186047">
on the annotated Twitter data described in Gim-
pel et al. (2011). Leaving out the dedicated test
set to avoid in-sample bias, we evaluate our mod-
els across three data sets: RITTER (the 10% test
split of the data in Ritter et al. (2011) used in Der-
czynski et al. (2013)), the test set from Foster et
al. (2011), and the data set described in Hovy et
al. (2014).
We will make the preprocessed data sets avail-
able to the public to facilitate comparison. In ad-
dition to a supervised model trained on expert an-
notations, we compare our tagging accuracy with
that of a weakly supervised system (Li et al., 2012)
re-trained on 400,000 unlabeled tweets to adapt to
Twitter, but using a crowdsourced lexicon, namely
Wiktionary, to constrain inference. We use param-
eter settings from Li et al. (2012), as well as their
Wikipedia dump, available from their project web-
site.5
3. POS tagging is often the first step for further
analysis, such as chunking, parsing, etc. We
test the downstream performance of the POS
models from the previous step on chunking and
NER. We use the models to annotate the training
data portion of each task with POS tags, and
use them as features in a chunking and NER
model. For both tasks, we train a CRF model
on the respective (POS-augmented) training set,
and evaluate it on several held-out test sets. For
chunking, we use the test sets from Foster et al.
(2011) and Ritter et al. (2011) (with the splits
from Derczynski et al. (2013)). For NER, we use
data from Finin et al. (2010) and again Ritter et al.
(2011). For chunking, we follow Sha and Pereira
(2003) for the set of features, including token
and POS information. For NER, we use standard
features, including POS tags (from the previous
experiments), indicators for hyphens, digits,
single quotes, upper/lowercase, 3-character prefix
and suffix information, and Brown word cluster
features6 with 2,4,8,16 bitstring prefixes estimated
from a large Twitter corpus (Owoputi et al., 2013).
We report macro-averages over all these data sets.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999282666666667">
Agreement with expert annotators Table 1
shows the accuracy of each aggregation compared
to the gold labels. The crowdsourced annotations
</bodyText>
<footnote confidence="0.999967333333333">
5https://code.google.com/p/
wikily-supervised-pos-tagger/
6http://www.ark.cs.cmu.edu/TweetNLP/
</footnote>
<page confidence="0.994486">
379
</page>
<bodyText confidence="0.4461374">
majority
MACE-EM
majority+Wiktionary
MACE-EM+Wiktionary
oracle 89.63
</bodyText>
<tableCaption confidence="0.964391">
Table 1: Accuracy (%) of different annotations wrt
gold data
</tableCaption>
<bodyText confidence="0.997979826086956">
aggregated using MV agree with the expert anno-
tations in 79.54% of the cases. If we pre-filter the
data using Wiktionary, the agreement becomes
80.58%. MACE leads to higher agreement with
expert annotations under both conditions (79.89
and 80.75). The small difference indicates that
annotators are consistent and largely reliable,
thus confirming the Bronze-level qualification
we required. Both schemes cannot recover the
correct answer for the 1497 cases where none of
the crowdsourced labels matched the gold label,
i.e. y ∈/ ZZ. The best possible result either of them
could achieve (the oracle) would be matching all
but the missing labels, an agreement of 89.63%.
Most of the cases where the correct label was
not among the annotations belong to a small set
of confusions. The most frequent was mislabeling
“:” and “...”, both mapped to X. Annotators
mostly decided to label these tokens as punctu-
ation (.). They also predominantly labeled your,
my and this as PRON (for the former two), and a
variety of labels for the latter, when the gold label
is DET.
</bodyText>
<table confidence="0.988820125">
RITTER FOSTER HOVY
73.8 77.4 79.7
80.5 81.6 83.7
80.4 81.7 82.6
80.4 82.1 83.7
80.5 81.9 83.7
82.4 83.7 85.1
82.6 84.7 86.8
</table>
<tableCaption confidence="0.998223">
Table 2: POS tagging accuracies (%).
</tableCaption>
<bodyText confidence="0.994782909090909">
Effect on POS Tagging Accuracy Usually, we
don’t want to match a gold standard, but we
rather want to create new annotated training
data. Crowdsourcing matches our gold standard
to about 80%, but the question remains how useful
this data is when training models on it. After all,
inter-annotator agreement among professional an-
notators on this task is only around 90% (Gimpel
et al., 2011; Hovy et al., 2014). In order to evalu-
ate how much each aggregation scheme influences
tagging performance of the resulting model, we
train separate models on each scheme’s annota-
tions and test on the same four data sets. Table
2 shows the results. Note that the differences be-
tween the four schemes are insignificant. More
importantly, however, POS tagging accuracy us-
ing crowdsourced annotations are on average only
2.6% worse than gold using professional annota-
tions. On the other hand, performance is much
better than the weakly supervised approach by Li
et al. (2012), which only relies on a crowdsourced
POS lexicon.
</bodyText>
<figure confidence="0.905234125">
POS model from
MV
MACE
MV+Wik
MACE+Wik
Upper bounds
oracle
gold
</figure>
<tableCaption confidence="0.777244">
Table 3: Downstream accuracy for chunking (l)
</tableCaption>
<bodyText confidence="0.972444052631579">
and NER (r) of models using POS.
Downstream Performance Table 3 shows the
accuracy when using the POS models trained
in the previous evaluation step. Note that we
present the average over the two data sets used
for each task. Note also how the Wiktionary con-
straints lead to improvements in downstream per-
formance. In chunking, we see that using the
crowdsourced annotations leads to worse perfor-
mance than using the professional annotations.
For NER, however, we find that some of the POS
taggers trained on aggregated data produce bet-
ter NER performance than POS taggers trained on
expert-annotated gold data. Since the only dif-
ference between models are the respective POS
features, the results suggest that at least for some
tasks, POS taggers learned from crowdsourced an-
notations may be as good as those learned from
expert annotations.
</bodyText>
<sectionHeader confidence="0.999444" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.855703666666667">
There is considerable work in the literature on
modeling answer correctness and annotator com-
petence as latent variables (Dawid and Skene,
</bodyText>
<figure confidence="0.969103">
79.54
79.89
80.58
80.75
Li et al. (2012)
MV
MACE
MV+Wik
MACE+Wik
Upper bounds
oracle
gold
CHUNKING NER
74.80 75.74
75.04 75.83
75.86 76.08
75.86 76.15
76.22 75.85
79.97 75.81
</figure>
<page confidence="0.99138">
380
</page>
<bodyText confidence="0.96136415625">
1979; Smyth et al., 1995; Carpenter, 2008; White-
hill et al., 2009; Welinder et al., 2010; Yan et al.,
2010; Raykar and Yu, 2012). Rodrigues et al.
(2013) recently presented a sequential model for
this. They estimate annotator competence as la-
tent variables in a CRF model using EM. They
evaluate their approach on synthetic and NER data
annotated on Mechanical Turk, showing improve-
ments over the MV baselines and the multi-label
model by Dredze et al. (2009). The latter do not
model annotator reliability but rather model label
priors by integrating them into the CRF objective,
and re-estimating them during learning. Both re-
quire annotators to supply a full sentence, while
we use minimal context, which requires less anno-
tator commitment and makes the task more flexi-
ble. Unfortunately, we could not run those mod-
els on our data due to label incompatibility and
the fact that we typically do not have complete se-
quences annotated by the same annotators.
Mainzer (2011) actually presents an earlier pa-
per on crowdsourcing POS tagging. However, it
differs from our approach in several ways. It uses
the Penn Treebank tag set to annotate Wikipedia
data (which is much more canonical than Twitter)
via a Java applet. The applet automatically labels
certain categories, and only presents the users with
a series of multiple choice questions for the re-
mainder. This is highly effective, as it eliminates
some sources of possible disagreement. In con-
trast, we do not pre-label any tokens, but always
present the annotators with all labels.
</bodyText>
<sectionHeader confidence="0.987796" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999988833333333">
We use crowdsourcing to collect POS annotations
with minimal context (five-word windows). While
the performance of POS models learned from
this data is still slightly below that of models
trained on expert annotations, models learned
from aggregations approach oracle performance
for POS tagging. In general, we find that the
use of a dictionary tends to make aggregations
more useful, irrespective of aggregation method.
For some downstream tasks, models using the
aggregated POS tags perform even better than
models using expert-annotated tags.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.86145375">
We would like to thank the anonymous review-
ers for valuable comments and feedback. This re-
search is funded by the ERC Starting Grant LOW-
LANDS No. 313695.
</bodyText>
<sectionHeader confidence="0.882067" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99911244">
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazon’s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20–28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39(1):1–
38.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multi-
ple labels. In ECML/PKDD Workshop on Learning
from Multi-Label Data.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon’s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014.
When pos datasets don t add up: Combatting sample
bias. In LREC.
</reference>
<page confidence="0.992237">
381
</page>
<reference confidence="0.996418625">
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara
Rosenthal, and Kathleen McKeown. 2010. Corpus
creation for new genres: A crowdsourced approach
to pp attachment. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jacob Emil Mainzer. 2011. Labeling parts of
speech using untrained annotators on mechanical
turk. Master’s thesis, The Ohio State University.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155–171.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP. Association for Computational Lin-
guistics.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491–518.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Filipe Rodrigues, Francisco Pereira, and Bernardete
Ribeiro. 2013. Sequence labeling with multiple an-
notators. Machine Learning, pages 1–17.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Ad-
vances in neural information processing systems,
pages 1085–1092.
Rion Snow, Brendan O’Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast—but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, Mar(1):1–12.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. Advances in
Neural Information Processing Systems, 22:2035–
2043.
Yan Yan, R´omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator exper-
tise: Learning when everybody knows a bit of some-
thing. In International Conference on Artificial In-
telligence and Statistics.
</reference>
<page confidence="0.9983">
382
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.826019">
<title confidence="0.988528">Experiments with crowdsourced re-annotation of a POS tagging data set</title>
<author confidence="0.966995">Dirk Hovy</author>
<author confidence="0.966995">Barbara Plank</author>
<author confidence="0.966995">Anders</author>
<affiliation confidence="0.9991">Center for Language University of</affiliation>
<address confidence="0.892532">Njalsgade 140, 2300</address>
<email confidence="0.984321">soegaard@hum.ku.dk</email>
<abstract confidence="0.9982561875">Crowdsourcing lets us collect multiple annotations for an item from several annotators. Typically, these are annotations for non-sequential classification tasks. While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced. This paper shows workers annotate sequential data almost as well as experts. Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating Speech and Language Data With Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="1337" citStr="Callison-Burch and Dredze, 2010" startWordPosition="191" endWordPosition="194">, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. Only a few recent studies have investigated crowdsourcing seque</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating Speech and Language Data With Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Multilevel Bayesian models of categorical data annotation.</title>
<date>2008</date>
<tech>Technical report, LingPipe.</tech>
<contexts>
<context position="16073" citStr="Carpenter, 2008" startWordPosition="2574" endWordPosition="2575">old data. Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 7 Related Work There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene, 79.54 79.89 80.58 80.75 Li et al. (2012) MV MACE MV+Wik MACE+Wik Upper bounds oracle gold CHUNKING NER 74.80 75.74 75.04 75.83 75.86 76.08 75.86 76.15 76.22 75.85 79.97 75.81 380 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012). Rodrigues et al. (2013) recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both require annotat</context>
</contexts>
<marker>Carpenter, 2008</marker>
<rawString>Bob Carpenter. 2008. Multilevel Bayesian models of categorical data annotation. Technical report, LingPipe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Philip Dawid</author>
<author>Allan M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics,</title>
<date>1979</date>
<pages>20--28</pages>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. Philip Dawid and Allan M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<pages>38</pages>
<contexts>
<context position="8321" citStr="Dempster et al., 1977" startWordPosition="1305" endWordPosition="1308">average results over 100 runs. We refer to this as MAJORITY VOTING (MV). Note that in MV we trust all annotators to the same degree. However, crowdsourcing attracts people with different mo378 tives, and not all of them are equally reliable— even the ones with Bronze level. Ideally, we would like to factor this into our decision process. We use MACE4 (Hovy et al., 2013) as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977). We use MACE with default parameter settings to give us the weighted average for each annotated example. Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags. Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (T¨ackstr¨om et al., 20</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter part-of-speech tagging for all: overcoming sparse and noisy data. In RANLP.</title>
<date>2013</date>
<contexts>
<context position="10383" citStr="Derczynski et al. (2013)" startWordPosition="1642" endWordPosition="1646">nal annotation. 2. Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data. To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013) 4http://www.isi.edu/publications/ licensed-sw/mace/ on the annotated Twitter data described in Gimpel et al. (2011). Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014). We will make the preprocessed data sets available to the public to facilitate comparison. In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system (Li et al., 2012) re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference. We use parameter settings from Li et al. (2012), as well as their Wikipedia dump, available from their project website.5 3. P</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all: overcoming sparse and noisy data. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
</authors>
<title>Partha Pratim Talukdar, and Koby Crammer.</title>
<date>2009</date>
<booktitle>In ECML/PKDD Workshop on Learning from Multi-Label Data.</booktitle>
<marker>Dredze, 2009</marker>
<rawString>Mark Dredze, Partha Pratim Talukdar, and Koby Crammer. 2009. Sequence learning from data with multiple labels. In ECML/PKDD Workshop on Learning from Multi-Label Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2752" citStr="Eisenstein, 2013" startWordPosition="405" endWordPosition="406">, and the data contains mostly non-entities, so the complexity is manageable. The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1 In this paper, we investigate how well lay annotators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary. Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. We use a minimum of instructions and require few qualifications. Obviously, lay annotation is generally less reliable than professional annotation. It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations. In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013). We also sho</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. What to do about bad language on the internet. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>Will Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in Twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="2008" citStr="Finin et al., 2010" startWordPosition="283" endWordPosition="286">have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition (Finin et al., 2010; Rodrigues et al., 2013). Results for this are good. However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable. The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1 In this paper, we investigate how well lay annotators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of anno</context>
<context position="11619" citStr="Finin et al. (2010)" startWordPosition="1862" endWordPosition="1865">en the first step for further analysis, such as chunking, parsing, etc. We test the downstream performance of the POS models from the previous step on chunking and NER. We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model. For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets. For chunking, we use the test sets from Foster et al. (2011) and Ritter et al. (2011) (with the splits from Derczynski et al. (2013)). For NER, we use data from Finin et al. (2010) and again Ritter et al. (2011). For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information. For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features6 with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013). We report macro-averages over all these data sets. 6 Results Agreement with expert annotators Table 1 shows the accuracy of each aggrega</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in Twitter data with crowdsourcing. In NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Josef Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirde Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comments: Resources and benchmarks for parsing the language of Web 2.0.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Ashish Vaswani</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning whom to trust with MACE.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3339" citStr="Hovy et al., 2013" startWordPosition="498" endWordPosition="501">ictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary. Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. We use a minimum of instructions and require few qualifications. Obviously, lay annotation is generally less reliable than professional annotation. It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations. In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013). We also show how we can use Wiktionary, a crowdsourced lexicon, to filter crowdsourced annotations. We evaluate the annotations in several ways: (a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on 1One of the reviewers alerted us to an unpublished masters thesis, which uses pre-annotation to reduce tagging to fewer multiple-choice questions. See Related Work section for details. 377 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382, Baltimore, Maryland, US</context>
<context position="8071" citStr="Hovy et al., 2013" startWordPosition="1262" endWordPosition="1265">e annotations to derive a single answer for each token. In the simplest scheme, we choose the majority label, i.e., the label picked by most annotators. In case of ties, we select the final label at random. Since this is a stochastic process, we average results over 100 runs. We refer to this as MAJORITY VOTING (MV). Note that in MV we trust all annotators to the same degree. However, crowdsourcing attracts people with different mo378 tives, and not all of them are equally reliable— even the ones with Bronze level. Ideally, we would like to factor this into our decision process. We use MACE4 (Hovy et al., 2013) as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977). We use MACE with default parameter settings to give us the weighted average for each annotated example. Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags.</context>
</contexts>
<marker>Hovy, Berg-Kirkpatrick, Vaswani, Hovy, 2013</marker>
<rawString>Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with MACE. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Barbara Plank</author>
<author>Anders Søgaard</author>
</authors>
<title>When pos datasets don t add up: Combatting sample bias.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="6240" citStr="Hovy et al. (2014)" startWordPosition="946" endWordPosition="949">OUN,NOUN,X,NOUN,-,NOUN NOUN : .,.,-,.,.,. X I PRON,NOUN,PRON,NOUN,PRON,- PRON owe VERB,VERB,-,VERB,VERB,VERB VERB U PRON,X,-,NOUN,NOUN,PRON PRON 0 = 0.9, 0.4, 0.2, 0.8, 0.8, 0.9 Figure 1: Five annotations per token, supplied by 6 different annotators (- = missing annotation), gold label y. 0 = competence values for each annotator. 2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://crowdflower.com 3 Crowdsourcing Sequential Annotation In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels. Figure 2: Screen shot of the annotation interface on Crowdflower Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu. For each tag, we spell out the name of the syntactic category, and provide a few example words. See Figure 2 for a screenshot of the interface. Annotators were also told that words can belong to several classes, depending on the context. No additional guidelines were given. Only trusted annotators (in Crowdflower: Bronze skil</context>
<context position="10474" citStr="Hovy et al. (2014)" startWordPosition="1661" endWordPosition="1664">re annotation quality influences model performance on held-out test data. To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013) 4http://www.isi.edu/publications/ licensed-sw/mace/ on the annotated Twitter data described in Gimpel et al. (2011). Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014). We will make the preprocessed data sets available to the public to facilitate comparison. In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system (Li et al., 2012) re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference. We use parameter settings from Li et al. (2012), as well as their Wikipedia dump, available from their project website.5 3. POS tagging is often the first step for further analysis, such as chunking, parsing, etc. We</context>
<context position="14143" citStr="Hovy et al., 2014" startWordPosition="2255" endWordPosition="2258">bels for the latter, when the gold label is DET. RITTER FOSTER HOVY 73.8 77.4 79.7 80.5 81.6 83.7 80.4 81.7 82.6 80.4 82.1 83.7 80.5 81.9 83.7 82.4 83.7 85.1 82.6 84.7 86.8 Table 2: POS tagging accuracies (%). Effect on POS Tagging Accuracy Usually, we don’t want to match a gold standard, but we rather want to create new annotated training data. Crowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it. After all, inter-annotator agreement among professional annotators on this task is only around 90% (Gimpel et al., 2011; Hovy et al., 2014). In order to evaluate how much each aggregation scheme influences tagging performance of the resulting model, we train separate models on each scheme’s annotations and test on the same four data sets. Table 2 shows the results. Note that the differences between the four schemes are insignificant. More importantly, however, POS tagging accuracy using crowdsourced annotations are on average only 2.6% worse than gold using professional annotations. On the other hand, performance is much better than the weakly supervised approach by Li et al. (2012), which only relies on a crowdsourced POS lexico</context>
</contexts>
<marker>Hovy, Plank, Søgaard, 2014</marker>
<rawString>Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014. When pos datasets don t add up: Combatting sample bias. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mukund Jha</author>
<author>Jacob Andreas</author>
<author>Kapil Thadani</author>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Corpus creation for new genres: A crowdsourced approach to pp attachment.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1303" citStr="Jha et al., 2010" startWordPosition="187" endWordPosition="190">s experts. Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. Only a few recent studies hav</context>
</contexts>
<marker>Jha, Andreas, Thadani, Rosenthal, McKeown, 2010</marker>
<rawString>Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal, and Kathleen McKeown. 2010. Corpus creation for new genres: A crowdsourced approach to pp attachment. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="8766" citStr="Johnson, 2007" startWordPosition="1379" endWordPosition="1380">te for each of the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977). We use MACE with default parameter settings to give us the weighted average for each annotated example. Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags. Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (T¨ackstr¨om et al., 2013). We follow Li et al. (2012) in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry. If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations. Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktiona</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="9988" citStr="Lafferty et al., 2001" startWordPosition="1579" endWordPosition="1582">ry as a pre-filtering step, such that MV and MACE only operate on the reduced annotations. 5 Experiments Each of the two aggregation schemes above produces a final label sequence yˆ for our training corpus. We evaluate the resulting annotated data in three ways. 1. We compare yˆ to the available expert annotation on the training data. This tells us how similar lay annotation is to professional annotation. 2. Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data. To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013) 4http://www.isi.edu/publications/ licensed-sw/mace/ on the annotated Twitter data described in Gimpel et al. (2011). Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014). We will make the preprocessed data sets available to the public to facilitate comparison. In addition to a super</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Emil Mainzer</author>
</authors>
<title>Labeling parts of speech using untrained annotators on mechanical turk. Master’s thesis,</title>
<date>2011</date>
<institution>The Ohio State University.</institution>
<contexts>
<context position="17005" citStr="Mainzer (2011)" startWordPosition="2730" endWordPosition="2731">l Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both require annotators to supply a full sentence, while we use minimal context, which requires less annotator commitment and makes the task more flexible. Unfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators. Mainzer (2011) actually presents an earlier paper on crowdsourcing POS tagging. However, it differs from our approach in several ways. It uses the Penn Treebank tag set to annotate Wikipedia data (which is much more canonical than Twitter) via a Java applet. The applet automatically labels certain categories, and only presents the users with a series of multiple choice questions for the remainder. This is highly effective, as it eliminates some sources of possible disagreement. In contrast, we do not pre-label any tokens, but always present the annotators with all labels. 8 Conclusion We use crowdsourcing t</context>
</contexts>
<marker>Mainzer, 2011</marker>
<rawString>Jacob Emil Mainzer. 2011. Labeling parts of speech using untrained annotators on mechanical turk. Master’s thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="8751" citStr="Merialdo, 1994" startWordPosition="1377" endWordPosition="1378">ompetence estimate for each of the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977). We use MACE with default parameter settings to give us the weighted average for each annotated example. Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags. Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (T¨ackstr¨om et al., 2013). We follow Li et al. (2012) in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry. If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations. Since we aggregate annotations independently (unlike Viterbi decoding), we basical</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="6293" citStr="Petrov et al., 2012" startWordPosition="957" endWordPosition="960">NOUN,PRON,NOUN,PRON,- PRON owe VERB,VERB,-,VERB,VERB,VERB VERB U PRON,X,-,NOUN,NOUN,PRON PRON 0 = 0.9, 0.4, 0.2, 0.8, 0.8, 0.9 Figure 1: Five annotations per token, supplied by 6 different annotators (- = missing annotation), gold label y. 0 = competence values for each annotator. 2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://crowdflower.com 3 Crowdsourcing Sequential Annotation In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels. Figure 2: Screen shot of the annotation interface on Crowdflower Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu. For each tag, we spell out the name of the syntactic category, and provide a few example words. See Figure 2 for a screenshot of the interface. Annotators were also told that words can belong to several classes, depending on the context. No additional guidelines were given. Only trusted annotators (in Crowdflower: Bronze skills) that had answered correctly on 4 gold tokens (ran</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized Models for Unsupervised Part-of-Speech Tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8790" citStr="Ravi and Knight, 2009" startWordPosition="1381" endWordPosition="1384">the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977). We use MACE with default parameter settings to give us the weighted average for each annotated example. Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags. Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (T¨ackstr¨om et al., 2013). We follow Li et al. (2012) in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry. If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations. Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering st</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized Models for Unsupervised Part-of-Speech Tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
</authors>
<title>Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--491</pages>
<contexts>
<context position="16160" citStr="Raykar and Yu, 2012" startWordPosition="2589" endWordPosition="2592">, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 7 Related Work There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene, 79.54 79.89 80.58 80.75 Li et al. (2012) MV MACE MV+Wik MACE+Wik Upper bounds oracle gold CHUNKING NER 74.80 75.74 75.04 75.83 75.86 76.08 75.86 76.15 76.22 75.85 79.97 75.81 380 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012). Rodrigues et al. (2013) recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both require annotators to supply a full sentence, while we use minimal context, which requires less annota</context>
</contexts>
<marker>Raykar, Yu, 2012</marker>
<rawString>Vikas C. Raykar and Shipeng Yu. 2012. Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks. Journal of Machine Learning Research, 13:491–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="10350" citStr="Ritter et al. (2011)" startWordPosition="1636" endWordPosition="1639">ay annotation is to professional annotation. 2. Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data. To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013) 4http://www.isi.edu/publications/ licensed-sw/mace/ on the annotated Twitter data described in Gimpel et al. (2011). Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014). We will make the preprocessed data sets available to the public to facilitate comparison. In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system (Li et al., 2012) re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference. We use parameter settings from Li et al. (2012), as well as their Wikipedia dump, available </context>
<context position="11650" citStr="Ritter et al. (2011)" startWordPosition="1868" endWordPosition="1871">analysis, such as chunking, parsing, etc. We test the downstream performance of the POS models from the previous step on chunking and NER. We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model. For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets. For chunking, we use the test sets from Foster et al. (2011) and Ritter et al. (2011) (with the splits from Derczynski et al. (2013)). For NER, we use data from Finin et al. (2010) and again Ritter et al. (2011). For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information. For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features6 with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013). We report macro-averages over all these data sets. 6 Results Agreement with expert annotators Table 1 shows the accuracy of each aggregation compared to the gold label</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filipe Rodrigues</author>
<author>Francisco Pereira</author>
<author>Bernardete Ribeiro</author>
</authors>
<title>Sequence labeling with multiple annotators.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--17</pages>
<contexts>
<context position="2033" citStr="Rodrigues et al., 2013" startWordPosition="287" endWordPosition="291">with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition (Finin et al., 2010; Rodrigues et al., 2013). Results for this are good. However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable. The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1 In this paper, we investigate how well lay annotators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of annotating Twitter data is no</context>
<context position="8511" citStr="Rodrigues et al. (2013)" startWordPosition="1337" endWordPosition="1340">378 tives, and not all of them are equally reliable— even the ones with Bronze level. Ideally, we would like to factor this into our decision process. We use MACE4 (Hovy et al., 2013) as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977). We use MACE with default parameter settings to give us the weighted average for each annotated example. Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags. Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (T¨ackstr¨om et al., 2013). We follow Li et al. (2012) in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not license</context>
<context position="16185" citStr="Rodrigues et al. (2013)" startWordPosition="2593" endWordPosition="2596">that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 7 Related Work There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene, 79.54 79.89 80.58 80.75 Li et al. (2012) MV MACE MV+Wik MACE+Wik Upper bounds oracle gold CHUNKING NER 74.80 75.74 75.04 75.83 75.86 76.08 75.86 76.15 76.22 75.85 79.97 75.81 380 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012). Rodrigues et al. (2013) recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both require annotators to supply a full sentence, while we use minimal context, which requires less annotator commitment and makes </context>
</contexts>
<marker>Rodrigues, Pereira, Ribeiro, 2013</marker>
<rawString>Filipe Rodrigues, Francisco Pereira, and Bernardete Ribeiro. 2013. Sequence labeling with multiple annotators. Machine Learning, pages 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="11698" citStr="Sha and Pereira (2003)" startWordPosition="1876" endWordPosition="1879">est the downstream performance of the POS models from the previous step on chunking and NER. We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model. For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets. For chunking, we use the test sets from Foster et al. (2011) and Ritter et al. (2011) (with the splits from Derczynski et al. (2013)). For NER, we use data from Finin et al. (2010) and again Ritter et al. (2011). For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information. For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features6 with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013). We report macro-averages over all these data sets. 6 Results Agreement with expert annotators Table 1 shows the accuracy of each aggregation compared to the gold labels. The crowdsourced annotations 5https://code.go</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>Usama Fayyad</author>
<author>Mike Burl</author>
<author>Pietro Perona</author>
<author>Pierre Baldi</author>
</authors>
<title>Inferring ground truth from subjective labelling of Venus images. Advances in neural information processing systems,</title>
<date>1995</date>
<pages>1085--1092</pages>
<contexts>
<context position="16056" citStr="Smyth et al., 1995" startWordPosition="2570" endWordPosition="2573">n expert-annotated gold data. Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 7 Related Work There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene, 79.54 79.89 80.58 80.75 Li et al. (2012) MV MACE MV+Wik MACE+Wik Upper bounds oracle gold CHUNKING NER 74.80 75.74 75.04 75.83 75.86 76.08 75.86 76.15 76.22 75.85 79.97 75.81 380 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012). Rodrigues et al. (2013) recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Bot</context>
</contexts>
<marker>Smyth, Fayyad, Burl, Perona, Baldi, 1995</marker>
<rawString>Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Perona, and Pierre Baldi. 1995. Inferring ground truth from subjective labelling of Venus images. Advances in neural information processing systems, pages 1085–1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Dan Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Dan Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<tech>TACL, Mar(1):1–12.</tech>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. TACL, Mar(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Welinder</author>
<author>Steve Branson</author>
<author>Serge Belongie</author>
<author>Pietro Perona</author>
</authors>
<title>The multidimensional wisdom of crowds.</title>
<date>2010</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="16120" citStr="Welinder et al., 2010" startWordPosition="2581" endWordPosition="2584">en models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 7 Related Work There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene, 79.54 79.89 80.58 80.75 Li et al. (2012) MV MACE MV+Wik MACE+Wik Upper bounds oracle gold CHUNKING NER 74.80 75.74 75.04 75.83 75.86 76.08 75.86 76.15 76.22 75.85 79.97 75.81 380 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012). Rodrigues et al. (2013) recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both require annotators to supply a full sentence, while we use min</context>
</contexts>
<marker>Welinder, Branson, Belongie, Perona, 2010</marker>
<rawString>Peter Welinder, Steve Branson, Serge Belongie, and Pietro Perona. 2010. The multidimensional wisdom of crowds. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Paul Ruvolo</author>
<author>Tingfan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier Movellan</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>22--2035</pages>
<contexts>
<context position="16097" citStr="Whitehill et al., 2009" startWordPosition="2576" endWordPosition="2580">he only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 7 Related Work There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene, 79.54 79.89 80.58 80.75 Li et al. (2012) MV MACE MV+Wik MACE+Wik Upper bounds oracle gold CHUNKING NER 74.80 75.74 75.04 75.83 75.86 76.08 75.86 76.15 76.22 75.85 79.97 75.81 380 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012). Rodrigues et al. (2013) recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both require annotators to supply a full sen</context>
</contexts>
<marker>Whitehill, Ruvolo, Wu, Bergsma, Movellan, 2009</marker>
<rawString>Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. Advances in Neural Information Processing Systems, 22:2035– 2043.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Yan</author>
<author>R´omer Rosales</author>
<author>Glenn Fung</author>
<author>Mark Schmidt</author>
<author>Gerardo Hermosillo</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
<author>Jennifer Dy</author>
</authors>
<title>Modeling annotator expertise: Learning when everybody knows a bit of something.</title>
<date>2010</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="16138" citStr="Yan et al., 2010" startWordPosition="2585" endWordPosition="2588">ctive POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 7 Related Work There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene, 79.54 79.89 80.58 80.75 Li et al. (2012) MV MACE MV+Wik MACE+Wik Upper bounds oracle gold CHUNKING NER 74.80 75.74 75.04 75.83 75.86 76.08 75.86 76.15 76.22 75.85 79.97 75.81 380 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012). Rodrigues et al. (2013) recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009). The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both require annotators to supply a full sentence, while we use minimal context, whic</context>
</contexts>
<marker>Yan, Rosales, Fung, Schmidt, Hermosillo, Bogoni, Moy, Dy, 2010</marker>
<rawString>Yan Yan, R´omer Rosales, Glenn Fung, Mark Schmidt, Gerardo Hermosillo, Luca Bogoni, Linda Moy, and Jennifer Dy. 2010. Modeling annotator expertise: Learning when everybody knows a bit of something. In International Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>