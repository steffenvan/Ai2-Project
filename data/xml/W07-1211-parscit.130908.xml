<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.227704">
<title confidence="0.998794">
A Task-based Comparison of Information Extraction Pattern Models
</title>
<author confidence="0.961523">
Mark A. Greenwood and Mark Stevenson
</author>
<affiliation confidence="0.9973835">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.983456">
Sheffield, S1 4DP, UK
</address>
<email confidence="0.998795">
{m.greenwood, marks}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824294117647">
Several recent approaches to Information
Extraction (IE) have used dependency trees
as the basis for an extraction pattern repre-
sentation. These approaches have used a va-
riety of pattern models (schemes which de-
fine the parts of the dependency tree which
can be used to form extraction patterns).
Previous comparisons of these pattern mod-
els are limited by the fact that they have used
indirect tasks to evaluate each model. This
limitation is addressed here in an experiment
which compares four pattern models using
an unsupervised learning algorithm and a
standard IE scenario. It is found that there
is a wide variation between the models’ per-
formance and suggests that one model is the
most useful for IE.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999656510638298">
A common approach to Information Extraction (IE)
is to (manually or automatically) create a set of pat-
terns which match against text to identify informa-
tion of interest. Muslea (1999) reviewed the ap-
proaches which were used at the time and found
that the most common techniques relied on lexico-
syntactic patterns being applied to text which has
undergone relatively shallow linguistic processing.
For example, the extraction rules used by Soderland
(1999) and Riloff (1996) match text in which syn-
tactic chunks have been identified. More recently
researchers have begun to employ deeper syntactic
analysis, such as dependency parsing (Yangarber et
al., 2000; Stevenson and Greenwood, 2005; Sudo et
al., 2001; Sudo et al., 2003; Yangarber, 2003). In
these approaches extraction patterns are essentially
parts of the dependency tree. To perform extraction
they are compared against the dependency analysis
of a sentence to determine whether it contains the
pattern.
Each of these approaches relies on a pattern
model to define which parts of the dependency tree
can be used to form the extraction patterns. A vari-
ety of pattern models have been proposed. For ex-
ample the patterns used by Yangarber et al. (2000)
are the subject-verb-object tuples from the depen-
dency tree (the remainder of the dependency parse is
discarded) while Sudo et al. (2003) allow any sub-
tree within the dependency parse to act as an ex-
traction pattern. Stevenson and Greenwood (2006)
showed that the choice of pattern model has impor-
tant implications for IE algorithms including signifi-
cant differences between the various models in terms
of their ability to identify information of interest in
text.
However, there has been little comparison be-
tween the various pattern models. Those which have
been carried out have been limited by the fact that
they used indirect tasks to evaluate the various mod-
els and did not compare them in an IE scenario.
We address this limitation here by presenting a di-
rect comparison of four previously described pattern
models using an unsupervised learning method ap-
plied to a commonly used IE scenario.
The remainder of the paper is organised as fol-
lows. The next section presents four pattern models
which have been previously introduced in the litera-
</bodyText>
<page confidence="0.987905">
81
</page>
<note confidence="0.931071">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 81–88,
Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999066166666667">
ture. Section 3 describes two previous studies which
compared these models and their limitations. Sec-
tion 4 describes an experiment which compares the
four models on an IE task, the results of which are
described in Section 5. Finally, Section 6 discusses
the conclusions which may be drawn from this work.
</bodyText>
<sectionHeader confidence="0.993655" genericHeader="method">
2 IE Pattern Models
</sectionHeader>
<bodyText confidence="0.999002625">
In dependency analysis (Mel’ˇcuk, 1987) the syntax
of a sentence is represented by a set of directed bi-
nary links between a word (the head) and one of its
modifiers. These links may be labelled to indicate
the relation between the head and modifier (e.g. sub-
ject, object). An example dependency analysis for
the sentence “Acme hired Smith as their new CEO,
replacing Bloggs.” is shown Figure 1.
</bodyText>
<figureCaption confidence="0.998791">
Figure 1: An example dependency tree.
</figureCaption>
<bodyText confidence="0.970626714285714">
The remainder of this section outlines four mod-
els for representing extraction patterns which can be
derived from dependency trees.
Predicate-Argument Model (SVO): A simple
approach, used by Yangarber et al. (2000), Yangar-
ber (2003) and Stevenson and Greenwood (2005),
is to use subject-verb-object tuples from the depen-
dency parse as extraction patterns. These consist of
a verb and its subject and/or direct object. Figure
2 shows the two SVO patterns1 which are produced
for the dependency tree shown in Figure 1.
This model can identify information which is ex-
pressed using simple predicate-argument construc-
tions such as the relation between Acme and Smith
1The formalism used for representing dependency patterns
is similar to the one introduced by Sudo et al. (2003). Each
node in the tree is represented in the format a[b/c] (e.g.
subj[N/Acme]) where c is the lexical item (Acme), b its
grammatical tag (N) and a the dependency relation between this
node and its parent (subj). The relationship between nodes is
represented as X(A+B+C) which indicates that nodes A, B and
C are direct descendents of node X.
in the dependency tree shown in Figure 1. How-
ever, the SVO model cannot represent information
described using other linguistic constructions such
as nominalisations or prepositional phrases. For ex-
ample the SVO model would not be able to recog-
nise that Smith’s new job title is CEO since these
patterns ignore the part of the dependency tree con-
taining that information.
Chains: A pattern is defined as a path between a
verb node and any other node in the dependency tree
passing through zero or more intermediate nodes
(Sudo et al., 2001). Figure 2 shows examples of the
chains which can be extracted from the tree in Figure
1.
Chains provide a mechanism for encoding infor-
mation beyond the direct arguments of predicates
and includes areas of the dependency tree ignored by
the SVO model. For example, they can represent in-
formation expressed as a nominalisation or within a
prepositional phrase, e.g. “The resignation of Smith
from the board of Acme ...” However, a potential
shortcoming of this model is that it cannot represent
the link between arguments of a verb. Patterns in the
chain model format are unable to represent even the
simplest of sentences containing a transitive verb,
e.g. “Smith left Acme”.
Linked Chains: The linked chains model
(Greenwood et al., 2005) represents extraction pat-
terns as a pair of chains which share the same verb
but no direct descendants. Example linked chains
are shown in Figure 2. This pattern representa-
tion encodes most of the information in the sen-
tence with the advantage of being able to link to-
gether event participants which neither of the SVO
or chain model can, for example the relation be-
tween “Smith” and “Bloggs” in Figure 1.
Subtrees: The final model to be considered is the
subtree model (Sudo et al., 2003). In this model any
subtree of a dependency tree can be used as an ex-
traction pattern, where a subtree is any set of nodes
in the tree which are connected to one another. Sin-
gle nodes are not considered to be subtrees. The
subtree model is a richer representation than those
discussed so far and can represent any part of a de-
pendency tree. Each of the previous models form a
proper subset of the subtrees. By choosing an appro-
priate subtree it is possible to link together any pair
of nodes in a tree and consequently this model can
</bodyText>
<page confidence="0.992835">
82
</page>
<figure confidence="0.9945966875">
SVO
[V/hire](subj[N/Acme]+obj[N/Smith])
[V/replace](obj[N/Bloggs])
Chains
[V/hire](subj[N/Acme])
[V/hire](obj[N/Smith])
[V/hire](obj[N/Smith](as[N/CEO]))
[V/hire](obj[N/Smith](as[N/CEO](gen[N/their])))
Linked Chains
[V/hire](subj[N/Acme]+obj[N/Smith])
[V/hire](subj[N/Acme]+obj[N/Smith](as[N/CEO]))
[V/hire](obj[N/Smith]+vpsc mod[V/replace](obj[N/Bloggs]))
Subtrees
[V/hire](subj[N/Acme]+obj[N/Smith]+vpsc mod[V/replace])
[V/hire](subj[N/Acme]+vpsc mod[V/replace](obj[N/Bloggs]))
[N/Smith](as[N/CEO](gen[N/their]+mod[A/new]))
</figure>
<figureCaption confidence="0.999361">
Figure 2: Example patterns for four models
</figureCaption>
<bodyText confidence="0.969856">
represent the relation between any set of items in the
sentence.
</bodyText>
<sectionHeader confidence="0.98228" genericHeader="method">
3 Previous Comparisons
</sectionHeader>
<bodyText confidence="0.999992087719298">
There have been few direct comparisons of the var-
ious pattern models. Sudo et al. (2003) compared
three models (SVO, chains and subtrees) on two
IE scenarios using a entity extraction task. Mod-
els were evaluated in terms of their ability to iden-
tify entities taking part in events and distinguish
them from those which did not. They found the
SVO model performed poorly in comparison with
the other two models and that the performance of
the subtree model was generally the same as, or
better than, the chain model. However, they did
not attempt to determine whether the models could
identify the relations between these entities, simply
whether they could identify the entities participating
in relevant events.
Stevenson and Greenwood (2006) compared the
four pattern models described in Section 2 in terms
of their complexity and ability to represent rela-
tions found in text. The complexity of each model
was analysed in terms of the number of patterns
which would be generated from a given depen-
dency parse. This is important since several of
the algorithms which have been proposed to make
use of dependency-based IE patterns use iterative
learning (e.g. (Yangarber et al., 2000; Yangarber,
2003; Stevenson and Greenwood, 2005)) and are un-
likely to cope with very large sets of candidate pat-
terns. The number of patterns generated therefore
has an effect on how practical computations using
that model may be. It was found that the number
of patterns generated for the SVO model is a lin-
ear function of the size of the dependency tree. The
number of chains and linked chains is a polynomial
function while the number of subtrees is exponen-
tial.
Stevenson and Greenwood (2006) also analysed
the representational power of each model by measur-
ing how many of the relations found in a standard IE
corpus they are expressive enough to represent. (The
documents used were taken from newswire texts and
biomedical journal articles.) They found that the
SVO and chain model could only represent a small
proportion of the relations in the corpora. The sub-
tree model could represent more of the relations than
any other model but that there was no statistical dif-
ference between those relations and the ones cov-
ered by the linked chain model. They concluded
that the linked chain model was optional since it is
expressive enough to represent the information of
interest without introducing a potentially unwieldy
number of patterns.
There is some agreement between these two stud-
ies, for example that the SVO model performs
poorly in comparison with other models. However,
Stevenson and Greenwood (2006) also found that
the coverage of the chain model was significantly
worse than the subtree model, although Sudo et al.
</bodyText>
<page confidence="0.993286">
83
</page>
<bodyText confidence="0.9993744">
(2003) found that in some cases their performance
could not be distinguished. In addition to these dis-
agreements, these studies are also limited by the fact
that they are indirect; they do not evaluate the vari-
ous pattern models on an IE task.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999995272727273">
We compared each of the patterns models described
in Section 2 using an unsupervised IE experiment
similar to one described by Sudo et al. (2003).
Let D be a corpus of documents and R a set of
documents which are relevant to a particular extrac-
tion task. In this context “relevant” means that the
document contains the information we are interested
in identifying. D and R are such that D = R U R
and Rfl R = 0. As assumption behind this approach
is that useful patterns will be far more likely to occur
in R than D overall.
</bodyText>
<subsectionHeader confidence="0.996148">
4.1 Ranking Patterns
</subsectionHeader>
<bodyText confidence="0.99995975">
Patterns for each model are ranked using a technique
inspired by the tf-idf scoring commonly used in In-
formation Retrieval (Manning and Sch¨utze, 1999).
The score for each pattern, p, is given by:
</bodyText>
<equation confidence="0.998737666666667">
score(p) = tfp x
dfp
N � (1)
</equation>
<bodyText confidence="0.998935789473684">
where tfp is the number of times pattern p ap-
pears in relevant documents, N is the total number
of documents in the corpus and dfp the number of
documents in the collection containing the pattern
p.
Equation 1 combines two factors: the term fre-
quency (in relevant documents) and inverse docu-
ment frequency (across the corpus). Patterns which
occur frequently in relevant documents without be-
ing too prevalent in the corpus are preferred. Sudo
et al. (2003) found that it was important to find the
appropriate balance between these two factors. They
introduced the Q parameter as a way of controlling
the relative contribution of the inverse document fre-
quency. Q is tuned for each extraction task and pat-
tern model combination.
Although simple, this approach has the advantage
that it can be applied to each of the four pattern mod-
els to provide a direct comparison.
</bodyText>
<subsectionHeader confidence="0.993421">
4.2 Extraction Scenario
</subsectionHeader>
<bodyText confidence="0.999958823529412">
The ranking process was applied to the IE scenario
used for the sixth Message Understanding confer-
ence (MUC-6). The aim of this task was to iden-
tify management succession events from a corpus
of newswire texts. Relevant information describes
an executive entering or leaving a position within a
company, for example “Last month Smith resigned
as CEO of Rooter Ltd.”. This sentence described as
event involving three items: a person (Smith), po-
sition (CEO) and company (Rooter Ltd). We made
use of a version of the MUC-6 corpus described by
Soderland (1999) which consists of 598 documents.
For these experiments relevant documents were
identified using annotations in the corpus. However,
this is not necessary since Sudo et al. (2003) showed
that adequate knowledge about document relevance
could be obtained automatically using an IR system.
</bodyText>
<subsectionHeader confidence="0.996904">
4.3 Pattern Generation
</subsectionHeader>
<bodyText confidence="0.999539037037037">
The texts used for these experiments were parsed
using the Stanford dependency parser (Klein and
Manning, 2002). The dependency trees were pro-
cessed to replace the names of entities belonging
to specific semantic classes with a general token.
Three of these classes were used for the manage-
ment succession domain (PERSON, ORGANISA-
TION and POST). For example, in the dependency
analysis of “Smith will became CEO next year”,
“Smith” is replaced by PERSON and “CEO” by
POST. This process allows more general patterns to
be extracted from the dependency trees. For exam-
ple, [V/become](subj[N/PERSON]+obj[N/POST]).
In the MUC-6 corpus items belonging to the relevant
semantic classes are already identified.
Patterns for each of the four models were ex-
tracted from the processed dependency trees. For
the SVO, chain and linked chain models this was
achieved using depth-first search. However, the
enumeration of all subtrees is less straightforward
and has been shown to be a #P-complete prob-
lem (Goldberg and Jerrum, 2000). We made use of
the rightmost extension algorithm (Abe et al., 2002;
Zaki, 2002) which is an efficient way of enumerating
all subtrees. This approach constructs subtrees iter-
atively by combining together subtrees which have
already been observed. The algorithm starts with a
</bodyText>
<page confidence="0.996119">
84
</page>
<bodyText confidence="0.999197210526316">
set of trees, each of which consists of a single node.
At each stage the known trees are extended by the
addition of a single node. In order to avoid dupli-
cation the extension is restricted to allowing nodes
only to be added to the nodes on the rightmost path
of the tree. Applying the process recursively creates
a search space in which all subtrees are enumerated
with minimal duplication.
The rightmost extension algorithm is most suited
to finding subtrees which occur multiple times and,
even using this efficient approach, we were unable
to generate subtrees which occurred fewer than four
times in the MUC-6 texts in a reasonable time. Sim-
ilar restrictions have been encountered within other
approaches which have relied on the generation of
a comprehensive set of subtrees from a parse for-
est. For example, Kudo et al. (2005) used subtrees
for parse ranking but could only generate subtrees
which appear at least ten times in a 40,000 sentence
corpus. They comment that the size of their data set
meant that it would have been difficult to complete
the experiments with less restrictive parameters. In
addition, Sudo et al. (2003) only generated subtrees
which appeared in at least three documents. Kudo
et al. (2005) and Sudo et al. (2003) both used the
rightmost extension algorithm to generate subtrees.
To provide a direct comparison of the pattern
models we also produced versions of the sets of pat-
terns extracted for the SVO, chain and linked chain
models in which patterns which occurred fewer than
four times were removed. Table 1 shows the num-
ber of patterns generated for each of the four mod-
els when the patterns are both filtered and unfil-
tered. (Although the set of unfiltered subtree pat-
terns were not generated it is possible to determine
the number of patterns which would be generated
using a process described by Stevenson and Green-
wood (2006).)
</bodyText>
<table confidence="0.9958508">
Model Filtered Unfiltered
SVO 9,189 23,128
Chains 16,563 142,019
Linked chains 23,452 493,463
Subtrees 369,453 1.69 ×1012
</table>
<tableCaption confidence="0.8582985">
Table 1: Number of patterns generated by each
model
</tableCaption>
<bodyText confidence="0.994524625">
It can be seen that the various pattern models gen-
erate vastly different numbers of patterns and that
the number of subtrees is significantly greater than
the other three models. Previous analysis (see Sec-
tion 3) suggested that the number of subtrees which
would be generated from a corpus could be difficult
to process computationally and this is supported by
our findings here.
</bodyText>
<subsectionHeader confidence="0.996029">
4.4 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.9999801875">
The value of Q in equation 1 was set using a sep-
arate corpus from which the patterns were gener-
ated, a methodology suggested by Sudo et al. (2003).
To generate this additional text we used the Reuters
Corpus (Rose et al., 2002) which consists of a year’s
worth of newswire output. Each document in the
Reuters corpus has been manually annotated with
topic codes indicating its general subject area(s).
One of these topic codes (C411) refers to man-
agement succession events and was used to identify
documents which are relevant to the MUC6 IE sce-
nario. A corpus consisting of 348 documents anno-
tated with code C411 and 250 documents without
that code, representing irrelevant documents, were
taken from the Reuters corpus to create a corpus
with the same distribution of relevant and irrelevant
documents as found in the MUC-6 corpus. Unlike
the MUC-6 corpus, items belonging to the required
semantic classes are not annotated in the Reuters
Corpus. They were identified automatically using
a named entity identifier.
The patterns generated from the MUC-6 texts
were ranked using formula 1 with a variety of val-
ues of Q. These sets of ranked patterns were then
used to carry out a document filtering task on the
Reuters corpus - the aim of which is to differentiate
documents based on whether or not they contain a
relation of interest. The various values for Q were
compared by computing the area under the curve. It
was found that the optimal value for Q was 2 for all
pattern models and this setting was used for the ex-
periments.
</bodyText>
<subsectionHeader confidence="0.972848">
4.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.99952575">
Evaluation was carried out by comparing the ranked
lists of patterns against the dependency trees for the
MUC-6 texts. When a pattern is found to match
against a tree the items which match any seman-
</bodyText>
<page confidence="0.999425">
85
</page>
<bodyText confidence="0.9998528">
tic classes in the pattern are extracted. These items
are considered to be related and compared against
the gold standard data in the corpus to determine
whether they are in fact related.
The precision of a set of patterns is computed as
the proportion of the relations which were identified
that are listed in the gold standard data. The recall is
the proportion of relations in the gold standard data
which are identified by the set of patterns.
The ranked set of patterns are evaluated incremen-
tally with the precision and recall of the first (highest
ranked) pattern computed. The next pattern is then
added to the relations extracted by both are evalu-
ated. This process continues until all patterns are
exhausted.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999446140350877">
Figure 3 shows the results when the four filtered pat-
tern models, ranked using equation 1, are compared.
A first observation is that the chain model
performs poorly in comparison to the other
three models. The highest precision achieved by
this model is 19.9% and recall never increases
beyond 9%. In comparison the SVO model in-
cludes patterns with extremely high precision but
the maximum recall achieved by this model is
low. Analysis showed that the first three SVO
patterns had very high precision. These were
[V/succeed](subj[N/PERSON]+obj[N/PERSON]),
[V/be](subj[N/PERSON]+obj[N/POST]) and
[V/become](subj[N/PERSON]+obj[N/POST]),
which have precision of 90.1%, 80.8% and 78.9%
respectively. If these high precision patterns are
removed the maximum precision of the SVO model
is around 32%, which is comparable with the linked
chain and subtree models. This suggests that, while
the SVO model includes very useful extraction
patterns, the format is restrictive and is unable to
represent much of the information in this corpus.
The remaining two pattern models, linked chains
and subtrees, have very similar performance and
each achieves higher recall than the SVO model, al-
beit with lower precision. The maximum recall ob-
tained by the linked chain model is slightly lower
than the subtree model but it does maintain higher
precision at higher recall levels.
The maximum recall achieved by all four models
is very low in this evaluation and part of the reason
for this is the fact that the patterns have been filtered
to allow direct comparison with the subtree model.
Figure 4 shows the results when the unfiltered SVO,
chain and linked chain patterns are used. (Perfor-
mance of the filtered subtrees are also included in
this graph for comparison.)
This result shows that the addition of extra pat-
terns for each model improves recall without effect-
ing the maximum precision achieved. The chain
model also performs badly in this experiment. Pre-
cision of the SVO model is still high (again this is
due to the same three highly accurate patterns) how-
ever the maximum recall achieved by this model is
not particularly increased by the addition of the un-
filtered patterns. The linked chain model benefits
most from the unfiltered patterns. The extra patterns
lead to a maximum recall which is more than dou-
ble any of the other models without overly degrad-
ing precision. The fact that the linked chain model
is able to achieve such a high recall shows that it is
able to represent the relations found in the MUC-6
text, unlike the SVO and chain models. It is likely
that the subtrees model would also produce a set of
patterns with high recall but the number of poten-
tial patterns which are allowable within this model
makes this impractical.
</bodyText>
<sectionHeader confidence="0.998251" genericHeader="conclusions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999944555555556">
Some of the results reported for each model in these
experiments are low. Precision levels are generally
below 40% (with the exception of the SVO model
which achieves high precision using a small number
of patterns). One reason for this that the the patterns
were ranked using a simple unsupervised learning
algorithm which allowed direct comparison of four
different pattern models. This approach only made
use of information about the distribution of patterns
in the corpus and it is likely that results could be im-
proved for a particular pattern model by employing
more sophisticated approaches which make use of
additional information, for example the structure of
the patterns.
The results presented here provide insight into the
usefulness of the various pattern models by evaluat-
ing them on an actual IE task. It is found that SVO
patterns are capable of high precision but that the
</bodyText>
<page confidence="0.974312">
86
</page>
<figure confidence="0.999466352941176">
0.9
0.8
Subject-Verb-Object
Chains
Linked Chains
Subtrees
Precision
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.0 0.1 0.2 0.3 0.4
Recall
</figure>
<figureCaption confidence="0.997752">
Figure 3: Comparisons of filtered pattern models.
</figureCaption>
<figure confidence="0.998776235294118">
Subject-Verb-Object
Chains
Linked Chains
Subtrees
0.9
0.8
Precision
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Recall
</figure>
<figureCaption confidence="0.999919">
Figure 4: Comparison of unfiltered models.
</figureCaption>
<page confidence="0.99509">
87
</page>
<bodyText confidence="0.999960692307692">
restricted set of possible patterns leads to low re-
call. The chain model was found to perform badly
with low recall and precision regardless of whether
the patterns were filtered. Performance of the linked
chain and subtree models were similar when the pat-
terns were filtered but unfiltered linked chains were
capable of achieving far higher recall than the fil-
tered subtrees.
These experiments suggest that the linked chain
model is a useful one for IE since it is simple enough
for an unfiltered set of patterns to be extracted and
able to represent a wider range of information than
the SVO and chain models.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985744047619">
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki
Arimura, and Setsuo Arikawa. 2002. Optimised Sub-
structure Discovery for Semi-Structured Data. In Pro-
ceedings of the 6th European Conference on Princi-
ples and Practice ofKnowledge in Databases (PKDD-
2002), pages 1–14.
Leslie Ann Goldberg and Mark Jerrum. 2000. Counting
Unlabelled Subtrees of a Tree is #P-Complete. Lon-
don Mathmatical Society Journal of Computation and
Mathematics, 3:117–124.
Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk
Harkema, and Angus Roberts. 2005. Automati-
cally Acquiring a Linguistically Motivated Genic In-
teraction Extraction System. In Proceedings of the
4th Learning Language in Logic Workshop (LLL05),
Bonn, Germany.
Dan Klein and Christopher D. Manning. 2002. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), Vancouver,
Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based Parse Reranking with Subtree Fea-
tures. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
189–196, Ann Arbour, MI.
Chritopher Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Igor Mel’ˇcuk. 1987. Dependency Syntax: Theory and
Practice. SUNY Press, New York.
Ion Muslea. 1999. Extraction Patterns for Information
Extraction: A Survey. In Proceedings of the AAAI-99
workshop on Machine Learning for Information Ex-
traction, Orlando, FL.
Ellen Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1044–1049, Portland, OR.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1 - from Yes-
terday’s News to Tomorrow’s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-02),
pages 827–832, La Palmas de Gran Canaria.
Stephen Soderland. 1999. Learning Information Extrac-
tion Rules for Semi-structured and Free Text. Machine
Learning, 31(1-3):233–272.
Mark Stevenson and Mark A. Greenwood. 2005. A Se-
mantic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379–386, Ann Ar-
bor, MI.
Mark Stevenson and Mark A. Greenwood. 2006. Com-
paring Information Extraction Pattern Models. In Pro-
ceedings of the Information Extraction Beyond The
Document Workshop (COLING/ACL 2006), pages 12–
19, Sydney, Australia.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001),
San Diego, CA.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-03), pages
224–231, Sapporo, Japan.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Unsupervised Discov-
ery of Scenario-level Patterns for Information Extrac-
tion. In Proceedings of the Applied Natural Language
Processing Conference (ANLP 2000), pages 282–289,
Seattle, WA.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-03), pages 343–350, Sapporo,
Japan.
Mohammed Zaki. 2002. Effectively Mining Frequent
Trees in a Forest. In 8th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 71–80, Edmonton, Canada.
</reference>
<page confidence="0.999408">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577856">
<title confidence="0.999717">A Task-based Comparison of Information Extraction Pattern Models</title>
<author confidence="0.999448">A Greenwood</author>
<affiliation confidence="0.995897">Department of Computer University of</affiliation>
<address confidence="0.592882">Sheffield, S1 4DP,</address>
<abstract confidence="0.998622388888889">Several recent approaches to Information Extraction (IE) have used dependency trees as the basis for an extraction pattern representation. These approaches have used a variety of pattern models (schemes which define the parts of the dependency tree which can be used to form extraction patterns). Previous comparisons of these pattern models are limited by the fact that they have used indirect tasks to evaluate each model. This limitation is addressed here in an experiment which compares four pattern models using an unsupervised learning algorithm and a standard IE scenario. It is found that there is a wide variation between the models’ performance and suggests that one model is the most useful for IE.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenji Abe</author>
<author>Shinji Kawasoe</author>
<author>Tatsuya Asai</author>
<author>Hiroki Arimura</author>
<author>Setsuo Arikawa</author>
</authors>
<title>Optimised Substructure Discovery for Semi-Structured Data.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th European Conference on Principles and Practice ofKnowledge in Databases (PKDD2002),</booktitle>
<pages>1--14</pages>
<contexts>
<context position="14809" citStr="Abe et al., 2002" startWordPosition="2391" endWordPosition="2394">. This process allows more general patterns to be extracted from the dependency trees. For example, [V/become](subj[N/PERSON]+obj[N/POST]). In the MUC-6 corpus items belonging to the relevant semantic classes are already identified. Patterns for each of the four models were extracted from the processed dependency trees. For the SVO, chain and linked chain models this was achieved using depth-first search. However, the enumeration of all subtrees is less straightforward and has been shown to be a #P-complete problem (Goldberg and Jerrum, 2000). We made use of the rightmost extension algorithm (Abe et al., 2002; Zaki, 2002) which is an efficient way of enumerating all subtrees. This approach constructs subtrees iteratively by combining together subtrees which have already been observed. The algorithm starts with a 84 set of trees, each of which consists of a single node. At each stage the known trees are extended by the addition of a single node. In order to avoid duplication the extension is restricted to allowing nodes only to be added to the nodes on the rightmost path of the tree. Applying the process recursively creates a search space in which all subtrees are enumerated with minimal duplicatio</context>
</contexts>
<marker>Abe, Kawasoe, Asai, Arimura, Arikawa, 2002</marker>
<rawString>Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and Setsuo Arikawa. 2002. Optimised Substructure Discovery for Semi-Structured Data. In Proceedings of the 6th European Conference on Principles and Practice ofKnowledge in Databases (PKDD2002), pages 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie Ann Goldberg</author>
<author>Mark Jerrum</author>
</authors>
<title>Counting Unlabelled Subtrees of a Tree is #P-Complete.</title>
<date>2000</date>
<journal>London Mathmatical Society Journal of Computation and Mathematics,</journal>
<pages>3--117</pages>
<contexts>
<context position="14741" citStr="Goldberg and Jerrum, 2000" startWordPosition="2379" endWordPosition="2382">th will became CEO next year”, “Smith” is replaced by PERSON and “CEO” by POST. This process allows more general patterns to be extracted from the dependency trees. For example, [V/become](subj[N/PERSON]+obj[N/POST]). In the MUC-6 corpus items belonging to the relevant semantic classes are already identified. Patterns for each of the four models were extracted from the processed dependency trees. For the SVO, chain and linked chain models this was achieved using depth-first search. However, the enumeration of all subtrees is less straightforward and has been shown to be a #P-complete problem (Goldberg and Jerrum, 2000). We made use of the rightmost extension algorithm (Abe et al., 2002; Zaki, 2002) which is an efficient way of enumerating all subtrees. This approach constructs subtrees iteratively by combining together subtrees which have already been observed. The algorithm starts with a 84 set of trees, each of which consists of a single node. At each stage the known trees are extended by the addition of a single node. In order to avoid duplication the extension is restricted to allowing nodes only to be added to the nodes on the rightmost path of the tree. Applying the process recursively creates a searc</context>
</contexts>
<marker>Goldberg, Jerrum, 2000</marker>
<rawString>Leslie Ann Goldberg and Mark Jerrum. 2000. Counting Unlabelled Subtrees of a Tree is #P-Complete. London Mathmatical Society Journal of Computation and Mathematics, 3:117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
<author>Mark Stevenson</author>
<author>Yikun Guo</author>
<author>Henk Harkema</author>
<author>Angus Roberts</author>
</authors>
<title>Automatically Acquiring a Linguistically Motivated Genic Interaction Extraction System.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th Learning Language in Logic Workshop (LLL05),</booktitle>
<location>Bonn, Germany.</location>
<contexts>
<context position="6531" citStr="Greenwood et al., 2005" startWordPosition="1059" endWordPosition="1062"> for encoding information beyond the direct arguments of predicates and includes areas of the dependency tree ignored by the SVO model. For example, they can represent information expressed as a nominalisation or within a prepositional phrase, e.g. “The resignation of Smith from the board of Acme ...” However, a potential shortcoming of this model is that it cannot represent the link between arguments of a verb. Patterns in the chain model format are unable to represent even the simplest of sentences containing a transitive verb, e.g. “Smith left Acme”. Linked Chains: The linked chains model (Greenwood et al., 2005) represents extraction patterns as a pair of chains which share the same verb but no direct descendants. Example linked chains are shown in Figure 2. This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between “Smith” and “Bloggs” in Figure 1. Subtrees: The final model to be considered is the subtree model (Sudo et al., 2003). In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any </context>
</contexts>
<marker>Greenwood, Stevenson, Guo, Harkema, Roberts, 2005</marker>
<rawString>Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk Harkema, and Angus Roberts. 2005. Automatically Acquiring a Linguistically Motivated Genic Interaction Extraction System. In Proceedings of the 4th Learning Language in Logic Workshop (LLL05), Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS 2002),</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="13833" citStr="Klein and Manning, 2002" startWordPosition="2236" endWordPosition="2239">ooter Ltd.”. This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd). We made use of a version of the MUC-6 corpus described by Soderland (1999) which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since Sudo et al. (2003) showed that adequate knowledge about document relevance could be obtained automatically using an IR system. 4.3 Pattern Generation The texts used for these experiments were parsed using the Stanford dependency parser (Klein and Manning, 2002). The dependency trees were processed to replace the names of entities belonging to specific semantic classes with a general token. Three of these classes were used for the management succession domain (PERSON, ORGANISATION and POST). For example, in the dependency analysis of “Smith will became CEO next year”, “Smith” is replaced by PERSON and “CEO” by POST. This process allows more general patterns to be extracted from the dependency trees. For example, [V/become](subj[N/PERSON]+obj[N/POST]). In the MUC-6 corpus items belonging to the relevant semantic classes are already identified. Pattern</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based Parse Reranking with Subtree Features.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Ann Arbour, MI.</location>
<contexts>
<context position="15853" citStr="Kudo et al. (2005)" startWordPosition="2565" endWordPosition="2568"> be added to the nodes on the rightmost path of the tree. Applying the process recursively creates a search space in which all subtrees are enumerated with minimal duplication. The rightmost extension algorithm is most suited to finding subtrees which occur multiple times and, even using this efficient approach, we were unable to generate subtrees which occurred fewer than four times in the MUC-6 texts in a reasonable time. Similar restrictions have been encountered within other approaches which have relied on the generation of a comprehensive set of subtrees from a parse forest. For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, Sudo et al. (2003) only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and Sudo et al. (2003) both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for t</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based Parse Reranking with Subtree Features. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Ann Arbour, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chritopher Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Chritopher Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1987</date>
<publisher>SUNY Press,</publisher>
<location>New York.</location>
<marker>Mel’ˇcuk, 1987</marker>
<rawString>Igor Mel’ˇcuk. 1987. Dependency Syntax: Theory and Practice. SUNY Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Muslea</author>
</authors>
<title>Extraction Patterns for Information Extraction: A Survey.</title>
<date>1999</date>
<booktitle>In Proceedings of the AAAI-99 workshop on Machine Learning for Information Extraction,</booktitle>
<location>Orlando, FL.</location>
<contexts>
<context position="1132" citStr="Muslea (1999)" startWordPosition="176" endWordPosition="177">. Previous comparisons of these pattern models are limited by the fact that they have used indirect tasks to evaluate each model. This limitation is addressed here in an experiment which compares four pattern models using an unsupervised learning algorithm and a standard IE scenario. It is found that there is a wide variation between the models’ performance and suggests that one model is the most useful for IE. 1 Introduction A common approach to Information Extraction (IE) is to (manually or automatically) create a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patter</context>
</contexts>
<marker>Muslea, 1999</marker>
<rawString>Ion Muslea. 1999. Extraction Patterns for Information Extraction: A Survey. In Proceedings of the AAAI-99 workshop on Machine Learning for Information Extraction, Orlando, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Thirteenth National Conference on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1044--1049</pages>
<location>Portland, OR.</location>
<contexts>
<context position="1422" citStr="Riloff (1996)" startWordPosition="221" endWordPosition="222"> found that there is a wide variation between the models’ performance and suggests that one model is the most useful for IE. 1 Introduction A common approach to Information Extraction (IE) is to (manually or automatically) create a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used t</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044–1049, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Rose</author>
<author>Mark Stevenson</author>
<author>Miles Whitehead</author>
</authors>
<title>The Reuters Corpus Volume 1 - from Yesterday’s News to Tomorrow’s Language Resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-02),</booktitle>
<pages>827--832</pages>
<contexts>
<context position="17695" citStr="Rose et al., 2002" startWordPosition="2877" endWordPosition="2880">seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models. Previous analysis (see Section 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. 4.4 Parameter Tuning The value of Q in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by Sudo et al. (2003). To generate this additional text we used the Reuters Corpus (Rose et al., 2002) which consists of a year’s worth of newswire output. Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s). One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario. A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 c</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Tony Rose, Mark Stevenson, and Miles Whitehead. 2002. The Reuters Corpus Volume 1 - from Yesterday’s News to Tomorrow’s Language Resources. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-02), pages 827–832, La Palmas de Gran Canaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
</authors>
<title>Learning Information Extraction Rules for Semi-structured and Free Text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>31--1</pages>
<contexts>
<context position="1404" citStr="Soderland (1999)" startWordPosition="218" endWordPosition="219">rd IE scenario. It is found that there is a wide variation between the models’ performance and suggests that one model is the most useful for IE. 1 Introduction A common approach to Information Extraction (IE) is to (manually or automatically) create a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency </context>
<context position="13411" citStr="Soderland (1999)" startWordPosition="2176" endWordPosition="2177">els to provide a direct comparison. 4.2 Extraction Scenario The ranking process was applied to the IE scenario used for the sixth Message Understanding conference (MUC-6). The aim of this task was to identify management succession events from a corpus of newswire texts. Relevant information describes an executive entering or leaving a position within a company, for example “Last month Smith resigned as CEO of Rooter Ltd.”. This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd). We made use of a version of the MUC-6 corpus described by Soderland (1999) which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since Sudo et al. (2003) showed that adequate knowledge about document relevance could be obtained automatically using an IR system. 4.3 Pattern Generation The texts used for these experiments were parsed using the Stanford dependency parser (Klein and Manning, 2002). The dependency trees were processed to replace the names of entities belonging to specific semantic classes with a general token. Three of these classes were used for the manag</context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>Stephen Soderland. 1999. Learning Information Extraction Rules for Semi-structured and Free Text. Machine Learning, 31(1-3):233–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Mark A Greenwood</author>
</authors>
<title>A Semantic Approach to IE Pattern Induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>379--386</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1637" citStr="Stevenson and Greenwood, 2005" startWordPosition="251" endWordPosition="254">ually or automatically) create a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of </context>
<context position="4404" citStr="Stevenson and Greenwood (2005)" startWordPosition="704" endWordPosition="707">presented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the relation between the head and modifier (e.g. subject, object). An example dependency analysis for the sentence “Acme hired Smith as their new CEO, replacing Bloggs.” is shown Figure 1. Figure 1: An example dependency tree. The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000), Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure 2 shows the two SVO patterns1 which are produced for the dependency tree shown in Figure 1. This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003). Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the</context>
<context position="9447" citStr="Stevenson and Greenwood, 2005" startWordPosition="1491" endWordPosition="1494">ions between these entities, simply whether they could identify the entities participating in relevant events. Stevenson and Greenwood (2006) compared the four pattern models described in Section 2 in terms of their complexity and ability to represent relations found in text. The complexity of each model was analysed in terms of the number of patterns which would be generated from a given dependency parse. This is important since several of the algorithms which have been proposed to make use of dependency-based IE patterns use iterative learning (e.g. (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005)) and are unlikely to cope with very large sets of candidate patterns. The number of patterns generated therefore has an effect on how practical computations using that model may be. It was found that the number of patterns generated for the SVO model is a linear function of the size of the dependency tree. The number of chains and linked chains is a polynomial function while the number of subtrees is exponential. Stevenson and Greenwood (2006) also analysed the representational power of each model by measuring how many of the relations found in a standard IE corpus they are expressive enough </context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>Mark Stevenson and Mark A. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 379–386, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Mark A Greenwood</author>
</authors>
<title>Comparing Information Extraction Pattern Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Information Extraction Beyond The Document Workshop (COLING/ACL</booktitle>
<pages>12--19</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2406" citStr="Stevenson and Greenwood (2006)" startWordPosition="379" endWordPosition="382">ree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while Sudo et al. (2003) allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text. However, there has been little comparison between the various pattern models. Those which have been carried out have been limited by the fact that they used indirect tasks to evaluate the various models and did not compare them in an IE scenario. We address this limitation here by presenting a direct comparison of four previously described pattern models using an unsupervised lea</context>
<context position="8958" citStr="Stevenson and Greenwood (2006)" startWordPosition="1411" endWordPosition="1414">VO, chains and subtrees) on two IE scenarios using a entity extraction task. Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not. They found the SVO model performed poorly in comparison with the other two models and that the performance of the subtree model was generally the same as, or better than, the chain model. However, they did not attempt to determine whether the models could identify the relations between these entities, simply whether they could identify the entities participating in relevant events. Stevenson and Greenwood (2006) compared the four pattern models described in Section 2 in terms of their complexity and ability to represent relations found in text. The complexity of each model was analysed in terms of the number of patterns which would be generated from a given dependency parse. This is important since several of the algorithms which have been proposed to make use of dependency-based IE patterns use iterative learning (e.g. (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005)) and are unlikely to cope with very large sets of candidate patterns. The number of patterns generated therefo</context>
<context position="10813" citStr="Stevenson and Greenwood (2006)" startWordPosition="1721" endWordPosition="1724">el could only represent a small proportion of the relations in the corpora. The subtree model could represent more of the relations than any other model but that there was no statistical difference between those relations and the ones covered by the linked chain model. They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns. There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models. However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although Sudo et al. 83 (2003) found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. 4 Experiments We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by Sudo et al. (2003). Let D be a corpus of documents and R a set of documents which are relevant</context>
<context position="16890" citStr="Stevenson and Greenwood (2006)" startWordPosition="2741" endWordPosition="2745">. (2003) both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed. Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006).) Model Filtered Unfiltered SVO 9,189 23,128 Chains 16,563 142,019 Linked chains 23,452 493,463 Subtrees 369,453 1.69 ×1012 Table 1: Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models. Previous analysis (see Section 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. 4.4 Parameter Tuning The value of Q in equa</context>
</contexts>
<marker>Stevenson, Greenwood, 2006</marker>
<rawString>Mark Stevenson and Mark A. Greenwood. 2006. Comparing Information Extraction Pattern Models. In Proceedings of the Information Extraction Beyond The Document Workshop (COLING/ACL 2006), pages 12– 19, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Automatic Pattern Acquisition for Japanese Information Extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT2001),</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="1656" citStr="Sudo et al., 2001" startWordPosition="255" endWordPosition="258">a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency pars</context>
<context position="5792" citStr="Sudo et al., 2001" startWordPosition="937" endWordPosition="940">A+B+C) which indicates that nodes A, B and C are direct descendents of node X. in the dependency tree shown in Figure 1. However, the SVO model cannot represent information described using other linguistic constructions such as nominalisations or prepositional phrases. For example the SVO model would not be able to recognise that Smith’s new job title is CEO since these patterns ignore the part of the dependency tree containing that information. Chains: A pattern is defined as a path between a verb node and any other node in the dependency tree passing through zero or more intermediate nodes (Sudo et al., 2001). Figure 2 shows examples of the chains which can be extracted from the tree in Figure 1. Chains provide a mechanism for encoding information beyond the direct arguments of predicates and includes areas of the dependency tree ignored by the SVO model. For example, they can represent information expressed as a nominalisation or within a prepositional phrase, e.g. “The resignation of Smith from the board of Acme ...” However, a potential shortcoming of this model is that it cannot represent the link between arguments of a verb. Patterns in the chain model format are unable to represent even the </context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2001</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2001. Automatic Pattern Acquisition for Japanese Information Extraction. In Proceedings of the Human Language Technology Conference (HLT2001), San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>224--231</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1675" citStr="Sudo et al., 2003" startWordPosition="259" endWordPosition="262">hich match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) whi</context>
<context position="4910" citStr="Sudo et al. (2003)" startWordPosition="786" endWordPosition="789">l (SVO): A simple approach, used by Yangarber et al. (2000), Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure 2 shows the two SVO patterns1 which are produced for the dependency tree shown in Figure 1. This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003). Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj). The relationship between nodes is represented as X(A+B+C) which indicates that nodes A, B and C are direct descendents of node X. in the dependency tree shown in Figure 1. However, the SVO model cannot represent information described using other linguistic constructions such as nominalisations or prepositional phrases. For example the SVO model would not be able to recognise that Smit</context>
<context position="7021" citStr="Sudo et al., 2003" startWordPosition="1146" endWordPosition="1149"> sentences containing a transitive verb, e.g. “Smith left Acme”. Linked Chains: The linked chains model (Greenwood et al., 2005) represents extraction patterns as a pair of chains which share the same verb but no direct descendants. Example linked chains are shown in Figure 2. This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between “Smith” and “Bloggs” in Figure 1. Subtrees: The final model to be considered is the subtree model (Sudo et al., 2003). In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another. Single nodes are not considered to be subtrees. The subtree model is a richer representation than those discussed so far and can represent any part of a dependency tree. Each of the previous models form a proper subset of the subtrees. By choosing an appropriate subtree it is possible to link together any pair of nodes in a tree and consequently this model can 82 SVO [V/hire](subj[N/Acme]+obj[N/Smith]) [V/replace](obj[N/Blogg</context>
<context position="8303" citStr="Sudo et al. (2003)" startWordPosition="1305" endWordPosition="1308">](obj[N/Smith](as[N/CEO])) [V/hire](obj[N/Smith](as[N/CEO](gen[N/their]))) Linked Chains [V/hire](subj[N/Acme]+obj[N/Smith]) [V/hire](subj[N/Acme]+obj[N/Smith](as[N/CEO])) [V/hire](obj[N/Smith]+vpsc mod[V/replace](obj[N/Bloggs])) Subtrees [V/hire](subj[N/Acme]+obj[N/Smith]+vpsc mod[V/replace]) [V/hire](subj[N/Acme]+vpsc mod[V/replace](obj[N/Bloggs])) [N/Smith](as[N/CEO](gen[N/their]+mod[A/new])) Figure 2: Example patterns for four models represent the relation between any set of items in the sentence. 3 Previous Comparisons There have been few direct comparisons of the various pattern models. Sudo et al. (2003) compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task. Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not. They found the SVO model performed poorly in comparison with the other two models and that the performance of the subtree model was generally the same as, or better than, the chain model. However, they did not attempt to determine whether the models could identify the relations between these entities, simply whether they could identify the entities participa</context>
<context position="11337" citStr="Sudo et al. (2003)" startWordPosition="1812" endWordPosition="1815">model performs poorly in comparison with other models. However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although Sudo et al. 83 (2003) found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. 4 Experiments We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by Sudo et al. (2003). Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task. In this context “relevant” means that the document contains the information we are interested in identifying. D and R are such that D = R U R and Rfl R = 0. As assumption behind this approach is that useful patterns will be far more likely to occur in R than D overall. 4.1 Ranking Patterns Patterns for each model are ranked using a technique inspired by the tf-idf scoring commonly used in Information Retrieval (Manning and Sch¨utze, 1999). The score for each pattern, p, is given by: sc</context>
<context position="13590" citStr="Sudo et al. (2003)" startWordPosition="2201" endWordPosition="2204">im of this task was to identify management succession events from a corpus of newswire texts. Relevant information describes an executive entering or leaving a position within a company, for example “Last month Smith resigned as CEO of Rooter Ltd.”. This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd). We made use of a version of the MUC-6 corpus described by Soderland (1999) which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since Sudo et al. (2003) showed that adequate knowledge about document relevance could be obtained automatically using an IR system. 4.3 Pattern Generation The texts used for these experiments were parsed using the Stanford dependency parser (Klein and Manning, 2002). The dependency trees were processed to replace the names of entities belonging to specific semantic classes with a general token. Three of these classes were used for the management succession domain (PERSON, ORGANISATION and POST). For example, in the dependency analysis of “Smith will became CEO next year”, “Smith” is replaced by PERSON and “CEO” by P</context>
<context position="16158" citStr="Sudo et al. (2003)" startWordPosition="2616" endWordPosition="2619">pproach, we were unable to generate subtrees which occurred fewer than four times in the MUC-6 texts in a reasonable time. Similar restrictions have been encountered within other approaches which have relied on the generation of a comprehensive set of subtrees from a parse forest. For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, Sudo et al. (2003) only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and Sudo et al. (2003) both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed. Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it </context>
<context position="17614" citStr="Sudo et al. (2003)" startWordPosition="2863" endWordPosition="2866">369,453 1.69 ×1012 Table 1: Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models. Previous analysis (see Section 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. 4.4 Parameter Tuning The value of Q in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by Sudo et al. (2003). To generate this additional text we used the Reuters Corpus (Rose et al., 2002) which consists of a year’s worth of newswire output. Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s). One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario. A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with t</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 224–231, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Unsupervised Discovery of Scenario-level Patterns for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the Applied Natural Language Processing Conference (ANLP</booktitle>
<pages>282--289</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1606" citStr="Yangarber et al., 2000" startWordPosition="247" endWordPosition="250">traction (IE) is to (manually or automatically) create a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dep</context>
<context position="4351" citStr="Yangarber et al. (2000)" startWordPosition="696" endWordPosition="699">Mel’ˇcuk, 1987) the syntax of a sentence is represented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the relation between the head and modifier (e.g. subject, object). An example dependency analysis for the sentence “Acme hired Smith as their new CEO, replacing Bloggs.” is shown Figure 1. Figure 1: An example dependency tree. The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000), Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure 2 shows the two SVO patterns1 which are produced for the dependency tree shown in Figure 1. This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003). Each node in the tree is represented in</context>
<context position="9398" citStr="Yangarber et al., 2000" startWordPosition="1485" endWordPosition="1488">ether the models could identify the relations between these entities, simply whether they could identify the entities participating in relevant events. Stevenson and Greenwood (2006) compared the four pattern models described in Section 2 in terms of their complexity and ability to represent relations found in text. The complexity of each model was analysed in terms of the number of patterns which would be generated from a given dependency parse. This is important since several of the algorithms which have been proposed to make use of dependency-based IE patterns use iterative learning (e.g. (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005)) and are unlikely to cope with very large sets of candidate patterns. The number of patterns generated therefore has an effect on how practical computations using that model may be. It was found that the number of patterns generated for the SVO model is a linear function of the size of the dependency tree. The number of chains and linked chains is a polynomial function while the number of subtrees is exponential. Stevenson and Greenwood (2006) also analysed the representational power of each model by measuring how many of the relations found in</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Unsupervised Discovery of Scenario-level Patterns for Information Extraction. In Proceedings of the Applied Natural Language Processing Conference (ANLP 2000), pages 282–289, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Counter-training in the Discovery of Semantic Patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>343--350</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1693" citStr="Yangarber, 2003" startWordPosition="263" endWordPosition="264">text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while Sudo et al. (20</context>
<context position="4369" citStr="Yangarber (2003)" startWordPosition="700" endWordPosition="702">x of a sentence is represented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the relation between the head and modifier (e.g. subject, object). An example dependency analysis for the sentence “Acme hired Smith as their new CEO, replacing Bloggs.” is shown Figure 1. Figure 1: An example dependency tree. The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000), Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure 2 shows the two SVO patterns1 which are produced for the dependency tree shown in Figure 1. This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003). Each node in the tree is represented in the format a[b/c]</context>
<context position="9415" citStr="Yangarber, 2003" startWordPosition="1489" endWordPosition="1490">dentify the relations between these entities, simply whether they could identify the entities participating in relevant events. Stevenson and Greenwood (2006) compared the four pattern models described in Section 2 in terms of their complexity and ability to represent relations found in text. The complexity of each model was analysed in terms of the number of patterns which would be generated from a given dependency parse. This is important since several of the algorithms which have been proposed to make use of dependency-based IE patterns use iterative learning (e.g. (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005)) and are unlikely to cope with very large sets of candidate patterns. The number of patterns generated therefore has an effect on how practical computations using that model may be. It was found that the number of patterns generated for the SVO model is a linear function of the size of the dependency tree. The number of chains and linked chains is a polynomial function while the number of subtrees is exponential. Stevenson and Greenwood (2006) also analysed the representational power of each model by measuring how many of the relations found in a standard IE co</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>Roman Yangarber. 2003. Counter-training in the Discovery of Semantic Patterns. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 343–350, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Zaki</author>
</authors>
<title>Effectively Mining Frequent Trees in a Forest.</title>
<date>2002</date>
<booktitle>In 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>71--80</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="14822" citStr="Zaki, 2002" startWordPosition="2395" endWordPosition="2396">ows more general patterns to be extracted from the dependency trees. For example, [V/become](subj[N/PERSON]+obj[N/POST]). In the MUC-6 corpus items belonging to the relevant semantic classes are already identified. Patterns for each of the four models were extracted from the processed dependency trees. For the SVO, chain and linked chain models this was achieved using depth-first search. However, the enumeration of all subtrees is less straightforward and has been shown to be a #P-complete problem (Goldberg and Jerrum, 2000). We made use of the rightmost extension algorithm (Abe et al., 2002; Zaki, 2002) which is an efficient way of enumerating all subtrees. This approach constructs subtrees iteratively by combining together subtrees which have already been observed. The algorithm starts with a 84 set of trees, each of which consists of a single node. At each stage the known trees are extended by the addition of a single node. In order to avoid duplication the extension is restricted to allowing nodes only to be added to the nodes on the rightmost path of the tree. Applying the process recursively creates a search space in which all subtrees are enumerated with minimal duplication. The rightm</context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed Zaki. 2002. Effectively Mining Frequent Trees in a Forest. In 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 71–80, Edmonton, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>