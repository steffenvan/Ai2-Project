<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000090">
<title confidence="0.998671">
Experiments with an Annotation Scheme for a Knowledge-rich Noun Phrase
Interpretation System
</title>
<author confidence="0.995387">
Roxana Girju
</author>
<affiliation confidence="0.998912">
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.99834">
girju@uiuc.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999365">
This paper presents observations on our ex-
perience with an annotation scheme that was
used in the training of a state-of-the-art noun
phrase semantic interpretation system. The
system relies on cross-linguistic evidence
from a set of five Romance languages: Span-
ish, Italian, French, Portuguese, and Roma-
nian. Given a training set of English noun
phrases in context along with their transla-
tions in the five Romance languages, our
algorithm automatically learns a classifica-
tion function that is later on applied to un-
seen test instances for semantic interpreta-
tion. As training and test data we used two
text collections of different genre: Europarl
and CLUVI. The training data was annotated
with contextual features based on two state-
of-the-art classification tag sets.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996463">
Linguistically annotated corpora are valuable re-
sources for both theoretical and computational lin-
guistics. They have played an important role in any
aspect of natural language processing research, from
supervised learning to evaluation, and have been
used in many applications such as Syntactic and Se-
mantic Parsing, Information Extraction, and Ques-
tion Answering.
A long-term research topic in linguistics, compu-
tational linguistics1, and artificial intelligence has
</bodyText>
<footnote confidence="0.791967">
1In the past few years at many workshops, tutorials, and
competitions this research topic has received considerable inter-
</footnote>
<bodyText confidence="0.999927310344828">
been the semantic interpretation of noun phrases
(NPs). The basic problem is simple to define: given
a noun phrase constructed out of a pair of concepts
expressed by words or phrases, c1 – c2, one rep-
resenting the head and the other the modifier, de-
termine the semantic relationship between the two
concepts. For example, a compound family estate
should be interpreted as the estate OWNED BY the
family; an NP such as dress of silk should be inter-
preted as denoting a dress MADE FROM silk. The
problem, while simple to state is hard to solve. The
reason is that the meaning of these constructions is
most of the time ambiguous or implicit.
Currently, the best-performing English NP inter-
pretation methods in computational linguistics fo-
cus mostly on two consecutive noun instances (noun
compounds) and are either (weakly) supervised,
knowledge-intensive (Rosario and Hearst, 2001),
(Rosario et al., 2002), (Moldovan et al., 2004),
(Pantel and Pennacchiotti, 2006), (Pennacchiotti and
Pantel, 2006), (Kim and Baldwin, 2006), (Snow et
al., 2006), (Girju et al., 2005; Girju et al., 2006),
or use statistical models on large collections of un-
labeled data (Berland and Charniak, 1999), (Lap-
ata and Keller, 2004), (Nakov and Hearst, 2005),
(Turney, 2006). Unlike unsupervised models, su-
pervised knowledge-rich approaches rely heavily on
large sets of annotated training data. For example,
we previously showed (Girju et al., 2006) that, for
</bodyText>
<footnote confidence="0.894339833333333">
est from the computational linguistics community: Workshop
on Multiword Expressions at COLING/ACL 2006, 2004, 2003;
Computational Lexical Semantics Workshop at ACL 2004; Tu-
torial on Knowledge Discovery from Text at ACL 2003; Shared
task on Semantic Role Labeling at CONLL 2005, 2004 and at
SENSEVAL 2005.
</footnote>
<page confidence="0.895225">
168
</page>
<note confidence="0.7199455">
Proceedings of the Linguistic Annotation Workshop, pages 168–175,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999880875">
the task of automatic detection of part-whole rela-
tions, our system’s learning curve reached a plateau
at 74% F-measure when trained on approximatively
10,000 positive and negative examples.
Interpreting NPs correctly requires various types
of information from world knowledge to complex
context features. Since the training data needs to be
as accurate as possible, many of such features are
manually identified and annotated. Thus, the anno-
tation process is an important task that requires not
only considerable amount of time, but also experi-
ence with various annotation schemas and tools, and
a good understanding of the research topic. More-
over, the extension of the noun phrase interpretation
task to other natural languages brings forward new
annotation issues.
This paper presents observations on our experi-
ence with an annotation scheme that was used in the
training of a state-of-the-art noun phrase semantic
interpretation system (Girju, 2007). The system re-
lies on cross-linguistic evidence from a set of five
Romance languages: Spanish, Italian, French, Por-
tuguese, and Romanian. Given a training set of En-
glish noun phrases in context along with their trans-
lations in the five Romance languages, our algo-
rithm automatically learns a classification function
that is later on applied to unseen test instances for
semantic interpretation. As training and test data
we used two text collections of different genre: Eu-
roparl2 and CLUVI3. The training data was anno-
tated with contextual features based on two state-of-
the-art classification tag sets: Lauer’s set of 8 prepo-
sitions (Lauer, 1995) and our list of 22 semantic re-
lations. The system achieved an accuracy of 77.9%
(Europarl) and 74.31% (CLUVI).
The paper is organized as follows. Section 2
presents a summary of linguistic considerations of
noun phrases. In Section 3 we describe the list of se-
mantic interpretation categories used along with ob-
servations regarding their distribution on the two dif-
</bodyText>
<footnote confidence="0.9814342">
2http://www.isi.edu/koehn/europarl/
This corpus contains over 20 million words in eleven official
languages of the European Union covering the proceedings of
the European Parliament from 1996 to 2001.
3CLUVI - Linguistic Corpus of the University of Vigo Par-
allel Corpus 2.1 - http://sli.uvigo.es/CLUVI/. CLUVI is an open
text repository of parallel corpora of contemporary oral and
written texts in some of the Romance languages, such as Gali-
cian, French, Spanish, Portuguese, Basque parallel text collec-
tions.
</footnote>
<bodyText confidence="0.9934725">
ferent cross-lingual corpora. Section 4 presents the
data used along with observations on corpus annota-
tion and inter-annotator agreement. Finally, Section
5 offers some discussion and conclusions.
</bodyText>
<sectionHeader confidence="0.7861435" genericHeader="method">
2 Linguistic considerations of noun
phrases
</sectionHeader>
<bodyText confidence="0.999937025">
The automatic discovery of semantic relations must
start with a thorough understanding of the linguistic
aspects of the underlying relations. These consider-
ations are not only employed as features in the su-
pervised noun phrase interpretation model, but they
are also used in the annotation process.
Noun phrases can be compositional when their
meaning is derived from the meaning of the con-
stituent nouns (e.g., door knob – PART-WHOLE,
kiss in the morning – TEMPORAL), or idiosyn-
cratic, when the meaning is a matter of conven-
tion (e.g., soap opera, sea lion). NPs can also ex-
press metaphorical names (eg, ladyfinger), proper
names (e.g., John Doe), and binomial (dvandva)
compounds in which neither noun is the head (e.g.,
player-coach).
NPs can also be classified into synthetic (verbal)
and root (non-verbal) constructions. It is widely held
(Levi, 1978), (Selkirk, 1982) that the modifier noun
of a synthetic noun compound, for example, may be
associated with a theta-role of the verbal head. For
instance, in truck driver, the noun truck satisfies the
THEME relation associated with the direct object in
the corresponding argument structure of the verb to
drive.
Studied cross-linguistically, noun phrases can ex-
press variations from one language to another. For
example, English compounds of the form N1 N2
(e.g., wood stove) usually translate in Romance lan-
guages as N2 P N1 (e.g., four a´ bois (French) –
stove at/to wood). Romance languages have very
few N N compounds and they are of limited se-
mantic categories, such as TYPE (e.g., legge quadro
(Italian) – framework law). Moreover, while En-
glish N N compounds are right-headed (e.g., frame-
work/modifier law/head), Romance compounds are
left-headed (e.g., legge/head quadro/modifier).
For this research we focus only on English–
Romance compositional noun phrases of the type
N N and N P N and disregard metaphorical and
</bodyText>
<page confidence="0.998118">
169
</page>
<bodyText confidence="0.996900666666667">
proper names. In the following section we present
two different state-of-the-art classification sets used
in NP interpretation.
</bodyText>
<sectionHeader confidence="0.713123" genericHeader="method">
3 Lists of semantic classification relations
</sectionHeader>
<bodyText confidence="0.999979025">
Although researchers (Downing, 1977), (Jespersen,
1954) argued that noun compounds, and NPs in gen-
eral, encode an infinite set of semantic relations,
many agree (Finin, 1980), (Levi, 1978) there is a
limited number of relations that occur with high fre-
quency in these constructions. However, the num-
ber and the level of abstraction of these frequently
used semantic categories are not agreed upon. They
can vary from a few prepositions (Lauer, 1995) to
hundreds and even thousands more specific seman-
tic relations (Finin, 1980). The more abstract the
categories, the more noun phrases are covered, but
also the more room for variation as to which cat-
egory a phrase should be assigned. Lauer (Lauer,
1995), for example, considers a set of eight prepo-
sitions as semantic classification categories that can
link the head and the modifier nouns in a noun com-
pound: of, for, with, in, on, at, about, and from.
However, according to this classification, the noun
compound love story, for instance, can be classified
both as story of love and story about love. The main
problem with these abstract categories is that much
of the meaning of individual compounds is lost, and
sometimes there is no way to decide whether a form
is derived from one category or another. On the
other hand, lists of very specific semantic relations
are difficult to build as they usually contain a very
large number of predicates, such as the list of all
possible verbs that can link the noun constituents.
Finin (Finin, 1980), for example, uses semantic cat-
egories such as “dissolved in” to build interpreta-
tions of compounds such as “salt water” and “sugar
water”.
In this research we experiment with two sets of
semantic classification categories defined at differ-
ent abstraction levels. The first is a core set of 22 se-
mantic relations (22 SRs), set which was identified
by us from the linguistics literature and from vari-
ous experiments after many iterations over a period
of time (Moldovan and Girju, 2003)4. We proved
</bodyText>
<footnote confidence="0.9724295">
4There are also other lists of semantic relations used by the
research community (e.g., (Barker and Szpakowicz, 1998)), but
</footnote>
<bodyText confidence="0.999925891891892">
empirically that this set is encoded by noun – noun
pairs in noun phrases and is a subset of our larger
list of 35 semantic relations. This list, presented
in Table 1 along with examples and semantic ar-
gument frames, is general enough to cover a large
majority of text semantics while keeping the seman-
tic relations to a manageable number. A semantic
argument frame is defined for each semantic rela-
tion and indicates the position of each semantic ar-
gument in the underlying relation. For example,
“Arg1 is part of (whole) Arg2” identifies the part
(Arg1) and the whole (Arg2) entities of this rela-
tion. This representation is important since it allows
to distinguish between different arrangements of the
arguments for given relation instances. For exam-
ple, most of the time, in N N compounds Arg1 pre-
cedes Arg2, while in N P N constructions the po-
sition is reversed (Arg2 P Arg1). However, this
is not always the case as shown by N N instances
such as “ham/Arg1 sandwich/Arg2” and “door/Arg2
knob/Arg1”. These argument frames were intro-
duced to provide consistent guide to the annotators
to easily test the goodness-of-fit of the relations.
The second set is Lauer’s list of 8 prepositions and
can be applied only to noun–noun compounds. We
selected these two state-of-the-art sets as they are
of different size and contain semantic classification
categories at different levels of abstraction. Lauer’s
list is more abstract and, thus capable of encoding a
large number of noun compound instances found in
a corpus, while our list contains finer grained seman-
tic categories. Details about the coverage of these
semantic lists on the two different corpora (Europarl
and CLUVI), how well they solve the interpretation
problem of noun phrases, and the mapping from one
list to another are provided in a companion paper
(Girju, 2007).
</bodyText>
<sectionHeader confidence="0.97699" genericHeader="method">
4 The data
</sectionHeader>
<bodyText confidence="0.9997948">
For a better understanding of the semantic relations
encoded by N N and N P N instances, we analyzed
the semantic behavior of these constructions on a
large cross-linguistic corpora of examples. Our in-
tention is to answer questions such as:
</bodyText>
<listItem confidence="0.823010666666667">
(1) What syntactic constructions are used to
translate the English instances to the target Ro-
they overlap considerably with our list of 22-SR.
</listItem>
<page confidence="0.973349">
170
</page>
<table confidence="0.996696">
No. Semantic Default argument frame Examples
Relations
1 POSSESSION Arg1 POSSESSES Arg2 family#2/Arg1 estate#2/Arg2
2 KINSHIP Arg1 IS IN KINSHIP REL. WITH Arg2 the boy#1/Arg1’s sister#1/Arg2
3 PROPERTY Arg2 IS PROPERTY OF Arg1 lubricant#1/Arg1 viscosity#1/Arg2
4 AGENT Arg1 IS AGENT OF Arg2 investigation#2/Arg2 of the crew#2/Arg1
5 TEMPORAL Arg2 IS TEMPORAL LOCATION OF Arg1 morning#1/Arg2 news#3/Arg1
6 DEPICTION-DEPICTED Arg1 DEPICTS Arg2 a picture#1Arg1 of the nice#1/Arg2
7 PART-WHOLE Arg2 IS PART OF (whole) Arg1 faces#1/Arg2 of children#1/Arg1
8 HYPERNYMY (IS-A) Arg2 IS A Arg1 daisy#1/Arg2 flower#1/Arg1
9 CAUSE Arg1 CAUSES Arg2 scream#1/Arg2 ofpain#1/Arg1
10 MAKE/PRODUCE Arg1 PRODUCES Arg2 chocolate#2/Arg2 factory#1/Arg1
11 INSTRUMENT Arg2 IS INSTRUMENT OF Arg1 laser#1/Arg2 treatment#1/Arg1
12 LOCATION Arg2 IS LOCATED IN Arg1 castle#1/Arg2 in the desert#1/Arg1
13 PURPOSE Arg2 IS PURPOSE OF Arg1 cough#1/Arg2 syrup#1/Arg1
14 SOURCE Arg2 IS SOURCE OF Arg1 grapefruit#2/Arg2 oil#3/Arg1
15 TOPIC Arg2 IS TOPIC OF Arg1 weather#1/Arg2 report#2/Arg2
16 MANNER Arg2 IS MANNER OF Arg1 performance#3/Arg1 with passion#1/Arg2
17 MEANS Arg2 IS MEANS OF Arg1 bus#1/Arg2 service#1/Arg1
18 EXPERIENCER Arg1 IS EXPERIENCER OF Arg2 the girl#1/Arg1’s fear#1/Arg2
19 MEASURE Arg2 IS MEASURE OF Arg1 cup#2/Arg2 ofsugar#1/Arg1
20 RESEMBLANCE/TYPE Arg2 RESEMBLES OR IS A TYPE OF Arg1 framework#1/Arg1 law#2/Arg2
21 THEME Arg2 IS THEME OF Arg1 acquisition#1/Arg1 ofstock#1/Arg2
22 BENEFICIARY Arg1 IS BENEFICIARY OF Arg2 reward#1/Arg2 for the finder#1/Arg1
OTHERS altar#1 boys#1
</table>
<tableCaption confidence="0.984226">
Table 1: The set of 22 semantic relations along with examples interpreted in context and the semantic
argument frame.
</tableCaption>
<bodyText confidence="0.7184455">
mance languages and vice-versa? (cross-linguistic
syntactic mapping),
</bodyText>
<listItem confidence="0.993460142857143">
(2) What semantic relations do these construc-
tions encode? (cross-linguistic semantic mapping),
(3) What is the corpus distribution of the seman-
tic relations per each syntactic construction?, and
finally
(4) What is the role of English and Romance
prepositions in the NP interpretation?
</listItem>
<bodyText confidence="0.999883">
Thus, we collected the data from two text col-
lections with different distributions and of different
genre, Europarl and CLUVI.
</bodyText>
<subsectionHeader confidence="0.583478">
The Europarl text collection
</subsectionHeader>
<bodyText confidence="0.990182371428571">
Europarl is a parallel corpora of over 20 million
words in eleven official languages of the Euro-
pean Union covering the proceedings of the Eu-
ropean Parliament from 1996 to 2001. The cor-
pus was assembled by combining four of the bilin-
gual sentence-aligned corpora made public as part
of the freely available Europarl corpus. Specifi-
cally, the Spanish-English, Italian-English, French-
English and Portuguese-English corpora were au-
tomatically aligned based on exact matches of En-
glish translations. Then, only those English sen-
tences which appeared verbatim in all four language
pairs were considered. The resulting English cor-
pus contained 10,000 sentences which were syntac-
tically parsed (Charniak, 2000). From these we ex-
tracted the first 3,000 NP instances (N N: 48.82%
and N P N: 51.18%).
The CLUVI text collection
CLUVI (Linguistic Corpus of the University of
Vigo) is an open text repository of parallel cor-
pora of contemporary oral and written languages,
resource that besides Galician also contains literary
text collections in other Romance languages. We fo-
cused only on the English-Portuguese and English-
Spanish literary parallel texts from the works of
John Steinbeck, H. G. Wells, J. Salinger, among
others. Using the CLUVI search interface we cre-
ated a sentence-aligned parallel corpus of 2,800
English-Spanish and English-Portuguese sentences.
The English versions were automatically parsed af-
ter which each N N and N P N instance thus iden-
tified was manually mapped to the corresponding
translations. The resulting corpus contains 2,200
English instances with a distribution of 26.77% N N
and 73.23% N P N.
</bodyText>
<page confidence="0.990794">
171
</page>
<subsectionHeader confidence="0.988188">
4.1 Corpus annotation
</subsectionHeader>
<bodyText confidence="0.999970444444444">
For each corpus, each NP instance was presented
separately to two experienced annotators5 in a web
interface in context along with the English sentence
and its translations. Since the corpora do not cover
some of the languages (Romanian in Europarl and
CLUVI, and Italian and French in CLUVI), three
other native speakers of these languages and flu-
ent in English provided the translations which were
added to the list.
</bodyText>
<subsectionHeader confidence="0.769114">
WordNet senses
</subsectionHeader>
<bodyText confidence="0.999945625">
The two computational semantics annotators had
to tag each English constituent noun with its cor-
responding WordNet sense6. If the word was not
found in WordNet the instance was not considered.
Tagging each noun constituent with the corre-
sponding WordNet sense in context is important not
only as a feature employed in the training models,
but also as guidance for the annotators to select the
right semantic relation. For instance, in the fol-
lowing sentences, daisy flower expresses a PART-
WHOLE relation in (1) and a IS-A relation in (2) de-
pending on the sense of the noun flower (cf. Word-
Net 2.1: flower#2 is a “reproductive organ of an-
giosperm plants especially one having showy or col-
orful parts”, while flower#1 is “a plant cultivated for
its blooms or blossoms”).
</bodyText>
<listItem confidence="0.9399122">
(1) “Usually, more than one daisy#1 flower#2
grows on top of a single stem.”
(2) “Try them with orange or yellow flowers of
red-hot poker, solidago or other late daisy#1
flowers#1, such as rudbeckias and heliopsis.”
</listItem>
<bodyText confidence="0.99960975">
In cases where noun senses were not enough for
relation selection, the annotators had to rely on a
larger context provided by the sentence and its trans-
lations as shown below.
</bodyText>
<subsectionHeader confidence="0.790433">
Semantic argument frame
</subsectionHeader>
<bodyText confidence="0.99994">
The annotators were also asked to identify the trans-
lation phrases, tag each instance with the corre-
sponding semantic relation, and identify the seman-
tic arguments Arg1 and Arg2 in the semantic argu-
ment frame of the corresponding relation.
</bodyText>
<footnote confidence="0.99785575">
5The annotators have extensive expertise in computational
semantics and are fluent in at least two of the Romance lan-
guages considered for this task.
6For the purpose of this research we used WordNet 2.1.
</footnote>
<bodyText confidence="0.999975777777778">
Thus, since the order of the semantic arguments
in an NP is not fixed (Girju et al., 2005), the an-
notators were presented with the semantic argu-
ment frame for each of the 22 semantic relations
and were asked to tag the NP instances accord-
ingly. For example, in PART-WHOLE instances such
as chair/Arg2 arm/Arg1 the part arm follows the
whole chair, while in button/Arg1 shirt/Arg2 the or-
der is reversed.
</bodyText>
<subsectionHeader confidence="0.637206">
Translation instances
</subsectionHeader>
<bodyText confidence="0.999402939393939">
In the annotation process the annotators were asked
to identify and use, if necessary, the five correspond-
ing translations as additional information in select-
ing the semantic relation. Since only N N and N P N
noun phrase constructions were considered, the an-
notators had to discard those instances encoded by
different syntactic constructions in the Romance lan-
guages.
For instance, the context provided by the Europarl
English sentence in (3) below does not give enough
information for the disambiguation of the English
noun phrase “judgment of the presidency” which
can mean either AGENT or THEME. The annotators
had to rely on the Romance translations in order to
identify the correct meaning in context (in this case
THEME): valoraci´on sobre la Presidencia (Es.), avis
sur la pr´esidence (Fr.), giudizio sulla Presidenza
(It.), veredicto sobre a Presidˆencia (Port.), evalu-
area Presendit¸iei (Ro.)7.
En.: “If you do , our final judgment of the
Spanish presidency will be even more
positive than it has been so far.”
Es.: “Si se hace, nuestra valoraci´on sobre
la Presidencia espa˜nola del Consejo ser´a
a´un mucho m´as positiva de lo que es hasta
ahora.”
Fr.: “Si cela arrive, notre avis sur la
pr´esidence espagnole du Conseil sera
encore beaucoup plus positif que ce n’est
d´ej`a le cas.”
It.: “Se ci riuscir`a il nostro giudizio sulla
Presidenza spagnola sar`a ancora pi`u
positivo di quanto non sia stato finora.”
</bodyText>
<footnote confidence="0.3694335">
7En. means English, Es. – Spanish, Fr. – French, It. –
Italian, Port. – Portuguese, and Ro. – Romanian.
</footnote>
<page confidence="0.997437">
172
</page>
<bodyText confidence="0.998348166666667">
Port.: “Se isso acontecer, o nosso veredicto
sobre a Presidˆencia espanhola ser´a ainda
muito mais positivo do que o actual.”
Ro.: “Dacˇa are loc, evaluarea Pres¸edint¸iei
spaniole va fi ˆıncˇa mai pozitivˇa decit
inˇa acum.”
</bodyText>
<subsectionHeader confidence="0.937277">
Semantic relations
</subsectionHeader>
<bodyText confidence="0.999980307692308">
Whenever the annotators found an example encod-
ing a semantic relation or a preposition paraphrase
other than those provided or they didn’t know what
interpretation to give, they had to tag it as OTHER-
SR and OTHER-PP, respectively . For example, in
the CLUVI sentences (4) and (5) below, the noun
phrases melody of the pearl and cry of death (the cry
announcing death) were tagged as OTHER-SR since
here the context of the sentences does not indicate
the association between the two nouns. Moreover,
noun compound instances such as the corner box
and knowledge searches were tagged as OTHER-PP
(box in the corner, searches after knowledge).
</bodyText>
<listItem confidence="0.902717666666667">
(3) LPE-284: “And because the need was great
and the desire was great, the little secret
melody of the pearl that might be was
stronger this morning.” (En.)
(4) LPE-1582: “And then Kino’s brain cleared
from its red concentration and he knew the
</listItem>
<bodyText confidence="0.984675692307692">
sound - the keening, moaning, rising hyster-
ical cry from the little cave in the side of the
stone mountain, the cry of death.” (En.)
Moreover, most of the time one instance was
tagged with one semantic relation, and respectively
preposition paraphrase, but there were also situa-
tions in which an example could belong to more
than one classification category in the same con-
text. For example, Texas city is tagged as PART-
WHOLE/PLACE-AREA, but also as a LOCATION re-
lation using the 22-SR classification category, and
respectively as of, from, in based on the 8-PP cat-
egory (e.g., city of Texas, city from Texas, and
city in Texas). Other instances, however, can en-
code a total of three semantic relations in a par-
ticular context. One such instance is cup#2 of
hot chocolate#1 in example (6) below, which was
tagged in CLUVI as MEASURE/OTHER(CONTENT-
CONTAINER)/LOC. Sense #2 of cup in WordNet
refers to “the quantity the cup will hold” (cf. Word-
Net 2.1), thus mostly indicating a MEASURE rela-
tion.
(5) 557-AGU: “Wouldn’t you like a cup of hot
chocolate before you go?” (En.)
However, since most hot beverages (such as tea,
coffee, and chocolate) are served in cups, it stands
to reason that the instance can be easily paraphrased
as a cup holding hold chocolate. Although our cur-
rent NP interpretation system (Girju, 2007) does
not differentiate between LOCATION and CONTENT-
CONTAINER (as other researchers (Tyler and Evans,
2003)8, we consider CONTENT-CONTAINER as a
special type of LOCATION), we capture them in our
annotation scheme.
Other examples of multiple annotations are
MEASURE/PART-WHOLE (e.g., an abundance of
buildings, a bunch ofguys), Overall, 0.5% Europarl
and 6.9% CLUVI instances were tagged with more
than one semantic relation, and almost all noun com-
pound instances were tagged with more than one
preposition.
Thus, the annotated instances used in the cor-
pus analysis and system training phases have
the following format: &lt;NPEn ;NPE3; NPzt;
NPFr; NPPort; NPRo; target&gt;. The word tar-
get is one of the 23 (22 + OTHER) semantic
relations or one of the eight prepositions con-
sidered. For example, &lt;judgment#2/Arg1 of
presidency#2/Arg2; valoraci´on sobre la Presiden-
cia; avis sur la pr´esidence; giudizio sulla Pres-
idenza; veredicto sobre a Presidˆencia; evaluarea
Pres¸edinjiei; THEME&gt;.
</bodyText>
<subsectionHeader confidence="0.934493">
4.2 Inter-annotator agreement
</subsectionHeader>
<bodyText confidence="0.999146666666667">
The annotators’ agreement was measured using
Kappa statistics, one of the most frequently used
measure of inter-annotator agreement for classifica-
tion tasks: K = Pr(a)−Pr(E) 1−Pr(E) , where Pr(A) is the
proportion of times the annotators agree and Pr(E)
is the probability of agreement by chance. The K
coefficient is 1 if there is a total agreement among
the annotators, and 0 if there is no agreement other
than that expected to occur by chance.
</bodyText>
<footnote confidence="0.599127">
8(Tyler and Evans, 2003) cite child language acquisition
studies which show there is a strong cognitive relationship be-
tween LOCATION and CONTENT-CONTAINER.
</footnote>
<note confidence="0.330374">
p
</note>
<page confidence="0.997785">
173
</page>
<bodyText confidence="0.99998225">
The Kappa values obtained on each corpus are
shown in Table 2. We also computed the number
of pairs that were tagged with OTHER by both an-
notators for each semantic relation and preposition
paraphrase, over the number of examples classified
in that category by at least one of the judges. For the
noun compound instances that encoded more than
one classification category, the agreement was done
on one of the relations only.
The agreement obtained for the Europarl corpus
is higher than the one for CLUVI on both classifica-
tion sets. This is partially explained by the distribu-
tion of semantic relations in both corpora. Overall,
the K coefficient shows a fair to good level of agree-
ment for the corpus data on the set of 22-SRs, tak-
ing into consideration the task difficulty. The level
of agreement for the prepositional paraphrases was
much higher. All these can be explained by the in-
structions the annotators received prior to the anno-
tation and by their expertise in lexical semantics.
</bodyText>
<table confidence="0.996150857142857">
Corpus Classification Kappa Agreement OTHER
tag sets
N N NPN
Europarl 8-PP 0.80 N/A 91%
22-SR 0.61 0.67 78%
CLUVI 8-PP 0.77 N/A 86%
22-SR 0.56 0.58 69%
</table>
<tableCaption confidence="0.997208">
Table 2: The inter-annotator agreement on the NP annotation
</tableCaption>
<bodyText confidence="0.956815777777778">
on the two corpora. For the noun compound instances that en-
coded more than one semantic classification category, the agree-
ment was done on one of the relations only. “N/A” means not
applicable.
13.05% of Europarl9 and 1.9% of CLUVI in-
stances that could not be tagged with Lauer’s prepo-
sitions were included in OTHER-PP category. About
99% of the Europarl N N instances encode TYPE re-
lations (e.g., framework law), while in CLUVI most
of them were TYPE (e.g., nightmare sensation), fol-
lowed by OTHER-SR (e.g., altar boys), and IS-A
(e.g., Winchester carbine).
From the initial corpus we considered those En-
glish instances that had all the translations encoded
by N N and N P N. Out of these, we selected only
1,023 Europarl and 1,008 CLUVI instances encoded
by N N and N P N in all languages considered and
resulted after agreement10. We split the corpora us-
</bodyText>
<footnote confidence="0.99939075">
9Only 5.70% of the TYPE instances in the Europarl corpus
were unique.
10The annotated corpora resulted in this research are avail-
able at http://apfel.ai.uiuc.edu.
</footnote>
<bodyText confidence="0.99773">
ing a 8:2 training - test ratio and used it to train and
test our system. Details about the experiments and
the results obtained are presented in (Girju, 2007).
</bodyText>
<sectionHeader confidence="0.971342" genericHeader="discussions">
5 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.999632744186046">
In this paper we presented some observations on our
experience with an annotation scheme that was used
in the training of a state-of-the-art noun phrase se-
mantic interpretation system. These observations
are defined in the framework of a larger project. This
project is to investigate various linguistic issues and
develop specific language models for the interpreta-
tion of noun phrase constructions in Germanic, Ro-
mance, and other classes of languages.
Our approach to NP interpretation, and thus an-
notation procedure, is novel in several ways. We
define the problem in a cross-linguistic framework
and provide empirical observations on various an-
notation issues based on a set of two different cor-
pora using two state-of-the-art classification tag sets:
Lauer’s prepositions and our list of 22 relations.
The linguistic implications are also important to
mention here. The annotation investigations done in
this research provide new insights into the research
topic at hand, the semantic interpretation of noun
phrases, in particular and the identification of se-
mantic relations between nominals (irrespective of
the syntactic constructions that link the two nouns),
in general. One such linguistic aspect is the impor-
tance of context for this task. Sometimes, the local
context of the noun phrase is not enough to disam-
biguate the underlying instances. For this, the anno-
tators need to relay on world and domain specific
knowledge and the entire context of the sentence,
or consider a larger context window (from a simple
paragraph including the sentence, to the discourse of
the text) as shown below in (6), (7), and (8). In (6)
and (7), for example, neither the context of the sen-
tence, nor the context of their paragraph provide the
meaning of the NPs. Many of the CLUVI instances
tagged as OTHER-SR (such as the music of the pearl
in (6)), are naming phrases – they were defined only
once in the text collection and later on mentioned to
refer to the initial concept.
In (8), on the other hand, the meaning of the
NP the destruction of the Palestinian Authority is
THEME and not AGENT as might be considered by
default.
</bodyText>
<page confidence="0.992036">
174
</page>
<listItem confidence="0.844265166666667">
(6) LPE-390: “And the music of the pearl
rose like a chorus of trumpets in his ears.”
(CLUVI)
(7) “Mr President, the violent destruction of the
State ofIsrael.” (Europarl)
(8) “The spread of the settlements, the seizing
</listItem>
<bodyText confidence="0.991036857142857">
of land, the curfews, the Palestinians im-
prisoned in their own villages, the summary
executions, the ambulances prevented from
reaching their destinations, the women giv-
ing birth at check points, the destruction of
the Palestinian Authority: these are not mis-
takes or accidents.” (Europarl)
</bodyText>
<sectionHeader confidence="0.9996" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999960666666667">
We would like to thank all the people who helped
with the corpus creation and annotation, and those
with whom we had nice discussions about vari-
ous semantic relations. Without them this research
wouldn’t have been possible: Archna Bhatia, Gus-
tavo Cavallin, Brian Drexler, Matt Garley, Tania
Ionin, Matt Niemi, Dustin Parr, and Chris Struven.
And last, but not least we like to thank the reviewers
for their useful comments.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929234567901">
K. Barker and S. Szpakowicz. 1998. Semi-automatic recogni-
tion of noun modifier relationships. In the Proceedings of
the Association for Computational Linguistics / Conference
on Computational Linguistics.
M. Berland and E. Charniak. 1999. Finding Parts in Very Large
Corpora. In the Proceedings of the Association for Compu-
tational Linguistics (ACL), University of Maryland.
E. Charniak. 2000. A Maximum-entropy-inspired Parser. In
the Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL), Seattle,
Washington.
P. Downing. 1977. On the Creation and Use of English Com-
pound Nouns. Language, 53(4):810–842.
T. W. Finin. 1980. The Semantic Interpretation of Compound
Nominals. Ph.D. thesis, University of Illinois at Urbana-
Champaign.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479–496.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic
discovery of part-whole relations. Computational Linguis-
tics, 32(1).
R. Girju. 2007. Improving the interpretation of noun phrases
with cross-linguistic information. In the Proceedings of the
Association for Computational Linguistics (ACL), Prague.
O. Jespersen. 1954. A Modern English Grammar on Historical
Principles. London.
S. N. Kim and T. Baldwin. 2006. In the Proceedings of the As-
sociation for Computational Linguistics, Sydney, Australia.
M. Lapata and F. Keller. 2004. The Web as a baseline: Evaluat-
ing the performance of unsupervised Web-based models for
a range of NLP tasks. In the Proceedings of the Human Lan-
guage Technology Conference /North American Chapter of
the Association of Computational Linguistics (HLT-NAACL).
M. Lauer. 1995. Corpus statistics meet the noun compound:
Some empirical results. In the Proceedings of Association
for Computational Linguistics (ACL), Cambridge, Mass.
J. Levi. 1978. The Syntax and Semantics of Complex Nominals.
Academic Press, New York.
D. Moldovan and R. Girju. 2003. Knowledge discovery from
text. In the Tutorial Proceedings of the Association for Com-
putational Linguistics (ACL), Sapporo, Japan.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics, Boston, MA.
P. Nakov and M. Hearst. 2005. Search engine statistics be-
yond the n-gram: Application to noun compo und bracket-
ing. In the Proceedings of the Computational Natural Lan-
guage Learning Conference.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leverag-
ing generic patterns for automatically harvesting semantic
relations. In the Proceedings of the International Confer-
ence for Computational Linguistics (COLING/ACL), Syd-
ney, Australia.
M. Pennacchiotti and P. Pantel. 2006. Ontologizing semantic
relations. In the Proceedings of Conference on Computa-
tional Linguistics /Association for Computational Linguis-
tics (COLING/ACL-06), Sydney, Australia. Association for
Computational Linguistics.
B. Rosario and M. Hearst. 2001. Classifying the semantic re-
lations in noun compounds. In the Proceedings of the 2001
EMNLP Conference.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The descent of
hierarchy, and selection in relational semantics. In the Pro-
ceedings of the Association for Computational Linguistics.
E. Selkirk. 1982. Syntax of words. In Linguistic Inquiry Mono-
graph. MIT Press.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy
induction from heterogenous evidence. In the Proceedings
of the Conference on Computational Linguistics / Associa-
tion for Computational Linguistics (COLING-ACL), Sydney,
Australia.
P. Turney. 2006. Expressing implicit semantic relations with-
out supervision. In the Proceedings of the Conference on
Computational Linguistics /Association for Computational
Linguistics (COLING/ACL), Sydney, Australia.
A. Tyler and V. Evans. 2003. Spatial Experience, Lexical Struc-
ture and Motivation: The Case of In. In G. Radden and K.
Panther. Studies in Linguistic Motivation. Berlin and New
York: Mouton de Gruyter.
</reference>
<page confidence="0.998722">
175
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935396">
<title confidence="0.995392">Experiments with an Annotation Scheme for a Knowledge-rich Noun Interpretation System</title>
<author confidence="0.999922">Roxana Girju</author>
<affiliation confidence="0.997538">University of Illinois at</affiliation>
<email confidence="0.998276">girju@uiuc.edu</email>
<abstract confidence="0.996448947368421">This paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system. The system relies on cross-linguistic evidence from a set of five Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the five Romance languages, our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl and CLUVI. The training data was annotated with contextual features based on two stateof-the-art classification tag sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Barker</author>
<author>S Szpakowicz</author>
</authors>
<title>Semi-automatic recognition of noun modifier relationships.</title>
<date>1998</date>
<booktitle>In the Proceedings of the Association for Computational Linguistics / Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="10321" citStr="Barker and Szpakowicz, 1998" startWordPosition="1613" endWordPosition="1616">Finin, 1980), for example, uses semantic categories such as “dissolved in” to build interpretations of compounds such as “salt water” and “sugar water”. In this research we experiment with two sets of semantic classification categories defined at different abstraction levels. The first is a core set of 22 semantic relations (22 SRs), set which was identified by us from the linguistics literature and from various experiments after many iterations over a period of time (Moldovan and Girju, 2003)4. We proved 4There are also other lists of semantic relations used by the research community (e.g., (Barker and Szpakowicz, 1998)), but empirically that this set is encoded by noun – noun pairs in noun phrases and is a subset of our larger list of 35 semantic relations. This list, presented in Table 1 along with examples and semantic argument frames, is general enough to cover a large majority of text semantics while keeping the semantic relations to a manageable number. A semantic argument frame is defined for each semantic relation and indicates the position of each semantic argument in the underlying relation. For example, “Arg1 is part of (whole) Arg2” identifies the part (Arg1) and the whole (Arg2) entities of this</context>
</contexts>
<marker>Barker, Szpakowicz, 1998</marker>
<rawString>K. Barker and S. Szpakowicz. 1998. Semi-automatic recognition of noun modifier relationships. In the Proceedings of the Association for Computational Linguistics / Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding Parts in Very Large Corpora.</title>
<date>1999</date>
<booktitle>In the Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<institution>University of Maryland.</institution>
<contexts>
<context position="2731" citStr="Berland and Charniak, 1999" startWordPosition="416" endWordPosition="419"> is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annot</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding Parts in Very Large Corpora. In the Proceedings of the Association for Computational Linguistics (ACL), University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A Maximum-entropy-inspired Parser.</title>
<date>2000</date>
<booktitle>In the Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="15464" citStr="Charniak, 2000" startWordPosition="2411" endWordPosition="2412">the European Union covering the proceedings of the European Parliament from 1996 to 2001. The corpus was assembled by combining four of the bilingual sentence-aligned corpora made public as part of the freely available Europarl corpus. Specifically, the Spanish-English, Italian-English, FrenchEnglish and Portuguese-English corpora were automatically aligned based on exact matches of English translations. Then, only those English sentences which appeared verbatim in all four language pairs were considered. The resulting English corpus contained 10,000 sentences which were syntactically parsed (Charniak, 2000). From these we extracted the first 3,000 NP instances (N N: 48.82% and N P N: 51.18%). The CLUVI text collection CLUVI (Linguistic Corpus of the University of Vigo) is an open text repository of parallel corpora of contemporary oral and written languages, resource that besides Galician also contains literary text collections in other Romance languages. We focused only on the English-Portuguese and EnglishSpanish literary parallel texts from the works of John Steinbeck, H. G. Wells, J. Salinger, among others. Using the CLUVI search interface we created a sentence-aligned parallel corpus of 2,8</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A Maximum-entropy-inspired Parser. In the Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Downing</author>
</authors>
<title>On the Creation and Use</title>
<date>1977</date>
<journal>of English Compound Nouns. Language,</journal>
<volume>53</volume>
<issue>4</issue>
<contexts>
<context position="8244" citStr="Downing, 1977" startWordPosition="1266" endWordPosition="1267"> they are of limited semantic categories, such as TYPE (e.g., legge quadro (Italian) – framework law). Moreover, while English N N compounds are right-headed (e.g., framework/modifier law/head), Romance compounds are left-headed (e.g., legge/head quadro/modifier). For this research we focus only on English– Romance compositional noun phrases of the type N N and N P N and disregard metaphorical and 169 proper names. In the following section we present two different state-of-the-art classification sets used in NP interpretation. 3 Lists of semantic classification relations Although researchers (Downing, 1977), (Jespersen, 1954) argued that noun compounds, and NPs in general, encode an infinite set of semantic relations, many agree (Finin, 1980), (Levi, 1978) there is a limited number of relations that occur with high frequency in these constructions. However, the number and the level of abstraction of these frequently used semantic categories are not agreed upon. They can vary from a few prepositions (Lauer, 1995) to hundreds and even thousands more specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation a</context>
</contexts>
<marker>Downing, 1977</marker>
<rawString>P. Downing. 1977. On the Creation and Use of English Compound Nouns. Language, 53(4):810–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W Finin</author>
</authors>
<title>The Semantic Interpretation of Compound Nominals.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at UrbanaChampaign.</institution>
<contexts>
<context position="8382" citStr="Finin, 1980" startWordPosition="1288" endWordPosition="1289"> are right-headed (e.g., framework/modifier law/head), Romance compounds are left-headed (e.g., legge/head quadro/modifier). For this research we focus only on English– Romance compositional noun phrases of the type N N and N P N and disregard metaphorical and 169 proper names. In the following section we present two different state-of-the-art classification sets used in NP interpretation. 3 Lists of semantic classification relations Although researchers (Downing, 1977), (Jespersen, 1954) argued that noun compounds, and NPs in general, encode an infinite set of semantic relations, many agree (Finin, 1980), (Levi, 1978) there is a limited number of relations that occur with high frequency in these constructions. However, the number and the level of abstraction of these frequently used semantic categories are not agreed upon. They can vary from a few prepositions (Lauer, 1995) to hundreds and even thousands more specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation as to which category a phrase should be assigned. Lauer (Lauer, 1995), for example, considers a set of eight prepositions as semantic class</context>
<context position="9705" citStr="Finin, 1980" startWordPosition="1514" endWordPosition="1515">t, about, and from. However, according to this classification, the noun compound love story, for instance, can be classified both as story of love and story about love. The main problem with these abstract categories is that much of the meaning of individual compounds is lost, and sometimes there is no way to decide whether a form is derived from one category or another. On the other hand, lists of very specific semantic relations are difficult to build as they usually contain a very large number of predicates, such as the list of all possible verbs that can link the noun constituents. Finin (Finin, 1980), for example, uses semantic categories such as “dissolved in” to build interpretations of compounds such as “salt water” and “sugar water”. In this research we experiment with two sets of semantic classification categories defined at different abstraction levels. The first is a core set of 22 semantic relations (22 SRs), set which was identified by us from the linguistics literature and from various experiments after many iterations over a period of time (Moldovan and Girju, 2003)4. We proved 4There are also other lists of semantic relations used by the research community (e.g., (Barker and S</context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>T. W. Finin. 1980. The Semantic Interpretation of Compound Nominals. Ph.D. thesis, University of Illinois at UrbanaChampaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>D Moldovan</author>
<author>M Tatu</author>
<author>D Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="2615" citStr="Girju et al., 2005" startWordPosition="397" endWordPosition="400">rpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Sha</context>
<context position="18561" citStr="Girju et al., 2005" startWordPosition="2924" endWordPosition="2927">ovided by the sentence and its translations as shown below. Semantic argument frame The annotators were also asked to identify the translation phrases, tag each instance with the corresponding semantic relation, and identify the semantic arguments Arg1 and Arg2 in the semantic argument frame of the corresponding relation. 5The annotators have extensive expertise in computational semantics and are fluent in at least two of the Romance languages considered for this task. 6For the purpose of this research we used WordNet 2.1. Thus, since the order of the semantic arguments in an NP is not fixed (Girju et al., 2005), the annotators were presented with the semantic argument frame for each of the 22 semantic relations and were asked to tag the NP instances accordingly. For example, in PART-WHOLE instances such as chair/Arg2 arm/Arg1 the part arm follows the whole chair, while in button/Arg1 shirt/Arg2 the order is reversed. Translation instances In the annotation process the annotators were asked to identify and use, if necessary, the five corresponding translations as additional information in selecting the semantic relation. Since only N N and N P N noun phrase constructions were considered, the annotato</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19(4):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="2636" citStr="Girju et al., 2006" startWordPosition="401" endWordPosition="404">a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic </context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Improving the interpretation of noun phrases with cross-linguistic information.</title>
<date>2007</date>
<booktitle>In the Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<location>Prague.</location>
<contexts>
<context position="4384" citStr="Girju, 2007" startWordPosition="664" endWordPosition="665">as accurate as possible, many of such features are manually identified and annotated. Thus, the annotation process is an important task that requires not only considerable amount of time, but also experience with various annotation schemas and tools, and a good understanding of the research topic. Moreover, the extension of the noun phrase interpretation task to other natural languages brings forward new annotation issues. This paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system (Girju, 2007). The system relies on cross-linguistic evidence from a set of five Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the five Romance languages, our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl2 and CLUVI3. The training data was annotated with contextual features based on two state-ofthe-art classification tag se</context>
<context position="12159" citStr="Girju, 2007" startWordPosition="1922" endWordPosition="1923">noun–noun compounds. We selected these two state-of-the-art sets as they are of different size and contain semantic classification categories at different levels of abstraction. Lauer’s list is more abstract and, thus capable of encoding a large number of noun compound instances found in a corpus, while our list contains finer grained semantic categories. Details about the coverage of these semantic lists on the two different corpora (Europarl and CLUVI), how well they solve the interpretation problem of noun phrases, and the mapping from one list to another are provided in a companion paper (Girju, 2007). 4 The data For a better understanding of the semantic relations encoded by N N and N P N instances, we analyzed the semantic behavior of these constructions on a large cross-linguistic corpora of examples. Our intention is to answer questions such as: (1) What syntactic constructions are used to translate the English instances to the target Rothey overlap considerably with our list of 22-SR. 170 No. Semantic Default argument frame Examples Relations 1 POSSESSION Arg1 POSSESSES Arg2 family#2/Arg1 estate#2/Arg2 2 KINSHIP Arg1 IS IN KINSHIP REL. WITH Arg2 the boy#1/Arg1’s sister#1/Arg2 3 PROPER</context>
<context position="22876" citStr="Girju, 2007" startWordPosition="3644" endWordPosition="3645">a particular context. One such instance is cup#2 of hot chocolate#1 in example (6) below, which was tagged in CLUVI as MEASURE/OTHER(CONTENTCONTAINER)/LOC. Sense #2 of cup in WordNet refers to “the quantity the cup will hold” (cf. WordNet 2.1), thus mostly indicating a MEASURE relation. (5) 557-AGU: “Wouldn’t you like a cup of hot chocolate before you go?” (En.) However, since most hot beverages (such as tea, coffee, and chocolate) are served in cups, it stands to reason that the instance can be easily paraphrased as a cup holding hold chocolate. Although our current NP interpretation system (Girju, 2007) does not differentiate between LOCATION and CONTENTCONTAINER (as other researchers (Tyler and Evans, 2003)8, we consider CONTENT-CONTAINER as a special type of LOCATION), we capture them in our annotation scheme. Other examples of multiple annotations are MEASURE/PART-WHOLE (e.g., an abundance of buildings, a bunch ofguys), Overall, 0.5% Europarl and 6.9% CLUVI instances were tagged with more than one semantic relation, and almost all noun compound instances were tagged with more than one preposition. Thus, the annotated instances used in the corpus analysis and system training phases have th</context>
<context position="26881" citStr="Girju, 2007" startWordPosition="4303" endWordPosition="4304"> initial corpus we considered those English instances that had all the translations encoded by N N and N P N. Out of these, we selected only 1,023 Europarl and 1,008 CLUVI instances encoded by N N and N P N in all languages considered and resulted after agreement10. We split the corpora us9Only 5.70% of the TYPE instances in the Europarl corpus were unique. 10The annotated corpora resulted in this research are available at http://apfel.ai.uiuc.edu. ing a 8:2 training - test ratio and used it to train and test our system. Details about the experiments and the results obtained are presented in (Girju, 2007). 5 Discussion and conclusions In this paper we presented some observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system. These observations are defined in the framework of a larger project. This project is to investigate various linguistic issues and develop specific language models for the interpretation of noun phrase constructions in Germanic, Romance, and other classes of languages. Our approach to NP interpretation, and thus annotation procedure, is novel in several ways. We define the problem i</context>
</contexts>
<marker>Girju, 2007</marker>
<rawString>R. Girju. 2007. Improving the interpretation of noun phrases with cross-linguistic information. In the Proceedings of the Association for Computational Linguistics (ACL), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Jespersen</author>
</authors>
<title>A Modern English Grammar on Historical Principles.</title>
<date>1954</date>
<location>London.</location>
<contexts>
<context position="8263" citStr="Jespersen, 1954" startWordPosition="1268" endWordPosition="1269">ted semantic categories, such as TYPE (e.g., legge quadro (Italian) – framework law). Moreover, while English N N compounds are right-headed (e.g., framework/modifier law/head), Romance compounds are left-headed (e.g., legge/head quadro/modifier). For this research we focus only on English– Romance compositional noun phrases of the type N N and N P N and disregard metaphorical and 169 proper names. In the following section we present two different state-of-the-art classification sets used in NP interpretation. 3 Lists of semantic classification relations Although researchers (Downing, 1977), (Jespersen, 1954) argued that noun compounds, and NPs in general, encode an infinite set of semantic relations, many agree (Finin, 1980), (Levi, 1978) there is a limited number of relations that occur with high frequency in these constructions. However, the number and the level of abstraction of these frequently used semantic categories are not agreed upon. They can vary from a few prepositions (Lauer, 1995) to hundreds and even thousands more specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation as to which category</context>
</contexts>
<marker>Jespersen, 1954</marker>
<rawString>O. Jespersen. 1954. A Modern English Grammar on Historical Principles. London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S N Kim</author>
<author>T Baldwin</author>
</authors>
<date>2006</date>
<booktitle>In the Proceedings of the Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2573" citStr="Kim and Baldwin, 2006" startWordPosition="389" endWordPosition="392">ly; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Know</context>
</contexts>
<marker>Kim, Baldwin, 2006</marker>
<rawString>S. N. Kim and T. Baldwin. 2006. In the Proceedings of the Association for Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.</title>
<date>2004</date>
<booktitle>In the Proceedings of the Human Language Technology Conference /North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="2758" citStr="Lapata and Keller, 2004" startWordPosition="420" endWordPosition="424">constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annotation Workshop, pages 168–1</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>M. Lapata and F. Keller. 2004. The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks. In the Proceedings of the Human Language Technology Conference /North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: Some empirical results.</title>
<date>1995</date>
<booktitle>In the Proceedings of Association for Computational Linguistics (ACL),</booktitle>
<location>Cambridge, Mass.</location>
<contexts>
<context position="5031" citStr="Lauer, 1995" startWordPosition="768" endWordPosition="769">stic evidence from a set of five Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the five Romance languages, our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl2 and CLUVI3. The training data was annotated with contextual features based on two state-ofthe-art classification tag sets: Lauer’s set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. The system achieved an accuracy of 77.9% (Europarl) and 74.31% (CLUVI). The paper is organized as follows. Section 2 presents a summary of linguistic considerations of noun phrases. In Section 3 we describe the list of semantic interpretation categories used along with observations regarding their distribution on the two dif2http://www.isi.edu/koehn/europarl/ This corpus contains over 20 million words in eleven official languages of the European Union covering the proceedings of the European Parliament from 1996 to 2001. 3CLUVI - Linguistic Corpus of the</context>
<context position="8657" citStr="Lauer, 1995" startWordPosition="1334" endWordPosition="1335">es. In the following section we present two different state-of-the-art classification sets used in NP interpretation. 3 Lists of semantic classification relations Although researchers (Downing, 1977), (Jespersen, 1954) argued that noun compounds, and NPs in general, encode an infinite set of semantic relations, many agree (Finin, 1980), (Levi, 1978) there is a limited number of relations that occur with high frequency in these constructions. However, the number and the level of abstraction of these frequently used semantic categories are not agreed upon. They can vary from a few prepositions (Lauer, 1995) to hundreds and even thousands more specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation as to which category a phrase should be assigned. Lauer (Lauer, 1995), for example, considers a set of eight prepositions as semantic classification categories that can link the head and the modifier nouns in a noun compound: of, for, with, in, on, at, about, and from. However, according to this classification, the noun compound love story, for instance, can be classified both as story of love and story about l</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>M. Lauer. 1995. Corpus statistics meet the noun compound: Some empirical results. In the Proceedings of Association for Computational Linguistics (ACL), Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="7006" citStr="Levi, 1978" startWordPosition="1073" endWordPosition="1074">el, but they are also used in the annotation process. Noun phrases can be compositional when their meaning is derived from the meaning of the constituent nouns (e.g., door knob – PART-WHOLE, kiss in the morning – TEMPORAL), or idiosyncratic, when the meaning is a matter of convention (e.g., soap opera, sea lion). NPs can also express metaphorical names (eg, ladyfinger), proper names (e.g., John Doe), and binomial (dvandva) compounds in which neither noun is the head (e.g., player-coach). NPs can also be classified into synthetic (verbal) and root (non-verbal) constructions. It is widely held (Levi, 1978), (Selkirk, 1982) that the modifier noun of a synthetic noun compound, for example, may be associated with a theta-role of the verbal head. For instance, in truck driver, the noun truck satisfies the THEME relation associated with the direct object in the corresponding argument structure of the verb to drive. Studied cross-linguistically, noun phrases can express variations from one language to another. For example, English compounds of the form N1 N2 (e.g., wood stove) usually translate in Romance languages as N2 P N1 (e.g., four a´ bois (French) – stove at/to wood). Romance languages have ve</context>
<context position="8396" citStr="Levi, 1978" startWordPosition="1290" endWordPosition="1291">ed (e.g., framework/modifier law/head), Romance compounds are left-headed (e.g., legge/head quadro/modifier). For this research we focus only on English– Romance compositional noun phrases of the type N N and N P N and disregard metaphorical and 169 proper names. In the following section we present two different state-of-the-art classification sets used in NP interpretation. 3 Lists of semantic classification relations Although researchers (Downing, 1977), (Jespersen, 1954) argued that noun compounds, and NPs in general, encode an infinite set of semantic relations, many agree (Finin, 1980), (Levi, 1978) there is a limited number of relations that occur with high frequency in these constructions. However, the number and the level of abstraction of these frequently used semantic categories are not agreed upon. They can vary from a few prepositions (Lauer, 1995) to hundreds and even thousands more specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation as to which category a phrase should be assigned. Lauer (Lauer, 1995), for example, considers a set of eight prepositions as semantic classification cate</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>J. Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>R Girju</author>
</authors>
<title>Knowledge discovery from text.</title>
<date>2003</date>
<booktitle>In the Tutorial Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="10191" citStr="Moldovan and Girju, 2003" startWordPosition="1593" endWordPosition="1596"> contain a very large number of predicates, such as the list of all possible verbs that can link the noun constituents. Finin (Finin, 1980), for example, uses semantic categories such as “dissolved in” to build interpretations of compounds such as “salt water” and “sugar water”. In this research we experiment with two sets of semantic classification categories defined at different abstraction levels. The first is a core set of 22 semantic relations (22 SRs), set which was identified by us from the linguistics literature and from various experiments after many iterations over a period of time (Moldovan and Girju, 2003)4. We proved 4There are also other lists of semantic relations used by the research community (e.g., (Barker and Szpakowicz, 1998)), but empirically that this set is encoded by noun – noun pairs in noun phrases and is a subset of our larger list of 35 semantic relations. This list, presented in Table 1 along with examples and semantic argument frames, is general enough to cover a large majority of text semantics while keeping the semantic relations to a manageable number. A semantic argument frame is defined for each semantic relation and indicates the position of each semantic argument in the</context>
</contexts>
<marker>Moldovan, Girju, 2003</marker>
<rawString>D. Moldovan and R. Girju. 2003. Knowledge discovery from text. In the Tutorial Proceedings of the Association for Computational Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Badulescu</author>
<author>M Tatu</author>
<author>D Antohe</author>
<author>R Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>In the Proceedings of the HLT/NAACL Workshop on Computational Lexical Semantics,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2480" citStr="Moldovan et al., 2004" startWordPosition="377" endWordPosition="380">. For example, a compound family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING</context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju. 2004. Models for the semantic classification of noun phrases. In the Proceedings of the HLT/NAACL Workshop on Computational Lexical Semantics, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>M Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compo und bracketing.</title>
<date>2005</date>
<booktitle>In the Proceedings of the Computational Natural Language Learning Conference.</booktitle>
<contexts>
<context position="2784" citStr="Nakov and Hearst, 2005" startWordPosition="425" endWordPosition="428">e time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annotation Workshop, pages 168–175, Prague, June 2007. c�2</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>P. Nakov and M. Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compo und bracketing. In the Proceedings of the Computational Natural Language Learning Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In the Proceedings of the International Conference for Computational Linguistics (COLING/ACL),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2514" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="381" endWordPosition="384"> family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computation</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In the Proceedings of the International Conference for Computational Linguistics (COLING/ACL), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pennacchiotti</author>
<author>P Pantel</author>
</authors>
<title>Ontologizing semantic relations.</title>
<date>2006</date>
<booktitle>In the Proceedings of Conference on Computational Linguistics /Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="2548" citStr="Pennacchiotti and Pantel, 2006" startWordPosition="385" endWordPosition="388">ed as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at A</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2006</marker>
<rawString>M. Pennacchiotti and P. Pantel. 2006. Ontologizing semantic relations. In the Proceedings of Conference on Computational Linguistics /Association for Computational Linguistics (COLING/ACL-06), Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds.</title>
<date>2001</date>
<booktitle>In the Proceedings of the 2001 EMNLP Conference.</booktitle>
<contexts>
<context position="2431" citStr="Rosario and Hearst, 2001" startWordPosition="369" endWordPosition="372">e the semantic relationship between the two concepts. For example, a compound family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics commu</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>B. Rosario and M. Hearst. 2001. Classifying the semantic relations in noun compounds. In the Proceedings of the 2001 EMNLP Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
<author>C Fillmore</author>
</authors>
<title>The descent of hierarchy, and selection in relational semantics.</title>
<date>2002</date>
<booktitle>In the Proceedings of the Association for Computational Linguistics. E. Selkirk.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2455" citStr="Rosario et al., 2002" startWordPosition="373" endWordPosition="376">between the two concepts. For example, a compound family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiw</context>
</contexts>
<marker>Rosario, Hearst, Fillmore, 2002</marker>
<rawString>B. Rosario, M. Hearst, and C. Fillmore. 2002. The descent of hierarchy, and selection in relational semantics. In the Proceedings of the Association for Computational Linguistics. E. Selkirk. 1982. Syntax of words. In Linguistic Inquiry Monograph. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In the Proceedings of the Conference on Computational Linguistics / Association for Computational Linguistics (COLING-ACL),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2594" citStr="Snow et al., 2006" startWordPosition="393" endWordPosition="396">f silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In the Proceedings of the Conference on Computational Linguistics / Association for Computational Linguistics (COLING-ACL), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<booktitle>In the Proceedings of the Conference on Computational Linguistics /Association for Computational Linguistics (COLING/ACL),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2800" citStr="Turney, 2006" startWordPosition="429" endWordPosition="430">it. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annotation Workshop, pages 168–175, Prague, June 2007. c�2007 Association </context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>P. Turney. 2006. Expressing implicit semantic relations without supervision. In the Proceedings of the Conference on Computational Linguistics /Association for Computational Linguistics (COLING/ACL), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tyler</author>
<author>V Evans</author>
</authors>
<title>Spatial Experience, Lexical Structure and Motivation: The Case of In. In</title>
<date>2003</date>
<location>Berlin and New York: Mouton</location>
<note>de Gruyter.</note>
<contexts>
<context position="22983" citStr="Tyler and Evans, 2003" startWordPosition="3657" endWordPosition="3660">as tagged in CLUVI as MEASURE/OTHER(CONTENTCONTAINER)/LOC. Sense #2 of cup in WordNet refers to “the quantity the cup will hold” (cf. WordNet 2.1), thus mostly indicating a MEASURE relation. (5) 557-AGU: “Wouldn’t you like a cup of hot chocolate before you go?” (En.) However, since most hot beverages (such as tea, coffee, and chocolate) are served in cups, it stands to reason that the instance can be easily paraphrased as a cup holding hold chocolate. Although our current NP interpretation system (Girju, 2007) does not differentiate between LOCATION and CONTENTCONTAINER (as other researchers (Tyler and Evans, 2003)8, we consider CONTENT-CONTAINER as a special type of LOCATION), we capture them in our annotation scheme. Other examples of multiple annotations are MEASURE/PART-WHOLE (e.g., an abundance of buildings, a bunch ofguys), Overall, 0.5% Europarl and 6.9% CLUVI instances were tagged with more than one semantic relation, and almost all noun compound instances were tagged with more than one preposition. Thus, the annotated instances used in the corpus analysis and system training phases have the following format: &lt;NPEn ;NPE3; NPzt; NPFr; NPPort; NPRo; target&gt;. The word target is one of the 23 (22 + </context>
<context position="24359" citStr="Tyler and Evans, 2003" startWordPosition="3871" endWordPosition="3874">a; avis sur la pr´esidence; giudizio sulla Presidenza; veredicto sobre a Presidˆencia; evaluarea Pres¸edinjiei; THEME&gt;. 4.2 Inter-annotator agreement The annotators’ agreement was measured using Kappa statistics, one of the most frequently used measure of inter-annotator agreement for classification tasks: K = Pr(a)−Pr(E) 1−Pr(E) , where Pr(A) is the proportion of times the annotators agree and Pr(E) is the probability of agreement by chance. The K coefficient is 1 if there is a total agreement among the annotators, and 0 if there is no agreement other than that expected to occur by chance. 8(Tyler and Evans, 2003) cite child language acquisition studies which show there is a strong cognitive relationship between LOCATION and CONTENT-CONTAINER. p 173 The Kappa values obtained on each corpus are shown in Table 2. We also computed the number of pairs that were tagged with OTHER by both annotators for each semantic relation and preposition paraphrase, over the number of examples classified in that category by at least one of the judges. For the noun compound instances that encoded more than one classification category, the agreement was done on one of the relations only. The agreement obtained for the Euro</context>
</contexts>
<marker>Tyler, Evans, 2003</marker>
<rawString>A. Tyler and V. Evans. 2003. Spatial Experience, Lexical Structure and Motivation: The Case of In. In G. Radden and K. Panther. Studies in Linguistic Motivation. Berlin and New York: Mouton de Gruyter.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>