<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9092745">
COMPILING TRACE 8z UNIFICATION GRAMMAR FOR PARSING AND
GENERATION
</note>
<address confidence="0.247788">
Hans Ulrich Block
Siemens AG, Corporate Research, ZFE IS INF 23
Otto Hahn-Ring 6
D-8000 Munchen 83
Germany
</address>
<email confidence="0.459925">
blockaztivax.uucp
</email>
<sectionHeader confidence="0.981268" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999871444444444">
This paper presents Trace &amp; Unification Gram-
mar (TUG), a declarative and reversible grammar
formalism that brings together Unification Gram-
mar (UG) and ideas of Government &amp; Binding
Theory (GB) in an undogmatic way. A grammar
compiler is presented that transforms a grammar
written in the TUG formalism into two different
forms, one being useful for parsing, the other be-
ing useful for generation.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.99994685483871">
During the last years there has been a growing
interest in NL systems that can be used for both
parsing and generation. The invention of unifica-
tion grammar that allows for a declarative descrip-
tion of language made it possible to use the same
grammar for both tasks. The main goal of a gram-
mar then is to describe a relation between nor-
malized (semantic) representations and language
strings. A grammar that can be used in both di-
rections is called &amp;quot;reversible&amp;quot;.
We can distinguish three levels of reversibility.
On the first level, not only the same grammar is
used for parsing and generation, but also the in-
terpreter for parsing and generation is reversible.
This approach is taken in Shieber (1988). Besides
elegance the approach has the advantage that the
reversibility is guaranteed. Rather advantages are
mentioned in Neumann (1991). As a disadvantage,
it is yet unclear whether and how these systems
can be made efficient.
On the second level we find systems where the
same reversible grammar is processed by two dif-
ferent interpreters, one for parsing, one for gener-
ation. The advantage of these systems is that the
grammar can be changed and tested easily, which
helps to shorten the development cycle. The dis-
advantage again is that grammar interpreters are
usually too slow to be used in realistic systems.
On the third level we finally find systems, where
the linguistic description is given in a reversible
declarative grammar. This grammar is then com-
piled into two different forms, one being usefull
only for parsing, the other only for generation.
Whereas here we have to face the disadvantage
that compiling can take some time and therefore
prolongs the development cycle, the advantage lies
in the efficient processing that can be achieved
with compiled grammars. Strzalkowski (1990) and
Strzalkowski/Peng (1990) describe a compiler that
transforms a grammar originally written for pars-
ing into an efficient generator.
In the follwing section I will present a system of
the third type and show by means of which com-
piling methods a grammar written in a perspicious
formalism, TRACE AND UNIFICATION GRAMMAR
(TUG) can be transformed to fast parsers and gen-
erators. The proposed compilers and their mod-
ular architecture have the further advantage that
most of their parts can be used also for other for-
malisms than the one described, e.g. DCGS.
The whole system is part of a polyfunctional
linguistic processor for German called LINGUISTIC
KERNEL PROCESSOR (LKP). The LKP contains a
grammar of German with broad coverage. The
grammar describes the relation between a subset
of German and a subset of QLF, the intermedi-
ate semantic form that is used in the Core Lan-
guage Engine of SRI Cambridge (Alshawi 1990).
The LKP has been implemented in PROLOG. Pars-
ing and Generation of a sentence up to 15 words
normally takes between 1 and 10 seconds, with a
strong tendency to the lower bound.
</bodyText>
<page confidence="0.978702">
100
</page>
<note confidence="0.721177333333333">
2 FORMALISM np =&gt; f(agr:agrmnt).
vp =&gt; f(agr:agrmnt).
The design of Trace and Unification Grammar =&gt; f(agr:agrmnt).
</note>
<bodyText confidence="0.875004">
has been guided by the following goals:
</bodyText>
<listItem confidence="0.969286">
agrmnt =&gt; f(number:number,person:person).
• Perspicuity. We are convinced that the gen-
erality, coverage, reliability and development
speed of a grammar are a direct function of
its perspicuiiy, just as programming in Pas-
cal is less errorprone than programming in as-
sembler. In the optimal case, the grammar
writer should be freed of reflections on how
to code things best for processing but should
only be guided by linguistic criteria. These
goals led for .example to the introduction of
unrestricted disjunction into the TUG formal-
ism.
• Compatibility to GB Theory. It was a ma-
jor objective Of the LKP to base the grammar
on well understood and motivated grounds.
As most of the newer linguistic descriptions
on German are in the framework of GB theory,
TUG was designed to be somehow compatible
with this theory though it was not our goal
to &amp;quot;hardwire&amp;quot; every GB principle.
• Efficiency. &apos;As the LKP is supposed to be
</listItem>
<bodyText confidence="0.8167518">
the basis of products for interactive usage of
natural language, efficiency is a very impor-
tant goal. Making efficiency a design goal of
the formalism led e.g. to the introduction of
feature types and the separation of the move-
ment rules into head movement and argument
movement.
The basis of TUG is formed by a context free
grammar that is augmented by PATR n-style fea-
ture equations. Besides this basis, the main
features of TUG are feature typing, mixing of
attribute-value-pair and (PaoLoo-) term unifica-
tion, flexible macros, unrestricted disjunction and
special rule types &apos; for argument and head move-
ment.
</bodyText>
<subsectionHeader confidence="0.967134">
2.1 BASIC FEATURES
</subsectionHeader>
<bodyText confidence="0.929615">
As a very simple example we will look at the
TUG version of the example grammar in Shieber
(1984).
%.type definition
</bodyText>
<listItem confidence="0.852716">
• =&gt;t.
</listItem>
<equation confidence="0.961418">
number =&gt; {singular ,plural}.
person =&gt; {first,second,third}.
% rules
S ---&gt; fly, vp I
np:agr = vp:agr.
vp ---&gt; v, up I
vp:agr = v:agr.
% lexicon
lexicon(&apos;Uther&apos;,np) I
agr:number = singular,
agr:person = third.
lexicon(&apos;Arthur&apos;,np) I
agr:number = singular,
agr:person = third.
lexicon(knights,v) I
agr:number = singular,
agr:person = third.
lexicon(knight,v) I
( agr:number = singular,
( agr:person = first
; agr:person = second
agr:number = plural
) .
</equation>
<bodyText confidence="0.999122411764706">
The two main differences to PATR II in the ba-
sic framwork are that first, TUG is less flexible in
that it has a &amp;quot;hard&amp;quot; contextfree backbone, whereas
in PATR II categories of the context free part are
placeholders for feature structures, their names
beeing taken as the value of the cat feature in
the structure. Second, TUG has a strict typing.
For a feature path to be well defined, each of its
attributes has to be declared in the type definition.
Besides defined attribute-value-pairs, TUG al-
lows for the mixing of attribute-value-pair unifica-
tion with arbitrary structures like PROLOG terms
using a back-quote notation. This can be re-
garded as the unificational variant of the BUILDQ
operation known from ATNS. As an example con-
sider the following lexicon entry of each that con-
structs a predicate logic notation out of det :base,
</bodyText>
<page confidence="0.997613">
101
</page>
<bodyText confidence="0.848676">
det :scope and det :var.
</bodyText>
<equation confidence="0.8863975">
lexicon(each,det) I
det:sem =
&apos;all(det:var,det:base -&gt;
dot: scope)
</equation>
<bodyText confidence="0.99693">
During our work on the German grammar we
found that this feature was very useful for the con-
struction of semantic forms.
TUG provides templates for a clearer organiza-
tion of the grammar. The agreement in the above
mentioned grammar might have been formulated
like this:
</bodyText>
<equation confidence="0.8885842">
agree(X,Y) short_for
X:agr = Y:agr.
• • •
s np, vp I
agree(np,vp).
</equation>
<bodyText confidence="0.999773">
TUG allows for arbitrary disjunction of feature
equations. Disjunctions and Conjunction may be
mixed freely. Besides well known cases as in the
entry for knight above, we found many cases where
disjunctions of path equations are useful, e.g. for
the description of the extraposed relative clauses&apos;.
</bodyText>
<subsectionHeader confidence="0.992226">
2.2 MOVEMENT RULES
</subsectionHeader>
<bodyText confidence="0.9999767">
Further to these more standard UG-features,
TUG provides special rule formats for the de-
scription of discontinuous dependencies, so called
&amp;quot;movement rules&amp;quot;. Two main types of movement
are distinguished: argument movement and head
movement. The format and processing of argu-
ment movement rules is greatly inspired by Chen
e. a. (1988) and Chen (1990), the processing of
head movement is based on GPSG like slash fea-
tures.
</bodyText>
<subsectionHeader confidence="0.995429">
Head Movement
</subsectionHeader>
<bodyText confidence="0.998604">
A head movement rule defines a relation be-
tween two positions in a parse tree, one is the land-
ing site, the other the trace position. Head move-
ment is constrained by the condition that the trace
is the head of a specified sister (the root node) of
1Block/Sclunid (1991) describes our processing tech-
nique for disjunctions.
the landing site2. Trace and Antecedent are iden-
tical with the exception that the landing site con-
tains overt material, the trace does&apos;nt. Suppose,
that v is the head of vk, vk the head of vp and vp
the head of a, then only the first of the following
structures is a correct head movement, the second
is excluded because up is not head of vp, the third
because antecedent and trace are unequal.
</bodyText>
<equation confidence="0.997353833333333">
Ca, vi Cs • • • [vp • • •
[irk • • • trace(v)i ..
[a, al); [a ... [vp trace(np)i
Cvk • • • • ..3 .. .3
[5, up; is • • • [vp • • •
[vk • • • trace(v)1 ...3 ... —3
</equation>
<bodyText confidence="0.965682">
To formulate head movement in TUG the follow-
ing format is used. First, a head definition defines
which category is the head of which other.
v is_head_of
vk is_head_of vp.
vp is_head_of s.
Second, the landing site is defined by a rule like
</bodyText>
<equation confidence="0.578598">
s ---&gt; v+s 1 ...
</equation>
<bodyText confidence="0.91630368">
To include recursive rules in the head path,
heads are defined by the following head definitions.
In a structure Cm DI ... DO Di is the head of M
if either Di is_head_of M is defined or Di has the
same category as it and either Di is_head_of X or
X is_head_of Di is defined for any category X.
Head movement rules are very well suited for
a concise description of the positions of the finite
verb in German (sentence initial, second and final)
as in
HA der Mann der Frau das Buch gegeben ?
Has; the man the woman the book given ti
Der Mann hati der Frau dos Buch gegeben ij
The man has; the woman the book given ti
dafi der Mann der Frau dos Buch gegeben hat
... that the man the woman the book given has
All that is needed are the head definitions and
the rule that introduces the landing site3.
2 Here, &amp;quot;head of&amp;quot; is a transitive relation s.t. if x is head
of y and y is head of z then x is head of z.
3Even though verb movement is not supposed to be a
topic for English grammar, one might think of describing
English Subj-Aux inversion in terms of head movement.
Peter has been reading a book
Has, Peter ti been reading a book
</bodyText>
<page confidence="0.993543">
102
</page>
<subsectionHeader confidence="0.928558">
Argument Movement
</subsectionHeader>
<bodyText confidence="0.974035673469388">
Argument movement rules describe a relation
between a landing site and a trace. The trace is
always c-commanded by the landing site, its an-
tecedent. Two different traces are distinguished,
anaphoric traces and variable traces. Anaphoric
traces must find their antecedent within the same
bounding node, variable trace binding is con-
strained by subjacency, e.a. the binding of
the trace to its antecedent must not cross two
bounding nodes. Anaphoric traces are found
for example in English passive constructions
[s [up The book of this author]i was read ti]
whereas variable traces are usually found in wh-
constructions and topicalization. Similar to the
proposal in Chen e. a. (1988), argument movement
is coded in TUG by a rule that describes the land-
ing site, as for example in
s2 ---&gt; np:ante&lt;trace(var,np:trace), 81 I
ante:fx = trace:fx,
This rule states that np: ante4 is the antecedent
of an np-trace that is dominated by sl. This rule
describes a leftward movement. Following Chen&apos;s
proposal, TUG also provides for rightward move-
ment rules, though these are not needed in the
German grammar. A rightward movement rule
might look like this.
s2 ---&gt; si, trace(var,np:trace)&gt;np:ante
ante:fx = trace:fx,
The first argument in the trace-term indicates
whether the landing site is for a variable (var)
or for an anaphoric (ana) trace. Other than head
movement, where trace and antecedent are by def-
inition identical, the feature sharing of argument
traces with their antecedents has to be defined
in the grammar by feature equations (ante:ix =
trace: fx, ...). ,Furthermore, it is not necessary
that the antecedent and the trace have the same
syntactic category. A rule for pronoun fronting in
German might e.g. look like this:
spr ---&gt; pron&lt;trace(ana,np), s I ...
4The notation Cat :Index is used to distinguish two or
more occurrences of the same category in the same rule in
the equation part. :ante and :trace are arbitrary names
used as index to refer to the two different nps.
The current version of the formalisms requires
that the grammar contains a declaration on which
categories are possible traces. In such a declara-
tion it is possible to assign features to a trace, for
example marking it as empty:
</bodyText>
<equation confidence="0.773818">
trace(np) I np:empty = yes.
</equation>
<bodyText confidence="0.889302076923077">
Bounding nodes have to be declared as such in
the grammar by statements of the form
bounding_node(np).
bounding_node(s) I s:tense = yes.
As in the second case, bounding nodes may
be defined in terms of category symbols and
features5. Typical long distance movement phe-
nomena are described within this formalism as in
GB by trace hopping. Below is a grammar frag-
ment to describe the sentence Which booksi do you
think ti John knows ti Mary did&apos;nt understand 14:
bounding_node(s).
bounding_node(np).
</bodyText>
<table confidence="0.838871857142857">
si ---&gt; np&lt;trace(var,np), s I ...
$ ---&gt; np, vp I ...
s ---&gt; aux, np, vp I ...
np ---&gt; propernoun I
np ---&gt; det, n I
vp ---&gt; v, al 1 ...
vp ---&gt; v, np I ...
</table>
<bodyText confidence="0.9517835">
trace(np).
The main difference of argument movement to
other approaches for the description of discontinu-
ities like extraposition grammars (Pereira 1981)
is that argument movement is not restricted to
nested rule application. This makes the approach
especially atractive for a scrambling analysis of the
relative free word order in the German Mittelfeld
as in
Am; hati das Buchk keiner tj tk gegeben ti.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="method">
3 PROCESSING TRACE &amp;
UNIFICATION GRAMMAR
</sectionHeader>
<bodyText confidence="0.9963814">
TUG can be processed by a parser and a genera-
tor. Before parsing and generation, the grammar
is compiled to a more efficient form.
5Currently, only conjunction of equations is allowed in
the definition of bounding nodes.
</bodyText>
<page confidence="0.99731">
103
</page>
<bodyText confidence="0.9998515">
The first compilation step is common to gener-
ation and parsing. The attribute-value-pair struc-
ture is transformed to (PROLoG) term structure
by a TUG-to-DCG converter. This transformation
makes use of the type definitions. As an example
consider the transformation of the grammar
</bodyText>
<equation confidence="0.991400125">
a =&gt; f(al:t1).
=&gt; f(al:t1).
ti &gt; f(a2:t2,a3:t3).
t2 =&gt; (1,2).
t3 =&gt; (2,3).
a ---&gt; b I
a:a1:a3 = 2,
( a:a1:a2 = 1; a:a1 = b:a1 ).
</equation>
<bodyText confidence="0.945455">
It is transformed to the following grammar in a
DCG like format6.
</bodyText>
<equation confidence="0.947733">
a(t1(A,2))
[b(B), (A = 1 ; tl(A,2) =
</equation>
<bodyText confidence="0.999768">
The compilation steps following the TUG-to-DCG
converter are different for parsing and generation.
</bodyText>
<subsectionHeader confidence="0.957027">
3.1 THE PARSER GENERATOR
</subsectionHeader>
<bodyText confidence="0.99767175">
In the LKP, a TUG is processed by a Tomita
parser (Tomita 1986). For usage in that parser the
result of the TUG-to-DCG converter is compiled in
several steps:
</bodyText>
<listItem confidence="0.9998536">
• expansion of head movement rules
• transformation of argument movement rules
• elimination of empty productions
• conversion to LR(K) format
• computation of LR tables
</listItem>
<bodyText confidence="0.937058333333333">
First, head movement rules are eliminated and
the grammar is expanded by introducing slash
rules for the head path by the head movement ex-
pander. Suppose the TUG-to-DCG converter has
produced the following fragment:
°Note that the goal {A 1 ; t 1 (A .2) a. B) is inter-
preted as a constraint and not as a PROLOG goal as in
DCGs. See Block/Schmid (1991) for the evaluation of the
constraints.
</bodyText>
<figure confidence="0.932767">
v(_) is_head_of vk(_).
vk(_) is_head_of vp(J.
vp(_) is_head_of s(_).
s1(S1) ---&gt; [v(V) + s(S)].
s(S) ---&gt;
vp(VP) ---&gt;
vk(VK) ---&gt; [...,v(V),...].
</figure>
<bodyText confidence="0.664081333333333">
Then, the head movement expander introduces
slash rules7 along the head-path, thereby introduc-
ing the empty nonterminals push(X) and pop(X).
</bodyText>
<figure confidence="0.964374692307692">
% rules sofar
s(S) ---&gt;
vp(VP) ---&gt; [...,vk(VK),...].
vk(VK) ---&gt; [...,v(V),...].
% newly introduced slash rules
sl(S1) ---&gt; [v(V), push(v(V)), s_v(S)].
s_v(S) ---&gt; [...,vp_v(VP),...].
vp_v(VP) ---&gt; [...,vk_v(VK),...].
vk_v(VK) ---&gt; [...,v_v(V),...].
v_v(V)---&gt; [pop(v(V))].
% empty productions for push and pop
push(X) ---&gt; .
pop(X) ---&gt; .
</figure>
<bodyText confidence="0.993202095238095">
push(X) and pop(X) are &amp;quot;marker rules&amp;quot;
(Aho/Sethi/Ullman 1986) that invoke the parser
to push and pop their argument onto and off a
left-to-right stack. This treatment of head move-
ment leads to a twofold prediction in the Tomita
parser. First, the new slash categories will lead to
LR parsing tables that predict that the verb will be
missing if rule si ---&gt; ... has applied. Second,
the feature structure of the verb is transported to
the right on the left-to-right stack. Therefore, as
soon as a v_v is expected, the whole information
of the verb, e.g. its subcategorization frame, is
available. This strategy leads to a considerable
increase in parsing efficiency.
In the next compilation phase, argument move-
ment rules are transformed to the internal format.
For the control of gaps a gap-threadding mecha-
nism is introduced. Following Chen e.a. (1988),
the gap features are designed as multisets, thus al-
lowing crossing binding relations as mentioned in
section 2.
</bodyText>
<footnote confidence="0.8980765">
7A slashed category x/Y is represented using the under-
score character X_Y.
</footnote>
<page confidence="0.998727">
104
</page>
<bodyText confidence="0.996575333333333">
To see the effect of this compilation step, take
the following fragment as output of the head move-
ment expander.
</bodyText>
<figure confidence="0.640594428571429">
bounding_node(s(i).
sl(S1) np(NP)&lt;trace(var,np(Trace)),
s(S).
s(S) np(NP), vp(VP).
vp(VP) v(V).
vp(VP) v(V), np(1112).
trace(np(_)).
</figure>
<bodyText confidence="0.7624635">
The argument movement expander transforms
this to the following grammar.
</bodyText>
<figure confidence="0.973738818181818">
sl(Gi,Go,S1) —7&gt; np(Gi,Gt,NP),
s(Gs,Go,S),
(cut_trace (trace (var,np(Trace) ) ,
Gs,Gt)).
s(Gi,Go,S) np(Gi,Gt,NP),
vp(Gt,Go,VP),
{bound(Gi)}.
vp(Gi,Go,VP) v(Gi,Go,V).
vp(Gi,Go,VP) v(Gi,Gt,V),
np(Gt,Go,NP).
np( [trace(„np(NP)) I G] ,G,NP) 0 .
</figure>
<bodyText confidence="0.996475142857143">
The predicates cut_trace/3 and bound/1 are
defined as in Chen e.a. (1988).
The next step, the empty production eliminater,
eliminates all empty productions except those for
push and pop. This transforms the output of
the argument movement expander to the follow-
ing grammar.
</bodyText>
<figure confidence="0.909803214285714">
s1(Gi,Go,S1) np(Gi,Gt,NP),
s(Gs,Go,S),
(cut_trace (trace (var ,np(Trace) ) ,
Gs ,G01.
siatrace(„np(NP)) I Gt] ,Go,S1)
s (Gs , Go, S) ,
(cut_trace (trace (var ,np(Trace) ) ,
Gs ,G01.
s(Gi,Go,S) np(Gi,Gt,NP),
vp(Gt,Go,VP),
{bound(Gi)}.
satrace(„np(Np)) I Gt] ,Go,S)
vp(Gt,Go,VP),
{bound(Gi)}.
</figure>
<tableCaption confidence="0.75693">
vp(Gi,Go,VP) v(Gi,Go,V).
vp(Gi,Go,VP) --7&gt; v(Gi,Gt,V),
np(Gt,Go,NP).
vp(Gi,Go,VP) --=&gt;
v(Gi, [trace (_,np(NP) ) I Go] , V) .
</tableCaption>
<bodyText confidence="0.833444705882353">
Elimination of empty productions allows for
a simpler implementation of the Tomita parser,
which again leads to an increased efficiency.
The next step, the DCG-to-LRK converter splits
the grammar rules into a context free and
a DCG part. A context free rule is repre-
sented as rule (No ,LHS ,RHS) , a DCG rule as
dcg_rul e (No ,LHS ,RHS , Constraint ). Rules are
synchronized by their numbers. After this step
the above grammar fragment is represented in the
following format.
rule(1,s1,[np,s]).
rule(2,81, Cs]).
rule(3,s, [np,vp]).
rule(4,s, ).
rule(6,vp, [v]).
rule(6,vp,N,n0).
</bodyText>
<figure confidence="0.825773583333333">
dcg_rule(1,s1(Gi,Go,S1),
[np(Gi,Gt,NP) ,s(Gs,Go,$)3 ,
cut_trace(trace(var,np(Trace)) ,
Gs ,Gt))
dcg_rule(2, 81( [trace(_,np(NP)) I Gt] ,
Go,S1),
[s (Gs , Go , S)] ,
cut_trace (trace (var , np (Trace) )
Gs,Gt)).
dcg_rule(3,s(Gi,Go,S),
[np(Gi,Gt,NP),vp(Gt,Go,VP)],
bound(Gi)).
dcg_rule(4,s( [trace(_onp(NP))1Gt] ,
Go ,S) ,
[vp(Gt,Go,VP)]
bound(Gi)).
dcg_rule(6,vp(Gi,Go,VP),
[v(Gi,Gv,V)],
( Gv = Go
; Gv = [trace(_,np(NP)) I Go3
) ).
dcg_rule(6,vp(Gi,Go,VP),
[v(Gi,Gt,V),np(Gt,Go,NP)],
true).
</figure>
<bodyText confidence="0.9998488">
Note that during this step, different rules that
share the same context free backbone are trans-
formed to a single context free rule. The differ-
ence in their feature structure is expressed in a
disjunction in the Constraint (e.g. rule 5). As
very often traces occur in optional positions (e.g.
objects, as in vp ---&gt; v. vp ---&gt; v, np), the
elimination of empty productions (traces) consid-
erably reduces the amount of edges the parser has
to build.
</bodyText>
<page confidence="0.998357">
105
</page>
<bodyText confidence="0.99704225">
After these compilation steps the context free
rules are transformed to YACC format and YAcC
is used to compute the LR parsing table. Finally,
YAcC&apos;s y . output file is transformed to PROLOG.
</bodyText>
<subsectionHeader confidence="0.906654">
3.2 THE GENERATOR
GENERATOR
</subsectionHeader>
<bodyText confidence="0.9996906">
For generation with TUG an improved version
of the semantic-head-driven generator (sHDG) (see
Shieber e.a. 1990) is used. Before beeing useful
for generation, the grammar is transformed in the
following steps:
</bodyText>
<listItem confidence="0.9901105">
• expansion of head movement rules
• transformation to the semantic head driven
generator format
• expansion of movement rules
• elimination of nonchainrules with uninstanti-
ated semantics
• goal reordering and transformation to exe-
cutable prolog code
</listItem>
<bodyText confidence="0.905043416666667">
First, the head movement expander transforms
the head movement rules. As in the parser gen-
erator, slashed categories are generated along the
head path, but no push and pop categories are in-
troduces. Instead, the head movement rule and
the trace are treated similar to argument move-
ment. The resulting relevant new rules from the
example above are:
% newly introduced slash rules
s1(S1) ---&gt; [v(V)&lt;trace(var,v_v(V)),
s_v(S)].
s_v(S) ---&gt; [...,vp_v(VP),...].
vp_v(VP) ---&gt; [...,vk_v(VK),...].
vk_v(VK) ---&gt;
trace(_,v_v(V)).
In the next step rule symbols are transformed
to the node(Cat,S, SO) format needed by the
semantic-head-driven generator. Thereby disjunc-
tions on the semantic argument as in the following
example
a(Sem) ---&gt; b(BSem), c(CSem),
(BSem = Sem; CSem = Sem).
are unfolded (multiplied out) to different rules.
The output of this step for the above rule is:
</bodyText>
<note confidence="0.803101333333333">
node(a(Sem),S,S0) ---&gt;
node(b(Sem),S,S1),
node(c(CSem),S1,S0).
node(a(Sem),S,S0) ---&gt;
node(b(BSem),S,S1),
node(c(Swm),S1,S0).
</note>
<bodyText confidence="0.9834616">
Obviously, unfolding of semantic disjunctions is
necessary for a correct choice of the semantic head.
The next compilation cycle expands the move-
ment rules. Similar to the parser generator two
arguments for gap threadding are introduced. The
filling of the arguments and the transformation of
the movement rules is different from the parser
generator. It is a rather complicated operation
which is sensitive to the semantics control flow.
Given a rule
</bodyText>
<equation confidence="0.464528">
a(A) ---&gt; b(B)&lt;trace(var,b(BT)), c(C))
we can distinguish two cases:
</equation>
<bodyText confidence="0.919817444444444">
1) The rule is a nonchain rule in the sense of
Shieber e.a. (1990) or it is a chain rule and the
antecedent of the trace is the semantic head. In
this case the antecedent has to be generated prior
to the trace. A typical example is a predicate logic
analysis as in:
node(81(Sem),S,S0) ---&gt;
node(np(Sem,SemIn) &lt;
trace(var,np(NPSem,NPSem)),
S,S1),
node(s(SemIn),S1,S0).
As the antecedent carries the semantic informa-
tion, it is expanded at the landing site, while the
trace is just empty:
node(sl(Gi,Go,Sem),S,S0) ---&gt;
node(np(Gi,Gt,Sem,SemIn),S,S1),
node(s(Gt,Gs,SemIn),S1,S0),
{cut_trace(trace(var,np(NPSem,NPSem),
</bodyText>
<subsectionHeader confidence="0.5972">
Gs,Gi)1.
</subsectionHeader>
<bodyText confidence="0.874605428571429">
node(np(Ctrace(var,np(NPSem,NPSem))IGo],
Go,NPSem,NPSem),S,S).
2) If any element other than the antecedent is
the semantic head, then this head has to be gen-
erated prior to the antecedent. As the head might
contain the trace, it also has to be generated prior
to its antecedent. Consider the rule:
</bodyText>
<footnote confidence="0.301161">
node(s1(Sem),S,S0) ---&gt;
</footnote>
<page confidence="0.99212">
106
</page>
<bodyText confidence="0.917081157894737">
node (np(NPSem)&lt;trace (var ,np(NPSem) ) ,
S,S1),
node (s (Sem) , Si, SO) .
In this rule s is generated prior to np. Within
s, the trace of np Will be generated. Following the
suggestion in Shieber c.a. (1990), rules like this
are compiled in such a way that an antecedent
is generated in the trace position without linking
it to the input string. This antecedent is then
added to the set of gaps together with its starting
and ending position (coded as a difference list).
When generation comes to the landing site, the
antecedent is cut out of the trace set. Thereby
its starting and ending position is unified with the
landing site&apos;s start. and end positions. The trans-
lation of the above rule is:
node(s1(Gi,Go,Sem),S,S0) ---&gt;
node(s(Gs,Go,Sem),S1,S0),
(cut_trace(trace(var,np(NPSem),S,S1),
</bodyText>
<subsectionHeader confidence="0.337032">
Gs,Gi)1.
</subsectionHeader>
<bodyText confidence="0.957696275862069">
node(npatrace(var,np(NPSem),S,S0)1Go],
Go,NPSem),SX,SX) ---&gt;
node(np(G,G;NPSem),S,S0).
In the next step, a certain class of nonchain
rules is eliminated from the grammar. One of
the basic inefficiencies of the semantic-head-driven
generator in Shieber e. a. (1990) has its origin in
nonchain rules whOse left-hand-side-semantics is a
variable. This kind of nonchain rule often results
from empty productions or lexicon entries of se-
mantically empty words. For instance, in a gram-
mar and lexicon fragment like
vk(SC)/Sem ---&gt; aux(VKSem,SC,VKSC)/Sem,
vk(VKSC)/VKSem.
aux (VKSem, SC ,SC)/past (VKSem) ---&gt; [has] .
aux(Sem, SC , [Subj I SC] )/Sem ---&gt; [is].
the rule introducing is is a nonchain rule whose
semantics is a variable and thus cannot be indexed
properly. Rules like this one are eliminated by a
partial evaluation technique. For each grammar
rule that contains the left-hand-side of the rule on
its right-hand-side,, a copy of the rule is produced
where the variables are unified with the left-hand-
side of the nonchain rule and the corresponding
right-hand-side element is replaced with the right-
hand-side of the nonchain rule. E.g. insertion of
the rule for is into the vk-rule above leads to
vk(SC)/Sem ---&gt; [is], vk( [Subj I SCJ )/Sem.
which is a normal chain rule.
A final compilation transforms the rules to exe-
cutable PROLOG code and sorts the right hand side
to achieve a proper semantics information flow.
Suppose that, in the following nonchain rule the
first argument of a category is its semantics argu-
ment.
node(a(t (Sem)) ,S ,S0) ---&gt;
node(b(BSem),S,S1),
node(c(CSem,BSem) ,S1,S2) •
node (d(Sem, CSem) ,S2, SO) .
The righthand side has to be ordered in such a
way that all semantics arguments have a chance to
be instantiated when the corresponding category
is expanded, as in the following rule:
node(a(f (Sem)) ,S ,S0) ---&gt;
node (d(Sem, CSem) ,S2, SO) ,
node(c(CSem,BSem),S1,S2),
node(b(BSem),S,S1).
This ordering is achieved by a bubble-sort like
mechanism. Elements of the right-hand-side are
sorted into the new right-hand-side from right to
left. To insert a new element env,&apos; into an (already
sorted) list el , enew is inserted into el
if the semantics argument of en„„ is not equal to
some argument of e1, otherwise it is sorted after
e.
In the final PROLOG code nonchain rules are in-
dexed by the functor of their lefthand side&apos;s se-
mantics as in the following example.
</bodyText>
<reference confidence="0.727013571428571">
f(f(Sem),Exp) :-
generate(Sem,node(d(Sem,CSem),
S2,S0)),
generate(CSem,node(c(CSem,BSem),
Sl,S2)),
generate(BSem,node(b(BSem),S,S1)),
a(node(a(i(Sem)),S,S0),Exp).
Chain rules (like e.g. the one that re-
sults by replacing node (a(f (Sem)) , S , SO) by
node(a(Sem),S,S0) in the rule above) are in-
dexed by their syntactic category:
d(node(d(Sem),S2,S0),Exp) :-
link(a(Sem),Exp),
generate(CSem,node(c(CSem,BSem),
</reference>
<page confidence="0.997077">
107
</page>
<bodyText confidence="0.902866076923077">
Si,S2)),
generate(BSem,node(b(BSem),S,S1)),
a(node(a(Sem),S,S0),Exp).
The auxiliary predicates needed for the genera-
tor then can be reduced to bottom-up termination
rules C(X , X) for all syntactic category symbols C
and the predicate for generate/2:
generate(Sem,Exp) :-
functor(Sem,F,A),
functor(Goal,F,2),
arg(1,Goal,Sem),
arg(2,Goal,Exp),
call(Goal).
</bodyText>
<sectionHeader confidence="0.999701" genericHeader="conclusions">
4 CONCLUSION
</sectionHeader>
<bodyText confidence="0.999944894736842">
We have distinguished three levels of reversibil-
ity: runtime reversibility, interpretation reversibil-
ity and compilation reversibility. We then have
presented Trace St Unification Grammar, a gram-
mar formalism that tries to bridge the gap be-
tween uc and GB theory in an undogmatic way
and have presented a parser generator and a gen-
erator generator that lead to effient runtime code
of the grammar both for parsing and for genera-
tion. No special effort has been invested to opti-
mize the compilers themselves, so the compilation
takes about 1.5 secs. per rule or lexicon entry.
Due to space limitations many details of the com-
pilation phase could not be discussed.
The presented grammar formalism has been
used to describe a relevant subset of German lan-
guage and a smaller subset of Chinese. The gram-
mars describe a mapping between German and
Chinese and QLF expressions.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999502">
I would like to thank Ms. Ping Peng and my
collegues Manfred Gehrke, Rudi Hunze, Steffi
Schachtl and Ludwig Schmid for many discussions
on the TUG-formalism.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999844905660377">
Aho, A.V., R. Sethi and J.D. Ullman (1986)
Compilers. Principles, Techniques and Tools.
Addison-Wesley Publishing Company.
Alshawi, H. (1990) &amp;quot;Resolving Quasi Logical
Forms&amp;quot;, Computational Linguistics, Vol. 16,
pp. 133-144.
Block, H. U. (forthcoming) &amp;quot;Two optimizations
for Semantic-Head-Driven Generators&amp;quot;.
Block, H. U. and L. A. Schmid (forthcoming) &amp;quot;Us-
ing Disjunctive Constraints in a Bottom-Up
Parser&amp;quot;.
Chen, H.-H., I-P. Lin and C.-P. Wu (1988) &amp;quot;A
new design of Prolog-based bottom-up Pars-
ing System with Government-Binding The-
ory&amp;quot;, Proc. 12th International Conference
on Computational Linguistics (COLING-88),
pp. 112-116.
Chen, H.-H. (1990) &amp;quot;A Logic-Based Government-
Binding Parser for Mandarin Chinese&amp;quot;, Proc.
13th International Conference on Computa-
tional Linguistics (COLING-90), pp. 1-6.
Neumann, G. (1991) &amp;quot;A Bidirectional Model for
Natural Language Processing&amp;quot; Proc. 5th
Conf. of the European Chapter of the ACL
(EACL-91) pp. 245-250.
Pereira, F. (1981) &amp;quot;Extraposition Grammar&amp;quot;
Computational Linguistics Vol. 7, pp. 243-
256.
Shieber, S.M. (1984) &amp;quot;The design of a Computer
Language for Linguistic Information&amp;quot; Proc.
10th International Conference on Computa-
tional Linguistics (COLING-84), pp. 362-366.
Shieber, S.M. (1988) &amp;quot;A Uniform Architecture for
Parsing and Generation&amp;quot;, Proc. 12th Interna-
tional Conference on Computational Linguis-
tics (COLING-88), pp. 614-619.
Shieber, S.M., G. van Noord, F.C.N. Pereira
and R.C. Moore (1990). &amp;quot;Semantic-Head-
Driven Generation&amp;quot;. Computational Linguis-
tics, Vol. 16, pp. 30-43.
Strzalkowsi, T. and Ping Peng (1990). &amp;quot;Auto-
mated Inversion of Logic Grammars for Gen-
eration&amp;quot; Proc. Conf. of the 28th Annual
Meeting of the ACL, (ACL-90) pp. 212-219.
Strzalkowsiki, T. (1990). &amp;quot;How to Invert a Nat-
ural Language Parser into an Efficient Gen-
erator: An Algorithm for Logic Grammars&amp;quot;
Proc. 13th International Conference on Com-
putational Linguistics (COLING-90), pp. 347-
352.
Tomita, M. (1986). Efficient Parsing for Natural
Language: A fast Algorithm for Practical Sys-
tems. Boston: Kluwer Academic Publishers.
</reference>
<page confidence="0.998366">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.269763">
<title confidence="0.993453">TRACE GRAMMAR FOR PARSING AND GENERATION</title>
<author confidence="0.976248">Hans Ulrich</author>
<affiliation confidence="0.844826">Siemens AG, Corporate Research, ZFE IS INF</affiliation>
<author confidence="0.866491">Otto Hahn-Ring</author>
<abstract confidence="0.899195833333333">D-8000 Munchen blockaztivax.uucp ABSTRACT This paper presents Trace &amp; Unification Gramdeclarative and reversible grammar formalism that brings together Unification Gramideas of Government &amp; Binding an undogmatic grammar compiler is presented that transforms a grammar in the into two different forms, one being useful for parsing, the other being useful for generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>f(f(Sem),Exp) :-generate(Sem,node(d(Sem,CSem), S2,S0)), a(node(a(i(Sem)),S,S0),Exp).</note>
<marker></marker>
<rawString>f(f(Sem),Exp) :-generate(Sem,node(d(Sem,CSem), S2,S0)), a(node(a(i(Sem)),S,S0),Exp).</rawString>
</citation>
<citation valid="false">
<title>Chain rules (like e.g. the one that results by replacing node (a(f (Sem)) , S , SO) by node(a(Sem),S,S0) in the rule above) are indexed by their syntactic category:</title>
<marker></marker>
<rawString>Chain rules (like e.g. the one that results by replacing node (a(f (Sem)) , S , SO) by node(a(Sem),S,S0) in the rule above) are indexed by their syntactic category:</rawString>
</citation>
<citation valid="false">
<note>d(node(d(Sem),S2,S0),Exp) :-link(a(Sem),Exp), generate(CSem,node(c(CSem,BSem),</note>
<marker></marker>
<rawString>d(node(d(Sem),S2,S0),Exp) :-link(a(Sem),Exp), generate(CSem,node(c(CSem,BSem),</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>R Sethi</author>
<author>J D Ullman</author>
</authors>
<date>1986</date>
<booktitle>Compilers. Principles, Techniques and Tools.</booktitle>
<publisher>Addison-Wesley Publishing Company.</publisher>
<marker>Aho, Sethi, Ullman, 1986</marker>
<rawString>Aho, A.V., R. Sethi and J.D. Ullman (1986) Compilers. Principles, Techniques and Tools. Addison-Wesley Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Resolving Quasi Logical Forms&amp;quot;,</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<pages>133--144</pages>
<contexts>
<context position="3338" citStr="Alshawi 1990" startWordPosition="546" endWordPosition="547">ATION GRAMMAR (TUG) can be transformed to fast parsers and generators. The proposed compilers and their modular architecture have the further advantage that most of their parts can be used also for other formalisms than the one described, e.g. DCGS. The whole system is part of a polyfunctional linguistic processor for German called LINGUISTIC KERNEL PROCESSOR (LKP). The LKP contains a grammar of German with broad coverage. The grammar describes the relation between a subset of German and a subset of QLF, the intermediate semantic form that is used in the Core Language Engine of SRI Cambridge (Alshawi 1990). The LKP has been implemented in PROLOG. Parsing and Generation of a sentence up to 15 words normally takes between 1 and 10 seconds, with a strong tendency to the lower bound. 100 2 FORMALISM np =&gt; f(agr:agrmnt). vp =&gt; f(agr:agrmnt). The design of Trace and Unification Grammar =&gt; f(agr:agrmnt). has been guided by the following goals: agrmnt =&gt; f(number:number,person:person). • Perspicuity. We are convinced that the generality, coverage, reliability and development speed of a grammar are a direct function of its perspicuiiy, just as programming in Pascal is less errorprone than programming in</context>
</contexts>
<marker>Alshawi, 1990</marker>
<rawString>Alshawi, H. (1990) &amp;quot;Resolving Quasi Logical Forms&amp;quot;, Computational Linguistics, Vol. 16, pp. 133-144.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H U Block</author>
</authors>
<title>(forthcoming) &amp;quot;Two optimizations for Semantic-Head-Driven Generators&amp;quot;.</title>
<marker>Block, </marker>
<rawString>Block, H. U. (forthcoming) &amp;quot;Two optimizations for Semantic-Head-Driven Generators&amp;quot;.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H U Block</author>
<author>L A</author>
</authors>
<title>Schmid (forthcoming) &amp;quot;Using Disjunctive Constraints in a Bottom-Up Parser&amp;quot;.</title>
<marker>Block, A, </marker>
<rawString>Block, H. U. and L. A. Schmid (forthcoming) &amp;quot;Using Disjunctive Constraints in a Bottom-Up Parser&amp;quot;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-H Chen</author>
<author>I-P Lin</author>
<author>C-P Wu</author>
</authors>
<title>A new design of Prolog-based bottom-up Parsing System with Government-Binding Theory&amp;quot;,</title>
<date>1988</date>
<booktitle>Proc. 12th International Conference on Computational Linguistics (COLING-88),</booktitle>
<pages>112--116</pages>
<marker>Chen, Lin, Wu, 1988</marker>
<rawString>Chen, H.-H., I-P. Lin and C.-P. Wu (1988) &amp;quot;A new design of Prolog-based bottom-up Parsing System with Government-Binding Theory&amp;quot;, Proc. 12th International Conference on Computational Linguistics (COLING-88), pp. 112-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-H Chen</author>
</authors>
<title>A Logic-Based GovernmentBinding Parser for Mandarin Chinese&amp;quot;,</title>
<date>1990</date>
<booktitle>Proc. 13th International Conference on Computational Linguistics (COLING-90),</booktitle>
<pages>1--6</pages>
<contexts>
<context position="7740" citStr="Chen (1990)" startWordPosition="1275" endWordPosition="1276">s. Disjunctions and Conjunction may be mixed freely. Besides well known cases as in the entry for knight above, we found many cases where disjunctions of path equations are useful, e.g. for the description of the extraposed relative clauses&apos;. 2.2 MOVEMENT RULES Further to these more standard UG-features, TUG provides special rule formats for the description of discontinuous dependencies, so called &amp;quot;movement rules&amp;quot;. Two main types of movement are distinguished: argument movement and head movement. The format and processing of argument movement rules is greatly inspired by Chen e. a. (1988) and Chen (1990), the processing of head movement is based on GPSG like slash features. Head Movement A head movement rule defines a relation between two positions in a parse tree, one is the landing site, the other the trace position. Head movement is constrained by the condition that the trace is the head of a specified sister (the root node) of 1Block/Sclunid (1991) describes our processing technique for disjunctions. the landing site2. Trace and Antecedent are identical with the exception that the landing site contains overt material, the trace does&apos;nt. Suppose, that v is the head of vk, vk the head of vp</context>
</contexts>
<marker>Chen, 1990</marker>
<rawString>Chen, H.-H. (1990) &amp;quot;A Logic-Based GovernmentBinding Parser for Mandarin Chinese&amp;quot;, Proc. 13th International Conference on Computational Linguistics (COLING-90), pp. 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
</authors>
<title>A Bidirectional Model for Natural Language Processing&amp;quot;</title>
<date>1991</date>
<booktitle>Proc. 5th Conf. of the European Chapter of the ACL (EACL-91)</booktitle>
<pages>245--250</pages>
<contexts>
<context position="1453" citStr="Neumann (1991)" startWordPosition="234" endWordPosition="235">e to use the same grammar for both tasks. The main goal of a grammar then is to describe a relation between normalized (semantic) representations and language strings. A grammar that can be used in both directions is called &amp;quot;reversible&amp;quot;. We can distinguish three levels of reversibility. On the first level, not only the same grammar is used for parsing and generation, but also the interpreter for parsing and generation is reversible. This approach is taken in Shieber (1988). Besides elegance the approach has the advantage that the reversibility is guaranteed. Rather advantages are mentioned in Neumann (1991). As a disadvantage, it is yet unclear whether and how these systems can be made efficient. On the second level we find systems where the same reversible grammar is processed by two different interpreters, one for parsing, one for generation. The advantage of these systems is that the grammar can be changed and tested easily, which helps to shorten the development cycle. The disadvantage again is that grammar interpreters are usually too slow to be used in realistic systems. On the third level we finally find systems, where the linguistic description is given in a reversible declarative gramma</context>
</contexts>
<marker>Neumann, 1991</marker>
<rawString>Neumann, G. (1991) &amp;quot;A Bidirectional Model for Natural Language Processing&amp;quot; Proc. 5th Conf. of the European Chapter of the ACL (EACL-91) pp. 245-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Extraposition Grammar&amp;quot;</title>
<date>1981</date>
<journal>Computational Linguistics</journal>
<volume>7</volume>
<pages>243--256</pages>
<contexts>
<context position="13160" citStr="Pereira 1981" startWordPosition="2257" endWordPosition="2258">rms of category symbols and features5. Typical long distance movement phenomena are described within this formalism as in GB by trace hopping. Below is a grammar fragment to describe the sentence Which booksi do you think ti John knows ti Mary did&apos;nt understand 14: bounding_node(s). bounding_node(np). si ---&gt; np&lt;trace(var,np), s I ... $ ---&gt; np, vp I ... s ---&gt; aux, np, vp I ... np ---&gt; propernoun I np ---&gt; det, n I vp ---&gt; v, al 1 ... vp ---&gt; v, np I ... trace(np). The main difference of argument movement to other approaches for the description of discontinuities like extraposition grammars (Pereira 1981) is that argument movement is not restricted to nested rule application. This makes the approach especially atractive for a scrambling analysis of the relative free word order in the German Mittelfeld as in Am; hati das Buchk keiner tj tk gegeben ti. 3 PROCESSING TRACE &amp; UNIFICATION GRAMMAR TUG can be processed by a parser and a generator. Before parsing and generation, the grammar is compiled to a more efficient form. 5Currently, only conjunction of equations is allowed in the definition of bounding nodes. 103 The first compilation step is common to generation and parsing. The attribute-value</context>
</contexts>
<marker>Pereira, 1981</marker>
<rawString>Pereira, F. (1981) &amp;quot;Extraposition Grammar&amp;quot; Computational Linguistics Vol. 7, pp. 243-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>The design of a Computer Language for Linguistic Information&amp;quot;</title>
<date>1984</date>
<booktitle>Proc. 10th International Conference on Computational Linguistics (COLING-84),</booktitle>
<pages>362--366</pages>
<contexts>
<context position="5325" citStr="Shieber (1984)" startWordPosition="882" endWordPosition="883">ency a design goal of the formalism led e.g. to the introduction of feature types and the separation of the movement rules into head movement and argument movement. The basis of TUG is formed by a context free grammar that is augmented by PATR n-style feature equations. Besides this basis, the main features of TUG are feature typing, mixing of attribute-value-pair and (PaoLoo-) term unification, flexible macros, unrestricted disjunction and special rule types &apos; for argument and head movement. 2.1 BASIC FEATURES As a very simple example we will look at the TUG version of the example grammar in Shieber (1984). %.type definition • =&gt;t. number =&gt; {singular ,plural}. person =&gt; {first,second,third}. % rules S ---&gt; fly, vp I np:agr = vp:agr. vp ---&gt; v, up I vp:agr = v:agr. % lexicon lexicon(&apos;Uther&apos;,np) I agr:number = singular, agr:person = third. lexicon(&apos;Arthur&apos;,np) I agr:number = singular, agr:person = third. lexicon(knights,v) I agr:number = singular, agr:person = third. lexicon(knight,v) I ( agr:number = singular, ( agr:person = first ; agr:person = second agr:number = plural ) . The two main differences to PATR II in the basic framwork are that first, TUG is less flexible in that it has a &amp;quot;hard&amp;quot; c</context>
</contexts>
<marker>Shieber, 1984</marker>
<rawString>Shieber, S.M. (1984) &amp;quot;The design of a Computer Language for Linguistic Information&amp;quot; Proc. 10th International Conference on Computational Linguistics (COLING-84), pp. 362-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>A Uniform Architecture for Parsing and Generation&amp;quot;,</title>
<date>1988</date>
<booktitle>Proc. 12th International Conference on Computational Linguistics (COLING-88),</booktitle>
<pages>614--619</pages>
<contexts>
<context position="1316" citStr="Shieber (1988)" startWordPosition="215" endWordPosition="216">r both parsing and generation. The invention of unification grammar that allows for a declarative description of language made it possible to use the same grammar for both tasks. The main goal of a grammar then is to describe a relation between normalized (semantic) representations and language strings. A grammar that can be used in both directions is called &amp;quot;reversible&amp;quot;. We can distinguish three levels of reversibility. On the first level, not only the same grammar is used for parsing and generation, but also the interpreter for parsing and generation is reversible. This approach is taken in Shieber (1988). Besides elegance the approach has the advantage that the reversibility is guaranteed. Rather advantages are mentioned in Neumann (1991). As a disadvantage, it is yet unclear whether and how these systems can be made efficient. On the second level we find systems where the same reversible grammar is processed by two different interpreters, one for parsing, one for generation. The advantage of these systems is that the grammar can be changed and tested easily, which helps to shorten the development cycle. The disadvantage again is that grammar interpreters are usually too slow to be used in re</context>
</contexts>
<marker>Shieber, 1988</marker>
<rawString>Shieber, S.M. (1988) &amp;quot;A Uniform Architecture for Parsing and Generation&amp;quot;, Proc. 12th International Conference on Computational Linguistics (COLING-88), pp. 614-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>G van Noord</author>
<author>F C N Pereira</author>
<author>R C Moore</author>
</authors>
<date>1990</date>
<journal>Semantic-HeadDriven Generation&amp;quot;. Computational Linguistics,</journal>
<volume>16</volume>
<pages>30--43</pages>
<marker>Shieber, van Noord, Pereira, Moore, 1990</marker>
<rawString>Shieber, S.M., G. van Noord, F.C.N. Pereira and R.C. Moore (1990). &amp;quot;Semantic-HeadDriven Generation&amp;quot;. Computational Linguistics, Vol. 16, pp. 30-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowsi</author>
<author>Ping Peng</author>
</authors>
<title>Automated Inversion of Logic Grammars for Generation&amp;quot;</title>
<date>1990</date>
<booktitle>Proc. Conf. of the 28th Annual Meeting of the ACL, (ACL-90)</booktitle>
<pages>212--219</pages>
<marker>Strzalkowsi, Peng, 1990</marker>
<rawString>Strzalkowsi, T. and Ping Peng (1990). &amp;quot;Automated Inversion of Logic Grammars for Generation&amp;quot; Proc. Conf. of the 28th Annual Meeting of the ACL, (ACL-90) pp. 212-219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowsiki</author>
</authors>
<title>How to Invert a Natural Language Parser into an Efficient Generator: An Algorithm for Logic Grammars&amp;quot;</title>
<date>1990</date>
<booktitle>Proc. 13th International Conference on Computational Linguistics (COLING-90),</booktitle>
<pages>347--352</pages>
<marker>Strzalkowsiki, 1990</marker>
<rawString>Strzalkowsiki, T. (1990). &amp;quot;How to Invert a Natural Language Parser into an Efficient Generator: An Algorithm for Logic Grammars&amp;quot; Proc. 13th International Conference on Computational Linguistics (COLING-90), pp. 347-352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language: A fast Algorithm for Practical Systems.</title>
<date>1986</date>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Boston:</location>
<contexts>
<context position="14376" citStr="Tomita 1986" startWordPosition="2464" endWordPosition="2465">r structure is transformed to (PROLoG) term structure by a TUG-to-DCG converter. This transformation makes use of the type definitions. As an example consider the transformation of the grammar a =&gt; f(al:t1). =&gt; f(al:t1). ti &gt; f(a2:t2,a3:t3). t2 =&gt; (1,2). t3 =&gt; (2,3). a ---&gt; b I a:a1:a3 = 2, ( a:a1:a2 = 1; a:a1 = b:a1 ). It is transformed to the following grammar in a DCG like format6. a(t1(A,2)) [b(B), (A = 1 ; tl(A,2) = The compilation steps following the TUG-to-DCG converter are different for parsing and generation. 3.1 THE PARSER GENERATOR In the LKP, a TUG is processed by a Tomita parser (Tomita 1986). For usage in that parser the result of the TUG-to-DCG converter is compiled in several steps: • expansion of head movement rules • transformation of argument movement rules • elimination of empty productions • conversion to LR(K) format • computation of LR tables First, head movement rules are eliminated and the grammar is expanded by introducing slash rules for the head path by the head movement expander. Suppose the TUG-to-DCG converter has produced the following fragment: °Note that the goal {A 1 ; t 1 (A .2) a. B) is interpreted as a constraint and not as a PROLOG goal as in DCGs. See Bl</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Tomita, M. (1986). Efficient Parsing for Natural Language: A fast Algorithm for Practical Systems. Boston: Kluwer Academic Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>