<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.062855">
<title confidence="0.997125">
Covering Treebanks with GLARF
</title>
<author confidence="0.978521">
Adam Meyers and Ralph Grishman and Michiko Kosaka and Shubin Zhao
</author>
<affiliation confidence="0.9089795">
New York University, 719 Broadway, 7th Floor, NY, NY 10003 USA
Monmouth University, West Long Branch, N.J. 07764, USA
</affiliation>
<email confidence="0.99694">
meyers/grishman/shubinz@cs.nyu.edu, kosaka@monmouth.edu
</email>
<sectionHeader confidence="0.993847" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999085">
This paper introduces GLARF, a frame-
work for predicate argument structure.
We report on converting the Penn Tree-
bank II into GLARF by automatic
methods that achieved about 90% pre-
cision/recall on test sentences from the
Penn Treebank. Plans for a corpus
of hand-corrected output, extensions of
GLARF to Japanese and applications
for MT are also discussed.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920511111112">
Applications using annotated corpora are often,
by design, limited by the information found in
those corpora. Since most English treebanks pro-
vide limited predicate-argument (PRED-ARG)
information, parsers based on these treebanks do
not produce more detailed predicate argument
structures (PRED-ARG structures). The Penn
Treebank II (Marcus et al., 1994) marks sub-
jects (SBJ), logical objects of passives (LGS),
some reduced relative clauses (RRC), as well as
other grammatical information, but does not mark
each constituent with a grammatical role. In our
view, a full PRED-ARG description of a sen-
tence would do just that: assign each constituent
a grammatical role that relates that constituent to
one or more other constituents in the sentence.
For example, the role HEAD relates a constituent
to its parent and the role OBJ relates a constituent
to the HEAD of its parent. We believe that the
absence of this detail limits the range of appli-
cations for treebank-based parsers. In particu-
lar, they limit the extent to which it is possible
to generalize, e.g., marking IND-OBJ and OBJ
roles allows one to generalize a single pattern to
cover two related examples (“John gave Mary a
book” = “John gave a book to Mary”). Distin-
guishing complement PPs (COMP) from adjunct
PPs (ADV) is useful because the former is likely
to have an idiosyncratic interpretation, e.g., the
object of “at” in “John is angry at Mary” is not
a locative and should be distinguished from the
locative case by many applications.
In an attempt to fill this gap, we have begun
a project to add this information using both au-
tomatic procedures and hand-annotation. We are
implementing automatic procedures for mapping
the Penn Treebank II (PTB) into a PRED-ARG
representation and then we are correcting the out-
put of these procedures manually. In particular,
we are hoping to encode information that will en-
able a greater level of regularization across lin-
guistic structures than is possible with PTB.
This paper introduces GLARF, the Grammati-
cal and Logical Argument Representation Frame-
work. We designed GLARF with four objec-
tives in mind: (1) capturing regularizations —
noncanonical constructions (e.g., passives, filler-
gap constructions, etc.) are represented in terms
of their canonical counterparts (simple declara-
tive clauses); (2) representing all phenomena us-
ing one simple data structure: the typed feature
structure (3) consistently labeling all arguments
and adjuncts for phrases with clear heads; and (4)
producing clear and consistent PRED-ARGs for
phrases that do not have heads, e.g., conjoined
structures, named entities, etc. — rather than try-
ing to squeeze these phrases into an X-bar mold,
we customized our representations to reflect their
head-less properties. We believe that a framework
for PRED-ARG needs to satisfy these objectives
to adequately cover a corpus like PTB.
We believe that GLARF, because of its uni-
form treatment of PRED-ARG relations, will be
valuable for many applications, including ques-
tion answering, information extraction, and ma-
chine translation. In particular, for MT, we ex-
pect it will benefit procedures which learn trans-
lation rules from syntactically analyzed parallel
corpora, such as (Matsumoto et al., 1993; Mey-
ers et al., 1996). Much closer alignments will
be possible using GLARF, because of its multi-
ple levels of representation, than would be pos-
sible with surface structure alone (An example is
provided at the end of Section 2). For this reason,
we are currently investigating the extension of our
mapping procedure to treebanks of Japanese (the
Kyoto Corpus) and Spanish (the UAM Treebank
(Moreno et al., 2000)). Ultimately, we intend to
create a parallel trilingual treebank using a com-
bination of automatic methods and human correc-
tion. Such a treebank would be valuable resource
for corpus-trained MT systems.
The primary goal of this paper is to discuss the
considerations for adding PRED-ARG informa-
tion to PTB, and to report on the performance of
our mapping procedure. We intend to wait until
these procedures are mature before beginning an-
notation on a larger scale. We also describe our
initial research on covering the Kyoto Corpus of
Japanese with GLARF.
</bodyText>
<sectionHeader confidence="0.952384" genericHeader="method">
2 Previous Treebanks
</sectionHeader>
<bodyText confidence="0.999027892857143">
There are several corpora annotated with PRED-
ARG information, but each encode some dis-
tinctions that are different. The Susanne Cor-
pus (Sampson, 1995) consists of about 1/6 of the
Brown Corpus annotated with detailed syntactic
information. Unlike GLARF, the Susanne frame-
work does not guarantee that each constituent be
assigned a grammatical role. Some grammatical
roles (e.g., subject, object) are marked explicitly,
others are implied by phrasetags (Fr corresponds
to the GLARF node label SBAR under a REL-
ATIVE arc label) and other constituents are not
assigned roles (e.g., constituents of NPs). Apart
from this concern, it is reasonable to ask why
we did not adapt this scheme for our use. Su-
sanne’s granularity surpasses PTB-based GLARF
in many areas with about 350 wordtags (part of
speech) and 100 phrasetags (phrase node labels).
However, GLARF would express many of the de-
tails in other ways, using fewer node and part of
speech (POS) labels and more attributes and role
labels. In the feature structure tradition, GLARF
can represent varying levels of detail by adding
or subtracting attributes or defining subsumption
hierarchies. Thus both Susanne’s NP1p word-
tag and Penn’s NNP wordtag would correspond
to GLARF’s NNP POS tag. A GLARF-style
Susanne analysis of “Ontario, Canada” is (NP
(PROVINCE (NNP Ontario)) (PUNCTUATION
(, ,)) (COUNTRY (NNP Canada)) (PATTERN
NAME) (SEM-FEATURE LOC)). A GLARF-
style PTB analysis uses the roles NAME1 and
NAME2 instead of PROVINCE and COUNTRY,
where name roles (NAME1, NAME2) are more
general than PROVINCE and COUNTRY in a
subsumption hierarchy. In contrast, attempts to
convert PTB into Susanne would fail because de-
tail would be unavailable. Similarly, attempts to
convert Susanne into the PTB framework would
lose information. In summary, GLARF’s ability
to represent varying levels of detail allows dif-
ferent types of treebank formats to be converted
into GLARF, even if they cannot be converted into
each other. Perhaps, GLARF can become a lingua
franca among annotated treebanks.
The Negra Corpus (Brants et al., 1997) pro-
vides PRED-ARG information for German, simi-
lar in granularity to GLARF. The most significant
difference is that GLARF regularizes some phe-
nomena which a Negra version of English would
probably not, e.g., control phenomena. Another
novel feature of GLARF is the ability to represent
paraphrases (in the Harrisian sense) that are not
entirely syntactic, e.g., nominalizations as sen-
tences. Other schemes seem to only regularize
strictly syntactic phenomena.
</bodyText>
<sectionHeader confidence="0.929534" genericHeader="method">
3 The Structure of GLARF
</sectionHeader>
<bodyText confidence="0.999823892857143">
In GLARF, each sentence is represented by a
typed feature structure. As is standard, we
model feature structures as single-rooted directed
acyclic graphs (DAGs). Each nonterminal is la-
beled with a phrase category, and each leaf is la-
beled with either: (a) a (PTB) POS label and a
word (eat, fish, etc.) or (b) an attribute value (e.g.,
singular, passive, etc.). Types are based on non-
terminal node labels, POSs and other attributes
(Carpenter, 1992). Each arc bears a feature label
which represents either a grammatical role (SBJ,
OBJ, etc.) or some attribute of a word or phrase
(morphological features, tense, semantic features,
etc.).1 For example, the subject of a sentence is
the head of a SBJ arc, an attribute like SINGU-
LAR is the head of a GRAM-NUMBER arc, etc.
A constituent involved in multiple surface or log-
ical relations may be at the head of multiple arcs.
For example, the surface subject (S-SBJ) of a pas-
sive verb is also the logical object (L-OBJ). These
two roles are represented as two arcs which share
the same head. This sort of structure sharing anal-
ysis originates with Relational Grammar and re-
lated frameworks (Perlmutter, 1984; Johnson and
Postal, 1980) and is common in Feature Structure
frameworks (LFG, HPSG, etc.). Following (John-
son et al., 1993)2, arcs are typed. There are five
different types of role labels:
Attribute roles: Gram-Number (grammati-
cal number), Mood, Tense, Sem-Feature (se-
mantic features like temporal/locative), etc.
Surface-only relations (prefixed with S-),
e.g., the surface subject (S-SBJ) of a passive.
Logical-only Roles (prefixed with L-), e.g.,
the logical object (L-OBJ) of a passive.
Intermediate roles (prefixed with I-) repre-
senting neither surface, nor logical positions.
In “John seemed to be kidnapped by aliens”,
“John” is the surface subject of “seem”, the
logical object of “kidnapped”, and the in-
termediate subject of “to be”. Intermedi-
ate arcs capture are helpful for modeling the
way sentences conform to constraints. The
intermediate subject arc obeys lexical con-
straints and connect the surface subjects of
“seem” (COMLEX Syntax class TO-INF-
RS (Macleod et al., 1998a)) to the subject
of the infinitive. However, the subject of the
infinitive in this case is not a logical sub-
ject due to the passive. In some cases, in-
termediate arcs are subject to number agree-
ment, e.g., in “Which aliens did you say
were seen?”, the I-SBJ of “were seen” agrees
with “were”.
Combined surface/logical roles (unprefixed
arcs, which we refer to as SL- arcs). For ex-
</bodyText>
<footnote confidence="0.81196225">
1A few grammatical roles are nonfunctional, e.g., a con-
stituent can have multiple ADV constituents. We number
these roles (ADV1, ADV2, ) to preserve functionality.
2That paper uses two arc types: category and relational.
</footnote>
<bodyText confidence="0.999684938775511">
ample, “John” in “John ate cheese” would be
the target of a SBJ subject arc.
Logical relations, encoded with SL- and L-
arcs, are defined more broadly in GLARF than
in most frameworks. Any regularization from a
non-canonical linguistic structure to a canonical
one results in logical relations. Following (Harris,
1968) and others, our model of canonical linguis-
tic structure is the tensed active indicative sen-
tence with no missing arguments. The following
argument types will be at the head of logical (L-)
arcs based on counterparts in canonical sentences
which are at the head of SL- arcs: logical argu-
ments of passives, understood subjects of infini-
tives, understood fillers of gaps, and interpreted
arguments of nominalizations (In “Rome’s de-
struction of Carthage”, “Rome” is the logical sub-
ject and “Carthage” is the logical object). While
canonical sentence structure provides one level
of regularization, canonical verb argument struc-
tures provide another. In the case of argument al-
ternations (Levin, 1993), the same role marks an
alternating argument regardless of where it occurs
in a sentence. Thus “the man” is the indirect ob-
ject (IND-OBJ) and “a dollar” is the direct object
(OBJ) in both “She gave the man a dollar” and
“She gave a dollar to the man” (the dative alter-
nation). Similarly, “the people” is the logical ob-
ject (L-OBJ) of both “The people evacuated from
the town” and “The troops evacuated the people
from the town”, when we assume the appropriate
regularization. Encoding this information allows
applications to generalize. For example, a single
Information Extraction pattern that recognizes the
IND-OBJ/OBJ distinction would be able to han-
dle these two examples. Without this distinction,
2 patterns would be needed.
Due to the diverse types of logical roles, we
sub-type roles according to the type of regu-
larization that they reflect. Depending on the
application, one can apply different filters to a
detailed GLARF representation, only looking at
certain types of arcs. For example, one might
choose all logical (L- and SL-) roles for an
application that is trying to acquire selection
restrictions, or all surface (S- and SL-) roles
if one was interested in obtaining a surface
parse. For other applications, one might want to
choose between subtypes of logical arcs. Given
</bodyText>
<equation confidence="0.994663076923077">
(S (NP-SBJ (PRP they))
(VP (VP (VBD spent)
(NP-2 ($ $)
(CD 325,000)
(-NONE- *U*))
(PP-TMP-3 (IN in)
(NP (CD 1989))))
(CC and)
(VP (NP=2 ($ $)
(CD 340,000)
(-NONE- *U*))
(PP-TMP=3 (IN in)
(NP (CD 1990))))))
</equation>
<figureCaption confidence="0.998178">
Figure 1: Penn representation of gapping
</figureCaption>
<bodyText confidence="0.998833611111111">
a trilingual treebank, suppose that a Spanish
treebank sentence corresponds to a Japanese
nominalization phrase and an English nominal-
ization phrase, e.g.,
Disney ha comprado Apple Computers
Disney’s acquisition of Apple Computers
Furthermore, suppose that the English treebank
analyzes the nominalization phrase both as an
NP (Disney = possessive, Apple Computers =
object of preposition) and as a paraphrase of a
sentence (Disney = subject, Apple Computers
= object). For an MT system that aligns the
Spanish and English graph representation, it
may be useful to view the nominalization phrase
in terms of the clausal arguments. However,
in a Japanese/English system, we may only
want to look at the structure of the English
nominalization phrase as an NP.
</bodyText>
<sectionHeader confidence="0.897956" genericHeader="method">
4 GLARF and the Penn Treebank
</sectionHeader>
<bodyText confidence="0.999930571428571">
This section focuses on some characteristics of
English GLARF and how we map PTB into
GLARF, as exemplified by mapping the PTB rep-
resentation in Figure 1 to the GLARF representa-
tion in Figure 2. In the process, we will discuss
how some of the more interesting linguistic phe-
nomena are represented in GLARF.
</bodyText>
<subsectionHeader confidence="0.986974">
4.1 Mapping into GLARF
</subsectionHeader>
<bodyText confidence="0.9996258">
Our procedure for mapping PTB into GLARF
uses a sequence of transformations. The first
transformation applies to PTB, and the out-
put of each is the input of
. As many of these transfor-
mations are trivial, we focus on the most interest-
ing set of problems. In addition, we explain how
GLARF is used to represent some of the more dif-
ficult phenomena.
(Brants et al., 1997) describes an effort to min-
imize human effort in the annotation of raw text
with comparable PRED-ARG information. In
contrast, we are starting with annotated corpus
and want to add as much detail as possible auto-
matically. We are as much concerned with finding
good procedures for PTB-based parser output as
we are minimizing the effort of future human tag-
gers. The procedures are designed to get the right
answer most of the time. Human taggers will cor-
rect the results when they are wrong.
</bodyText>
<subsectionHeader confidence="0.642362">
4.1.1 Conjunctions
</subsectionHeader>
<bodyText confidence="0.999859172413793">
The treatment of coordinate conjunction in
PTB is not uniform. Words labeled CC and
phrases labeled CONJP usually function as co-
ordinate conjunctions in PTB. However, a num-
ber of problems arise when one attempts to un-
ambiguously identify the phrases which are con-
joined. Most significantly, given a phrase XP
with conjunctions and commas and some set of
other constituents , it is not always
clear which are conjuncts and which are not,
i.e., Penn does not explicitly mark items as con-
juncts and one cannot assume that all are con-
juncts. In GLARF, conjoined phrases are clearly
identified and conjuncts in those phrases are dis-
tinguished from non-conjuncts. We will discuss
each problematic case that we observed in turn.
Instances of words that are marked CC in Penn
do not always function as conjunctions. They
may play the role of a sentential adverb, a preposi-
tion or the head of a parenthetical constituents. In
GLARF, conjoined phrases are explicitly marked
with the attribute value (CONJOINED T). The
mapping procedures recognize that phrases be-
ginning with CCs, PRN phrases containing CCs,
among others are not conjoined phrases.
A sister of a conjunction (other than a con-
junction) need not be a conjunct. There are two
cases. First of all, a sister of a conjunction can
be a shared modifier, e.g., the right node raised
</bodyText>
<figureCaption confidence="0.989564">
Figure 2: GLARF representation of gapping
</figureCaption>
<figure confidence="0.998087408163265">
T
PP
SEMFEAT
VBD NP PP OBJ
spent
NP TMP
NP TMP
$ CD NUMBER
CD TIME $ CD NUMBER
1989 $ 340,000
S
SEMFEAT
SBJ
VP
CONJOINED
CONJ1
CONJUNCTION1
PRP
they
CONJ2
PRD
CC
VP
and VP
ADV
HEAD
OBJ
ADV
HEAD
PATTERN
HEAD OBJ
$ 325,000
YEAR PATTERN
UNIT
NUM
IN
in
OBJ
UNIT
NUM
NP
PATTERN
IN
in
YEAR
CD TIME
PATTERN
L−GAPPING−HEAD
1990
</figure>
<bodyText confidence="0.997514972222222">
PP modifier in “[NP senior vice president] and
[NP general manager] [PP of this U.S. sales and
marketing arm]”; and the locative “there” in “de-
terring U.S. high-technology firms from [invest-
ing or [marketing their best products] there]”. In
addition, the boundaries of the conjoined phrase
and/or the conjuncts that they contain are omit-
ted in some environments, particularly when sin-
gle words are conjoined and/or when the phrases
occur before the head of a noun phrase or quan-
tifier phrase. Some phrases which are under
a single nonterminal node in the treebank (and
are not further broken down) include the follow-
ing: “between $190 million and $195 million”,
“Hollingsworth &amp; Vose Co.”, “cotton and acetate
fibers”, “those workers and managers”, “this U.S.
sales and marketing arm”, and “Messrs. Cray
and Barnum”. To overcome this sort of prob-
lem, procedures introduce brackets and mark con-
stituents as conjuncts. Considerations included
POS categories, similarity measures, construction
type (e.g., &amp; is typically part of a name), among
other factors.
CONJPs have a different distribution than CCs.
Different considerations are needed for identify-
ing the conjuncts. CONJPs, unlike CCs, can oc-
cur initially, e.g., “[Not only] [was Fred a good
doctor], [he was a good friend as well].”). Sec-
ondly, they can be embedded in the first conjunct,
e.g., “[Fred, not only, liked to play doctor], [he
was good at it as well.]”.
In Figure 2, the conjuncts are labeled explic-
itly with their roles CONJ1 and CONJ2, the con-
junction is labeled as CONJUNCTION1 and the
top-most VP is explicitly marked as a conjoined
phrase with the attribute/value (CONJOINED T).
</bodyText>
<subsectionHeader confidence="0.987564">
4.1.2 Applying Lexical Resources
</subsectionHeader>
<bodyText confidence="0.999972482758621">
We merged together two lexical resources
NOMLEX (Macleod et al., 1998b) and COM-
LEX Syntax 3.1 (Macleod et al., 1998a), deriv-
ing PP complements of nouns from NOMLEX
and using COMLEX for other types of lexical
information.We use these resources to help add
additional brackets, make additional role distinc-
tions and fill a gap when its filler is not marked
in PTB. Although Penn’s -CLR tags are good in-
dicators of complement-hood, they only apply to
verbal complements. Thus procedures for making
adjunct/complement distinctions benefited from
the dictionary classes. Similarly, COMLEX’s
NP-FOR-NP class helped identify those -BNF
constituents which were indirect objects (“John
baked Mary a cake”, “John baked a cake [for
Mary]”). The class PRE-ADJ identified those ad-
verbial modifiers within NPs which really mod-
ify the adjective. Thus we could add the follow-
ing brackets to the NP: “[even brief] exposures”.
NTITLE and NUNIT were useful for the analysis
of pattern type noun phrases, e.g., “President Bill
Clinton”, “five million dollars”. Our procedures
for identifying the logical subjects of infinitives
make extensive use of the control/raising proper-
ties of COMLEX classes. For example, X is the
subject of the infinitives in “X appeared to leave”
and “X was likely to bring attention to the prob-
lem”.
</bodyText>
<subsubsectionHeader confidence="0.483379">
4.1.3 NEs and Other Patterns
</subsubsectionHeader>
<bodyText confidence="0.99988078125">
Over the past few years, there has been a lot of
interest in automatically recognizing named enti-
ties, time phrases, quantities, among other special
types of noun phrases. These phrases have a num-
ber of things in common including: (1) their in-
ternal structure can have idiosyncratic properties
relative to other types of noun phrases, e.g., per-
son names typically consist of optional titles plus
one or more names (first, middle, last) plus an op-
tional post-honorific; and (2) externally, they can
occur wherever some more typical phrasal con-
stituent (usually NP) occurs. Identifying these
patterns makes it possible to describe these dif-
ferences in structure, e.g., instead of identifying
a head for “John Smith, Esq.”, we identify two
names and a posthonorific. If this named entity
went unrecognized, we would incorrectly assume
that “Esq.” was the head. Currently, we merge the
output of a named entity tagger to the Penn Tree-
bank prior to processing. In addition to NE tagger
output, we use procedures based on Penn’s proper
noun wordtags.
In Figure 2, there are four patterns: two
NUMBER and two TIME patterns. The TIME
patterns are very simple, each consisting just
of YEAR elements, although MONTH, DAY,
HOUR, MINUTE, etc. elements are possible.
The NUMBER patterns each consist of a sin-
gle NUMBER (although multiple NUMBER con-
stituents are possible, e.g., “one thousand”) and
one UNIT constituent. The types of these patterns
are indicated by the PATTERN attribute.
</bodyText>
<subsectionHeader confidence="0.886518">
4.1.4 Gapping Constructions
</subsectionHeader>
<bodyText confidence="0.99963612195122">
Figures 1 and 2 are corresponding PTB and
GLARF representations of gapping. Penn rep-
resents gapping via “parallel” indices for corre-
sponding arguments. In GLARF, the shared verb
is at the head of two HEAD arcs. GLARF over-
comes some problems with structure sharing anal-
yses of gapping constructions. The verb gap is a
“sloppy” (Ross, 1967) copy of the original verb.
Two separate spending events are represented by
one verb. Intuitively, structure sharing implies to-
ken identity, whereas type identity would be more
appropriate. In addition, the copied verb need not
agree with the subject in the second conjunct, e.g.,
“was”, not “were” would agree with the second
conjunct in “the risks too high and the po-
tential payoff too far in the future”. It is thus
problematic to view the gap as identical in ev-
ery way to the filler in this case. In GLARF, we
can thus distinguish the gapping sort of logical arc
(L-GAPPING-HEAD) from the other types of L-
HEAD arcs. We can stipulate that a gapping logi-
cal arc represents an appropriately inflected copy
of the phrase at the head of that arc.
In GLARF, the predicate is always explicit.
However, Penn’s representation (H. Koti, pc) pro-
vides an easy way to represent complex cases,
e.g., “John wanted to buy gold, and Mary *gap*
silver. In GLARF, the gap would be filled by the
nonconstituent “wanted to buy”. Unfortunately,
we believe that this is a necessary burden. A
goal of GLARF is to explicitly mark all PRED-
ARG relations. Given parallel indices, the user
must extract the predicate from the text by (imper-
fect) automatic means. The current solution for
GLARF is to provide multiple gaps. The second
conjunct of the example in question would have
the following analysis: (S (SBJ ) (PRD
(VP (HEAD ) (COMP (S (PRD (VP
(HEAD ) (OBJ silver)))))))), where
is filled by “wanted”, is filled by “to buy”
and is bound to Mary.
</bodyText>
<sectionHeader confidence="0.998553" genericHeader="method">
5 Japanese GLARF
</sectionHeader>
<bodyText confidence="0.980564">
Japanese GLARF will have many of the same
specifications described above. To illustrate how
we will extend GLARF to Japanese, we discuss
</bodyText>
<figureCaption confidence="0.998198">
Figure 3: Stacked Postpositions in GLARF
</figureCaption>
<bodyText confidence="0.999431789473684">
two difficult-to-represent phenomena: elision and
stacked postpositions.
Grammatical analyses of Japanese are often de-
pendency trees which use postpositions as arc la-
bels. Arguments, when elided, are omitted from
the analysis. In GLARF, however, we use role
labels like SBJ, OBJ, IND-OBJ and COMP and
mark elided constituents as zeroed arguments. In
the case of stacked postpositions, we represent the
different roles via different arcs. We also rean-
alyze certain postpositions as being complemen-
tizers (subordinators) or adverbs, thus excluding
them from canonical roles. By reanalyzing this
way, we arrived at two types of true stacked post-
positions: nominalization and topicalization. For
example, in Figure 3, the topicalized NP is at the
head of two arcs, labeled S-TOP and L-COMP
and the associated postpositions are analyzed as
morphological case attributes.
</bodyText>
<sectionHeader confidence="0.970064" genericHeader="method">
6 Testing the Procedures
</sectionHeader>
<bodyText confidence="0.999989416666666">
To test our mapping procedures, we apply them
to some PTB files and then correct the result-
ing representation using ANNOTATE (Brants and
Plaehn, 2000), a program for annotating edge-
labeled trees and DAGs, originally created for the
NEGRA corpus. We chose both files that we have
used extensively to tune the mapping procedures
(training) and other files. We then convert the
resulting GLARF Feature Structures into triples
of the form Role-Name Pivot Non-Pivot for all
logical arcs (cf. (Caroll et al., 1998)), using some
automatic procedures. The “pivot” is the head of
headed structures, but may be some other con-
stituent in non-headed structures. For example,
in a conjoined phrase, the pivot is the conjunc-
tion, and the head would be the list of heads of
the conjuncts. Rather than listing the whole Pivot
and non-pivot phrases in the triples, we simply
list the heads of these phrases, which is usually
a single word. Finally, we compute precision and
recall by comparing the triples generated from our
procedures to triples generated from the corrected
GLARF.3 An exact match is a correct answer and
anything else is incorrect.4
</bodyText>
<subsectionHeader confidence="0.998188">
6.1 The Test and the Results
</subsectionHeader>
<bodyText confidence="0.999993521739131">
We developed our mapping procedures in two
stages. We implemented some mapping proce-
dures based on PTB manuals, related papers and
actual usage of labels in PTB. After our initial im-
plementation, we tuned the procedures based on a
training set of 64 sentences from two PTB files:
wsj 0003 and wsj 0051, yielding 1285 + triples.
Then we tested these procedures against a test set
consisting of 65 sentences from wsj 0089 (1369
triples). Our results are provided in Figure 4. Pre-
cision and recall are calculated on a per sentence
basis and then averaged. The precision for a sen-
tence is the number of correct triples divided by
the total number of triples generated. The recall
is the total number of correct triples divided by
the total number of triples in the answer key.
Out of 187 incorrect triples in the test corpus,
31 reflected the incorrect role being selected, e.g.,
the adjunct/complement distinction, 139 reflected
errors or omissions in our procedures and 7 triples
related to other factors. We expect a sizable im-
provement as we increase the size of our train-
ing corpus and expand the coverage of our pro-
</bodyText>
<footnote confidence="0.9973092">
3We admit a bias towards our output in a small num-
ber of cases (less than 1%). For example, it is unimportant
whether “exposed to it” modifies “the group” or “workers”
in “a group of workers exposed to it”. The output will get
full credit for this example regardless of where the reduced
relative is attached.
4(Caroll et al., 1998) report about 88% precision and re-
call for similar triples derived from parser output. However,
they allow triples to match in some cases when the roles are
different and they do not mark modifier relations.
</footnote>
<table confidence="0.990137">
Data Sentences Recall Precision
Training 64 94.4 94.3
Test 65 89.0 89.7
</table>
<figureCaption confidence="0.995759">
Figure 4: Results
</figureCaption>
<bodyText confidence="0.9931265">
cedures, particularly since one omission often re-
sulted in several incorrect triples.
</bodyText>
<sectionHeader confidence="0.986374" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.99999064">
We show that it is possible to automatically map
PTB input into PRED-ARG structure with high
accuracy. While our initial results are promising,
mapping procedures are limited by available re-
sources. To produce the best possible GLARF re-
source, hand correction will be necessary.
We are improving our mapping procedures and
extending them to PTB-based parser output. We
are creating mapping procedures for the Susanne
corpus, the Kyoto Corpus and the UAM Tree-
bank. This work is a precursor to the creation of
a trilingual GLARF treebank.
We are currently defining the problem of map-
ping treebanks into GLARF. Subsequently, we in-
tend to create standardized mapping rules which
can be applied by any number of algorithms. The
end result may be that detailed parsing can be car-
ried out in two stages. In the first stage, one de-
rives a parse at the level of detail of the Penn Tree-
bank II. In the second stage, one derives a more
detailed parse. The advantage of such division
should be obvious: one is free to find the best pro-
cedures for each stage and combine them. These
procedures could come from different sources and
use totally different methods.
</bodyText>
<sectionHeader confidence="0.998183" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.648347">
This research was supported by the Defense Ad-
vanced Research Projects Agency under Grant
N66001-00-1-8917 from the Space and Naval
Warfare Systems Center, San Diago and by the
National Science Foundation under Grant IIS-
0081962.
</bodyText>
<sectionHeader confidence="0.967267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982144">
T. Brants and O. Plaehn. 2000. Interactive corpus an-
notation. LREC 2000, pages 453–459.
T. Brants, W. Skut, and B. Krenn. 1997. Tagging
Grammatical Functions. In EMNLP-2.
J. Caroll, T. Briscoe, and A. Sanfillippo. 1998. Parse
Evaluation: a Survey and a New Proposal. LREC
1998, pages 447–454.
B. Carpenter. 1992. The Logic of Typed Features.
Cambridge University Press, New York.
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley-Interscience, New York.
D. Johnson and P. Postal. 1980. Arc Pair Grammar.
Princeton University Press, Princeton.
D. Johnson, A. Meyers, and L. Moss. 1993. A
Unification-Based Parser for Relational Grammar.
ACL 1993, pages 97–104.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press, Chicago.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31(6):459–481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. Euralex98.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The penn treebank: Annotating pred-
icate argument structure. In Proceedings of the
1994 ARPA Human Language Technology Work-
shop.
Y. Matsumoto, H. Ishimoto, T. Utsuro, and M. Na-
gao. 1993. Structural Matching of Parallel Texts.
In ACL 1993
A. Meyers, R. Yangarber, and R. Grishman. 1996.
Alignment of Shared Forests for Bilingual Corpora.
Coling 1996, pages 460–465.
A. Meyers, M. Kosaka, and R. Grishman. 2000.
Chart-Based Transfer Rule Application in Machine
Translation. Coling 2000, pages 537–543.
A. Moreno, R. Grishman, S. Lopez, F. Sanchez, and
S. Sekine. 2000. A treebank of Spanish and its
application to parsing. LREC, pages 107–111.
D. Perlmutter. 1984. Studies in Relational Grammar
1. University of Chicago Press, Chicago.
J. Ross. 1967. Constraints on Variables in Syntax.
Ph.D. thesis, MIT.
G. Sampson. 1995. English for the Computer: The
Susanne Corpus and Analytic Scheme. Clarendon
Press, Oxford.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564310">
<title confidence="0.999741">Covering Treebanks with GLARF</title>
<author confidence="0.968452">Meyers Grishman Kosaka Zhao</author>
<address confidence="0.8185195">New York University, 719 Broadway, 7th Floor, NY, NY 10003 USA Monmouth University, West Long Branch, N.J. 07764, USA</address>
<email confidence="0.999946">meyers/grishman/shubinz@cs.nyu.edu,kosaka@monmouth.edu</email>
<abstract confidence="0.983219454545455">This paper introduces GLARF, a framework for predicate argument structure. We report on converting the Penn Treebank II into GLARF by automatic methods that achieved about 90% precision/recall on test sentences from the Penn Treebank. Plans for a corpus of hand-corrected output, extensions of GLARF to Japanese and applications for MT are also discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>O Plaehn</author>
</authors>
<title>Interactive corpus annotation. LREC</title>
<date>2000</date>
<pages>453--459</pages>
<contexts>
<context position="24048" citStr="Brants and Plaehn, 2000" startWordPosition="3911" endWordPosition="3914"> via different arcs. We also reanalyze certain postpositions as being complementizers (subordinators) or adverbs, thus excluding them from canonical roles. By reanalyzing this way, we arrived at two types of true stacked postpositions: nominalization and topicalization. For example, in Figure 3, the topicalized NP is at the head of two arcs, labeled S-TOP and L-COMP and the associated postpositions are analyzed as morphological case attributes. 6 Testing the Procedures To test our mapping procedures, we apply them to some PTB files and then correct the resulting representation using ANNOTATE (Brants and Plaehn, 2000), a program for annotating edgelabeled trees and DAGs, originally created for the NEGRA corpus. We chose both files that we have used extensively to tune the mapping procedures (training) and other files. We then convert the resulting GLARF Feature Structures into triples of the form Role-Name Pivot Non-Pivot for all logical arcs (cf. (Caroll et al., 1998)), using some automatic procedures. The “pivot” is the head of headed structures, but may be some other constituent in non-headed structures. For example, in a conjoined phrase, the pivot is the conjunction, and the head would be the list of </context>
</contexts>
<marker>Brants, Plaehn, 2000</marker>
<rawString>T. Brants and O. Plaehn. 2000. Interactive corpus annotation. LREC 2000, pages 453–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>W Skut</author>
<author>B Krenn</author>
</authors>
<title>Tagging Grammatical Functions.</title>
<date>1997</date>
<booktitle>In EMNLP-2.</booktitle>
<contexts>
<context position="6949" citStr="Brants et al., 1997" startWordPosition="1095" endWordPosition="1098">NAME2 instead of PROVINCE and COUNTRY, where name roles (NAME1, NAME2) are more general than PROVINCE and COUNTRY in a subsumption hierarchy. In contrast, attempts to convert PTB into Susanne would fail because detail would be unavailable. Similarly, attempts to convert Susanne into the PTB framework would lose information. In summary, GLARF’s ability to represent varying levels of detail allows different types of treebank formats to be converted into GLARF, even if they cannot be converted into each other. Perhaps, GLARF can become a lingua franca among annotated treebanks. The Negra Corpus (Brants et al., 1997) provides PRED-ARG information for German, similar in granularity to GLARF. The most significant difference is that GLARF regularizes some phenomena which a Negra version of English would probably not, e.g., control phenomena. Another novel feature of GLARF is the ability to represent paraphrases (in the Harrisian sense) that are not entirely syntactic, e.g., nominalizations as sentences. Other schemes seem to only regularize strictly syntactic phenomena. 3 The Structure of GLARF In GLARF, each sentence is represented by a typed feature structure. As is standard, we model feature structures as</context>
<context position="14219" citStr="Brants et al., 1997" startWordPosition="2283" endWordPosition="2286">B into GLARF, as exemplified by mapping the PTB representation in Figure 1 to the GLARF representation in Figure 2. In the process, we will discuss how some of the more interesting linguistic phenomena are represented in GLARF. 4.1 Mapping into GLARF Our procedure for mapping PTB into GLARF uses a sequence of transformations. The first transformation applies to PTB, and the output of each is the input of . As many of these transformations are trivial, we focus on the most interesting set of problems. In addition, we explain how GLARF is used to represent some of the more difficult phenomena. (Brants et al., 1997) describes an effort to minimize human effort in the annotation of raw text with comparable PRED-ARG information. In contrast, we are starting with annotated corpus and want to add as much detail as possible automatically. We are as much concerned with finding good procedures for PTB-based parser output as we are minimizing the effort of future human taggers. The procedures are designed to get the right answer most of the time. Human taggers will correct the results when they are wrong. 4.1.1 Conjunctions The treatment of coordinate conjunction in PTB is not uniform. Words labeled CC and phras</context>
</contexts>
<marker>Brants, Skut, Krenn, 1997</marker>
<rawString>T. Brants, W. Skut, and B. Krenn. 1997. Tagging Grammatical Functions. In EMNLP-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Caroll</author>
<author>T Briscoe</author>
<author>A Sanfillippo</author>
</authors>
<title>Parse Evaluation: a Survey and a New Proposal. LREC</title>
<date>1998</date>
<pages>447--454</pages>
<contexts>
<context position="24406" citStr="Caroll et al., 1998" startWordPosition="3969" endWordPosition="3972">L-COMP and the associated postpositions are analyzed as morphological case attributes. 6 Testing the Procedures To test our mapping procedures, we apply them to some PTB files and then correct the resulting representation using ANNOTATE (Brants and Plaehn, 2000), a program for annotating edgelabeled trees and DAGs, originally created for the NEGRA corpus. We chose both files that we have used extensively to tune the mapping procedures (training) and other files. We then convert the resulting GLARF Feature Structures into triples of the form Role-Name Pivot Non-Pivot for all logical arcs (cf. (Caroll et al., 1998)), using some automatic procedures. The “pivot” is the head of headed structures, but may be some other constituent in non-headed structures. For example, in a conjoined phrase, the pivot is the conjunction, and the head would be the list of heads of the conjuncts. Rather than listing the whole Pivot and non-pivot phrases in the triples, we simply list the heads of these phrases, which is usually a single word. Finally, we compute precision and recall by comparing the triples generated from our procedures to triples generated from the corrected GLARF.3 An exact match is a correct answer and an</context>
<context position="26511" citStr="Caroll et al., 1998" startWordPosition="4334" endWordPosition="4337">ected the incorrect role being selected, e.g., the adjunct/complement distinction, 139 reflected errors or omissions in our procedures and 7 triples related to other factors. We expect a sizable improvement as we increase the size of our training corpus and expand the coverage of our pro3We admit a bias towards our output in a small number of cases (less than 1%). For example, it is unimportant whether “exposed to it” modifies “the group” or “workers” in “a group of workers exposed to it”. The output will get full credit for this example regardless of where the reduced relative is attached. 4(Caroll et al., 1998) report about 88% precision and recall for similar triples derived from parser output. However, they allow triples to match in some cases when the roles are different and they do not mark modifier relations. Data Sentences Recall Precision Training 64 94.4 94.3 Test 65 89.0 89.7 Figure 4: Results cedures, particularly since one omission often resulted in several incorrect triples. 7 Concluding Remarks We show that it is possible to automatically map PTB input into PRED-ARG structure with high accuracy. While our initial results are promising, mapping procedures are limited by available resourc</context>
</contexts>
<marker>Caroll, Briscoe, Sanfillippo, 1998</marker>
<rawString>J. Caroll, T. Briscoe, and A. Sanfillippo. 1998. Parse Evaluation: a Survey and a New Proposal. LREC 1998, pages 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>The Logic of Typed Features.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="7883" citStr="Carpenter, 1992" startWordPosition="1246" endWordPosition="1247">nse) that are not entirely syntactic, e.g., nominalizations as sentences. Other schemes seem to only regularize strictly syntactic phenomena. 3 The Structure of GLARF In GLARF, each sentence is represented by a typed feature structure. As is standard, we model feature structures as single-rooted directed acyclic graphs (DAGs). Each nonterminal is labeled with a phrase category, and each leaf is labeled with either: (a) a (PTB) POS label and a word (eat, fish, etc.) or (b) an attribute value (e.g., singular, passive, etc.). Types are based on nonterminal node labels, POSs and other attributes (Carpenter, 1992). Each arc bears a feature label which represents either a grammatical role (SBJ, OBJ, etc.) or some attribute of a word or phrase (morphological features, tense, semantic features, etc.).1 For example, the subject of a sentence is the head of a SBJ arc, an attribute like SINGULAR is the head of a GRAM-NUMBER arc, etc. A constituent involved in multiple surface or logical relations may be at the head of multiple arcs. For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These two roles are represented as two arcs which share the same head. This sort of</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>B. Carpenter. 1992. The Logic of Typed Features. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley-Interscience,</publisher>
<location>New York.</location>
<contexts>
<context position="10492" citStr="Harris, 1968" startWordPosition="1670" endWordPosition="1671">face/logical roles (unprefixed arcs, which we refer to as SL- arcs). For ex1A few grammatical roles are nonfunctional, e.g., a constituent can have multiple ADV constituents. We number these roles (ADV1, ADV2, ) to preserve functionality. 2That paper uses two arc types: category and relational. ample, “John” in “John ate cheese” would be the target of a SBJ subject arc. Logical relations, encoded with SL- and Larcs, are defined more broadly in GLARF than in most frameworks. Any regularization from a non-canonical linguistic structure to a canonical one results in logical relations. Following (Harris, 1968) and others, our model of canonical linguistic structure is the tensed active indicative sentence with no missing arguments. The following argument types will be at the head of logical (L-) arcs based on counterparts in canonical sentences which are at the head of SL- arcs: logical arguments of passives, understood subjects of infinitives, understood fillers of gaps, and interpreted arguments of nominalizations (In “Rome’s destruction of Carthage”, “Rome” is the logical subject and “Carthage” is the logical object). While canonical sentence structure provides one level of regularization, canon</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Z. Harris. 1968. Mathematical Structures of Language. Wiley-Interscience, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Johnson</author>
<author>P Postal</author>
</authors>
<title>Arc Pair Grammar.</title>
<date>1980</date>
<publisher>Princeton University Press, Princeton.</publisher>
<contexts>
<context position="8613" citStr="Johnson and Postal, 1980" startWordPosition="1369" endWordPosition="1372">ibute of a word or phrase (morphological features, tense, semantic features, etc.).1 For example, the subject of a sentence is the head of a SBJ arc, an attribute like SINGULAR is the head of a GRAM-NUMBER arc, etc. A constituent involved in multiple surface or logical relations may be at the head of multiple arcs. For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These two roles are represented as two arcs which share the same head. This sort of structure sharing analysis originates with Relational Grammar and related frameworks (Perlmutter, 1984; Johnson and Postal, 1980) and is common in Feature Structure frameworks (LFG, HPSG, etc.). Following (Johnson et al., 1993)2, arcs are typed. There are five different types of role labels: Attribute roles: Gram-Number (grammatical number), Mood, Tense, Sem-Feature (semantic features like temporal/locative), etc. Surface-only relations (prefixed with S-), e.g., the surface subject (S-SBJ) of a passive. Logical-only Roles (prefixed with L-), e.g., the logical object (L-OBJ) of a passive. Intermediate roles (prefixed with I-) representing neither surface, nor logical positions. In “John seemed to be kidnapped by aliens”,</context>
</contexts>
<marker>Johnson, Postal, 1980</marker>
<rawString>D. Johnson and P. Postal. 1980. Arc Pair Grammar. Princeton University Press, Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Johnson</author>
<author>A Meyers</author>
<author>L Moss</author>
</authors>
<title>A Unification-Based Parser for Relational Grammar. ACL</title>
<date>1993</date>
<pages>97--104</pages>
<contexts>
<context position="8711" citStr="Johnson et al., 1993" startWordPosition="1384" endWordPosition="1388">subject of a sentence is the head of a SBJ arc, an attribute like SINGULAR is the head of a GRAM-NUMBER arc, etc. A constituent involved in multiple surface or logical relations may be at the head of multiple arcs. For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These two roles are represented as two arcs which share the same head. This sort of structure sharing analysis originates with Relational Grammar and related frameworks (Perlmutter, 1984; Johnson and Postal, 1980) and is common in Feature Structure frameworks (LFG, HPSG, etc.). Following (Johnson et al., 1993)2, arcs are typed. There are five different types of role labels: Attribute roles: Gram-Number (grammatical number), Mood, Tense, Sem-Feature (semantic features like temporal/locative), etc. Surface-only relations (prefixed with S-), e.g., the surface subject (S-SBJ) of a passive. Logical-only Roles (prefixed with L-), e.g., the logical object (L-OBJ) of a passive. Intermediate roles (prefixed with I-) representing neither surface, nor logical positions. In “John seemed to be kidnapped by aliens”, “John” is the surface subject of “seem”, the logical object of “kidnapped”, and the intermediate </context>
</contexts>
<marker>Johnson, Meyers, Moss, 1993</marker>
<rawString>D. Johnson, A. Meyers, and L. Moss. 1993. A Unification-Based Parser for Relational Grammar. ACL 1993, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="11189" citStr="Levin, 1993" startWordPosition="1779" endWordPosition="1780">ive sentence with no missing arguments. The following argument types will be at the head of logical (L-) arcs based on counterparts in canonical sentences which are at the head of SL- arcs: logical arguments of passives, understood subjects of infinitives, understood fillers of gaps, and interpreted arguments of nominalizations (In “Rome’s destruction of Carthage”, “Rome” is the logical subject and “Carthage” is the logical object). While canonical sentence structure provides one level of regularization, canonical verb argument structures provide another. In the case of argument alternations (Levin, 1993), the same role marks an alternating argument regardless of where it occurs in a sentence. Thus “the man” is the indirect object (IND-OBJ) and “a dollar” is the direct object (OBJ) in both “She gave the man a dollar” and “She gave a dollar to the man” (the dative alternation). Similarly, “the people” is the logical object (L-OBJ) of both “The people evacuated from the town” and “The troops evacuated the people from the town”, when we assume the appropriate regularization. Encoding this information allows applications to generalize. For example, a single Information Extraction pattern that reco</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macleod</author>
<author>R Grishman</author>
<author>A Meyers</author>
</authors>
<date>1998</date>
<booktitle>COMLEX Syntax. Computers and the Humanities,</booktitle>
<pages>31--6</pages>
<contexts>
<context position="9573" citStr="Macleod et al., 1998" startWordPosition="1515" endWordPosition="1518">e surface subject (S-SBJ) of a passive. Logical-only Roles (prefixed with L-), e.g., the logical object (L-OBJ) of a passive. Intermediate roles (prefixed with I-) representing neither surface, nor logical positions. In “John seemed to be kidnapped by aliens”, “John” is the surface subject of “seem”, the logical object of “kidnapped”, and the intermediate subject of “to be”. Intermediate arcs capture are helpful for modeling the way sentences conform to constraints. The intermediate subject arc obeys lexical constraints and connect the surface subjects of “seem” (COMLEX Syntax class TO-INFRS (Macleod et al., 1998a)) to the subject of the infinitive. However, the subject of the infinitive in this case is not a logical subject due to the passive. In some cases, intermediate arcs are subject to number agreement, e.g., in “Which aliens did you say were seen?”, the I-SBJ of “were seen” agrees with “were”. Combined surface/logical roles (unprefixed arcs, which we refer to as SL- arcs). For ex1A few grammatical roles are nonfunctional, e.g., a constituent can have multiple ADV constituents. We number these roles (ADV1, ADV2, ) to preserve functionality. 2That paper uses two arc types: category and relational</context>
<context position="18176" citStr="Macleod et al., 1998" startWordPosition="2951" endWordPosition="2954">eded for identifying the conjuncts. CONJPs, unlike CCs, can occur initially, e.g., “[Not only] [was Fred a good doctor], [he was a good friend as well].”). Secondly, they can be embedded in the first conjunct, e.g., “[Fred, not only, liked to play doctor], [he was good at it as well.]”. In Figure 2, the conjuncts are labeled explicitly with their roles CONJ1 and CONJ2, the conjunction is labeled as CONJUNCTION1 and the top-most VP is explicitly marked as a conjoined phrase with the attribute/value (CONJOINED T). 4.1.2 Applying Lexical Resources We merged together two lexical resources NOMLEX (Macleod et al., 1998b) and COMLEX Syntax 3.1 (Macleod et al., 1998a), deriving PP complements of nouns from NOMLEX and using COMLEX for other types of lexical information.We use these resources to help add additional brackets, make additional role distinctions and fill a gap when its filler is not marked in PTB. Although Penn’s -CLR tags are good indicators of complement-hood, they only apply to verbal complements. Thus procedures for making adjunct/complement distinctions benefited from the dictionary classes. Similarly, COMLEX’s NP-FOR-NP class helped identify those -BNF constituents which were indirect objects</context>
</contexts>
<marker>Macleod, Grishman, Meyers, 1998</marker>
<rawString>C. Macleod, R. Grishman, and A. Meyers. 1998a. COMLEX Syntax. Computers and the Humanities, 31(6):459–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macleod</author>
<author>R Grishman</author>
<author>A Meyers</author>
<author>L Barrett</author>
<author>R Reeves</author>
</authors>
<title>Nomlex: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>Euralex98.</booktitle>
<contexts>
<context position="9573" citStr="Macleod et al., 1998" startWordPosition="1515" endWordPosition="1518">e surface subject (S-SBJ) of a passive. Logical-only Roles (prefixed with L-), e.g., the logical object (L-OBJ) of a passive. Intermediate roles (prefixed with I-) representing neither surface, nor logical positions. In “John seemed to be kidnapped by aliens”, “John” is the surface subject of “seem”, the logical object of “kidnapped”, and the intermediate subject of “to be”. Intermediate arcs capture are helpful for modeling the way sentences conform to constraints. The intermediate subject arc obeys lexical constraints and connect the surface subjects of “seem” (COMLEX Syntax class TO-INFRS (Macleod et al., 1998a)) to the subject of the infinitive. However, the subject of the infinitive in this case is not a logical subject due to the passive. In some cases, intermediate arcs are subject to number agreement, e.g., in “Which aliens did you say were seen?”, the I-SBJ of “were seen” agrees with “were”. Combined surface/logical roles (unprefixed arcs, which we refer to as SL- arcs). For ex1A few grammatical roles are nonfunctional, e.g., a constituent can have multiple ADV constituents. We number these roles (ADV1, ADV2, ) to preserve functionality. 2That paper uses two arc types: category and relational</context>
<context position="18176" citStr="Macleod et al., 1998" startWordPosition="2951" endWordPosition="2954">eded for identifying the conjuncts. CONJPs, unlike CCs, can occur initially, e.g., “[Not only] [was Fred a good doctor], [he was a good friend as well].”). Secondly, they can be embedded in the first conjunct, e.g., “[Fred, not only, liked to play doctor], [he was good at it as well.]”. In Figure 2, the conjuncts are labeled explicitly with their roles CONJ1 and CONJ2, the conjunction is labeled as CONJUNCTION1 and the top-most VP is explicitly marked as a conjoined phrase with the attribute/value (CONJOINED T). 4.1.2 Applying Lexical Resources We merged together two lexical resources NOMLEX (Macleod et al., 1998b) and COMLEX Syntax 3.1 (Macleod et al., 1998a), deriving PP complements of nouns from NOMLEX and using COMLEX for other types of lexical information.We use these resources to help add additional brackets, make additional role distinctions and fill a gap when its filler is not marked in PTB. Although Penn’s -CLR tags are good indicators of complement-hood, they only apply to verbal complements. Thus procedures for making adjunct/complement distinctions benefited from the dictionary classes. Similarly, COMLEX’s NP-FOR-NP class helped identify those -BNF constituents which were indirect objects</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>C. Macleod, R. Grishman, A. Meyers, L. Barrett, and R. Reeves. 1998b. Nomlex: A lexicon of nominalizations. Euralex98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M A Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The penn treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="1005" citStr="Marcus et al., 1994" startWordPosition="141" endWordPosition="144">n Treebank II into GLARF by automatic methods that achieved about 90% precision/recall on test sentences from the Penn Treebank. Plans for a corpus of hand-corrected output, extensions of GLARF to Japanese and applications for MT are also discussed. 1 Introduction Applications using annotated corpora are often, by design, limited by the information found in those corpora. Since most English treebanks provide limited predicate-argument (PRED-ARG) information, parsers based on these treebanks do not produce more detailed predicate argument structures (PRED-ARG structures). The Penn Treebank II (Marcus et al., 1994) marks subjects (SBJ), logical objects of passives (LGS), some reduced relative clauses (RRC), as well as other grammatical information, but does not mark each constituent with a grammatical role. In our view, a full PRED-ARG description of a sentence would do just that: assign each constituent a grammatical role that relates that constituent to one or more other constituents in the sentence. For example, the role HEAD relates a constituent to its parent and the role OBJ relates a constituent to the HEAD of its parent. We believe that the absence of this detail limits the range of applications</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The penn treebank: Annotating predicate argument structure. In Proceedings of the 1994 ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>H Ishimoto</author>
<author>T Utsuro</author>
<author>M Nagao</author>
</authors>
<title>Structural Matching of Parallel Texts.</title>
<date>1993</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="3888" citStr="Matsumoto et al., 1993" startWordPosition="603" endWordPosition="606">tc. — rather than trying to squeeze these phrases into an X-bar mold, we customized our representations to reflect their head-less properties. We believe that a framework for PRED-ARG needs to satisfy these objectives to adequately cover a corpus like PTB. We believe that GLARF, because of its uniform treatment of PRED-ARG relations, will be valuable for many applications, including question answering, information extraction, and machine translation. In particular, for MT, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora, such as (Matsumoto et al., 1993; Meyers et al., 1996). Much closer alignments will be possible using GLARF, because of its multiple levels of representation, than would be possible with surface structure alone (An example is provided at the end of Section 2). For this reason, we are currently investigating the extension of our mapping procedure to treebanks of Japanese (the Kyoto Corpus) and Spanish (the UAM Treebank (Moreno et al., 2000)). Ultimately, we intend to create a parallel trilingual treebank using a combination of automatic methods and human correction. Such a treebank would be valuable resource for corpus-traine</context>
</contexts>
<marker>Matsumoto, Ishimoto, Utsuro, Nagao, 1993</marker>
<rawString>Y. Matsumoto, H. Ishimoto, T. Utsuro, and M. Nagao. 1993. Structural Matching of Parallel Texts. In ACL 1993</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Yangarber</author>
<author>R Grishman</author>
</authors>
<title>Alignment of Shared Forests for Bilingual Corpora. Coling</title>
<date>1996</date>
<pages>460--465</pages>
<contexts>
<context position="3910" citStr="Meyers et al., 1996" startWordPosition="607" endWordPosition="611"> to squeeze these phrases into an X-bar mold, we customized our representations to reflect their head-less properties. We believe that a framework for PRED-ARG needs to satisfy these objectives to adequately cover a corpus like PTB. We believe that GLARF, because of its uniform treatment of PRED-ARG relations, will be valuable for many applications, including question answering, information extraction, and machine translation. In particular, for MT, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora, such as (Matsumoto et al., 1993; Meyers et al., 1996). Much closer alignments will be possible using GLARF, because of its multiple levels of representation, than would be possible with surface structure alone (An example is provided at the end of Section 2). For this reason, we are currently investigating the extension of our mapping procedure to treebanks of Japanese (the Kyoto Corpus) and Spanish (the UAM Treebank (Moreno et al., 2000)). Ultimately, we intend to create a parallel trilingual treebank using a combination of automatic methods and human correction. Such a treebank would be valuable resource for corpus-trained MT systems. The prim</context>
</contexts>
<marker>Meyers, Yangarber, Grishman, 1996</marker>
<rawString>A. Meyers, R. Yangarber, and R. Grishman. 1996. Alignment of Shared Forests for Bilingual Corpora. Coling 1996, pages 460–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>M Kosaka</author>
<author>R Grishman</author>
</authors>
<title>Chart-Based Transfer Rule Application in Machine Translation. Coling</title>
<date>2000</date>
<pages>537--543</pages>
<marker>Meyers, Kosaka, Grishman, 2000</marker>
<rawString>A. Meyers, M. Kosaka, and R. Grishman. 2000. Chart-Based Transfer Rule Application in Machine Translation. Coling 2000, pages 537–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moreno</author>
<author>R Grishman</author>
<author>S Lopez</author>
<author>F Sanchez</author>
<author>S Sekine</author>
</authors>
<title>A treebank of Spanish and its application to parsing. LREC,</title>
<date>2000</date>
<pages>107--111</pages>
<contexts>
<context position="4299" citStr="Moreno et al., 2000" startWordPosition="672" endWordPosition="675">tion extraction, and machine translation. In particular, for MT, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora, such as (Matsumoto et al., 1993; Meyers et al., 1996). Much closer alignments will be possible using GLARF, because of its multiple levels of representation, than would be possible with surface structure alone (An example is provided at the end of Section 2). For this reason, we are currently investigating the extension of our mapping procedure to treebanks of Japanese (the Kyoto Corpus) and Spanish (the UAM Treebank (Moreno et al., 2000)). Ultimately, we intend to create a parallel trilingual treebank using a combination of automatic methods and human correction. Such a treebank would be valuable resource for corpus-trained MT systems. The primary goal of this paper is to discuss the considerations for adding PRED-ARG information to PTB, and to report on the performance of our mapping procedure. We intend to wait until these procedures are mature before beginning annotation on a larger scale. We also describe our initial research on covering the Kyoto Corpus of Japanese with GLARF. 2 Previous Treebanks There are several corpo</context>
</contexts>
<marker>Moreno, Grishman, Lopez, Sanchez, Sekine, 2000</marker>
<rawString>A. Moreno, R. Grishman, S. Lopez, F. Sanchez, and S. Sekine. 2000. A treebank of Spanish and its application to parsing. LREC, pages 107–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perlmutter</author>
</authors>
<title>Studies in Relational Grammar 1.</title>
<date>1984</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="8586" citStr="Perlmutter, 1984" startWordPosition="1367" endWordPosition="1368">etc.) or some attribute of a word or phrase (morphological features, tense, semantic features, etc.).1 For example, the subject of a sentence is the head of a SBJ arc, an attribute like SINGULAR is the head of a GRAM-NUMBER arc, etc. A constituent involved in multiple surface or logical relations may be at the head of multiple arcs. For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These two roles are represented as two arcs which share the same head. This sort of structure sharing analysis originates with Relational Grammar and related frameworks (Perlmutter, 1984; Johnson and Postal, 1980) and is common in Feature Structure frameworks (LFG, HPSG, etc.). Following (Johnson et al., 1993)2, arcs are typed. There are five different types of role labels: Attribute roles: Gram-Number (grammatical number), Mood, Tense, Sem-Feature (semantic features like temporal/locative), etc. Surface-only relations (prefixed with S-), e.g., the surface subject (S-SBJ) of a passive. Logical-only Roles (prefixed with L-), e.g., the logical object (L-OBJ) of a passive. Intermediate roles (prefixed with I-) representing neither surface, nor logical positions. In “John seemed </context>
</contexts>
<marker>Perlmutter, 1984</marker>
<rawString>D. Perlmutter. 1984. Studies in Relational Grammar 1. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross</author>
</authors>
<date>1967</date>
<booktitle>Constraints on Variables in Syntax. Ph.D. thesis, MIT.</booktitle>
<contexts>
<context position="21285" citStr="Ross, 1967" startWordPosition="3455" endWordPosition="3456"> elements are possible. The NUMBER patterns each consist of a single NUMBER (although multiple NUMBER constituents are possible, e.g., “one thousand”) and one UNIT constituent. The types of these patterns are indicated by the PATTERN attribute. 4.1.4 Gapping Constructions Figures 1 and 2 are corresponding PTB and GLARF representations of gapping. Penn represents gapping via “parallel” indices for corresponding arguments. In GLARF, the shared verb is at the head of two HEAD arcs. GLARF overcomes some problems with structure sharing analyses of gapping constructions. The verb gap is a “sloppy” (Ross, 1967) copy of the original verb. Two separate spending events are represented by one verb. Intuitively, structure sharing implies token identity, whereas type identity would be more appropriate. In addition, the copied verb need not agree with the subject in the second conjunct, e.g., “was”, not “were” would agree with the second conjunct in “the risks too high and the potential payoff too far in the future”. It is thus problematic to view the gap as identical in every way to the filler in this case. In GLARF, we can thus distinguish the gapping sort of logical arc (L-GAPPING-HEAD) from the other t</context>
</contexts>
<marker>Ross, 1967</marker>
<rawString>J. Ross. 1967. Constraints on Variables in Syntax. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the Computer: The Susanne Corpus and Analytic Scheme.</title>
<date>1995</date>
<publisher>Clarendon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="5026" citStr="Sampson, 1995" startWordPosition="792" endWordPosition="793">man correction. Such a treebank would be valuable resource for corpus-trained MT systems. The primary goal of this paper is to discuss the considerations for adding PRED-ARG information to PTB, and to report on the performance of our mapping procedure. We intend to wait until these procedures are mature before beginning annotation on a larger scale. We also describe our initial research on covering the Kyoto Corpus of Japanese with GLARF. 2 Previous Treebanks There are several corpora annotated with PREDARG information, but each encode some distinctions that are different. The Susanne Corpus (Sampson, 1995) consists of about 1/6 of the Brown Corpus annotated with detailed syntactic information. Unlike GLARF, the Susanne framework does not guarantee that each constituent be assigned a grammatical role. Some grammatical roles (e.g., subject, object) are marked explicitly, others are implied by phrasetags (Fr corresponds to the GLARF node label SBAR under a RELATIVE arc label) and other constituents are not assigned roles (e.g., constituents of NPs). Apart from this concern, it is reasonable to ask why we did not adapt this scheme for our use. Susanne’s granularity surpasses PTB-based GLARF in many</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>G. Sampson. 1995. English for the Computer: The Susanne Corpus and Analytic Scheme. Clarendon Press, Oxford.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>