<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002850">
<title confidence="0.9978915">
Bilingual Word Embeddings from Non-Parallel Document-Aligned Data
Applied to Bilingual Lexicon Induction
</title>
<author confidence="0.99579">
Ivan Vuli´c and Marie-Francine Moens
</author>
<affiliation confidence="0.999346">
Department of Computer Science
</affiliation>
<address confidence="0.608141">
KU Leuven, Belgium
</address>
<email confidence="0.99836">
{ivan.vulic|marie-francine.moens}@cs.kuleuven.be
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843">
We propose a simple yet effective
approach to learning bilingual word
embeddings (BWEs) from non-parallel
document-aligned data (based on the
omnipresent skip-gram model), and its
application to bilingual lexicon induction
(BLI). We demonstrate the utility of
the induced BWEs in the BLI task by
reporting on benchmarking BLI datasets
for three language pairs: (1) We show
that our BWE-based BLI models signifi-
cantly outperform the MuPTM-based and
context-counting models in this setting,
and obtain the best reported BLI results
for all three tested language pairs; (2)
We also show that our BWE-based BLI
models outperform other BLI models
based on recently proposed BWEs that
require parallel data for bilingual training.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999684133333334">
Dense real-valued vectors known as distributed
representations of words or word embeddings
(WEs) (Bengio et al., 2003; Collobert and We-
ston, 2008; Mikolov et al., 2013a; Pennington
et al., 2014) have been introduced recently as
part of neural network architectures for statisti-
cal language modeling. Recent studies (Levy and
Goldberg, 2014; Levy et al., 2015) have show-
cased a direct link and comparable performance to
“more traditional” distributional models (Turney
and Pantel, 2010), but the skip-gram model with
negative sampling (SGNS) (Mikolov et al., 2013c)
is still established as the state-of-the-art word rep-
resentation model, due to its simplicity, fast train-
ing, as well as its solid and robust performance
across a wide variety of semantic tasks (Baroni et
al., 2014; Levy et al., 2015).
A natural extension of interest from monolin-
gual to multilingual word embeddings has oc-
curred recently (Klementiev et al., 2012; Zou et
al., 2013; Mikolov et al., 2013b; Hermann and
Blunsom, 2014a; Hermann and Blunsom, 2014b;
Gouws et al., 2014; Chandar et al., 2014; Soyer
et al., 2015; Luong et al., 2015). When operat-
ing in multilingual settings, it is highly desirable to
learn embeddings for words denoting similar con-
cepts that are very close in the shared inter-lingual
embedding space (e.g., the representations for the
English word school and the Spanish word es-
cuela should be very similar). These shared inter-
lingual embedding spaces may then be used in a
myriad of multilingual natural language process-
ing tasks, such as fundamental tasks of comput-
ing cross-lingual and multilingual semantic word
similarity and bilingual lexicon induction (BLI),
etc. However, all these models critically require at
least sentence-aligned parallel data and/or readily-
available translation dictionaries to induce bilin-
gual word embeddings (BWEs) that are consistent
and closely aligned over languages in the same se-
mantic space.
Contributions In this work, we alleviate the re-
quirements: (1) We present the first model that
is able to induce bilingual word embeddings from
non-parallel data without any other readily avail-
able translation resources such as pre-given bilin-
gual lexicons; (2) We demonstrate the utility of
BWEs induced by this simple yet effective model
in the BLI task from comparable Wikipedia data
on benchmarking datasets for three language pairs
(Vuli´c and Moens, 2013b). Our BLI model based
on our novel BWEs significantly outperforms a se-
ries of strong baselines that reported previous best
scores on these datasets in the same learning set-
ting, as well as other BLI models based on re-
cently proposed BWE induction models (Gouws
et al., 2014; Chandar et al., 2014). The focus of
the work is on learning lexicons from document-
aligned comparable corpora (e.g., Wikipedia arti-
cles aligned through inter-wiki links).
</bodyText>
<page confidence="0.891115">
719
</page>
<note confidence="0.372043">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 719–725,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.996544">
Figure 1: The architecture of our BWE Skip-Gram model for learning bilingual word embeddings from
</figureCaption>
<bodyText confidence="0.923969">
document-aligned comparable data. Source language words and documents are drawn as gray boxes,
while target language words and documents are drawn as blue boxes. The right side of the figure (sepa-
rated by a vertical dashed line) illustrates how a pseudo-bilingual document is constructed from a pair of
two aligned documents; two documents are first merged, and then words in the pseudo-bilingual docu-
ment are randomly shuffled to ensure that both source and target language words occur as context words.
</bodyText>
<sectionHeader confidence="0.933533" genericHeader="method">
2 Model Architecture
</sectionHeader>
<bodyText confidence="0.998822178571429">
In the following architecture description, we as-
sume that the reader is familiar with the main
assumptions and training procedure of SGNS
(Mikolov et al., 2013a; Mikolov et al., 2013c).
We extend the SGNS model to work with bilingual
document-aligned comparable data. An overview
of our architecture for learning BWEs from such
comparable data is given in fig. 1.
Let us assume that we possess a document-
aligned comparable corpus which is defined as
C = {d1, d2, ... , dN} = {(dS1 , dT1 ), (dS2 , d2 T),
..., (dS N, dTD)}, where dj = (dSj , dTj ) denotes a
pair of aligned documents in the source language
LS and the target language LT, respectively, and
N is the number of documents in the corpus.
V S and V T are vocabularies associated with lan-
guages LS and LT. The goal is to learn word em-
beddings for all words in both V S and VT that will
be semantically coherent and closely aligned over
languages in a shared cross-lingual word embed-
ding space.
In the first step, we merge two documents dSj
and dTj from the aligned document pair dj into
a single “pseudo-bilingual” document d&apos;j and re-
move sentence boundaries. Following that, we
randomly shuffle the newly constructed pseudo-
bilingual document. The intuition behind this pre-
training completely random shuffling step1 (see
</bodyText>
<footnote confidence="0.731619">
1In this paper, we investigate only the random shuffling
procedure and show that the model is fairly robust to different
</footnote>
<bodyText confidence="0.983069939393939">
fig. 1) is to assure that each word w, regardless
of its actual language, obtains word collocates
from both vocabularies. The idea of having bilin-
gual contexts for each pivot word in each pseudo-
bilingual document will steer the final model to-
wards constructing a shared inter-lingual embed-
ding space. Since the model depends on the align-
ment at the document level, in order to ensure
the bilingual contexts instead of monolingual con-
texts, it is intuitive to assume that larger window
sizes will lead to better bilingual embeddings. We
test this hypothesis and the effect of window size
in sect. 4.
The final model called BWE Skip-Gram
(BWESG) then relies on the monolingual vari-
ant of skip-gram trained on the shuffled pseudo-
bilingual documents.2 The model learns word em-
beddings for source and target language words
that are aligned over the d embedding dimen-
sions and may be represented in the same shared
cross-lingual embedding space. The BWESG-
based representation of word w, regardless of its
actual language, is then a d-dimensional vector:
w�= [f1, . . . , fk, . . . , fd], where fk E R denotes
the score for the k-th inter-lingual feature within
the d-dimensional shared embedding space. Since
all words share the embedding space, semantic
similarity between words may be computed both
outputs of the procedure if the window size is large enough.
As one line of future work, we plan to investigate other, more
systematic and deterministic shuffling algorithms.
2We were also experimenting with GloVe and CBOW, but
they were falling behind SGNS on average.
</bodyText>
<page confidence="0.972474">
720
</page>
<bodyText confidence="0.999938384615385">
monolingually and across languages. Given w,
the most similar word cross-lingually should be its
one-to-one translation, and we may use this intu-
ition to induce one-to-one bilingual lexicons from
comparable data.
In another interpretation, BWESG actually
builds BWEs based on (pseudo-bilingual) docu-
ment level co-occurrence. The window size pa-
rameter then just controls the amount of random
data dropout. With larger windows, the model
becomes prohibitively computationally expensive,
but in sect. 4 we show that the BLI performance
flattens out for “reasonably large” windows.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.980810652173913">
Training Data We use comparable Wikipedia data
introduced in (Vuli´c and Moens, 2013a; Vuli´c and
Moens, 2013b) available in three language pairs
to induce bilingual word embeddings: (i) a collec-
tion of 13,696 Spanish-English Wikipedia article
pairs (ES-EN), (ii) a collection of 18,898 Italian-
English Wikipedia article pairs (IT-EN), and (iii) a
collection of 7,612 Dutch-English Wikipedia arti-
cle pairs (NL-EN). All corpora are theme-aligned
comparable corpora, that is, the aligned docu-
ment pairs discuss similar themes, but are in gen-
eral not direct translations. Following prior work
(Haghighi et al., 2008; Prochasson and Fung,
2011; Vuli´c and Moens, 2013b), we retain only
nouns that occur at least 5 times in the corpus.
Lemmatized word forms are recorded when avail-
able, and original forms otherwise. TreeTag-
ger (Schmid, 1994) is used for POS tagging and
lemmatization. After the preprocessing vocabular-
ies comprise between 7,000 and 13,000 noun types
for each language in each language pair. Exactly
the same training data and vocabularies are used to
induce bilingual lexicons with all other BLI mod-
els in comparison.
BWESG Training Setup We have trained the
BWESG model with random shuffling on 10 ran-
dom corpora shuffles for all three training cor-
pora with the following parameters from the
word2vec package (Mikolov et al., 2013c):
stochastic gradient descent with a default learning
rate of 0.025, negative sampling with 25 samples,
and a subsampling rate of value 1e−4. All models
are trained for 15 epochs. We have varied the num-
ber of embedding dimensions: d = 100, 200, 300,
and have also trained the model with d = 40 to
be directly comparable to pre-trained state-of-the-
art BWEs from (Gouws et al., 2014; Chandar et
al., 2014). Moreover, in order to test the effect of
window size on final results, we have varied the
maximum window size cs from 4 to 60 in steps of
4.3 Since cosine is used for all similarity compu-
tations in the BLI task, we call our new BLI model
BWESG+cos.
Baseline BLI Models We compare BWESG+cos
to a series of state-of-the-art BLI models from
document-aligned comparable data:
</bodyText>
<listItem confidence="0.7374006875">
(1) BiLDA-BLI - A BLI model that relies on the
induction of latent cross-lingual topics (Mimno et
al., 2009) by the bilingual LDA model and repre-
sents words as probability distributions over these
topics (Vuli´c et al., 2011).
(2) Assoc-BLI - A BLI model that represents
words as vectors of association norms (Roller and
Schulte im Walde, 2013) over both vocabularies,
where these norms are computed using a multilin-
gual topic model (Vuli´c and Moens, 2013a).
(3) PPMI+cos - A standard distributional model
for BLI relying on positive pointwise mutual infor-
mation and cosine similarity (Bullinaria and Levy,
2007). The seed lexicon is bootstrapped using the
method from (Peirsman and Pad´o, 2011; Vuli´c and
Moens, 2013b).
</listItem>
<bodyText confidence="0.999469826086957">
All parameters of the baseline BLI models (i.e.,
topic models and their settings, the number of
dimensions K, feature pruning values, window
size) are set to their optimal values according to
suggestions in prior work (Steyvers and Griffiths,
2007; Vuli´c and Moens, 2013a; Vuli´c and Moens,
2013b; Kiela and Clark, 2014). Due to space con-
straints, for (much) more details about the base-
lines we point to the relevant literature (Peirsman
and Pad´o, 2011; Tamura et al., 2012; Vuli´c and
Moens, 2013a; Vuli´c and Moens, 2013b).
Test Data For each language pair, we evaluate on
standard 1,000 ground truth one-to-one translation
pairs built for the three language pairs (ES/IT/NL-
EN) (Vuli´c and Moens, 2013a; Vuli´c and Moens,
2013b). Translation direction is ES/IT/NL → EN.
Evaluation Metrics Since we can build a one-
to-one bilingual lexicon by harvesting one-to-one
translation pairs, the lexicon qualiy is best re-
flected in the Acca score, that is, the number
of source language (ES/IT/NL) words ws from
ground truth translation pairs for which the top
ranked word cross-lingually is the correct trans-
</bodyText>
<footnote confidence="0.995592">
3We will make all our BWESG BWEs available at:
http://people.cs.kuleuven.be/∼ivan.vulic/
</footnote>
<page confidence="0.974003">
721
</page>
<table confidence="0.993999181818182">
Spanish-English (ES-EN) Italian-English (IT-EN) Dutch-English (NL-EN)
(1) reina (2) reina (3) reina (1) madre (2) madre (3) madre (1) schilder (2) schilder (3) schilder
(Spanish) (English) (Combined) (Italian) (English) (Combined) (Dutch) (English) (Combined)
rey queen(+) queen(+)
trono heir rey
monarca throne trono
heredero king heir
matrimonio royal throne
hijo reign monarca
reino succession heredero
reinado princess king
regencia marriage matrimonio
padre mother(+) mother(+)
moglie father padre
sorella sister moglie
figlia wife father
figlio daughter sorella
fratello son figlia
casa friend figlio
amico childhood sister
marito family fratello
kunstschilder painter(+) painter(+)
schilderij painting kunstschilder
kunstenaar portrait painting
olieverf artist schilderij
olieverfschilderij canvas kunstenaar
schilderen impressionist portrait
frans cubism olieverf
nederlands art olieverfschilderij
componist poet schilderen
duque prince royal
donna cousin wife
beeldhouwer drawing artist
</table>
<tableCaption confidence="0.6676262">
Table 1: Example lists of top 10 semantically similar words for all 3 language pairs obtained using
BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target
language words are skipped (monolingual similarity); (2) only target language words (EN) are listed
(cross-lingual similarity); (3) words from both languages are listed (multilingual similarity). EN words
are given in italic. The correct one-to-one translation for each source word is marked by (+).
</tableCaption>
<bodyText confidence="0.9938995">
lation in the other language (EN) according to the
ground truth over the total number of ground truth
translation pairs (=1000) (Gaussier et al., 2004;
Tamura et al., 2012; Vuli´c and Moens, 2013b).
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999508933333333">
Exp 0: Qualitative Analysis Tab. 1 displays
top 10 semantically similar words monolingually,
across-languages and combined/multilingually for
one ES, IT and NL word. The BWESG+cos model
is able to find semantically coherent lists of words
for all three directions of similarity (i.e., mono-
lingual, cross-lingual, multilingual). In the com-
bined (multilingual) ranked lists, words from both
languages are represented as top similar words.
This initial qualitative analysis already demon-
strates the ability of BWESG to induce a shared
cross-lingual embedding space using only docu-
ment alignments as bilingual signals.
Exp I: BWESG+cos vs. Baseline Models In the
first experiment, we test whether our BWESG+cos
BLI model produces better results than the base-
line BLI models which obtain current state-of-the-
art results for BLI from comparable data on these
test sets. Tab. 2 summarizes the BLI results.
As the most striking finding, the results reveal
superior performance of the BWESG-cos model
for BLI which relies on our new framework for in-
ducing bilingual word embeddings over other BLI
models relying on previously used bilingual word
representations. The relative increase in Accl
scores over the best scoring baseline BLI mod-
els from comparable data is 19.4% for the ES-EN
pair, 6.1% for IT-EN (significant at p &lt; 0.05 us-
ing McNemar’s test) and 65.4% for NL-EN. For
large enough values for cs (cs &gt; 20) (see also
</bodyText>
<table confidence="0.999789625">
Pair: ES-EN IT-EN NL-EN
Model Accl Accl Accl
BiLDA-BLI 0.441 0.575 0.237
Assoc-BLI 0.518 0.618 0.236
PPMI+cos 0.577 0.647 0.206
BWESG+cos 0.617 0.599 0.300
d:100,cs:16
d:100,cs:48 0.667 0.669 0.389
d:200,cs:16 0.613 0.601 0.254
d:200,cs:48 0.685 0.683 0.392
d:300,cs:16 0.596 0.583 0.224
d:300,cs:48 0.689 0.683 0.363
d: 40,cs:16 0.558 0.533 0.266
d: 40,cs:48 0.578 0.595 0.308
CHANDAR 0.432 - -
GOUWS 0.516 0.557 0.575
</table>
<tableCaption confidence="0.715988">
Table 2: BLI performance for all tested BLI
models for ES/IT/NL-EN, with all bilingual word
representations except CHANDAR and GOUWS
learned from comparable Wikipedia data. The
scores for BWESG+cos are computed as post-hoc
averages over 10 random shuffles.
</tableCaption>
<bodyText confidence="0.993669294117647">
fig. 2(a)-2(c)), almost all BWESG+cos models for
all language pairs outperform the highest baseline
results. We may also observe that the performance
of BWESG+cos is fairly stable for all models with
larger values for cs (cs &gt; 20). This finding re-
veals that even a coarse tuning of these parameters
might lead to optimal or near-optimal scores in the
BLI task with BWESG+cos.
Exp II: Shuffling and Window Size Since our
BWESG model relies on the pre-training random
shuffling procedure, we also test whether the shuf-
fling has significant or rather minor impact on the
induction of BWEs and final BLI scores. There-
fore, in fig. 2, we present maximum, minimum,
and average Accl scores for all three language
pairs obtained using 10 different random corpora
shuffles with d = 100, 200,300 and varying val-
</bodyText>
<page confidence="0.979181">
722
</page>
<figure confidence="0.999896711111111">
���1 scores
0.
0.
0.5
0.4
0.3
0.2
0.1
0
( )
( )
( )
( )
( )
( )
( )
( )
( )
���1 scores
0.
0.
0.5
0.4
0.3
0.2
0.1
0
���1 scores
0.
0.
0.5
0.4
0.3
0.2
0.1
0
4 8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size
(a) d = 100
4 8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size
(b) d = 200
4 8 12 16 20 24 28 32 36 40 44 48 52 56
Window size
(c) d = 300
</figure>
<figureCaption confidence="0.984176">
Figure 2: Maximum (MAX), minimum (MIN) and average (AVG) Acca scores with BWESG+cos in
the BLI task over 10 different random corpora shuffles for all 3 language pairs, and varying values for
parameters cs and d. Solid horizontal lines denote the highest baseline Acca scores for each language
pair. NOS (thicker dotted lines) refers to BWESG+cos without random shuffling.
</figureCaption>
<bodyText confidence="0.999839">
ues for cs. Results reveal that random shuffling
affects the overall BLI scores, but the variance of
results is minimal and often highly insignificant. It
is important to mark that even the minimum Acci
scores over these 10 different random shuffles are
typically higher than the previous state-of-the-art
baseline scores for large enough values for d and
cs (compare the results in tab. 2 and fig. 2(a)-2(c)).
A comparison with the BWESG model without
shuffling (NOS on fig. 2) reveals that shuffling is
useful even for larger cs-s.
Exp III: BWESG+cos vs. BWE-Based BLI We
also compare our BWESG BLI model with two
other models that are most similar to ours in spirit,
as they also induce shared cross-lingual word em-
bedding spaces (Chandar et al., 2014; Gouws et
al., 2014), proven superior to or on a par with
the BLI model from (Mikolov et al., 2013b). We
use their pre-trained BWEs (obtained from the au-
thors) and report the BLI scores in tab. 2. To
make the comparison fair, we search for transla-
tions over the same vocabulary as with all other
models. The results clearly reveal that, although
both other BWE models critically rely on paral-
lel Europarl data for training, and Gouws et al.
(2014) in addition train on entire monolingual
Wikipedias in both languages, our simple BWE in-
duction model trained on much smaller amounts of
document-aligned non-parallel data produces sig-
nificantly higher BLI scores for IT-EN and ES-EN
with sufficiently large windows.
However, the results for NL-EN with all BLI
models from comparable data from tab. 2 are sig-
nificantly lower than with the GOUWS BWEs. We
attribute it to using less (and clearly insufficient)
document-aligned training data for NL-EN (i.e.,
training corpora for ES-EN and IT-EN are almost
double or triple the size of training corpora for NL-
EN, see sect. 3).
</bodyText>
<sectionHeader confidence="0.991659" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977806451613">
We have proposed Bilingual Word Embeddings
Skip-Gram (BWESG), a simple yet effective
model that is able to learn bilingual word em-
beddings solely on the basis of document-aligned
comparable data. We have demonstrated its utility
in the task of bilingual lexicon induction from such
comparable data, where our new BWESG-based
BLI model outperforms state-of-the-art models for
BLI from document-aligned comparable data and
related BWE induction models.
The low-cost BWEs may be used in other (se-
mantic) tasks besides the ones discussed here, and
it would be interesting to experiment with other
types of context aggregation and selection beyond
random shuffling, and other objective functions.
Preliminary studies also demonstrate the utility of
the BWEs in monolingual and cross-lingual infor-
mation retrieval (Vuli´c and Moens, 2015).
Finally, we may use the knowledge of BWEs
obtained by BWESG from document-aligned data
to learn bilingual correspondences (e.g., word
translation pairs or lists of semantically simi-
lar words across languages) which may in turn
be used for representation learning from large
unaligned multilingual datasets as proposed in
(Haghighi et al., 2008; Mikolov et al., 2013b;
Vuli´c and Moens, 2013b). In the long run, this
idea may lead to large-scale fully data-driven rep-
resentation learning models from huge amounts of
multilingual data without any “pre-requirement”
for parallel data or manually built lexicons.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994508">
We would like to thank the reviewers for their
insightful comments and suggestions. This re-
search has been carried out in the frameworks of
the SCATE project (IWT-SBO 130041) and the
PARIS project (IWT-SBO 110067).
</bodyText>
<page confidence="0.997859">
723
</page>
<sectionHeader confidence="0.996367" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999755320388349">
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510–526.
Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh M. Khapra, Balaraman Ravindran, Vikas C.
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In NIPS, pages 1853–1861.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML,
pages 160–167.
´Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526–533.
Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2014. BilBOWA: Fast bilingual distributed rep-
resentations without word alignments. CoRR,
abs/1410.2455.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771–779.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual distributed representations without word
alignment. In ICLR.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual models for compositional distributed se-
mantics. In ACL, pages 58–68.
Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters. In
CVSC Workshop at EACL, pages 21–30.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed repre-
sentations of words. In COLING, pages 1459–1474.
Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In NIPS,
pages 2177–2185.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the ACL,
3:211–225.
Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Bilingual word representations with
monolingual quality in mind. In Proceedings of the
1st Workshop on Vector Space Modeling for Natural
Language Processing, pages 151–159.
Tomas Mikolov, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. 2013a. Efficient estimation of word
representations in vector space. In ICLR Workshop
Papers.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. CoRR, abs/1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013c. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111–3119.
David Mimno, Hanna Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP, pages 880–
889.
Yves Peirsman and Sebastian Pad´o. 2011. Semantic
relations in bilingual lexicons. ACM Transactions
on Speech and Language Processing, 8(2):article 3.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In EMNLP, pages 1532–1543.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In ACL, pages 1327–1335.
Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal LDA model integrating textual, cogni-
tive and visual modalities. In EMNLP, pages 1146–
1157.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa.
2015. Leveraging monolingual data for crosslingual
compositional word representations. In ICLR.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analy-
sis, 427(7):424–440.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In EMNLP, pages
24–36.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artifical Intelligence Research,
37(1):141–188.
</reference>
<page confidence="0.98097">
724
</page>
<reference confidence="0.9997502">
Ivan Vuli´c and Marie-Francine Moens. 2013a. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In NAACL, pages
106–116.
Ivan Vuli´c and Marie-Francine Moens. 2013b. A study
on bootstrapping bilingual vector spaces from non-
parallel data (and nothing else). In EMNLP, pages
1613–1624.
Ivan Vuli´c and Marie-Francine Moens. 2015. Mono-
lingual and cross-lingual information retrieval mod-
els based on (bilingual) word embeddings. In SI-
GIR, to appear.
Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compara-
ble corpora using latent topic models. In ACL, pages
479–484.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP, pages 1393–1398.
</reference>
<page confidence="0.998435">
725
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.638992">
<title confidence="0.997574">Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction</title>
<author confidence="0.993233">Ivan Vuli´c</author>
<author confidence="0.993233">Marie-Francine</author>
<affiliation confidence="0.998668">Department of Computer</affiliation>
<address confidence="0.648309">KU Leuven,</address>
<abstract confidence="0.9997549">We propose a simple yet effective approach to learning bilingual word embeddings (BWEs) from non-parallel document-aligned data (based on the omnipresent skip-gram model), and its application to bilingual lexicon induction (BLI). We demonstrate the utility of the induced BWEs in the BLI task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="1769" citStr="Baroni et al., 2014" startWordPosition="255" endWordPosition="258">., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela s</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1107" citStr="Bengio et al., 2003" startWordPosition="151" endWordPosition="154">strate the utility of the induced BWEs in the BLI task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performanc</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="11030" citStr="Bullinaria and Levy, 2007" startWordPosition="1762" endWordPosition="1765">arable data: (1) BiLDA-BLI - A BLI model that relies on the induction of latent cross-lingual topics (Mimno et al., 2009) by the bilingual LDA model and represents words as probability distributions over these topics (Vuli´c et al., 2011). (2) Assoc-BLI - A BLI model that represents words as vectors of association norms (Roller and Schulte im Walde, 2013) over both vocabularies, where these norms are computed using a multilingual topic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Mo</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarath Chandar</author>
<author>Stanislas Lauly</author>
<author>Hugo Larochelle</author>
<author>Mitesh M Khapra</author>
<author>Balaraman Ravindran</author>
<author>Vikas C Raykar</author>
<author>Amrita Saha</author>
</authors>
<title>An autoencoder approach to learning bilingual word representations.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>1853--1861</pages>
<contexts>
<context position="2057" citStr="Chandar et al., 2014" startWordPosition="303" endWordPosition="306">ional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc. Ho</context>
<context position="3673" citStr="Chandar et al., 2014" startWordPosition="559" endWordPosition="562">s from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs (Vuli´c and Moens, 2013b). Our BLI model based on our novel BWEs significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other BLI models based on recently proposed BWE induction models (Gouws et al., 2014; Chandar et al., 2014). The focus of the work is on learning lexicons from documentaligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links). 719 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 719–725, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: The architecture of our BWE Skip-Gram model for learning bilingual word embeddings from document-aligned comparable data. Source language words and documents are</context>
<context position="10043" citStr="Chandar et al., 2014" startWordPosition="1597" endWordPosition="1600">n. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for all three training corpora with the following parameters from the word2vec package (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with 25 samples, and a subsampling rate of value 1e−4. All models are trained for 15 epochs. We have varied the number of embedding dimensions: d = 100, 200, 300, and have also trained the model with d = 40 to be directly comparable to pre-trained state-of-theart BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computations in the BLI task, we call our new BLI model BWESG+cos. Baseline BLI Models We compare BWESG+cos to a series of state-of-the-art BLI models from document-aligned comparable data: (1) BiLDA-BLI - A BLI model that relies on the induction of latent cross-lingual topics (Mimno et al., 2009) by the bilingual LDA model and represents words as probability distributions over these topics (Vuli´c et al., 2011).</context>
<context position="18435" citStr="Chandar et al., 2014" startWordPosition="2963" endWordPosition="2966">ly insignificant. It is important to mark that even the minimum Acci scores over these 10 different random shuffles are typically higher than the previous state-of-the-art baseline scores for large enough values for d and cs (compare the results in tab. 2 and fig. 2(a)-2(c)). A comparison with the BWESG model without shuffling (NOS on fig. 2) reveals that shuffling is useful even for larger cs-s. Exp III: BWESG+cos vs. BWE-Based BLI We also compare our BWESG BLI model with two other models that are most similar to ours in spirit, as they also induce shared cross-lingual word embedding spaces (Chandar et al., 2014; Gouws et al., 2014), proven superior to or on a par with the BLI model from (Mikolov et al., 2013b). We use their pre-trained BWEs (obtained from the authors) and report the BLI scores in tab. 2. To make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2014) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligne</context>
</contexts>
<marker>Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, Saha, 2014</marker>
<rawString>Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh M. Khapra, Balaraman Ravindran, Vikas C. Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In NIPS, pages 1853–1861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1135" citStr="Collobert and Weston, 2008" startWordPosition="155" endWordPosition="159"> the induced BWEs in the BLI task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of s</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irina Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>526--533</pages>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>´Eric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In ACL, pages 526–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Yoshua Bengio</author>
<author>Greg Corrado</author>
</authors>
<title>BilBOWA: Fast bilingual distributed representations without word alignments.</title>
<date>2014</date>
<location>CoRR, abs/1410.2455.</location>
<contexts>
<context position="2035" citStr="Gouws et al., 2014" startWordPosition="299" endWordPosition="302">aditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon in</context>
<context position="3650" citStr="Gouws et al., 2014" startWordPosition="555" endWordPosition="558">ngual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs (Vuli´c and Moens, 2013b). Our BLI model based on our novel BWEs significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other BLI models based on recently proposed BWE induction models (Gouws et al., 2014; Chandar et al., 2014). The focus of the work is on learning lexicons from documentaligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links). 719 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 719–725, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: The architecture of our BWE Skip-Gram model for learning bilingual word embeddings from document-aligned comparable data. Source language </context>
<context position="10020" citStr="Gouws et al., 2014" startWordPosition="1593" endWordPosition="1596"> models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for all three training corpora with the following parameters from the word2vec package (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with 25 samples, and a subsampling rate of value 1e−4. All models are trained for 15 epochs. We have varied the number of embedding dimensions: d = 100, 200, 300, and have also trained the model with d = 40 to be directly comparable to pre-trained state-of-theart BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computations in the BLI task, we call our new BLI model BWESG+cos. Baseline BLI Models We compare BWESG+cos to a series of state-of-the-art BLI models from document-aligned comparable data: (1) BiLDA-BLI - A BLI model that relies on the induction of latent cross-lingual topics (Mimno et al., 2009) by the bilingual LDA model and represents words as probability distributions over these topics</context>
<context position="18456" citStr="Gouws et al., 2014" startWordPosition="2967" endWordPosition="2970">s important to mark that even the minimum Acci scores over these 10 different random shuffles are typically higher than the previous state-of-the-art baseline scores for large enough values for d and cs (compare the results in tab. 2 and fig. 2(a)-2(c)). A comparison with the BWESG model without shuffling (NOS on fig. 2) reveals that shuffling is useful even for larger cs-s. Exp III: BWESG+cos vs. BWE-Based BLI We also compare our BWESG BLI model with two other models that are most similar to ours in spirit, as they also induce shared cross-lingual word embedding spaces (Chandar et al., 2014; Gouws et al., 2014), proven superior to or on a par with the BLI model from (Mikolov et al., 2013b). We use their pre-trained BWEs (obtained from the authors) and report the BLI scores in tab. 2. To make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2014) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligned non-parallel data p</context>
</contexts>
<marker>Gouws, Bengio, Corrado, 2014</marker>
<rawString>Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2014. BilBOWA: Fast bilingual distributed representations without word alignments. CoRR, abs/1410.2455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="8906" citStr="Haghighi et al., 2008" startWordPosition="1406" endWordPosition="1409">tal Setup Training Data We use comparable Wikipedia data introduced in (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13,696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18,898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7,612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013b), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 </context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In ACL, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual distributed representations without word alignment.</title>
<date>2014</date>
<booktitle>In ICLR.</booktitle>
<contexts>
<context position="1986" citStr="Hermann and Blunsom, 2014" startWordPosition="291" endWordPosition="294">sed a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual </context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014a. Multilingual distributed representations without word alignment. In ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual models for compositional distributed semantics.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>58--68</pages>
<contexts>
<context position="1986" citStr="Hermann and Blunsom, 2014" startWordPosition="291" endWordPosition="294">sed a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual </context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014b. Multilingual models for compositional distributed semantics. In ACL, pages 58–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Stephen Clark</author>
</authors>
<title>A systematic study of semantic vector space model parameters.</title>
<date>2014</date>
<booktitle>In CVSC Workshop at EACL,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="11461" citStr="Kiela and Clark, 2014" startWordPosition="1830" endWordPosition="1833">opic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NLEN) (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflecte</context>
</contexts>
<marker>Kiela, Clark, 2014</marker>
<rawString>Douwe Kiela and Stephen Clark. 2014. A systematic study of semantic vector space model parameters. In CVSC Workshop at EACL, pages 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1459--1474</pages>
<contexts>
<context position="1918" citStr="Klementiev et al., 2012" startWordPosition="279" endWordPosition="282">t studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, su</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In COLING, pages 1459–1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="1329" citStr="Levy and Goldberg, 2014" startWordPosition="185" endWordPosition="188">counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<journal>Transactions of the ACL,</journal>
<pages>3--211</pages>
<contexts>
<context position="1349" citStr="Levy et al., 2015" startWordPosition="189" endWordPosition="192">etting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et </context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the ACL, 3:211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Hieu Pham</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word representations with monolingual quality in mind.</title>
<date>2015</date>
<booktitle>In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,</booktitle>
<pages>151--159</pages>
<contexts>
<context position="2098" citStr="Luong et al., 2015" startWordPosition="311" endWordPosition="314">t the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc. However, all these models critically requir</context>
</contexts>
<marker>Luong, Pham, Manning, 2015</marker>
<rawString>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 151–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In ICLR Workshop Papers.</booktitle>
<contexts>
<context position="1157" citStr="Mikolov et al., 2013" startWordPosition="160" endWordPosition="163"> task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni </context>
<context position="4885" citStr="Mikolov et al., 2013" startWordPosition="742" endWordPosition="745">ents are drawn as gray boxes, while target language words and documents are drawn as blue boxes. The right side of the figure (separated by a vertical dashed line) illustrates how a pseudo-bilingual document is constructed from a pair of two aligned documents; two documents are first merged, and then words in the pseudo-bilingual document are randomly shuffled to ensure that both source and target language words occur as context words. 2 Model Architecture In the following architecture description, we assume that the reader is familiar with the main assumptions and training procedure of SGNS (Mikolov et al., 2013a; Mikolov et al., 2013c). We extend the SGNS model to work with bilingual document-aligned comparable data. An overview of our architecture for learning BWEs from such comparable data is given in fig. 1. Let us assume that we possess a documentaligned comparable corpus which is defined as C = {d1, d2, ... , dN} = {(dS1 , dT1 ), (dS2 , d2 T), ..., (dS N, dTD)}, where dj = (dSj , dTj ) denotes a pair of aligned documents in the source language LS and the target language LT, respectively, and N is the number of documents in the corpus. V S and V T are vocabularies associated with languages LS an</context>
<context position="9638" citStr="Mikolov et al., 2013" startWordPosition="1526" endWordPosition="1529">orpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for all three training corpora with the following parameters from the word2vec package (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with 25 samples, and a subsampling rate of value 1e−4. All models are trained for 15 epochs. We have varied the number of embedding dimensions: d = 100, 200, 300, and have also trained the model with d = 40 to be directly comparable to pre-trained state-of-theart BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computation</context>
<context position="18534" citStr="Mikolov et al., 2013" startWordPosition="2983" endWordPosition="2986"> random shuffles are typically higher than the previous state-of-the-art baseline scores for large enough values for d and cs (compare the results in tab. 2 and fig. 2(a)-2(c)). A comparison with the BWESG model without shuffling (NOS on fig. 2) reveals that shuffling is useful even for larger cs-s. Exp III: BWESG+cos vs. BWE-Based BLI We also compare our BWESG BLI model with two other models that are most similar to ours in spirit, as they also induce shared cross-lingual word embedding spaces (Chandar et al., 2014; Gouws et al., 2014), proven superior to or on a par with the BLI model from (Mikolov et al., 2013b). We use their pre-trained BWEs (obtained from the authors) and report the BLI scores in tab. 2. To make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2014) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligned non-parallel data produces significantly higher BLI scores for IT-EN and ES-EN with sufficiently </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In ICLR Workshop Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<location>CoRR, abs/1309.4168.</location>
<contexts>
<context position="1157" citStr="Mikolov et al., 2013" startWordPosition="160" endWordPosition="163"> task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni </context>
<context position="4885" citStr="Mikolov et al., 2013" startWordPosition="742" endWordPosition="745">ents are drawn as gray boxes, while target language words and documents are drawn as blue boxes. The right side of the figure (separated by a vertical dashed line) illustrates how a pseudo-bilingual document is constructed from a pair of two aligned documents; two documents are first merged, and then words in the pseudo-bilingual document are randomly shuffled to ensure that both source and target language words occur as context words. 2 Model Architecture In the following architecture description, we assume that the reader is familiar with the main assumptions and training procedure of SGNS (Mikolov et al., 2013a; Mikolov et al., 2013c). We extend the SGNS model to work with bilingual document-aligned comparable data. An overview of our architecture for learning BWEs from such comparable data is given in fig. 1. Let us assume that we possess a documentaligned comparable corpus which is defined as C = {d1, d2, ... , dN} = {(dS1 , dT1 ), (dS2 , d2 T), ..., (dS N, dTD)}, where dj = (dSj , dTj ) denotes a pair of aligned documents in the source language LS and the target language LT, respectively, and N is the number of documents in the corpus. V S and V T are vocabularies associated with languages LS an</context>
<context position="9638" citStr="Mikolov et al., 2013" startWordPosition="1526" endWordPosition="1529">orpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for all three training corpora with the following parameters from the word2vec package (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with 25 samples, and a subsampling rate of value 1e−4. All models are trained for 15 epochs. We have varied the number of embedding dimensions: d = 100, 200, 300, and have also trained the model with d = 40 to be directly comparable to pre-trained state-of-theart BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computation</context>
<context position="18534" citStr="Mikolov et al., 2013" startWordPosition="2983" endWordPosition="2986"> random shuffles are typically higher than the previous state-of-the-art baseline scores for large enough values for d and cs (compare the results in tab. 2 and fig. 2(a)-2(c)). A comparison with the BWESG model without shuffling (NOS on fig. 2) reveals that shuffling is useful even for larger cs-s. Exp III: BWESG+cos vs. BWE-Based BLI We also compare our BWESG BLI model with two other models that are most similar to ours in spirit, as they also induce shared cross-lingual word embedding spaces (Chandar et al., 2014; Gouws et al., 2014), proven superior to or on a par with the BLI model from (Mikolov et al., 2013b). We use their pre-trained BWEs (obtained from the authors) and report the BLI scores in tab. 2. To make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2014) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligned non-parallel data produces significantly higher BLI scores for IT-EN and ES-EN with sufficiently </context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1157" citStr="Mikolov et al., 2013" startWordPosition="160" endWordPosition="163"> task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni </context>
<context position="4885" citStr="Mikolov et al., 2013" startWordPosition="742" endWordPosition="745">ents are drawn as gray boxes, while target language words and documents are drawn as blue boxes. The right side of the figure (separated by a vertical dashed line) illustrates how a pseudo-bilingual document is constructed from a pair of two aligned documents; two documents are first merged, and then words in the pseudo-bilingual document are randomly shuffled to ensure that both source and target language words occur as context words. 2 Model Architecture In the following architecture description, we assume that the reader is familiar with the main assumptions and training procedure of SGNS (Mikolov et al., 2013a; Mikolov et al., 2013c). We extend the SGNS model to work with bilingual document-aligned comparable data. An overview of our architecture for learning BWEs from such comparable data is given in fig. 1. Let us assume that we possess a documentaligned comparable corpus which is defined as C = {d1, d2, ... , dN} = {(dS1 , dT1 ), (dS2 , d2 T), ..., (dS N, dTD)}, where dj = (dSj , dTj ) denotes a pair of aligned documents in the source language LS and the target language LT, respectively, and N is the number of documents in the corpus. V S and V T are vocabularies associated with languages LS an</context>
<context position="9638" citStr="Mikolov et al., 2013" startWordPosition="1526" endWordPosition="1529">orpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for all three training corpora with the following parameters from the word2vec package (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with 25 samples, and a subsampling rate of value 1e−4. All models are trained for 15 epochs. We have varied the number of embedding dimensions: d = 100, 200, 300, and have also trained the model with d = 40 to be directly comparable to pre-trained state-of-theart BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computation</context>
<context position="18534" citStr="Mikolov et al., 2013" startWordPosition="2983" endWordPosition="2986"> random shuffles are typically higher than the previous state-of-the-art baseline scores for large enough values for d and cs (compare the results in tab. 2 and fig. 2(a)-2(c)). A comparison with the BWESG model without shuffling (NOS on fig. 2) reveals that shuffling is useful even for larger cs-s. Exp III: BWESG+cos vs. BWE-Based BLI We also compare our BWESG BLI model with two other models that are most similar to ours in spirit, as they also induce shared cross-lingual word embedding spaces (Chandar et al., 2014; Gouws et al., 2014), proven superior to or on a par with the BLI model from (Mikolov et al., 2013b). We use their pre-trained BWEs (obtained from the authors) and report the BLI scores in tab. 2. To make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2014) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligned non-parallel data produces significantly higher BLI scores for IT-EN and ES-EN with sufficiently </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013c. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>880--889</pages>
<contexts>
<context position="10525" citStr="Mimno et al., 2009" startWordPosition="1682" endWordPosition="1685">ained the model with d = 40 to be directly comparable to pre-trained state-of-theart BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computations in the BLI task, we call our new BLI model BWESG+cos. Baseline BLI Models We compare BWESG+cos to a series of state-of-the-art BLI models from document-aligned comparable data: (1) BiLDA-BLI - A BLI model that relies on the induction of latent cross-lingual topics (Mimno et al., 2009) by the bilingual LDA model and represents words as probability distributions over these topics (Vuli´c et al., 2011). (2) Assoc-BLI - A BLI model that represents words as vectors of association norms (Roller and Schulte im Walde, 2013) over both vocabularies, where these norms are computed using a multilingual topic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and </context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In EMNLP, pages 880– 889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Semantic relations in bilingual lexicons.</title>
<date>2011</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>8</volume>
<issue>2</issue>
<pages>3</pages>
<marker>Peirsman, Pad´o, 2011</marker>
<rawString>Yves Peirsman and Sebastian Pad´o. 2011. Semantic relations in bilingual lexicons. ACM Transactions on Speech and Language Processing, 8(2):article 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>GloVe: Global vectors for word representation. In</title>
<date>2014</date>
<booktitle>EMNLP,</booktitle>
<pages>1532--1543</pages>
<contexts>
<context position="1184" citStr="Pennington et al., 2014" startWordPosition="164" endWordPosition="167">enchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., </context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In EMNLP, pages 1532–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Pascale Fung</author>
</authors>
<title>Rare word translation extraction from aligned comparable documents.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>1327--1335</pages>
<contexts>
<context position="8933" citStr="Prochasson and Fung, 2011" startWordPosition="1410" endWordPosition="1413"> We use comparable Wikipedia data introduced in (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13,696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18,898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7,612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013b), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for</context>
</contexts>
<marker>Prochasson, Fung, 2011</marker>
<rawString>Emmanuel Prochasson and Pascale Fung. 2011. Rare word translation extraction from aligned comparable documents. In ACL, pages 1327–1335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1146--1157</pages>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In EMNLP, pages 1146– 1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="9131" citStr="Schmid, 1994" startWordPosition="1445" endWordPosition="1446">sh Wikipedia article pairs (ES-EN), (ii) a collection of 18,898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7,612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013b), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for all three training corpora with the following parameters from the word2vec package (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hubert Soyer</author>
</authors>
<title>Pontus Stenetorp, and Akiko Aizawa.</title>
<date>2015</date>
<booktitle>In ICLR.</booktitle>
<marker>Soyer, 2015</marker>
<rawString>Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa. 2015. Leveraging monolingual data for crosslingual compositional word representations. In ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<date>2007</date>
<booktitle>Probabilistic topic models. Handbook of Latent Semantic Analysis,</booktitle>
<pages>427--7</pages>
<contexts>
<context position="11387" citStr="Steyvers and Griffiths, 2007" startWordPosition="1818" endWordPosition="1821">3) over both vocabularies, where these norms are computed using a multilingual topic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NLEN) (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by ha</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. Handbook of Latent Semantic Analysis, 427(7):424–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using label propagation.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>24--36</pages>
<contexts>
<context position="11615" citStr="Tamura et al., 2012" startWordPosition="1857" endWordPosition="1860">larity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NLEN) (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acca score, that is, the number of source language (ES/IT/NL) words ws from ground truth translation pairs for which the top ranked word cross-li</context>
<context position="14003" citStr="Tamura et al., 2012" startWordPosition="2197" endWordPosition="2200">r words for all 3 language pairs obtained using BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target language words are skipped (monolingual similarity); (2) only target language words (EN) are listed (cross-lingual similarity); (3) words from both languages are listed (multilingual similarity). EN words are given in italic. The correct one-to-one translation for each source word is marked by (+). lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vuli´c and Moens, 2013b). 4 Results and Discussion Exp 0: Qualitative Analysis Tab. 1 displays top 10 semantically similar words monolingually, across-languages and combined/multilingually for one ES, IT and NL word. The BWESG+cos model is able to find semantically coherent lists of words for all three directions of similarity (i.e., monolingual, cross-lingual, multilingual). In the combined (multilingual) ranked lists, words from both languages are represented as top similar words. This initial qualitative analysis already demonstrates the ability of BWESG to induce a shared cross-lingual e</context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2012</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2012. Bilingual lexicon extraction from comparable corpora using label propagation. In EMNLP, pages 24–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1475" citStr="Turney and Pantel, 2010" startWordPosition="207" endWordPosition="210">d BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 1 Introduction Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 20</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artifical Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Crosslingual semantic similarity of words as the similarity of their semantic word responses.</title>
<date>2013</date>
<booktitle>In NAACL,</booktitle>
<pages>106--116</pages>
<marker>Vuli´c, Moens, 2013</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2013a. Crosslingual semantic similarity of words as the similarity of their semantic word responses. In NAACL, pages 106–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else).</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1613--1624</pages>
<marker>Vuli´c, Moens, 2013</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2013b. A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else). In EMNLP, pages 1613–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings.</title>
<date>2015</date>
<booktitle>In SIGIR,</booktitle>
<note>to appear.</note>
<marker>Vuli´c, Moens, 2015</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2015. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In SIGIR, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Identifying word translations from comparable corpora using latent topic models.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>479--484</pages>
<marker>Vuli´c, De Smet, Moens, 2011</marker>
<rawString>Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens. 2011. Identifying word translations from comparable corpora using latent topic models. In ACL, pages 479–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="1936" citStr="Zou et al., 2013" startWordPosition="283" endWordPosition="286">erg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has occurred recently (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Gouws et al., 2014; Chandar et al., 2014; Soyer et al., 2015; Luong et al., 2015). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental </context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In EMNLP, pages 1393–1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>