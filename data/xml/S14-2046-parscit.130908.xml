<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046960">
<title confidence="0.9973635">
FBK-TR: Applying SVM with Multiple Linguistic Features for
Cross-Level Semantic Similarity
</title>
<author confidence="0.655268">
Ngoc Phuoc An Vo Tommaso Caselli Octavian Popescu
</author>
<affiliation confidence="0.793208">
Fondazione Bruno Kessler TrentoRISE Fondazione Bruno Kessler
University of Trento Trento, Italy Trento, Italy
</affiliation>
<address confidence="0.652763">
Trento, Italy t.caselli@trentorise.eu popescu@fbk.eu
</address>
<email confidence="0.995508">
ngoc@fbk.eu
</email>
<sectionHeader confidence="0.993786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953133333333">
Recently, the task of measuring seman-
tic similarity between given texts has
drawn much attention from the Natural
Language Processing community. Espe-
cially, the task becomes more interesting
when it comes to measuring the seman-
tic similarity between different-sized texts,
e.g paragraph-sentence, sentence-phrase,
phrase-word, etc. In this paper, we, the
FBK-TR team, describe our system par-
ticipating in Task 3 &amp;quot;Cross-Level Seman-
tic Similarity&amp;quot;, at SemEval 2014. We also
report the results obtained by our system,
compared to the baseline and other partic-
ipating systems in this task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963327524590164">
Measuring semantic text similarity has become a
hot trend in NLP as it can be applied to other
tasks, e.g. Information Retrieval, Paraphrasing,
Machine Translation Evaluation, Text Summariza-
tion, Question and Answering, and others. Several
approaches proposed to measure the semantic sim-
ilarity between given texts. The first approach is
based on vector space models (VSMs) (Meadow,
1992). A VSM transforms given texts into &amp;quot;bag-
of-words&amp;quot; and presents them as vectors. Then, it
deploys different distance metrics to compute the
closeness between vectors, which will return as
the distance or similarity between given texts. The
next well-known approach is using text-alignment.
By assuming that two given texts are semantically
similar, they could be aligned on word or phrase
levels. The alignment quality can serve as a simi-
larity measure. &amp;quot;It typically pairs words from the
two texts by maximizing the summation of the
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
word similarity of the resulting pairs&amp;quot; (Mihalcea
et al., 2006). In contrast, the third approach uses
machine learning techniques to learn models built
from different lexical, semantic and syntactic fea-
tures and then give predictions on degree of simi-
larity between given texts (Šari´c et al., 2012).
At SemEval 2014, the Task 3 &amp;quot;Cross-Level Se-
mantic Similarity&amp;quot; (Jurgens et al., 2014) is to eval-
uate the semantic similarity across different sizes
of texts, in particular, a larger-sized text is com-
pared to a smaller-sized one. The task consists
of four types of semantic similarity comparison:
paragraph to sentence, sentence to phrase, phrase
to word, and word to sense. The degree of similar-
ity ranges from 0 (different meanings) to 4 (simi-
lar meanings). For evaluation, systems were eval-
uated, first, within comparison type and second,
across all comparison types. Two methods are
used to evaluate between system outputs and gold
standard (human annotation), which are Pearson
correlation and Spearman’s rank correlation (rho).
The FBK-TR team participated in this task with
three different runs. In this paper, we present a
clear and comprehensive description of our sys-
tem which obtained competitive results. Our main
approach is using machine learning technique to
learn models from different lexical and semantic
features from train corpora to make prediction on
the test corpora. We used support vector machine
(SVM) regression model to solve the task.
The remainder of the paper is organized as fol-
lows. Section 2 presents the system overview.
Sections 3, 4 and 5 describe the Semantic Word
Similarity, String Similarity and other features, re-
spectively. Section 6 discusses about SVM ap-
proach. Section 7 presents the experiment settings
for each subtask. Finally, Sections 8 and 9 present
the evaluation and conclusion.
</bodyText>
<page confidence="0.97545">
284
</page>
<note confidence="0.830685">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 284–288,
Dublin, Ireland, August 23-24, 2014.
</note>
<figureCaption confidence="0.999655">
Figure 1: System Overview.
</figureCaption>
<sectionHeader confidence="0.953108" genericHeader="introduction">
2 System Overview
</sectionHeader>
<bodyText confidence="0.9999757">
Our system was built on different linguistic fea-
tures as shown in Figure 1. By constructing a
pipeline system, each linguistic feature can be
used independently or together with others to mea-
sure the semantic similarity of given texts as well
as to evaluate the significance of each feature to
the accuracy of system’s predictions. On top of
this, the system is expandable and scalable for
adopting more useful features aiming for improv-
ing the accuracy.
</bodyText>
<sectionHeader confidence="0.988942" genericHeader="method">
3 Semantic Word Similarity Measures
</sectionHeader>
<bodyText confidence="0.9999765">
At the lexical level, we built a simple, yet effec-
tive Semantic Word Similarity model consisting of
three components: WordNet similarity, Wikipedia
relatedness and Latent Semantic Analysis (LSA).
These components played important and compli-
mentary roles to each other.
</bodyText>
<subsectionHeader confidence="0.99876">
3.1 Data Processing
</subsectionHeader>
<bodyText confidence="0.999966285714286">
We used the TreeTagger tool (Schmid, 1994) to
extract Part-of-Speech (POS) from each given
text, then tokenize and lemmatize it. On the basis
of the POS tags, we only picked lemmas of con-
tent words (Nouns and Verbs) from the given texts
and then paired them up regarding to similar POS
tags.
</bodyText>
<subsectionHeader confidence="0.96503">
3.2 WordNet Similarity and Levenshtein
Distance
</subsectionHeader>
<bodyText confidence="0.99980775">
WordNet (Fellbaum, 1999) is a lexical database
for the English language in which words are
grouped into sets of synonyms (namely synsets,
each expressing a distinct concept) to provide
short, general definitions, and record the vari-
ous semantic relations between synsets. We used
Perdersen’s package WordNet:Similarity (Peder-
sen et al., 2004) to obtain similarity scores for
the lexical items covered in WordNet. Similarity
scores have been computed by means of the Lin
measure (Lin, 1998). The Lin measure is built on
Resnik’s measure of similarity (Resnik, 1995):
</bodyText>
<equation confidence="0.998542">
Simlin =
2 ∗ IC(LCS) (1 )
IC(concept1) + IC(concept2)
</equation>
<bodyText confidence="0.999987333333333">
where IC(LCS) is the information content (IC) of
the least common subsumer (LCS) of two con-
cepts.
To overcome the limit in coverage of WordNet,
we applied the Levenshtein distance (Levenshtein,
1966). The distance between two words is defined
by the minimum number of operations (insertions,
deletions and substitutions) needed to transform
one word into the other.
</bodyText>
<subsectionHeader confidence="0.998184">
3.3 Wikipedia Relatedness
</subsectionHeader>
<bodyText confidence="0.9999631">
Wikipedia Miner (Milne and Witten, 2013) is a
Java-based package developed for extracting se-
mantic information from Wikipedia. Through our
experiments, we observed that Wikipedia related-
ness plays an important role for providing extra
information to measure the semantic similarity be-
tween words. We used the package Wikipedia
Miner from University of Waikato (New Zealand)
to extract additional relatedness scores between
words.
</bodyText>
<subsectionHeader confidence="0.997604">
3.4 Latent Semantic Analysis (LSA)
</subsectionHeader>
<bodyText confidence="0.999978117647059">
We also took advantage from corpus-based ap-
proaches to measure the semantic similarity be-
tween words by using Latent Semantic Analysis
(LSA) technique (Landauer et al., 1998). LSA as-
sumes that similar and/or related words in terms
of meaning will occur in similar text contexts. In
general, a LSA matrix is built from a large cor-
pus. Rows in the matrix represent unique words
and columns represent paragraphs or documents.
The content of the matrix corresponds to the word
count per paragraph/document. Matrix size is then
reduced by means of Single Value Decomposition
(SVD) technique. Once the matrix has been ob-
tained, similarity and/or relatedness between the
words is computed by means of cosine values
(scaled between 0 and 1) for each word vector
in the matrix. Values close to 1 are assumed to
</bodyText>
<page confidence="0.990437">
285
</page>
<bodyText confidence="0.998948666666667">
be very similar/related, otherwise dissimilar. We
trained our LSA model on the British National
Corpus (BNC) 1 and Wikipedia 2 corpora.
</bodyText>
<sectionHeader confidence="0.875914" genericHeader="method">
4 String Similarity Measures
</sectionHeader>
<bodyText confidence="0.999725333333333">
The Longest Common Substring (LCS) is the
longest string in common between two or more
strings. Two given texts are considered similar if
they are overlapping/covering each other (e.g sen-
tence 1 covers a part of sentence 2, or otherwise).
We implemented a simple algorithm to extract the
LCS between two given texts. Then we divided the
LCS length by the product of normalized lengths
of two given texts and used it as a feature.
</bodyText>
<subsectionHeader confidence="0.99978">
4.1 Analysis Before and After LCS
</subsectionHeader>
<bodyText confidence="0.99965">
After extracting the LCS between two given texts,
we also considered the similarity for the parts be-
fore and after the LCS. The similarity between the
text portions before and after the LSC has been ob-
tained by means of the Lin measure and the Lev-
enshtein distance.
</bodyText>
<sectionHeader confidence="0.99564" genericHeader="method">
5 Other Features
</sectionHeader>
<bodyText confidence="0.99973525">
To take into account other levels of analysis for se-
mantic similarity between texts, we extended our
features by means of topic modeling and Named
Entities.
</bodyText>
<sectionHeader confidence="0.6840475" genericHeader="method">
5.1 Topic Modeling (Latent Dirichlet
Allocation - LDA)
</sectionHeader>
<bodyText confidence="0.999963333333333">
Topic modeling is a generative model of docu-
ments which allows to discover topics embedded
in a document collection and their balance in each
document. If two given texts are expressing the
same topic, they should be considered highly sim-
ilar. We applied topic modeling, particularly, La-
tent Dirichlet allocation (LDA) (Blei et al., 2003)
to predict the topics expressed by given texts.
The MALLET topic model package (McCal-
lum, 2002) is a Java-based tool used for inferring
hidden &amp;quot;topics&amp;quot; in new document collections us-
ing trained models. We used Mallet topic model-
ing tool to build different models using BNC and
Wikipedia corpora.
We noticed that, in LDA, the number of top-
ics plays an important role to fine grained predic-
tions. Hence, we built different models for differ-
ent numbers of topics, from minimum 20 topics to
</bodyText>
<footnote confidence="0.998186">
1http://www.natcorp.ox.ac.uk
2http://en.wikipedia.org/wiki/Wikipedia:Database_download
</footnote>
<bodyText confidence="0.999772714285714">
maximum 500 topics (20, 50, 100, 150, 200, 250,
300, 350, 400, 450 and 500). From the proportion
vectors (distribution of documents over topics) of
given texts, we applied three different measures to
compute the distance between each pair of texts,
which are Cosine similarity, Kullback-Leibler and
Jensen-Shannon divergences (Gella et al., 2013).
</bodyText>
<subsectionHeader confidence="0.997605">
5.2 Named-Entity Recognition (NER)
</subsectionHeader>
<bodyText confidence="0.999969083333333">
NER aims at identifying and classifying entities
in a text with respect to a predefined set of cate-
gories such as person names, organizations, loca-
tions, time expressions, quantities, monetary val-
ues, percentages, etc. By exploring the training
set, we observed that there are lot of texts in this
task containing named entities. We deployed the
Stanford Named Entity Recognizer tool (Finkel et
al., 2005) to extract the similar and overlapping
named entities between two given texts. Then we
divided the number of similar/overlapping named
entities by the sum length of two given texts.
</bodyText>
<sectionHeader confidence="0.969588" genericHeader="method">
6 Support Vector Machines (SVMs)
</sectionHeader>
<bodyText confidence="0.999936916666667">
Support vector machine (SVM) (Cortes and Vap-
nik, 1995) is a type of supervised learning ap-
proaches. We used the LibSVM package (Chang
and Lin, 2011) to learn models from the different
linguistic features described above. However, in
SVM the problem of finding optimal kernel pa-
rameters is critical and important for the learning
process. Hence, we used practical advice (Hsu et
al., 2003) for data scaling and a grid-search pro-
cess for finding the optimal parameters (C and
gamma) for building models. We trained the SVM
models in a regression framework.
</bodyText>
<sectionHeader confidence="0.988354" genericHeader="method">
7 Experiment Settings
</sectionHeader>
<bodyText confidence="0.999617384615385">
For subtasks paragraph-to-sentence and sentence-
to-phrase, since the length between two units is
completely different, we decided, first to apply
topic model to identify if two given texts are ex-
pressing a same topic. Furthermore, named enti-
ties play an important role in these subtasks. How-
ever, as there are many named entities which are
not English words and cannot be identified by the
NER tool, we developed a program to detect and
identify common words occurring in both given
texts. Then we continued to extract other lexical
and semantic features to measure the similarity be-
tween the two texts.
</bodyText>
<page confidence="0.994545">
286
</page>
<table confidence="0.9998745">
Team Para2Sent Para2Sent
(Pearson) (Spearman)
UNAL-NLP, run2 (ranked 1st) 0.837 0.820
ECNU, run1(ranked 1st) 0.834 0.821
FBK-TR, run2 0.77 0.775
FBK-TR, run3 0.759 0.770
FBK-TR, run1 0.751 0.759
Baseline (LCS) 0.527 0.613
</table>
<tableCaption confidence="0.910607">
Table 1: Results for paragraph-to-sentence.
</tableCaption>
<table confidence="0.999869125">
Team Sent2Phr Sent2Phr
(Pearson) (Spearman)
Meerkat_Mafia, 0.777 0.760
SuperSaiyan (ranked 1st)
FBK-TR, run3 0.702 0.695
FBK-TR, run1 0.685 0.681
FBK-TR, run2 0.648 0.642
Baseline (LCS) 0.562 0.626
</table>
<tableCaption confidence="0.990726">
Table 2: Results for sentence-to-phrase.
</tableCaption>
<bodyText confidence="0.999887666666667">
For the subtask word-to-sense, we used the Se-
mantic Word Similarity model which consists of
three components: WordNet similarity, Wikipedia
relatedness and LSA similarity (described in sec-
tion 3). For phrase-to-word, we extracted all
glosses of the given word, then computed the simi-
larity between the given phrase and each extracted
gloss. Finally, we selected the highest similarity
score for result.
</bodyText>
<sectionHeader confidence="0.990186" genericHeader="method">
8 Evaluations
</sectionHeader>
<bodyText confidence="0.999648">
As a result, we report our performance in the four
subtasks as follows.
</bodyText>
<subsectionHeader confidence="0.998929">
8.1 Subtasks: Paragraph-to-Sentence and
Sentence-to-Phrase
</subsectionHeader>
<bodyText confidence="0.9980505">
The evaluation results using Pearson and Spear-
man correlations show the difference between our
system and best system in these two subtasks in
the Tables 1 and 2.
</bodyText>
<table confidence="0.9992816">
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.811 0.742 0.415 0.356 2.324
(ranked 1st)
FBK-TR 0.759 0.702 0.305 0.155 1.95
Baseline 0.527 0.562 0.165 0.109 1.363
</table>
<tableCaption confidence="0.9922">
Table 3: Overall result using Pearson.
</tableCaption>
<table confidence="0.9999532">
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.801 0.728 0.424 0.344 2.297
(ranked 1st)
FBK-TR 0.770 0.695 0.298 0.150 1.913
Baseline 0.613 0.626 0.162 0.130 1.528
</table>
<tableCaption confidence="0.998643">
Table 4: Overall result using Spearman.
</tableCaption>
<subsectionHeader confidence="0.99633">
8.2 Subtasks: Phrase-to-Word and
Word-to-Sense
</subsectionHeader>
<bodyText confidence="0.999985625">
Even though we did not submit the results as
they looked very low, we report the scores for
the phrase-to-word and word-to-sense subtasks. In
the phrase-to-word subtask, we obtained a Pearson
score of 0.305 and Spearman value of 0.298. As
for the word-to-sense subtask, we scored 0.155 for
Pearson and 0.150 for Spearman.
Overall, with the submitted results for two sub-
tasks described in Section 8.1, our system’s runs
ranked 20th, 21st and 22nd among 38 participat-
ing systems. However, by taking into account the
un-submitted results for the two other subtasks,
our best run obtained 1.95 (Pearson correlation)
and 1.913 (Spearman correlation), which can be
ranked in the top 10 among 38 systems (figures
are reported in Table 3 and 4).
</bodyText>
<sectionHeader confidence="0.990875" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999981076923077">
In this paper, we describe our system participating
in the Task 3, at SemEval 2014. We present a com-
pact system using machine learning approach (par-
ticularly, SVMs) to learn models from a set of lex-
ical and semantic features to predict the degree of
similarity between different-sized texts. Although
we only submitted the results for two out of four
subtasks, we obtained competitive results among
the other participants. For future work, we are
planning to increase the number of topics in LDA,
as more fine-grained topics should allow predict-
ing better similarity scores. Finally, we will inves-
tigate more on the use of syntactic features.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994744375">
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning research, 3:993–1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
</reference>
<page confidence="0.983836">
287
</page>
<reference confidence="0.998396338461538">
Vector Networks. Machine learning, 20(3):273–
297.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370.
Spandana Gella, Bahar Salehi, Marco Lui, Karl
Grieser, Paul Cook, and Timothy Baldwin. 2013.
Unimelb_nlp-core: Integrating predictions from
multiple domains and feature sets for estimating se-
mantic textual similarity. Atlanta, Georgia, USA,
page 207.
Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al.
2003. A Practical Guide to Support Vector Classifi-
cation.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014) August 23-24, 2014, Dublin,
Ireland.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse processes, 25(2-3):259–284.
Vladimir I Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions and Reversals.
In Soviet physics doklady, volume 10, page 707.
Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In ICML, volume 98, pages 296–
304.
Andrew Kachites McCallum. 2002. Mallet: A Ma-
chine Learning for Language Toolkit.
Charles T Meadow. 1992. Text Information Retrieval
Systems. Academic Press, Inc., Orlando, FL, USA.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In AAAI, vol-
ume 6, pages 775–780.
David Milne and Ian H Witten. 2013. An Open-
Source Toolkit for Mining Wikipedia. Artificial In-
telligence, 194:222–239.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity -Measuring the Re-
latedness of Concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38–41.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. arXiv
preprint cmp-lg/9511007.
Frane Šari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan Šnajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Sys-
tems for Measuring Semantic Text Similarity. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 441–448.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
international conference on new methods in lan-
guage processing, volume 12, pages 44–49. Manch-
ester, UK.
</reference>
<page confidence="0.997108">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577371">
<title confidence="0.981828">FBK-TR: Applying SVM with Multiple Linguistic Features Cross-Level Semantic Similarity</title>
<author confidence="0.8340155">Ngoc Phuoc An Vo Tommaso Caselli Octavian Popescu Fondazione Bruno Kessler TrentoRISE Fondazione Bruno Kessler</author>
<affiliation confidence="0.998973">University of Trento Trento, Italy Trento, Italy</affiliation>
<address confidence="0.936928">Italy</address>
<email confidence="0.996053">ngoc@fbk.eu</email>
<abstract confidence="0.998399375">Recently, the task of measuring semantic similarity between given texts has drawn much attention from the Natural Language Processing community. Especially, the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts, e.g paragraph-sentence, sentence-phrase, phrase-word, etc. In this paper, we, the FBK-TR team, describe our system participating in Task 3 &amp;quot;Cross-Level Semantic Similarity&amp;quot;, at SemEval 2014. We also report the results obtained by our system, compared to the baseline and other participating systems in this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8942" citStr="Blei et al., 2003" startWordPosition="1396" endWordPosition="1399">s of the Lin measure and the Levenshtein distance. 5 Other Features To take into account other levels of analysis for semantic similarity between texts, we extended our features by means of topic modeling and Named Entities. 5.1 Topic Modeling (Latent Dirichlet Allocation - LDA) Topic modeling is a generative model of documents which allows to discover topics embedded in a document collection and their balance in each document. If two given texts are expressing the same topic, they should be considered highly similar. We applied topic modeling, particularly, Latent Dirichlet allocation (LDA) (Blei et al., 2003) to predict the topics expressed by given texts. The MALLET topic model package (McCallum, 2002) is a Java-based tool used for inferring hidden &amp;quot;topics&amp;quot; in new document collections using trained models. We used Mallet topic modeling tool to build different models using BNC and Wikipedia corpora. We noticed that, in LDA, the number of topics plays an important role to fine grained predictions. Hence, we built different models for different numbers of topics, from minimum 20 topics to 1http://www.natcorp.ox.ac.uk 2http://en.wikipedia.org/wiki/Wikipedia:Database_download maximum 500 topics (20, 5</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Allocation. The Journal of Machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="10669" citStr="Chang and Lin, 2011" startWordPosition="1664" endWordPosition="1667">ons, time expressions, quantities, monetary values, percentages, etc. By exploring the training set, we observed that there are lot of texts in this task containing named entities. We deployed the Stanford Named Entity Recognizer tool (Finkel et al., 2005) to extract the similar and overlapping named entities between two given texts. Then we divided the number of similar/overlapping named entities by the sum length of two given texts. 6 Support Vector Machines (SVMs) Support vector machine (SVM) (Cortes and Vapnik, 1995) is a type of supervised learning approaches. We used the LibSVM package (Chang and Lin, 2011) to learn models from the different linguistic features described above. However, in SVM the problem of finding optimal kernel parameters is critical and important for the learning process. Hence, we used practical advice (Hsu et al., 2003) for data scaling and a grid-search process for finding the optimal parameters (C and gamma) for building models. We trained the SVM models in a regression framework. 7 Experiment Settings For subtasks paragraph-to-sentence and sentenceto-phrase, since the length between two units is completely different, we decided, first to apply topic model to identify if</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>SupportVector Networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<pages>297</pages>
<contexts>
<context position="10575" citStr="Cortes and Vapnik, 1995" startWordPosition="1646" endWordPosition="1650"> a text with respect to a predefined set of categories such as person names, organizations, locations, time expressions, quantities, monetary values, percentages, etc. By exploring the training set, we observed that there are lot of texts in this task containing named entities. We deployed the Stanford Named Entity Recognizer tool (Finkel et al., 2005) to extract the similar and overlapping named entities between two given texts. Then we divided the number of similar/overlapping named entities by the sum length of two given texts. 6 Support Vector Machines (SVMs) Support vector machine (SVM) (Cortes and Vapnik, 1995) is a type of supervised learning approaches. We used the LibSVM package (Chang and Lin, 2011) to learn models from the different linguistic features described above. However, in SVM the problem of finding optimal kernel parameters is critical and important for the learning process. Hence, we used practical advice (Hsu et al., 2003) for data scaling and a grid-search process for finding the optimal parameters (C and gamma) for building models. We trained the SVM models in a regression framework. 7 Experiment Settings For subtasks paragraph-to-sentence and sentenceto-phrase, since the length be</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. SupportVector Networks. Machine learning, 20(3):273– 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1999</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="5221" citStr="Fellbaum, 1999" startWordPosition="796" endWordPosition="797">e, yet effective Semantic Word Similarity model consisting of three components: WordNet similarity, Wikipedia relatedness and Latent Semantic Analysis (LSA). These components played important and complimentary roles to each other. 3.1 Data Processing We used the TreeTagger tool (Schmid, 1994) to extract Part-of-Speech (POS) from each given text, then tokenize and lemmatize it. On the basis of the POS tags, we only picked lemmas of content words (Nouns and Verbs) from the given texts and then paired them up regarding to similar POS tags. 3.2 WordNet Similarity and Levenshtein Distance WordNet (Fellbaum, 1999) is a lexical database for the English language in which words are grouped into sets of synonyms (namely synsets, each expressing a distinct concept) to provide short, general definitions, and record the various semantic relations between synsets. We used Perdersen’s package WordNet:Similarity (Pedersen et al., 2004) to obtain similarity scores for the lexical items covered in WordNet. Similarity scores have been computed by means of the Lin measure (Lin, 1998). The Lin measure is built on Resnik’s measure of similarity (Resnik, 1995): Simlin = 2 ∗ IC(LCS) (1 ) IC(concept1) + IC(concept2) wher</context>
</contexts>
<marker>Fellbaum, 1999</marker>
<rawString>Christiane Fellbaum. 1999. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="10305" citStr="Finkel et al., 2005" startWordPosition="1604" endWordPosition="1607">ied three different measures to compute the distance between each pair of texts, which are Cosine similarity, Kullback-Leibler and Jensen-Shannon divergences (Gella et al., 2013). 5.2 Named-Entity Recognition (NER) NER aims at identifying and classifying entities in a text with respect to a predefined set of categories such as person names, organizations, locations, time expressions, quantities, monetary values, percentages, etc. By exploring the training set, we observed that there are lot of texts in this task containing named entities. We deployed the Stanford Named Entity Recognizer tool (Finkel et al., 2005) to extract the similar and overlapping named entities between two given texts. Then we divided the number of similar/overlapping named entities by the sum length of two given texts. 6 Support Vector Machines (SVMs) Support vector machine (SVM) (Cortes and Vapnik, 1995) is a type of supervised learning approaches. We used the LibSVM package (Chang and Lin, 2011) to learn models from the different linguistic features described above. However, in SVM the problem of finding optimal kernel parameters is critical and important for the learning process. Hence, we used practical advice (Hsu et al., 2</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spandana Gella</author>
<author>Bahar Salehi</author>
<author>Marco Lui</author>
<author>Karl Grieser</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Unimelb_nlp-core: Integrating predictions from multiple domains and feature sets for estimating semantic textual similarity.</title>
<date>2013</date>
<pages>207</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="9863" citStr="Gella et al., 2013" startWordPosition="1535" endWordPosition="1538">ticed that, in LDA, the number of topics plays an important role to fine grained predictions. Hence, we built different models for different numbers of topics, from minimum 20 topics to 1http://www.natcorp.ox.ac.uk 2http://en.wikipedia.org/wiki/Wikipedia:Database_download maximum 500 topics (20, 50, 100, 150, 200, 250, 300, 350, 400, 450 and 500). From the proportion vectors (distribution of documents over topics) of given texts, we applied three different measures to compute the distance between each pair of texts, which are Cosine similarity, Kullback-Leibler and Jensen-Shannon divergences (Gella et al., 2013). 5.2 Named-Entity Recognition (NER) NER aims at identifying and classifying entities in a text with respect to a predefined set of categories such as person names, organizations, locations, time expressions, quantities, monetary values, percentages, etc. By exploring the training set, we observed that there are lot of texts in this task containing named entities. We deployed the Stanford Named Entity Recognizer tool (Finkel et al., 2005) to extract the similar and overlapping named entities between two given texts. Then we divided the number of similar/overlapping named entities by the sum le</context>
</contexts>
<marker>Gella, Salehi, Lui, Grieser, Cook, Baldwin, 2013</marker>
<rawString>Spandana Gella, Bahar Salehi, Marco Lui, Karl Grieser, Paul Cook, and Timothy Baldwin. 2013. Unimelb_nlp-core: Integrating predictions from multiple domains and feature sets for estimating semantic textual similarity. Atlanta, Georgia, USA, page 207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Wei Hsu</author>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>A Practical Guide to Support Vector Classification.</title>
<date>2003</date>
<contexts>
<context position="10909" citStr="Hsu et al., 2003" startWordPosition="1702" endWordPosition="1705">et al., 2005) to extract the similar and overlapping named entities between two given texts. Then we divided the number of similar/overlapping named entities by the sum length of two given texts. 6 Support Vector Machines (SVMs) Support vector machine (SVM) (Cortes and Vapnik, 1995) is a type of supervised learning approaches. We used the LibSVM package (Chang and Lin, 2011) to learn models from the different linguistic features described above. However, in SVM the problem of finding optimal kernel parameters is critical and important for the learning process. Hence, we used practical advice (Hsu et al., 2003) for data scaling and a grid-search process for finding the optimal parameters (C and gamma) for building models. We trained the SVM models in a regression framework. 7 Experiment Settings For subtasks paragraph-to-sentence and sentenceto-phrase, since the length between two units is completely different, we decided, first to apply topic model to identify if two given texts are expressing a same topic. Furthermore, named entities play an important role in these subtasks. However, as there are many named entities which are not English words and cannot be identified by the NER tool, we developed</context>
</contexts>
<marker>Hsu, Chang, Lin, 2003</marker>
<rawString>Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al. 2003. A Practical Guide to Support Vector Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2014 Task 3: Cross-Level Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014)</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2444" citStr="Jurgens et al., 2014" startWordPosition="356" endWordPosition="359">ng the summation of the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ word similarity of the resulting pairs&amp;quot; (Mihalcea et al., 2006). In contrast, the third approach uses machine learning techniques to learn models built from different lexical, semantic and syntactic features and then give predictions on degree of similarity between given texts (Šari´c et al., 2012). At SemEval 2014, the Task 3 &amp;quot;Cross-Level Semantic Similarity&amp;quot; (Jurgens et al., 2014) is to evaluate the semantic similarity across different sizes of texts, in particular, a larger-sized text is compared to a smaller-sized one. The task consists of four types of semantic similarity comparison: paragraph to sentence, sentence to phrase, phrase to word, and word to sense. The degree of similarity ranges from 0 (different meanings) to 4 (similar meanings). For evaluation, systems were evaluated, first, within comparison type and second, across all comparison types. Two methods are used to evaluate between system outputs and gold standard (human annotation), which are Pearson cor</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. Semeval-2014 Task 3: Cross-Level Semantic Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014) August 23-24, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis. Discourse processes,</title>
<date>1998</date>
<pages>25--2</pages>
<contexts>
<context position="6848" citStr="Landauer et al., 1998" startWordPosition="1042" endWordPosition="1045">Milne and Witten, 2013) is a Java-based package developed for extracting semantic information from Wikipedia. Through our experiments, we observed that Wikipedia relatedness plays an important role for providing extra information to measure the semantic similarity between words. We used the package Wikipedia Miner from University of Waikato (New Zealand) to extract additional relatedness scores between words. 3.4 Latent Semantic Analysis (LSA) We also took advantage from corpus-based approaches to measure the semantic similarity between words by using Latent Semantic Analysis (LSA) technique (Landauer et al., 1998). LSA assumes that similar and/or related words in terms of meaning will occur in similar text contexts. In general, a LSA matrix is built from a large corpus. Rows in the matrix represent unique words and columns represent paragraphs or documents. The content of the matrix corresponds to the word count per paragraph/document. Matrix size is then reduced by means of Single Value Decomposition (SVD) technique. Once the matrix has been obtained, similarity and/or relatedness between the words is computed by means of cosine values (scaled between 0 and 1) for each word vector in the matrix. Value</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K Landauer, Peter W Foltz, and Darrell Laham. 1998. An Introduction to Latent Semantic Analysis. Discourse processes, 25(2-3):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals.</title>
<date>1966</date>
<booktitle>In Soviet physics doklady,</booktitle>
<volume>10</volume>
<pages>707</pages>
<contexts>
<context position="6016" citStr="Levenshtein, 1966" startWordPosition="922" endWordPosition="923">efinitions, and record the various semantic relations between synsets. We used Perdersen’s package WordNet:Similarity (Pedersen et al., 2004) to obtain similarity scores for the lexical items covered in WordNet. Similarity scores have been computed by means of the Lin measure (Lin, 1998). The Lin measure is built on Resnik’s measure of similarity (Resnik, 1995): Simlin = 2 ∗ IC(LCS) (1 ) IC(concept1) + IC(concept2) where IC(LCS) is the information content (IC) of the least common subsumer (LCS) of two concepts. To overcome the limit in coverage of WordNet, we applied the Levenshtein distance (Levenshtein, 1966). The distance between two words is defined by the minimum number of operations (insertions, deletions and substitutions) needed to transform one word into the other. 3.3 Wikipedia Relatedness Wikipedia Miner (Milne and Witten, 2013) is a Java-based package developed for extracting semantic information from Wikipedia. Through our experiments, we observed that Wikipedia relatedness plays an important role for providing extra information to measure the semantic similarity between words. We used the package Wikipedia Miner from University of Waikato (New Zealand) to extract additional relatedness</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. In Soviet physics doklady, volume 10, page 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity.</title>
<date>1998</date>
<booktitle>In ICML,</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<contexts>
<context position="5686" citStr="Lin, 1998" startWordPosition="868" endWordPosition="869">m the given texts and then paired them up regarding to similar POS tags. 3.2 WordNet Similarity and Levenshtein Distance WordNet (Fellbaum, 1999) is a lexical database for the English language in which words are grouped into sets of synonyms (namely synsets, each expressing a distinct concept) to provide short, general definitions, and record the various semantic relations between synsets. We used Perdersen’s package WordNet:Similarity (Pedersen et al., 2004) to obtain similarity scores for the lexical items covered in WordNet. Similarity scores have been computed by means of the Lin measure (Lin, 1998). The Lin measure is built on Resnik’s measure of similarity (Resnik, 1995): Simlin = 2 ∗ IC(LCS) (1 ) IC(concept1) + IC(concept2) where IC(LCS) is the information content (IC) of the least common subsumer (LCS) of two concepts. To overcome the limit in coverage of WordNet, we applied the Levenshtein distance (Levenshtein, 1966). The distance between two words is defined by the minimum number of operations (insertions, deletions and substitutions) needed to transform one word into the other. 3.3 Wikipedia Relatedness Wikipedia Miner (Milne and Witten, 2013) is a Java-based package developed fo</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In ICML, volume 98, pages 296– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<contexts>
<context position="9038" citStr="McCallum, 2002" startWordPosition="1413" endWordPosition="1415">ls of analysis for semantic similarity between texts, we extended our features by means of topic modeling and Named Entities. 5.1 Topic Modeling (Latent Dirichlet Allocation - LDA) Topic modeling is a generative model of documents which allows to discover topics embedded in a document collection and their balance in each document. If two given texts are expressing the same topic, they should be considered highly similar. We applied topic modeling, particularly, Latent Dirichlet allocation (LDA) (Blei et al., 2003) to predict the topics expressed by given texts. The MALLET topic model package (McCallum, 2002) is a Java-based tool used for inferring hidden &amp;quot;topics&amp;quot; in new document collections using trained models. We used Mallet topic modeling tool to build different models using BNC and Wikipedia corpora. We noticed that, in LDA, the number of topics plays an important role to fine grained predictions. Hence, we built different models for different numbers of topics, from minimum 20 topics to 1http://www.natcorp.ox.ac.uk 2http://en.wikipedia.org/wiki/Wikipedia:Database_download maximum 500 topics (20, 50, 100, 150, 200, 250, 300, 350, 400, 450 and 500). From the proportion vectors (distribution of</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A Machine Learning for Language Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles T Meadow</author>
</authors>
<title>Text Information Retrieval Systems.</title>
<date>1992</date>
<publisher>Academic Press, Inc.,</publisher>
<location>Orlando, FL, USA.</location>
<contexts>
<context position="1314" citStr="Meadow, 1992" startWordPosition="185" endWordPosition="186"> our system participating in Task 3 &amp;quot;Cross-Level Semantic Similarity&amp;quot;, at SemEval 2014. We also report the results obtained by our system, compared to the baseline and other participating systems in this task. 1 Introduction Measuring semantic text similarity has become a hot trend in NLP as it can be applied to other tasks, e.g. Information Retrieval, Paraphrasing, Machine Translation Evaluation, Text Summarization, Question and Answering, and others. Several approaches proposed to measure the semantic similarity between given texts. The first approach is based on vector space models (VSMs) (Meadow, 1992). A VSM transforms given texts into &amp;quot;bagof-words&amp;quot; and presents them as vectors. Then, it deploys different distance metrics to compute the closeness between vectors, which will return as the distance or similarity between given texts. The next well-known approach is using text-alignment. By assuming that two given texts are semantically similar, they could be aligned on word or phrase levels. The alignment quality can serve as a similarity measure. &amp;quot;It typically pairs words from the two texts by maximizing the summation of the This work is licensed under a Creative Commons Attribution 4.0 Inte</context>
</contexts>
<marker>Meadow, 1992</marker>
<rawString>Charles T Meadow. 1992. Text Information Retrieval Systems. Academic Press, Inc., Orlando, FL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="2122" citStr="Mihalcea et al., 2006" startWordPosition="305" endWordPosition="308"> the distance or similarity between given texts. The next well-known approach is using text-alignment. By assuming that two given texts are semantically similar, they could be aligned on word or phrase levels. The alignment quality can serve as a similarity measure. &amp;quot;It typically pairs words from the two texts by maximizing the summation of the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ word similarity of the resulting pairs&amp;quot; (Mihalcea et al., 2006). In contrast, the third approach uses machine learning techniques to learn models built from different lexical, semantic and syntactic features and then give predictions on degree of similarity between given texts (Šari´c et al., 2012). At SemEval 2014, the Task 3 &amp;quot;Cross-Level Semantic Similarity&amp;quot; (Jurgens et al., 2014) is to evaluate the semantic similarity across different sizes of texts, in particular, a larger-sized text is compared to a smaller-sized one. The task consists of four types of semantic similarity comparison: paragraph to sentence, sentence to phrase, phrase to word, and word</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An OpenSource Toolkit for Mining Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--222</pages>
<contexts>
<context position="6249" citStr="Milne and Witten, 2013" startWordPosition="954" endWordPosition="957">s have been computed by means of the Lin measure (Lin, 1998). The Lin measure is built on Resnik’s measure of similarity (Resnik, 1995): Simlin = 2 ∗ IC(LCS) (1 ) IC(concept1) + IC(concept2) where IC(LCS) is the information content (IC) of the least common subsumer (LCS) of two concepts. To overcome the limit in coverage of WordNet, we applied the Levenshtein distance (Levenshtein, 1966). The distance between two words is defined by the minimum number of operations (insertions, deletions and substitutions) needed to transform one word into the other. 3.3 Wikipedia Relatedness Wikipedia Miner (Milne and Witten, 2013) is a Java-based package developed for extracting semantic information from Wikipedia. Through our experiments, we observed that Wikipedia relatedness plays an important role for providing extra information to measure the semantic similarity between words. We used the package Wikipedia Miner from University of Waikato (New Zealand) to extract additional relatedness scores between words. 3.4 Latent Semantic Analysis (LSA) We also took advantage from corpus-based approaches to measure the semantic similarity between words by using Latent Semantic Analysis (LSA) technique (Landauer et al., 1998).</context>
</contexts>
<marker>Milne, Witten, 2013</marker>
<rawString>David Milne and Ian H Witten. 2013. An OpenSource Toolkit for Mining Wikipedia. Artificial Intelligence, 194:222–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity -Measuring the Relatedness of Concepts. In Demonstration Papers at HLT-NAACL</title>
<date>2004</date>
<pages>38--41</pages>
<contexts>
<context position="5539" citStr="Pedersen et al., 2004" startWordPosition="841" endWordPosition="845">f-Speech (POS) from each given text, then tokenize and lemmatize it. On the basis of the POS tags, we only picked lemmas of content words (Nouns and Verbs) from the given texts and then paired them up regarding to similar POS tags. 3.2 WordNet Similarity and Levenshtein Distance WordNet (Fellbaum, 1999) is a lexical database for the English language in which words are grouped into sets of synonyms (namely synsets, each expressing a distinct concept) to provide short, general definitions, and record the various semantic relations between synsets. We used Perdersen’s package WordNet:Similarity (Pedersen et al., 2004) to obtain similarity scores for the lexical items covered in WordNet. Similarity scores have been computed by means of the Lin measure (Lin, 1998). The Lin measure is built on Resnik’s measure of similarity (Resnik, 1995): Simlin = 2 ∗ IC(LCS) (1 ) IC(concept1) + IC(concept2) where IC(LCS) is the information content (IC) of the least common subsumer (LCS) of two concepts. To overcome the limit in coverage of WordNet, we applied the Levenshtein distance (Levenshtein, 1966). The distance between two words is defined by the minimum number of operations (insertions, deletions and substitutions) n</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity -Measuring the Relatedness of Concepts. In Demonstration Papers at HLT-NAACL 2004, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy. arXiv preprint cmp-lg/9511007.</title>
<date>1995</date>
<contexts>
<context position="5761" citStr="Resnik, 1995" startWordPosition="880" endWordPosition="881"> 3.2 WordNet Similarity and Levenshtein Distance WordNet (Fellbaum, 1999) is a lexical database for the English language in which words are grouped into sets of synonyms (namely synsets, each expressing a distinct concept) to provide short, general definitions, and record the various semantic relations between synsets. We used Perdersen’s package WordNet:Similarity (Pedersen et al., 2004) to obtain similarity scores for the lexical items covered in WordNet. Similarity scores have been computed by means of the Lin measure (Lin, 1998). The Lin measure is built on Resnik’s measure of similarity (Resnik, 1995): Simlin = 2 ∗ IC(LCS) (1 ) IC(concept1) + IC(concept2) where IC(LCS) is the information content (IC) of the least common subsumer (LCS) of two concepts. To overcome the limit in coverage of WordNet, we applied the Levenshtein distance (Levenshtein, 1966). The distance between two words is defined by the minimum number of operations (insertions, deletions and substitutions) needed to transform one word into the other. 3.3 Wikipedia Relatedness Wikipedia Miner (Milne and Witten, 2013) is a Java-based package developed for extracting semantic information from Wikipedia. Through our experiments, </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. arXiv preprint cmp-lg/9511007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Šari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
<author>Jan Šnajder</author>
<author>Bojana Dalbelo Basi´c</author>
</authors>
<title>Takelab: Systems for Measuring Semantic Text Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>441--448</pages>
<marker>Šari´c, Glavas, Karan, Šnajder, Basi´c, 2012</marker>
<rawString>Frane Šari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan Šnajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Systems for Measuring Semantic Text Similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of international conference on new methods in language processing,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="4899" citStr="Schmid, 1994" startWordPosition="742" endWordPosition="743">similarity of given texts as well as to evaluate the significance of each feature to the accuracy of system’s predictions. On top of this, the system is expandable and scalable for adopting more useful features aiming for improving the accuracy. 3 Semantic Word Similarity Measures At the lexical level, we built a simple, yet effective Semantic Word Similarity model consisting of three components: WordNet similarity, Wikipedia relatedness and Latent Semantic Analysis (LSA). These components played important and complimentary roles to each other. 3.1 Data Processing We used the TreeTagger tool (Schmid, 1994) to extract Part-of-Speech (POS) from each given text, then tokenize and lemmatize it. On the basis of the POS tags, we only picked lemmas of content words (Nouns and Verbs) from the given texts and then paired them up regarding to similar POS tags. 3.2 WordNet Similarity and Levenshtein Distance WordNet (Fellbaum, 1999) is a lexical database for the English language in which words are grouped into sets of synonyms (namely synsets, each expressing a distinct concept) to provide short, general definitions, and record the various semantic relations between synsets. We used Perdersen’s package Wo</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of international conference on new methods in language processing, volume 12, pages 44–49. Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>