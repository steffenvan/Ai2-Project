<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007737">
<sectionHeader confidence="0.799699" genericHeader="abstract">
COMPACT REPRESENTATIONS BY FINITE-STATE
TRANSDUCERS
</sectionHeader>
<subsectionHeader confidence="0.337456">
Mehryar Mohri
</subsectionHeader>
<address confidence="0.68672075">
Institut Gaspard Monge-LADL
Universite Marne-la-Vallee
2, rue de la Butte verte
93160 Noisy-le-Grand, FRANCE
</address>
<email confidence="0.686796">
Internet: mohriOuniv-mlvir
</email>
<sectionHeader confidence="0.97954" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999655888888889">
Finite-state transducers give efficient represen-
tations of many Natural Language phenomena.
They allow to account for complex lexicon restric-
tions encountered, without involving the use of a
large set of complex rules difficult to analyze. We
here show that these representations can be made
very compact, indicate how to perform the corre-
sponding minimization, and point out interesting
linguistic side-effects of this operation.
</bodyText>
<sectionHeader confidence="0.998367" genericHeader="method">
1. MOTIVATION
</sectionHeader>
<bodyText confidence="0.999950173913043">
Finite-state transducers constitute appropriate
representations of Natural Language phenomena.
Indeed, they have been shown to be sufficient tools
to describe morphological and phonetic forms of a
language (Karttunen et al., 1992; Kay and Ka-
plan, 1994). Transducers can then be viewed as
functions which map lexical representations to the
surface forms, or inflected forms to their phonetic
pronunciations, and vice versa. They allow to
avoid the use of a great set of complex rules of-
ten difficult to check, handle, or even understand.
Finite-state automata and transducers can
also be used to represent the syntactic constraints
of languages such as English or French (Kosken-
niemi, 1990; Mohri, 1993; Pereira, 1991; Roche,
1993). The syntactic analysis can then be reduced
to performing the intersection of two automata,
or to the application of a transducer to an au-
tomaton. However, whereas first results show that
the size of the syntactic transducer exceeds several
hundreds of thousands of states, no upper bound
has been proposed for it, as the representation of
all syntactic entries has not been done yet. Thus,
one may ask whether such representations could
succeed on a large scale.
It is therefore crucial to control or to limit
the size of these transducers in order to avoid a
blow up. Classic minimization algorithms permit
to reduce to the minimal the size of a determinis-
tic automaton recognizing a given language (Aho
et al., 1974). No similar algorithm has been pro-
posed in the case of sequential transducers, namely
transducers whose associated input automata are
deterministic.
We here briefly describe an algorithm which
allows to compute a minimal transducer, namely
one with the least number of states, from a given
subsequential transducer. In addition to the de-
sired property of minimization, the transducer ob-
tained in such a way has interesting linguistic
properties that we shall indicate. We have fully
implemented and experimented this algorithm in
the case of large scale dictionaries. In the last
section, we shall describe experiments and corre-
sponding results. They show this algorithm to be
very efficient.
</bodyText>
<sectionHeader confidence="0.998282" genericHeader="method">
2. ALGORITHM
</sectionHeader>
<bodyText confidence="0.989650625">
Our algorithm can be applied to any sequential
transducer T = (V,i,F, A, B,6, a) where: V is the
set of the states of T, i its initial state, F the set
of its final states, A and B respectively the input
and output alphabet of the transducer, 6 the state
transition function which maps V x A to V, and
a the output function which maps V x A to Bt.
With this definition, input labels are elements of
the alphabet, whereas output labels can be words.
Figure 1 gives an example of a sequential trans-
ducer.
Transducers can be considered as automata
over the alphabet A x Bt. Thus, considered as
such they can be submitted to the minimization
in the sense of automata. Notice however that
the application of the minimization algorithm for
automata does not permit to reduce the number
of states of the transducer T. We shall describe in
the following how the algorithm we propose allows
to reduce the number of states of this transducer.
This algorithm works in two stages. The first
one modifies only the output automaton associ-
ated with the given sequential transducer T. Thus,
we can denote by (V, i,F, A,B, 6, cr2) the trans-
</bodyText>
<page confidence="0.996957">
204
</page>
<figure confidence="0.99922425">
0 bth 0
0 d:d 0
0 b:c 0 cd %
a:a
C 7 C
b:b
b:db
e:d
</figure>
<figureCaption confidence="0.999987">
Figure 1. Transducer T.
</figureCaption>
<bodyText confidence="0.975435657894737">
ducer 1&apos;2 obtained after this first stage. Let P be
the function which maps V to 13* which associates
with each state q of T the greatest common prefix
of all the words which can be read on the output
labels of T from q to a final state. The value of
P(5) is for instance db since this is the greatest
common prefix of the labels of all output paths
leaving 3. In particular, if q is a final state then
P(q) is the empty word E. In order to simplify this
presentation, we shall assume in the following that
P(i)= E. The output function 02 of T2 is defined
by:
Vg E &apos;V, Va E A,
cr2(q, a) = (P(q))-1 a .(q, a)P (5 (q, a)).
Namely, the output labels of T are modified in
such a way that they include every letter which
would necessarily be read later on the following
transitions. Figure 2 illustrates these modifica-
tions.
T if beginning with the transition (0, 1). The out-
put label of the following transition of T2 is now
empty. Indeed, anything which could be read from
the transition (1,2) on the output labels has now
been included in the previous transition (0,1).
It is easy to show that the transducer T2 ob-
tained after the first stage is equivalent to T.
Namely, these two transducers correspond to the
same function mapping At to Bt. One may no-
tice, however, that unlike T this transducer can be
minimized in the sense of automata and that this
leads to a transducer with only six states. Figure
3 indicates the transducer T3 obtained in such a
way.
The second stage of our algorithm precisely
consists of the application of the minimization in
the sense of automata, that is, of merging equiv-
alent states of the transducer. It can be showed
that the application of the two presented stages to
</bodyText>
<figureCaption confidence="0.997945">
Figure 2. Transducer T2.
</figureCaption>
<bodyText confidence="0.9999875">
It shows the transducer T2 obtained from T by
performing the operations described above. Notice
that only the output labels of T have been mod-
ified. The output label a corresponding to the
transition linking states 0 and 1 of the transducer
has now become abcdb as this is the longest word
which is necessarily read from the initial state 0 of
a sequential transducer T systematically leads to
an equivalent sequential transducer with the min-
imal number of states (Mohri, 1994). Indeed, the
states of this minimal transducer can be charac-
terized by the following equivalence relation: two
states of a sequential transducer are equivalent if
and only if one can read the same words from
</bodyText>
<page confidence="0.991084">
205
</page>
<figure confidence="0.872641">
a:abcdb d:cdb
</figure>
<figureCaption confidence="0.999546">
Figure 3. Transducer T3.
</figureCaption>
<bodyText confidence="0.999489074626866">
these states using the left automaton associated
with this transducer (equivalence in the sense of
automata) and if the corresponding outputs from
these states differ by the same prefix for any word
leading to a final state. Thus, the described algo-
rithm can be considered as optimal.
Notice that we here only considered sequen-
tial transducers, but not all transducers represent-
ing sequential functions are sequential. However,
transducers which are not sequential though repre-
senting a sequential function can be determinized
using a procedure close to the one used for the de-
terminization of automata. The algorithm above
can then be applied to such determinized trans-
ducers.
The complexity of the application of a non
sequential transducer to a string is not linear.
This is not the case even for non-deterministic
automata. Indeed, recognizing a word w with
a non-deterministic automaton of IVI states each
containing at most e leaving transitions requires
0(011w&apos;) (see Aho et al., 1974). The application
of a non-sequential transducer is even more time
consuming, so the determinization of transducers
clearly improves their application. We have con-
sidered above sequential transducers, but trans-
ducers can be used in two ways. These transduc-
ers, although they allow linear time application
on left, are generally not sequential considered as
right input transducers. However, the first stage
of the presented algorithm constitutes a pseudo-
determinization of right input transducers. In-
deed, as right labels (outputs) are brought closer
to the initial state as much as possible, irrelevant
paths are sooner rejected.
Consider for example the string x = abcdbcdbe
and compare the application of transducers T and
Tg to this sequence on right input. Using the
transducer T, the first three letters of this se-
quence lead to the single state 5, but then reading
db leads to a set of states {1, 5, 6}. Thus, in or-
der to proceed with the recognition, one needs to
store this set and consider all possible transitions
or paths from its states. Using the transducer Tg
and reading abcdb give the single state 1. Hence,
although the right input transducer is not sequen-
tial, it still permits to reduce the number of paths
and states to visit. This can be considered as an-
other advantage of the method proposed for the
minimization of sequential transducers: not only
the transducer is sequential and minimal on one
side, but it is also pseudo-sequential on the other
side.
The representation of language often reveals
ambiguities. The sequential transducers we have
just described do not allow them. However, real
ambiguities encountered in Natural Language Pro-
cessing can be assumed to be finite and bounded
by an integer p. The use of the algorithm above
can be easily extended to the case of subsequential
transducers and even to a larger category of trans-
ducers which can represent ambiguities and which
we shall call p-subsequential transducers. These
transducers are provided with p final functions cfri,
(i E [1,29]) mapping F, the set of final states, to
B. Figure 4 gives an example of a 2-subsequential
transducer.
</bodyText>
<figureCaption confidence="0.959018">
Figure 4. 2-subsequential transducer 7&apos;4.
</figureCaption>
<bodyText confidence="0.9957045">
The application of these transducers to a
string x is similar to the one generally used for
sequential ones. It outputs a string correspond-
ing to the concatenation of consecutive labels en-
coutered. However, the output string obtained
once reaching state q must here be completed by
the (Â¢;(q) without reading any additional input let-
ter. The application of the transducer T4 to the
word abc for instance provides the two outputs
abca and abcb.
The extension of the use of the algorithm
above is easy. Indeed, in all cases p-subsequential
</bodyText>
<page confidence="0.997084">
206
</page>
<bodyText confidence="0.9992985">
transducers can be transformed into sequential
transducers by adding p new letters to the alpha-
bet A, and by replacing the p final functions by
transitions labeled with these new letters on in-
put and the corresponding values of the functions
on output. These transitions would leave the final
states and reach a newly created state which would
become the single final state of the transducer.
The minimal transducer associated with the 2-
subsequential transducer T4 is shown on figure 5.
It results from T4 by merging the states 2 and 4
after the first stage of pseudo-determinization.
</bodyText>
<figureCaption confidence="0.966894">
Figure 5. Minimal 2-subsequential transducer T5.
</figureCaption>
<bodyText confidence="0.999924166666667">
In the following section, we shall describe
some of the experiments we carried out and the
corresponding results. These experiments use the
notion of p-subsequential transducers just devel-
opped as they all deal with cases where ambigui-
ties appear.
</bodyText>
<sectionHeader confidence="0.9998005" genericHeader="method">
3. EXPERIMENTS, RESULTS,
AND PROPERTIES
</sectionHeader>
<bodyText confidence="0.998411928571429">
We have experimented the algorithm described
above by applying it to several large scale dictio-
naries. We have applied it to the transducer which
associates with each French word the set of its pho-
netic pronunciations. This transducer can be built
from a dictionary (DELAPF) of inflected forms of
French, each followed by its pronunciations (La-
porte, 1988). It can be easily transformed into
a sequential or p-subsequential transducer, where
p, the maximum number of ambiguities for this
transducer, is about four (about 30 words admit
4 different pronunciations). This requires that the
transducer be kept deterministic while new asso-
ciations are added to it.
The dictionary contains about 480.000 entries
of words and phonetic pronunciations and its size
is about 10 Mb. The whole minimization algo-
rithm, including building the transducer from the
dictionary and the compression of the final trans-
ducer, was quite fast: it took about 9 minutes
using a HP 9000/755 with 128 Mb of RAM. The
resulting transducer contains about 47.000 states
and 130.000 transitions. Since it is sequential, it
can be better compressed as one only needs to
store the set of its transitions. The minimal trans-
ducer obtained has been put in a compact form
occupying about 1,1 Mb. Also, as the transducer
is sequential, it allows faster recognition times.
In addition to the above results, the trans-
ducer obtained by this algorithm has interesting
properties. Indeed, when applied to an input word
w which may not be a French word this transducer
outputs the longest common prefix of the phonetic
transcriptions of all words beginning with w. The
input w = opio for instance, though it does not
constitute a French word, yields opjoman. Also,
w = opht gives oftalm. This property of mini-
mal transducers as defined above could be used in
applications such as OCR or spellchecking, in or-
der to restore the correct form of a word from its
beginning, or from the beginning of its pronunci-
ation.
</bodyText>
<tableCaption confidence="0.999194">
Table 1. Results of minimization experiments
</tableCaption>
<table confidence="0.999102222222222">
DELAPF FDELAF EDELAF
Initial size 9,6 Mb 22,3 Mb 3,5 Mb
Entries 480.000 780.000 145.000
Max. ambg 4 15 8
Final size 1,1 Mb 1,6 Mb 1 Mb
States 47.000 66.000 47.000
Transitions 130.000 195.000 115.000
Alphabet 13.500 20.000 14.000
Time spent 9&apos; 20&apos; 7&apos;
</table>
<bodyText confidence="0.999947962962963">
We have also performed the same experi-
ment using 2 other large dictionaries: French
(FDELAF) (Courtois, 1989) and English (EDE-
LAF) (Klarsfeld, 1991) dictionaries of inflected
forms. These dictionaries are made of associ-
ations of inflected forms and their correspond-
ing canonical representations. It took about 20
minutes constructing the 15-subsequential trans-
ducer associated with the French dictionary of
about 22 Mb. Here again, properties of the ob-
tained transducers seem interesting for various ap-
plications. Given the input w=transducte for in-
stance the transducer provides the output trans-
ducteur.N1:m. Thus, although w is not a cor-
rect French word, it provides two additional let-
ters completing this word, and indicates that it is
a masculine noun. Notice that no information is
given about the number of this noun as it can be
completed by an ending a or not. Analogous re-
sults were obtained using the English dictionary.
A part of them is illustrated by the table above.
It allows to compare the initial size of the file
representing these dictionaries and the size of the
equivalent transducers in memory (final size). The
third line of the table gives the maximum num-
ber of lexical ambiguities encountered in each dic-
tionary. The following lines indicate the number
</bodyText>
<page confidence="0.99098">
207
</page>
<bodyText confidence="0.999983714285714">
of states and transitions of the transducers and
also the size of the alphabet needed to represent
the output labels. These experiments show that
this size remains small compared to the number
of transitions. Hence, the use of an additional al-
phabet does not increase noticeably the size of the
transducer. Also notice that the time indicated
corresponds to the entire process of transforma-
tion of the file dictionaries into tranducers. This
includes of course the time spent for I/O&apos;s. We
have not tried to optimize these results. Several
available methods should help both to reduce the
size of the obtained transducers and the time spent
for the algorithm.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="conclusions">
4. CONCLUSION
</sectionHeader>
<bodyText confidence="0.999926">
We have informally described an algorithm which
allows to compact sequential transducers used in
the description of language. Experiments on large
scale dictionaries have proved this algorithm to be
efficient. In addition to its use in several applica-
tions, it could help to limit the growth of the size
of the representations of syntactic constraints.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998891930232558">
Aho, Alfred, John Hoperoft, Jeffery Ullman. 1974.
The design and analysis of computer algorithms.
Reading, Mass.: Addison Wesley.
Courtois, Blandine. 1989. DELAS: Diction-
naire Electronique du LADL pour les mots simples
du franais Technical Report, LADL, Paris, France.
Karttunen, Laura, Ronald M. Kaplan, and
Annie Zaenen. 1992. Two-level Morphology with
Composition. Proceedings of the fifteenth Inter-
national Conference on Computational Linguistics
(COLING&apos;92), Nantes, France, August.
Kay, Martin, and Ronald M. Kaplan. 1994.
Regular Models of Phonological Rule Systems. To
appear in Computational Linguistics.
Klarsfeld, Gaby. 1991. Dictionnaire mor-
phologique de Panglais. Technical Report, LADL,
Paris, France.
Koskenniemi Kimmo. 1990. Finite-state
Parsing and Disambiguation. Proceedings of the
thirteenth International Conference on Computa-
tional Linguistics (COLING &apos;90), Helsinki, Fin-
land.
Laporte, Eric. 1988. Methodes algorithmiques
et lexicales de phonetisation de textes. Ph.D the-
sis, Universite Paris 7, Paris, France.
Mohri, Mehryar. 1993. Analyse et
representation par automates de structures syntax-
iques composies. Ph.D thesis, Universite Paris 7,
Paris, France.
Mohri, Mehryar. 1994. Minimization of Se-
quential Transducers. Proceedings of Combinato-
rial Pattern Matchnig (CPM&apos;94), Springer-Verlag,
Berlin Heidelberg New York. Also Submitted to
Theoretical Computer Science.
Pereira, Fernando C. N. 1991. Finite-
State Approximation of Phrase Structure Gram-
mars. Proceedings of the 29th Annual Meeting
of the Association for Computational Linguistics
(A CL &apos;91), Berkeley, California.
Roche Emmanuel. 1993. Analyse syntax-
ique transformationnelle du frangais par transduc-
teur et lexique-grammaire. Ph.D thesis, Universite
Paris 7, Paris, France.
</reference>
<page confidence="0.988993">
208
</page>
<figure confidence="0.283659">
Mehryar MOHRI
Institut Gaspard Monge
Universite Marne-la-Vallee
2, Rue de la Butte Verte
93166 NOISY-LE-GRAND CEDEX
FRANCE
Ph: 33 (1) 49 32 60 54
Fax: 33 (1) 43 04 16 05
</figure>
<page confidence="0.987227">
209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000456">
<title confidence="0.993091">COMPACT REPRESENTATIONS BY FINITE-STATE TRANSDUCERS</title>
<author confidence="0.961937">Mehryar Mohri</author>
<affiliation confidence="0.974861">Institut Gaspard Monge-LADL Universite Marne-la-Vallee</affiliation>
<address confidence="0.9944415">2, rue de la Butte verte 93160 Noisy-le-Grand, FRANCE</address>
<email confidence="0.984634">Internet:mohriOuniv-mlvir</email>
<abstract confidence="0.999002037800688">Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation. 1. MOTIVATION Finite-state transducers constitute appropriate representations of Natural Language phenomena. Indeed, they have been shown to be sufficient tools to describe morphological and phonetic forms of a language (Karttunen et al., 1992; Kay and Ka- 1994). Transducers can then be viewed functions which map lexical representations to the surface forms, or inflected forms to their phonetic pronunciations, and vice versa. They allow to avoid the use of a great set of complex rules often difficult to check, handle, or even understand. Finite-state automata and transducers can also be used to represent the syntactic constraints of languages such as English or French (Koskenniemi, 1990; Mohri, 1993; Pereira, 1991; Roche, 1993). The syntactic analysis can then be reduced to performing the intersection of two automata, or to the application of a transducer to an automaton. However, whereas first results show that the size of the syntactic transducer exceeds several hundreds of thousands of states, no upper bound has been proposed for it, as the representation of all syntactic entries has not been done yet. Thus, one may ask whether such representations could succeed on a large scale. It is therefore crucial to control or to limit the size of these transducers in order to avoid a blow up. Classic minimization algorithms permit to reduce to the minimal the size of a deterministic automaton recognizing a given language (Aho et al., 1974). No similar algorithm has been proposed in the case of sequential transducers, namely transducers whose associated input automata are deterministic. We here briefly describe an algorithm which allows to compute a minimal transducer, namely one with the least number of states, from a given subsequential transducer. In addition to the desired property of minimization, the transducer obtained in such a way has interesting linguistic properties that we shall indicate. We have fully implemented and experimented this algorithm in the case of large scale dictionaries. In the last section, we shall describe experiments and corresponding results. They show this algorithm to be very efficient. 2. ALGORITHM Our algorithm can be applied to any sequential = (V,i,F, A, B,6, V is the of the states of i initial set its final states, the input output alphabet of the transducer, state transition function which maps V x A to V, and output function which maps V x A to With this definition, input labels are elements of the alphabet, whereas output labels can be words. Figure 1 gives an example of a sequential transducer. Transducers can be considered as automata the alphabet A x considered as such they can be submitted to the minimization sense of automata. Notice however that the application of the minimization algorithm for automata does not permit to reduce the number states of the transducer shall describe in the following how the algorithm we propose allows reduce the number states of this transducer. This algorithm works in two stages. The first one modifies only the output automaton associwith the given sequential transducer can denote by (V, A,B, trans- 204 bth0 0 d:d 0 0 b:c 0 cd % a:a 7 b:b b:db e:d 1. Transducer 1&apos;2 obtained after this first stage. Let function which maps 13* which associates each state greatest common prefix of all the words which can be read on the output of a final state. The value of is for instance this is the greatest common prefix of the labels of all output paths 3. In particular, if is a state then the empty word order to simplify this presentation, we shall assume in the following that E. output function 02 of defined by: Va A, = a a)P (5 (q, a)). the output labels of modified in such a way that they include every letter which would necessarily be read later on the following transitions. Figure 2 illustrates these modifications. beginning with the transition (0, 1). The outlabel of the following transition of now empty. Indeed, anything which could be read from the transition (1,2) on the output labels has now been included in the previous transition (0,1). is easy to show that the transducer obafter the first stage is equivalent to Namely, these two transducers correspond to the same function mapping At to Bt. One may nohowever, that unlike transducer can be minimized in the sense of automata and that this leads to a transducer with only six states. Figure indicates the transducer in such a way. The second stage of our algorithm precisely consists of the application of the minimization in the sense of automata, that is, of merging equivalent states of the transducer. It can be showed that the application of the two presented stages to 2. Transducer shows the transducer from performing the operations described above. Notice only the output labels of been modified. The output label a corresponding to the transition linking states 0 and 1 of the transducer now become this is the longest word which is necessarily read from the initial state 0 of sequential transducer leads to an equivalent sequential transducer with the minimal number of states (Mohri, 1994). Indeed, the states of this minimal transducer can be characterized by the following equivalence relation: two states of a sequential transducer are equivalent if and only if one can read the same words from 205 a:abcdb d:cdb 3. Transducer these states using the left automaton associated with this transducer (equivalence in the sense of automata) and if the corresponding outputs from these states differ by the same prefix for any word to final Thus, the described algorithm can be considered as optimal. Notice that we here only considered sequential transducers, but not all transducers representing sequential functions are sequential. However, transducers which are not sequential though representing a sequential function can be determinized using a procedure close to the one used for the determinization of automata. The algorithm above can then be applied to such determinized transducers. The complexity of the application of a non sequential transducer to a string is not linear. This is not the case even for non-deterministic automata. Indeed, recognizing a word w with a non-deterministic automaton of IVI states each containing at most e leaving transitions requires 0(011w&apos;) (see Aho et al., 1974). The application of a non-sequential transducer is even more time consuming, so the determinization of transducers clearly improves their application. We have considered above sequential transducers, but transducers can be used in two ways. These transducers, although they allow linear time application on left, are generally not sequential considered as right input transducers. However, the first stage of the presented algorithm constitutes a pseudodeterminization of right input transducers. Indeed, as right labels (outputs) are brought closer to the initial state as much as possible, irrelevant paths are sooner rejected. for example the string = abcdbcdbe compare the application of transducers this sequence on right input. Using the first three letters of this sequence lead to the single state 5, but then reading to a set of states {1, 5, 6}. Thus, in order to proceed with the recognition, one needs to store this set and consider all possible transitions paths from its states. Using the transducer reading the single state 1. Hence, although the right input transducer is not sequential, it still permits to reduce the number of paths and states to visit. This can be considered as another advantage of the method proposed for the minimization of sequential transducers: not only the transducer is sequential and minimal on one side, but it is also pseudo-sequential on the other side. The representation of language often reveals ambiguities. The sequential transducers we have just described do not allow them. However, real ambiguities encountered in Natural Language Processing can be assumed to be finite and bounded an integer use of the algorithm above can be easily extended to the case of subsequential transducers and even to a larger category of transducers which can represent ambiguities and which shall call transducers. are provided with functions cfri, mapping set of final states, to 4 gives an example of a 2-subsequential transducer. Figure 4. 2-subsequential transducer 7&apos;4. The application of these transducers to a similar to the one generally used for sequential ones. It outputs a string corresponding to the concatenation of consecutive labels encoutered. However, the output string obtained reaching state here be completed by reading any additional input let- The application of the transducer the instance provides the two outputs The extension of the use of the algorithm above is easy. Indeed, in all cases p-subsequential 206 transducers can be transformed into sequential by adding letters to the alpha- A, and by replacing the functions by transitions labeled with these new letters on input and the corresponding values of the functions on output. These transitions would leave the final states and reach a newly created state which would become the single final state of the transducer. The minimal transducer associated with the 2transducer shown on figure 5. results from merging the states 2 and 4 after the first stage of pseudo-determinization. 5. Minimal 2-subsequential transducer In the following section, we shall describe some of the experiments we carried out and the corresponding results. These experiments use the notion of p-subsequential transducers just developped as they all deal with cases where ambiguities appear. 3. EXPERIMENTS, RESULTS, AND PROPERTIES We have experimented the algorithm described above by applying it to several large scale dictionaries. We have applied it to the transducer which associates with each French word the set of its phonetic pronunciations. This transducer can be built from a dictionary (DELAPF) of inflected forms of French, each followed by its pronunciations (Laporte, 1988). It can be easily transformed into a sequential or p-subsequential transducer, where maximum number of ambiguities for this transducer, is about four (about 30 words admit 4 different pronunciations). This requires that the transducer be kept deterministic while new associations are added to it. The dictionary contains about 480.000 entries of words and phonetic pronunciations and its size is about 10 Mb. The whole minimization algorithm, including building the transducer from the dictionary and the compression of the final transducer, was quite fast: it took about 9 minutes using a HP 9000/755 with 128 Mb of RAM. The resulting transducer contains about 47.000 states and 130.000 transitions. Since it is sequential, it can be better compressed as one only needs to store the set of its transitions. The minimal transducer obtained has been put in a compact form occupying about 1,1 Mb. Also, as the transducer is sequential, it allows faster recognition times. In addition to the above results, the transducer obtained by this algorithm has interesting properties. Indeed, when applied to an input word w which may not be a French word this transducer outputs the longest common prefix of the phonetic transcriptions of all words beginning with w. The w = instance, though it does not constitute a French word, yields opjoman. Also, = property of minimal transducers as defined above could be used in applications such as OCR or spellchecking, in order to restore the correct form of a word from its beginning, or from the beginning of its pronunciation.</abstract>
<note confidence="0.889830444444445">Table 1. Results of minimization experiments DELAPF FDELAF EDELAF Initial size 9,6 Mb 22,3 Mb 3,5 Mb Entries 480.000 780.000 145.000 Max. ambg 4 15 8 Final size 1,1 Mb 1,6 Mb 1 Mb States 47.000 66.000 47.000 Transitions 130.000 195.000 115.000 Alphabet 13.500 20.000 14.000</note>
<abstract confidence="0.992019588235294">Time spent 9&apos; 20&apos; 7&apos; We have also performed the same experiment using 2 other large dictionaries: French (FDELAF) (Courtois, 1989) and English (EDE- LAF) (Klarsfeld, 1991) dictionaries of inflected forms. These dictionaries are made of associations of inflected forms and their corresponding canonical representations. It took about 20 minutes constructing the 15-subsequential transducer associated with the French dictionary of about 22 Mb. Here again, properties of the obtained transducers seem interesting for various applications. Given the input w=transducte for inthe transducer provides the output transalthough w is not a correct French word, it provides two additional letters completing this word, and indicates that it is a masculine noun. Notice that no information is given about the number of this noun as it can be by an ending not. Analogous results were obtained using the English dictionary. A part of them is illustrated by the table above. It allows to compare the initial size of the file representing these dictionaries and the size of the equivalent transducers in memory (final size). The third line of the table gives the maximum number of lexical ambiguities encountered in each dictionary. The following lines indicate the number 207 of states and transitions of the transducers and also the size of the alphabet needed to represent the output labels. These experiments show that this size remains small compared to the number of transitions. Hence, the use of an additional alphabet does not increase noticeably the size of the transducer. Also notice that the time indicated corresponds to the entire process of transformation of the file dictionaries into tranducers. This includes of course the time spent for I/O&apos;s. We have not tried to optimize these results. Several available methods should help both to reduce the size of the obtained transducers and the time spent for the algorithm. 4. CONCLUSION We have informally described an algorithm which allows to compact sequential transducers used in the description of language. Experiments on large scale dictionaries have proved this algorithm to be efficient. In addition to its use in several applications, it could help to limit the growth of the size of the representations of syntactic constraints.</abstract>
<note confidence="0.9491111875">REFERENCES Aho, Alfred, John Hoperoft, Jeffery Ullman. 1974. The design and analysis of computer algorithms. Reading, Mass.: Addison Wesley. Blandine. 1989. Dictiondu LADL mots simples du franais Technical Report, LADL, Paris, France. Karttunen, Laura, Ronald M. Kaplan, and Annie Zaenen. 1992. Two-level Morphology with of the fifteenth International Conference on Computational Linguistics France, August. Kay, Martin, and Ronald M. Kaplan. 1994. Regular Models of Phonological Rule Systems. To Computational Linguistics. Gaby. 1991. Dictionnaire mor-</note>
<affiliation confidence="0.868554">de Panglais. Report, LADL,</affiliation>
<address confidence="0.859187">Paris, France.</address>
<author confidence="0.302569">Finite-state</author>
<abstract confidence="0.7346708">and Disambiguation. of the thirteenth International Conference on Computa- Linguistics (COLING &apos;90), Finland. Eric. 1988. algorithmiques lexicales de phonetisation de textes. thesis, Universite Paris 7, Paris, France. Mehryar. 1993. et de structures syntaxcomposies. thesis, Universite Paris 7,</abstract>
<address confidence="0.3517765">Paris, France. Mohri, Mehryar. 1994. Minimization of Se-</address>
<affiliation confidence="0.86432525">Transducers. of Combinato- Pattern Matchnig (CPM&apos;94), Berlin Heidelberg New York. Also Submitted to Theoretical Computer Science.</affiliation>
<note confidence="0.885241">Fernando C. N. 1991. Finite- State Approximation of Phrase Structure Gramof the 29th Annual Meeting of the Association for Computational Linguistics CL &apos;91), California. Emmanuel. 1993. syntaxtransformationnelle du frangais transducet lexique-grammaire. thesis, Universite Paris 7, Paris, France. 208</note>
<title confidence="0.933984">Mehryar MOHRI</title>
<author confidence="0.934657">Institut Gaspard Monge</author>
<affiliation confidence="0.999332">Universite Marne-la-Vallee</affiliation>
<address confidence="0.943845666666667">2, Rue de la Butte Verte 93166 NOISY-LE-GRAND CEDEX FRANCE</address>
<phone confidence="0.977833">Ph: 33 (1) 49 32 60 54 Fax: 33 (1) 43 04 16 05</phone>
<intro confidence="0.422321">209</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred Aho</author>
<author>John Hoperoft</author>
<author>Jeffery Ullman</author>
</authors>
<title>The design and analysis of computer algorithms.</title>
<date>1974</date>
<publisher>Addison Wesley.</publisher>
<location>Reading, Mass.:</location>
<contexts>
<context position="2106" citStr="Aho et al., 1974" startWordPosition="319" endWordPosition="322">application of a transducer to an automaton. However, whereas first results show that the size of the syntactic transducer exceeds several hundreds of thousands of states, no upper bound has been proposed for it, as the representation of all syntactic entries has not been done yet. Thus, one may ask whether such representations could succeed on a large scale. It is therefore crucial to control or to limit the size of these transducers in order to avoid a blow up. Classic minimization algorithms permit to reduce to the minimal the size of a deterministic automaton recognizing a given language (Aho et al., 1974). No similar algorithm has been proposed in the case of sequential transducers, namely transducers whose associated input automata are deterministic. We here briefly describe an algorithm which allows to compute a minimal transducer, namely one with the least number of states, from a given subsequential transducer. In addition to the desired property of minimization, the transducer obtained in such a way has interesting linguistic properties that we shall indicate. We have fully implemented and experimented this algorithm in the case of large scale dictionaries. In the last section, we shall d</context>
<context position="7463" citStr="Aho et al., 1974" startWordPosition="1261" endWordPosition="1264">enting sequential functions are sequential. However, transducers which are not sequential though representing a sequential function can be determinized using a procedure close to the one used for the determinization of automata. The algorithm above can then be applied to such determinized transducers. The complexity of the application of a non sequential transducer to a string is not linear. This is not the case even for non-deterministic automata. Indeed, recognizing a word w with a non-deterministic automaton of IVI states each containing at most e leaving transitions requires 0(011w&apos;) (see Aho et al., 1974). The application of a non-sequential transducer is even more time consuming, so the determinization of transducers clearly improves their application. We have considered above sequential transducers, but transducers can be used in two ways. These transducers, although they allow linear time application on left, are generally not sequential considered as right input transducers. However, the first stage of the presented algorithm constitutes a pseudodeterminization of right input transducers. Indeed, as right labels (outputs) are brought closer to the initial state as much as possible, irrelev</context>
</contexts>
<marker>Aho, Hoperoft, Ullman, 1974</marker>
<rawString>Aho, Alfred, John Hoperoft, Jeffery Ullman. 1974. The design and analysis of computer algorithms. Reading, Mass.: Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blandine Courtois</author>
</authors>
<title>DELAS: Dictionnaire Electronique du LADL pour les mots simples du franais</title>
<date>1989</date>
<tech>Technical Report,</tech>
<location>LADL, Paris, France.</location>
<contexts>
<context position="13474" citStr="Courtois, 1989" startWordPosition="2256" endWordPosition="2257">of minimal transducers as defined above could be used in applications such as OCR or spellchecking, in order to restore the correct form of a word from its beginning, or from the beginning of its pronunciation. Table 1. Results of minimization experiments DELAPF FDELAF EDELAF Initial size 9,6 Mb 22,3 Mb 3,5 Mb Entries 480.000 780.000 145.000 Max. ambg 4 15 8 Final size 1,1 Mb 1,6 Mb 1 Mb States 47.000 66.000 47.000 Transitions 130.000 195.000 115.000 Alphabet 13.500 20.000 14.000 Time spent 9&apos; 20&apos; 7&apos; We have also performed the same experiment using 2 other large dictionaries: French (FDELAF) (Courtois, 1989) and English (EDELAF) (Klarsfeld, 1991) dictionaries of inflected forms. These dictionaries are made of associations of inflected forms and their corresponding canonical representations. It took about 20 minutes constructing the 15-subsequential transducer associated with the French dictionary of about 22 Mb. Here again, properties of the obtained transducers seem interesting for various applications. Given the input w=transducte for instance the transducer provides the output transducteur.N1:m. Thus, although w is not a correct French word, it provides two additional letters completing this w</context>
</contexts>
<marker>Courtois, 1989</marker>
<rawString>Courtois, Blandine. 1989. DELAS: Dictionnaire Electronique du LADL pour les mots simples du franais Technical Report, LADL, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Karttunen</author>
<author>Ronald M Kaplan</author>
<author>Annie Zaenen</author>
</authors>
<title>Two-level Morphology with Composition.</title>
<date>1992</date>
<booktitle>Proceedings of the fifteenth International Conference on Computational Linguistics (COLING&apos;92),</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="885" citStr="Karttunen et al., 1992" startWordPosition="116" endWordPosition="119">ons of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation. 1. MOTIVATION Finite-state transducers constitute appropriate representations of Natural Language phenomena. Indeed, they have been shown to be sufficient tools to describe morphological and phonetic forms of a language (Karttunen et al., 1992; Kay and Kaplan, 1994). Transducers can then be viewed as functions which map lexical representations to the surface forms, or inflected forms to their phonetic pronunciations, and vice versa. They allow to avoid the use of a great set of complex rules often difficult to check, handle, or even understand. Finite-state automata and transducers can also be used to represent the syntactic constraints of languages such as English or French (Koskenniemi, 1990; Mohri, 1993; Pereira, 1991; Roche, 1993). The syntactic analysis can then be reduced to performing the intersection of two automata, or to </context>
</contexts>
<marker>Karttunen, Kaplan, Zaenen, 1992</marker>
<rawString>Karttunen, Laura, Ronald M. Kaplan, and Annie Zaenen. 1992. Two-level Morphology with Composition. Proceedings of the fifteenth International Conference on Computational Linguistics (COLING&apos;92), Nantes, France, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
<author>Ronald M Kaplan</author>
</authors>
<title>Regular Models of Phonological Rule Systems.</title>
<date>1994</date>
<note>To appear in Computational Linguistics.</note>
<contexts>
<context position="908" citStr="Kay and Kaplan, 1994" startWordPosition="120" endWordPosition="124">uage phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation. 1. MOTIVATION Finite-state transducers constitute appropriate representations of Natural Language phenomena. Indeed, they have been shown to be sufficient tools to describe morphological and phonetic forms of a language (Karttunen et al., 1992; Kay and Kaplan, 1994). Transducers can then be viewed as functions which map lexical representations to the surface forms, or inflected forms to their phonetic pronunciations, and vice versa. They allow to avoid the use of a great set of complex rules often difficult to check, handle, or even understand. Finite-state automata and transducers can also be used to represent the syntactic constraints of languages such as English or French (Koskenniemi, 1990; Mohri, 1993; Pereira, 1991; Roche, 1993). The syntactic analysis can then be reduced to performing the intersection of two automata, or to the application of a tr</context>
</contexts>
<marker>Kay, Kaplan, 1994</marker>
<rawString>Kay, Martin, and Ronald M. Kaplan. 1994. Regular Models of Phonological Rule Systems. To appear in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaby Klarsfeld</author>
</authors>
<title>Dictionnaire morphologique de Panglais.</title>
<date>1991</date>
<tech>Technical Report,</tech>
<location>LADL, Paris, France.</location>
<contexts>
<context position="13513" citStr="Klarsfeld, 1991" startWordPosition="2262" endWordPosition="2263">e could be used in applications such as OCR or spellchecking, in order to restore the correct form of a word from its beginning, or from the beginning of its pronunciation. Table 1. Results of minimization experiments DELAPF FDELAF EDELAF Initial size 9,6 Mb 22,3 Mb 3,5 Mb Entries 480.000 780.000 145.000 Max. ambg 4 15 8 Final size 1,1 Mb 1,6 Mb 1 Mb States 47.000 66.000 47.000 Transitions 130.000 195.000 115.000 Alphabet 13.500 20.000 14.000 Time spent 9&apos; 20&apos; 7&apos; We have also performed the same experiment using 2 other large dictionaries: French (FDELAF) (Courtois, 1989) and English (EDELAF) (Klarsfeld, 1991) dictionaries of inflected forms. These dictionaries are made of associations of inflected forms and their corresponding canonical representations. It took about 20 minutes constructing the 15-subsequential transducer associated with the French dictionary of about 22 Mb. Here again, properties of the obtained transducers seem interesting for various applications. Given the input w=transducte for instance the transducer provides the output transducteur.N1:m. Thus, although w is not a correct French word, it provides two additional letters completing this word, and indicates that it is a masculi</context>
</contexts>
<marker>Klarsfeld, 1991</marker>
<rawString>Klarsfeld, Gaby. 1991. Dictionnaire morphologique de Panglais. Technical Report, LADL, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koskenniemi Kimmo</author>
</authors>
<title>Finite-state Parsing and Disambiguation.</title>
<date>1990</date>
<booktitle>Proceedings of the thirteenth International Conference on Computational Linguistics (COLING &apos;90),</booktitle>
<location>Helsinki, Finland.</location>
<marker>Kimmo, 1990</marker>
<rawString>Koskenniemi Kimmo. 1990. Finite-state Parsing and Disambiguation. Proceedings of the thirteenth International Conference on Computational Linguistics (COLING &apos;90), Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Laporte</author>
</authors>
<title>Methodes algorithmiques et lexicales de phonetisation de textes.</title>
<date>1988</date>
<tech>Ph.D thesis,</tech>
<institution>Universite Paris</institution>
<location>Paris, France.</location>
<contexts>
<context position="11457" citStr="Laporte, 1988" startWordPosition="1916" endWordPosition="1918">hall describe some of the experiments we carried out and the corresponding results. These experiments use the notion of p-subsequential transducers just developped as they all deal with cases where ambiguities appear. 3. EXPERIMENTS, RESULTS, AND PROPERTIES We have experimented the algorithm described above by applying it to several large scale dictionaries. We have applied it to the transducer which associates with each French word the set of its phonetic pronunciations. This transducer can be built from a dictionary (DELAPF) of inflected forms of French, each followed by its pronunciations (Laporte, 1988). It can be easily transformed into a sequential or p-subsequential transducer, where p, the maximum number of ambiguities for this transducer, is about four (about 30 words admit 4 different pronunciations). This requires that the transducer be kept deterministic while new associations are added to it. The dictionary contains about 480.000 entries of words and phonetic pronunciations and its size is about 10 Mb. The whole minimization algorithm, including building the transducer from the dictionary and the compression of the final transducer, was quite fast: it took about 9 minutes using a HP</context>
</contexts>
<marker>Laporte, 1988</marker>
<rawString>Laporte, Eric. 1988. Methodes algorithmiques et lexicales de phonetisation de textes. Ph.D thesis, Universite Paris 7, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Analyse et representation par automates de structures syntaxiques composies.</title>
<date>1993</date>
<tech>Ph.D thesis,</tech>
<institution>Universite Paris</institution>
<location>Paris, France.</location>
<contexts>
<context position="1357" citStr="Mohri, 1993" startWordPosition="195" endWordPosition="196">ena. Indeed, they have been shown to be sufficient tools to describe morphological and phonetic forms of a language (Karttunen et al., 1992; Kay and Kaplan, 1994). Transducers can then be viewed as functions which map lexical representations to the surface forms, or inflected forms to their phonetic pronunciations, and vice versa. They allow to avoid the use of a great set of complex rules often difficult to check, handle, or even understand. Finite-state automata and transducers can also be used to represent the syntactic constraints of languages such as English or French (Koskenniemi, 1990; Mohri, 1993; Pereira, 1991; Roche, 1993). The syntactic analysis can then be reduced to performing the intersection of two automata, or to the application of a transducer to an automaton. However, whereas first results show that the size of the syntactic transducer exceeds several hundreds of thousands of states, no upper bound has been proposed for it, as the representation of all syntactic entries has not been done yet. Thus, one may ask whether such representations could succeed on a large scale. It is therefore crucial to control or to limit the size of these transducers in order to avoid a blow up. </context>
</contexts>
<marker>Mohri, 1993</marker>
<rawString>Mohri, Mehryar. 1993. Analyse et representation par automates de structures syntaxiques composies. Ph.D thesis, Universite Paris 7, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Minimization of Sequential Transducers.</title>
<date>1994</date>
<booktitle>Proceedings of Combinatorial Pattern Matchnig (CPM&apos;94),</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>Berlin Heidelberg New York.</location>
<note>Also Submitted to Theoretical Computer Science.</note>
<contexts>
<context position="6219" citStr="Mohri, 1994" startWordPosition="1063" endWordPosition="1064">ing equivalent states of the transducer. It can be showed that the application of the two presented stages to Figure 2. Transducer T2. It shows the transducer T2 obtained from T by performing the operations described above. Notice that only the output labels of T have been modified. The output label a corresponding to the transition linking states 0 and 1 of the transducer has now become abcdb as this is the longest word which is necessarily read from the initial state 0 of a sequential transducer T systematically leads to an equivalent sequential transducer with the minimal number of states (Mohri, 1994). Indeed, the states of this minimal transducer can be characterized by the following equivalence relation: two states of a sequential transducer are equivalent if and only if one can read the same words from 205 a:abcdb d:cdb Figure 3. Transducer T3. these states using the left automaton associated with this transducer (equivalence in the sense of automata) and if the corresponding outputs from these states differ by the same prefix for any word leading to a final state. Thus, the described algorithm can be considered as optimal. Notice that we here only considered sequential transducers, but</context>
</contexts>
<marker>Mohri, 1994</marker>
<rawString>Mohri, Mehryar. 1994. Minimization of Sequential Transducers. Proceedings of Combinatorial Pattern Matchnig (CPM&apos;94), Springer-Verlag, Berlin Heidelberg New York. Also Submitted to Theoretical Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
</authors>
<title>FiniteState Approximation of Phrase Structure Grammars.</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (A CL &apos;91),</booktitle>
<location>Berkeley, California.</location>
<contexts>
<context position="1372" citStr="Pereira, 1991" startWordPosition="197" endWordPosition="198">they have been shown to be sufficient tools to describe morphological and phonetic forms of a language (Karttunen et al., 1992; Kay and Kaplan, 1994). Transducers can then be viewed as functions which map lexical representations to the surface forms, or inflected forms to their phonetic pronunciations, and vice versa. They allow to avoid the use of a great set of complex rules often difficult to check, handle, or even understand. Finite-state automata and transducers can also be used to represent the syntactic constraints of languages such as English or French (Koskenniemi, 1990; Mohri, 1993; Pereira, 1991; Roche, 1993). The syntactic analysis can then be reduced to performing the intersection of two automata, or to the application of a transducer to an automaton. However, whereas first results show that the size of the syntactic transducer exceeds several hundreds of thousands of states, no upper bound has been proposed for it, as the representation of all syntactic entries has not been done yet. Thus, one may ask whether such representations could succeed on a large scale. It is therefore crucial to control or to limit the size of these transducers in order to avoid a blow up. Classic minimiz</context>
</contexts>
<marker>Pereira, 1991</marker>
<rawString>Pereira, Fernando C. N. 1991. FiniteState Approximation of Phrase Structure Grammars. Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (A CL &apos;91), Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roche Emmanuel</author>
</authors>
<title>Analyse syntaxique transformationnelle du frangais par transducteur et lexique-grammaire.</title>
<date>1993</date>
<tech>Ph.D thesis,</tech>
<institution>Universite Paris</institution>
<location>Paris, France.</location>
<marker>Emmanuel, 1993</marker>
<rawString>Roche Emmanuel. 1993. Analyse syntaxique transformationnelle du frangais par transducteur et lexique-grammaire. Ph.D thesis, Universite Paris 7, Paris, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>