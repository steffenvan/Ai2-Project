<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012314">
<title confidence="0.97511">
The DCU Dependency-Based Metric in WMT-MetricsMATR 2010
</title>
<author confidence="0.997502">
Yifan He Jinhua Du Andy Way Josef van Genabith
</author>
<affiliation confidence="0.992765666666667">
Centre for Next Generation Localisation
School of Computing
Dublin City University
</affiliation>
<address confidence="0.83138">
Dublin 9, Ireland
</address>
<email confidence="0.999139">
{yhe,jdu,away,josef}@computing.dcu.ie
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99985085">
We describe DCU’s LFG dependency-
based metric submitted to the shared eval-
uation task of WMT-MetricsMATR 2010.
The metric is built on the LFG F-structure-
based approach presented in (Owczarzak
et al., 2007). We explore the following
improvements on the original metric: 1)
we replace the in-house LFG parser with
an open source dependency parser that
directly parses strings into LFG depen-
dencies; 2) we add a stemming module
and unigram paraphrases to strengthen the
aligner; 3) we introduce a chunk penalty
following the practice of METEOR to re-
ward continuous matches; and 4) we intro-
duce and tune parameters to maximize the
correlation with human judgement. Exper-
iments show that these enhancements im-
prove the dependency-based metric’s cor-
relation with human judgement.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999776637931034">
String-based automatic evaluation metrics such as
BLEU (Papineni et al., 2002) have led directly
to quality improvements in machine translation
(MT). These metrics provide an alternative to ex-
pensive human evaluations, and enable tuning of
MT systems based on automatic evaluation results.
However, there is widespread recognition in
the MT community that string-based metrics are
not discriminative enough to reflect the translation
quality of today’s MT systems, many of which
have gone beyond pure string-based approaches
(cf. (Callison-Burch et al., 2006)).
With that in mind, a number of researchers have
come up with metrics which incorporate more so-
phisticated and linguistically motivated resources.
Examples include METEOR (Banerjee and Lavie,
2005; Lavie and Denkowski, 2009) and TERP
(Snover et al., 2010), both of which now uti-
lize stemming, WordNet and paraphrase informa-
tion. Experimental and evaluation campaign re-
sults have shown that these metrics can obtain bet-
ter correlation with human judgements than met-
rics that only use surface-level information.
Given that many of today’s MT systems incor-
porate some kind of syntactic information, it was
perhaps natural to use syntax in automatic MT
evaluation as well. This direction was first ex-
plored by (Liu and Gildea, 2005), who used syn-
tactic structure and dependency information to go
beyond the surface level matching.
Owczarzak et al. (2007) extended this line of
research with the use of a term-based encoding of
Lexical Functional Grammar (LFG:(Kaplan and
Bresnan, 1982)) labelled dependency graphs into
unordered sets of dependency triples, and calculat-
ing precision, recall, and F-score on the triple sets
corresponding to the translation and reference sen-
tences. With the addition of partial matching and
n-best parses, Owczarzak et al. (2007)’s method
considerably outperforms Liu and Gildea’s (2005)
w.r.t. correlation with human judgement.
The EDPM metric (Kahn et al., 2010) im-
proves this line of research by using arc labels
derived from a Probabilistic Context-Free Gram-
mar (PCFG) parse to replace the LFG labels,
showing that a PCFG parser is sufficient for pre-
processing, compared to a dependency parser in
(Liu and Gildea, 2005) and (Owczarzak et al.,
2007). EDPM also incorporates more information
sources: e.g. the parser confidence, the Porter
stemmer, WordNet synonyms and paraphrases.
Besides the metrics that rely solely on the de-
pendency structures, information from the depen-
dency parser is a component of some other metrics
that use more diverse resources, such as the textual
entailment-based metric of (Pado et al., 2009).
In this paper we extend the work of (Owczarzak
et al., 2007) in a different manner: we use an
</bodyText>
<page confidence="0.989614">
349
</page>
<note confidence="0.4539885">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999704625">
adapted version of the Malt parser (Nivre et al.,
2006) to produce 1-best LFG dependencies and
allow triple matches where the dependency la-
bels are different. We incorporate stemming, syn-
onym and paraphrase information as in (Kahn et
al., 2010), and at the same time introduce a chunk
penalty in the spirit of METEOR to penalize dis-
continuous matches. We sort the matches accord-
ing to the match level and the dependency type,
and weight the matches to maximize correlation
with human judgement.
The remainder of the paper is organized as fol-
lows. Section 2 reviews the dependency-based
metric. Sections 3, 4, 5 and 6 introduce our im-
provements on this metric. We report experimen-
tal results in Section 7 and conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.955303" genericHeader="introduction">
2 The Dependency-Based Metric
</sectionHeader>
<bodyText confidence="0.9991805">
In this section, we briefly review the metric pre-
sented in (Owczarzak et al., 2007).
</bodyText>
<subsectionHeader confidence="0.990248">
2.1 C-Structure and F-Structure in LFG
</subsectionHeader>
<bodyText confidence="0.999296888888889">
In Lexical Functional Grammar (Kaplan and Bres-
nan, 1982), a sentence is represented as both a hi-
erarchical c-(onstituent) structure which captures
the phrasal organization of a sentence, and a f-
(unctional) structure which captures the functional
relations between different parts of the sentence.
Our metric currently only relies on the f-structure,
which is encoded as labeled dependencies in our
metric.
</bodyText>
<subsectionHeader confidence="0.992661">
2.2 MT Evaluation as Dependency Triple
Matching
</subsectionHeader>
<bodyText confidence="0.999936923076923">
The basic method of (Owczarzak et al., 2007) can
be illustrated by the example in Table 1.
The metric in (Owczarzak et al., 2007) performs
triple matching over the Hyp- and Ref-Triples and
calculates the metric score using the F-score of
matching precision and recall. Let m be the num-
ber of matches, h be the number of triples in the
hypothesis and e be the number of triples in the
reference. Then we have the matching precision
P = m/h and recall R = m/e. The score of the
hypothesis in (Owczarzak et al., 2007) is the F-
score based on the precision and recall of match-
ing as in (1):
</bodyText>
<equation confidence="0.841449">
Fscore = PPR (1)
</equation>
<tableCaption confidence="0.993761">
Table 1: Sample Hypothesis and Reference
</tableCaption>
<table confidence="0.370451333333333">
Hypothesis
rice will be held talks in egypt next week
Hyp-Triples
adjunct(will, rice)
xcomp(will, be)
adjunct(talks, held)
xcomp(be, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
adjunct(talks, week)
Reference
rice to hold talks in egypt next week
Ref-Triples
obl(rice, to)
obj(hold, to)
adjunct(week, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
obj(hold, week)
</table>
<subsectionHeader confidence="0.997256">
2.3 Details of the Matching Strategy
</subsectionHeader>
<bodyText confidence="0.999977153846154">
(Owczarzak et al., 2007) uses several techniques
to facilitate triple matching. First of all, consider-
ing that the MT-generated hypotheses have vari-
able quality and are sometimes ungrammatical,
the metric will search the 50-best parses of both
the hypothesis and reference and use the pair that
has the highest F-score to compensate for parser
noise.
Secondly, the metric performs complete or par-
tial matching according to the dependency labels,
so the metric will find more matches on depen-
dency structures that are presumably more infor-
mative.
More specifically, for all except the LFG
Predicate-Only labeled triples of the form
dep(head, modifier), the method does not
allow a match if the dependency labels (deps)
are different, thus enforcing a complete match.
For the Predicate-Only dependencies, par-
tial matching is allowed: i.e. two triples are con-
sidered identical even if only the head or the
modifier are the same.
Finally, the metric also uses linguistic resources
for better coverage. Besides using WordNet syn-
onyms, the method also uses the lemmatized out-
put of the LFG parser, which is equivalent to using
</bodyText>
<page confidence="0.984335">
350
</page>
<bodyText confidence="0.9994835">
an English lemmatizer.
If we do not consider these additional lin-
guistic resources, the metric would find the fol-
lowing matches in the example in Table 1:
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next), as these three
triples appear both in the reference and in the hy-
pothesis.
</bodyText>
<subsectionHeader confidence="0.718918">
2.4 Points for Improvement
</subsectionHeader>
<bodyText confidence="0.971371">
We see several points for improvement from Table
1 and the analysis above.
</bodyText>
<listItem confidence="0.951296">
• More linguistic resources: we can use more
linguistic resources than WordNet in pursuit
of better coverage.
• Using the 1-best parse instead of 50-best
parses: the parsing model we currently use
does not produce k-best parses and using only
the 1-best parse significantly improves the
speed of triple matching. We allow ‘soft’
triple matches to capture the triple matches
which we might otherwise miss using the 1-
best parse.
• Rewarding continuous matches: it
would be more desirable to reflect
the fact that the 3 matching triples
adjunct(talks, in), obj(in,
egypt) and adjunct(week, next)
are continuous in Table 1.
</listItem>
<bodyText confidence="0.999765">
We introduce our improvements to the metric
in response to these observations in the following
sections.
</bodyText>
<sectionHeader confidence="0.989096" genericHeader="method">
3 Producing and Matching LFG
Dependency Triples
</sectionHeader>
<subsectionHeader confidence="0.999401">
3.1 The LFG Parser
</subsectionHeader>
<bodyText confidence="0.999927083333333">
The metric described in (Owczarzak et al., 2007)
uses the DCU LFG parser (Cahill et al., 2004)
to produce LFG dependency triples. The parser
uses a Penn treebank-trained parser to produce
c-structures (constituency trees) and an LFG f-
structure annotation algorithm on the c-structure
to obtain f-structures. In (Owczarzak et al., 2007),
triple matching on f-structures produced by this
paradigm correlates well with human judgement,
but this paradigm is not adequate for the WMT-
MetricsMatr evaluation in two respects: 1) the in-
house LFG annotation algorithm is not publicly
available and 2) the speed of this paradigm is not
satisfactory.
We instead use the Malt Parser1 (Nivre et al.,
2006) with a parsing model trained on LFG de-
pendencies to produce the f-structure triples. Our
collaborators2 first apply the LFG annotation algo-
rithm to the Penn Treebank training data to obtain
f-structures, and then the f-structures are converted
into dependency trees in CoNLL format to train
the parsing model. We use the liblinear (Fan et
al., 2008) classification module to for fast parsing
speed.
</bodyText>
<subsectionHeader confidence="0.999375">
3.2 Hard and Soft Dependency Matching
</subsectionHeader>
<bodyText confidence="0.999316076923077">
Currently our parser produces only the 1-best
outputs. Compared to the 50-best parses in
(Owczarzak et al., 2007), the 1-best parse limits
the number of triple matches that can be found. To
compensate for this, we allow triple matches that
have the same Head and Modifier to consti-
tute a match, even if their dependency labels are
different. Therefore for triples Dep1(Head1,
Mod1) and Dep2(Head2, Mod2), we allow
three types of match: a complete match if
the two triples are identical, a partial match if
Dep1=Dep2 and Head1=Head2, and a soft
match if Head1=Head2 and Mod1=Mod2.
</bodyText>
<sectionHeader confidence="0.953439" genericHeader="method">
4 Capturing Variations in Language
</sectionHeader>
<bodyText confidence="0.986675214285714">
In (Owczarzak et al., 2007), lexical variations at
the word-level are captured by WordNet. We
use a Porter stemmer and a unigram paraphrase
database to allow more lexical variations.
With these two resources combined, there are
four stages of word level matching in our sys-
tem: exact match, stem match, WordNet match and
unigram paraphrase match. The stemming mod-
ule uses Porter’s stemmer implementation3 and the
WordNet module uses the JAWS WordNet inter-
face.4 Our metric only considers unigram para-
phrases, which are extracted from the paraphrase
database in TERP5 using the script in the ME-
TEOR6 metric.
</bodyText>
<footnote confidence="0.992322">
1http://maltparser.org/index.html
2 ¨Ozlem C¸etino˘glu and Jennifer Foster at the National
Centre for Language Technology, Dublin City University
3http://tartarus.org/-martin/
PorterStemmer/
4http://lyle.smu.edu/-tspell/jaws/
index.html
5http://www.umiacs.umd.edu/-snover/
terp/
6http://www.cs.cmu.edu/-alavie/METEOR/
</footnote>
<page confidence="0.998444">
351
</page>
<sectionHeader confidence="0.966998" genericHeader="method">
5 Adding Chunk Penalty to the
Dependency-Based Metric
</sectionHeader>
<bodyText confidence="0.998632833333333">
The metric described in (Owczarzak et al., 2007)
does not explicitly consider word order and flu-
ency. METEOR, on the other hand, utilizes this in-
formation through a chunk penalty. We introduce
a chunk penalty to our dependency-based metric
following METEOR’s string-based approach.
Given a reference r = wr1...wrn, we denote
wri as ‘covered’ if it is the head or modifier of
a matched triple. We only consider the wris that
appear as head or modifier in the reference
triples. After this notation, we follow METEOR’s
approach by counting the number of chunks in
the reference string, where a chunk wrj...wrk is
a sequence of adjacent covered words in the refer-
ence. Using the hypothesis and reference in Ta-
ble 1 as an example, the three matched triples
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next) will cover a con-
tinuous word sequence in the reference (under-
lined), constituting one single chunk:
rice to hold talks (in) egypt next week
Based on this observation, we introduce a simi-
lar chunk penalty Pen as in METEOR in our met-
ric, as in 2:
</bodyText>
<equation confidence="0.985977">
Pen = y · ( #chunks (2)
#matches)�
</equation>
<bodyText confidence="0.999807">
where Q and -y are free parameters, which we tune
in Section 6.2. We add this penalty to the depen-
dency based metric (cf. Eq. (1)), as in Eq. (3).
</bodyText>
<equation confidence="0.896583">
score = (1 − Pen) · Fscore (3)
</equation>
<sectionHeader confidence="0.996251" genericHeader="method">
6 Parameter Tuning
</sectionHeader>
<subsectionHeader confidence="0.999992">
6.1 Parameters of the Metric
</subsectionHeader>
<bodyText confidence="0.999987333333333">
In our metric, dependency triple matches can be
categorized according to many criteria. We as-
sume that some matches are more critical than
others and encode the importance of matches by
weighting them differently. The final match will
be the sum of weighted matches, as in (4):
</bodyText>
<equation confidence="0.976573">
∑m = Atmt (4)
</equation>
<bodyText confidence="0.999610714285714">
where At and mt are the weight and number of
match category t. We categorize a triple match ac-
cording to three perspectives: 1) the level of match
L={complete, partial}; 2) the linguistic resource
used in matching R={exact, stem, WordNet, para-
phrase}; and 3) the type of dependency D. To
avoid too large a number of parameters, we only
allow a set of frequent dependency types, along
with the type other, which represents all the other
types and the type soft for soft matches. We have
D={app, subj, obj, poss, adjunct, topicrel, other,
soft}.
Therefore for each triple match m, we can have
the type of the match t E L x R x D.
</bodyText>
<subsectionHeader confidence="0.999854">
6.2 Tuning
</subsectionHeader>
<bodyText confidence="0.9999451">
In sum, we have the following parameters to tune
in our metric: precision weight α, chunk penalty
parameters Q, -y, and the match type weights
A1...An. We perform Powell’s line search (Press et
al., 2007) on the sufficient statistics of our metric
to find the set of parameters that maximizes Pear-
son’s p on the segment level. We perform the op-
timization on the MT06 portion of the NIST Met-
ricsMATR 2010 development set with 2-fold cross
validation.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.994123615384615">
We experiment with four settings of the metric:
HARD, SOFT, SOFTALL and WEIGHTED in or-
der to validate our enhancements. The first two
settings compare the effect of allowing/not al-
lowing soft matches, but only uses WordNet as
in (Owczarzak et al., 2007). The third setting ap-
plies our additional linguistic features and the final
setting tunes parameter weights for higher correla-
tion with human judgement.
We report Pearson’s r, Spearman’s p and
Kendall’s T on segment and system levels on the
NIST MetricsMATR 2010 development set using
Snover’s scoring tool.7
</bodyText>
<tableCaption confidence="0.998378">
Table 2: Correlation on the Segment Level
</tableCaption>
<table confidence="0.980884">
r p T
HARD 0.557 0.586 0.176
SOFT 0.600 0.634 0.213
SOFTALL 0.633 0.662 0.235
WEIGHTED 0.673 0.709 0.277
</table>
<bodyText confidence="0.9062025">
Table 2 shows that allowing soft triple matches
and using more linguistic features all lead
to higher correlation with human judgement.
Though the parameters might somehow overfit on
</bodyText>
<footnote confidence="0.990927">
7http://www.umiacs.umd.edu/˜snover/
terp/scoring/
</footnote>
<page confidence="0.994971">
352
</page>
<bodyText confidence="0.998733666666667">
the data set even if we apply cross validation, this
certainly confirms the necessity of weighing de-
pendency matches according to their types.
</bodyText>
<tableCaption confidence="0.999087">
Table 3: Correlation on the System Level
</tableCaption>
<table confidence="0.9986738">
r p Ir
HARD 0.948 0.905 0.786
SOFT 0.964 0.905 0.786
SOFTALL 0.975 0.976 0.929
WEIGHTED 0.989 1.000 1.000
</table>
<bodyText confidence="0.9955132">
When considering the system-level correlation
in Table 3, the trend is very similar to that of the
segment level. The improvements we introduce all
lead to improvements in correlation with human
judgement.
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999578266666667">
In this paper we describe DCU’s dependency-
based MT evaluation metric submitted to WMT-
MetricsMATR 2010. Building upon the LFG-
based metric described in (Owczarzak et al.,
2007), we use a publicly available parser instead
of an in-house parser to produce dependency la-
bels, so that the metric can run on a third party
machine. We improve the metric by allowing more
lexical variations and weighting dependency triple
matches depending on their importance according
to correlation with human judgement.
For future work, we hope to apply this method
to languages other than English, and perform more
refinement on dependency type labels and linguis-
tic resources.
</bodyText>
<sectionHeader confidence="0.996508" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998806166666667">
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank ¨Ozlem C¸etino˘glu and Jennifer Foster for providing
us with the LFG parsing model for the Malt Parser, as well as
the anonymous reviewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.999214" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999092701492537">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, pages
65–72, Ann Arbor, MI.
Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance depen-
dency resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings of the
42nd Meeting of the Association for Computational Lin-
guistics (ACL-2004), pages 319–326, Barcelona, Spain.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluation the role of bleu in machine trans-
lation research. In Proceedings of 11th Conference of the
European Chapter of the Association for Computational
Linguistics, pages 249–256, Trento, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. Journal of Machine Learning
Research, 9:1871–1874.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2010. Expected dependency pair match: predicting trans-
lation quality with expected syntactic structure. Machine
Translation.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammatical
representation. The mental representation of grammatical
relations, pages 173–281.
Alon Lavie and Michael J. Denkowski. 2009. he meteor met-
ric for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization,
pages 25–32, Ann Arbor, MI.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In In The fifth international conference on Lan-
guage Resources and Evaluation (LREC-2006), pages
2216–2219, Genoa, Italy.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation eval-
uation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 104–111, Prague, Czech
Republic.
Sebastian Pado, Michel Galley, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Robust machine translation
evaluation with entailment features. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 297–305,
Suntec, Singapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics
(ACL-2002), pages 311–318, Philadelphia, PA.
William H. Press, Saul A. Teukolsky, William T. Vetterling,
and Brian P. Flannery. 2007. Numerical Recipes 3rd Edi-
tion: The Art of Scientific Computing. Cambridge Univer-
sity Press, New York, NY.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2010. Ter-plus: paraphrase, semantic, and
alignment enhancements to translation edit rate. Machine
Translation.
</reference>
<page confidence="0.999346">
353
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558408">
<title confidence="0.921042">The DCU Dependency-Based Metric in WMT-MetricsMATR 2010</title>
<author confidence="0.998769">Yifan He Jinhua Du Andy Way Josef van</author>
<affiliation confidence="0.989632">Centre for Next Generation School of Dublin City</affiliation>
<address confidence="0.654333">Dublin 9,</address>
<abstract confidence="0.99691180952381">describe DCU’s LFG metric submitted to the shared uation task of WMT-MetricsMATR 2010. The metric is built on the LFG F-structurebased approach presented in (Owczarzak et al., 2007). We explore the following improvements on the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penalty the practice of reward continuous matches; and 4) we introduce and tune parameters to maximize the correlation with human judgement. Experiments show that these enhancements improve the dependency-based metric’s correlation with human judgement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1796" citStr="Banerjee and Lavie, 2005" startWordPosition="263" endWordPosition="266">ranslation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TERP (Snover et al., 2010), both of which now utilize stemming, WordNet and paraphrase information. Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well. This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyo</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth O’Donovan</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL-2004),</booktitle>
<pages>319--326</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef van Genabith, and Andy Way. 2004. Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL-2004), pages 319–326, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluation the role of bleu in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1598" citStr="Callison-Burch et al., 2006" startWordPosition="234" endWordPosition="237">ency-based metric’s correlation with human judgement. 1 Introduction String-based automatic evaluation metrics such as BLEU (Papineni et al., 2002) have led directly to quality improvements in machine translation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TERP (Snover et al., 2010), both of which now utilize stemming, WordNet and paraphrase information. Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluation the role of bleu in machine translation research. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 249–256, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="9709" citStr="Fan et al., 2008" startWordPosition="1535" endWordPosition="1538">dgement, but this paradigm is not adequate for the WMTMetricsMatr evaluation in two respects: 1) the inhouse LFG annotation algorithm is not publicly available and 2) the speed of this paradigm is not satisfactory. We instead use the Malt Parser1 (Nivre et al., 2006) with a parsing model trained on LFG dependencies to produce the f-structure triples. Our collaborators2 first apply the LFG annotation algorithm to the Penn Treebank training data to obtain f-structures, and then the f-structures are converted into dependency trees in CoNLL format to train the parsing model. We use the liblinear (Fan et al., 2008) classification module to for fast parsing speed. 3.2 Hard and Soft Dependency Matching Currently our parser produces only the 1-best outputs. Compared to the 50-best parses in (Owczarzak et al., 2007), the 1-best parse limits the number of triple matches that can be found. To compensate for this, we allow triple matches that have the same Head and Modifier to constitute a match, even if their dependency labels are different. Therefore for triples Dep1(Head1, Mod1) and Dep2(Head2, Mod2), we allow three types of match: a complete match if the two triples are identical, a partial match if Dep1=D</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
<author>Matthew Snover</author>
<author>Mari Ostendorf</author>
</authors>
<title>Expected dependency pair match: predicting translation quality with expected syntactic structure. Machine Translation.</title>
<date>2010</date>
<contexts>
<context position="2992" citStr="Kahn et al., 2010" startWordPosition="449" endWordPosition="452">information to go beyond the surface level matching. Owczarzak et al. (2007) extended this line of research with the use of a term-based encoding of Lexical Functional Grammar (LFG:(Kaplan and Bresnan, 1982)) labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and F-score on the triple sets corresponding to the translation and reference sentences. With the addition of partial matching and n-best parses, Owczarzak et al. (2007)’s method considerably outperforms Liu and Gildea’s (2005) w.r.t. correlation with human judgement. The EDPM metric (Kahn et al., 2010) improves this line of research by using arc labels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for preprocessing, compared to a dependency parser in (Liu and Gildea, 2005) and (Owczarzak et al., 2007). EDPM also incorporates more information sources: e.g. the parser confidence, the Porter stemmer, WordNet synonyms and paraphrases. Besides the metrics that rely solely on the dependency structures, information from the dependency parser is a component of some other metrics that use more diverse resources, suc</context>
</contexts>
<marker>Kahn, Snover, Ostendorf, 2010</marker>
<rawString>Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf. 2010. Expected dependency pair match: predicting translation quality with expected syntactic structure. Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexicalfunctional grammar: A formal system for grammatical representation. The mental representation of grammatical relations,</title>
<date>1982</date>
<pages>173--281</pages>
<contexts>
<context position="2581" citStr="Kaplan and Bresnan, 1982" startWordPosition="389" endWordPosition="392"> campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well. This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. Owczarzak et al. (2007) extended this line of research with the use of a term-based encoding of Lexical Functional Grammar (LFG:(Kaplan and Bresnan, 1982)) labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and F-score on the triple sets corresponding to the translation and reference sentences. With the addition of partial matching and n-best parses, Owczarzak et al. (2007)’s method considerably outperforms Liu and Gildea’s (2005) w.r.t. correlation with human judgement. The EDPM metric (Kahn et al., 2010) improves this line of research by using arc labels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for pr</context>
<context position="4884" citStr="Kaplan and Bresnan, 1982" startWordPosition="757" endWordPosition="761"> to penalize discontinuous matches. We sort the matches according to the match level and the dependency type, and weight the matches to maximize correlation with human judgement. The remainder of the paper is organized as follows. Section 2 reviews the dependency-based metric. Sections 3, 4, 5 and 6 introduce our improvements on this metric. We report experimental results in Section 7 and conclude in Section 8. 2 The Dependency-Based Metric In this section, we briefly review the metric presented in (Owczarzak et al., 2007). 2.1 C-Structure and F-Structure in LFG In Lexical Functional Grammar (Kaplan and Bresnan, 1982), a sentence is represented as both a hierarchical c-(onstituent) structure which captures the phrasal organization of a sentence, and a f(unctional) structure which captures the functional relations between different parts of the sentence. Our metric currently only relies on the f-structure, which is encoded as labeled dependencies in our metric. 2.2 MT Evaluation as Dependency Triple Matching The basic method of (Owczarzak et al., 2007) can be illustrated by the example in Table 1. The metric in (Owczarzak et al., 2007) performs triple matching over the Hyp- and Ref-Triples and calculates th</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. 1982. Lexicalfunctional grammar: A formal system for grammatical representation. The mental representation of grammatical relations, pages 173–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>he meteor metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="1824" citStr="Lavie and Denkowski, 2009" startWordPosition="267" endWordPosition="270">rics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TERP (Snover et al., 2010), both of which now utilize stemming, WordNet and paraphrase information. Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well. This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matchin</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. he meteor metric for automatic evaluation of machine translation. Machine Translation, 23(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2328" citStr="Liu and Gildea, 2005" startWordPosition="350" endWordPosition="353">and linguistically motivated resources. Examples include METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TERP (Snover et al., 2010), both of which now utilize stemming, WordNet and paraphrase information. Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well. This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. Owczarzak et al. (2007) extended this line of research with the use of a term-based encoding of Lexical Functional Grammar (LFG:(Kaplan and Bresnan, 1982)) labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and F-score on the triple sets corresponding to the translation and reference sentences. With the addition of partial matching and n-best parses, Owczarzak et al. (2007)’s method considerably outperforms Liu and Gildea’s (2005) w.r.t. corre</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 25–32, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing. In</title>
<date>2006</date>
<booktitle>In The fifth international conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<pages>2216--2219</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="3998" citStr="Nivre et al., 2006" startWordPosition="609" endWordPosition="612">WordNet synonyms and paraphrases. Besides the metrics that rely solely on the dependency structures, information from the dependency parser is a component of some other metrics that use more diverse resources, such as the textual entailment-based metric of (Pado et al., 2009). In this paper we extend the work of (Owczarzak et al., 2007) in a different manner: we use an 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorporate stemming, synonym and paraphrase information as in (Kahn et al., 2010), and at the same time introduce a chunk penalty in the spirit of METEOR to penalize discontinuous matches. We sort the matches according to the match level and the dependency type, and weight the matches to maximize correlation with human judgement. The remainder of the paper is organized as follows. Section 2 reviews the dependency-based metric. Sections 3, 4, 5 and 6 introduce our improvements on this met</context>
<context position="9359" citStr="Nivre et al., 2006" startWordPosition="1478" endWordPosition="1481"> (Cahill et al., 2004) to produce LFG dependency triples. The parser uses a Penn treebank-trained parser to produce c-structures (constituency trees) and an LFG fstructure annotation algorithm on the c-structure to obtain f-structures. In (Owczarzak et al., 2007), triple matching on f-structures produced by this paradigm correlates well with human judgement, but this paradigm is not adequate for the WMTMetricsMatr evaluation in two respects: 1) the inhouse LFG annotation algorithm is not publicly available and 2) the speed of this paradigm is not satisfactory. We instead use the Malt Parser1 (Nivre et al., 2006) with a parsing model trained on LFG dependencies to produce the f-structure triples. Our collaborators2 first apply the LFG annotation algorithm to the Penn Treebank training data to obtain f-structures, and then the f-structures are converted into dependency trees in CoNLL format to train the parsing model. We use the liblinear (Fan et al., 2008) classification module to for fast parsing speed. 3.2 Hard and Soft Dependency Matching Currently our parser produces only the 1-best outputs. Compared to the 50-best parses in (Owczarzak et al., 2007), the 1-best parse limits the number of triple ma</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In In The fifth international conference on Language Resources and Evaluation (LREC-2006), pages 2216–2219, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Labelled dependencies in machine translation evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>104--111</pages>
<location>Prague, Czech Republic.</location>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Labelled dependencies in machine translation evaluation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 104–111, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust machine translation evaluation with entailment features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>297--305</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="3655" citStr="Pado et al., 2009" startWordPosition="555" endWordPosition="558">labels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for preprocessing, compared to a dependency parser in (Liu and Gildea, 2005) and (Owczarzak et al., 2007). EDPM also incorporates more information sources: e.g. the parser confidence, the Porter stemmer, WordNet synonyms and paraphrases. Besides the metrics that rely solely on the dependency structures, information from the dependency parser is a component of some other metrics that use more diverse resources, such as the textual entailment-based metric of (Pado et al., 2009). In this paper we extend the work of (Owczarzak et al., 2007) in a different manner: we use an 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorporate stemming, synonym and paraphrase information as in (Kahn et al., 2010), and at the same time introduce a chunk penalty in the spirit of ME</context>
</contexts>
<marker>Pado, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pado, Michel Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Robust machine translation evaluation with entailment features. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 297–305, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1117" citStr="Papineni et al., 2002" startWordPosition="164" endWordPosition="167">on the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penalty following the practice of METEOR to reward continuous matches; and 4) we introduce and tune parameters to maximize the correlation with human judgement. Experiments show that these enhancements improve the dependency-based metric’s correlation with human judgement. 1 Introduction String-based automatic evaluation metrics such as BLEU (Papineni et al., 2002) have led directly to quality improvements in machine translation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and lingui</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes 3rd Edition: The Art of Scientific Computing.</title>
<date>2007</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="13848" citStr="Press et al., 2007" startWordPosition="2218" endWordPosition="2221">stem, WordNet, paraphrase}; and 3) the type of dependency D. To avoid too large a number of parameters, we only allow a set of frequent dependency types, along with the type other, which represents all the other types and the type soft for soft matches. We have D={app, subj, obj, poss, adjunct, topicrel, other, soft}. Therefore for each triple match m, we can have the type of the match t E L x R x D. 6.2 Tuning In sum, we have the following parameters to tune in our metric: precision weight α, chunk penalty parameters Q, -y, and the match type weights A1...An. We perform Powell’s line search (Press et al., 2007) on the sufficient statistics of our metric to find the set of parameters that maximizes Pearson’s p on the segment level. We perform the optimization on the MT06 portion of the NIST MetricsMATR 2010 development set with 2-fold cross validation. 7 Experiments We experiment with four settings of the metric: HARD, SOFT, SOFTALL and WEIGHTED in order to validate our enhancements. The first two settings compare the effect of allowing/not allowing soft matches, but only uses WordNet as in (Owczarzak et al., 2007). The third setting applies our additional linguistic features and the final setting tu</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2007</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2007. Numerical Recipes 3rd Edition: The Art of Scientific Computing. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation.</title>
<date>2010</date>
<contexts>
<context position="1855" citStr="Snover et al., 2010" startWordPosition="273" endWordPosition="276">ive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TERP (Snover et al., 2010), both of which now utilize stemming, WordNet and paraphrase information. Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well. This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. Owczarzak et al. (2007) exte</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2010</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2010. Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>