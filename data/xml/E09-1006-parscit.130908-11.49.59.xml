<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007762">
<title confidence="0.989453">
Supervised Domain Adaption for WSD
</title>
<author confidence="0.608561">
Eneko Agirre and Oier Lopez de Lacalle
</author>
<affiliation confidence="0.707291666666667">
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
</affiliation>
<email confidence="0.9945">
{e.agirre,oier.lopezdelacalle}@ehu.es
</email>
<sectionHeader confidence="0.993769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853666666667">
The lack of positive results on super-
vised domain adaptation for WSD have
cast some doubts on the utility of hand-
tagging general corpora and thus devel-
oping generic supervised WSD systems.
In this paper we show for the first time
that our WSD system trained on a general
source corpus (BNC) and the target corpus,
obtains up to 22% error reduction when
compared to a system trained on the tar-
get corpus alone. In addition, we show
that as little as 40% of the target corpus
(when supplemented with the source cor-
pus) is sufficient to obtain the same results
as training on the full target data. The key
for success is the use of unlabeled data
with SVD, a combination of kernels and
SVM.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945166666667">
In many Natural Language Processing (NLP)
tasks we find that a large collection of manually-
annotated text is used to train and test supervised
machine learning models. While these models
have been shown to perform very well when tested
on the text collection related to the training data
(what we call the source domain), the perfor-
mance drops considerably when testing on text
from other domains (called target domains).
In order to build models that perform well in
new (target) domains we usually find two settings
(Daum´e III, 2007). In the semi-supervised setting,
the training hand-annotated text from the source
domain is supplemented with unlabeled data from
the target domain. In the supervised setting, we
use training data from both the source and target
domains to test on the target domain.
In (Agirre and Lopez de Lacalle, 2008) we
studied semi-supervised Word Sense Disambigua-
tion (WSD) adaptation, and in this paper we fo-
cus on supervised WSD adaptation. We compare
the performance of similar supervised WSD sys-
tems on three different scenarios. In the source
to target scenario the WSD system is trained on
the source domain and tested on the target do-
main. In the target scenario the WSD system
is trained and tested on the target domain (using
cross-validation). In the adaptation scenario the
WSD system is trained on both source and target
domain and tested in the target domain (also using
cross-validation over the target data). The source
to target scenario represents a weak baseline for
domain adaptation, as it does not use any exam-
ples from the target domain. The target scenario
represents the hard baseline, and in fact, if the do-
main adaptation scenario does not yield better re-
sults, the adaptation would have failed, as it would
mean that the source examples are not useful when
we do have hand-labeled target examples.
Previous work shows that current state-of-the-
art WSD systems are not able to obtain better re-
sults on the adaptation scenario compared to the
target scenario (Escudero et al., 2000; Agirre and
Martinez, 2004; Chan and Ng, 2007). This would
mean that if a user of a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would need to adapt it to a specific do-
main, he would be better off throwing away the
generic examples and hand-tagging domain exam-
ples directly. This paper will show that domain
adaptation is feasible, even for difficult domain-
related words, in the sense that generic corpora
can be reused when deploying WSD systems in
specific domains. We will also show that, given
the source corpus, our technique can save up to
60% of effort when tagging domain-related occur-
rences.
We performed on a publicly available corpus
which was designed to study the effect of domains
in WSD (Koeling et al., 2005). It comprises 41
</bodyText>
<note confidence="0.923022">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 42–50,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998811">
42
</page>
<bodyText confidence="0.999972043478261">
nouns which are highly relevant in the SPORTS
and FINANCES domains, with 300 examples for
each. The use of two target domains strengthens
the conclusions of this paper.
Our system uses Singular Value Decomposi-
tion (SVD) in order to find correlations between
terms, which are helpful to overcome the scarcity
of training data in WSD (Gliozzo et al., 2005).
This work explores how this ability of SVD and
a combination of the resulting feature spaces im-
proves domain adaptation. We present two ways
to combine the reduced spaces: kernel combina-
tion with Support Vector Machines (SVM), and k
Nearest-Neighbors (k-NN) combination.
The paper is structured as follows. Section 2 re-
views prior work in the area. Section 3 presents
the data sets used. In Section 4 we describe
the learning features, including the application of
SVD, and in Section 5 the learning methods and
the combination. The experimental results are pre-
sented in Section 6. Section 7 presents the discus-
sion and some analysis of this paper and finally
Section 8 draws the conclusions.
</bodyText>
<sectionHeader confidence="0.991685" genericHeader="introduction">
2 Prior work
</sectionHeader>
<bodyText confidence="0.99993538961039">
Domain adaptation is a practical problem attract-
ing more and more attention. In the supervised
setting, a recent paper by Daum´e III (2007) shows
that a simple feature augmentation method for
SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously ex-
plored more sophisticated methods (Daum´e III
and Marcu, 2006; Chelba and Acero, 2004). The
feature augmentation consists in making three ver-
sion of the original features: a general, a source-
specific and a target-specific versions. That way
the augmented source contains the general and
source-specific version and the augmented target
data general and specific versions. The idea be-
hind this is that target domain data has twice the
influence as the source when making predictions
about test target data. We reimplemented this
method and show that our results are better.
Regarding WSD, some initial works made a ba-
sic analysis of domain adaptation issues. Escud-
ero et al. (2000) tested the supervised adaptation
scenario on the DSO corpus, which had examples
from the Brown corpus and Wall Street Journal
corpus. They found that the source corpus did
not help when tagging the target corpus, show-
ing that tagged corpora from each domain would
suffice, and concluding that hand tagging a large
general corpus would not guarantee robust broad-
coverage WSD. Agirre and Mart´ınez (2000) used
the DSO corpus in the supervised scenario to show
that training on a subset of the source corpora that
is topically related to the target corpus does allow
for some domain adaptation.
More recently, Chan and Ng (2007) performed
supervised domain adaptation on a manually se-
lected subset of 21 nouns from the DSO corpus.
They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding
just 30% of the target data to the source exam-
ples the same precision as the full combination of
target and source data could be achieved. They
also showed that using the source corpus allowed
to significantly improve results when only 10%-
30% of the target corpus was used for training.
Unfortunately, no data was given about the target
corpus results, thus failing to show that domain-
adaptation succeeded. In followup work (Zhong et
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation
experiment. They reduced significantly the ef-
fort of hand-tagging, but only obtained domain-
adaptation for smaller fractions of the source and
target corpus. Similarly to these works we show
that we can save annotation effort on the target
corpus, but, in contrast, we do get domain adap-
tation when using the full dataset. In a way our
approach is complementary, and we could also ap-
ply active learning to further reduce the number of
target examples to be tagged.
Though not addressing domain adaptation,
other works on WSD also used SVD and are
closely related to the present paper. Ando (2006)
used Alternative Structured Optimization. She
first trained one linear predictor for each target
word, and then performed SVD on 7 carefully se-
lected submatrices of the feature-to-predictor ma-
trix of weights. The system attained small but
consistent improvements (no significance data was
given) on the Senseval-3 lexical sample datasets
using SVD and unlabeled data.
Gliozzo et al. (2005) used SVD to reduce the
space of the term-to-document matrix, and then
computed the similarity between train and test
</bodyText>
<page confidence="0.999584">
43
</page>
<bodyText confidence="0.999815944444445">
instances using a mapping to the reduced space
(similar to our SMA method in Section 4.2). They
combined other knowledge sources into a complex
kernel using SVM. They report improved perfor-
mance on a number of languages in the Senseval-
3 lexical sample dataset. Our present paper dif-
fers from theirs in that we propose an additional
method to use SVD (the OMT method), and that
we focus on domain adaptation.
In the semi-supervised setting, Blitzer et al.
(2006) used Structural Correspondence Learning
and unlabeled data to adapt a Part-of-Speech tag-
ger. They carefully select so-called ‘pivot fea-
tures’ to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source
and target domains. Our technique also uses SVD,
but we directly apply it to all features, and thus
avoid the need to define pivot features. In prelim-
inary work we unsuccessfully tried to carry along
the idea of pivot features to WSD. On the contrary,
in (Agirre and Lopez de Lacalle, 2008) we show
that methods closely related to those presented in
this paper produce positive semi-supervised do-
main adaptation results for WSD.
The methods used in this paper originated in
(Agirre et al., 2005; Agirre and Lopez de Lacalle,
2007), where SVD over a feature-to-documents
matrix improved WSD performance with and
without unlabeled data. The use of several k-
NN classifiers trained on a number of reduced and
original spaces was shown to get the best results
in the Senseval-3 dataset and ranked second in the
SemEval 2007 competition. The present paper ex-
tends this work and applies it to domain adapta-
tion.
</bodyText>
<sectionHeader confidence="0.977348" genericHeader="method">
3 Data sets
</sectionHeader>
<bodyText confidence="0.999925447368421">
The dataset we use was designed for domain-
related WSD experiments by Koeling et al. (2005),
and is publicly available. The examples come
from the BNC (Leech, 1992) and the SPORTS and
FINANCES sections of the Reuters corpus (Rose
et al., 2002), comprising around 300 examples
(roughly 100 from each of those corpora) for each
of the 41 nouns. The nouns were selected be-
cause they were salient in either the SPORTS or
FINANCES domains, or because they had senses
linked to those domains. The occurrences were
hand-tagged with the senses from WordNet (WN)
version 1.7.1 (Fellbaum, 1998). In our experi-
ments the BNC examples play the role of general
source corpora, and the FINANCES and SPORTS
examples the role of two specific domain target
corpora.
Compared to the DSO corpus used in prior work
(cf. Section 2) this corpus has been explicitly cre-
ated for domain adaptation studies. DSO con-
tains texts coming from the Brown corpus and the
Wall Street Journal, but the texts are not classi-
fied according to specific domains (e.g. Sports,
Finances), which make DSO less suitable to study
domain adaptation. The fact that the selected
nouns are related to the target domain makes
the (Koeling et al., 2005) corpus more demanding
than the DSO corpus, because one would expect
the performance of a generic WSD system to drop
when moving to the domain corpus for domain-
related words (cf. Table 1), while the performance
would be similar for generic words.
In addition to the labeled data, we also use
unlabeled data coming from the three sources
used in the labeled corpus: the ’written’ part
of the BNC (89.7M words), the FINANCES part
of Reuters (32.5M words), and the SPORTS part
(9.1M words).
</bodyText>
<sectionHeader confidence="0.979072" genericHeader="method">
4 Original and SVD features
</sectionHeader>
<bodyText confidence="0.9992375">
In this section, we review the features and two
methods to apply SVD over the features.
</bodyText>
<subsectionHeader confidence="0.923144">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999999285714286">
We relied on the usual features used in previous
WSD work, grouped in three main sets. Local
collocations comprise the bigrams and trigrams
formed around the target word (using either lem-
mas, word-forms, or PoS tags) , those formed
with the previous/posterior lemma/word-form in
the sentence, and the content words in a ±4-word
window around the target. Syntactic dependen-
cies use the object, subject, noun-modifier, prepo-
sition, and sibling lemmas, when available. Fi-
nally, Bag-of-words features are the lemmas of
the content words in the whole context, plus the
salient bigrams in the context (Pedersen, 2001).
We refer to these features as original features.
</bodyText>
<subsectionHeader confidence="0.987383">
4.2 SVD features
</subsectionHeader>
<bodyText confidence="0.99994375">
Apart from the original space of features, we have
used the so called SVD features, obtained from
the projection of the feature vectors into the re-
duced space (Deerwester et al., 1990). Basically,
</bodyText>
<page confidence="0.995054">
44
</page>
<bodyText confidence="0.998522589285714">
we set a term-by-document or feature-by-example
matrix M from the corpus (see section below for
more details). SVD decomposes M into three ma-
trices, M = UEVT . If the desired number of
dimensions in the reduced space is p, we select p
rows from E and V , yielding Ep and Vp respec-
tively. We can map any feature vector i� (which
represents either a train or test example) into the
p-dimensional space as follows: tp = �iT VpE�1
p .
Those mapped vectors have p dimensions, and
each of the dimensions is what we call a SVD fea-
ture. We have explored two different variants in
order to build the reduced matrix and obtain the
SVD features, as follows.
Single Matrix for All target words (SVD-
SMA). The method comprises the following steps:
(i) extract bag-of-word features (terms in this case)
from unlabeled corpora, (ii) build the term-by-
document matrix, (iii) decompose it with SVD, and
(iv) map the labeled data (train/test). This tech-
nique is very similar to previous work on SVD
(Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001).
The dimensionality reduction is performed once,
over the whole unlabeled corpus, and it is then ap-
plied to the labeled data of each word. The re-
duced space is constructed only with terms, which
correspond to bag-of-words features, and thus dis-
cards the rest of the features. Given that the WSD
literature shows that all features are necessary for
optimal performance (Pradhan et al., 2007), we
propose the following alternative to construct the
matrix.
One Matrix per Target word (SVD-OMT). For
each word: (i) construct a corpus with its occur-
rences in the labeled and, if desired, unlabeled cor-
pora, (ii) extract all features, (iii) build the feature-
by-example matrix, (iv) decompose it with SVD,
and (v) map all the labeled training and test data
for the word. Note that this variant performs one
SVD process for each target word separately, hence
its name.
When building the SVD-OMT matrices we can
use only the training data (TRAIN) or both the train
and unlabeled data (+UNLAB). When building the
SVD-SMA matrices, given the small size of the in-
dividual word matrices, we always use both the
train and unlabeled data (+UNLAB). Regarding the
amount of data, based also on previous work, we
used 50% of the available data for OMT, and the
whole corpora for SMA. An important parameter
when doing SVD is the number of dimensions in
the reduced space (p). We tried two different val-
ues for p (25 and 200) in the BNC domain, and
set a dimension for each classifier/matrix combi-
nation.
</bodyText>
<subsectionHeader confidence="0.996771">
4.3 Motivation
</subsectionHeader>
<bodyText confidence="0.999929">
The motivation behind our method is that although
the train and test feature vectors overlap suffi-
ciently in the usual WSD task, the domain dif-
ference makes such overlap more scarce. SVD
implicitly finds correlations among features, as it
maps related features into nearby regions in the re-
duced space. In the case of SMA, SVD is applied
over the joint term-by-document matrix of labeled
(and possibly unlabeled corpora), and it thus can
find correlations among closely related words (e.g.
cat and dog). These correlations can help reduce
the gap among bag-of-words features from the
source and target examples. In the case of OMT,
SVD over the joint feature-by-example matrix of
labeled and unlabeled examples of a word allows
to find correlations among features that show sim-
ilar occurrence patterns in the source and target
corpora for the target word.
</bodyText>
<sectionHeader confidence="0.999749" genericHeader="method">
5 Learning methods
</sectionHeader>
<bodyText confidence="0.999918428571428">
k-NN is a memory based learning method, where
the neighbors are the k most similar labeled exam-
ples to the test example. The similarity among in-
stances is measured by the cosine of their vectors.
The test instance is labeled with the sense obtain-
ing the maximum sum of the weighted vote of the
k most similar contexts. We set k to 5 based on
previous results published in (Agirre and Lopez de
Lacalle, 2007).
Regarding SVM, we used linear kernels, but
also purpose-built kernels for the reduced spaces
and the combinations (cf. Section 5.2). We used
the default soft margin (C=0). In previous ex-
periments we learnt that C is very dependent on
the feature set and training data used. As we
will experiment with different features and train-
ing datasets, it did not make sense to optimize it
across all settings.
We will now detail how we combined the origi-
nal and SVD features in each of the machine learn-
ing methods.
</bodyText>
<subsectionHeader confidence="0.600867">
5.1 k-NN combinations
</subsectionHeader>
<bodyText confidence="0.798263">
Our k-NN combination method (Agirre et al.,
2005; Agirre and Lopez de Lacalle, 2007) takes
</bodyText>
<page confidence="0.997129">
45
</page>
<bodyText confidence="0.9999958125">
advantage of the properties of k-NN classifiers and
exploit the fact that a classifier can be seen as
k points (number of nearest neighbor) each cast-
ing one vote. This makes easy to combine sev-
eral classifiers, one for each feature space. For in-
stance, taking two k-NN classifiers of k = 5, C1
and C2, we can combine them into a single k = 10
classifier, where five votes come from C1 and five
from C2. This allows to smoothly combine classi-
fiers from different feature spaces.
In this work we built three single k-NN classi-
fiers trained on OMT, SMA and the original fea-
tures, respectively. In order to combine them we
weight each vote by the inverse ratio of its position
in the rank of the single classifier, (k − ri + 1)/k,
where ri is the rank.
</bodyText>
<subsectionHeader confidence="0.991852">
5.2 Kernel combination
</subsectionHeader>
<bodyText confidence="0.999988105263158">
The basic idea of kernel methods is to find a suit-
able mapping function (0) in order to get a better
generalization. Instead of doing this mapping ex-
plicitly, kernels give the chance to do it inside the
algorithm. We will formalize it as follows. First,
we define the mapping function 0 : X → F. Once
the function is defined, we can use it in the kernel
function in order to become an implicit function
K(x, z) = h0(x) · 0(z)i, where h·i denotes a in-
ner product between vectors in the feature space.
This way, we can very easily define mappings
representing different information sources and use
this mappings in several machine learning algo-
rithm. In our work we use SVM.
We defined three individual kernels (OMT, SMA
and original features) and the combined kernel.
The original feature kernel (KOrig) is given by
the identity function over the features 0 : X → X,
defining the following kernel:
</bodyText>
<equation confidence="0.962658">
KOrig(xi, xj) = �hxi · xii hxj · xji
hxi · xji
</equation>
<bodyText confidence="0.9995502">
where the denominator is used to normalize and
avoid any kind of bias in the combination.
The OMT kernel (KOmt) and SMA kernel
(KSma) are defined using OMT and SMA projec-
tion matrices, respectively (cf. Section 4.2). Given
the OMT function mapping 0omt : Rm → Rp,
where m is the number of the original features
and p the reduced dimensionality, then we define
KOmt(xi, xj) as follows (KSma is defined simi-
larly):
</bodyText>
<equation confidence="0.991883">
h0omt(xi) · 0omt(xj)i
00omt(xi) · 0omt(xi)i h0omt(xj) · 0omt(xj)i
</equation>
<table confidence="0.9804775">
BNC → X SPORTS FINANCES
MFS 39.0 51.2
k-NN 51.7 60.4
SVM 53.9 62.9
</table>
<tableCaption confidence="0.974477">
Table 1: Source to target results: Train on BNC,
test on SPORTS and FINANCES.
</tableCaption>
<bodyText confidence="0.995911">
Finally, we define the kernel combination:
</bodyText>
<equation confidence="0.946618">
Kl(xi, xj)
V�Kl(xi, xi)Kl(xj, xj)
</equation>
<bodyText confidence="0.999855">
where n is the number of single kernels explained
above, and l the index for the kernel type.
</bodyText>
<sectionHeader confidence="0.98852" genericHeader="method">
6 Domain adaptation experiments
</sectionHeader>
<bodyText confidence="0.999977714285714">
In this section we present the results in our two ref-
erence scenarios (source to target, target) and our
reference scenario (domain adaptation). Note that
all methods presented here have full coverage, i.e.
they return a sense for all test examples, and there-
fore precision equals recall, and suffices to com-
pare among systems.
</bodyText>
<subsectionHeader confidence="0.98433">
6.1 Source to target scenario: BNC → X
</subsectionHeader>
<bodyText confidence="0.999975">
In this scenario our supervised WSD systems are
trained on the general source corpus (BNC) and
tested on the specific target domains separately
(SPORTS and FINANCES). We do not perform any
kind of adaptation, and therefore the results are
those expected for a generic WSD system when
applied to domain-specific texts.
Table 1 shows the results for k-NN and SVM
trained with the original features on the BNC. In
addition, we also show the results for the Most
Frequent Sense baseline (MFS) taken from the
BNC. The second column denotes the accuracies
obtained when testing on SPORTS, and the third
column the accuracies for FINANCES. The low ac-
curacy obtained with MFS, e.g. 39.0 of precision
in SPORTS, shows the difficulty of this task. Both
classifiers improve over MFS. These classifiers are
weak baselines for the domain adaptation system.
</bodyText>
<subsectionHeader confidence="0.98773">
6.2 Target scenario X → X
</subsectionHeader>
<bodyText confidence="0.999686">
In this scenario we lay the harder baseline which
the domain adaptation experiments should im-
prove on (cf. next section). The WSD systems
are trained and tested on each of the target cor-
pora (SPORTS and FINANCES) using 3-fold cross-
validation.
</bodyText>
<equation confidence="0.980199333333333">
n
KComb(xi, xj) =
l=1
</equation>
<page confidence="0.996675">
46
</page>
<table confidence="0.999714">
X → X SPORTS FINANCES
TRAIN +UNLAB TRAIN +UNLAB
MFS 77.8 - 82.3 -
k-NN 84.5 - 87.1 -
SVM 85.1 - 87.0 -
k-NN-OMT 85.0 86.1 87.3 87.6
SVM-OMT 82.9 85.1 85.3 86.4
k-NN-SMA - 81.1 - 83.2
SVM-SMA - 81.3 - 84.1
k-NN-COMB 86. 0 86.7 87.9 88.6
SVM-COMB - 86.5 - 88.5
</table>
<tableCaption confidence="0.7891168">
Table 2: Target results: train and test on SPORTS,
train and test on FINANCES, using 3-fold cross-
validation.
Table 2 summarizes the results for this scenario.
TRAIN denotes that only tagged data was used to
</tableCaption>
<bodyText confidence="0.998587277777778">
train, +UNLAB denotes that we added unlabeled
data related to the source corpus when computing
SVD. The rows denote the classifier and the feature
spaces used, which are organized in four sections.
On the top rows we show the three baseline clas-
sifiers on the original features. The two sections
below show the results of those classifiers on the
reduced dimensions, OMT and SMA (cf. Section
4.2). Finally, the last rows show the results of the
combination strategies (cf. Sections 5.1 and 5.2).
Note that some of the cells have no result, because
that combination is not applicable (e.g. using the
train and unlabeled data in the original space).
First of all note that the results for the base-
lines (MFS, SVM, k-NN) are much larger than
those in Table 1, showing that this dataset is spe-
cially demanding for supervised WSD, and partic-
ularly difficult for domain adaptation experiments.
These results seem to indicate that the examples
from the source general corpus could be of little
use when tagging the target corpora. Note spe-
cially the difference in MFS performance. The pri-
ors of the senses are very different in the source
and target corpora, which is a well-known short-
coming for supervised systems. Note the high re-
sults of the baseline classifiers, which leave small
room for improvement.
The results for the more sophisticated methods
show that SVD and unlabeled data helps slightly,
except for k-NN-OMT on SPORTS. SMA de-
creases the performance compared to the classi-
fiers trained on original features. The best im-
provements come when the three strategies are
combined in one, as both the kernel and k-NN
combinations obtain improvements over the re-
spective single classifiers. Note that both the k-NN
</bodyText>
<table confidence="0.998328928571429">
BNC + X SPORTS FINANCES
→ X TRAIN + UNLAB TRAIN + UNLAB
BNC → X 53.9 - 62.9 -
X → X 86.0 86.7 87.9 88.5
MFS 68.2 - 73.1 -
k-NN 81.3 - 86.0 -
SVM 84.7 - 87.5 -
k-NN-OMT 84.0 84.7 87.5 86.0
SVM-OMT 85.1 84.7 84.2 85.5
k-NN-SMA - 77.1 - 81.6
SVM-SMA - 78.1 - 80.7
k-NN-COMB 84.5 87.2 88.1 88.7
SVM-COMB - 88.4 - 89.7
SVM-AUG 85.9 - 88.1 -
</table>
<tableCaption confidence="0.905058">
Table 3: Domain adaptation results: Train on
BNC and SPORTS, test on SPORTS (same for FI-
NANCES).
</tableCaption>
<bodyText confidence="0.982628583333333">
and SVM combinations perform similarly.
In the combination strategy we show that unla-
beled data helps slightly, because instead of only
combining OMT and original features we have the
opportunity to introduce SMA. Note that it was not
our aim to improve the results of the basic classi-
fiers on this scenario, but given the fact that we are
going to apply all these techniques in the domain
adaptation scenario, we need to show these results
as baselines. That is, in the next section we will try
to obtain results which improve significantly over
the best results in this section.
</bodyText>
<subsectionHeader confidence="0.991634">
6.3 Domain adaptation scenario
</subsectionHeader>
<equation confidence="0.628385">
BNC + X → X
</equation>
<bodyText confidence="0.998708523809524">
In this last scenario we try to show that our WSD
system trained on both source (BNC) and tar-
get (SPORTS and FINANCES) data performs better
than the one trained on the target data alone. We
also use 3-fold cross-validation for the target data,
but the entire source data is used in each turn. The
unlabeled data here refers to the combination of
unlabeled source and target data.
The results are presented in table 3. Again, the
columns denote if unlabeled data has been used in
the learning process. The rows correspond to clas-
sifiers and the feature spaces involved. The first
rows report the best results in the previous scenar-
ios: BNC → X for the source to target scenario,
and X → X for the target scenario. The rest
of the table corresponds to the domain adaptation
scenario. The rows below correspond to MFS and
the baseline classifiers, followed by the OMT and
SMA results, and the combination results. The last
row shows the results for the feature augmentation
algorithm (Daum´e III, 2007).
</bodyText>
<page confidence="0.997313">
47
</page>
<table confidence="0.997844076923077">
SPORTS FINANCES
BNC → X
MFS 39.0 51.2
SVM 53.9 62.9
X → X
MFS 77.8 82.3
SVM 85.1 87.0
k-NN-COMB (+UNLAB) 86.7 88.6
BNC +X → X
MFS 68.2 73.1
SVM 84.7 87.5
SVM-AUG 85.9 88.1
SVM-COMB (+UNLAB) 88.4 89.7
</table>
<tableCaption confidence="0.9014345">
Table 4: The most important results in each sce-
nario.
</tableCaption>
<bodyText confidence="0.999933173913043">
Focusing on the results, the table shows that
MFS decreases with respect to the target scenario
(cf. Table 2) when the source data is added, prob-
ably caused by the different sense distributions in
BNC and the target corpora. The baseline classi-
fiers (k-NN and SVM) are not able to improve over
the baseline classifiers on the target data alone,
which is coherent with past research, and shows
that straightforward domain adaptation does not
work.
The following rows show that our reduction
methods on themselves (OMT, SMA used by k-
NN and SVM) also fail to perform better than in
the target scenario, but the combinations using
unlabeled data (k-NN-COMB and specially SVM-
COMB) do manage to improve the best results for
the target scenario, showing that we were able to
attain domain adaptation. The feature augmenta-
tion approach (SVM-AUG) does improve slightly
over SVM in the target scenario, but not over the
best results in the target scenario, showing the dif-
ficulty of domain adaptation for WSD, at least on
this dataset.
</bodyText>
<sectionHeader confidence="0.934186" genericHeader="discussions">
7 Discussion and analysis
</sectionHeader>
<bodyText confidence="0.99982675">
Table 4 summarizes the most important results.
The kernel combination method with unlabeled
data on the adaptation scenario reduces the error
on 22.1% and 17.6% over the baseline SVM on
the target scenario (SPORTS and FINANCES re-
spectively), and 12.7% and 9.0% over the k-NN
combination method on the target scenario. These
gains are remarkable given the already high base-
line, specially taking into consideration that the
41 nouns are closely related to the domains. The
differences, including SVM-AUG, are statistically
significant according to the Wilcoxon test with
</bodyText>
<figureCaption confidence="0.936474833333333">
Figure 1: Learning curves for SPORTS. The X
axis denotes the amount of SPORTS data and the
Y axis corresponds to accuracy.
Figure 2: Learning curves for FINANCES. The X
axis denotes the amount of FINANCES data and Y
axis corresponds to the accuracy.
</figureCaption>
<bodyText confidence="0.999411631578947">
p &lt; 0.01.
In addition, we carried extra experiments to ex-
amine the learning curves, and to check, given
the source examples, how many additional ex-
amples from the target corpus are needed to ob-
tain the same results as in the target scenario us-
ing all available examples. We fixed the source
data and used increasing amounts of target data.
We show the original SVM on the target scenario,
and SVM-COMB (+UNLAB) and SVM-AUG as the
domain adaptation approaches. The results are
shown in figure 1 for SPORTS and figure 2 for FI-
NANCES. The horizontal line corresponds to the
performance of SVM on the target domain. The
point where the learning curves cross the horizon-
tal line show that our domain adaptation method
needs only around 40% of the target data in order
to get the same performance as the baseline SVM
on the target data. The learning curves also shows
</bodyText>
<figure confidence="0.999155173913043">
%25 %32 %50 %62 %75 %82 %100
sports (%)
accuracy (%)
88
86
84
82
80
SVM-COMB (+UNLAB, BNC + SPORTS -&gt; SPORTS)
SVM-AUG (BNC + SPORTS -&gt; SPORTS)
SVM-ORIG (SPORTS -&gt; SPORTS)
y=85.1
%25 %32 %50 %62 %75 %82 %100
finances (%)
accuracy (%)
90
88
86
84
SVM-COMB (+UNLAB, BNC + FIN. -&gt; FIN.)
SVM-AUG (BNC + FIN. -&gt; FIN.)
SVM-ORIG (FIN. -&gt; FIN.)
y=87.0
</figure>
<page confidence="0.998097">
48
</page>
<bodyText confidence="0.9999325">
that the domain adaptation kernel combination ap-
proach, no matter the amount of target data, is al-
ways above the rest of the classifiers, showing the
robustness of our approach.
</bodyText>
<sectionHeader confidence="0.800451" genericHeader="conclusions">
8 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.99991026">
In this paper we explore supervised domain adap-
tation for WSD with positive results, that is,
whether hand-labeling general domain (source)
text is worth the effort when training WSD sys-
tems that are to be applied to specific domains (tar-
gets). We performed several experiments in three
scenarios. In the first scenario (source to target
scenario), the classifiers were trained on source
domain data (the BNC) and tested on the target do-
mains, composed by the SPORTS and FINANCES
sections of Reuters. In the second scenario (tar-
get scenario) we set the main baseline for our do-
main adaptation experiment, training and testing
our classifiers on the target domain data. In the last
scenario (domain adaptation scenario), we com-
bine both source and target data for training, and
test on the target data.
We report results in each scenario for k-NN and
SVM classifiers, for reduced features obtained us-
ing SVD over the training data, for the use of un-
labeled data, and for k-NN and SVM combinations
of all.
Our results show that our best domain adap-
tation strategy (using kernel combination of SVD
features and unlabeled data related to the training
data) yields statistically significant improvements:
up to 22% error reduction compared to SVM on
the target domain data alone. We also show that
our domain adaptation method only needs 40% of
the target data (in addition to the source data) in
order to get the same results as SVM on the target
alone.
We obtain coherent results in two target scenar-
ios, and consistent improvement at all levels of
the learning curves, showing the robustness or our
findings. We think that our dataset, which com-
prises examples for 41 nouns that are closely re-
lated to the target domains, is specially demand-
ing, as one would expect the performance of a
generic WSD system to drop when moving to
the domain corpus, specially on domain-related
words, while we could expect the performance to
be similar for generic or unrelated words.
In the future we would like to evaluate
our method on other datasets (e.g. DSO or
OntoNotes), to test whether the positive results are
confirmed. We would also like to study word-by-
word behaviour, in order to assess whether target
examples are really necessary for words which are
less related to the domain.
</bodyText>
<sectionHeader confidence="0.995498" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9946786">
This work has been partially funded by the EU Commission
(project KYOTO ICT-2007-211423) and Spanish Research
Department (project KNOW TIN2006-15049-C03-01). Oier
Lopez de Lacalle has a PhD grant from the Basque Govern-
ment.
</bodyText>
<sectionHeader confidence="0.998335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99927295">
Eneko Agirre and Oier Lopez de Lacalle. 2007. Ubc-
alm: Combining k-nn with svd for wsd. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 342–
345, Prague, Czech Republic, June. Association for
Computational Linguistics.
Eneko Agirre and Oier Lopez de Lacalle. 2008. On
robustness and domain adaptation using SVD for
word sense disambiguation. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 17–24, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Eneko Agirre and David Martinez. 2004. The effect
of bias on an automatically-built word sense corpus.
Proceedings of the 4rd International Conference on
Languages Resources and Evaluations (LREC).
E. Agirre, O.Lopez de Lacalle, and David Martinez.
2005. Exploring feature spaces with svd and un-
labeled data for Word Sense Disambiguation. In
Proceedings of the Conference on Recent Advances
on Natural Language Processing (RANLP’05),
Borovets, Bulgaria.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
77–84, New York City.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120–128, Sydney, Australia, July.
Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49–56, Prague, Czech Republic,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.987943">
49
</page>
<reference confidence="0.999862986486486">
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of of th Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101–126.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Scott Deerwester, Susan Dumais, Goerge Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal
of the American Society for Information Science,
41(6):391–407.
Gerard Escudero, Lluiz M´arquez, and German Rigau.
2000. An Empirical Study of the Domain Depen-
dence of Supervised Word Sense Didanbiguation
Systems. Proceedings of the joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Carlo Strapparava. 2005. Domain Kernels for Word
Sense Disambiguation. 43nd Annual Meeting of the
Association for Computational Linguistics. (ACL-
05).
R. Koeling, D. McCarthy, and J. Carroll. 2005.
Domain-specific sense distributions and predomi-
nant sense acquisition. In Proceedings of the Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing. HLT/EMNLP, pages 419–426, Ann Ar-
bor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1–13.
David Martinez and Eneko Agirre. 2000. One Sense
per Collocation and Genre/Topic Variations. Con-
ference on Empirical Method in Natural Language.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), Pittsburgh, PA.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87–92,
Prague, Czech Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volumen 1 from yester-
day’s news to tomorrow’s language resources. In
Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-
2002), pages 827–832, Las Palmas, Canary Islands.
Sarah Zelikovitz and Haym Hirsh. 2001. Using LSI
for text classification in the presence of background
text. In Henrique Paques, Ling Liu, and David
Grossman, editors, Proceedings of CIKM-01, 10th
ACM International Conference on Information and
Knowledge Management, pages 113–118, Atlanta,
US. ACM Press, New York, US.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1002–1010, Honolulu, Hawaii,
October. Association for Computational Linguistics.
</reference>
<page confidence="0.997689">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.412784">
<title confidence="0.999251">Supervised Domain Adaption for WSD</title>
<author confidence="0.927835">Eneko Agirre</author>
<author confidence="0.927835">Oier Lopez de_Lacalle</author>
<affiliation confidence="0.998855">IXA NLP Group University of the Basque Country</affiliation>
<address confidence="0.693286">Donostia, Basque Contry</address>
<abstract confidence="0.979923611111111">The lack of positive results on supervised domain adaptation for WSD have cast some doubts on the utility of handtagging general corpora and thus developing generic supervised WSD systems. In this paper we show for the first time that our WSD system trained on a general corpus and the target corpus, obtains up to 22% error reduction when compared to a system trained on the target corpus alone. In addition, we show that as little as 40% of the target corpus (when supplemented with the source corpus) is sufficient to obtain the same results as training on the full target data. The key for success is the use of unlabeled data a combination of kernels and</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Ubcalm: Combining k-nn with svd for wsd.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>342--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Agirre, de Lacalle, 2007</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2007. Ubcalm: Combining k-nn with svd for wsd. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 342– 345, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>On robustness and domain adaptation using SVD for word sense disambiguation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>17--24</pages>
<location>Manchester, UK,</location>
<marker>Agirre, de Lacalle, 2008</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2008. On robustness and domain adaptation using SVD for word sense disambiguation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 17–24, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>The effect of bias on an automatically-built word sense corpus.</title>
<date>2004</date>
<booktitle>Proceedings of the 4rd International Conference on Languages Resources and Evaluations (LREC).</booktitle>
<contexts>
<context position="2955" citStr="Agirre and Martinez, 2004" startWordPosition="491" endWordPosition="494">he target data). The source to target scenario represents a weak baseline for domain adaptation, as it does not use any examples from the target domain. The target scenario represents the hard baseline, and in fact, if the domain adaptation scenario does not yield better results, the adaptation would have failed, as it would mean that the source examples are not useful when we do have hand-labeled target examples. Previous work shows that current state-of-theart WSD systems are not able to obtain better results on the adaptation scenario compared to the target scenario (Escudero et al., 2000; Agirre and Martinez, 2004; Chan and Ng, 2007). This would mean that if a user of a generic WSD system (i.e. based on hand-annotated examples from a generic corpus) would need to adapt it to a specific domain, he would be better off throwing away the generic examples and hand-tagging domain examples directly. This paper will show that domain adaptation is feasible, even for difficult domainrelated words, in the sense that generic corpora can be reused when deploying WSD systems in specific domains. We will also show that, given the source corpus, our technique can save up to 60% of effort when tagging domain-related oc</context>
</contexts>
<marker>Agirre, Martinez, 2004</marker>
<rawString>Eneko Agirre and David Martinez. 2004. The effect of bias on an automatically-built word sense corpus. Proceedings of the 4rd International Conference on Languages Resources and Evaluations (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O Lopez de Lacalle</author>
<author>David Martinez</author>
</authors>
<title>Exploring feature spaces with svd and unlabeled data for Word Sense Disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Recent Advances on Natural Language Processing (RANLP’05), Borovets,</booktitle>
<marker>Agirre, de Lacalle, Martinez, 2005</marker>
<rawString>E. Agirre, O.Lopez de Lacalle, and David Martinez. 2005. Exploring feature spaces with svd and unlabeled data for Word Sense Disambiguation. In Proceedings of the Conference on Recent Advances on Natural Language Processing (RANLP’05), Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
</authors>
<title>Applying alternating structure optimization to word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>77--84</pages>
<location>New York City.</location>
<contexts>
<context position="8051" citStr="Ando (2006)" startWordPosition="1340" endWordPosition="1341">main-adaptation experiment. They reduced significantly the effort of hand-tagging, but only obtained domainadaptation for smaller fractions of the source and target corpus. Similarly to these works we show that we can save annotation effort on the target corpus, but, in contrast, we do get domain adaptation when using the full dataset. In a way our approach is complementary, and we could also apply active learning to further reduce the number of target examples to be tagged. Though not addressing domain adaptation, other works on WSD also used SVD and are closely related to the present paper. Ando (2006) used Alternative Structured Optimization. She first trained one linear predictor for each target word, and then performed SVD on 7 carefully selected submatrices of the feature-to-predictor matrix of weights. The system attained small but consistent improvements (no significance data was given) on the Senseval-3 lexical sample datasets using SVD and unlabeled data. Gliozzo et al. (2005) used SVD to reduce the space of the term-to-document matrix, and then computed the similarity between train and test 43 instances using a mapping to the reduced space (similar to our SMA method in Section 4.2)</context>
</contexts>
<marker>Ando, 2006</marker>
<rawString>Rie Kubota Ando. 2006. Applying alternating structure optimization to word sense disambiguation. In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 77–84, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="9023" citStr="Blitzer et al. (2006)" startWordPosition="1495" endWordPosition="1498">d unlabeled data. Gliozzo et al. (2005) used SVD to reduce the space of the term-to-document matrix, and then computed the similarity between train and test 43 instances using a mapping to the reduced space (similar to our SMA method in Section 4.2). They combined other knowledge sources into a complex kernel using SVM. They report improved performance on a number of languages in the Senseval3 lexical sample dataset. Our present paper differs from theirs in that we propose an additional method to use SVD (the OMT method), and that we focus on domain adaptation. In the semi-supervised setting, Blitzer et al. (2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of-Speech tagger. They carefully select so-called ‘pivot features’ to learn linear predictors, perform SVD on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains. Our technique also uses SVD, but we directly apply it to all features, and thus avoid the need to define pivot features. In preliminary work we unsuccessfully tried to carry along the idea of pivot features to WSD. On the contrary, in (Agirre and Lopez de Lacalle, 2008) we show that methods clo</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>49--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2975" citStr="Chan and Ng, 2007" startWordPosition="495" endWordPosition="498"> to target scenario represents a weak baseline for domain adaptation, as it does not use any examples from the target domain. The target scenario represents the hard baseline, and in fact, if the domain adaptation scenario does not yield better results, the adaptation would have failed, as it would mean that the source examples are not useful when we do have hand-labeled target examples. Previous work shows that current state-of-theart WSD systems are not able to obtain better results on the adaptation scenario compared to the target scenario (Escudero et al., 2000; Agirre and Martinez, 2004; Chan and Ng, 2007). This would mean that if a user of a generic WSD system (i.e. based on hand-annotated examples from a generic corpus) would need to adapt it to a specific domain, he would be better off throwing away the generic examples and hand-tagging domain examples directly. This paper will show that domain adaptation is feasible, even for difficult domainrelated words, in the sense that generic corpora can be reused when deploying WSD systems in specific domains. We will also show that, given the source corpus, our technique can save up to 60% of effort when tagging domain-related occurrences. We perfor</context>
<context position="6630" citStr="Chan and Ng (2007)" startWordPosition="1104" endWordPosition="1107">the supervised adaptation scenario on the DSO corpus, which had examples from the Brown corpus and Wall Street Journal corpus. They found that the source corpus did not help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging a large general corpus would not guarantee robust broadcoverage WSD. Agirre and Mart´ınez (2000) used the DSO corpus in the supervised scenario to show that training on a subset of the source corpora that is topically related to the target corpus does allow for some domain adaptation. More recently, Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus. They used active learning, count-merging, and predominant sense estimation in order to save target annotation effort. They showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved. They also showed that using the source corpus allowed to significantly improve results when only 10%- 30% of the target corpus was used for training. Unfortunately, no data was given about the target corpus results, </context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49–56, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy classifier: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In Proceedings of of th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5410" citStr="Chelba and Acero, 2004" startWordPosition="904" endWordPosition="907">esults are presented in Section 6. Section 7 presents the discussion and some analysis of this paper and finally Section 8 draws the conclusions. 2 Prior work Domain adaptation is a practical problem attracting more and more attention. In the supervised setting, a recent paper by Daum´e III (2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks. His method improves or equals over previously explored more sophisticated methods (Daum´e III and Marcu, 2006; Chelba and Acero, 2004). The feature augmentation consists in making three version of the original features: a general, a sourcespecific and a target-specific versions. That way the augmented source contains the general and source-specific version and the augmented target data general and specific versions. The idea behind this is that target domain data has twice the influence as the source when making predictions about test target data. We reimplemented this method and show that our results are better. Regarding WSD, some initial works made a basic analysis of domain adaptation issues. Escudero et al. (2000) teste</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy classifier: Little data can help a lot. In Proceedings of of th Conference on Empirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>Goerge Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="12905" citStr="Deerwester et al., 1990" startWordPosition="2144" endWordPosition="2147">e previous/posterior lemma/word-form in the sentence, and the content words in a ±4-word window around the target. Syntactic dependencies use the object, subject, noun-modifier, preposition, and sibling lemmas, when available. Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001). We refer to these features as original features. 4.2 SVD features Apart from the original space of features, we have used the so called SVD features, obtained from the projection of the feature vectors into the reduced space (Deerwester et al., 1990). Basically, 44 we set a term-by-document or feature-by-example matrix M from the corpus (see section below for more details). SVD decomposes M into three matrices, M = UEVT . If the desired number of dimensions in the reduced space is p, we select p rows from E and V , yielding Ep and Vp respectively. We can map any feature vector i� (which represents either a train or test example) into the p-dimensional space as follows: tp = �iT VpE�1 p . Those mapped vectors have p dimensions, and each of the dimensions is what we call a SVD feature. We have explored two different variants in order to bui</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, Goerge Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Escudero</author>
<author>Lluiz M´arquez</author>
<author>German Rigau</author>
</authors>
<title>An Empirical Study of the Domain Dependence of Supervised Word Sense Didanbiguation Systems.</title>
<date>2000</date>
<booktitle>Proceedings of the joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, EMNLP/VLC.</booktitle>
<marker>Escudero, M´arquez, Rigau, 2000</marker>
<rawString>Gerard Escudero, Lluiz M´arquez, and German Rigau. 2000. An Empirical Study of the Domain Dependence of Supervised Word Sense Didanbiguation Systems. Proceedings of the joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, EMNLP/VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10808" citStr="Fellbaum, 1998" startWordPosition="1794" endWordPosition="1795"> domain adaptation. 3 Data sets The dataset we use was designed for domainrelated WSD experiments by Koeling et al. (2005), and is publicly available. The examples come from the BNC (Leech, 1992) and the SPORTS and FINANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 (Fellbaum, 1998). In our experiments the BNC examples play the role of general source corpora, and the FINANCES and SPORTS examples the role of two specific domain target corpora. Compared to the DSO corpus used in prior work (cf. Section 2) this corpus has been explicitly created for domain adaptation studies. DSO contains texts coming from the Brown corpus and the Wall Street Journal, but the texts are not classified according to specific domains (e.g. Sports, Finances), which make DSO less suitable to study domain adaptation. The fact that the selected nouns are related to the target domain makes the (Koel</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Massimiliano Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain Kernels for Word Sense Disambiguation. 43nd Annual Meeting of the Association for Computational Linguistics.</title>
<date>2005</date>
<contexts>
<context position="4238" citStr="Gliozzo et al., 2005" startWordPosition="708" endWordPosition="711">as designed to study the effect of domains in WSD (Koeling et al., 2005). It comprises 41 Proceedings of the 12th Conference of the European Chapter of the ACL, pages 42–50, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 42 nouns which are highly relevant in the SPORTS and FINANCES domains, with 300 examples for each. The use of two target domains strengthens the conclusions of this paper. Our system uses Singular Value Decomposition (SVD) in order to find correlations between terms, which are helpful to overcome the scarcity of training data in WSD (Gliozzo et al., 2005). This work explores how this ability of SVD and a combination of the resulting feature spaces improves domain adaptation. We present two ways to combine the reduced spaces: kernel combination with Support Vector Machines (SVM), and k Nearest-Neighbors (k-NN) combination. The paper is structured as follows. Section 2 reviews prior work in the area. Section 3 presents the data sets used. In Section 4 we describe the learning features, including the application of SVD, and in Section 5 the learning methods and the combination. The experimental results are presented in Section 6. Section 7 presen</context>
<context position="8441" citStr="Gliozzo et al. (2005)" startWordPosition="1396" endWordPosition="1399">d we could also apply active learning to further reduce the number of target examples to be tagged. Though not addressing domain adaptation, other works on WSD also used SVD and are closely related to the present paper. Ando (2006) used Alternative Structured Optimization. She first trained one linear predictor for each target word, and then performed SVD on 7 carefully selected submatrices of the feature-to-predictor matrix of weights. The system attained small but consistent improvements (no significance data was given) on the Senseval-3 lexical sample datasets using SVD and unlabeled data. Gliozzo et al. (2005) used SVD to reduce the space of the term-to-document matrix, and then computed the similarity between train and test 43 instances using a mapping to the reduced space (similar to our SMA method in Section 4.2). They combined other knowledge sources into a complex kernel using SVM. They report improved performance on a number of languages in the Senseval3 lexical sample dataset. Our present paper differs from theirs in that we propose an additional method to use SVD (the OMT method), and that we focus on domain adaptation. In the semi-supervised setting, Blitzer et al. (2006) used Structural C</context>
<context position="13921" citStr="Gliozzo et al., 2005" startWordPosition="2325" endWordPosition="2328">p-dimensional space as follows: tp = �iT VpE�1 p . Those mapped vectors have p dimensions, and each of the dimensions is what we call a SVD feature. We have explored two different variants in order to build the reduced matrix and obtain the SVD features, as follows. Single Matrix for All target words (SVDSMA). The method comprises the following steps: (i) extract bag-of-word features (terms in this case) from unlabeled corpora, (ii) build the term-bydocument matrix, (iii) decompose it with SVD, and (iv) map the labeled data (train/test). This technique is very similar to previous work on SVD (Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001). The dimensionality reduction is performed once, over the whole unlabeled corpus, and it is then applied to the labeled data of each word. The reduced space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features. Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix. One Matrix per Target word (SVD-OMT). For each word: (i) construct a corpus with its occurrences in the labeled and,</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Massimiliano Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain Kernels for Word Sense Disambiguation. 43nd Annual Meeting of the Association for Computational Linguistics. (ACL05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Koeling</author>
<author>D McCarthy</author>
<author>J Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. HLT/EMNLP,</booktitle>
<pages>419--426</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="3689" citStr="Koeling et al., 2005" startWordPosition="618" endWordPosition="621">es from a generic corpus) would need to adapt it to a specific domain, he would be better off throwing away the generic examples and hand-tagging domain examples directly. This paper will show that domain adaptation is feasible, even for difficult domainrelated words, in the sense that generic corpora can be reused when deploying WSD systems in specific domains. We will also show that, given the source corpus, our technique can save up to 60% of effort when tagging domain-related occurrences. We performed on a publicly available corpus which was designed to study the effect of domains in WSD (Koeling et al., 2005). It comprises 41 Proceedings of the 12th Conference of the European Chapter of the ACL, pages 42–50, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 42 nouns which are highly relevant in the SPORTS and FINANCES domains, with 300 examples for each. The use of two target domains strengthens the conclusions of this paper. Our system uses Singular Value Decomposition (SVD) in order to find correlations between terms, which are helpful to overcome the scarcity of training data in WSD (Gliozzo et al., 2005). This work explores how this ability of SVD and a </context>
<context position="10315" citStr="Koeling et al. (2005)" startWordPosition="1710" endWordPosition="1713">ervised domain adaptation results for WSD. The methods used in this paper originated in (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007), where SVD over a feature-to-documents matrix improved WSD performance with and without unlabeled data. The use of several kNN classifiers trained on a number of reduced and original spaces was shown to get the best results in the Senseval-3 dataset and ranked second in the SemEval 2007 competition. The present paper extends this work and applies it to domain adaptation. 3 Data sets The dataset we use was designed for domainrelated WSD experiments by Koeling et al. (2005), and is publicly available. The examples come from the BNC (Leech, 1992) and the SPORTS and FINANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 (Fellbaum, 1998). In our experiments the BNC examples play the role of general source corpora, and the FINANCES and SPORTS </context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. HLT/EMNLP, pages 419–426, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="10388" citStr="Leech, 1992" startWordPosition="1724" endWordPosition="1725">ed in (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007), where SVD over a feature-to-documents matrix improved WSD performance with and without unlabeled data. The use of several kNN classifiers trained on a number of reduced and original spaces was shown to get the best results in the Senseval-3 dataset and ranked second in the SemEval 2007 competition. The present paper extends this work and applies it to domain adaptation. 3 Data sets The dataset we use was designed for domainrelated WSD experiments by Koeling et al. (2005), and is publicly available. The examples come from the BNC (Leech, 1992) and the SPORTS and FINANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 (Fellbaum, 1998). In our experiments the BNC examples play the role of general source corpora, and the FINANCES and SPORTS examples the role of two specific domain target corpora. Compared to the </context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>G. Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(1):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Eneko Agirre</author>
</authors>
<date>2000</date>
<booktitle>One Sense per Collocation and Genre/Topic Variations. Conference on Empirical Method in Natural Language.</booktitle>
<marker>Martinez, Agirre, 2000</marker>
<rawString>David Martinez and Eneko Agirre. 2000. One Sense per Collocation and Genre/Topic Variations. Conference on Empirical Method in Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-01),</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="12653" citStr="Pedersen, 2001" startWordPosition="2103" endWordPosition="2104">res We relied on the usual features used in previous WSD work, grouped in three main sets. Local collocations comprise the bigrams and trigrams formed around the target word (using either lemmas, word-forms, or PoS tags) , those formed with the previous/posterior lemma/word-form in the sentence, and the content words in a ±4-word window around the target. Syntactic dependencies use the object, subject, noun-modifier, preposition, and sibling lemmas, when available. Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001). We refer to these features as original features. 4.2 SVD features Apart from the original space of features, we have used the so called SVD features, obtained from the projection of the feature vectors into the reduced space (Deerwester et al., 1990). Basically, 44 we set a term-by-document or feature-by-example matrix M from the corpus (see section below for more details). SVD decomposes M into three matrices, M = UEVT . If the desired number of dimensions in the reduced space is p, we select p rows from E and V , yielding Ep and Vp respectively. We can map any feature vector i� (which repr</context>
</contexts>
<marker>Pedersen, 2001</marker>
<rawString>T. Pedersen. 2001. A Decision Tree of Bigrams is an Accurate Predictor of Word Sense. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-01), Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task-17: English lexical sample, srl and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14341" citStr="Pradhan et al., 2007" startWordPosition="2394" endWordPosition="2397">led corpora, (ii) build the term-bydocument matrix, (iii) decompose it with SVD, and (iv) map the labeled data (train/test). This technique is very similar to previous work on SVD (Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001). The dimensionality reduction is performed once, over the whole unlabeled corpus, and it is then applied to the labeled data of each word. The reduced space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features. Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix. One Matrix per Target word (SVD-OMT). For each word: (i) construct a corpus with its occurrences in the labeled and, if desired, unlabeled corpora, (ii) extract all features, (iii) build the featureby-example matrix, (iv) decompose it with SVD, and (v) map all the labeled training and test data for the word. Note that this variant performs one SVD process for each target word separately, hence its name. When building the SVD-OMT matrices we can use only the training data (TRAIN) or both the train and unlabeled data (+UNLAB). When </context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task-17: English lexical sample, srl and all words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony G Rose</author>
<author>Mark Stevenson</author>
<author>Miles Whitehead</author>
</authors>
<title>The reuters corpus volumen 1 from yesterday’s news to tomorrow’s language resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC2002),</booktitle>
<pages>827--832</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="10467" citStr="Rose et al., 2002" startWordPosition="1736" endWordPosition="1739"> over a feature-to-documents matrix improved WSD performance with and without unlabeled data. The use of several kNN classifiers trained on a number of reduced and original spaces was shown to get the best results in the Senseval-3 dataset and ranked second in the SemEval 2007 competition. The present paper extends this work and applies it to domain adaptation. 3 Data sets The dataset we use was designed for domainrelated WSD experiments by Koeling et al. (2005), and is publicly available. The examples come from the BNC (Leech, 1992) and the SPORTS and FINANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 (Fellbaum, 1998). In our experiments the BNC examples play the role of general source corpora, and the FINANCES and SPORTS examples the role of two specific domain target corpora. Compared to the DSO corpus used in prior work (cf. Section 2) this corpus has been explicitly c</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Tony G. Rose, Mark Stevenson, and Miles Whitehead. 2002. The reuters corpus volumen 1 from yesterday’s news to tomorrow’s language resources. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC2002), pages 827–832, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Zelikovitz</author>
<author>Haym Hirsh</author>
</authors>
<title>Using LSI for text classification in the presence of background text.</title>
<date>2001</date>
<booktitle>In Henrique Paques, Ling Liu, and</booktitle>
<pages>113--118</pages>
<editor>David Grossman, editors,</editor>
<publisher>ACM Press,</publisher>
<location>Atlanta, US.</location>
<contexts>
<context position="13950" citStr="Zelikovitz and Hirsh, 2001" startWordPosition="2329" endWordPosition="2332"> follows: tp = �iT VpE�1 p . Those mapped vectors have p dimensions, and each of the dimensions is what we call a SVD feature. We have explored two different variants in order to build the reduced matrix and obtain the SVD features, as follows. Single Matrix for All target words (SVDSMA). The method comprises the following steps: (i) extract bag-of-word features (terms in this case) from unlabeled corpora, (ii) build the term-bydocument matrix, (iii) decompose it with SVD, and (iv) map the labeled data (train/test). This technique is very similar to previous work on SVD (Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001). The dimensionality reduction is performed once, over the whole unlabeled corpus, and it is then applied to the labeled data of each word. The reduced space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features. Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix. One Matrix per Target word (SVD-OMT). For each word: (i) construct a corpus with its occurrences in the labeled and, if desired, unlabeled corpor</context>
</contexts>
<marker>Zelikovitz, Hirsh, 2001</marker>
<rawString>Sarah Zelikovitz and Haym Hirsh. 2001. Using LSI for text classification in the presence of background text. In Henrique Paques, Ling Liu, and David Grossman, editors, Proceedings of CIKM-01, 10th ACM International Conference on Information and Knowledge Management, pages 113–118, Atlanta, US. ACM Press, New York, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
<author>Yee Seng Chan</author>
</authors>
<title>Word sense disambiguation using OntoNotes: An empirical study.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1002--1010</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="7321" citStr="Zhong et al., 2008" startWordPosition="1218" endWordPosition="1221"> 21 nouns from the DSO corpus. They used active learning, count-merging, and predominant sense estimation in order to save target annotation effort. They showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved. They also showed that using the source corpus allowed to significantly improve results when only 10%- 30% of the target corpus was used for training. Unfortunately, no data was given about the target corpus results, thus failing to show that domainadaptation succeeded. In followup work (Zhong et al., 2008), the feature augmentation approach was combined with active learning and tested on the OntoNotes corpus, on a large domain-adaptation experiment. They reduced significantly the effort of hand-tagging, but only obtained domainadaptation for smaller fractions of the source and target corpus. Similarly to these works we show that we can save annotation effort on the target corpus, but, in contrast, we do get domain adaptation when using the full dataset. In a way our approach is complementary, and we could also apply active learning to further reduce the number of target examples to be tagged. T</context>
</contexts>
<marker>Zhong, Ng, Chan, 2008</marker>
<rawString>Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008. Word sense disambiguation using OntoNotes: An empirical study. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1002–1010, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>