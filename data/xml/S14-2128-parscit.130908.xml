<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000073">
<note confidence="0.8182355">
DLSI
UMCC
_
_
</note>
<title confidence="0.9988615">
SemSim: Multilingual System for Measuring
Semantic Textual Similarity
</title>
<author confidence="0.911969">
Alexander ChÃ¡vez
HÃ©ctor DÃ¡vila
</author>
<affiliation confidence="0.923126">
DI, University of Matanzas, Cuba.
</affiliation>
<note confidence="0.359538">
{alexander.chavez,
</note>
<email confidence="0.856557">
hector.davila}@umcc.cu
</email>
<sectionHeader confidence="0.987924" genericHeader="abstract">
Abstract
</sectionHeader>
<subsectionHeader confidence="0.728237">
In this paper we describe the
</subsectionHeader>
<bodyText confidence="0.999982576923077">
specifications and results of
UMCC_DLSI system, which was
involved in Semeval-2014 addressing two
subtasks of Semantic Textual Similarity
(STS, Task 10, for English and Spanish),
and one subtask of Cross-Level Semantic
Similarity (Task 3). As a supervised
system, it was provided by different types
of lexical and semantic features to train a
classifier which was used to decide the
correct answers for distinct subtasks.
These features were obtained applying the
Hungarian algorithm over a semantic
network to create semantic alignments
among words. Regarding the Spanish
subtask of Task 10 two runs were
submitted, where our Run2 was the best
ranked with a general correlation of 0.807.
However, for English subtask our best run
(Run1 of our 3 runs) reached 16th place of
38 of the official ranking, obtaining a
general correlation of 0.682. In terms of
Task 3, only addressing Paragraph to
Sentence subtask, our best run (Run1 of 2
runs) obtained a correlation value of 0.760
reaching 3rd place of 34.
</bodyText>
<sectionHeader confidence="0.99076" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995207333333333">
Many applications of language processing rely on
measures of proximity or remoteness of various
kinds of linguistic units (words, meanings,
</bodyText>
<note confidence="0.8471347">
This work is licensed under a Creative Commons
Attribution 4.0 International Licence. Page numbers and
proceedings footer are added by the organisers. Licence
details: http://creativecommons.org/licenses/by/4.0/
Yoan GutiÃ©rrez
Antonio FernÃ¡ndez-OrquÃ­n
AndrÃ©s Montoyo
Rafael MuÃ±oz
DLSI, University of Alicante, Spain.
{ygutierrez
</note>
<email confidence="0.852413">
montoyo,rafael}@dlsi.ua.es,
antonybr@yahoo.com
</email>
<bodyText confidence="0.999706625">
sentences, documents). Thus, issues such as
disambiguation of meanings, detection of lexical
chains, establishing relationships between
documents, clustering, etc., require accurate
similarity measures.
The problem of formalizing and quantifying an
intuitive notion of similarity has a long history in
philosophy, psychology, artificial intelligence,
and through the years has followed many different
perspectives (Hirst, 2001). Recent research in the
field of Computational Linguistics has
emphasized the perspective of semantic relations
between two lexemes in a lexical resource, or its
inverse, semantic distance. The similarity of
sentences is a confidence score that reflects the
relationship between the meanings of two
sentences. This similarity has been addressed in
the literature with terminologies such as affinity,
proximity, distance, difference and divergence
(Jenhani, et al., 2007). The different applications
of text similarity have been separated into a group
of similarity tasks: between two long texts, for
document classification; between a short text with
a long text, for Web search; and between two short
texts, for paraphrase recognition, automatic
machine translation, etc. (Han, et al., 2013).
At present, the calculation of the similarity
between texts has been tackled from different
points of views. Some have opted for a single
measure to capture all the features of texts and
other models have been trained with various
measures to take text features separately. In this
work, we addressed the combination of several
measures using a Supervised Machine Learning
(SVM) approach. Moreover, we intend to
introduce a new approach to calculate textual
similarities using a knowledge-based system,
which is based on a set of cases composed by a
vector with values of several measures. We also
combined both approaches.
</bodyText>
<page confidence="0.582261">
716
</page>
<note confidence="0.9525395">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 716â€“721,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999797625">
After this introduction, the rest of the paper is
organized as follows. Section 2 shows the Pre-
processing stage. Subsequently, in Section 3 we
show the different features used in our system. In
Section 4 we describe our knowledge-based
system. Tasks and runs are provided in Section 5.
Finally, the conclusions and further work can be
found in Section 6.
</bodyText>
<sectionHeader confidence="0.567738" genericHeader="introduction">
2 Pre-processing
</sectionHeader>
<bodyText confidence="0.995054666666667">
Below are listed the pre-processing steps
performed by our system. In bold we emphasize
some cases which were used in different tasks.
</bodyText>
<listItem confidence="0.984849225806451">
â€¢ All brackets were removed.
â€¢ The abbreviations were expanded to their
respective meanings. It was applied using a
list of the most common abbreviations in
English, with 819 and Spanish with 473.
Phrases like â€œThe G8â€ and â€œThe Group of
Eightâ€ are detected as identical.
â€¢ Deletion of hyphen to identify words
forms. For example, â€œwell-studiedâ€ was
replaced by â€œwell studiedâ€. Example taken
from line 13 of MSRpar corpus in test set
of Semeval STS 2012 (Agirre, et al., 2012).
â€¢ The sentences were tokenized and POS-
tagged using Freeling 3.0 (PadrÃ³ and
Stanilovsky, 2012).
â€¢ All contractions were expanded. For
example: n&apos;t, &apos;mand &apos;s. In the case of &apos;s was
replaced with â€œisâ€ or â€œofâ€, â€œTom&apos;s badâ€ to
â€œTom is badâ€ and â€œTom&apos;s childâ€ by &amp;quot;Child
of Tom&amp;quot;. (Only for English tasks).
â€¢ Punctuation marks were removed from the
tokens except for the decimal point in
numbers.
â€¢ Stop words were removed. We used a list
of the most common stop words. (28 for
English and 48 for Spanish).
â€¢ The words were mapped to the most
common sense of WordNet 3.0. (Only for
Spanish task).
â€¢ A syntactic tree was built for every
sentence using Freeling 3.0.
</listItem>
<bodyText confidence="0.9085104">
1 The windows is the number of intermediate words
between two words.
2 Dataset of high quality English paragraphs containing over
three billion words and it is available in
http://ebiquity.umbc.edu/resource/html/id/351
</bodyText>
<sectionHeader confidence="0.988344" genericHeader="method">
3 Features Extraction
</sectionHeader>
<bodyText confidence="0.999600444444444">
Measures of semantic similarity have been
traditionally used between words or concepts, and
much less between text segments, (i.e. two or
more words). The emphasis on word to word
similarity is probably due to the availability of
resources that specifically encode relations
between words or concepts (e.g. WordNet)
(Mihalcea, et al., 2006). Following we describe
the similarity measures used in this approach.
</bodyText>
<subsectionHeader confidence="0.99948">
3.1 Semantic Similarity of Words
</subsectionHeader>
<bodyText confidence="0.999867428571429">
A relatively large number of word to word
similarity metrics have previously been proposed
in the literature, ranging from distance-oriented
measures computed on semantic networks, to
metrics based on models of distributional
similarity learned from large text collections
(Mihalcea, et al., 2006).
</bodyText>
<subsectionHeader confidence="0.996097">
3.2 Corpus-based Measures
</subsectionHeader>
<bodyText confidence="0.999836153846154">
Corpus-based measures of word semantic
similarity try to identify the degree of similarity
between words using information exclusively
derived from large corpora (Mihalcea, et al.,
2006). We considered one metric named Latent
Semantic Analysis (LSA) (Landauer, et al., 1998).
Latent Semantic Analysis: The Latent
semantic analysis (LSA) is a corpus/document
based measure proposed by Landauer in 1998. In
LSA term co-occurrences in a corpus are captured
by means of a dimensionality reduction operated
by singular value decomposition (SVD) on the
term-by-document matrix ğ‘‡ representing the
corpus (Mihalcea, et al., 2006). There is a
variation of LSA called HAL (Hyperspace
Analog to Language) (Burgess, et al., 1998) that
is based on the co-occurrence of words in a
common context. The variation consists of
counting the number of occurrences in that two
words appear at n1 distance (called windows).
For the co-occurrence matrix of words we took
as core the UMBC WebBase corpus2 (Han, et al.,
2013), which is derived from the Stanford
WebBase project3. For the calculation of HAL
measure we used the Cosine Similarity between
the vectors for each pair of words.
</bodyText>
<figure confidence="0.376068">
3 Stanford WebBase 2001. http://bit.ly/WebBase.
717
</figure>
<subsectionHeader confidence="0.968423">
3.3 Knowledge-based Measures
</subsectionHeader>
<bodyText confidence="0.987699428571429">
There are many measures developed to quantify
the degree of semantic relation between two
words senses using semantic network
information. For example:
Leacock &amp; Chodorow Similarity: The
Leacock &amp; Chodorow (LC) similarity is
determined as follows:
</bodyText>
<equation confidence="0.78138">
ğ‘†ğ‘–ğ‘šğ‘™ğ‘ = âˆ’log (ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„2âˆ—ğ· ) (1)
</equation>
<bodyText confidence="0.9853321">
Where length is the length of the shortest path
between senses using node-counting and D is the
maximum depth of the taxonomy (Leacock and
Chodorow, 1998)
Wu and Palmer: The Wu and Palmer
similarity metric (Wup) measures the depth of two
given senses in the WordNet taxonomy, and the
depth of the least common subsumer (LCS), and
combine them into a similarity score (Wu and
Palmer, 1994):
</bodyText>
<equation confidence="0.994707">
2âˆ—ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ¿ğ¶ğ‘†)
ğ‘†ğ‘–ğ‘šğ‘Šğ‘¢ğ‘ = ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)+ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) (2)
</equation>
<bodyText confidence="0.961258666666667">
Resnik: The Resnik similarity (Res) returns
the information content (IC) of the LCS of two
senses:
</bodyText>
<equation confidence="0.993239666666667">
ğ‘†ğ‘–ğ‘šğ‘…ğ‘’ğ‘  = ğ¼ğ¶(ğ¿ğ¶ğ‘†) (3)
Where IC is defined as:
ğ¼ğ¶(ğ‘) = âˆ’logğ‘ƒ(ğ‘) (4)
</equation>
<bodyText confidence="0.987829285714286">
And P(c) is the probability of encountering an
instance of sense c in a large corpus (Resnik,
1995) (Mihalcea, et al., 2006).
Lin: The Lin similarity builds on Resnikâ€™s
measure and adds a normalization factor
consisting of the information content of the two
inputs senses (Lin, 1998):
</bodyText>
<equation confidence="0.948845428571429">
2âˆ—ğ¼ğ¶(ğ¿ğ¶ğ‘†)
ğ‘†ğ‘–ğ‘šğ¿ğ‘–ğ‘› = (5)
ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’)+ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2)
Jiang &amp; Conrath: The Jiang and Conrath
similarity (JC) is defined as follows (Jiang and
Conrath, 1997):
ğ‘†ğ‘–ğ‘šğ‘ƒğ‘ğ‘¡â„ = âˆ’log ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)(7)
</equation>
<bodyText confidence="0.875293">
Where ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1) is the
number of edges in the shortest path between
ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1and ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2.
Word Similarity: In order to calculate the
similarity between two words (WS) we used the
following expression:
</bodyText>
<equation confidence="0.9985685">
ğ‘Šğ‘†(ğ‘¤1, ğ‘¤2) = ğ‘šğ‘ğ‘¥ğ‘ 1 âˆˆ ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’ğ‘ (ğ‘¤1) ğ‘ ğ‘–ğ‘š(ğ‘ 1, ğ‘ 2)
ğ‘ 2 âˆˆ ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’ğ‘ (ğ‘¤2)
</equation>
<bodyText confidence="0.994741">
Where ğ‘ ğ‘–ğ‘š(ğ‘ 1, ğ‘ 2) is one of the similarity
metrics at sense level previously described.
</bodyText>
<subsectionHeader confidence="0.973219">
3.4 Lexical Features
</subsectionHeader>
<bodyText confidence="0.9405847">
We used a well-known lexical attributes similarity
measures based on distances between strings.
Dice-Similarity, Euclidean-Distance, Jaccard-
Similarity, Jaro-Winkler, Levenstein Distance,
Overlap-Coefficient, QGrams Distance, Smith-
Waterman, Smith-Waterman-Gotoh, Smith-
Waterman-Gotoh-Windowed-Affine.
These metrics have been obtained from an API
(Application Program Interface) SimMetrics
library v1.5 for.NET4 2.0.
</bodyText>
<subsectionHeader confidence="0.959592">
3.5 Word Similarity Models
</subsectionHeader>
<bodyText confidence="0.9857765">
With the purpose of calculating the similarity
between two words, we developed two models
involving the previous word similarity metrics.
These were defined as:
Max Word Similarity: The Max Word
Similarity (MaxSim) is defined as follows:
</bodyText>
<equation confidence="0.856311666666667">
ğ‘€ğ‘ğ‘¥ğ‘†ğ‘–ğ‘š(ğ‘¤1, ğ‘¤2) =
{1 ğ‘–ğ‘“ğ‘„ğºğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¤1,ğ‘¤2) = 1
ğ‘€ğ‘ğ‘¥(ğ‘†ğ‘–ğ‘šğ»ğ‘ğ‘™ (ğ‘¤1, ğ‘¤2), ğ‘†ğ‘–ğ‘šğ‘Šğ‘¢ğ‘ (ğ‘¤1, ğ‘¤2))
</equation>
<bodyText confidence="0.9980886">
Where ğ‘„ğºğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¤1,ğ‘¤2)is the QGram-
Distance between w1 and w2.
Statistics and Weight Ratio: For calculating
the weight ratio in this measure of similarity was
used WordNet 3.0 and it was defined in (10):
</bodyText>
<figure confidence="0.944169384615385">
1
ğ‘†ğ‘–ğ‘šğ‘—ğ‘ =ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’
1)+ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2)âˆ’2âˆ—ğ¼ğ¶(ğ¿ğ¶ğ‘†) (6)
PathLen: The PathLen similarity (Len) ğ‘†ğ‘¡ğ‘ğ‘Šğ‘’ğ‘–ğ‘…ğ‘ğ‘¡ (ğ‘¤1,ğ‘¤2) = (ğ‘†ğ‘–ğ‘šğ»ğ‘ğ‘™(ğ‘¤1,ğ‘¤2) + ( 1 ))
involves the path lengths between two senses in ğ‘Šğ‘’ğ‘–ğ‘…ğ‘ğ‘¡(ğ‘¤1,ğ‘¤2)
the taxonomy (Pedersen, et al., 2004). 2
(10)
4 Copyright (c) 2006 by Chris Parkinson, available in
http://sourceforge.net/projects/simmetrics/
718
Euclidian Distance was assigned to the new pair
of phrases.
5 Tasks and runs
</figure>
<bodyText confidence="0.881331333333333">
Where WeiRat(w1, w2) takes a value based
on the type of relationship between w1 and w2.
The possible values are defined in Table 1.
</bodyText>
<table confidence="0.9980455">
Value Relation between w1 and w2
10 Antonym.
1 Synonym.
2 Direct Hypernym, Similar_To or
Derivationally Related Form.
3 Two-links indirect Hypernym, Similar_To
or Derivationally Related Form.
3 One word is often found in the gloss of the
other.
9 Otherwise.
</table>
<tableCaption confidence="0.999701">
Table 1: Values of Weight Ratio.
</tableCaption>
<subsectionHeader confidence="0.985334">
3.6 Sentence Alignment
</subsectionHeader>
<bodyText confidence="0.9999825625">
In the recognition of textsâ€™ similarities, several
methods of lexical alignment have been used and
can be appreciated by different point of views
(Brockett, 2007) (Dagan, et al., 2005). Glickman
(2006) used the measurement of the overleap
grade between bags of words as a form of
sentence alignment. Rada et al. (2006) made
reference to an all-for-all alignment, leaving open
the possibility when the same word of a sentence
is aligned with several sentences. For this task, we
used the Hungarian assignment algorithm as a
way to align two sentences (Kuhn, 1955). Using
that, the alignment cost between the sentences was
reduced. To increase the semantic possibilities we
used all word similarity metrics (including the two
word similarity models) as a function cost.
</bodyText>
<subsectionHeader confidence="0.888884">
3.7 N-Grams Alignment
</subsectionHeader>
<bodyText confidence="0.999394">
Using the Max Word Similarity model, we
calculated three features based on 2-gram, 3-gram
and 4-gram alignment with the Hungarian
algorithm.
</bodyText>
<sectionHeader confidence="0.995394" genericHeader="method">
4 Knowledge-based System
</sectionHeader>
<bodyText confidence="0.999844928571428">
For similarity calculation between two phrases,
we developed a knowledge-based system using
SemEval-2012, SemEval-2013 and SemEval-
2014 training corpus (Task 10 and Task 1 for the
last one). For each training pair of phrases we
obtained a vector with all measures explained
above. Having it, we estimated the similarity
value between two new phrases by applying the
Euclidian distance between the new vector (made
with the sentence pair we want to estimate the
similarity value) and each vector in the training
corpus. Then, the value of the instance with minor
Our system participated in Sentence to Phrase
subtask of Task 3: â€œCross-Level Semantic
Similarityâ€ (Jurgens, et al., 2014) and in two
subtasks of Task 10: â€œMultilingual Semantic
Textual Similarityâ€ of SemEval-2014. It is
important to remark that our system, using SVM
approach, did not participate in Task 1:
â€œEvaluation of compositional distributional
semantic models on full sentences through
semantic relatedness and textual entailmentâ€, due
to deadline issues. We compared our system
results with the final ranking of Task 1 and we
could have reached the 6th place of the ranking for
Relatedness Subtask with a 0.781 of correlation
coefficient, and the 9th place for Entailment
Subtask with an accuracy of 77.41%.
</bodyText>
<table confidence="0.999762586206897">
Task Task 10 Task 3
10 En Sentence
Sp to
Phrase
Features/Runs 1 2 1 2 3 1 2
PathLenAlign x x x x x
ResAlign x x x x x
LcAlign x x x x x
WupAlign x x x x x
Res x x x x x
Lc x x x x x
DiceSimilarity x x x x x x
EuclideanDistance x x x x x x
JaccardSimilarity x x x x x x
JaroWinkler x x x x x x
Levenstein x x x x x x
Overlap- x x x x x x
Coefficient
QGramsDistance x x x x x x
SmithWaterman x x x x x x
SmithWatermanGotoh x x x x x x
SmithWatermanGotoh- x x x x x x
WindowedAffine
BiGramAlingHungMax x x x x x
TriGramAlingHungMax x x x x x
TetraGramAlingHungMax x x x x x
WordAlingHungStatWeigthRatio x x x x x
SentenceLengthPhrase1 x x x x x
SentenceLengthPhrase2 x x x x x
</table>
<tableCaption confidence="0.961225">
Table 2: Features and runs. Spanish (Sp) and
English (En).
</tableCaption>
<bodyText confidence="0.99535375">
In Table 2 is important to remark the
following situations:
ï‚· In Task 10 Spanish (two runs), we used the
training corpus of Task 10 English.
</bodyText>
<page confidence="0.707671">
719
</page>
<listItem confidence="0.989547153846154">
â€¢ In Run2 of Task 10 English, the similarity
score was replaced for the knowledge-
based system value if Euclidean Distance
of the most similar case was less than 0.30.
â€¢ Run3 of Task 10 English was a knowledge-
based system.
â€¢ In Run1 of Sentence to Phrase of Task 3,
we trained the SVM model using only the
training corpus of this task.
â€¢ In Run2 of Sentence to Phrase of Task 3,
we trained the SVM model using the
training corpus of this task and the training
corpus of Task 10 English.
</listItem>
<sectionHeader confidence="0.994619" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998769">
In this paper we introduced a new framework for
recognizing Semantic Textual Similarity,
involving feature extraction for SVM model and a
knowledge-based system. We analyzed different
ways to estimate textual similarities applying this
framework. We can see in Table 3 that all runs
obtained encouraging results. Our best run was
first position of the ranking for task 10 (Spanish)
and other important positions were reached in the
others subtasks. According to our participation,
we used a SVM which works with features
extracted from six different strategies: String-
Based Similarity Measures, Semantic Similarity
Measures, Lexical-Semantic Alignment,
Statistical Similarity Measures and Semantic
Alignment. Finally, we can conclude that our
system achieved important results and it is able to
be applied on different scenarios, such as task 10,
task 3.1 and task 1. See Table 3 and the beginning
of Section 5.
</bodyText>
<table confidence="0.999391">
Subtask Run SemEval-
2014
Position
Task 10- Run1 4
Spanish
Run2 1
Task 10- Run1 16
English
Run2 18
Run3 33
Task-3 Run1 3
Run2 16
</table>
<tableCaption confidence="0.999086">
Table 3: SemEval-2014 results.
</tableCaption>
<bodyText confidence="0.9996525">
As further work, we plan to analyze the main
differences between task 10 for Spanish and
English in order to homogenise both systemâ€™s
results.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998920666666667">
This research work has been partially funded by
the University of Alicante, Generalitat
Valenciana, Spanish Government and the
European Commission through the projects,
&amp;quot;Tratamiento inteligente de la informaciÃ³n para la
ayuda a la toma de decisiones&amp;quot; (GRE12-44),
ATTOS (TIN2012-38536-C03-03), LEGOLANG
(TIN2012-31224), SAM (FP7-611312), FIRST
(FP7-287607) and ACOMP/2013/067.
</bodyText>
<sectionHeader confidence="0.712418" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.9898865125">
Eneko Agirre, Mona Diab, Daniel Cer and Aitor
Gonzalez-Agirre, 2012. SemEval 2012 Task 6: A
Pilot on Semantic Textual Similarity.. s.l., First
Join Conference on Lexical and Computational
Semantic (*SEM), MontrÃ©al, Canada. 2012., pp.
385-393.
Chris Brockett, 2007. Aligning the RTE 2006 Corpus.
Microsoft Research, p. 14.
Curt Burgess, Kay Livesay and Kevin Lund, 1998.
Explorations in Context Space: Words,
Sentences, Discourse. Discourse Processes, Issue
25, pp. 211 - 257.
Ido Dagan, Oren Glickman and Bernardo Magnini,
2005. The PASCAL Recognising Textual
Entailment Challenge. En: Proceedings of the
PASCAL Challenges Workshop on Recognising
Textual Entailment.
Oren Glickman, Ido Dagan and Moshe Koppel, 2006.
A Lexical Alignment Model for Probabilistic
Textual Entailment. In: Proceedings of the First
International Conference on Machine Learning
Challenges: Evaluating Predictive Uncertainty
Visual Object Classification, and Recognizing
Textual Entailment. Southampton, UK: Springer-
Verlag, pp. 287--298.
Lushan Han et al., 2013. UMBC_EBIQUITY-CORE:
Semantic Textual Similarity Systems. s.l., s.n.
Alexander B. Hirst and Graeme, 2001. Semantic
distance in WordNet: An experimental,
application-oriented evaluation of five measures.
Ilyes Jenhani, Nahla Ben Amor and Zi Elouedi, 2007.
Information Affinity: A New Similarity Measure
for Possibilistic Uncertain Information. En:
Symbolic and Quantitative Approaches to
Reasoning with Uncertainty. s.l.:Springer Berlin
Heidelberg, pp. 840-852.
Jay Jiang and David Conrath, 1997. Semantic
similarity based on corpus statistics and lexical
taxonomy. s.l., Proceedings of the International
Conference on Research in Computational
Linguistics.
David Jurgens, Mohammad Taher and Roberto
Navigli, 2014. SemEval-2014 Task 3: Cross-
720
Level Semantic Similarity. Dublin, Ireland, In
Proceedings of the 8th International Workshop on
Semantic Evaluation., pp. 23-24.
Harold W. Kuhn, 1955. The Hungarian Method for the
assignment problem. Naval Research Logistics
Quarterly.
Thomas K. Landauer, Peter W. Foltz and Darrell
Laham, 1998. Introduction to latent semantic
analysis. Discourse Processes, Issue 25, pp. 259-
284.
Claudia Leacock and Martin Chodorow, 1998.
Combining local context and WordNet sense
similarity for word sense identification. s.l.:s.n.
Lin Dekang, 1998. An information-theoretic definition
of similarity. s.l., Proceedings of the International
Conf. on Machine Learning.
Rada Mihalcea, Courtney Corley and Carlo
Strapparava, 2006. Corpus-based and
knowledge-based measures of text semantic
similarity. In: IN AAAIâ€™06. s.l.:21st National
Conference on Artificial Intelligence, pp. 775--
780.
LuÃ­s PadrÃ³ and Evgeny Stanilovsky, 2012. FreeLing
3.0: Towards Wider Multilinguality. Istanbul,
Turkey, Proceedings of the Language Resources
and Evaluation Conference (LREC 2012) ELRA.
Ted Pedersen, Siddharth Patwardhan and Jason
Michelizzi, 2004. WordNet::Similarity -
Measuring the Relatedness of Concepts.
American Association for Artificial Intelligence.
Philip Resnik, 1995. Using information content to
evaluate semantic similarity. s.l., Proceedings of
the 14th International Joint Conference on
Artificial Intelligence.
Zhibiao Wu and Martha Palmer, 1994. Verb semantics
and lexical selection.
</reference>
<page confidence="0.892829">
721
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048122">
<title confidence="0.625373666666667">DLSI UMCC _ _ SemSim: Multilingual System for Measuring Semantic Textual Similarity</title>
<author confidence="0.6266335">Alexander HÃ©ctor DÃ¡vila</author>
<affiliation confidence="0.960733">DI, University of Matanzas, Cuba.</affiliation>
<email confidence="0.996619">hector.davila}@umcc.cu</email>
<abstract confidence="0.97959875">In this paper we describe the specifications and results UMCC_DLSI system, which was involved in Semeval-2014 addressing two subtasks of Semantic Textual Similarity (STS, Task 10, for English and Spanish), and one subtask of Cross-Level Semantic Similarity (Task 3). As a supervised system, it was provided by different types of lexical and semantic features to train a classifier which was used to decide the correct answers for distinct subtasks. These features were obtained applying the Hungarian algorithm over a semantic network to create semantic alignments among words. Regarding the Spanish subtask of Task 10 two runs were submitted, where our Run2 was the best ranked with a general correlation of 0.807. However, for English subtask our best run of our 3 runs) reached place of 38 of the official ranking, obtaining a general correlation of 0.682. In terms of Task 3, only addressing Paragraph to Sentence subtask, our best run (Run1 of 2 runs) obtained a correlation value of 0.760 place of 34.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<date>2012</date>
<booktitle>SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity.. s.l., First Join Conference on Lexical and Computational Semantic (*SEM),</booktitle>
<pages>385--393</pages>
<location>MontrÃ©al,</location>
<contexts>
<context position="4735" citStr="Agirre, et al., 2012" startWordPosition="705" endWordPosition="708">processing Below are listed the pre-processing steps performed by our system. In bold we emphasize some cases which were used in different tasks. â€¢ All brackets were removed. â€¢ The abbreviations were expanded to their respective meanings. It was applied using a list of the most common abbreviations in English, with 819 and Spanish with 473. Phrases like â€œThe G8â€ and â€œThe Group of Eightâ€ are detected as identical. â€¢ Deletion of hyphen to identify words forms. For example, â€œwell-studiedâ€ was replaced by â€œwell studiedâ€. Example taken from line 13 of MSRpar corpus in test set of Semeval STS 2012 (Agirre, et al., 2012). â€¢ The sentences were tokenized and POStagged using Freeling 3.0 (PadrÃ³ and Stanilovsky, 2012). â€¢ All contractions were expanded. For example: n&apos;t, &apos;mand &apos;s. In the case of &apos;s was replaced with â€œisâ€ or â€œofâ€, â€œTom&apos;s badâ€ to â€œTom is badâ€ and â€œTom&apos;s childâ€ by &amp;quot;Child of Tom&amp;quot;. (Only for English tasks). â€¢ Punctuation marks were removed from the tokens except for the decimal point in numbers. â€¢ Stop words were removed. We used a list of the most common stop words. (28 for English and 48 for Spanish). â€¢ The words were mapped to the most common sense of WordNet 3.0. (Only for Spanish task). â€¢ A syntac</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer and Aitor Gonzalez-Agirre, 2012. SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity.. s.l., First Join Conference on Lexical and Computational Semantic (*SEM), MontrÃ©al, Canada. 2012., pp. 385-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<journal>Corpus. Microsoft Research,</journal>
<pages>14</pages>
<contexts>
<context position="11378" citStr="Brockett, 2007" startWordPosition="1728" endWordPosition="1729">d runs Where WeiRat(w1, w2) takes a value based on the type of relationship between w1 and w2. The possible values are defined in Table 1. Value Relation between w1 and w2 10 Antonym. 1 Synonym. 2 Direct Hypernym, Similar_To or Derivationally Related Form. 3 Two-links indirect Hypernym, Similar_To or Derivationally Related Form. 3 One word is often found in the gloss of the other. 9 Otherwise. Table 1: Values of Weight Ratio. 3.6 Sentence Alignment In the recognition of textsâ€™ similarities, several methods of lexical alignment have been used and can be appreciated by different point of views (Brockett, 2007) (Dagan, et al., 2005). Glickman (2006) used the measurement of the overleap grade between bags of words as a form of sentence alignment. Rada et al. (2006) made reference to an all-for-all alignment, leaving open the possibility when the same word of a sentence is aligned with several sentences. For this task, we used the Hungarian assignment algorithm as a way to align two sentences (Kuhn, 1955). Using that, the alignment cost between the sentences was reduced. To increase the semantic possibilities we used all word similarity metrics (including the two word similarity models) as a function </context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett, 2007. Aligning the RTE 2006 Corpus. Microsoft Research, p. 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgess</author>
<author>Kay Livesay</author>
<author>Kevin Lund</author>
</authors>
<date>1998</date>
<booktitle>Explorations in Context Space: Words, Sentences, Discourse. Discourse Processes, Issue 25,</booktitle>
<pages>211--257</pages>
<contexts>
<context position="7120" citStr="Burgess, et al., 1998" startWordPosition="1075" endWordPosition="1078">een words using information exclusively derived from large corpora (Mihalcea, et al., 2006). We considered one metric named Latent Semantic Analysis (LSA) (Landauer, et al., 1998). Latent Semantic Analysis: The Latent semantic analysis (LSA) is a corpus/document based measure proposed by Landauer in 1998. In LSA term co-occurrences in a corpus are captured by means of a dimensionality reduction operated by singular value decomposition (SVD) on the term-by-document matrix ğ‘‡ representing the corpus (Mihalcea, et al., 2006). There is a variation of LSA called HAL (Hyperspace Analog to Language) (Burgess, et al., 1998) that is based on the co-occurrence of words in a common context. The variation consists of counting the number of occurrences in that two words appear at n1 distance (called windows). For the co-occurrence matrix of words we took as core the UMBC WebBase corpus2 (Han, et al., 2013), which is derived from the Stanford WebBase project3. For the calculation of HAL measure we used the Cosine Similarity between the vectors for each pair of words. 3 Stanford WebBase 2001. http://bit.ly/WebBase. 717 3.3 Knowledge-based Measures There are many measures developed to quantify the degree of semantic rel</context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>Curt Burgess, Kay Livesay and Kevin Lund, 1998. Explorations in Context Space: Words, Sentences, Discourse. Discourse Processes, Issue 25, pp. 211 - 257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. En:</title>
<date>2005</date>
<booktitle>Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="11400" citStr="Dagan, et al., 2005" startWordPosition="1730" endWordPosition="1733">at(w1, w2) takes a value based on the type of relationship between w1 and w2. The possible values are defined in Table 1. Value Relation between w1 and w2 10 Antonym. 1 Synonym. 2 Direct Hypernym, Similar_To or Derivationally Related Form. 3 Two-links indirect Hypernym, Similar_To or Derivationally Related Form. 3 One word is often found in the gloss of the other. 9 Otherwise. Table 1: Values of Weight Ratio. 3.6 Sentence Alignment In the recognition of textsâ€™ similarities, several methods of lexical alignment have been used and can be appreciated by different point of views (Brockett, 2007) (Dagan, et al., 2005). Glickman (2006) used the measurement of the overleap grade between bags of words as a form of sentence alignment. Rada et al. (2006) made reference to an all-for-all alignment, leaving open the possibility when the same word of a sentence is aligned with several sentences. For this task, we used the Hungarian assignment algorithm as a way to align two sentences (Kuhn, 1955). Using that, the alignment cost between the sentences was reduced. To increase the semantic possibilities we used all word similarity metrics (including the two word similarity models) as a function cost. 3.7 N-Grams Alig</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman and Bernardo Magnini, 2005. The PASCAL Recognising Textual Entailment Challenge. En: Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>A Lexical Alignment Model for Probabilistic Textual Entailment. In:</title>
<date>2006</date>
<booktitle>Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment.</booktitle>
<pages>287--298</pages>
<publisher>SpringerVerlag,</publisher>
<location>Southampton, UK:</location>
<marker>Glickman, Dagan, Koppel, 2006</marker>
<rawString>Oren Glickman, Ido Dagan and Moshe Koppel, 2006. A Lexical Alignment Model for Probabilistic Textual Entailment. In: Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment. Southampton, UK: SpringerVerlag, pp. 287--298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
</authors>
<title>UMBC_EBIQUITY-CORE: Semantic Textual Similarity Systems.</title>
<date>2013</date>
<note>s.l., s.n.</note>
<marker>Han, 2013</marker>
<rawString>Lushan Han et al., 2013. UMBC_EBIQUITY-CORE: Semantic Textual Similarity Systems. s.l., s.n.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander B Hirst</author>
<author>Graeme</author>
</authors>
<title>Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures.</title>
<date>2001</date>
<marker>Hirst, Graeme, 2001</marker>
<rawString>Alexander B. Hirst and Graeme, 2001. Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilyes Jenhani</author>
<author>Nahla Ben Amor</author>
<author>Zi Elouedi</author>
</authors>
<title>Information Affinity: A New Similarity Measure for Possibilistic Uncertain Information. En: Symbolic and Quantitative Approaches to Reasoning with Uncertainty. s.l.:Springer</title>
<date>2007</date>
<pages>840--852</pages>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="2664" citStr="Jenhani, et al., 2007" startWordPosition="372" endWordPosition="375">ity has a long history in philosophy, psychology, artificial intelligence, and through the years has followed many different perspectives (Hirst, 2001). Recent research in the field of Computational Linguistics has emphasized the perspective of semantic relations between two lexemes in a lexical resource, or its inverse, semantic distance. The similarity of sentences is a confidence score that reflects the relationship between the meanings of two sentences. This similarity has been addressed in the literature with terminologies such as affinity, proximity, distance, difference and divergence (Jenhani, et al., 2007). The different applications of text similarity have been separated into a group of similarity tasks: between two long texts, for document classification; between a short text with a long text, for Web search; and between two short texts, for paraphrase recognition, automatic machine translation, etc. (Han, et al., 2013). At present, the calculation of the similarity between texts has been tackled from different points of views. Some have opted for a single measure to capture all the features of texts and other models have been trained with various measures to take text features separately. In</context>
</contexts>
<marker>Jenhani, Amor, Elouedi, 2007</marker>
<rawString>Ilyes Jenhani, Nahla Ben Amor and Zi Elouedi, 2007. Information Affinity: A New Similarity Measure for Possibilistic Uncertain Information. En: Symbolic and Quantitative Approaches to Reasoning with Uncertainty. s.l.:Springer Berlin Heidelberg, pp. 840-852.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Jiang</author>
<author>David Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy. s.l.,</title>
<date>1997</date>
<booktitle>Proceedings of the International Conference on Research in Computational Linguistics.</booktitle>
<contexts>
<context position="8966" citStr="Jiang and Conrath, 1997" startWordPosition="1372" endWordPosition="1375">â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)+ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) (2) Resnik: The Resnik similarity (Res) returns the information content (IC) of the LCS of two senses: ğ‘†ğ‘–ğ‘šğ‘…ğ‘’ğ‘  = ğ¼ğ¶(ğ¿ğ¶ğ‘†) (3) Where IC is defined as: ğ¼ğ¶(ğ‘) = âˆ’logğ‘ƒ(ğ‘) (4) And P(c) is the probability of encountering an instance of sense c in a large corpus (Resnik, 1995) (Mihalcea, et al., 2006). Lin: The Lin similarity builds on Resnikâ€™s measure and adds a normalization factor consisting of the information content of the two inputs senses (Lin, 1998): 2âˆ—ğ¼ğ¶(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ¿ğ‘–ğ‘› = (5) ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’)+ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) Jiang &amp; Conrath: The Jiang and Conrath similarity (JC) is defined as follows (Jiang and Conrath, 1997): ğ‘†ğ‘–ğ‘šğ‘ƒğ‘ğ‘¡â„ = âˆ’log ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)(7) Where ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1) is the number of edges in the shortest path between ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1and ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2. Word Similarity: In order to calculate the similarity between two words (WS) we used the following expression: ğ‘Šğ‘†(ğ‘¤1, ğ‘¤2) = ğ‘šğ‘ğ‘¥ğ‘ 1 âˆˆ ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’ğ‘ (ğ‘¤1) ğ‘ ğ‘–ğ‘š(ğ‘ 1, ğ‘ 2) ğ‘ 2 âˆˆ ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’ğ‘ (ğ‘¤2) Where ğ‘ ğ‘–ğ‘š(ğ‘ 1, ğ‘ 2) is one of the similarity metrics at sense level previously described. 3.4 Lexical Features We used a well-known lexical attributes similarity measures based on distances between strings. Dice-Similarity, Euclidean-Distance, JaccardSimilarity, Jaro-Winkler, Levenstein</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay Jiang and David Conrath, 1997. Semantic similarity based on corpus statistics and lexical taxonomy. s.l., Proceedings of the International Conference on Research in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher</author>
<author>Roberto Navigli</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 3: Cross720</booktitle>
<contexts>
<context position="12855" citStr="Jurgens, et al., 2014" startWordPosition="1959" endWordPosition="1962">a knowledge-based system using SemEval-2012, SemEval-2013 and SemEval2014 training corpus (Task 10 and Task 1 for the last one). For each training pair of phrases we obtained a vector with all measures explained above. Having it, we estimated the similarity value between two new phrases by applying the Euclidian distance between the new vector (made with the sentence pair we want to estimate the similarity value) and each vector in the training corpus. Then, the value of the instance with minor Our system participated in Sentence to Phrase subtask of Task 3: â€œCross-Level Semantic Similarityâ€ (Jurgens, et al., 2014) and in two subtasks of Task 10: â€œMultilingual Semantic Textual Similarityâ€ of SemEval-2014. It is important to remark that our system, using SVM approach, did not participate in Task 1: â€œEvaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailmentâ€, due to deadline issues. We compared our system results with the final ranking of Task 1 and we could have reached the 6th place of the ranking for Relatedness Subtask with a 0.781 of correlation coefficient, and the 9th place for Entailment Subtask with an accuracy of 77.41%. Task</context>
</contexts>
<marker>Jurgens, Taher, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher and Roberto Navigli, 2014. SemEval-2014 Task 3: Cross720</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ireland Dublin</author>
</authors>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation.,</booktitle>
<pages>23--24</pages>
<marker>Dublin, </marker>
<rawString>Level Semantic Similarity. Dublin, Ireland, In Proceedings of the 8th International Workshop on Semantic Evaluation., pp. 23-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian Method for the assignment problem. Naval Research Logistics Quarterly.</title>
<date>1955</date>
<contexts>
<context position="11778" citStr="Kuhn, 1955" startWordPosition="1795" endWordPosition="1796">: Values of Weight Ratio. 3.6 Sentence Alignment In the recognition of textsâ€™ similarities, several methods of lexical alignment have been used and can be appreciated by different point of views (Brockett, 2007) (Dagan, et al., 2005). Glickman (2006) used the measurement of the overleap grade between bags of words as a form of sentence alignment. Rada et al. (2006) made reference to an all-for-all alignment, leaving open the possibility when the same word of a sentence is aligned with several sentences. For this task, we used the Hungarian assignment algorithm as a way to align two sentences (Kuhn, 1955). Using that, the alignment cost between the sentences was reduced. To increase the semantic possibilities we used all word similarity metrics (including the two word similarity models) as a function cost. 3.7 N-Grams Alignment Using the Max Word Similarity model, we calculated three features based on 2-gram, 3-gram and 4-gram alignment with the Hungarian algorithm. 4 Knowledge-based System For similarity calculation between two phrases, we developed a knowledge-based system using SemEval-2012, SemEval-2013 and SemEval2014 training corpus (Task 10 and Task 1 for the last one). For each trainin</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn, 1955. The Hungarian Method for the assignment problem. Naval Research Logistics Quarterly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes, Issue 25,</booktitle>
<pages>259--284</pages>
<contexts>
<context position="6677" citStr="Landauer, et al., 1998" startWordPosition="1008" endWordPosition="1011">tic Similarity of Words A relatively large number of word to word similarity metrics have previously been proposed in the literature, ranging from distance-oriented measures computed on semantic networks, to metrics based on models of distributional similarity learned from large text collections (Mihalcea, et al., 2006). 3.2 Corpus-based Measures Corpus-based measures of word semantic similarity try to identify the degree of similarity between words using information exclusively derived from large corpora (Mihalcea, et al., 2006). We considered one metric named Latent Semantic Analysis (LSA) (Landauer, et al., 1998). Latent Semantic Analysis: The Latent semantic analysis (LSA) is a corpus/document based measure proposed by Landauer in 1998. In LSA term co-occurrences in a corpus are captured by means of a dimensionality reduction operated by singular value decomposition (SVD) on the term-by-document matrix ğ‘‡ representing the corpus (Mihalcea, et al., 2006). There is a variation of LSA called HAL (Hyperspace Analog to Language) (Burgess, et al., 1998) that is based on the co-occurrence of words in a common context. The variation consists of counting the number of occurrences in that two words appear at n1</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz and Darrell Laham, 1998. Introduction to latent semantic analysis. Discourse Processes, Issue 25, pp. 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet sense similarity for word sense identification.</title>
<date>1998</date>
<tech>s.l.:s.n.</tech>
<contexts>
<context position="8080" citStr="Leacock and Chodorow, 1998" startWordPosition="1228" endWordPosition="1231">ct3. For the calculation of HAL measure we used the Cosine Similarity between the vectors for each pair of words. 3 Stanford WebBase 2001. http://bit.ly/WebBase. 717 3.3 Knowledge-based Measures There are many measures developed to quantify the degree of semantic relation between two words senses using semantic network information. For example: Leacock &amp; Chodorow Similarity: The Leacock &amp; Chodorow (LC) similarity is determined as follows: ğ‘†ğ‘–ğ‘šğ‘™ğ‘ = âˆ’log (ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„2âˆ—ğ· ) (1) Where length is the length of the shortest path between senses using node-counting and D is the maximum depth of the taxonomy (Leacock and Chodorow, 1998) Wu and Palmer: The Wu and Palmer similarity metric (Wup) measures the depth of two given senses in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combine them into a similarity score (Wu and Palmer, 1994): 2âˆ—ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ‘Šğ‘¢ğ‘ = ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)+ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) (2) Resnik: The Resnik similarity (Res) returns the information content (IC) of the LCS of two senses: ğ‘†ğ‘–ğ‘šğ‘…ğ‘’ğ‘  = ğ¼ğ¶(ğ¿ğ¶ğ‘†) (3) Where IC is defined as: ğ¼ğ¶(ğ‘) = âˆ’logğ‘ƒ(ğ‘) (4) And P(c) is the probability of encountering an instance of sense c in a large corpus (Resnik, 1995) (Mihalcea, et al., 2006). Lin: The Lin simila</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow, 1998. Combining local context and WordNet sense similarity for word sense identification. s.l.:s.n.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Dekang</author>
</authors>
<title>An information-theoretic definition of similarity. s.l.,</title>
<date>1998</date>
<booktitle>Proceedings of the International Conf. on Machine Learning.</booktitle>
<marker>Dekang, 1998</marker>
<rawString>Lin Dekang, 1998. An information-theoretic definition of similarity. s.l., Proceedings of the International Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity. In:</title>
<date>2006</date>
<booktitle>IN AAAIâ€™06. s.l.:21st National Conference on Artificial Intelligence,</booktitle>
<pages>775--780</pages>
<contexts>
<context position="5974" citStr="Mihalcea, et al., 2006" startWordPosition="909" endWordPosition="912">lt for every sentence using Freeling 3.0. 1 The windows is the number of intermediate words between two words. 2 Dataset of high quality English paragraphs containing over three billion words and it is available in http://ebiquity.umbc.edu/resource/html/id/351 3 Features Extraction Measures of semantic similarity have been traditionally used between words or concepts, and much less between text segments, (i.e. two or more words). The emphasis on word to word similarity is probably due to the availability of resources that specifically encode relations between words or concepts (e.g. WordNet) (Mihalcea, et al., 2006). Following we describe the similarity measures used in this approach. 3.1 Semantic Similarity of Words A relatively large number of word to word similarity metrics have previously been proposed in the literature, ranging from distance-oriented measures computed on semantic networks, to metrics based on models of distributional similarity learned from large text collections (Mihalcea, et al., 2006). 3.2 Corpus-based Measures Corpus-based measures of word semantic similarity try to identify the degree of similarity between words using information exclusively derived from large corpora (Mihalcea</context>
<context position="8659" citStr="Mihalcea, et al., 2006" startWordPosition="1325" endWordPosition="1328">of the taxonomy (Leacock and Chodorow, 1998) Wu and Palmer: The Wu and Palmer similarity metric (Wup) measures the depth of two given senses in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combine them into a similarity score (Wu and Palmer, 1994): 2âˆ—ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ‘Šğ‘¢ğ‘ = ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)+ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) (2) Resnik: The Resnik similarity (Res) returns the information content (IC) of the LCS of two senses: ğ‘†ğ‘–ğ‘šğ‘…ğ‘’ğ‘  = ğ¼ğ¶(ğ¿ğ¶ğ‘†) (3) Where IC is defined as: ğ¼ğ¶(ğ‘) = âˆ’logğ‘ƒ(ğ‘) (4) And P(c) is the probability of encountering an instance of sense c in a large corpus (Resnik, 1995) (Mihalcea, et al., 2006). Lin: The Lin similarity builds on Resnikâ€™s measure and adds a normalization factor consisting of the information content of the two inputs senses (Lin, 1998): 2âˆ—ğ¼ğ¶(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ¿ğ‘–ğ‘› = (5) ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’)+ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) Jiang &amp; Conrath: The Jiang and Conrath similarity (JC) is defined as follows (Jiang and Conrath, 1997): ğ‘†ğ‘–ğ‘šğ‘ƒğ‘ğ‘¡â„ = âˆ’log ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)(7) Where ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1) is the number of edges in the shortest path between ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1and ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2. Word Similarity: In order to calculate the similarity between two words (WS) we used the following expression: ğ‘Šğ‘†(ğ‘¤1, ğ‘¤2) = ğ‘šğ‘ğ‘¥ğ‘ 1 âˆˆ ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’ğ‘ (ğ‘¤1) ğ‘ ğ‘–ğ‘š(ğ‘ 1</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley and Carlo Strapparava, 2006. Corpus-based and knowledge-based measures of text semantic similarity. In: IN AAAIâ€™06. s.l.:21st National Conference on Artificial Intelligence, pp. 775--780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LuÃ­s PadrÃ³</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>FreeLing 3.0: Towards Wider Multilinguality. Istanbul, Turkey,</title>
<date>2012</date>
<booktitle>Proceedings of the Language Resources and Evaluation Conference (LREC 2012) ELRA.</booktitle>
<contexts>
<context position="4830" citStr="PadrÃ³ and Stanilovsky, 2012" startWordPosition="720" endWordPosition="723"> emphasize some cases which were used in different tasks. â€¢ All brackets were removed. â€¢ The abbreviations were expanded to their respective meanings. It was applied using a list of the most common abbreviations in English, with 819 and Spanish with 473. Phrases like â€œThe G8â€ and â€œThe Group of Eightâ€ are detected as identical. â€¢ Deletion of hyphen to identify words forms. For example, â€œwell-studiedâ€ was replaced by â€œwell studiedâ€. Example taken from line 13 of MSRpar corpus in test set of Semeval STS 2012 (Agirre, et al., 2012). â€¢ The sentences were tokenized and POStagged using Freeling 3.0 (PadrÃ³ and Stanilovsky, 2012). â€¢ All contractions were expanded. For example: n&apos;t, &apos;mand &apos;s. In the case of &apos;s was replaced with â€œisâ€ or â€œofâ€, â€œTom&apos;s badâ€ to â€œTom is badâ€ and â€œTom&apos;s childâ€ by &amp;quot;Child of Tom&amp;quot;. (Only for English tasks). â€¢ Punctuation marks were removed from the tokens except for the decimal point in numbers. â€¢ Stop words were removed. We used a list of the most common stop words. (28 for English and 48 for Spanish). â€¢ The words were mapped to the most common sense of WordNet 3.0. (Only for Spanish task). â€¢ A syntactic tree was built for every sentence using Freeling 3.0. 1 The windows is the number of interm</context>
</contexts>
<marker>PadrÃ³, Stanilovsky, 2012</marker>
<rawString>LuÃ­s PadrÃ³ and Evgeny Stanilovsky, 2012. FreeLing 3.0: Towards Wider Multilinguality. Istanbul, Turkey, Proceedings of the Language Resources and Evaluation Conference (LREC 2012) ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity -Measuring the Relatedness of Concepts. American Association for Artificial Intelligence.</title>
<date>2004</date>
<contexts>
<context position="10582" citStr="Pedersen, et al., 2004" startWordPosition="1600" endWordPosition="1603"> metrics. These were defined as: Max Word Similarity: The Max Word Similarity (MaxSim) is defined as follows: ğ‘€ğ‘ğ‘¥ğ‘†ğ‘–ğ‘š(ğ‘¤1, ğ‘¤2) = {1 ğ‘–ğ‘“ğ‘„ğºğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¤1,ğ‘¤2) = 1 ğ‘€ğ‘ğ‘¥(ğ‘†ğ‘–ğ‘šğ»ğ‘ğ‘™ (ğ‘¤1, ğ‘¤2), ğ‘†ğ‘–ğ‘šğ‘Šğ‘¢ğ‘ (ğ‘¤1, ğ‘¤2)) Where ğ‘„ğºğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¤1,ğ‘¤2)is the QGramDistance between w1 and w2. Statistics and Weight Ratio: For calculating the weight ratio in this measure of similarity was used WordNet 3.0 and it was defined in (10): 1 ğ‘†ğ‘–ğ‘šğ‘—ğ‘ =ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’ 1)+ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2)âˆ’2âˆ—ğ¼ğ¶(ğ¿ğ¶ğ‘†) (6) PathLen: The PathLen similarity (Len) ğ‘†ğ‘¡ğ‘ğ‘Šğ‘’ğ‘–ğ‘…ğ‘ğ‘¡ (ğ‘¤1,ğ‘¤2) = (ğ‘†ğ‘–ğ‘šğ»ğ‘ğ‘™(ğ‘¤1,ğ‘¤2) + ( 1 )) involves the path lengths between two senses in ğ‘Šğ‘’ğ‘–ğ‘…ğ‘ğ‘¡(ğ‘¤1,ğ‘¤2) the taxonomy (Pedersen, et al., 2004). 2 (10) 4 Copyright (c) 2006 by Chris Parkinson, available in http://sourceforge.net/projects/simmetrics/ 718 Euclidian Distance was assigned to the new pair of phrases. 5 Tasks and runs Where WeiRat(w1, w2) takes a value based on the type of relationship between w1 and w2. The possible values are defined in Table 1. Value Relation between w1 and w2 10 Antonym. 1 Synonym. 2 Direct Hypernym, Similar_To or Derivationally Related Form. 3 Two-links indirect Hypernym, Similar_To or Derivationally Related Form. 3 One word is often found in the gloss of the other. 9 Otherwise. Table 1: Values of Wei</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan and Jason Michelizzi, 2004. WordNet::Similarity -Measuring the Relatedness of Concepts. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity. s.l.,</title>
<date>1995</date>
<booktitle>Proceedings of the 14th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8634" citStr="Resnik, 1995" startWordPosition="1323" endWordPosition="1324"> maximum depth of the taxonomy (Leacock and Chodorow, 1998) Wu and Palmer: The Wu and Palmer similarity metric (Wup) measures the depth of two given senses in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combine them into a similarity score (Wu and Palmer, 1994): 2âˆ—ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ‘Šğ‘¢ğ‘ = ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)+ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) (2) Resnik: The Resnik similarity (Res) returns the information content (IC) of the LCS of two senses: ğ‘†ğ‘–ğ‘šğ‘…ğ‘’ğ‘  = ğ¼ğ¶(ğ¿ğ¶ğ‘†) (3) Where IC is defined as: ğ¼ğ¶(ğ‘) = âˆ’logğ‘ƒ(ğ‘) (4) And P(c) is the probability of encountering an instance of sense c in a large corpus (Resnik, 1995) (Mihalcea, et al., 2006). Lin: The Lin similarity builds on Resnikâ€™s measure and adds a normalization factor consisting of the information content of the two inputs senses (Lin, 1998): 2âˆ—ğ¼ğ¶(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ¿ğ‘–ğ‘› = (5) ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’)+ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) Jiang &amp; Conrath: The Jiang and Conrath similarity (JC) is defined as follows (Jiang and Conrath, 1997): ğ‘†ğ‘–ğ‘šğ‘ƒğ‘ğ‘¡â„ = âˆ’log ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)(7) Where ğ‘ğ‘ğ‘¡â„ğ‘™ğ‘’ğ‘›(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1, ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1) is the number of edges in the shortest path between ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1and ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2. Word Similarity: In order to calculate the similarity between two words (WS) we used the following expression: ğ‘Šğ‘†(ğ‘¤1, ğ‘¤2) = </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik, 1995. Using information content to evaluate semantic similarity. s.l., Proceedings of the 14th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<contexts>
<context position="8314" citStr="Wu and Palmer, 1994" startWordPosition="1269" endWordPosition="1272">the degree of semantic relation between two words senses using semantic network information. For example: Leacock &amp; Chodorow Similarity: The Leacock &amp; Chodorow (LC) similarity is determined as follows: ğ‘†ğ‘–ğ‘šğ‘™ğ‘ = âˆ’log (ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„2âˆ—ğ· ) (1) Where length is the length of the shortest path between senses using node-counting and D is the maximum depth of the taxonomy (Leacock and Chodorow, 1998) Wu and Palmer: The Wu and Palmer similarity metric (Wup) measures the depth of two given senses in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combine them into a similarity score (Wu and Palmer, 1994): 2âˆ—ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ‘Šğ‘¢ğ‘ = ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’1)+ğ‘‘ğ‘’ğ‘ğ‘¡â„(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) (2) Resnik: The Resnik similarity (Res) returns the information content (IC) of the LCS of two senses: ğ‘†ğ‘–ğ‘šğ‘…ğ‘’ğ‘  = ğ¼ğ¶(ğ¿ğ¶ğ‘†) (3) Where IC is defined as: ğ¼ğ¶(ğ‘) = âˆ’logğ‘ƒ(ğ‘) (4) And P(c) is the probability of encountering an instance of sense c in a large corpus (Resnik, 1995) (Mihalcea, et al., 2006). Lin: The Lin similarity builds on Resnikâ€™s measure and adds a normalization factor consisting of the information content of the two inputs senses (Lin, 1998): 2âˆ—ğ¼ğ¶(ğ¿ğ¶ğ‘†) ğ‘†ğ‘–ğ‘šğ¿ğ‘–ğ‘› = (5) ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’)+ğ¼ğ¶(ğ‘ ğ‘’ğ‘›ğ‘ ğ‘’2) Jiang &amp; Conrath: The Jiang and Conrath similarity </context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer, 1994. Verb semantics and lexical selection.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>