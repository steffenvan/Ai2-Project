<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000531">
<title confidence="0.980755">
Coping with problems in grammars automatically extracted from treebanks
</title>
<author confidence="0.979596">
Carlos A. Prolo
</author>
<affiliation confidence="0.9949185">
Computer and Information Science Department
University of Pennsylvania
</affiliation>
<address confidence="0.988774">
Suite 400A, 3401 Walnut Street
Philadelphia, PA, USA, 19104-6228
</address>
<email confidence="0.998947">
prolo@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979111111111">
We report in this paper on an experiment on auto-
matic extraction of a Tree Adjoining Grammar from
the WSJ corpus of the Penn Treebank. We use an
automatic tool developed by (Xia, 2001) properly
adapted to our particular need. Rather than address-
ing general aspects of the automatic extraction we
focus on the problems we have found to extract a
linguistically (and computationally) sound grammar
and approaches to handle them.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999504511627907">
Much linguistic research is oriented to finding
general principles for natural language, classify-
ing linguistic phenomena, building regular mod-
els (e.g., grammars) for the well-behaved (or well-
understood) part of languages and studying remain-
ing “interesting” problems in a compartmentalized
way. With the availability of large natural language
corpora annotated for syntactic structure, the tree-
banks, e.g., (Marcus et al., 1993), automatic gram-
mar extraction became possible (Chen and Vijay-
Shanker, 2000; Xia, 1999). Suddenly, grammars
started being extracted with an attempt to have
“full” coverage of the constructions in a certain lan-
guage (of course, to the extent that the used corpora
represents the language) and that immediately poses
a question: If we do not know how to model many
phenomena grammatically how can that be that we
are extracting such a wide-coverage grammar?.
To answer that question we have to start a new
thread at the edge of linguistics and computational
linguistics. More than numbers to express coverage,
we have to start analyzing the quality of automat-
ically generated grammars, identifying extraction
problems and uncovering whatever solutions are be-
ing given for them, however interesting or ugly they
might be, challenging the current paradigms of lin-
guistic research to provide answers for the problems
on a “by-need” basis.
In this paper we report on a particular experi-
ence of automatic extraction of an English grammar
from the WSJ corpus of the Penn Treebank (PTB)
(Marcus et al., 1994)1 using Tree Adjoining Gram-
mar (TAGs, (Joshi and Schabes, 1997)). We use an
automatic tool developed by (Xia, 2001) properly
adapted to our particular needs and focus on some
problems we have found to extract a linguistically
(and computationally) sound grammar and the so-
lutions we gave to them. The list of problems is
a sample, far from being exhaustive2 Likewise, the
solutions will not always be satisfactory.
In Section 2 we introduce the method of grammar
extraction employed. The problems are discussed in
Section 3. We conclude in Section 4.
</bodyText>
<sectionHeader confidence="0.920524" genericHeader="method">
2 The extracted grammar
</sectionHeader>
<subsectionHeader confidence="0.726264">
2.1 TAGs
</subsectionHeader>
<bodyText confidence="0.999982">
A TAG is a set of lexicalized elementary trees that
can be combined, through the operations of tree ad-
junction and tree substitution, to derive syntac-
tic structures for sentences. We follow a common
approach to grammar development for natural lan-
guage using TAGs, under which, driven by local-
ity principles, each elementary tree for a given lex-
ical head is expected to contain its projection, and
slots for its arguments (e.g., (Frank, 2002)). Figure
1 shows typical grammar template trees that can be
selected by lexical items and combined to generate
the structure in Figure 2. The derivation tree, to the
right, contains the history of the tree grafting pro-
cess that generated the derived tree, to the left.3
</bodyText>
<footnote confidence="0.9965355">
1We assume some familiarity with the basic notations in the
PTB as in (Marcus et al., 1994).
2(Prolo, 2002) includes a more comprehensive and detailed
discussion of grammar extraction alternatives and problems.
3For a more comprehensive introduction to TAGs and Lexi-
calized TAGs we refer the reader to (Joshi and Schabes, 1997).
</footnote>
<bodyText confidence="0.999679166666667">
foot, with the sibling subtree (after being recursively
processed) being carried together into the auxiliary
tree. Notice that the auxiliary trees are therefore ei-
ther strictly right or left branching, the foot always
immediately under the root node. Other kinds of
auxiliary trees are therefore not allowed.
</bodyText>
<figure confidence="0.993600416666667">
VP
PP
VP* DT NP*
NP
N
NP
S
VP
V NP
P NP
np vt ppright det
NP
</figure>
<figureCaption confidence="0.919401">
Figure 1: An example of Tree Adjoining Grammar
</figureCaption>
<figure confidence="0.999692745454545">
[John]
NP
N
[saw]
V NP
VP*
S
Derived tree Derivation tree
[Mary]
N
VP
[from]
P
PP
NP
DT
[the]
[window]
NP*
N
vt[saw]
np[John] np[Mary] pp[from]
np[window]
det[the]
VBG
NP
S
PP−LOC
at
NNP
policies
using
NNS
S
PRP
VP
VP
VP
S−MNR
NP−SBJ
ADVP
FNX
they
RB
still
NP−SBJ
NP
draft
NNS
ε
IN
NP
VBP
VP
pens
</figure>
<figureCaption confidence="0.999827">
Figure 2: Derivation of John saw Mary from the window
</figureCaption>
<subsectionHeader confidence="0.995284">
2.2 LexTract
</subsectionHeader>
<bodyText confidence="0.999359291666667">
Given an annotated sentence from the PTB as in-
put Xia’s LexTract tool (Xia, 1999; Xia, 2001) first
executes a rebracketing. More precisely, additional
nodes are inserted to separate arguments and mod-
ifiers and to structure the modifying process as bi-
nary branching. A typical rebracketed PTB tree is
shown in Figure 3,4 in which we have distinguished
the tree nodes inserted by LexTract.
The second stage is the extraction of the grammar
trees proper shown in Figure 4. In particular, re-
cursive modifier structures have to be detected and
factored out of the derived tree to compose the aux-
iliary trees, the rest becoming an initial tree. The
process is recursive also in the sense that factored
subtree structures still undergo the spinning off pro-
cess until we have all modifiers with their own trees,
all the arguments of a head as substitution nodes of
the tree containing their head, and the material un-
der the argument nodes defining additional initial
trees for themselves. Auxiliary trees are extracted
from parent-child pairs with matching labels if the
child is elected the parent’s head and the child’s sib-
ling is marked as modifier: the parent is mapped
into a root of an auxiliary tree, the head-child into its
</bodyText>
<footnote confidence="0.848009">
4Figures 3 and 4 are thanks to Fei Xia. We are also grateful
to her for allowing us to use LexTract and make changes to its
source code to customize to our needs.
</footnote>
<figureCaption confidence="0.999049">
Figure 3: LexTract rebracketing stage
</figureCaption>
<bodyText confidence="0.99999075">
To extract a grammar with Xia’s tool one has to
define tables for finding: the head child of a con-
stituent expansion; which of the siblings of a head
are acceptable arguments; and which constituent la-
bels are plausible modifiers of another. Special pro-
visions are made for handling coordination. For ad-
ditional information see (Xia, 2001). In this paper
we refer to (Xia, 1999)’s table settings and extracted
grammar, which we used as our starting point, as
Xia’s sample. We used a customized version of Lex-
Tract, plus additional pre-processing of the PTB in-
put and post-processing of the extracted trees.
</bodyText>
<sectionHeader confidence="0.990225" genericHeader="method">
3 Extraction Problems
</sectionHeader>
<bodyText confidence="0.999912583333333">
Extraction problems arise from several sources, in-
cluding: (1) lack of proper linguistic account,5 (2)
the (Penn Treebank) annotation style, (3) the (Lex-
Tract) extraction tool, (4) possible unsuitability of
the (TAG) model, and (5) annotation errors. We
refrained from making a rigid classification of the
problems we present according to these sources. In
particular it is often difficult to decide whether to
blame sources (1), (3), or (5) for a certain problem.
We will not discuss in this paper problems due to
annotation errors. As for the PTB style problems
we only discuss one, the first listed below.
</bodyText>
<footnote confidence="0.5545075">
5Here included the (occasional) inability on the part of
grammar developers to find or make use of an existing account.
</footnote>
<figure confidence="0.998688738636364">
#5
S1.t
IN
PP
NP1.t
S1.b
NP2.t
S2.t
S2.b
VP1.t
#5
NP1.b NP2.b
VP1.b
#1
at
#2
NNP
FNX
#3
PRP
they
#4
ADVP
still
RB
draft
VBP
VP3.t
VP3.b
NP3.t
NP3.b
NNS
VP2.t
VP2.b
NP4
ε
VBG
S3
VP
5
NP5.t
NP5.b
using NNS
#5
policies
#7
pens
#6
#8
at
#5: #6:
#7: #8:
NP
VP
S
NP VP
VBP
draft
NP
policies
NNS VP* S
NP
VP
NP
ε VBG
using
#2:
#1:
NP
PP
S
S*
NNP
IN NP
FNX
#3:
NP
#4:
VP
PRP
they
ADVP VP*
RB
still
NP
NNS
pens
a) Input tree decomposition b) Extracted elementary trees
</figure>
<figureCaption confidence="0.992672">
Figure 4: LexTract extraction stage
</figureCaption>
<figure confidence="0.44547175">
(S-3 (NP-SBJ (PRP We))
(VP (VBP make)
(SBAR-NOM (WHNP-1 (WP what))
(S we know
how to make))))
a) As a sentential clause in the PTB
(S-3 (NP-SBJ (PRP We))
(VP (VBP make)
(NP (NP (WP what))
(SBAR (WHNP-1 (-NONE- 0))
(S we know ...)))))
b) As a Noun phrase after pre-processed
</figure>
<figureCaption confidence="0.927957">
Figure 5: Free relatives in the Treebank
</figureCaption>
<subsectionHeader confidence="0.997757">
3.1 Free Relatives
</subsectionHeader>
<bodyText confidence="0.99993425">
Free relatives are annotated in the Penn Treebank
as sentential complements as in Figure 5.a. The
extracted tree corresponding to the occurrence of
“make” would be of a verb that takes a sentential
complement (SBAR). This does not seem to be cor-
rect6, as the proper subcategorization of the verb oc-
currence is transitive.
In fact, free relatives may occur wherever an NP
argument may occur. So, the only reasonable ex-
traction account consistent with maintaining them
as SBARs would be one in which every NP sub-
stitution node in an extracted tree would admit the
</bodyText>
<footnote confidence="0.96315925">
6In both standard accounts for free relatives, the Head Ac-
count (e.g., (Bresnan and Grimshaw, 1978)) and the Comp Ac-
count (e.g., (Groos and von Riemsdijk, 1979)), commonly dis-
cussed in the literature, the presence of the NP (or DP) is clear.
</footnote>
<bodyText confidence="0.999812111111111">
existence of a counterpart tree, identical to the first,
except that the NP argument label is replaced with
an SBAR. Instead we opted to reflect the NP char-
acter of the free relatives by pre-processing the cor-
pus (using the Head-analysis, for practical conve-
nience). The annotated example is then automat-
ically replaced with the one in Figure 5.b. Other
cases of free-relatives (non-NP) are rare and not
likely to interfere with verb subcategorization.
</bodyText>
<subsectionHeader confidence="0.997907">
3.2 Wh percolation up
</subsectionHeader>
<bodyText confidence="0.999988666666667">
In the Penn Treebank the same constituent is anno-
tated with different syntactic categories depending
on whether it possesses or not the wh feature. For
instance, a regular noun phrase has the syntactic
category NP, whereas when the constituent is wh-
marked, and is in the landing site of wh-movement,
it carries the label WHNP.7 While that might look
appealing since the two constituents seem to have
distinct distributional properties, it poses a design
problem. While regular constituents inherit their
syntactic categorial feature (i.e. their label) from
their heads, wh projections are often formed by in-
heritance from their modifiers. For instance: “the
father” is an NP, but modified by a wh expression
(“the father of whom”, “whose father”, “which fa-
ther”), it becomes a WHNP. The only solution we
see is to allow for nouns and NPs to freely project up
to WHNPs during extraction.8 On the other hand, in
</bodyText>
<footnote confidence="0.98491475">
7When the constituent is not wh-moved, it is correctly pre-
served as an NP, as “what” in “Who ate what?”.
8Of course another simple solution would be merging the
wh constituents with their non-wh counterparts.
</footnote>
<figure confidence="0.5356615">
(NP (UCP (NN construction)
(CC and)
(JJ commercial))
(NNS loans))
a) NP modifiers
(VP (VB be)
(UCP-PRD (NP (CD 35))
(CC or)
(ADJP (JJR older))))
b) non-verbal Predicates
(VP (VB take)
(NP (NN effect))
(UCP-TMP (ADVP 96 days later)
(, ,)
(CC or)
(PP in early February)))
</figure>
<figureCaption confidence="0.768961">
c) adverbial modifiers
Figure 6: “Unlike Coordinated Phrases”
</figureCaption>
<bodyText confidence="0.999638333333333">
cases when the wh constituent is in a non-wh posi-
tion, we need the opposite effect: a WHNP (or wh-
noun POS tag) is allowed to project up to an NP.
</bodyText>
<subsectionHeader confidence="0.999569">
3.3 Unlike Coordinated Phrases (UCP)
</subsectionHeader>
<bodyText confidence="0.999615653846154">
This is the expression used in the PTB to denote
coordinated phrases in which the coordinated con-
stituents are not of the same syntactic category. The
rationale for the existence of such constructions is
that the coordinated constituents are alternative re-
alizations of the same grammatical function with
respect to a lexical head. In Figure 6.a, both a
noun and an adjective are allowed to modify another
noun, and therefore they can be conjoined while re-
alizing that function. Two other common cases are:
coordination of predicates in copular constructions
(Figure 6.b) and adverbial modification (Figure 6.c).
We deal with the problem as follows. First, we al-
low for a UCP to be extracted as an argument when
the head is a verb and the UCP is marked predica-
tive (PRD function tag) in the training example; or
whenever the head is seen to have an obligatory
argument requirement (e.g., prepositions: “They
come from ((NP the house) and (PP behind the
tree))”). Second, a UCP is allowed to modify (ad-
join to) most of the nodes, according to evidence in
the corpus and common sense (in the first and third
examples above we had NP and VP modification).
With respect to the host tree, when attached as an ar-
gument they are treated like any other non-terminal:
a substitution node. The left tree in Figure 7 shows
</bodyText>
<figureCaption confidence="0.997642">
Figure 7: Extracted trees for UCP
</figureCaption>
<bodyText confidence="0.999892888888889">
the case where the UCP is treated as a modifier. In
fact the trees are both for the example in Figure 6.a.
Notice that the tree is non-lexicalized to avoid ef-
fects of sparseness. The UCP is then expanded as in
the right tree in Figure 7: an initial tree anchored by
the conjunction (the tree attaches either to a tree like
the one in the left or as a true argument – the latter
would be the case for the example in Figure 6.b).
Now, the caveats. First, we are giving the UCP
the status of an independent non-terminal, as if it
had some intrinsic categorial significance (as a syn-
tactic projection). The assumption of independence
of expansion, that for context-free grammars is in-
herent to each non-terminal, in TAGs is further re-
stricted to the substitution nodes. For example,
when an NP appears as substitution node, in a sub-
ject or object position, or as an argument of a prepo-
sition or a genitive marker, we are stating that any
possible expansion for the NP is licensed there. The
same happens for other labels in argument positions
as well. While that is an overgenerating assumption
(e.g. the expletive “there” cannot be the realization
of an NP in object position), it is generally true. For
the UCP, however, we know that its expansion is
in fact strongly dependent on where the substitu-
tion node is, as we have argued before. In fact it
is lexically dependent (cf. “I know ((the problem)
and (that there is no solution to it))”, where the con-
juncts are licensed by the subcategorizations of the
verb “know”). On the other hand, it does not seem
reasonable to expand the UCP node at the hosting
tree – a cross product explosion. A possible way of
alleviating this effect could be to expand only the
auxiliary trees (a UCP modifying a VP is distinct
from a UCP modifying an NP, and moreover they
are independent of lexical items). But for true argu-
ment positions there seems to be no clear solution.
Second, the oddity of the UCP as a label becomes
apparent once again when there are multiple con-
juncts, as in Figure 8: it is enough for one of them to
be distinct to turn the entire constituent into a UCP.
Recursive decomposition in the grammar in these
situations clearly leads to some non-standard trees.
Finally, and more crucially, we have omitted one
case in our discussion: the case in which the UCP
</bodyText>
<equation confidence="0.589217882352941">
(NP (UCP (JJ electronic)
(, ,)
(NN computer)
(CC and)
(NN building))
(NNS products))
Figure 8: UCP with multiple conjuncts
(S (NP-SBJ-1 The Series 1989 B bonds)
(VP (VBP are)
(VP (VBN rated)
(S *-1 double-A))))
(S (NP-SBJ-1 The Series 1989 B bonds)
(VP (VBP are)
(UCP-PRD (ADJP-PRD (JJ uninsured))
(CC and)
(VP (VBN rated)
(S *-1 double-A)))))
</equation>
<figureCaption confidence="0.994681">
Figure 9: UCP involving VP argument of the copula
</figureCaption>
<bodyText confidence="0.99997548">
is the natural head-child of some node. Under some
accounts of grammar development this never hap-
pens: we have observed that UCP does not appear
as head child in the account where the head is the
syntactic head of a node. We have not always fol-
lowed this rule. With respect to the VP head, so far
we have followed one major tendency in the com-
putational implementation of lexicalized grammars,
according to which lexical verbs are prefered to aux-
iliary verbs to head the VP. Now, consider the pair
of sentences in Figure 9.
Under the lexical verb paradigm, in the first sen-
tence the derivation would start with an initial tree
anchored by the past participle verb (“rated”). But
then we have an interesting problem in the second
sentence, for which we do not currently have a neat
solution. Following Xia’s sample settings of Lex-
Tract parameters, in these cases the extraction is
rescued by switching to the other paradigm: the ini-
tial tree is extracted anchored by the auxiliary verb
with a UCP argument, and the VP is accepted as a
possible conjunct. A systematic move to the syn-
tactic head paradigm, which we may indeed try,
would have important consequences in the locality
assumptions for the grammar development.
</bodyText>
<subsectionHeader confidence="0.958701">
3.4 VP topicalization
</subsectionHeader>
<bodyText confidence="0.99710325">
Another problem with the lexical verb paradigm
(see also discussion under UCP above) is the VP
topicalization as in the sentence in Figure 10. The
solution currently adopted (again, inherited from
</bodyText>
<equation confidence="0.9966733">
(SINV (ADVP (RB Also))
(VP-TPC-2 (VBN excluded)
(NP (-NONE- *-1)))
(VP (MD will)
(VP (VB be)
(VP (-NONE- *T*-2))))
(NP-SBJ-1 investments in ...))
Figure 10: VP topicalization
(S (NP-SBJ (NNP Congress))
(VP (MD could)
(VP (VB pass)
(ADVP-MNR (RB quickly))
(NP (NP (DT a)
(\\ \\)
(JJ clean)
(’’ ’’)
(NN bill))
(VP (VBG containing)
(ADVP (JJ only))
(NP ... ))))))
</equation>
<figureCaption confidence="0.995595">
Figure 11: The extraposition problem
</figureCaption>
<bodyText confidence="0.999373666666667">
Xia’s sample settings) is as above: the paradigm is
switched and the auxiliary verb (“be”) is chosen as
the anchor of the initial tree.
</bodyText>
<subsectionHeader confidence="0.986235">
3.5 Extraposition and Verb Subcategorization
</subsectionHeader>
<bodyText confidence="0.9999511">
One of the key design principles that have been
guiding grammar development with TAGs is to keep
verb arguments as substitution slots local to the
tree anchored by the verb. It is widely known that
the Penn Treebank does not distinguish verb ob-
jects from adjuncts. So some sorts of heuristics are
needed to decide, among the candidates, which are
to be taken as arguments (Kinyon and Prolo, 2002);
the rest is extracted as separate VP modifier trees.
However, this step is not enough for the trees to
correctly reflect verb subcategorizations. The oc-
currence of discontinuous arguments, frequently ex-
plained as argument extraposition (the argument is
raised past the adjunct) creates a problem. In the
sentence in Figure 11 the verb “pass” should anchor
a tree with one NP object.
However in such a tree it would be impossible to
adjoin the tree for the intervening ADVP “quickly”
as a VP modifier and still have it between the verb
and the NP.9 LexTract then would instead extract an
</bodyText>
<footnote confidence="0.95178175">
9A striking use of sister adjunction in (Chiang, 2000) is ex-
actly the elegant way it solves this problem: the non-argument
tree can be adjoined onto a node (say, VP), positioning itself in
between the VP’s children, which is not possible with TAGs.
</footnote>
<note confidence="0.542672">
(NP (NP the 3 billion New Zealand dollars)
(PRN (-LRB- -LRB-)
</note>
<figure confidence="0.964593777777778">
(NP US$ 1.76 billion *U*)
(-RRB- -RRB-)))
a) A parenthetical NP attached to another NP
(S (NP-SBJ The total relationship)
(PRN (, ,)
(SBAR-ADV as Mr. Lee sees it)
(, ,))
(VP (VBZ is) ...))
b) A parenthetical S between subject and verb
</figure>
<figureCaption confidence="0.994959">
Figure 12: Parentheticals
</figureCaption>
<bodyText confidence="0.999970954545455">
intransitive tree for the VB “pass”, onto which the
ADVP modifier tree would adjoin. The second odd-
ity is that the NP object would also be extracted as a
VP modifier tree. In a nutshell, objects in extracted
trees are restricted to those which are not extraposed
and hence the trees may not truly reflect the proper
domain of locality. One view is that the set of trees
for a certain subcategorization frame would include
these degenerate cases. LexTract has an option to
allow limited discontinuity, i.e., a non-argument se-
quence between the verb and the first object (but not
between two objects). The non-arguments would
then be adjoined to the V node.10 So far we have
used only the latter alternative.
It is worth mentioning two other cases of extra-
position. Subject extraposition is handled by having
the extraposed subject, usually a sentential form, ad-
join at the VP of which it is the logical subject (the
original position is still occupied by an NP with the
expletive pronoun “it”). Relative clause extraposi-
tion is modeled by a relative clause tree, only it ad-
joins at a VP, instead of at an NP as is usual.
</bodyText>
<subsectionHeader confidence="0.936836">
3.6 Parentheticals
</subsectionHeader>
<bodyText confidence="0.999840777777778">
Parenthetical expressions are ubiquitous in lan-
guage: they may appear almost everywhere in a sen-
tence and can be of almost any category (Fig. 12).
We model them as adjoining, either to the left or
right of the constituent they are dominated by, de-
pending on whether they are to the left or right of the
head child of the parent’s node. Occasionally such
trees can also be initial. The respective trees for the
examples of Figure 12 are drawn in Figure 13. It
</bodyText>
<footnote confidence="0.97733875">
10Of course, although the solution covers most of the occur-
rences, and apart of any linguistic concern, there are still un-
covered cases, e.g., when a parenthetical expression intervenes
between the first and the second argument.
</footnote>
<figureCaption confidence="0.998544">
Figure 13: Extracted trees for parentheticals
</figureCaption>
<bodyText confidence="0.999836">
is always the case that the label PRN dominates a
single substitution node. Whenever this was not the
case in the training corpus, heuristics based on ob-
servation were used to enforce that, by inserting an
appropriate missing node.
</bodyText>
<subsectionHeader confidence="0.984138">
3.7 Projection labels
</subsectionHeader>
<bodyText confidence="0.99995">
LexTract extracts trees with no concern for the ap-
propriate projective structure of constituents when
not explicitly marked in the PTB. Figure 14 shows
two examples of NP modification where the modi-
fiers are single lexical items. The extracted modifier
trees, shown on the right, do not have the projec-
tion for the modifiers JJR “stronger” and the NNP
“October” (which should be, respectively, an ADJP
and an NP). That is so, because those nodes are not
found in the annotation.
</bodyText>
<figure confidence="0.9272888">
(NP (DT a)
(JJR stronger)
(NN argument))
(NP-SBJ-1 (NNP October)
(NN weather))
</figure>
<figureCaption confidence="0.9888625">
Figure 14: Simple modification annotation and ex-
tracted trees
</figureCaption>
<bodyText confidence="0.999808785714286">
However, if the modifiers are complex, that is, if
the modifiers are themselves modified, the PTB in-
serts their respective projections, and therefore they
appear in the extracted trees, as shown in Figure 15.
There seems to be no reason for the two pairs
of extracted trees to be different. Much of this is
caused by the acknowledged flatness in the Penn
Treebank annotation. That said, the trees like those
in the second pair should be preferred. The projec-
tion node (ADJP or NP) is understood to be domi-
nating its head even when there is no further mod-
ification, and it should be a concern of a good ex-
traction process to insert the missing node into the
grammar. Since LexTract do not allow us to spec-
</bodyText>
<figure confidence="0.975305857142857">
(NP (DT an)
(ADJP (RB even)
(JJR stronger))
(NN argument))
(NP-SBJ-1 (NP (JJ late)
(NNP October))
(NN weather))
</figure>
<figureCaption confidence="0.993407">
Figure 15: Complex modification annotation and
extracted trees
</figureCaption>
<bodyText confidence="0.999993642857143">
ify for the insertion of “obligatory” projections we
had to accomplish this through a somewhat compli-
cated post-processing step using a projection table.
Some of our current projections are: nouns, per-
sonal pronouns and the existential expletive to NP;
adjectives to ADJP; adverbs to ADVP; sentences ei-
ther to SBAR (S, SINV) or to SBARQ (SQ); Cardi-
nals (CD) to Quantifier Phrases (QP) which them-
selves project to NP. Notice that not all categories
are forcefully projected. For instance, verbs are
not, allowing for simple auxiliary extraction. IN
is also not projected due to its double role as PP
head (true preposition) and subordinate conjunc-
tion, which should project onto SBARs.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999482375">
We discussed an experiment in grammar extraction
from corpora with focus on problems arising while
trying to give an adequate account for naturally oc-
curing phenomena. Without being exhaustive in our
list, we expect to have brougt some attention to the
need to discuss solutions for them which are as rea-
sonable as possible given the current state-of-the-art
of the linguistic research, computational grammar
development and automatic extraction, and given
the current corpus resources at our disposition.
ACKNOWLEDGEMENTS: Thanks to Tonia
Bleam, Erwin Chan, Alexandra Kinyon, Rashmi
Prasad, Beatrice Santorini, Fei Xia and the XTAG
Group for valuable discussions along the realization
of this work and/or comments on this paper or re-
lated material.
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99909362745098">
Joan Bresnan and Jane Grimshaw. 1978. The syn-
tax of free relatives in english. Linguistic Inquiry,
9(3):331–391.
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn Treebank. In
Proceedings ofthe 6th International Workshop on
Parsing Technologies, Trento, Italy.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Gram-
mar. In Proceedings of the 38th Annual Meeting
of the Association for Computational Linguistics,
Hong Kong, China.
Robert Frank. 2002. Phrase Structure Composition
and Syntactic Dependencies. MIT Press, Cam-
bridge, MA, USA.
Anneke Groos and Henk von Riemsdijk. 1979. The
matching effects in free relatives: a parameter of
core grammar. In Theory of Markedness in Gen-
erative Grammar. Scuola Normale Superiore di
Pisa, Italy.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Handbook of Formal
Languages, volume 3, pages 69–123. Springer-
Verlag, Berlin.
Alexandra Kinyon and Carlos A. Prolo. 2002.
Identifying verb arguments and their syntactic
function in the Penn Treebank. In Proc. of the
Third LREC, pages 1982–87, Las Palmas, Spain.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Compu-
tational Linguistics, 19(2):313–330.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings
of the 1994 Human Language Technology
Workshop.
Carlos A. Prolo. 2002. LR parsing for Tree Adjoin-
ing Grammars and its application to corpus-based
natural language parsing. Ph.D. Dissertation Pro-
posal, University of Pennsylvania.
Fei Xia. 1999. Extracting tree adjoining gram-
mars from bracketed corpora. In Proceedings of
the 5th Natural Language Processing Pacific Rim
Symposium(NLPRS-99), Beijing, China.
Fei Xia. 2001. Investigating the Relationship be-
tween Grammars and Treebanks for Natural Lan-
guages. Ph.D. thesis, Department of Computer
and Information Science, Un. of Pennsylvania.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.815695">
<title confidence="0.999445">Coping with problems in grammars automatically extracted from treebanks</title>
<author confidence="0.998014">A Carlos</author>
<affiliation confidence="0.99938">Computer and Information Science University of</affiliation>
<address confidence="0.91275">Suite 400A, 3401 Walnut Philadelphia, PA, USA,</address>
<email confidence="0.999768">prolo@linc.cis.upenn.edu</email>
<abstract confidence="0.9986341">We report in this paper on an experiment on automatic extraction of a Tree Adjoining Grammar from the WSJ corpus of the Penn Treebank. We use an automatic tool developed by (Xia, 2001) properly adapted to our particular need. Rather than addressing general aspects of the automatic extraction we focus on the problems we have found to extract a linguistically (and computationally) sound grammar and approaches to handle them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Jane Grimshaw</author>
</authors>
<title>The syntax of free relatives in english.</title>
<date>1978</date>
<journal>Linguistic Inquiry,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="8908" citStr="Bresnan and Grimshaw, 1978" startWordPosition="1508" endWordPosition="1511">e Penn Treebank as sentential complements as in Figure 5.a. The extracted tree corresponding to the occurrence of “make” would be of a verb that takes a sentential complement (SBAR). This does not seem to be correct6, as the proper subcategorization of the verb occurrence is transitive. In fact, free relatives may occur wherever an NP argument may occur. So, the only reasonable extraction account consistent with maintaining them as SBARs would be one in which every NP substitution node in an extracted tree would admit the 6In both standard accounts for free relatives, the Head Account (e.g., (Bresnan and Grimshaw, 1978)) and the Comp Account (e.g., (Groos and von Riemsdijk, 1979)), commonly discussed in the literature, the presence of the NP (or DP) is clear. existence of a counterpart tree, identical to the first, except that the NP argument label is replaced with an SBAR. Instead we opted to reflect the NP character of the free relatives by pre-processing the corpus (using the Head-analysis, for practical convenience). The annotated example is then automatically replaced with the one in Figure 5.b. Other cases of free-relatives (non-NP) are rare and not likely to interfere with verb subcategorization. 3.2 </context>
</contexts>
<marker>Bresnan, Grimshaw, 1978</marker>
<rawString>Joan Bresnan and Jane Grimshaw. 1978. The syntax of free relatives in english. Linguistic Inquiry, 9(3):331–391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated extraction of TAGs from the Penn Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings ofthe 6th International Workshop on Parsing Technologies,</booktitle>
<location>Trento, Italy.</location>
<marker>Chen, Vijay-Shanker, 2000</marker>
<rawString>John Chen and K. Vijay-Shanker. 2000. Automated extraction of TAGs from the Penn Treebank. In Proceedings ofthe 6th International Workshop on Parsing Technologies, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted Tree Adjoining Grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="18272" citStr="Chiang, 2000" startWordPosition="3128" endWordPosition="3129">arate VP modifier trees. However, this step is not enough for the trees to correctly reflect verb subcategorizations. The occurrence of discontinuous arguments, frequently explained as argument extraposition (the argument is raised past the adjunct) creates a problem. In the sentence in Figure 11 the verb “pass” should anchor a tree with one NP object. However in such a tree it would be impossible to adjoin the tree for the intervening ADVP “quickly” as a VP modifier and still have it between the verb and the NP.9 LexTract then would instead extract an 9A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem: the non-argument tree can be adjoined onto a node (say, VP), positioning itself in between the VP’s children, which is not possible with TAGs. (NP (NP the 3 billion New Zealand dollars) (PRN (-LRB- -LRB-) (NP US$ 1.76 billion *U*) (-RRB- -RRB-))) a) A parenthetical NP attached to another NP (S (NP-SBJ The total relationship) (PRN (, ,) (SBAR-ADV as Mr. Lee sees it) (, ,)) (VP (VBZ is) ...)) b) A parenthetical S between subject and verb Figure 12: Parentheticals intransitive tree for the VB “pass”, onto which the ADVP modifier tree would adjoi</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted Tree Adjoining Grammar. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frank</author>
</authors>
<title>Phrase Structure Composition and Syntactic Dependencies.</title>
<date>2002</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3254" citStr="Frank, 2002" startWordPosition="513" endWordPosition="514">. In Section 2 we introduce the method of grammar extraction employed. The problems are discussed in Section 3. We conclude in Section 4. 2 The extracted grammar 2.1 TAGs A TAG is a set of lexicalized elementary trees that can be combined, through the operations of tree adjunction and tree substitution, to derive syntactic structures for sentences. We follow a common approach to grammar development for natural language using TAGs, under which, driven by locality principles, each elementary tree for a given lexical head is expected to contain its projection, and slots for its arguments (e.g., (Frank, 2002)). Figure 1 shows typical grammar template trees that can be selected by lexical items and combined to generate the structure in Figure 2. The derivation tree, to the right, contains the history of the tree grafting process that generated the derived tree, to the left.3 1We assume some familiarity with the basic notations in the PTB as in (Marcus et al., 1994). 2(Prolo, 2002) includes a more comprehensive and detailed discussion of grammar extraction alternatives and problems. 3For a more comprehensive introduction to TAGs and Lexicalized TAGs we refer the reader to (Joshi and Schabes, 1997). </context>
</contexts>
<marker>Frank, 2002</marker>
<rawString>Robert Frank. 2002. Phrase Structure Composition and Syntactic Dependencies. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anneke Groos</author>
<author>Henk von Riemsdijk</author>
</authors>
<title>The matching effects in free relatives: a parameter of core grammar.</title>
<date>1979</date>
<booktitle>In Theory of Markedness in Generative Grammar. Scuola Normale Superiore</booktitle>
<location>di Pisa, Italy.</location>
<marker>Groos, von Riemsdijk, 1979</marker>
<rawString>Anneke Groos and Henk von Riemsdijk. 1979. The matching effects in free relatives: a parameter of core grammar. In Theory of Markedness in Generative Grammar. Scuola Normale Superiore di Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>TreeAdjoining Grammars.</title>
<date>1997</date>
<booktitle>In Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--123</pages>
<publisher>SpringerVerlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="2292" citStr="Joshi and Schabes, 1997" startWordPosition="350" endWordPosition="353">and computational linguistics. More than numbers to express coverage, we have to start analyzing the quality of automatically generated grammars, identifying extraction problems and uncovering whatever solutions are being given for them, however interesting or ugly they might be, challenging the current paradigms of linguistic research to provide answers for the problems on a “by-need” basis. In this paper we report on a particular experience of automatic extraction of an English grammar from the WSJ corpus of the Penn Treebank (PTB) (Marcus et al., 1994)1 using Tree Adjoining Grammar (TAGs, (Joshi and Schabes, 1997)). We use an automatic tool developed by (Xia, 2001) properly adapted to our particular needs and focus on some problems we have found to extract a linguistically (and computationally) sound grammar and the solutions we gave to them. The list of problems is a sample, far from being exhaustive2 Likewise, the solutions will not always be satisfactory. In Section 2 we introduce the method of grammar extraction employed. The problems are discussed in Section 3. We conclude in Section 4. 2 The extracted grammar 2.1 TAGs A TAG is a set of lexicalized elementary trees that can be combined, through th</context>
<context position="3852" citStr="Joshi and Schabes, 1997" startWordPosition="609" endWordPosition="612">ments (e.g., (Frank, 2002)). Figure 1 shows typical grammar template trees that can be selected by lexical items and combined to generate the structure in Figure 2. The derivation tree, to the right, contains the history of the tree grafting process that generated the derived tree, to the left.3 1We assume some familiarity with the basic notations in the PTB as in (Marcus et al., 1994). 2(Prolo, 2002) includes a more comprehensive and detailed discussion of grammar extraction alternatives and problems. 3For a more comprehensive introduction to TAGs and Lexicalized TAGs we refer the reader to (Joshi and Schabes, 1997). foot, with the sibling subtree (after being recursively processed) being carried together into the auxiliary tree. Notice that the auxiliary trees are therefore either strictly right or left branching, the foot always immediately under the root node. Other kinds of auxiliary trees are therefore not allowed. VP PP VP* DT NP* NP N NP S VP V NP P NP np vt ppright det NP Figure 1: An example of Tree Adjoining Grammar [John] NP N [saw] V NP VP* S Derived tree Derivation tree [Mary] N VP [from] P PP NP DT [the] [window] NP* N vt[saw] np[John] np[Mary] pp[from] np[window] det[the] VBG NP S PP−LOC a</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. TreeAdjoining Grammars. In Handbook of Formal Languages, volume 3, pages 69–123. SpringerVerlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Kinyon</author>
<author>Carlos A Prolo</author>
</authors>
<title>Identifying verb arguments and their syntactic function in the Penn Treebank.</title>
<date>2002</date>
<booktitle>In Proc. of the Third LREC,</booktitle>
<pages>1982--87</pages>
<location>Las Palmas,</location>
<contexts>
<context position="17629" citStr="Kinyon and Prolo, 2002" startWordPosition="3017" endWordPosition="3020">. )))))) Figure 11: The extraposition problem Xia’s sample settings) is as above: the paradigm is switched and the auxiliary verb (“be”) is chosen as the anchor of the initial tree. 3.5 Extraposition and Verb Subcategorization One of the key design principles that have been guiding grammar development with TAGs is to keep verb arguments as substitution slots local to the tree anchored by the verb. It is widely known that the Penn Treebank does not distinguish verb objects from adjuncts. So some sorts of heuristics are needed to decide, among the candidates, which are to be taken as arguments (Kinyon and Prolo, 2002); the rest is extracted as separate VP modifier trees. However, this step is not enough for the trees to correctly reflect verb subcategorizations. The occurrence of discontinuous arguments, frequently explained as argument extraposition (the argument is raised past the adjunct) creates a problem. In the sentence in Figure 11 the verb “pass” should anchor a tree with one NP object. However in such a tree it would be impossible to adjoin the tree for the intervening ADVP “quickly” as a VP modifier and still have it between the verb and the NP.9 LexTract then would instead extract an 9A striking</context>
</contexts>
<marker>Kinyon, Prolo, 2002</marker>
<rawString>Alexandra Kinyon and Carlos A. Prolo. 2002. Identifying verb arguments and their syntactic function in the Penn Treebank. In Proc. of the Third LREC, pages 1982–87, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1130" citStr="Marcus et al., 1993" startWordPosition="161" endWordPosition="164">eral aspects of the automatic extraction we focus on the problems we have found to extract a linguistically (and computationally) sound grammar and approaches to handle them. 1 Introduction Much linguistic research is oriented to finding general principles for natural language, classifying linguistic phenomena, building regular models (e.g., grammars) for the well-behaved (or wellunderstood) part of languages and studying remaining “interesting” problems in a compartmentalized way. With the availability of large natural language corpora annotated for syntactic structure, the treebanks, e.g., (Marcus et al., 1993), automatic grammar extraction became possible (Chen and VijayShanker, 2000; Xia, 1999). Suddenly, grammars started being extracted with an attempt to have “full” coverage of the constructions in a certain language (of course, to the extent that the used corpora represents the language) and that immediately poses a question: If we do not know how to model many phenomena grammatically how can that be that we are extracting such a wide-coverage grammar?. To answer that question we have to start a new thread at the edge of linguistics and computational linguistics. More than numbers to express co</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 Human Language Technology Workshop.</booktitle>
<contexts>
<context position="2229" citStr="Marcus et al., 1994" startWordPosition="340" endWordPosition="343">n we have to start a new thread at the edge of linguistics and computational linguistics. More than numbers to express coverage, we have to start analyzing the quality of automatically generated grammars, identifying extraction problems and uncovering whatever solutions are being given for them, however interesting or ugly they might be, challenging the current paradigms of linguistic research to provide answers for the problems on a “by-need” basis. In this paper we report on a particular experience of automatic extraction of an English grammar from the WSJ corpus of the Penn Treebank (PTB) (Marcus et al., 1994)1 using Tree Adjoining Grammar (TAGs, (Joshi and Schabes, 1997)). We use an automatic tool developed by (Xia, 2001) properly adapted to our particular needs and focus on some problems we have found to extract a linguistically (and computationally) sound grammar and the solutions we gave to them. The list of problems is a sample, far from being exhaustive2 Likewise, the solutions will not always be satisfactory. In Section 2 we introduce the method of grammar extraction employed. The problems are discussed in Section 3. We conclude in Section 4. 2 The extracted grammar 2.1 TAGs A TAG is a set o</context>
<context position="3616" citStr="Marcus et al., 1994" startWordPosition="574" endWordPosition="577">follow a common approach to grammar development for natural language using TAGs, under which, driven by locality principles, each elementary tree for a given lexical head is expected to contain its projection, and slots for its arguments (e.g., (Frank, 2002)). Figure 1 shows typical grammar template trees that can be selected by lexical items and combined to generate the structure in Figure 2. The derivation tree, to the right, contains the history of the tree grafting process that generated the derived tree, to the left.3 1We assume some familiarity with the basic notations in the PTB as in (Marcus et al., 1994). 2(Prolo, 2002) includes a more comprehensive and detailed discussion of grammar extraction alternatives and problems. 3For a more comprehensive introduction to TAGs and Lexicalized TAGs we refer the reader to (Joshi and Schabes, 1997). foot, with the sibling subtree (after being recursively processed) being carried together into the auxiliary tree. Notice that the auxiliary trees are therefore either strictly right or left branching, the foot always immediately under the root node. Other kinds of auxiliary trees are therefore not allowed. VP PP VP* DT NP* NP N NP S VP V NP P NP np vt ppright</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the 1994 Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos A Prolo</author>
</authors>
<title>LR parsing for Tree Adjoining Grammars and its application to corpus-based natural language parsing.</title>
<date>2002</date>
<institution>Ph.D. Dissertation Proposal, University of Pennsylvania.</institution>
<contexts>
<context position="3632" citStr="Prolo, 2002" startWordPosition="578" endWordPosition="579"> to grammar development for natural language using TAGs, under which, driven by locality principles, each elementary tree for a given lexical head is expected to contain its projection, and slots for its arguments (e.g., (Frank, 2002)). Figure 1 shows typical grammar template trees that can be selected by lexical items and combined to generate the structure in Figure 2. The derivation tree, to the right, contains the history of the tree grafting process that generated the derived tree, to the left.3 1We assume some familiarity with the basic notations in the PTB as in (Marcus et al., 1994). 2(Prolo, 2002) includes a more comprehensive and detailed discussion of grammar extraction alternatives and problems. 3For a more comprehensive introduction to TAGs and Lexicalized TAGs we refer the reader to (Joshi and Schabes, 1997). foot, with the sibling subtree (after being recursively processed) being carried together into the auxiliary tree. Notice that the auxiliary trees are therefore either strictly right or left branching, the foot always immediately under the root node. Other kinds of auxiliary trees are therefore not allowed. VP PP VP* DT NP* NP N NP S VP V NP P NP np vt ppright det NP Figure 1</context>
<context position="17629" citStr="Prolo, 2002" startWordPosition="3019" endWordPosition="3020">gure 11: The extraposition problem Xia’s sample settings) is as above: the paradigm is switched and the auxiliary verb (“be”) is chosen as the anchor of the initial tree. 3.5 Extraposition and Verb Subcategorization One of the key design principles that have been guiding grammar development with TAGs is to keep verb arguments as substitution slots local to the tree anchored by the verb. It is widely known that the Penn Treebank does not distinguish verb objects from adjuncts. So some sorts of heuristics are needed to decide, among the candidates, which are to be taken as arguments (Kinyon and Prolo, 2002); the rest is extracted as separate VP modifier trees. However, this step is not enough for the trees to correctly reflect verb subcategorizations. The occurrence of discontinuous arguments, frequently explained as argument extraposition (the argument is raised past the adjunct) creates a problem. In the sentence in Figure 11 the verb “pass” should anchor a tree with one NP object. However in such a tree it would be impossible to adjoin the tree for the intervening ADVP “quickly” as a VP modifier and still have it between the verb and the NP.9 LexTract then would instead extract an 9A striking</context>
</contexts>
<marker>Prolo, 2002</marker>
<rawString>Carlos A. Prolo. 2002. LR parsing for Tree Adjoining Grammars and its application to corpus-based natural language parsing. Ph.D. Dissertation Proposal, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium(NLPRS-99),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="1217" citStr="Xia, 1999" startWordPosition="176" endWordPosition="177">uistically (and computationally) sound grammar and approaches to handle them. 1 Introduction Much linguistic research is oriented to finding general principles for natural language, classifying linguistic phenomena, building regular models (e.g., grammars) for the well-behaved (or wellunderstood) part of languages and studying remaining “interesting” problems in a compartmentalized way. With the availability of large natural language corpora annotated for syntactic structure, the treebanks, e.g., (Marcus et al., 1993), automatic grammar extraction became possible (Chen and VijayShanker, 2000; Xia, 1999). Suddenly, grammars started being extracted with an attempt to have “full” coverage of the constructions in a certain language (of course, to the extent that the used corpora represents the language) and that immediately poses a question: If we do not know how to model many phenomena grammatically how can that be that we are extracting such a wide-coverage grammar?. To answer that question we have to start a new thread at the edge of linguistics and computational linguistics. More than numbers to express coverage, we have to start analyzing the quality of automatically generated grammars, ide</context>
<context position="4715" citStr="Xia, 1999" startWordPosition="773" endWordPosition="774">ther kinds of auxiliary trees are therefore not allowed. VP PP VP* DT NP* NP N NP S VP V NP P NP np vt ppright det NP Figure 1: An example of Tree Adjoining Grammar [John] NP N [saw] V NP VP* S Derived tree Derivation tree [Mary] N VP [from] P PP NP DT [the] [window] NP* N vt[saw] np[John] np[Mary] pp[from] np[window] det[the] VBG NP S PP−LOC at NNP policies using NNS S PRP VP VP VP S−MNR NP−SBJ ADVP FNX they RB still NP−SBJ NP draft NNS ε IN NP VBP VP pens Figure 2: Derivation of John saw Mary from the window 2.2 LexTract Given an annotated sentence from the PTB as input Xia’s LexTract tool (Xia, 1999; Xia, 2001) first executes a rebracketing. More precisely, additional nodes are inserted to separate arguments and modifiers and to structure the modifying process as binary branching. A typical rebracketed PTB tree is shown in Figure 3,4 in which we have distinguished the tree nodes inserted by LexTract. The second stage is the extraction of the grammar trees proper shown in Figure 4. In particular, recursive modifier structures have to be detected and factored out of the derived tree to compose the auxiliary trees, the rest becoming an initial tree. The process is recursive also in the sens</context>
<context position="6437" citStr="Xia, 1999" startWordPosition="1068" endWordPosition="1069">y tree, the head-child into its 4Figures 3 and 4 are thanks to Fei Xia. We are also grateful to her for allowing us to use LexTract and make changes to its source code to customize to our needs. Figure 3: LexTract rebracketing stage To extract a grammar with Xia’s tool one has to define tables for finding: the head child of a constituent expansion; which of the siblings of a head are acceptable arguments; and which constituent labels are plausible modifiers of another. Special provisions are made for handling coordination. For additional information see (Xia, 2001). In this paper we refer to (Xia, 1999)’s table settings and extracted grammar, which we used as our starting point, as Xia’s sample. We used a customized version of LexTract, plus additional pre-processing of the PTB input and post-processing of the extracted trees. 3 Extraction Problems Extraction problems arise from several sources, including: (1) lack of proper linguistic account,5 (2) the (Penn Treebank) annotation style, (3) the (LexTract) extraction tool, (4) possible unsuitability of the (TAG) model, and (5) annotation errors. We refrained from making a rigid classification of the problems we present according to these sour</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium(NLPRS-99), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Investigating the Relationship between Grammars and Treebanks for Natural Languages.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, Un. of Pennsylvania.</institution>
<contexts>
<context position="2344" citStr="Xia, 2001" startWordPosition="361" endWordPosition="362">ge, we have to start analyzing the quality of automatically generated grammars, identifying extraction problems and uncovering whatever solutions are being given for them, however interesting or ugly they might be, challenging the current paradigms of linguistic research to provide answers for the problems on a “by-need” basis. In this paper we report on a particular experience of automatic extraction of an English grammar from the WSJ corpus of the Penn Treebank (PTB) (Marcus et al., 1994)1 using Tree Adjoining Grammar (TAGs, (Joshi and Schabes, 1997)). We use an automatic tool developed by (Xia, 2001) properly adapted to our particular needs and focus on some problems we have found to extract a linguistically (and computationally) sound grammar and the solutions we gave to them. The list of problems is a sample, far from being exhaustive2 Likewise, the solutions will not always be satisfactory. In Section 2 we introduce the method of grammar extraction employed. The problems are discussed in Section 3. We conclude in Section 4. 2 The extracted grammar 2.1 TAGs A TAG is a set of lexicalized elementary trees that can be combined, through the operations of tree adjunction and tree substitutio</context>
<context position="4727" citStr="Xia, 2001" startWordPosition="775" endWordPosition="776">of auxiliary trees are therefore not allowed. VP PP VP* DT NP* NP N NP S VP V NP P NP np vt ppright det NP Figure 1: An example of Tree Adjoining Grammar [John] NP N [saw] V NP VP* S Derived tree Derivation tree [Mary] N VP [from] P PP NP DT [the] [window] NP* N vt[saw] np[John] np[Mary] pp[from] np[window] det[the] VBG NP S PP−LOC at NNP policies using NNS S PRP VP VP VP S−MNR NP−SBJ ADVP FNX they RB still NP−SBJ NP draft NNS ε IN NP VBP VP pens Figure 2: Derivation of John saw Mary from the window 2.2 LexTract Given an annotated sentence from the PTB as input Xia’s LexTract tool (Xia, 1999; Xia, 2001) first executes a rebracketing. More precisely, additional nodes are inserted to separate arguments and modifiers and to structure the modifying process as binary branching. A typical rebracketed PTB tree is shown in Figure 3,4 in which we have distinguished the tree nodes inserted by LexTract. The second stage is the extraction of the grammar trees proper shown in Figure 4. In particular, recursive modifier structures have to be detected and factored out of the derived tree to compose the auxiliary trees, the rest becoming an initial tree. The process is recursive also in the sense that facto</context>
<context position="6398" citStr="Xia, 2001" startWordPosition="1060" endWordPosition="1061">nt is mapped into a root of an auxiliary tree, the head-child into its 4Figures 3 and 4 are thanks to Fei Xia. We are also grateful to her for allowing us to use LexTract and make changes to its source code to customize to our needs. Figure 3: LexTract rebracketing stage To extract a grammar with Xia’s tool one has to define tables for finding: the head child of a constituent expansion; which of the siblings of a head are acceptable arguments; and which constituent labels are plausible modifiers of another. Special provisions are made for handling coordination. For additional information see (Xia, 2001). In this paper we refer to (Xia, 1999)’s table settings and extracted grammar, which we used as our starting point, as Xia’s sample. We used a customized version of LexTract, plus additional pre-processing of the PTB input and post-processing of the extracted trees. 3 Extraction Problems Extraction problems arise from several sources, including: (1) lack of proper linguistic account,5 (2) the (Penn Treebank) annotation style, (3) the (LexTract) extraction tool, (4) possible unsuitability of the (TAG) model, and (5) annotation errors. We refrained from making a rigid classification of the prob</context>
</contexts>
<marker>Xia, 2001</marker>
<rawString>Fei Xia. 2001. Investigating the Relationship between Grammars and Treebanks for Natural Languages. Ph.D. thesis, Department of Computer and Information Science, Un. of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>