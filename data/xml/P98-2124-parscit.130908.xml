<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.5374706">
Word Clustering and Disambiguation Based on Co-occurrence
Data
Hang Li and Naoki Abe
Theory NEC Laboratory, Real WOrld Computing Partnership
c/o C&amp;C Media Research Laboratories., NEC
</note>
<address confidence="0.915149">
4-1-1 Miyazaki, Miyamae-ku, Kawasaki 216-8555, Japan
</address>
<email confidence="0.937226">
{lihang,abe}Occm.cl.nec.co.jp
</email>
<sectionHeader confidence="0.992793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997875">
We address the problem of clustering words (or con-
structing a thesaurus) based on co-occurrence data,
and using the acquired word classes to improve the
accuracy of syntactic disambiguation. We view this
problem as that of estimating a joint probability dis-
tribution specifying the joint probabilities of word
pairs, such as noun verb pairs. We propose an effi-
cient algorithm based on the Minimum Description
Length (MDL) principle for estimating such a prob-
ability distribution. Our method is a natural ex-
tension of those proposed in (Brown et al., 1992)
and (Li and Abe, 1996), and overcomes their draw-
backs while retaining their advantages. We then
combined this clustering method with the disam-
biguation method of (Li and Abe, 1995) to derive a
disambiguation method that makes use of both auto-
matically constructed thesauruses and a hand-made
thesaurus. The overall disambiguation accuracy
achieved by our method is 85.2%, which compares
favorably against the accuracy (82.4%) obtained by
the state-of-the-art disambiguation method of (Brill
and Resnik, 1994).
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946793650794">
We address the problem of clustering words, or that
of constructing a thesaurus, based on co-occurrence
data. We view this problem as that of estimating a
joint probability distribution over word pairs, speci-
fying the joint probabilitieb of word pairs, such as
noun verb pairs. In this paper, we assume that
the joint distribution can be expressed in the fol-
lowing manner, which is stated for noun verb pairs
for the sake of readability: The joint probability of
a noun and a verb is expressed as the product of the
joint probability of the noun class and the verb class
which the noun and the verb respectively belong to,
and the conditional probabilities of the noun and the
verb given their respective classes.
As a method for estimating such a probability
distribution, we propose an algorithm based on the
Minimum Description Length (MDL) principle. Our
clustering algorithm iteratively merges noun classes
and verb classes in turn, in a bottom up fashion. For
each merge it performs, it calculates the increase
in data description length resulting from merging
any noun (or verb) class pair, and performs the
merge having the least increase in data description
length, provided that the increase in data descrip-
tion length is less than the reduction in model de-
scription length.
There have been a number of methods proposed in
the literature to address the word clustering problem
(e.g., (Brown et al., 1992; Pereira et al., 1993; Li and
Abe, 1996)). The method proposed in this paper is
a natural extension of both Li Si Abe&apos;s and Brown
et al&apos;s methods, and is an attempt to overcome their
drawbacks while retaining their advantages.
The method of Brown et al, which is based on the
Maximum Likelihood Estimation (MLE), performs
a merge which would result in the least reduction
in (average) mutual information. Our method turns
out to be equivalent to performing the merge with
the least reduction in mutual information, provided
that the reduction is below a certain threshold which
depends on the size of the co-occurrence data and
the number of classes in the current situation. This
method, based on the MDL principle, takes into ac-
count both the fit to data and the simplicity of a
model, and thus can help cope with the over-fitting
problem that the MLE-based method of Brown et al
faces.
The model employed in (Li and Abe, 1996) is
based on the assumption that the word distribution
within a class is a uniform distribution, i.e. every
word in a same class is generated with an equal prob-
ability. Employing such a model has the undesirable
tendency of classifying into different classes those
words that have similar co-occurrence patterns but
have different absolute frequencies. The proposed
method, in contrast, employs a model in which dif-
ferent words within a same class can have different
conditional generation probabilities, and thus can
classify words in a way that is not affected by words&apos;
absolute frequencies and resolve the problem faced
by the method of (Li and Abe, 1996).
We evaluate our clustering method by using the
word classes and the joint probabilities obtained by
</bodyText>
<page confidence="0.997593">
749
</page>
<bodyText confidence="0.999969272727273">
it in syntactic disambiguation experiments. Our
experimental results indicate that using the word
classes constructed by our method gives better dis-
ambiguation results than when using Li &amp; Abe or
Brown et al&apos;s methods. By combining thesauruses
automatically constructed by our method and an
existing hand-made thesaurus (WordNet), we were
able to achieve the overall accuracy of 85.2% for pp-
attachment disambiguation, which compares favor-
ably against the accuracy (82.4%) obtained using the
state-of-the-art method of (Brill and Resnik, 1994).
</bodyText>
<sectionHeader confidence="0.990089" genericHeader="introduction">
2 Probability Model
</sectionHeader>
<bodyText confidence="0.99893825">
Suppose available to us are co-occurrence data over
two sets of words, such as the sample of verbs and
the head words of their direct objects given in Fig. 1.
Our goal is to (hierarchically) cluster the two sets
of words so that words having similar co-occurrence
patterns are classified in the same class, and output
a thesaurus for each set of words.
eat drink make
wine 0 3 1
beer 0 5 1
bread 4 0 2
rice 4 0 0
</bodyText>
<figureCaption confidence="0.999568">
Figure 1: Example co-occurrence data
</figureCaption>
<bodyText confidence="0.999932625">
We can view this problem as that of estimating
the best probability model from among a class of
models of (probability distributions) which can give
rise to the co-occurrence data.
In this paper, we consider the following type of
probability models. Assume without loss of gener-
ality that the two sets of words are a set of nouns
N. and a set of verbs V. A partition Tn of .AT is a
set of noun-classes satisfying Lic„ET„Cri = Al and
VCi,Ci E T,C2 n C3 = 0. A partition Tt, of V
can be defined analogously. We then define a proba-
bility model of noun-verb co-occurrence by defining
the joint probability of a noun n and a verb v as the
product of the joint probability of the noun and verb
classes that n and v belong to, and the conditional
probabilities of n and v given their classes, that is,
</bodyText>
<equation confidence="0.998524">
P(n,v)= P(Cn,C,) • P(nIC,B) • P(vIC,.,), (1)
</equation>
<bodyText confidence="0.999298">
where Cn and Ct, denote the (unique) classes to
which n and v belong. In this paper, we refer to
this model as the &apos;hard clustering model,&apos; since it is
based on a type of clustering in which each word can
belong to only one class. Fig. 2 shows an example of
the hard clustering model that can give rise to the
co-occurrence data in Fig. 1.
</bodyText>
<figure confidence="0.984774375">
P(vICv)
05 05
eat drink make P(Cn.Cv)
0.4 0.1
0.6 beer
0.6
0.4 0.1
0.4
</figure>
<figureCaption confidence="0.998853">
Figure 2: Example hard clustering model
</figureCaption>
<sectionHeader confidence="0.987194" genericHeader="method">
3 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.997716714285714">
A particular choice of partitions for a hard clustering
model is referred to as a &apos;discrete&apos; hard-clustering
model, with the probability parameters left to be
estimated. The values of these parameters can be
estimated based on the co-occurrence data by the
Maximum Likelihood Estimation. For a given set of
co-occurrence data
</bodyText>
<equation confidence="0.779935">
S vi),(nz, v2), • • • ,(nm., vm)},
</equation>
<bodyText confidence="0.993762666666667">
the maximum likelihood estimates of the parameters
are defined as the values that maximize the following
likelihood function with respect to the data:
</bodyText>
<equation confidence="0.993047">
fJ P(nz,v) = P(n, IG.,).P(vz ICv, ).P(Cn,
i = 1 i=1
</equation>
<bodyText confidence="0.9393155">
It is easy to see that this is possible by setting the
parameters as
</bodyText>
<equation confidence="0.953135333333333">
f
in
Vx E .AT U V, P(xIC,) - ff((cf,)).
</equation>
<bodyText confidence="0.99248">
Here, in denotes the entire data size, f(C„,C,) the
frequency of word pairs in class pair (Ca, Cy), f(x)
the frequency of word x, and f(C) the frequency of
words in class Cr.
</bodyText>
<sectionHeader confidence="0.969012" genericHeader="method">
4 Model Selection Criterion
</sectionHeader>
<bodyText confidence="0.999899777777778">
The question now is what criterion should we employ
to select the best model from among the possible
models. Here we adopt the Minimum Description
Length (MDL) principle. MDL (Rissanen, 1989) is
a criterion for data compression and statistical esti-
mation proposed in information theory.
In applying MDL, we calculate the code length for
encoding each model, referred to as the &apos;model de-
scription length&apos; L(M), the code length for encoding
</bodyText>
<figure confidence="0.328702">
wine
</figure>
<page confidence="0.929421">
750
</page>
<bodyText confidence="0.959289875">
the given data through the model, referred to as the
&apos;data description length&apos; L(SIM) and their sum:
L(M , S) = L(M) L(SIM).
The MDL principle stipulates that, for both data
compression and statistical estimation, the best
probability model with respect to given data is that
which requires the least total description length.
The data description length is calculated as
</bodyText>
<equation confidence="0.9925105">
L(sim)= - E log P(n, v),
(n,v)ES
</equation>
<bodyText confidence="0.997342333333333">
where P stands for the maximum likelihood estimate
of P (as defined in Section 3).
We then calculate the model description length as
</bodyText>
<equation confidence="0.9612595">
L(M) = — log m,
2
</equation>
<bodyText confidence="0.999989375">
where k denotes the number of free parameters in the
model, and in the entire data size.1 In this paper,
we ignore the code length for encoding a &apos;discrete
model,&apos; assuming implicitly that they are equal for
all models and consider only the description length
for encoding the parameters of a model as the model
description length.
If computation time were of no concern, we could
in principle calculate the total description length for
each model and select the optimal model in terms of
MDL. Since the number of hard clustering models
is of order 0(NN • 1/1/), where N and V denote the
size of the noun set and the verb set, respectively, it
would be infeasible to do so. We therefore need to
devise an efficient algorithm that heuristically per-
forms this task.
</bodyText>
<sectionHeader confidence="0.954225" genericHeader="method">
5 Clustering Algorithm
</sectionHeader>
<bodyText confidence="0.999335428571429">
The proposed algorithm, which we call `2D-
Clustering,&apos; iteratively selects a suboptimal MDL-
model from among those hard clustering models
which can be obtained from the current model by
merging a noun (or verb) class pair. As it turns out,
the minimum description length criterion can be re-
formalized in terms of (average) mutual information,
and a greedy heuristic algorithm can be formulated
to calculate, in each iteration, the reduction of mu-
tual information which would result from merging
any noun (or verb) class pair, and perform the merge
1We note that there are alternative ways of calculating
the parameter description length. For example, we can sep-
arately encode the different types of probability parameters;
the joint probabilities P(C,Cv), and the conditional prob-
abilities P(niCn) and P(vIC). Since these alternatives are
approximations of one another asymptotically, here we use
only the simplest formulation. In the full paper, we plan to
compare the empirical behavior of the alternatives.
having the least mutual information reduction, pro-
vided that the reduction is below a variable threshold.
</bodyText>
<equation confidence="0.576256">
2D-Clustering(S, bn, bv )
</equation>
<bodyText confidence="0.941176">
(S is the input co-occurrence data, and bn and bt,
are positive integers.)
</bodyText>
<listItem confidence="0.713205">
1. Initialize the set of noun classes Tn and the set
of verb classes Tv as:
</listItem>
<bodyText confidence="0.97672">
Tn = {{n}ln E Tv = {{v}lv E V},
where Ar and V denote the noun set and the
verb set, respectively.
</bodyText>
<listItem confidence="0.9910944">
2. Repeat the following three steps:
(a) execute Merge(S, Tn, T,„ bn) to update Tn,
(b) execute Merge(S, Tv , Tn, by) to update Tv,
(c) if Tn and Tv are unchanged, go to Step 3.
3. Construct and output a thesaurus for nouns
</listItem>
<bodyText confidence="0.9898662">
based on the history of Tn, and one for verbs
based on the history of Tv.
Next, we describe the procedure of &apos;Merge,&apos; as it
is being applied to the set of noun classes with the
set of verb classes fixed.
</bodyText>
<equation confidence="0.374002">
Merge(S, Tn, Tv, bn)
</equation>
<bodyText confidence="0.875778571428572">
1. For each class pair in Tn, calculate the reduc-
tion of mutual information which would result
from merging them. (The details will follow.)
Discard those class pairs whose mutual informa-
tion reduction (2) is not less than the threshold
of
(kB --kA).logra
2 • m
where in denotes the total data size, kB the
number of free parameters in the model before
the merge, and k A the number of free param-
eters in the model after the merge. Sort the
remaining class pairs in ascending order with
respect to mutual information reduction.
</bodyText>
<listItem confidence="0.9915605">
2. Merge the first bn class pairs in the sorted list.
3. Output current T.
</listItem>
<bodyText confidence="0.999436272727273">
We perform (maximum of) bn merges at step 2 for
improving efficiency, which will result in outputting
an at-most bn-ary tree. Note that, strictly speaking,
once we perform one merge, the model will change
and there will no longer be a guarantee that the
remaining merges still remain justifiable from the
viewpoint of MDL.
Next, we explain why the criterion in terms of
description length can be reformalized in terms of
mutual information. We denote the model before
a merge as MB and the model after the merge as
</bodyText>
<page confidence="0.991258">
751
</page>
<bodyText confidence="0.86919225">
MA. According to MDL, MA should have the least
increase in data description length
bLda, = L(S1MA) — L(SIMB) &gt; 0,
and at the same time satisfies
</bodyText>
<equation confidence="0.976074333333333">
(kB — kA ) log m
Ldat &lt;
2
</equation>
<bodyText confidence="0.9217075">
This is due to the fact that the decrease in model
description length equals
</bodyText>
<equation confidence="0.975966666666667">
(kB — kA ) log 772
L(MB) L(MA) = &gt;0,
2
</equation>
<bodyText confidence="0.998761">
and is identical for each merge.
In addition, suppose that MA is obtained by merg-
ing two noun classes C, and Ci in MB to a single
noun class Cii. We in fact need only calculate the
difference between description lengths with respect
to these classes, i.e.,
</bodyText>
<equation confidence="0.83452825">
- ECET, EnEc,,,vEc. log P(n, v)
+EcyET„ EnEC„vEC,, log P(n,v)
+c,ET EnEC,,vEC,, log P(n, v).
Now using the identity
P(n, v) = -2,0( P(C„ , Cy)
P(C,,C) D
P(C„) P(C„) (n) • P(v)
we can rewrite the above as
P(C,i,C,,)
ado= - Ec„ET„ f(cii, c.)1°g P(co) P(c.)
+ &gt;CET f (Ci Cv)1°g PP(
+ EcyET„ f ,Cv)log p (c( ))1:15Cc„)•
</equation>
<bodyText confidence="0.999915222222222">
Thus, the quantity bLda, is equivalent to the mutual
information reduction times the data size.2 We con-
clude therefore that in our present context, a cluster-
ing with the least data description length increase is
equivalent to that with the least mutual information
decrease.
Canceling out P(C,) and replacing the probabil-
ities with their maximum likelihood estimates, we
obtain
</bodyText>
<equation confidence="0.752540285714286">
,i6Ldat = (—EcoET„(f (Ci , Cy) f (Ci , Cy))
log f f
(2)
2 Average mutual information between Tn and Tv is defined
as
E (p(c,,, cv ) log prc(f)r,:pc(vc)v)) .
c. ET„ CET,.
</equation>
<bodyText confidence="0.997415352941176">
Therefore, we need calculate only this quantity for
each possible merge at Step 1 of Merge.
In our implementation of the algorithm, we first
load the co-occurrence data into a matrix, with
nouns corresponding to rows, verbs to columns.
When merging a noun class in row i and that in
row j (i &lt; j), for each C„ we add f (Ci , Cy) and
f (Ci , C„) obtaining f (Cii , Cy) , write f (Cii Cv) on
row i, move f Whist , Cv) to row j, and reduce the
matrix by one row.
By the above implementation, the worst case time
complexity of the algorithm is 0(N3 • V + 173 • N)
where N denotes the size of the noun set, V that of
the verb set. If we can merge b,-, and by classes at
each step, the algorithm will become slightly more
efficient with the time complexity of 0(1-vE— • V + -112- •
N).
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="method">
6 Related Work
</sectionHeader>
<subsectionHeader confidence="0.981821">
6.1 Models
</subsectionHeader>
<bodyText confidence="0.999994">
We can restrict the hard clustering model (1) by as-
suming that words within a same class are generated
with an equal probability, obtaining
</bodyText>
<equation confidence="0.946889333333333">
1
P(n, v) = P (C„, Cy) • 1 ,
IC&apos; iCv
</equation>
<bodyText confidence="0.999187">
which is equivalent to the model proposed by (Li and
Abe, 1996). Employing this restricted model has the
undesirable tendency to classify into different classes
those words that have similar co-occurrence patterns
but have different absolute frequencies.
The hard clustering model defined in (1) can also
be considered to be an extension of the model pro-
posed by Brown et al. First, dividing (1) by P(v),
we obtain
</bodyText>
<equation confidence="0.916577666666667">
P(n,v) = P(CniCy) P (nrn) (p(c,gsvico)
P(co P(vic,,)
Since hard clustering implies = 1
P(v)
holds, we have
P(CniCv) • P(niCn).
</equation>
<bodyText confidence="0.9999615">
In this way, the hard clustering model turns out to be
a class-based bigram model and is similar to Brown
et al&apos;s model. The difference is that the model of (3)
assumes that the clustering for G and the clustering
for Cy can be different, while the model of Brown et
al assumes that they are the same.
A very general model of noun verb joint probabil-
ities is a model of the following form:
</bodyText>
<equation confidence="0.9818585">
P(n, v) = E E P(C„, Cv)•P(nIC)•P(viCv).
c,tEr. C,Er„
Ldat =
f 4-f C.,
+ECET,, f (Ci,Cv)log I (fc(.4-)
f (C; , Cv) log f (fcc40)
</equation>
<page confidence="0.980575">
752
</page>
<bodyText confidence="0.999985444444444">
Here Fr, denotes a set of noun classes satisfying
Uc„Er,,Cr, = N, but not necessarily disjoint. Sim-
ilarly r, is a set of not necessarily disjoint verb
classes. We can view the problem of clustering words
in general as estimation of such a model. This type
of clustering in which a word can belong to several
different classes is generally referred to as &apos;soft clus-
tering.&apos; If we assume in the above model that each
verb forms a verb class by itself, then (4) becomes
</bodyText>
<equation confidence="0.4900395">
P(n, v) = E P(C, v) • P(nrn),
C.Ern
</equation>
<bodyText confidence="0.999860833333333">
which is equivalent to the model of Pereira et al. On
the other hand, if we restrict the general model of (4)
so that both noun classes and verb classes are dis-
joint, then we obtain the hard clustering model we
propose here (1). All of these models, therefore, are
some special cases of (4). Each specialization comes
with its merit and demerit. For example, employing
a model of soft clustering will make the clustering
process more flexible but also make the learning pro-
cess more computationally demanding. Our choice
of hard clustering obviously has the merits and de-
merits of the soft clustering model reversed.
</bodyText>
<subsectionHeader confidence="0.999563">
6.2 Estimation criteria
</subsectionHeader>
<bodyText confidence="0.999994875">
Our method is also an extension of that proposed
by Brown et al from the viewpoint of estimation cri-
terion. Their method merges word classes so that
the reduction in mutual information, or equivalently
the increase in data description length, is minimized.
Their method has the tendency to overfit the train-
ing data, since it is based on MLE. Employing MDL
can help solve this problem.
</bodyText>
<sectionHeader confidence="0.989756" genericHeader="method">
7 Disambiguation Method
</sectionHeader>
<bodyText confidence="0.999984692307692">
We apply the acquired word classes, or more specif-
ically the probability model of co-occurrence, to the
problem of structural disambiguation. In particular,
we consider the problem of resolving pp-attachment
ambiguities in quadruples, like (see, girl, with, tele-
scope) and that of resolving ambiguities in com-
pound noun triples, like (data, base, system). In
the former, we determine to which of &apos;see&apos; or &apos;girl&apos;
the phrase &apos;with telescope&apos; should be attached. In
the latter, we judge to which of &apos;base&apos; or &apos;system&apos;
the word &apos;data&apos; should be attached.
We can perform pp-attachment disambiguation by
comparing the probabilities
</bodyText>
<equation confidence="0.86029">
Pwith (telescopelsee), Pvvith(telescopelgirl)• (5)
</equation>
<bodyText confidence="0.999959545454546">
If the former is larger, we attach &apos;with telescope&apos;
to `see;&apos; if the latter is larger we attach it to `girl;&apos;
otherwise we make no decision. (Disambiguation on
compound noun triples can be performed similarly.)
Since the number of probabilities to be estimated
is extremely large, estimating all of these probabil-
ities accurately is generally infeasible (i.e., the data
sparseness problem). Using our clustering model to
calculate these conditional probabilities (by normal-
izing the joint probabilities with marginal probabil-
ities) can solve this problem.
We further enhance our disambiguation method
by the following back-off procedure: We first esti-
mate the two probabilities in question using hard
clustering models constructed by our method. We
also estimate the probabilities using an existing
(hand-made) thesaurus with the &apos;tree cut&apos; estima-
tion method of (Li and Abe, 1995), and use these
probability values when the probabilities estimated
based on hard clustering models are both zero. Fi-
nally, if both of them are still zero, we make a default
decision.
</bodyText>
<sectionHeader confidence="0.974843" genericHeader="evaluation">
8 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.997622">
8.1 Qualitative evaluation
</subsectionHeader>
<bodyText confidence="0.6037656">
In this experiment, we used heuristic rules to extract
verbs and the head words of their direct objects from
the sagged texts of the WSJ corpus (ACL/DCI CD-
ROM1) consisting of 126,084 sentences.
share, asset. data
stock, bond, security
inc. ,corp..co.
house, home
bank, group, firm
price. tea
money. Cash
car. vehicle
profit, risk
software. network
pressure, power
</bodyText>
<figureCaption confidence="0.999195">
Figure 3: A part of a constructed thesaurus
</figureCaption>
<bodyText confidence="0.999991666666667">
We then constructed a number of thesauruses
based on these data, using our method. Fig. 3 shows
a part of a thesaurus for 100 randomly selected
nouns, based on their appearances as direct objects
of 20 randomly selected verbs. The thesaurus seems
to agree with human intuition to some degree, al-
though it is constructed based on a relatively small
amount of co-occurrence data. For example, &apos;stock,&apos;
&apos;security,&apos; and &apos;bond&apos; are classified together, despite
the fact that their absolute frequencies in the data
vary a great deal (272, 59, and 79, respectively.)
The results demonstrate a desirable feature of our
method, namely, it classifies words based solely on
the similarities in co-occurrence data, and is not af-
fected by the absolute frequencies of the words.
</bodyText>
<subsectionHeader confidence="0.998896">
8.2 Compound noun disambiguation
</subsectionHeader>
<bodyText confidence="0.998689">
We extracted compound noun doubles (e.g., &apos;data
base&apos;) from the tagged texts of the WSJ corpus and
used them as training data, and then conducted
</bodyText>
<page confidence="0.99737">
753
</page>
<bodyText confidence="0.9998172">
structural disambiguation on compound noun triples
(e.g., &apos;data base system&apos;).
We first randomly selected 1,000 nouns from the
corpus, and extracted compound noun doubles con-
taining those nouns as training data and compound
noun triples containing those nouns as test data.
There were 8,604 training data and 299 test data.
We hand-labeled the test data with the correct dis-
ambiguation &apos;answers.&apos;
We performed clustering on the nouns on the
left position and the nouns on the right position in
the training data by using both our method (`2D-
Clustering&apos;) and Brown et al&apos;s method (`Brown&apos;).
We actually implemented an extended version of
their method, which separately conducts clustering
for nouns on the left and those on the right (which
should only improve the performance).
tach the first noun to the neighboring noun when
a decision cannot be made by each of the meth-
ods. We see that 2D-Clustering+Default performs
the best. These results demonstrate a desirable as-
pect of 2D-Clustering, namely, its ability of automat-
ically selecting the most appropriate level of clus-
tering, resulting in neither over-generalization nor
under-generalization.
</bodyText>
<subsectionHeader confidence="0.997554">
8.3 PP-attachment disambiguation
</subsectionHeader>
<bodyText confidence="0.998500777777778">
We extracted triples (e.g., &apos;see, with, telescope&apos;)
from the bracketed data of the WSJ corpus (Penn
Tree Bank), and conducted PP-attachment disam-
biguation on quadruples. We randomly generated
ten sets of data consisting of different training and
test data and conducted experiments through &apos;ten-
fold cross validation,&apos; i.e., all of the experimental
results reported below were obtained by taking av-
erage over ten trials.
</bodyText>
<tableCaption confidence="0.983107">
Table 2: PP-attachment disambiguation results
</tableCaption>
<table confidence="0.993691625">
Method Cov.(%) Acc.(%)
Default 100 56.2
Word-based 32.3 95.6
Brown 51.3 98.3
2D-Clustering 51.3 98.3
Li-Abe96 37.3 94.7
Word Net 74.3 94.5
NounClass-2DC 42.6 97.1
</table>
<figure confidence="0.998310625">
0 % 5
0.55
0.6
0.65
i&apos;Coverage 0&apos;&amp;quot;
0.8
0.85
09
</figure>
<figureCaption confidence="0.991321">
Figure 4: Compound noun disambiguation results
</figureCaption>
<figure confidence="0.9982122">
0.9
045
0.8
0.75
0.7
0.65
0.8
0.55
• •WorMiaseg:
.2D-Cluiferng•
</figure>
<bodyText confidence="0.999156285714286">
We next conducted structural disambiguation on
the test data, using the probabilities estimated based
on 2D-Clustering and Brown. We also tested the
method of using the probabilities estimated based
on word co-occurrences, denoted as &apos;Word-based.&apos;
Fig. 4 shows the results in terms of accuracy and
coverage, where coverage refers to the percentage
of test data for which the disambiguation method
was able to make a decision. Since for Brown the
number of classes finally created has to be designed
in advance, we tried a number of alternatives and
obtained results for each of them. (Note that, for
2D-Clustering, the optimal number of classes is au-
tomatically selected.)
</bodyText>
<tableCaption confidence="0.999787">
Table 1: Compound noun disambiguation results
</tableCaption>
<table confidence="0.998157">
Method Acc.(%)
Default 59.2
Word-based + Default 73.9
Brown + Default 77.3
2D-Clustering + Default 78.3
</table>
<bodyText confidence="0.999544041666666">
Tab. 1 shows the final results of all of the above
methods combined with &apos;Default,&apos; in which we at-
We constructed word classes using our method
(&apos;2D-Clustering&apos;) and the method of Brown et al
(`Brown&apos;). For both methods, following the pro-
posal due to (Tokunaga et al., 1995), we separately
conducted clustering with respect to each of the 10
most frequently occurring prepositions (e.g., &apos;for,&apos;
&apos;with,&apos; etc). We did not cluster words for rarely
occurring prepositions. We then performed disam-
biguation based on 2D-Clustering and Brown. We
also tested the method of using the probabilities es-
timated based on word co-occurrences, denoted as
Word-based.&apos;
Next, rather than using the conditional probabili-
ties estimated by our method, we only used the noun
thesauruses constructed by our method, and applied
the method of (Li and Abe, 1995) to estimate the
best &apos;tree cut models&apos; within the thesauruses3 in
order to estimate the conditional probabilities like
those in (5). We call the disambiguation method
using these probability values `NounClass-2DC.&apos; We
also tried the analogous method using thesauruses
constructed by the method of (Li and Abe, 1996)
</bodyText>
<footnote confidence="0.99957">
3The method of (Li and Abe, 1995) outputs a &apos;tree cut
model&apos; in a given thesaurus with conditional probabilities at-
tached to all the nodes in the tree cut. They use MDL to
select the best tree cut model.
</footnote>
<page confidence="0.997404">
754
</page>
<bodyText confidence="0.999753666666667">
and estimating the best tree cut models (this is ex-
actly the disambiguation method proposed in that
paper). Finally, we tried using a hand-made the-
saurus, WordNet (this is the same as the disam-
biguation method used in (Li and Abe, 1995)). We
denote these methods as `Li-Abe96&apos; and &apos;WordNet,&apos;
respectively.
Tab. 2 shows the results for all these methods in
terms of coverage and accuracy.
</bodyText>
<tableCaption confidence="0.996278">
Table 3: PP-attachment disambiguation results
</tableCaption>
<table confidence="0.998952111111111">
Method Acc.(%)
Word-based + Default 69.5
Brown + Default 76.2
2D-Clustering -I- Default 76.2
Li-Abe96 + Default 71.0
WordNet + Default 82.2
NounClass-2DC + Default 73.8
2D-Clustering + WordNet + Default 85.2
Brill-Resnik 82.4
</table>
<bodyText confidence="0.99996123255814">
We then enhanced each of these methods by using
a default rule when a decision cannot be made, which
is indicated as &apos;+Default.&apos; Tab. 3 shows the results
of these experiments.
We can make a number of observations from these
results. (1) 2D-Clustering achieves a broader cover-
age than NounClass-2DC. This is because in order
to estimate the probabilities for disambiguation, the
former exploits more information than the latter.
(2) For Brown, we show here only its best result,
which happens to be the same as the result for 2D-
Clustering, but in order to obtain this result we had
to take the trouble of conducting a number of tests to
find the best level of clustering. For 2D-Clustering,
this was done once and automatically. Compared
with Li-Abe96, 2D-Clustering clearly performs bet-
ter. Therefore we conclude that our method im-
proves these previous clustering methods in one way
or another. (3) 2D-Clustering outperforms WordNet
in term of accuracy, but not in terms of coverage.
This seems reasonable, since an automatically con-
structed thesaurus is more domain dependent and
therefore captures the domain dependent features
better, and thus can help achieve higher accuracy.
On the other hand, with the relatively small size of
training data we had available, its coverage is smaller
than that of a general purpose hand made thesaurus.
The result indicates that it makes sense to combine
automatically constructed thesauruses and a hand-
made thesaurus, as we have proposed in Section 7.
This method of combining both types of the-
sauruses &apos;2D-Clustering+WordNet+Default&apos; was
then tested. We see that this method performs the
best. (See Tab. 3.) Finally, for comparison, we
tested the &apos;transformation-based error-driven learn-
ing&apos; proposed in (Brill and Resnik, 1994), which is
a state-of-the-art method for pp-attachment disam-
biguation. Tab. 3 shows the result for this method
as &apos;Brill-Resnik.&apos; We see that our disambigua-
tion method also performs better than Brill-Resnik.
(Note further that for Brill &amp; Resnik&apos;s method, we
need to use quadruples as training data, whereas
ours only requires triples.)
</bodyText>
<sectionHeader confidence="0.997864" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999984888888889">
We have proposed a new method of clustering words
based on co-occurrence data. Our method employs
a probability model which naturally represents co-
occurrence patterns over word pairs, and makes use
of an efficient estimation algorithm based on the
MDL principle. Our clustering method improves
upon the previous methods proposed by Brown et al
and (Li and Abe, 1996), and furthermore it can be
used to derive a disambiguation method with overall
disambiguation accuracy of 85.2%, which improves
the performance of a state-of-the-art disambiguation
method.
The proposed algorithm, 2D-Clustering, can be
used in practice, as long as the data size is at the
level of the current Penn Tree Bank. Yet it is still
relatively computationally demanding, and thus an
important future task is to further improve on its
computational efficiency.
</bodyText>
<sectionHeader confidence="0.969874" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999579666666667">
We are grateful to Dr. S. Doi of NEC C&amp;C Media
Res. Labs. for his encouragement. We thank Ms. Y.
Yamaguchi of NIS for her programming efforts.
</bodyText>
<sectionHeader confidence="0.998967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999660285714286">
E. Brill and P. Resnik. A rule-based approach to
prepositional phrase attachment disambiguation.
Proc. of COLING&apos;94, pp. 1198-1204.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J.
C. Lai, and R. L. Mercer. 1992. Class-based n-
gram models of natural language. Comp. Ling.,
18(4):283-298.
H. Li and N. Abe. 1995. Generalizing case frames
using a thesaurus and the MDL principle. Comp.
Ling., (to appear).
H. Li and N. Abe. 1996. Clustering words with the
MDL principle. Proc. of COLING&apos;96, pp. 4-9.
F. Pereira, N. Tishby, and L. Lee. 1993. Distri-
butional clustering of English words. Proc. of
A C L &apos;93, pp. 183-190.
J. Rissanen. 1989. Stochastic Complexity in Statisti-
cal Inquiry. World Scientific Publishing Co., Sin-
gapore.
T. Tokunaga, M. Iwayama, and H. Tanaka. Auto-
matic thesaurus construction based-on grammat-
ical relations. Proc. of IJCAI&apos;95, pp. 1308-1313.
</reference>
<page confidence="0.998707">
755
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226398">
<title confidence="0.982395">Word Clustering and Disambiguation Based on Co-occurrence Data</title>
<author confidence="0.972104">Li Abe</author>
<affiliation confidence="0.925734">Theory NEC Laboratory, Real WOrld Computing Partnership</affiliation>
<address confidence="0.75843">c/o C&amp;C Media Research Laboratories., NEC Miyamae-ku, Kawasaki 216-8555, Japan</address>
<email confidence="0.980998">lihangOccm.cl.nec.co.jp</email>
<email confidence="0.980998">abeOccm.cl.nec.co.jp</email>
<abstract confidence="0.962546863636364">We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages. We then combined this clustering method with the disambiguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill</abstract>
<note confidence="0.895793">and Resnik, 1994).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>E Brill</author>
<author>P Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disambiguation.</title>
<booktitle>Proc. of COLING&apos;94,</booktitle>
<pages>1198--1204</pages>
<marker>Brill, Resnik, </marker>
<rawString>E. Brill and P. Resnik. A rule-based approach to prepositional phrase attachment disambiguation. Proc. of COLING&apos;94, pp. 1198-1204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based ngram models of natural language.</title>
<date>1992</date>
<journal>Comp. Ling.,</journal>
<pages>18--4</pages>
<contexts>
<context position="830" citStr="Brown et al., 1992" startWordPosition="120" endWordPosition="123">Kawasaki 216-8555, Japan {lihang,abe}Occm.cl.nec.co.jp Abstract We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages. We then combined this clustering method with the disambiguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill and Resnik, 1994). 1 Introduction We address the problem of clustering words, or that of constructing a thes</context>
<context position="2769" citStr="Brown et al., 1992" startWordPosition="434" endWordPosition="437">hm based on the Minimum Description Length (MDL) principle. Our clustering algorithm iteratively merges noun classes and verb classes in turn, in a bottom up fashion. For each merge it performs, it calculates the increase in data description length resulting from merging any noun (or verb) class pair, and performs the merge having the least increase in data description length, provided that the increase in data description length is less than the reduction in model description length. There have been a number of methods proposed in the literature to address the word clustering problem (e.g., (Brown et al., 1992; Pereira et al., 1993; Li and Abe, 1996)). The method proposed in this paper is a natural extension of both Li Si Abe&apos;s and Brown et al&apos;s methods, and is an attempt to overcome their drawbacks while retaining their advantages. The method of Brown et al, which is based on the Maximum Likelihood Estimation (MLE), performs a merge which would result in the least reduction in (average) mutual information. Our method turns out to be equivalent to performing the merge with the least reduction in mutual information, provided that the reduction is below a certain threshold which depends on the size o</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based ngram models of natural language. Comp. Ling., 18(4):283-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1995</date>
<journal>Comp. Ling.,</journal>
<note>(to appear).</note>
<contexts>
<context position="1011" citStr="Li and Abe, 1995" startWordPosition="150" endWordPosition="153">uired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages. We then combined this clustering method with the disambiguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill and Resnik, 1994). 1 Introduction We address the problem of clustering words, or that of constructing a thesaurus, based on co-occurrence data. We view this problem as that of estimating a joint probability distribution over word pairs, specifying the joint probabilitieb of word pairs, su</context>
<context position="18986" citStr="Li and Abe, 1995" startWordPosition="3279" endWordPosition="3282"> extremely large, estimating all of these probabilities accurately is generally infeasible (i.e., the data sparseness problem). Using our clustering model to calculate these conditional probabilities (by normalizing the joint probabilities with marginal probabilities) can solve this problem. We further enhance our disambiguation method by the following back-off procedure: We first estimate the two probabilities in question using hard clustering models constructed by our method. We also estimate the probabilities using an existing (hand-made) thesaurus with the &apos;tree cut&apos; estimation method of (Li and Abe, 1995), and use these probability values when the probabilities estimated based on hard clustering models are both zero. Finally, if both of them are still zero, we make a default decision. 8 Experimental Results 8.1 Qualitative evaluation In this experiment, we used heuristic rules to extract verbs and the head words of their direct objects from the sagged texts of the WSJ corpus (ACL/DCI CDROM1) consisting of 126,084 sentences. share, asset. data stock, bond, security inc. ,corp..co. house, home bank, group, firm price. tea money. Cash car. vehicle profit, risk software. network pressure, power Fi</context>
<context position="24204" citStr="Li and Abe, 1995" startWordPosition="4106" endWordPosition="4109"> following the proposal due to (Tokunaga et al., 1995), we separately conducted clustering with respect to each of the 10 most frequently occurring prepositions (e.g., &apos;for,&apos; &apos;with,&apos; etc). We did not cluster words for rarely occurring prepositions. We then performed disambiguation based on 2D-Clustering and Brown. We also tested the method of using the probabilities estimated based on word co-occurrences, denoted as Word-based.&apos; Next, rather than using the conditional probabilities estimated by our method, we only used the noun thesauruses constructed by our method, and applied the method of (Li and Abe, 1995) to estimate the best &apos;tree cut models&apos; within the thesauruses3 in order to estimate the conditional probabilities like those in (5). We call the disambiguation method using these probability values `NounClass-2DC.&apos; We also tried the analogous method using thesauruses constructed by the method of (Li and Abe, 1996) 3The method of (Li and Abe, 1995) outputs a &apos;tree cut model&apos; in a given thesaurus with conditional probabilities attached to all the nodes in the tree cut. They use MDL to select the best tree cut model. 754 and estimating the best tree cut models (this is exactly the disambiguation</context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>H. Li and N. Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. Comp. Ling., (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Clustering words with the MDL principle.</title>
<date>1996</date>
<booktitle>Proc. of COLING&apos;96,</booktitle>
<pages>4--9</pages>
<contexts>
<context position="853" citStr="Li and Abe, 1996" startWordPosition="125" endWordPosition="128">{lihang,abe}Occm.cl.nec.co.jp Abstract We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages. We then combined this clustering method with the disambiguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill and Resnik, 1994). 1 Introduction We address the problem of clustering words, or that of constructing a thesaurus, based on co-occu</context>
<context position="2810" citStr="Li and Abe, 1996" startWordPosition="442" endWordPosition="445"> (MDL) principle. Our clustering algorithm iteratively merges noun classes and verb classes in turn, in a bottom up fashion. For each merge it performs, it calculates the increase in data description length resulting from merging any noun (or verb) class pair, and performs the merge having the least increase in data description length, provided that the increase in data description length is less than the reduction in model description length. There have been a number of methods proposed in the literature to address the word clustering problem (e.g., (Brown et al., 1992; Pereira et al., 1993; Li and Abe, 1996)). The method proposed in this paper is a natural extension of both Li Si Abe&apos;s and Brown et al&apos;s methods, and is an attempt to overcome their drawbacks while retaining their advantages. The method of Brown et al, which is based on the Maximum Likelihood Estimation (MLE), performs a merge which would result in the least reduction in (average) mutual information. Our method turns out to be equivalent to performing the merge with the least reduction in mutual information, provided that the reduction is below a certain threshold which depends on the size of the co-occurrence data and the number o</context>
<context position="4358" citStr="Li and Abe, 1996" startWordPosition="703" endWordPosition="706">distribution within a class is a uniform distribution, i.e. every word in a same class is generated with an equal probability. Employing such a model has the undesirable tendency of classifying into different classes those words that have similar co-occurrence patterns but have different absolute frequencies. The proposed method, in contrast, employs a model in which different words within a same class can have different conditional generation probabilities, and thus can classify words in a way that is not affected by words&apos; absolute frequencies and resolve the problem faced by the method of (Li and Abe, 1996). We evaluate our clustering method by using the word classes and the joint probabilities obtained by 749 it in syntactic disambiguation experiments. Our experimental results indicate that using the word classes constructed by our method gives better disambiguation results than when using Li &amp; Abe or Brown et al&apos;s methods. By combining thesauruses automatically constructed by our method and an existing hand-made thesaurus (WordNet), we were able to achieve the overall accuracy of 85.2% for ppattachment disambiguation, which compares favorably against the accuracy (82.4%) obtained using the sta</context>
<context position="14874" citStr="Li and Abe, 1996" startWordPosition="2593" endWordPosition="2596"> reduce the matrix by one row. By the above implementation, the worst case time complexity of the algorithm is 0(N3 • V + 173 • N) where N denotes the size of the noun set, V that of the verb set. If we can merge b,-, and by classes at each step, the algorithm will become slightly more efficient with the time complexity of 0(1-vE— • V + -112- • N). 6 Related Work 6.1 Models We can restrict the hard clustering model (1) by assuming that words within a same class are generated with an equal probability, obtaining 1 P(n, v) = P (C„, Cy) • 1 , IC&apos; iCv which is equivalent to the model proposed by (Li and Abe, 1996). Employing this restricted model has the undesirable tendency to classify into different classes those words that have similar co-occurrence patterns but have different absolute frequencies. The hard clustering model defined in (1) can also be considered to be an extension of the model proposed by Brown et al. First, dividing (1) by P(v), we obtain P(n,v) = P(CniCy) P (nrn) (p(c,gsvico) P(co P(vic,,) Since hard clustering implies = 1 P(v) holds, we have P(CniCv) • P(niCn). In this way, the hard clustering model turns out to be a class-based bigram model and is similar to Brown et al&apos;s model. </context>
<context position="24520" citStr="Li and Abe, 1996" startWordPosition="4154" endWordPosition="4157">We also tested the method of using the probabilities estimated based on word co-occurrences, denoted as Word-based.&apos; Next, rather than using the conditional probabilities estimated by our method, we only used the noun thesauruses constructed by our method, and applied the method of (Li and Abe, 1995) to estimate the best &apos;tree cut models&apos; within the thesauruses3 in order to estimate the conditional probabilities like those in (5). We call the disambiguation method using these probability values `NounClass-2DC.&apos; We also tried the analogous method using thesauruses constructed by the method of (Li and Abe, 1996) 3The method of (Li and Abe, 1995) outputs a &apos;tree cut model&apos; in a given thesaurus with conditional probabilities attached to all the nodes in the tree cut. They use MDL to select the best tree cut model. 754 and estimating the best tree cut models (this is exactly the disambiguation method proposed in that paper). Finally, we tried using a hand-made thesaurus, WordNet (this is the same as the disambiguation method used in (Li and Abe, 1995)). We denote these methods as `Li-Abe96&apos; and &apos;WordNet,&apos; respectively. Tab. 2 shows the results for all these methods in terms of coverage and accuracy. Tab</context>
<context position="27875" citStr="Li and Abe, 1996" startWordPosition="4696" endWordPosition="4699">his method as &apos;Brill-Resnik.&apos; We see that our disambiguation method also performs better than Brill-Resnik. (Note further that for Brill &amp; Resnik&apos;s method, we need to use quadruples as training data, whereas ours only requires triples.) 9 Conclusions We have proposed a new method of clustering words based on co-occurrence data. Our method employs a probability model which naturally represents cooccurrence patterns over word pairs, and makes use of an efficient estimation algorithm based on the MDL principle. Our clustering method improves upon the previous methods proposed by Brown et al and (Li and Abe, 1996), and furthermore it can be used to derive a disambiguation method with overall disambiguation accuracy of 85.2%, which improves the performance of a state-of-the-art disambiguation method. The proposed algorithm, 2D-Clustering, can be used in practice, as long as the data size is at the level of the current Penn Tree Bank. Yet it is still relatively computationally demanding, and thus an important future task is to further improve on its computational efficiency. Acknowledgement We are grateful to Dr. S. Doi of NEC C&amp;C Media Res. Labs. for his encouragement. We thank Ms. Y. Yamaguchi of NIS f</context>
</contexts>
<marker>Li, Abe, 1996</marker>
<rawString>H. Li and N. Abe. 1996. Clustering words with the MDL principle. Proc. of COLING&apos;96, pp. 4-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>Proc. of A C L &apos;93,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="2791" citStr="Pereira et al., 1993" startWordPosition="438" endWordPosition="441">mum Description Length (MDL) principle. Our clustering algorithm iteratively merges noun classes and verb classes in turn, in a bottom up fashion. For each merge it performs, it calculates the increase in data description length resulting from merging any noun (or verb) class pair, and performs the merge having the least increase in data description length, provided that the increase in data description length is less than the reduction in model description length. There have been a number of methods proposed in the literature to address the word clustering problem (e.g., (Brown et al., 1992; Pereira et al., 1993; Li and Abe, 1996)). The method proposed in this paper is a natural extension of both Li Si Abe&apos;s and Brown et al&apos;s methods, and is an attempt to overcome their drawbacks while retaining their advantages. The method of Brown et al, which is based on the Maximum Likelihood Estimation (MLE), performs a merge which would result in the least reduction in (average) mutual information. Our method turns out to be equivalent to performing the merge with the least reduction in mutual information, provided that the reduction is below a certain threshold which depends on the size of the co-occurrence da</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. Proc. of A C L &apos;93, pp. 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic Complexity in Statistical Inquiry.</title>
<date>1989</date>
<publisher>World Scientific Publishing Co.,</publisher>
<contexts>
<context position="7859" citStr="Rissanen, 1989" startWordPosition="1326" endWordPosition="1327">the following likelihood function with respect to the data: fJ P(nz,v) = P(n, IG.,).P(vz ICv, ).P(Cn, i = 1 i=1 It is easy to see that this is possible by setting the parameters as f in Vx E .AT U V, P(xIC,) - ff((cf,)). Here, in denotes the entire data size, f(C„,C,) the frequency of word pairs in class pair (Ca, Cy), f(x) the frequency of word x, and f(C) the frequency of words in class Cr. 4 Model Selection Criterion The question now is what criterion should we employ to select the best model from among the possible models. Here we adopt the Minimum Description Length (MDL) principle. MDL (Rissanen, 1989) is a criterion for data compression and statistical estimation proposed in information theory. In applying MDL, we calculate the code length for encoding each model, referred to as the &apos;model description length&apos; L(M), the code length for encoding wine 750 the given data through the model, referred to as the &apos;data description length&apos; L(SIM) and their sum: L(M , S) = L(M) L(SIM). The MDL principle stipulates that, for both data compression and statistical estimation, the best probability model with respect to given data is that which requires the least total description length. The data descrip</context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>J. Rissanen. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific Publishing Co., Singapore.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Tokunaga</author>
<author>M Iwayama</author>
<author>H Tanaka</author>
</authors>
<title>Automatic thesaurus construction based-on grammatical relations.</title>
<booktitle>Proc. of IJCAI&apos;95,</booktitle>
<pages>1308--1313</pages>
<marker>Tokunaga, Iwayama, Tanaka, </marker>
<rawString>T. Tokunaga, M. Iwayama, and H. Tanaka. Automatic thesaurus construction based-on grammatical relations. Proc. of IJCAI&apos;95, pp. 1308-1313.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>