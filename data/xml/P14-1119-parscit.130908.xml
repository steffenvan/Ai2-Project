<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000276">
<title confidence="0.966993">
Automatic Keyphrase Extraction: A Survey of the State of the Art
</title>
<author confidence="0.905331">
Kazi Saidul Hasan and Vincent Ng
</author>
<affiliation confidence="0.962242">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.89887">
Richardson, TX 75083-0688
</address>
<email confidence="0.999701">
{saidul,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970444444444">
While automatic keyphrase extraction has
been examined extensively, state-of-the-
art performance on this task is still much
lower than that on many core natural lan-
guage processing tasks. We present a sur-
vey of the state of the art in automatic
keyphrase extraction, examining the major
sources of errors made by existing systems
and discussing the challenges ahead.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993728">
Automatic keyphrase extraction concerns “the au-
tomatic selection of important and topical phrases
from the body of a document” (Turney, 2000). In
other words, its goal is to extract a set of phrases
that are related to the main topics discussed in a
given document (Tomokiyo and Hurst, 2003; Liu
et al., 2009b; Ding et al., 2011; Zhao et al., 2011).
Document keyphrases have enabled fast and ac-
curate searching for a given document from a large
text collection, and have exhibited their potential
in improving many natural language processing
(NLP) and information retrieval (IR) tasks, such
as text summarization (Zhang et al., 2004), text
categorization (Hulth and Megyesi, 2006), opin-
ion mining (Berend, 2011), and document index-
ing (Gutwin et al., 1999).
Owing to its importance, automatic keyphrase
extraction has received a lot of attention. However,
the task is far from being solved: state-of-the-art
performance on keyphrase extraction is still much
lower than that on many core NLP tasks (Liu et al.,
2010). Our goal in this paper is to survey the state
of the art in keyphrase extraction, examining the
major sources of errors made by existing systems
and discussing the challenges ahead.
</bodyText>
<sectionHeader confidence="0.997199" genericHeader="introduction">
2 Corpora
</sectionHeader>
<bodyText confidence="0.9997421">
Automatic keyphrase extraction systems have
been evaluated on corpora from a variety of
sources ranging from long scientific publications
to short paper abstracts and email messages. Ta-
ble 1 presents a listing of the corpora grouped by
their sources as well as their statistics.1 There are
at least four corpus-related factors that affect the
difficulty of keyphrase extraction.
Length The difficulty of the task increases with
the length of the input document as longer doc-
uments yield more candidate keyphrases (i.e.,
phrases that are eligible to be keyphrases (see Sec-
tion 3.1)). For instance, each Inspec abstract has
on average 10 annotator-assigned keyphrases and
34 candidate keyphrases. In contrast, a scientific
paper typically has at least 10 keyphrases and hun-
dreds of candidate keyphrases, yielding a much
bigger search space (Hasan and Ng, 2010). Conse-
quently, it is harder to extract keyphrases from sci-
entific papers, technical reports, and meeting tran-
scripts than abstracts, emails, and news articles.
Structural consistency In a structured doc-
ument, there are certain locations where a
keyphrase is most likely to appear. For instance,
most of a scientific paper’s keyphrases should ap-
pear in the abstract and the introduction. While
structural information has been exploited to ex-
tract keyphrases from scientific papers (e.g., title,
section information) (Kim et al., 2013), web pages
(e.g., metadata) (Yih et al., 2006), and chats (e.g.,
dialogue acts) (Kim and Baldwin, 2012), it is most
useful when the documents from a source exhibit
structural similarity. For this reason, structural in-
formation is likely to facilitate keyphrase extrac-
tion from scientific papers and technical reports
because of their standard format (i.e., standard
sections such as abstract, introduction, conclusion,
etc.). In contrast, the lack of structural consistency
in other types of structured documents (e.g., web
pages, which can be blogs, forums, or reviews)
</bodyText>
<footnote confidence="0.999807333333333">
1Many of the publicly available corpora can be found
in http://github.com/snkim/AutomaticKeyphraseExtraction/
and http://code.google.com/p/maui-indexer/downloads/list.
</footnote>
<page confidence="0.835882">
1262
</page>
<note confidence="0.988932411764706">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Source Dataset/Contributor Statistics
Documents Tokens/doc Keys/doc
Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10
Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11
citeulike.org (Medelyan et al., 2009)∗ 180 - 5
SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15
Technical reports NZDL (Witten et al., 1999)∗ 1,800 - -
News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8
Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6
Web pages Yih et al. (2006) 828 - -
Hammouda et al. (2005)∗ 312 ≈500 -
Blogs (Grineva et al., 2009) 252 ≈1K 8
Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4
Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - -
Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10
</note>
<tableCaption confidence="0.999618">
Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗).
</tableCaption>
<bodyText confidence="0.99950459375">
may render structural information less useful.
Topic change An observation commonly ex-
ploited in keyphrase extraction from scientific ar-
ticles and news articles is that keyphrases typically
appear not only at the beginning (Witten et al.,
1999) but also at the end (Medelyan et al., 2009)
of a document. This observation does not neces-
sarily hold for conversational text (e.g., meetings,
chats), however. The reason is simple: in a conver-
sation, the topics (i.e., its talking points) change as
the interaction moves forward in time, and so do
the keyphrases associated with a topic. One way
to address this complication is to detect a topic
change in conversational text (Kim and Baldwin,
2012). However, topic change detection is not al-
ways easy: while the topics listed in the form of an
agenda at the beginning of formal meeting tran-
scripts can be exploited, such clues are absent in
casual conversations (e.g., chats).
Topic correlation Another observation com-
monly exploited in keyphrase extraction from
scientific articles and news articles is that the
keyphrases in a document are typically related to
each other (Turney, 2003; Mihalcea and Tarau,
2004). However, this observation does not nec-
essarily hold for informal text (e.g., emails, chats,
informal meetings, personal blogs), where people
can talk about any number of potentially uncorre-
lated topics. The presence of uncorrelated topics
implies that it may no longer be possible to exploit
relatedness and therefore increases the difficulty of
keyphrase extraction.
</bodyText>
<sectionHeader confidence="0.9961" genericHeader="method">
3 Keyphrase Extraction Approaches
</sectionHeader>
<bodyText confidence="0.999800285714286">
A keyphrase extraction system typically operates
in two steps: (1) extracting a list of words/phrases
that serve as candidate keyphrases using some
heuristics (Section 3.1); and (2) determining
which of these candidate keyphrases are correct
keyphrases using supervised (Section 3.2) or un-
supervised (Section 3.3) approaches.
</bodyText>
<subsectionHeader confidence="0.998732">
3.1 Selecting Candidate Words and Phrases
</subsectionHeader>
<bodyText confidence="0.999867923076923">
As noted before, a set of phrases and words is
typically extracted as candidate keyphrases using
heuristic rules. These rules are designed to avoid
spurious instances and keep the number of candi-
dates to a minimum. Typical heuristics include (1)
using a stop word list to remove stop words (Liu et
al., 2009b), (2) allowing words with certain part-
of-speech tags (e.g., nouns, adjectives, verbs) to be
candidate keywords (Mihalcea and Tarau, 2004;
Wan and Xiao, 2008b; Liu et al., 2009a), (3) al-
lowing n-grams that appear in Wikipedia article
titles to be candidates (Grineva et al., 2009), and
(4) extracting n-grams (Witten et al., 1999; Hulth,
2003; Medelyan et al., 2009) or noun phrases
(Barker and Cornacchia, 2000; Wu et al., 2005)
that satisfy pre-defined lexico-syntactic pattern(s)
(Nguyen and Phan, 2009).
Many of these heuristics have proven effective
with their high recall in extracting gold keyphrases
from various sources. However, for a long docu-
ment, the resulting list of candidates can be long.
Consequently, different pruning heuristics have
been designed to prune candidates that are un-
likely to be keyphrases (Huang et al., 2006; Kumar
and Srinathan, 2008; El-Beltagy and Rafea, 2009;
You et al., 2009; Newman et al., 2012).
</bodyText>
<subsectionHeader confidence="0.999154">
3.2 Supervised Approaches
</subsectionHeader>
<bodyText confidence="0.946610333333333">
Research on supervised approaches to keyphrase
extraction has focused on two issues: task refor-
mulation and feature design.
</bodyText>
<page confidence="0.930873">
1263
</page>
<subsectionHeader confidence="0.816343">
3.2.1 Task Reformulation
</subsectionHeader>
<bodyText confidence="0.999911923076923">
Early supervised approaches to keyphrase extrac-
tion recast this task as a binary classification prob-
lem (Frank et al., 1999; Turney, 1999; Witten et
al., 1999; Turney, 2000). The goal is to train a
classifier on documents annotated with keyphrases
to determine whether a candidate phrase is a
keyphrase. Keyphrases and non-keyphrases are
used to generate positive and negative examples,
respectively. Different learning algorithms have
been used to train this classifier, including naive
Bayes (Frank et al., 1999; Witten et al., 1999),
decision trees (Turney, 1999; Turney, 2000), bag-
ging (Hulth, 2003), boosting (Hulth et al., 2001),
maximum entropy (Yih et al., 2006; Kim and Kan,
2009), multi-layer perceptron (Lopez and Romary,
2010), and support vector machines (Jiang et al.,
2009; Lopez and Romary, 2010).
Recasting keyphrase extraction as a classifica-
tion problem has its weaknesses, however. Recall
that the goal of keyphrase extraction is to identify
the most representative phrases for a document.
In other words, if a candidate phrase c1 is more
representative than another candidate phrase c2, c1
should be preferred to c2. Note that a binary clas-
sifier classifies each candidate keyphrase indepen-
dently of the others, and consequently it does not
allow us to determine which candidates are better
than the others (Hulth, 2004; Wang and Li, 2011).
Motivated by this observation, Jiang et al.
(2009) propose a ranking approach to keyphrase
extraction, where the goal is to learn a ranker
to rank two candidate keyphrases. This pairwise
ranking approach therefore introduces competi-
tion between candidate keyphrases, and has been
shown to significantly outperform KEA (Witten
et al., 1999; Frank et al., 1999), a popular su-
pervised baseline that adopts the traditional super-
vised classification approach (Song et al., 2003;
Kelleher and Luz, 2005).
</bodyText>
<subsectionHeader confidence="0.641669">
3.2.2 Features
</subsectionHeader>
<bodyText confidence="0.999918333333333">
The features commonly used to represent an in-
stance for supervised keyphrase extraction can be
broadly divided into two categories.
</bodyText>
<subsectionHeader confidence="0.889293">
3.2.2.1 Within-Collection Features
</subsectionHeader>
<bodyText confidence="0.959342769230769">
Within-collection features are computed based
solely on the training documents. These features
can be further divided into three types.
Statistical features are computed based on sta-
tistical information gathered from the training
documents. Three such features have been exten-
sively used in supervised approaches. The first
one, tf*idf (Salton and Buckley, 1988), is com-
puted based on candidate frequency in the given
text and inverse document frequency (i.e., number
of other documents where the candidate appears).2
The second one, the distance of a phrase, is de-
fined as the number of words preceding its first
occurrence normalized by the number of words in
the document. Its usefulness stems from the fact
that keyphrases tend to appear early in a docu-
ment. The third one, supervised keyphraseness,
encodes the number of times a phrase appears as
a keyphrase in the training set. This feature is de-
signed based on the assumption that a phrase fre-
quently tagged as a keyphrase is more likely to be
a keyphrase in an unseen document. These three
features form the feature set of KEA (Witten et al.,
1999; Frank et al., 1999), and have been shown to
perform consistently well on documents from var-
ious sources (Yih et al., 2006; Kim et al., 2013).
Other statistical features include phrase length and
spread (i.e., the number of words between the first
and last occurrences of a phrase in the document).
Structural features encode how different in-
stances of a candidate keyphrase are located in
different parts of a document. A phrase is more
likely to be a keyphrase if it appears in the ab-
stract or introduction of a paper or in the metadata
section of a web page. In fact, features that en-
code how frequently a candidate keyphrase occurs
in various sections of a scientific paper (e.g., in-
troduction, conclusion) (Nguyen and Kan, 2007)
and those that encode the location of a candidate
keyphrase in a web page (e.g., whether it appears
in the title) (Chen et al., 2005; Yih et al., 2006)
have been shown to be useful for the task.
Syntactic features encode the syntactic pat-
terns of a candidate keyphrase. For example, a
candidate keyphrase has been encoded as (1) a
PoS tag sequence, which denotes the sequence of
part-of-speech tag(s) assigned to its word(s); and
(2) a suffix sequence, which is the sequence of
morphological suffixes of its words (Yih et al.,
2006; Nguyen and Kan, 2007; Kim and Kan,
2009). However, ablation studies conducted on
web pages (Yih et al., 2006) and scientific articles
</bodyText>
<footnote confidence="0.984733">
2A tf*idf-based baseline, where candidate keyphrases are
ranked and selected according to tf*idf, has been widely used
by both supervised and unsupervised approaches (Zhang et
al., 2005; Dredze et al., 2008; Paukkeri et al., 2008; Grineva
et al., 2009).
</footnote>
<page confidence="0.995357">
1264
</page>
<bodyText confidence="0.997423">
(Kim and Kan, 2009) reveal that syntactic features
are not useful for keyphrase extraction in the pres-
ence of other feature types.
</bodyText>
<subsectionHeader confidence="0.985076">
3.2.2.2 External Resource-Based Features
</subsectionHeader>
<bodyText confidence="0.99996623255814">
External resource-based features are computed
based on information gathered from resources
other than the training documents, such as lex-
ical knowledge bases (e.g., Wikipedia) or the
Web, with the goal of improving keyphrase extrac-
tion performance by exploiting external knowl-
edge. Below we give an overview of the exter-
nal resource-based features that have proven use-
ful for keyphrase extraction.
Wikipedia-based keyphraseness is computed as
a candidate’s document frequency multiplied by
the ratio of the number of Wikipedia articles where
the candidate appears as a link to the number of
articles where it appears (Medelyan et al., 2009).
This feature is motivated by the observation that
a candidate is likely to be a keyphrase if it occurs
frequently as a link in Wikipedia. Unlike super-
vised keyphraseness, Wikipedia-based keyphrase-
ness can be computed without using documents
annotated with keyphrases and can work even if
there is a mismatch between the training domain
and the test domain.
Yih et al. (2006) employ a feature that en-
codes whether a candidate keyphrase appears in
the query log of a search engine, exploiting the ob-
servation that a candidate is potentially important
if it was used as a search query. Terminological
databases have been similarly exploited to encode
the salience of candidate keyphrases in scientific
papers (Lopez and Romary, 2010).
While the aforementioned external resource-
based features attempt to encode how salient a
candidate keyphrase is, Turney (2003) proposes
features that encode the semantic relatedness be-
tween two candidate keyphrases. Noting that can-
didate keyphrases that are not semantically re-
lated to the predicted keyphrases are unlikely to
be keyphrases in technical reports, Turney em-
ploys coherence features to identify such can-
didate keyphrases. Semantic relatedness is en-
coded in the coherence features as two candidate
keyphrases’ pointwise mutual information, which
Turney computes by using the Web as a corpus.
</bodyText>
<subsectionHeader confidence="0.998359">
3.3 Unsupervised Approaches
</subsectionHeader>
<bodyText confidence="0.998416">
Existing unsupervised approaches to keyphrase
extraction can be categorized into four groups.
</bodyText>
<subsectionHeader confidence="0.52857">
3.3.1 Graph-Based Ranking
</subsectionHeader>
<bodyText confidence="0.999994813953489">
Intuitively, keyphrase extraction is about finding
the important words and phrases from a docu-
ment. Traditionally, the importance of a candi-
date has often been defined in terms of how related
it is to other candidates in the document. Infor-
mally, a candidate is important if it is related to (1)
a large number of candidates and (2) candidates
that are important. Researchers have computed re-
latedness between candidates using co-occurrence
counts (Mihalcea and Tarau, 2004; Matsuo and
Ishizuka, 2004) and semantic relatedness (Grineva
et al., 2009), and represented the relatedness in-
formation collected from a document as a graph
(Mihalcea and Tarau, 2004; Wan and Xiao, 2008a;
Wan and Xiao, 2008b; Bougouin et al., 2013).
The basic idea behind a graph-based approach
is to build a graph from the input document and
rank its nodes according to their importance us-
ing a graph-based ranking method (e.g., Brin and
Page (1998)). Each node of the graph corresponds
to a candidate keyphrase from the document and
an edge connects two related candidates. The
edge weight is proportional to the syntactic and/or
semantic relevance between the connected candi-
dates. For each node, each of its edges is treated
as a “vote” from the other node connected by the
edge. A node’s score in the graph is defined recur-
sively in terms of the edges it has and the scores of
the neighboring nodes. The top-ranked candidates
from the graph are then selected as keyphrases for
the input document. TextRank (Mihalcea and Ta-
rau, 2004) is one of the most well-known graph-
based approaches to keyphrase extraction.
This instantiation of a graph-based approach
overlooks an important aspect of keyphrase ex-
traction, however. A set of keyphrases for a doc-
ument should ideally cover the main topics dis-
cussed in it, but this instantiation does not guaran-
tee that all the main topics will be represented by
the extracted keyphrases. Despite this weakness, a
graph-based representation of text was adopted by
many approaches that propose different ways of
computing the similarity between two candidates.
</bodyText>
<subsectionHeader confidence="0.642942">
3.3.2 Topic-Based Clustering
</subsectionHeader>
<bodyText confidence="0.999302">
Another unsupervised approach to keyphrase
extraction involves grouping the candidate
keyphrases in a document into topics, such that
each topic is composed of all and only those
candidate keyphrases that are related to that topic
(Grineva et al., 2009; Liu et al., 2009b; Liu et
</bodyText>
<page confidence="0.941498">
1265
</page>
<bodyText confidence="0.999888883333333">
al., 2010). There are several motivations behind
this topic-based clustering approach. First, a
keyphrase should ideally be relevant to one or
more main topic(s) discussed in a document
(Liu et al., 2010; Liu et al., 2012). Second, the
extracted keyphrases should be comprehensive
in the sense that they should cover all the main
topics in a document (Liu et al., 2009b; Liu et al.,
2010; Liu et al., 2012). Below we examine three
representative systems that adopt this approach.
KeyCluster Liu et al. (2009b) adopt a
clustering-based approach (henceforth KeyClus-
ter) that clusters semantically similar candidates
using Wikipedia and co-occurrence-based statis-
tics. The underlying hypothesis is that each of
these clusters corresponds to a topic covered in
the document, and selecting the candidates close
to the centroid of each cluster as keyphrases
ensures that the resulting set of keyphrases covers
all the topics of the document.
While empirical results show that KeyCluster
performs better than both TextRank and Hulth’s
(2003) supervised system, KeyCluster has a poten-
tial drawback: by extracting keyphrases from each
topic cluster, it essentially gives each topic equal
importance. In practice, however, there could
be topics that are not important and these topics
should not have keyphrase(s) representing them.
Topical PageRank (TPR) Liu et al. (2010) pro-
pose TPR, an approach that overcomes the afore-
mentioned weakness of KeyCluster. It runs Tex-
tRank multiple times for a document, once for
each of its topics induced by a Latent Dirichlet Al-
location (Blei et al., 2003). By running TextRank
once for each topic, TPR ensures that the extracted
keyphrases cover the main topics of the document.
The final score of a candidate is computed as the
sum of its scores for each of the topics, weighted
by the probability of that topic in that document.
Hence, unlike KeyCluster, candidates belonging to
a less probable topic are given less importance.
TPR performs significantly better than both
tf*idf and TextRank on the DUC-2001 and Inspec
datasets. TPR’s superior performance strength-
ens the hypothesis of using topic clustering for
keyphrase extraction. However, though TPR is
conceptually better than KeyCluster, Liu et al. did
not compare TPR against KeyCluster.
CommunityCluster Grineva et al. (2009) pro-
pose CommunityCluster, a variant of the topic
clustering approach to keyphrase extraction. Like
TPR, CommunityCluster gives more weight to
more important topics, but unlike TPR, it extracts
all candidate keyphrases from an important topic,
assuming that a candidate that receives little focus
in the text should still be extracted as a keyphrase
as long as it is related to an important topic. Com-
munityCluster yields much better recall (without
losing precision) than extractors such as tf*idf,
TextRank, and the Yahoo! term extractor.
</bodyText>
<subsectionHeader confidence="0.905826">
3.3.3 Simultaneous Learning
</subsectionHeader>
<bodyText confidence="0.999926804878049">
Since keyphrases represent a dense summary of a
document, researchers hypothesized that text sum-
marization and keyphrase extraction can poten-
tially benefit from each other if these tasks are per-
formed simultaneously. Zha (2002) proposes the
first graph-based approach for simultaneous sum-
marization and keyphrase extraction, motivated by
a key observation: a sentence is important if it con-
tains important words, and important words ap-
pear in important sentences. Wan et al. (2007) ex-
tend Zha’s work by adding two assumptions: (1)
an important sentence is connected to other im-
portant sentences, and (2) an important word is
linked to other important words, a TextRank-like
assumption. Based on these assumptions, Wan et
al. (2007) build three graphs to capture the asso-
ciation between the sentences (S) and the words
(W) in an input document, namely, a S–S graph,
a bipartite S–W graph, and a W–W graph. The
weight of an edge connecting two sentence nodes
in a S–S graph corresponds to their content simi-
larity. An edge weight in a S–W graph denotes the
word’s importance in the sentence it appears. Fi-
nally, an edge weight in a W–W graph denotes the
co-occurrence or knowledge-based similarity be-
tween the two connected words. Once the graphs
are constructed for an input document, an itera-
tive reinforcement algorithm is applied to assign
scores to each sentence and word. The top-scored
words are used to form keyphrases.
The main advantage of this approach is that it
combines the strengths of both Zha’s approach
(i.e., bipartite S–W graphs) and TextRank (i.e., W–
W graphs) and performs better than both of them.
However, it has a weakness: like TextRank, it does
not ensure that the extracted keyphrases will cover
all the main topics. To address this problem, one
can employ a topic clustering algorithm on the W–
W graph to produce the topic clusters, and then en-
sure that keyphrases are chosen from every main
topic cluster.
</bodyText>
<page confidence="0.961271">
1266
</page>
<subsectionHeader confidence="0.516174">
3.3.4 Language Modeling
</subsectionHeader>
<bodyText confidence="0.99998234">
Many existing approaches have a separate, heuris-
tic module for extracting candidate keyphrases
prior to keyphrase ranking/extraction. In contrast,
Tomokiyo and Hurst (2003) propose an approach
(henceforth LMA) that combines these two steps.
LMA scores a candidate keyphrase based on
two features, namely, phraseness (i.e., the ex-
tent to which a word sequence can be treated as
a phrase) and informativeness (i.e., the extent to
which a word sequence captures the central idea of
the document it appears in). Intuitively, a phrase
that has high scores for phraseness and informa-
tiveness is likely to be a keyphrase. These feature
values are estimated using language models (LMs)
trained on a foreground corpus and a background
corpus. The foreground corpus is composed of
the set of documents from which keyphrases are
to be extracted. The background corpus is a large
corpus that encodes general knowledge about the
world (e.g., the Web). A unigram LM and an n-
gram LM are constructed for each of these two
corpora. Phraseness, defined using the foreground
LM, is calculated as the loss of information in-
curred as a result of assuming a unigram LM (i.e.,
conditional independence among the words of the
phrase) instead of an n-gram LM (i.e., the phrase
is drawn from an n-gram LM). Informativeness is
computed as the loss that results because of the
assumption that the candidate is sampled from the
background LM rather than the foreground LM.
The loss values are computed using Kullback-
Leibler divergence. Candidates are ranked accord-
ing to the sum of these two feature values.
In sum, LMA uses a language model rather than
heuristics to identify phrases, and relies on the lan-
guage model trained on the background corpus to
determine how “unique” a candidate keyphrase is
to the domain represented by the foreground cor-
pus. The more unique it is to the foreground’s do-
main, the more likely it is a keyphrase for that do-
main. While the use of language models to iden-
tify phrases cannot be considered a major strength
of this approach (because heuristics can identify
phrases fairly reliably), the use of a background
corpus to identify candidates that are unique to the
foreground’s domain is a unique aspect of this ap-
proach. We believe that this idea deserves further
investigation, as it would allow us to discover a
keyphrase that is unique to the foreground’s do-
main but may have a low tf*idf value.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999839">
In this section, we describe metrics for evaluating
keyphrase extraction systems as well as state-of-
the-art results on commonly-used datasets.
</bodyText>
<subsectionHeader confidence="0.976345">
4.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999937000000001">
Designing evaluation metrics for keyphrase ex-
traction is by no means an easy task. To score
the output of a keyphrase extraction system, the
typical approach, which is also adopted by the
SemEval-2010 shared task on keyphrase extrac-
tion, is (1) to create a mapping between the
keyphrases in the gold standard and those in the
system output using exact match, and then (2)
score the output using evaluation metrics such as
precision (P), recall (R), and F-score (F).
Conceivably, exact match is an overly strict con-
dition, considering a predicted keyphrase incor-
rect even if it is a variant of a gold keyphrase.
For instance, given the gold keyphrase “neural
network”, exact match will consider a predicted
phrase incorrect even if it is an expanded version
of the gold keyphrase (“artificial neural network”)
or one of its morphological (“neural networks”) or
lexical (“neural net”) variants. While morphologi-
cal variations can be handled using a stemmer (El-
Beltagy and Rafea, 2009), other variations may
not be handled easily and reliably.
Human evaluation has been suggested as a pos-
sibility (Matsuo and Ishizuka, 2004), but it is time-
consuming and expensive. For this reason, re-
searchers have experimented with two types of
automatic evaluation metrics. The first type of
metrics addresses the problem with exact match.
These metrics reward a partial match between a
predicted keyphrase and a gold keyphrase (i.e.,
overlapping n-grams) and are commonly used
in machine translation (MT) and summarization
evaluations. They include BLEU, METEOR, NIST,
and ROUGE. Nevertheless, experiments show that
these MT metrics only offer a partial solution to
problem with exact match: they can only detect a
subset of the near-misses (Kim et al., 2010a).
The second type of metrics focuses on how a
system ranks its predictions. Given that two sys-
tems A and B have the same number of correct
predictions, binary preference measure (Bpref)
and mean reciprocal rank (MRR) (Liu et al., 2010)
will award more credit to A than to B if the ranks
of the correct predictions in A’s output are higher
than those in B’s output. R-precision (Rp) is an
</bodyText>
<page confidence="0.969271">
1267
</page>
<bodyText confidence="0.999662285714286">
IR metric that focuses on ranking: given a docu-
ment with n gold keyphrases, it computes the pre-
cision of a system over its n highest-ranked can-
didates (Zesch and Gurevych, 2009). The motiva-
tion behind the design of Rp is simple: a system
will achieve a perfect Rp value if it ranks all the
keyphrases above the non-keyphrases.
</bodyText>
<subsectionHeader confidence="0.994532">
4.2 The State of the Art
</subsectionHeader>
<bodyText confidence="0.999974888888889">
Table 2 lists the best scores on some popular evalu-
ation datasets and the corresponding systems. For
example, the best F-scores on the Inspec test set,
the DUC-2001 dataset, and the SemEval-2010 test
set are 45.7, 31.7, and 27.5, respectively.3
Two points deserve mention. First, F-scores de-
crease as document length increases. These re-
sults are consistent with the observation we made
in Section 2 that it is more difficult to extract
keyphrases correctly from longer documents. Sec-
ond, recent unsupervised approaches have rivaled
their supervised counterparts in performance (Mi-
halcea and Tarau, 2004; El-Beltagy and Rafea,
2009; Liu et al., 2009b). For example, KP-Miner
(El-Beltagy and Rafea, 2010), an unsupervised
system, ranked third in the SemEval-2010 shared
task with an F-score of 25.2, which is comparable
to the best supervised system scoring 27.5.
</bodyText>
<sectionHeader confidence="0.994405" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999989333333333">
With the goal of providing directions for future
work, we identify the errors commonly made by
state-of-the-art keyphrase extractors below.
</bodyText>
<subsectionHeader confidence="0.871459">
5.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999954785714286">
Although a few researchers have presented a sam-
ple of their systems’ output and the corresponding
gold keyphrases to show the differences between
them (Witten et al., 1999; Nguyen and Kan, 2007;
Medelyan et al., 2009), a systematic analysis of
the major types of errors made by state-of-the-art
keyphrase extraction systems is missing.
To fill this gap, we ran four keyphrase extrac-
tion systems on four commonly-used datasets of
varying sources, including Inspec abstracts (Hulth,
2003), DUC-2001 news articles (Over, 2001), sci-
entific papers (Kim et al., 2010b), and meeting
transcripts (Liu et al., 2009a). Specifically, we ran-
domly selected 25 documents from each of these
</bodyText>
<footnote confidence="0.960597333333333">
3A more detailed analysis of the results of the SemEval-
2010 shared task and the approaches adopted by the partici-
pating systems can be found in Kim et al. (2013).
</footnote>
<table confidence="0.999204142857143">
Dataset Approach and System Score
[Supervised?]
P R F
Abstracts Topic clustering 35.0 66.0 45.7
(Inspec) (Liu et al., 2009b) [×]
Blogs 35.1 61.5 44.7
Topic community detection
(Grineva et al., 2009) [ × ]
News Graph-based ranking 28.8 35.4 31.7
(DUC for extended neighborhood
-2001) (Wan and Xiao, 2008b) [×]
Papers Statistical, semantic, and 27.2 27.8 27.5
(SemEval distributional features
-2010) (Lopez and Romary, 2010) [✓]
</table>
<tableCaption confidence="0.996397">
Table 2: Best scores achieved on various datasets.
</tableCaption>
<bodyText confidence="0.99981985">
four datasets and manually analyzed the output of
the four systems, including tf*idf, the most fre-
quently used baseline, as well as three state-of-the-
art keyphrase extractors, of which two are unsu-
pervised (Wan and Xiao, 2008b; Liu et al., 2009b)
and one is supervised (Medelyan et al., 2009).
Our analysis reveals that the errors fall into four
major types, each of which contributes signifi-
cantly to the overall errors made by the four sys-
tems, despite the fact that the contribution of each
of these error types varies from system to system.
Moreover, we do not observe any significant dif-
ference between the types of errors made by the
four systems other than the fact that the super-
vised system has the expected tendency to predict
keyphrases seen in the training data. Below we
describe these four major types of errors.
Overgeneration errors are a major type of pre-
cision error, contributing to 28–37% of the overall
error. Overgeneration errors occur when a system
correctly predicts a candidate as a keyphrase be-
cause it contains a word that appears frequently in
the associated document, but at the same time er-
roneously outputs other candidates as keyphrases
because they contain the same word. Recall that
for many systems, it is not easy to reject a non-
keyphrase containing a word with a high term fre-
quency: many unsupervised systems score a can-
didate by summing the score of each of its compo-
nent words, and many supervised systems use un-
igrams as features to represent a candidate. To be
more concrete, consider the news article on athlete
Ben Johnson in Figure 1, where the keyphrases are
boldfaced. As we can see, the word Olympic(s)
has a significant presence in the document. Con-
sequently, many systems not only correctly predict
Olympics as a keyphrase, but also erroneously pre-
dict Olympic movement as a keyphrase, yielding
overgeneration errors.
Infrequency errors are a major type of re-
</bodyText>
<page confidence="0.982855">
1268
</page>
<bodyText confidence="0.924963461538461">
Canadian Ben Johnson left the Olympics today “in a
complete state of shock,” accused of cheating with drugs
in the world’s fastest 100-meter dash and stripped of
his gold medal. The prize went to American Carl
Lewis. Many athletes accepted the accusation that John-
son used a muscle-building but dangerous and illegal an-
abolic steroid called stanozolol as confirmation of what
they said they know has been going on in track and field.
Two tests of Johnson’s urine sample proved positive and
his denials of drug use were rejected today. “This is
a blow for the Olympic Games and the Olympic move-
ment,” said International Olympic Committee President
Juan Antonio Samaranch.
</bodyText>
<figureCaption confidence="0.99922">
Figure 1: A news article on Ben Johnson from the
DUC-2001 dataset. The keyphrases are boldfaced.
</figureCaption>
<bodyText confidence="0.999833162790697">
call error contributing to 24–27% of the overall
error. Infrequency errors occur when a system
fails to identify a keyphrase owing to its infre-
quent presence in the associated document (Liu
et al., 2011). Handling infrequency errors is a
challenge because state-of-the-art keyphrase ex-
tractors rarely predict candidates that appear only
once or twice in a document. In the Ben Johnson
example, many keyphrase extractors fail to iden-
tify 100-meter dash and gold medal as keyphrases,
resulting in infrequency errors.
Redundancy errors are a type of precision er-
ror contributing to 8–12% of the overall error. Re-
dundancy errors occur when a system correctly
identifies a candidate as a keyphrase, but at the
same time outputs a semantically equivalent can-
didate (e.g., its alias) as a keyphrase. This type
of error can be attributed to a system’s failure
to determine that two candidates are semantically
equivalent. Nevertheless, some researchers may
argue that a system should not be penalized for re-
dundancy errors because the extracted candidates
are in fact keyphrases. In our example, Olympics
and Olympic games refer to the same concept, so
a system that predicts both of them as keyphrases
commits a redundancy error.
Evaluation errors are a type of recall error con-
tributing to 7–10% of the overall error. An evalu-
ation error occurs when a system outputs a can-
didate that is semantically equivalent to a gold
keyphrase, but is considered erroneous by a scor-
ing program because of its failure to recognize
that the predicted phrase and the corresponding
gold keyphrase are semantically equivalent. In
other words, an evaluation error is not an error
made by a keyphrase extractor, but an error due
to the naivety of a scoring program. In our exam-
ple, while Olympics and Olympic games refer to
the same concept, only the former is annotated as
keyphrase. Hence, an evaluation error occurs if a
system predicts Olympic games but not Olympics
as a keyphrase and the scoring program fails to
identify them as semantically equivalent.
</bodyText>
<subsectionHeader confidence="0.997178">
5.2 Recommendations
</subsectionHeader>
<bodyText confidence="0.999891363636364">
We recommend that background knowledge be
extracted from external lexical databases (e.g.,
YAGO2 (Suchanek et al., 2007), Freebase (Bol-
lacker et al., 2008), BabelNet (Navigli and
Ponzetto, 2012)) to address the four types of er-
rors discussed above.
First, we discuss how redundancy errors could
be addressed by using the background knowledge
extracted from external databases. Note that if we
can identify semantically equivalent candidates,
then we can reduce redundancy errors. The ques-
tion, then, is: can background knowledge be used
to help us identify semantically equivalent candi-
dates? To answer this question, note that Freebase,
for instance, has over 40 million topics (i.e., real-
world entities such as people, places, and things)
from over 70 domains (e.g., music, business, ed-
ucation). Hence, before a system outputs a set of
candidates as keyphrases, it can use Freebase to
determine whether any of them is mapped to the
same Freebase topic. Referring back to our run-
ning example, both Olympics and Olympic games
are mapped to a Freebase topic called Olympic
games. Based on this information, a keyphrase ex-
tractor can determine that the two candidates are
aliases and should output only one of them, thus
preventing a redundancy error.
Next, we discuss how infrequency errors
could be addressed using background knowledge.
A natural way to handle this problem would be
to make an infrequent keyphrase frequent. To ac-
complish this, we suggest exploiting an influen-
tial idea in the keyphrase extraction literature: the
importance of a candidate is defined in terms of
how related it is to other candidates in the text (see
Section 3.3.1). In other words, if we could relate
an infrequent keyphrase to other candidates in the
text, we could boost its importance.
We believe that this could be accomplished us-
ing background knowledge. The idea is to boost
the importance of infrequent keyphrases using
their frequent counterparts. Consider again our
running example. All four systems have managed
to identify Ben Johnson as a keyphrase due to its
</bodyText>
<page confidence="0.988924">
1269
</page>
<bodyText confidence="0.999983854166667">
significant presence. Hence, we can boost the im-
portance of 100-meter dash and gold medal if we
can relate them to Ben Johnson.
To do so, note that Freebase maps a candi-
date to one or more pre-defined topics, each of
which is associated with one or more types. Types
are similar to entity classes. For instance, the
candidate Ben Johnson is mapped to a Freebase
topic with the same name, which is associated
with Freebase types such as Person, Athlete, and
Olympic athlete. Types are defined for a specific
domain in Freebase. For instance, Person, Ath-
lete, and Olympic athlete are defined in the People,
Sports, and Olympics domains, respectively. Next,
consider the two infrequent candidates, 100-meter
dash and gold medal. 100-meter dash is mapped
to the topic Sprint of type Sports in the Sports do-
main, whereas gold medal is mapped to a topic
with the same name of type Olympic medal in the
Olympics domain. Consequently, we can relate
100-meter dash to Ben Johnson via the Sports do-
main (i.e., they belong to different types under the
same domain). Additionally, gold medal can be
related to Ben Johnson via the Olympics domain.
As discussed before, the relationship between
two candidates is traditionally established using
co-occurrence information. However, using co-
occurrence windows has its shortcomings. First,
an ad-hoc window size cannot capture related can-
didates that are not inside the window. So it
is difficult to predict 100-meter dash and gold
medal as keyphrases: they are more than 10 tokens
away from frequent words such as Johnson and
Olympics. Second, the candidates inside a window
are all assumed to be related to each other, but it is
apparently an overly simplistic assumption. There
have been a few attempts to design Wikipedia-
based relatedness measures, with promising ini-
tial results (Grineva et al., 2009; Liu et al., 2009b;
Medelyan et al., 2009).4
Overgeneration errors could similarly be ad-
dressed using background knowledge. Recall that
Olympic movement is not a keyphrase in our ex-
ample although it includes an important word (i.e.,
Olympic). Freebase maps Olympic movement to
a topic with the same name, which is associated
with a type called Musical Recording in the Mu-
sic domain. However, it does not map Olympic
</bodyText>
<footnote confidence="0.95592775">
4Note that it may be difficult to employ our recommen-
dations to address infrequency errors in informal text with
uncorrelated topics because the keyphrases it contains may
not be related to each other (see Section 2).
</footnote>
<bodyText confidence="0.999312533333333">
movement to any topic in the Olympics domain.
The absence of such a mapping in the Olympics
domain could be used by a keyphrase extractor as
a supporting evidence against predicting Olympic
movement as a keyphrase.
Finally, as mentioned before, evaluation errors
should not be considered errors made by a sys-
tem. Nevertheless, they reveal a problem with the
way keyphrase extractors are currently evaluated.
To address this problem, one possibility is to con-
duct human evaluations. Cheaper alternatives in-
clude having human annotators identify semanti-
cally equivalent keyphrases during manual label-
ing, and designing scoring programs that can au-
tomatically identify such semantic equivalences.
</bodyText>
<sectionHeader confidence="0.993756" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999437">
We have presented a survey of the state of the art
in automatic keyphrase extraction. While unsu-
pervised approaches have started to rival their su-
pervised counterparts in performance, the task is
far from being solved, as reflected by the fairly
poor state-of-the-art results on various commonly-
used evaluation datasets. Our analysis revealed
that there are at least three major challenges ahead.
</bodyText>
<listItem confidence="0.7337001">
1. Incorporating background knowledge.
While much recent work has focused on algo-
rithmic development, keyphrase extractors need
to have a deeper “understanding” of a document
in order to reach the next level of performance.
Such an understanding can be facilitated by the
incorporation of background knowledge.
2. Handling long documents. While it may be
possible to design better algorithms to handle the
large number of candidates in long documents, we
believe that employing sophisticated features, es-
pecially those that encode background knowledge,
will enable keyphrases and non-keyphrases to be
distinguished more easily even in the presence of
a large number of candidates.
3. Improving evaluation schemes. To more ac-
curately measure the performance of keyphrase
extractors, they should not be penalized for evalu-
ation errors. We have suggested several possibili-
ties as to how this problem can be addressed.
</listItem>
<sectionHeader confidence="0.989474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998948">
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts of
this paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142.
</bodyText>
<page confidence="0.989602">
1270
</page>
<sectionHeader confidence="0.927728" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992064612612612">
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In
Proceedings of the 13th Biennial Conference of the
Canadian Society on Computational Studies of In-
telligence, pages 40–52.
G´abor Berend. 2011. Opinion expression mining by
exploiting keyphrase extraction. In Proceedings of
the 5th International Joint Conference on Natural
Language Processing, pages 1162–1170.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247–1250.
Adrien Bougouin, Florian Boudin, and B´eatrice Daille.
2013. Topicrank: Graph-based topic ranking for
keyphrase extraction. In Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing, pages 543–551.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1–7):107–117.
Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan
Lam. 2005. A practical system of keyphrase ex-
traction for web pages. In Proceedings of the 14th
ACM International Conference on Information and
Knowledge Management, pages 277–278.
Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2011.
Keyphrase extraction from online news using binary
integer programming. In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing, pages 165–173.
Mark Dredze, Hanna M. Wallach, Danny Puller, and
Fernando Pereira. 2008. Generating summary key-
words for emails using topics. In Proceedings of the
13th International Conference on Intelligent User
Interfaces, pages 199–206.
Samhaa R. El-Beltagy and Ahmed A. Rafea. 2009.
KP-Miner: A keyphrase extraction system for En-
glish and Arabic documents. Information Systems,
34(1):132–144.
Samhaa R. El-Beltagy and Ahmed Rafea. 2010. KP-
Miner: Participation in SemEval-2. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 190–193.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of 16th International Joint Conference on Arti-
ficial Intelligence, pages 668–673.
Maria Grineva, Maxim Grinev, and Dmitry Lizorkin.
2009. Extracting key terms from noisy and multi-
theme documents. In Proceedings of the 18th In-
ternational Conference on World Wide Web, pages
661–670.
Carl Gutwin, Gordon Paynter, Ian Witten, Craig Nevill-
Manning, and Eibe Frank. 1999. Improving brows-
ing in digital libraries with keyphrase indexes. De-
cision Support Systems, 27:81–104.
Khaled M. Hammouda, Diego N. Matute, and Mo-
hamed S. Kamel. 2005. CorePhrase: Keyphrase ex-
traction for document clustering. In Proceedings of
the 4th International Conference on Machine Learn-
ing and Data Mining in Pattern Recognition, pages
265–274.
Kazi Saidul Hasan and Vincent Ng. 2010. Conun-
drums in unsupervised keyphrase extraction: Mak-
ing sense of the state-of-the-art. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 365–373.
Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of the 6th International Conference on
Data Mining, pages 275–284.
Anette Hulth and Be´ata B. Megyesi. 2006. A study
on automatically extracted keywords in text catego-
rization. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 537–544.
Anette Hulth, Jussi Karlgren, Anna Jonsson, Henrik
Bostr¨om, and Lars Asker. 2001. Automatic key-
word extraction using domain knowledge. In Pro-
ceedings of the 2nd International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, pages 472–482.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, pages 216–
223.
Anette Hulth. 2004. Enhancing linguistically ori-
ented automatic keyword extraction. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics: Short Papers, pages 17–
20.
Xin Jiang, Yunhua Hu, and Hang Li. 2009. A rank-
ing approach to keyphrase extraction. In Proceed-
ings of the 32nd International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 756–757.
Daniel Kelleher and Saturnino Luz. 2005. Automatic
hypertext keyphrase detection. In Proceedings of the
19th International Joint Conference on Artificial In-
telligence, pages 1608–1609.
</reference>
<page confidence="0.976073">
1271
</page>
<note confidence="0.80376">
Su Nam Kim and Timothy Baldwin. 2012. Extracting
keywords from multi-party live chats. In Proceed-
ings of the 26th Pacific Asia Conference on Lan-
</note>
<reference confidence="0.976128085714286">
guage, Information, and Computation, pages 199–
208.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. In Proceedings of the ACL-IJCNLP
Workshop on Multiword Expressions, pages 9–16.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2010a. Evaluating n-gram based evaluation metrics
for automatic keyphrase extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 572–580.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010b. SemEval-2010 Task 5:
Automatic keyphrase extraction from scientific arti-
cles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 21–26.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2013. Automatic keyphrase
extraction from scientific articles. Language Re-
sources and Evaluation, 47(3):723–742.
Niraj Kumar and Kannan Srinathan. 2008. Automatic
keyphrase extraction from scientific documents us-
ing n-gram filtration technique. In Proceedings of
the 8th ACM Symposium on Document Engineering,
pages 199–208.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of Human Language Technologies: The
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 620–628.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009b. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257–266.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 366–376.
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning, pages 135–144.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012.
Topical word trigger model for keyphrase extraction.
In Proceedings of the 24th International Conference
on Computational Linguistics, pages 1715–1730.
Patrice Lopez and Laurent Romary. 2010. HUMB:
Automatic key term extraction from scientific arti-
cles in GROBID. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
248–251.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318–1327.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 404–411.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmenta-
tion for index term identification and keyphrase ex-
traction. In Proceedings of the 24th International
Conference on Computational Linguistics, pages
2077–2092.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proceedings of the International Conference on
Asian Digital Libraries, pages 317–326.
Chau Q. Nguyen and Tuoi T. Phan. 2009. An
ontology-based approach for key phrase extraction.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics and the 4th International Joint
Conference on Natural Language Processing: Short
Papers, pages 181–184.
Paul Over. 2001. Introduction to DUC-2001: An in-
trinsic evaluation of generic news text summariza-
tion systems. In Proceedings of the 2001 Document
Understanding Conference.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti P¨oll¨a,
and Timo Honkela. 2008. A language-independent
approach to keyphrase extraction and evaluation. In
Proceedings of the 22nd International Conference
on Computational Linguistics: Companion Volume:
Posters, pages 83–86.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513–
523.
</reference>
<page confidence="0.825001">
1272
</page>
<reference confidence="0.999859611111111">
Min Song, Il-Yeol Song, and Xiaohua Hu. 2003.
KPSpotter: A flexible information gain-based
keyphrase extraction system. In Proceedings of the
5th ACM International Workshop on Web Informa-
tion and Data Management, pages 50–53.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proceedings of the 16th International
World Wide Web Conference, pages 697–706.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions, pages 33–40.
Peter Turney. 1999. Learning to extract keyphrases
from text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303–336.
Peter Turney. 2003. Coherent keyphrase extraction
via web mining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence,
pages 434–439.
Xiaojun Wan and Jianguo Xiao. 2008a. Col-
labRank: Towards a collaborative approach to
single-document keyphrase extraction. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics, pages 969–976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 855–860.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552–559.
Chen Wang and Sujian Li. 2011. CoRankBayes:
Bayesian learning to rank under the co-training
framework and its application in keyphrase extrac-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement, pages 2241–2244.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In Pro-
ceedings of the 4th ACM Conference on Digital Li-
braries, pages 254–255.
Yi-Fang Brook Wu, Quanzhi Li, Razvan Stefan Bot,
and Xin Chen. 2005. Domain-specific keyphrase
extraction. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 283–284.
Wen-Tau Yih, Joshua Goodman, and Vitor R. Carvalho.
2006. Finding advertising keywords on web pages.
In Proceedings of the 15th International Conference
on World Wide Web, pages 213–222.
Wei You, Dominique Fontaine, and Jean-Paul Barth`es.
2009. Automatic keyphrase extraction with a
refined candidate set. In Proceedings of the
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology,
pages 576–579.
Torsten Zesch and Iryna Gurevych. 2009. Approxi-
mate matching for evaluating keyphrase extraction.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
2009, pages 484–489.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings
of 25th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 113–120.
Yongzheng Zhang, Nur Zincir-Heywood, and Evange-
los Milios. 2004. World Wide Web site summariza-
tion. Web Intelligence and Agent Systems, 2:39–53.
Yongzheng Zhang, Nur Zincir-Heywood, and Evange-
los Milios. 2005. Narrative text classification for
automatic key phrase extraction in web document
corpora. In Proceedings of the 7th ACM Interna-
tional Workshop on Web Information and Data Man-
agement, pages 51–58.
Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn
Achanauparp, Ee-Peng Lim, and Xiaoming Li.
2011. Topical keyphrase extraction from Twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 379–388.
</reference>
<page confidence="0.981192">
1273
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.533230">
<title confidence="0.996845">Automatic Keyphrase Extraction: A Survey of the State of the Art</title>
<author confidence="0.996128">Saidul Hasan</author>
<affiliation confidence="0.990028">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.549019">Richardson, TX</address>
<abstract confidence="0.9993674">While automatic keyphrase extraction has been examined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Nadia Cornacchia</author>
</authors>
<title>Using noun phrase heads to extract document keyphrases.</title>
<date>2000</date>
<booktitle>In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence,</booktitle>
<pages>40--52</pages>
<contexts>
<context position="7633" citStr="Barker and Cornacchia, 2000" startWordPosition="1180" endWordPosition="1183">sing heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase</context>
</contexts>
<marker>Barker, Cornacchia, 2000</marker>
<rawString>Ken Barker and Nadia Cornacchia. 2000. Using noun phrase heads to extract document keyphrases. In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence, pages 40–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´abor Berend</author>
</authors>
<title>Opinion expression mining by exploiting keyphrase extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1162--1170</pages>
<contexts>
<context position="1333" citStr="Berend, 2011" startWordPosition="205" endWordPosition="206">rom the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora from a variety of sources rangin</context>
</contexts>
<marker>Berend, 2011</marker>
<rawString>G´abor Berend. 2011. Opinion expression mining by exploiting keyphrase extraction. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1162–1170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="19422" citStr="Blei et al., 2003" startWordPosition="3054" endWordPosition="3057">that KeyCluster performs better than both TextRank and Hulth’s (2003) supervised system, KeyCluster has a potential drawback: by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance. In practice, however, there could be topics that are not important and these topics should not have keyphrase(s) representing them. Topical PageRank (TPR) Liu et al. (2010) propose TPR, an approach that overcomes the aforementioned weakness of KeyCluster. It runs TextRank multiple times for a document, once for each of its topics induced by a Latent Dirichlet Allocation (Blei et al., 2003). By running TextRank once for each topic, TPR ensures that the extracted keyphrases cover the main topics of the document. The final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document. Hence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance. TPR performs significantly better than both tf*idf and TextRank on the DUC-2001 and Inspec datasets. TPR’s superior performance strengthens the hypothesis of using topic clustering for keyphrase extraction. However, though TP</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="35003" citStr="Bollacker et al., 2008" startWordPosition="5606" endWordPosition="5610">lly equivalent. In other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program. In our example, while Olympics and Olympic games refer to the same concept, only the former is annotated as keyphrase. Hence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent. 5.2 Recommendations We recommend that background knowledge be extracted from external lexical databases (e.g., YAGO2 (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), BabelNet (Navigli and Ponzetto, 2012)) to address the four types of errors discussed above. First, we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases. Note that if we can identify semantically equivalent candidates, then we can reduce redundancy errors. The question, then, is: can background knowledge be used to help us identify semantically equivalent candidates? To answer this question, note that Freebase, for instance, has over 40 million topics (i.e., realworld entities such as people, places, and things) from over 70 d</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrien Bougouin</author>
<author>Florian Boudin</author>
<author>B´eatrice Daille</author>
</authors>
<title>Topicrank: Graph-based topic ranking for keyphrase extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing,</booktitle>
<pages>543--551</pages>
<contexts>
<context position="16173" citStr="Bougouin et al., 2013" startWordPosition="2533" endWordPosition="2536">raditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected candidates. For each node, each of its edges is treated as a “vote” from the other node connected by the edge. A node’s score in the graph is defined recursively in terms of t</context>
</contexts>
<marker>Bougouin, Boudin, Daille, 2013</marker>
<rawString>Adrien Bougouin, Florian Boudin, and B´eatrice Daille. 2013. Topicrank: Graph-based topic ranking for keyphrase extraction. In Proceedings of the 6th International Joint Conference on Natural Language Processing, pages 543–551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks,</journal>
<pages>30--1</pages>
<contexts>
<context position="16375" citStr="Brin and Page (1998)" startWordPosition="2568" endWordPosition="2571">umber of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected candidates. For each node, each of its edges is treated as a “vote” from the other node connected by the edge. A node’s score in the graph is defined recursively in terms of the edges it has and the scores of the neighboring nodes. The top-ranked candidates from the graph are then selected as keyphrases for the input document. TextRank (Mihalcea and Tarau, 2004) is one of th</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Chen</author>
<author>Jian-Tao Sun</author>
<author>Hua-Jun Zeng</author>
<author>Kwok-Yan Lam</author>
</authors>
<title>A practical system of keyphrase extraction for web pages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>277--278</pages>
<contexts>
<context position="12357" citStr="Chen et al., 2005" startWordPosition="1936" endWordPosition="1939">nd last occurrences of a phrase in the document). Structural features encode how different instances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. In fact, features that encode how frequently a candidate keyphrase occurs in various sections of a scientific paper (e.g., introduction, conclusion) (Nguyen and Kan, 2007) and those that encode the location of a candidate keyphrase in a web page (e.g., whether it appears in the title) (Chen et al., 2005; Yih et al., 2006) have been shown to be useful for the task. Syntactic features encode the syntactic patterns of a candidate keyphrase. For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence, which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studies conducted on web pages (Yih et al., 2006) and scientific articles 2A tf*idf-based baseline, where candidate keyphrases are ranked a</context>
</contexts>
<marker>Chen, Sun, Zeng, Lam, 2005</marker>
<rawString>Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan Lam. 2005. A practical system of keyphrase extraction for web pages. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, pages 277–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoye Ding</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Keyphrase extraction from online news using binary integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>165--173</pages>
<contexts>
<context position="949" citStr="Ding et al., 2011" startWordPosition="145" endWordPosition="148">nce on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyp</context>
</contexts>
<marker>Ding, Zhang, Huang, 2011</marker>
<rawString>Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2011. Keyphrase extraction from online news using binary integer programming. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Hanna M Wallach</author>
<author>Danny Puller</author>
<author>Fernando Pereira</author>
</authors>
<title>Generating summary keywords for emails using topics.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th International Conference on Intelligent User Interfaces,</booktitle>
<pages>199--206</pages>
<contexts>
<context position="4815" citStr="Dredze et al., 2008" startWordPosition="736" endWordPosition="739">atistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the beginning (Witten et al., 1999) but also at the end (Medelyan et al., 2009) of a document. This observation does not necessarily hold for conversational text (e.g., meetings, chats), however. The reason is simple: in a c</context>
<context position="13098" citStr="Dredze et al., 2008" startWordPosition="2057" endWordPosition="2060">date keyphrase. For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence, which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studies conducted on web pages (Yih et al., 2006) and scientific articles 2A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches (Zhang et al., 2005; Dredze et al., 2008; Paukkeri et al., 2008; Grineva et al., 2009). 1264 (Kim and Kan, 2009) reveal that syntactic features are not useful for keyphrase extraction in the presence of other feature types. 3.2.2.2 External Resource-Based Features External resource-based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g., Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge. Below we give an overview of the external resource-based features that have proven useful for</context>
</contexts>
<marker>Dredze, Wallach, Puller, Pereira, 2008</marker>
<rawString>Mark Dredze, Hanna M. Wallach, Danny Puller, and Fernando Pereira. 2008. Generating summary keywords for emails using topics. In Proceedings of the 13th International Conference on Intelligent User Interfaces, pages 199–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samhaa R El-Beltagy</author>
<author>Ahmed A Rafea</author>
</authors>
<title>KP-Miner: A keyphrase extraction system for English and Arabic documents.</title>
<date>2009</date>
<journal>Information Systems,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="8119" citStr="El-Beltagy and Rafea, 2009" startWordPosition="1254" endWordPosition="1257">l., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative e</context>
<context position="28391" citStr="El-Beltagy and Rafea, 2009" startWordPosition="4524" endWordPosition="4527">e 2 lists the best scores on some popular evaluation datasets and the corresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.3 Two points deserve mention. First, F-scores decrease as document length increases. These results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents. Second, recent unsupervised approaches have rivaled their supervised counterparts in performance (Mihalcea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Med</context>
</contexts>
<marker>El-Beltagy, Rafea, 2009</marker>
<rawString>Samhaa R. El-Beltagy and Ahmed A. Rafea. 2009. KP-Miner: A keyphrase extraction system for English and Arabic documents. Information Systems, 34(1):132–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samhaa R El-Beltagy</author>
<author>Ahmed Rafea</author>
</authors>
<title>KPMiner: Participation in SemEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>190--193</pages>
<contexts>
<context position="28463" citStr="El-Beltagy and Rafea, 2010" startWordPosition="4535" endWordPosition="4538">orresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.3 Two points deserve mention. First, F-scores decrease as document length increases. These results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents. Second, recent unsupervised approaches have rivaled their supervised counterparts in performance (Mihalcea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors </context>
</contexts>
<marker>El-Beltagy, Rafea, 2010</marker>
<rawString>Samhaa R. El-Beltagy and Ahmed Rafea. 2010. KPMiner: Participation in SemEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 190–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of 16th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>668--673</pages>
<contexts>
<context position="8464" citStr="Frank et al., 1999" startWordPosition="1307" endWordPosition="1310">rces. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez an</context>
<context position="10062" citStr="Frank et al., 1999" startWordPosition="1555" endWordPosition="1558">e c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011). Motivated by this observation, Jiang et al. (2009) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA (Witten et al., 1999; Frank et al., 1999), a popular supervised baseline that adopts the traditional supervised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an instance for supervised keyphrase extraction can be broadly divided into two categories. 3.2.2.1 Within-Collection Features Within-collection features are computed based solely on the training documents. These features can be further divided into three types. Statistical features are computed based on statistical information gathered from the training documents. Three such features have been extensi</context>
<context position="11511" citStr="Frank et al., 1999" startWordPosition="1788" endWordPosition="1791">ppears).2 The second one, the distance of a phrase, is defined as the number of words preceding its first occurrence normalized by the number of words in the document. Its usefulness stems from the fact that keyphrases tend to appear early in a document. The third one, supervised keyphraseness, encodes the number of times a phrase appears as a keyphrase in the training set. This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. These three features form the feature set of KEA (Witten et al., 1999; Frank et al., 1999), and have been shown to perform consistently well on documents from various sources (Yih et al., 2006; Kim et al., 2013). Other statistical features include phrase length and spread (i.e., the number of words between the first and last occurrences of a phrase in the document). Structural features encode how different instances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. In fact, features that encode how frequently a candidate k</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings of 16th International Joint Conference on Artificial Intelligence, pages 668–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Grineva</author>
<author>Maxim Grinev</author>
<author>Dmitry Lizorkin</author>
</authors>
<title>Extracting key terms from noisy and multitheme documents.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International Conference on World Wide Web,</booktitle>
<pages>661--670</pages>
<contexts>
<context position="4706" citStr="Grineva et al., 2009" startWordPosition="716" endWordPosition="719">Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the beginning (Witten et al., 1999) but also at the end (Medelyan et al., 2009) of a document. This observation doe</context>
<context position="7502" citStr="Grineva et al., 2009" startWordPosition="1159" endWordPosition="1162">ting Candidate Words and Phrases As noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltag</context>
<context position="13144" citStr="Grineva et al., 2009" startWordPosition="2065" endWordPosition="2068">phrase has been encoded as (1) a PoS tag sequence, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence, which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studies conducted on web pages (Yih et al., 2006) and scientific articles 2A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches (Zhang et al., 2005; Dredze et al., 2008; Paukkeri et al., 2008; Grineva et al., 2009). 1264 (Kim and Kan, 2009) reveal that syntactic features are not useful for keyphrase extraction in the presence of other feature types. 3.2.2.2 External Resource-Based Features External resource-based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g., Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge. Below we give an overview of the external resource-based features that have proven useful for keyphrase extraction. Wikipedia-based keyphra</context>
<context position="15999" citStr="Grineva et al., 2009" startWordPosition="2504" endWordPosition="2507">traction can be categorized into four groups. 3.3.1 Graph-Based Ranking Intuitively, keyphrase extraction is about finding the important words and phrases from a document. Traditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected c</context>
<context position="17806" citStr="Grineva et al., 2009" startWordPosition="2799" endWordPosition="2802">ould ideally cover the main topics discussed in it, but this instantiation does not guarantee that all the main topics will be represented by the extracted keyphrases. Despite this weakness, a graph-based representation of text was adopted by many approaches that propose different ways of computing the similarity between two candidates. 3.3.2 Topic-Based Clustering Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics, such that each topic is composed of all and only those candidate keyphrases that are related to that topic (Grineva et al., 2009; Liu et al., 2009b; Liu et 1265 al., 2010). There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document (Liu et al., 2010; Liu et al., 2012). Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document (Liu et al., 2009b; Liu et al., 2010; Liu et al., 2012). Below we examine three representative systems that adopt this approach. KeyCluster Liu et al. (2009b) adopt a clustering-based approach (henceforth KeyCluster)</context>
<context position="20153" citStr="Grineva et al. (2009)" startWordPosition="3168" endWordPosition="3171">the document. The final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document. Hence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance. TPR performs significantly better than both tf*idf and TextRank on the DUC-2001 and Inspec datasets. TPR’s superior performance strengthens the hypothesis of using topic clustering for keyphrase extraction. However, though TPR is conceptually better than KeyCluster, Liu et al. did not compare TPR against KeyCluster. CommunityCluster Grineva et al. (2009) propose CommunityCluster, a variant of the topic clustering approach to keyphrase extraction. Like TPR, CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic. CommunityCluster yields much better recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo! term extractor. 3.3.3 Simultaneous Learning Since keyphrases represent a de</context>
<context position="29830" citStr="Grineva et al., 2009" startWordPosition="4753" endWordPosition="4756"> datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-theart keyphrase extractors, of which two are unsupervised (Wan and Xiao, 2008b; Liu et al., 2009b) and one is supervised (Medelyan et al., 2009). Our analysis reveals that </context>
<context position="38748" citStr="Grineva et al., 2009" startWordPosition="6221" endWordPosition="6224">lished using co-occurrence information. However, using cooccurrence windows has its shortcomings. First, an ad-hoc window size cannot capture related candidates that are not inside the window. So it is difficult to predict 100-meter dash and gold medal as keyphrases: they are more than 10 tokens away from frequent words such as Johnson and Olympics. Second, the candidates inside a window are all assumed to be related to each other, but it is apparently an overly simplistic assumption. There have been a few attempts to design Wikipediabased relatedness measures, with promising initial results (Grineva et al., 2009; Liu et al., 2009b; Medelyan et al., 2009).4 Overgeneration errors could similarly be addressed using background knowledge. Recall that Olympic movement is not a keyphrase in our example although it includes an important word (i.e., Olympic). Freebase maps Olympic movement to a topic with the same name, which is associated with a type called Musical Recording in the Music domain. However, it does not map Olympic 4Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be rela</context>
</contexts>
<marker>Grineva, Grinev, Lizorkin, 2009</marker>
<rawString>Maria Grineva, Maxim Grinev, and Dmitry Lizorkin. 2009. Extracting key terms from noisy and multitheme documents. In Proceedings of the 18th International Conference on World Wide Web, pages 661–670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Gutwin</author>
<author>Gordon Paynter</author>
<author>Ian Witten</author>
<author>Craig NevillManning</author>
<author>Eibe Frank</author>
</authors>
<title>Improving browsing in digital libraries with keyphrase indexes. Decision Support Systems,</title>
<date>1999</date>
<pages>27--81</pages>
<contexts>
<context position="1378" citStr="Gutwin et al., 1999" startWordPosition="211" endWordPosition="214">000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora from a variety of sources ranging from long scientific publications to short </context>
</contexts>
<marker>Gutwin, Paynter, Witten, NevillManning, Frank, 1999</marker>
<rawString>Carl Gutwin, Gordon Paynter, Ian Witten, Craig NevillManning, and Eibe Frank. 1999. Improving browsing in digital libraries with keyphrase indexes. Decision Support Systems, 27:81–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled M Hammouda</author>
<author>Diego N Matute</author>
<author>Mohamed S Kamel</author>
</authors>
<title>CorePhrase: Keyphrase extraction for document clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th International Conference on Machine Learning and Data Mining in Pattern Recognition,</booktitle>
<pages>265--274</pages>
<contexts>
<context position="4665" citStr="Hammouda et al. (2005)" startWordPosition="708" endWordPosition="711"> Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the beginning (Witten et al., 1999) but also at the end (Medelyan et al., </context>
</contexts>
<marker>Hammouda, Matute, Kamel, 2005</marker>
<rawString>Khaled M. Hammouda, Diego N. Matute, and Mohamed S. Kamel. 2005. CorePhrase: Keyphrase extraction for document clustering. In Proceedings of the 4th International Conference on Machine Learning and Data Mining in Pattern Recognition, pages 265–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Conundrums in unsupervised keyphrase extraction: Making sense of the state-of-the-art.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>365--373</pages>
<contexts>
<context position="2689" citStr="Hasan and Ng, 2010" startWordPosition="419" endWordPosition="422">ir sources as well as their statistics.1 There are at least four corpus-related factors that affect the difficulty of keyphrase extraction. Length The difficulty of the task increases with the length of the input document as longer documents yield more candidate keyphrases (i.e., phrases that are eligible to be keyphrases (see Section 3.1)). For instance, each Inspec abstract has on average 10 annotator-assigned keyphrases and 34 candidate keyphrases. In contrast, a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases, yielding a much bigger search space (Hasan and Ng, 2010). Consequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles. Structural consistency In a structured document, there are certain locations where a keyphrase is most likely to appear. For instance, most of a scientific paper’s keyphrases should appear in the abstract and the introduction. While structural information has been exploited to extract keyphrases from scientific papers (e.g., title, section information) (Kim et al., 2013), web pages (e.g., metadata) (Yih et al., 2006), and chats (e.g., </context>
</contexts>
<marker>Hasan, Ng, 2010</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2010. Conundrums in unsupervised keyphrase extraction: Making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 365–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Huang</author>
<author>Yonghong Tian</author>
<author>Zhi Zhou</author>
<author>Charles X Ling</author>
<author>Tiejun Huang</author>
</authors>
<title>Keyphrase extraction using semantic networks structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th International Conference on Data Mining,</booktitle>
<pages>275--284</pages>
<contexts>
<context position="8064" citStr="Huang et al., 2006" startWordPosition="1246" endWordPosition="1249">a article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-</context>
</contexts>
<marker>Huang, Tian, Zhou, Ling, Huang, 2006</marker>
<rawString>Chong Huang, Yonghong Tian, Zhi Zhou, Charles X. Ling, and Tiejun Huang. 2006. Keyphrase extraction using semantic networks structure analysis. In Proceedings of the 6th International Conference on Data Mining, pages 275–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
<author>Be´ata B Megyesi</author>
</authors>
<title>A study on automatically extracted keywords in text categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>537--544</pages>
<contexts>
<context position="1302" citStr="Hulth and Megyesi, 2006" startWordPosition="198" endWordPosition="201">lection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora f</context>
<context position="4595" citStr="Hulth and Megyesi, 2006" startWordPosition="692" endWordPosition="695">ceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the </context>
</contexts>
<marker>Hulth, Megyesi, 2006</marker>
<rawString>Anette Hulth and Be´ata B. Megyesi. 2006. A study on automatically extracted keywords in text categorization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
<author>Jussi Karlgren</author>
<author>Anna Jonsson</author>
<author>Henrik Bostr¨om</author>
<author>Lars Asker</author>
</authors>
<title>Automatic keyword extraction using domain knowledge.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>472--482</pages>
<marker>Hulth, Karlgren, Jonsson, Bostr¨om, Asker, 2001</marker>
<rawString>Anette Hulth, Jussi Karlgren, Anna Jonsson, Henrik Bostr¨om, and Lars Asker. 2001. Automatic keyword extraction using domain knowledge. In Proceedings of the 2nd International Conference on Computational Linguistics and Intelligent Text Processing, pages 472–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="4270" citStr="Hulth, 2003" startWordPosition="638" endWordPosition="639">ast, the lack of structural consistency in other types of structured documents (e.g., web pages, which can be blogs, forums, or reviews) 1Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Ba</context>
<context position="7564" citStr="Hulth, 2003" startWordPosition="1171" endWordPosition="1172">ords is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2</context>
<context position="8944" citStr="Hulth, 2003" startWordPosition="1381" endWordPosition="1382">mulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase</context>
<context position="29279" citStr="Hulth, 2003" startWordPosition="4663" endWordPosition="4664">ions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (D</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216– 223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Enhancing linguistically oriented automatic keyword extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: Short Papers,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="9681" citStr="Hulth, 2004" startWordPosition="1498" endWordPosition="1499">ry, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011). Motivated by this observation, Jiang et al. (2009) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA (Witten et al., 1999; Frank et al., 1999), a popular supervised baseline that adopts the traditional supervised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an instance for supervis</context>
</contexts>
<marker>Hulth, 2004</marker>
<rawString>Anette Hulth. 2004. Enhancing linguistically oriented automatic keyword extraction. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: Short Papers, pages 17– 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Jiang</author>
<author>Yunhua Hu</author>
<author>Hang Li</author>
</authors>
<title>A ranking approach to keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>756--757</pages>
<contexts>
<context position="9128" citStr="Jiang et al., 2009" startWordPosition="1408" endWordPosition="1411">000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011). Motivated by this observa</context>
</contexts>
<marker>Jiang, Hu, Li, 2009</marker>
<rawString>Xin Jiang, Yunhua Hu, and Hang Li. 2009. A ranking approach to keyphrase extraction. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 756–757.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Kelleher</author>
<author>Saturnino Luz</author>
</authors>
<title>Automatic hypertext keyphrase detection.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1608--1609</pages>
<contexts>
<context position="10200" citStr="Kelleher and Luz, 2005" startWordPosition="1576" endWordPosition="1579"> consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011). Motivated by this observation, Jiang et al. (2009) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA (Witten et al., 1999; Frank et al., 1999), a popular supervised baseline that adopts the traditional supervised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an instance for supervised keyphrase extraction can be broadly divided into two categories. 3.2.2.1 Within-Collection Features Within-collection features are computed based solely on the training documents. These features can be further divided into three types. Statistical features are computed based on statistical information gathered from the training documents. Three such features have been extensively used in supervised approaches. The first one, tf*idf (Salton and Buckley, 1988), is computed based on candidate frequency in the give</context>
</contexts>
<marker>Kelleher, Luz, 2005</marker>
<rawString>Daniel Kelleher and Saturnino Luz. 2005. Automatic hypertext keyphrase detection. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, pages 1608–1609.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Information guage</author>
<author>Computation</author>
</authors>
<pages>199--208</pages>
<marker>guage, Computation, </marker>
<rawString>guage, Information, and Computation, pages 199– 208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Min-Yen Kan</author>
</authors>
<title>Re-examining automatic keyphrase extraction approaches in scientific articles.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP Workshop on Multiword Expressions,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="9030" citStr="Kim and Kan, 2009" startWordPosition="1394" endWordPosition="1397"> a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which</context>
<context position="12798" citStr="Kim and Kan, 2009" startWordPosition="2012" endWordPosition="2015">oduction, conclusion) (Nguyen and Kan, 2007) and those that encode the location of a candidate keyphrase in a web page (e.g., whether it appears in the title) (Chen et al., 2005; Yih et al., 2006) have been shown to be useful for the task. Syntactic features encode the syntactic patterns of a candidate keyphrase. For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence, which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studies conducted on web pages (Yih et al., 2006) and scientific articles 2A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches (Zhang et al., 2005; Dredze et al., 2008; Paukkeri et al., 2008; Grineva et al., 2009). 1264 (Kim and Kan, 2009) reveal that syntactic features are not useful for keyphrase extraction in the presence of other feature types. 3.2.2.2 External Resource-Based Features External resource-based features are computed based on information gathered</context>
</contexts>
<marker>Kim, Kan, 2009</marker>
<rawString>Su Nam Kim and Min-Yen Kan. 2009. Re-examining automatic keyphrase extraction approaches in scientific articles. In Proceedings of the ACL-IJCNLP Workshop on Multiword Expressions, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
<author>Min-Yen Kan</author>
</authors>
<title>Evaluating n-gram based evaluation metrics for automatic keyphrase extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>572--580</pages>
<contexts>
<context position="4427" citStr="Kim et al., 2010" startWordPosition="663" endWordPosition="666">licly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful</context>
<context position="27014" citStr="Kim et al., 2010" startWordPosition="4289" endWordPosition="4292"> it is timeconsuming and expensive. For this reason, researchers have experimented with two types of automatic evaluation metrics. The first type of metrics addresses the problem with exact match. These metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e., overlapping n-grams) and are commonly used in machine translation (MT) and summarization evaluations. They include BLEU, METEOR, NIST, and ROUGE. Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses (Kim et al., 2010a). The second type of metrics focuses on how a system ranks its predictions. Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) (Liu et al., 2010) will award more credit to A than to B if the ranks of the correct predictions in A’s output are higher than those in B’s output. R-precision (Rp) is an 1267 IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates (Zesch and Gurevych, 2009). The motivation behind the </context>
<context position="29353" citStr="Kim et al., 2010" startWordPosition="4673" endWordPosition="4676">-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Stati</context>
</contexts>
<marker>Kim, Baldwin, Kan, 2010</marker>
<rawString>Su Nam Kim, Timothy Baldwin, and Min-Yen Kan. 2010a. Evaluating n-gram based evaluation metrics for automatic keyphrase extraction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 572–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>SemEval-2010 Task 5: Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>21--26</pages>
<contexts>
<context position="4427" citStr="Kim et al., 2010" startWordPosition="663" endWordPosition="666">licly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful</context>
<context position="27014" citStr="Kim et al., 2010" startWordPosition="4289" endWordPosition="4292"> it is timeconsuming and expensive. For this reason, researchers have experimented with two types of automatic evaluation metrics. The first type of metrics addresses the problem with exact match. These metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e., overlapping n-grams) and are commonly used in machine translation (MT) and summarization evaluations. They include BLEU, METEOR, NIST, and ROUGE. Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses (Kim et al., 2010a). The second type of metrics focuses on how a system ranks its predictions. Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) (Liu et al., 2010) will award more credit to A than to B if the ranks of the correct predictions in A’s output are higher than those in B’s output. R-precision (Rp) is an 1267 IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates (Zesch and Gurevych, 2009). The motivation behind the </context>
<context position="29353" citStr="Kim et al., 2010" startWordPosition="4673" endWordPosition="4676">-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Stati</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010b. SemEval-2010 Task 5: Automatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic keyphrase extraction from scientific articles. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>47--3</pages>
<contexts>
<context position="3223" citStr="Kim et al., 2013" startWordPosition="500" endWordPosition="503">s of candidate keyphrases, yielding a much bigger search space (Hasan and Ng, 2010). Consequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles. Structural consistency In a structured document, there are certain locations where a keyphrase is most likely to appear. For instance, most of a scientific paper’s keyphrases should appear in the abstract and the introduction. While structural information has been exploited to extract keyphrases from scientific papers (e.g., title, section information) (Kim et al., 2013), web pages (e.g., metadata) (Yih et al., 2006), and chats (e.g., dialogue acts) (Kim and Baldwin, 2012), it is most useful when the documents from a source exhibit structural similarity. For this reason, structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports because of their standard format (i.e., standard sections such as abstract, introduction, conclusion, etc.). In contrast, the lack of structural consistency in other types of structured documents (e.g., web pages, which can be blogs, forums, or reviews) 1Many of the publicly availa</context>
<context position="11632" citStr="Kim et al., 2013" startWordPosition="1810" endWordPosition="1813">ized by the number of words in the document. Its usefulness stems from the fact that keyphrases tend to appear early in a document. The third one, supervised keyphraseness, encodes the number of times a phrase appears as a keyphrase in the training set. This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. These three features form the feature set of KEA (Witten et al., 1999; Frank et al., 1999), and have been shown to perform consistently well on documents from various sources (Yih et al., 2006; Kim et al., 2013). Other statistical features include phrase length and spread (i.e., the number of words between the first and last occurrences of a phrase in the document). Structural features encode how different instances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. In fact, features that encode how frequently a candidate keyphrase occurs in various sections of a scientific paper (e.g., introduction, conclusion) (Nguyen and Kan, 2007) and tho</context>
<context position="29630" citStr="Kim et al. (2013)" startWordPosition="4721" endWordPosition="4724"> systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline,</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2013</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2013. Automatic keyphrase extraction from scientific articles. Language Resources and Evaluation, 47(3):723–742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niraj Kumar</author>
<author>Kannan Srinathan</author>
</authors>
<title>Automatic keyphrase extraction from scientific documents using n-gram filtration technique.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th ACM Symposium on Document Engineering,</booktitle>
<pages>199--208</pages>
<contexts>
<context position="8091" citStr="Kumar and Srinathan, 2008" startWordPosition="1250" endWordPosition="1253">be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to gene</context>
</contexts>
<marker>Kumar, Srinathan, 2008</marker>
<rawString>Niraj Kumar and Kannan Srinathan. 2008. Automatic keyphrase extraction from scientific documents using n-gram filtration technique. In Proceedings of the 8th ACM Symposium on Document Engineering, pages 199–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Deana Pennell</author>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Unsupervised approaches for automatic keyword extraction using meeting transcripts.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>620--628</pages>
<contexts>
<context position="929" citStr="Liu et al., 2009" startWordPosition="141" endWordPosition="144">-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art</context>
<context position="4759" citStr="Liu et al., 2009" startWordPosition="726" endWordPosition="729">putational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the beginning (Witten et al., 1999) but also at the end (Medelyan et al., 2009) of a document. This observation does not necessarily hold for conversational text (e.g.,</context>
<context position="7221" citStr="Liu et al., 2009" startWordPosition="1114" endWordPosition="1117">ps: (1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3.1); and (2) determining which of these candidate keyphrases are correct keyphrases using supervised (Section 3.2) or unsupervised (Section 3.3) approaches. 3.1 Selecting Candidate Words and Phrases As noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold ke</context>
<context position="17824" citStr="Liu et al., 2009" startWordPosition="2803" endWordPosition="2806"> main topics discussed in it, but this instantiation does not guarantee that all the main topics will be represented by the extracted keyphrases. Despite this weakness, a graph-based representation of text was adopted by many approaches that propose different ways of computing the similarity between two candidates. 3.3.2 Topic-Based Clustering Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics, such that each topic is composed of all and only those candidate keyphrases that are related to that topic (Grineva et al., 2009; Liu et al., 2009b; Liu et 1265 al., 2010). There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document (Liu et al., 2010; Liu et al., 2012). Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document (Liu et al., 2009b; Liu et al., 2010; Liu et al., 2012). Below we examine three representative systems that adopt this approach. KeyCluster Liu et al. (2009b) adopt a clustering-based approach (henceforth KeyCluster) that clusters sem</context>
<context position="28409" citStr="Liu et al., 2009" startWordPosition="4528" endWordPosition="4531"> some popular evaluation datasets and the corresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.3 Two points deserve mention. First, F-scores decrease as document length increases. These results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents. Second, recent unsupervised approaches have rivaled their supervised counterparts in performance (Mihalcea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009</context>
<context position="29754" citStr="Liu et al., 2009" startWordPosition="4741" endWordPosition="4744">his gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-theart keyphrase extractors, of which two are unsupervised (Wan and Xiao, 2008b; Liu et al., 2009</context>
<context position="38766" citStr="Liu et al., 2009" startWordPosition="6225" endWordPosition="6228">ence information. However, using cooccurrence windows has its shortcomings. First, an ad-hoc window size cannot capture related candidates that are not inside the window. So it is difficult to predict 100-meter dash and gold medal as keyphrases: they are more than 10 tokens away from frequent words such as Johnson and Olympics. Second, the candidates inside a window are all assumed to be related to each other, but it is apparently an overly simplistic assumption. There have been a few attempts to design Wikipediabased relatedness measures, with promising initial results (Grineva et al., 2009; Liu et al., 2009b; Medelyan et al., 2009).4 Overgeneration errors could similarly be addressed using background knowledge. Recall that Olympic movement is not a keyphrase in our example although it includes an important word (i.e., Olympic). Freebase maps Olympic movement to a topic with the same name, which is associated with a type called Musical Recording in the Music domain. However, it does not map Olympic 4Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other </context>
</contexts>
<marker>Liu, Pennell, Liu, Liu, 2009</marker>
<rawString>Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009a. Unsupervised approaches for automatic keyword extraction using meeting transcripts. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 620–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Peng Li</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Clustering to find exemplar terms for keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>257--266</pages>
<contexts>
<context position="929" citStr="Liu et al., 2009" startWordPosition="141" endWordPosition="144">-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art</context>
<context position="4759" citStr="Liu et al., 2009" startWordPosition="726" endWordPosition="729">putational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the beginning (Witten et al., 1999) but also at the end (Medelyan et al., 2009) of a document. This observation does not necessarily hold for conversational text (e.g.,</context>
<context position="7221" citStr="Liu et al., 2009" startWordPosition="1114" endWordPosition="1117">ps: (1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3.1); and (2) determining which of these candidate keyphrases are correct keyphrases using supervised (Section 3.2) or unsupervised (Section 3.3) approaches. 3.1 Selecting Candidate Words and Phrases As noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold ke</context>
<context position="17824" citStr="Liu et al., 2009" startWordPosition="2803" endWordPosition="2806"> main topics discussed in it, but this instantiation does not guarantee that all the main topics will be represented by the extracted keyphrases. Despite this weakness, a graph-based representation of text was adopted by many approaches that propose different ways of computing the similarity between two candidates. 3.3.2 Topic-Based Clustering Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics, such that each topic is composed of all and only those candidate keyphrases that are related to that topic (Grineva et al., 2009; Liu et al., 2009b; Liu et 1265 al., 2010). There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document (Liu et al., 2010; Liu et al., 2012). Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document (Liu et al., 2009b; Liu et al., 2010; Liu et al., 2012). Below we examine three representative systems that adopt this approach. KeyCluster Liu et al. (2009b) adopt a clustering-based approach (henceforth KeyCluster) that clusters sem</context>
<context position="28409" citStr="Liu et al., 2009" startWordPosition="4528" endWordPosition="4531"> some popular evaluation datasets and the corresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.3 Two points deserve mention. First, F-scores decrease as document length increases. These results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents. Second, recent unsupervised approaches have rivaled their supervised counterparts in performance (Mihalcea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009</context>
<context position="29754" citStr="Liu et al., 2009" startWordPosition="4741" endWordPosition="4744">his gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-theart keyphrase extractors, of which two are unsupervised (Wan and Xiao, 2008b; Liu et al., 2009</context>
<context position="38766" citStr="Liu et al., 2009" startWordPosition="6225" endWordPosition="6228">ence information. However, using cooccurrence windows has its shortcomings. First, an ad-hoc window size cannot capture related candidates that are not inside the window. So it is difficult to predict 100-meter dash and gold medal as keyphrases: they are more than 10 tokens away from frequent words such as Johnson and Olympics. Second, the candidates inside a window are all assumed to be related to each other, but it is apparently an overly simplistic assumption. There have been a few attempts to design Wikipediabased relatedness measures, with promising initial results (Grineva et al., 2009; Liu et al., 2009b; Medelyan et al., 2009).4 Overgeneration errors could similarly be addressed using background knowledge. Recall that Olympic movement is not a keyphrase in our example although it includes an important word (i.e., Olympic). Freebase maps Olympic movement to a topic with the same name, which is associated with a type called Musical Recording in the Music domain. However, it does not map Olympic 4Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other </context>
</contexts>
<marker>Liu, Li, Zheng, Sun, 2009</marker>
<rawString>Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Wenyi Huang</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic keyphrase extraction via topic decomposition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>366--376</pages>
<contexts>
<context position="1637" citStr="Liu et al., 2010" startWordPosition="252" endWordPosition="255">e searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extraction systems have been evaluated on corpora from a variety of sources ranging from long scientific publications to short paper abstracts and email messages. Table 1 presents a listing of the corpora grouped by their sources as well as their statistics.1 There are at least four corpus-related factors that affect the difficulty of keyphrase extraction. Length The difficulty of th</context>
<context position="18042" citStr="Liu et al., 2010" startWordPosition="2838" endWordPosition="2841">by many approaches that propose different ways of computing the similarity between two candidates. 3.3.2 Topic-Based Clustering Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics, such that each topic is composed of all and only those candidate keyphrases that are related to that topic (Grineva et al., 2009; Liu et al., 2009b; Liu et 1265 al., 2010). There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document (Liu et al., 2010; Liu et al., 2012). Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document (Liu et al., 2009b; Liu et al., 2010; Liu et al., 2012). Below we examine three representative systems that adopt this approach. KeyCluster Liu et al. (2009b) adopt a clustering-based approach (henceforth KeyCluster) that clusters semantically similar candidates using Wikipedia and co-occurrence-based statistics. The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates clo</context>
<context position="27251" citStr="Liu et al., 2010" startWordPosition="4330" endWordPosition="4333">tween a predicted keyphrase and a gold keyphrase (i.e., overlapping n-grams) and are commonly used in machine translation (MT) and summarization evaluations. They include BLEU, METEOR, NIST, and ROUGE. Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses (Kim et al., 2010a). The second type of metrics focuses on how a system ranks its predictions. Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) (Liu et al., 2010) will award more credit to A than to B if the ranks of the correct predictions in A’s output are higher than those in B’s output. R-precision (Rp) is an 1267 IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates (Zesch and Gurevych, 2009). The motivation behind the design of Rp is simple: a system will achieve a perfect Rp value if it ranks all the keyphrases above the non-keyphrases. 4.2 The State of the Art Table 2 lists the best scores on some popular evaluation datasets and the corresponding sy</context>
</contexts>
<marker>Liu, Huang, Zheng, Sun, 2010</marker>
<rawString>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. Automatic keyphrase extraction via topic decomposition. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 366–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Xinxiong Chen</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic keyphrase extraction by bridging vocabulary gap.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Conference on Computational Natural Language Learning,</booktitle>
<pages>135--144</pages>
<contexts>
<context position="32996" citStr="Liu et al., 2011" startWordPosition="5282" endWordPosition="5285"> confirmation of what they said they know has been going on in track and field. Two tests of Johnson’s urine sample proved positive and his denials of drug use were rejected today. “This is a blow for the Olympic Games and the Olympic movement,” said International Olympic Committee President Juan Antonio Samaranch. Figure 1: A news article on Ben Johnson from the DUC-2001 dataset. The keyphrases are boldfaced. call error contributing to 24–27% of the overall error. Infrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document (Liu et al., 2011). Handling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document. In the Ben Johnson example, many keyphrase extractors fail to identify 100-meter dash and gold medal as keyphrases, resulting in infrequency errors. Redundancy errors are a type of precision error contributing to 8–12% of the overall error. Redundancy errors occur when a system correctly identifies a candidate as a keyphrase, but at the same time outputs a semantically equivalent candidate (e.g., its alias) as a keyphrase. This type </context>
</contexts>
<marker>Liu, Chen, Zheng, Sun, 2011</marker>
<rawString>Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and Maosong Sun. 2011. Automatic keyphrase extraction by bridging vocabulary gap. In Proceedings of the 15th Conference on Computational Natural Language Learning, pages 135–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Chen Liang</author>
<author>Maosong Sun</author>
</authors>
<title>Topical word trigger model for keyphrase extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>1715--1730</pages>
<contexts>
<context position="18061" citStr="Liu et al., 2012" startWordPosition="2842" endWordPosition="2845"> that propose different ways of computing the similarity between two candidates. 3.3.2 Topic-Based Clustering Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics, such that each topic is composed of all and only those candidate keyphrases that are related to that topic (Grineva et al., 2009; Liu et al., 2009b; Liu et 1265 al., 2010). There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document (Liu et al., 2010; Liu et al., 2012). Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document (Liu et al., 2009b; Liu et al., 2010; Liu et al., 2012). Below we examine three representative systems that adopt this approach. KeyCluster Liu et al. (2009b) adopt a clustering-based approach (henceforth KeyCluster) that clusters semantically similar candidates using Wikipedia and co-occurrence-based statistics. The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates close to the centroid </context>
</contexts>
<marker>Liu, Liang, Sun, 2012</marker>
<rawString>Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012. Topical word trigger model for keyphrase extraction. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1715–1730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrice Lopez</author>
<author>Laurent Romary</author>
</authors>
<title>HUMB: Automatic key term extraction from scientific articles in GROBID.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>248--251</pages>
<contexts>
<context position="9079" citStr="Lopez and Romary, 2010" startWordPosition="1400" endWordPosition="1403">l., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 20</context>
<context position="14695" citStr="Lopez and Romary, 2010" startWordPosition="2309" endWordPosition="2312">uently as a link in Wikipedia. Unlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain. Yih et al. (2006) employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query. Terminological databases have been similarly exploited to encode the salience of candidate keyphrases in scientific papers (Lopez and Romary, 2010). While the aforementioned external resourcebased features attempt to encode how salient a candidate keyphrase is, Turney (2003) proposes features that encode the semantic relatedness between two candidate keyphrases. Noting that candidate keyphrases that are not semantically related to the predicted keyphrases are unlikely to be keyphrases in technical reports, Turney employs coherence features to identify such candidate keyphrases. Semantic relatedness is encoded in the coherence features as two candidate keyphrases’ pointwise mutual information, which Turney computes by using the Web as a c</context>
<context position="30054" citStr="Lopez and Romary, 2010" startWordPosition="4787" endWordPosition="4790">ected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-theart keyphrase extractors, of which two are unsupervised (Wan and Xiao, 2008b; Liu et al., 2009b) and one is supervised (Medelyan et al., 2009). Our analysis reveals that the errors fall into four major types, each of which contributes significantly to the overall errors made by the four systems, despite the fact that the contribution of each of these error types varies from system to system.</context>
</contexts>
<marker>Lopez, Romary, 2010</marker>
<rawString>Patrice Lopez and Laurent Romary. 2010. HUMB: Automatic key term extraction from scientific articles in GROBID. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 248–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Keyword extraction from a single document using word co-occurrence statistical information.</title>
<date>2004</date>
<journal>International Journal on Artificial Intelligence Tools,</journal>
<volume>13</volume>
<contexts>
<context position="15951" citStr="Matsuo and Ishizuka, 2004" startWordPosition="2497" endWordPosition="2500">ches Existing unsupervised approaches to keyphrase extraction can be categorized into four groups. 3.3.1 Graph-Based Ranking Intuitively, keyphrase extraction is about finding the important words and phrases from a document. Traditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic a</context>
<context position="26393" citStr="Matsuo and Ishizuka, 2004" startWordPosition="4192" endWordPosition="4195">n overly strict condition, considering a predicted keyphrase incorrect even if it is a variant of a gold keyphrase. For instance, given the gold keyphrase “neural network”, exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase (“artificial neural network”) or one of its morphological (“neural networks”) or lexical (“neural net”) variants. While morphological variations can be handled using a stemmer (ElBeltagy and Rafea, 2009), other variations may not be handled easily and reliably. Human evaluation has been suggested as a possibility (Matsuo and Ishizuka, 2004), but it is timeconsuming and expensive. For this reason, researchers have experimented with two types of automatic evaluation metrics. The first type of metrics addresses the problem with exact match. These metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e., overlapping n-grams) and are commonly used in machine translation (MT) and summarization evaluations. They include BLEU, METEOR, NIST, and ROUGE. Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-mis</context>
</contexts>
<marker>Matsuo, Ishizuka, 2004</marker>
<rawString>Yutaka Matsuo and Mitsuru Ishizuka. 2004. Keyword extraction from a single document using word co-occurrence statistical information. International Journal on Artificial Intelligence Tools, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>Eibe Frank</author>
<author>Ian H Witten</author>
</authors>
<title>Human-competitive tagging using automatic keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1318--1327</pages>
<contexts>
<context position="4387" citStr="Medelyan et al., 2009" startWordPosition="655" endWordPosition="658">be blogs, forums, or reviews) 1Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may r</context>
<context position="7588" citStr="Medelyan et al., 2009" startWordPosition="1173" endWordPosition="1176">ally extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches R</context>
<context position="13963" citStr="Medelyan et al., 2009" startWordPosition="2190" endWordPosition="2193">based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g., Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge. Below we give an overview of the external resource-based features that have proven useful for keyphrase extraction. Wikipedia-based keyphraseness is computed as a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears (Medelyan et al., 2009). This feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently as a link in Wikipedia. Unlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain. Yih et al. (2006) employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query. Terminological </context>
<context position="29010" citStr="Medelyan et al., 2009" startWordPosition="4621" endWordPosition="4624">009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found </context>
<context position="30402" citStr="Medelyan et al., 2009" startWordPosition="4845" endWordPosition="4848">7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-theart keyphrase extractors, of which two are unsupervised (Wan and Xiao, 2008b; Liu et al., 2009b) and one is supervised (Medelyan et al., 2009). Our analysis reveals that the errors fall into four major types, each of which contributes significantly to the overall errors made by the four systems, despite the fact that the contribution of each of these error types varies from system to system. Moreover, we do not observe any significant difference between the types of errors made by the four systems other than the fact that the supervised system has the expected tendency to predict keyphrases seen in the training data. Below we describe these four major types of errors. Overgeneration errors are a major type of precision error, contri</context>
<context position="38791" citStr="Medelyan et al., 2009" startWordPosition="6229" endWordPosition="6232">owever, using cooccurrence windows has its shortcomings. First, an ad-hoc window size cannot capture related candidates that are not inside the window. So it is difficult to predict 100-meter dash and gold medal as keyphrases: they are more than 10 tokens away from frequent words such as Johnson and Olympics. Second, the candidates inside a window are all assumed to be related to each other, but it is apparently an overly simplistic assumption. There have been a few attempts to design Wikipediabased relatedness measures, with promising initial results (Grineva et al., 2009; Liu et al., 2009b; Medelyan et al., 2009).4 Overgeneration errors could similarly be addressed using background knowledge. Recall that Olympic movement is not a keyphrase in our example although it includes an important word (i.e., Olympic). Freebase maps Olympic movement to a topic with the same name, which is associated with a type called Musical Recording in the Music domain. However, it does not map Olympic 4Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other (see Section 2). movement</context>
</contexts>
<marker>Medelyan, Frank, Witten, 2009</marker>
<rawString>Olena Medelyan, Eibe Frank, and Ian H. Witten. 2009. Human-competitive tagging using automatic keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318–1327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="6142" citStr="Mihalcea and Tarau, 2004" startWordPosition="951" endWordPosition="954">d so do the keyphrases associated with a topic. One way to address this complication is to detect a topic change in conversational text (Kim and Baldwin, 2012). However, topic change detection is not always easy: while the topics listed in the form of an agenda at the beginning of formal meeting transcripts can be exploited, such clues are absent in casual conversations (e.g., chats). Topic correlation Another observation commonly exploited in keyphrase extraction from scientific articles and news articles is that the keyphrases in a document are typically related to each other (Turney, 2003; Mihalcea and Tarau, 2004). However, this observation does not necessarily hold for informal text (e.g., emails, chats, informal meetings, personal blogs), where people can talk about any number of potentially uncorrelated topics. The presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness and therefore increases the difficulty of keyphrase extraction. 3 Keyphrase Extraction Approaches A keyphrase extraction system typically operates in two steps: (1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3.1); and (2) determining wh</context>
<context position="15923" citStr="Mihalcea and Tarau, 2004" startWordPosition="2493" endWordPosition="2496">s. 3.3 Unsupervised Approaches Existing unsupervised approaches to keyphrase extraction can be categorized into four groups. 3.3.1 Graph-Based Ranking Intuitively, keyphrase extraction is about finding the important words and phrases from a document. Traditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is pro</context>
<context position="28363" citStr="Mihalcea and Tarau, 2004" startWordPosition="4519" endWordPosition="4523"> The State of the Art Table 2 lists the best scores on some popular evaluation datasets and the corresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.3 Two points deserve mention. First, F-scores decrease as document length increases. These results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents. Second, recent unsupervised approaches have rivaled their supervised counterparts in performance (Mihalcea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 199</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="35042" citStr="Navigli and Ponzetto, 2012" startWordPosition="5612" endWordPosition="5615">evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program. In our example, while Olympics and Olympic games refer to the same concept, only the former is annotated as keyphrase. Hence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent. 5.2 Recommendations We recommend that background knowledge be extracted from external lexical databases (e.g., YAGO2 (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), BabelNet (Navigli and Ponzetto, 2012)) to address the four types of errors discussed above. First, we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases. Note that if we can identify semantically equivalent candidates, then we can reduce redundancy errors. The question, then, is: can background knowledge be used to help us identify semantically equivalent candidates? To answer this question, note that Freebase, for instance, has over 40 million topics (i.e., realworld entities such as people, places, and things) from over 70 domains (e.g., music, business, educatio</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Nagendra Koilada</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Bayesian text segmentation for index term identification and keyphrase extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>2077--2092</pages>
<contexts>
<context position="8159" citStr="Newman et al., 2012" startWordPosition="1262" endWordPosition="1265">et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learnin</context>
</contexts>
<marker>Newman, Koilada, Lau, Baldwin, 2012</marker>
<rawString>David Newman, Nagendra Koilada, Jey Han Lau, and Timothy Baldwin. 2012. Bayesian text segmentation for index term identification and keyphrase extraction. In Proceedings of the 24th International Conference on Computational Linguistics, pages 2077–2092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Dung Nguyen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Keyphrase extraction in scientific publications.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Asian Digital Libraries,</booktitle>
<pages>317--326</pages>
<contexts>
<context position="4337" citStr="Nguyen and Kan, 2007" startWordPosition="647" endWordPosition="650">structured documents (e.g., web pages, which can be blogs, forums, or reviews) 1Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly availab</context>
<context position="12224" citStr="Nguyen and Kan, 2007" startWordPosition="1911" endWordPosition="1914"> al., 2006; Kim et al., 2013). Other statistical features include phrase length and spread (i.e., the number of words between the first and last occurrences of a phrase in the document). Structural features encode how different instances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. In fact, features that encode how frequently a candidate keyphrase occurs in various sections of a scientific paper (e.g., introduction, conclusion) (Nguyen and Kan, 2007) and those that encode the location of a candidate keyphrase in a web page (e.g., whether it appears in the title) (Chen et al., 2005; Yih et al., 2006) have been shown to be useful for the task. Syntactic features encode the syntactic patterns of a candidate keyphrase. For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence, which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studie</context>
<context position="28986" citStr="Nguyen and Kan, 2007" startWordPosition="4617" endWordPosition="4620">l-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participati</context>
</contexts>
<marker>Nguyen, Kan, 2007</marker>
<rawString>Thuy Dung Nguyen and Min-Yen Kan. 2007. Keyphrase extraction in scientific publications. In Proceedings of the International Conference on Asian Digital Libraries, pages 317–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chau Q Nguyen</author>
<author>Tuoi T Phan</author>
</authors>
<title>An ontology-based approach for key phrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing: Short Papers,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="7728" citStr="Nguyen and Phan, 2009" startWordPosition="1193" endWordPosition="1196">ndidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task R</context>
</contexts>
<marker>Nguyen, Phan, 2009</marker>
<rawString>Chau Q. Nguyen and Tuoi T. Phan. 2009. An ontology-based approach for key phrase extraction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing: Short Papers, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
</authors>
<title>Introduction to DUC-2001: An intrinsic evaluation of generic news text summarization systems.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Document Understanding Conference.</booktitle>
<contexts>
<context position="29316" citStr="Over, 2001" startWordPosition="4668" endWordPosition="4669">errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (</context>
</contexts>
<marker>Over, 2001</marker>
<rawString>Paul Over. 2001. Introduction to DUC-2001: An intrinsic evaluation of generic news text summarization systems. In Proceedings of the 2001 Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari-Sanna Paukkeri</author>
<author>Ilari T Nieminen</author>
<author>Matti P¨oll¨a</author>
<author>Timo Honkela</author>
</authors>
<title>A language-independent approach to keyphrase extraction and evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics: Companion Volume: Posters,</booktitle>
<pages>83--86</pages>
<marker>Paukkeri, Nieminen, P¨oll¨a, Honkela, 2008</marker>
<rawString>Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti P¨oll¨a, and Timo Honkela. 2008. A language-independent approach to keyphrase extraction and evaluation. In Proceedings of the 22nd International Conference on Computational Linguistics: Companion Volume: Posters, pages 83–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<pages>523</pages>
<contexts>
<context position="10746" citStr="Salton and Buckley, 1988" startWordPosition="1653" endWordPosition="1656"> supervised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an instance for supervised keyphrase extraction can be broadly divided into two categories. 3.2.2.1 Within-Collection Features Within-collection features are computed based solely on the training documents. These features can be further divided into three types. Statistical features are computed based on statistical information gathered from the training documents. Three such features have been extensively used in supervised approaches. The first one, tf*idf (Salton and Buckley, 1988), is computed based on candidate frequency in the given text and inverse document frequency (i.e., number of other documents where the candidate appears).2 The second one, the distance of a phrase, is defined as the number of words preceding its first occurrence normalized by the number of words in the document. Its usefulness stems from the fact that keyphrases tend to appear early in a document. The third one, supervised keyphraseness, encodes the number of times a phrase appears as a keyphrase in the training set. This feature is designed based on the assumption that a phrase frequently tag</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Song</author>
<author>Il-Yeol Song</author>
<author>Xiaohua Hu</author>
</authors>
<title>KPSpotter: A flexible information gain-based keyphrase extraction system.</title>
<date>2003</date>
<booktitle>In Proceedings of the 5th ACM International Workshop on Web Information and Data Management,</booktitle>
<pages>50--53</pages>
<contexts>
<context position="10175" citStr="Song et al., 2003" startWordPosition="1572" endWordPosition="1575"> of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011). Motivated by this observation, Jiang et al. (2009) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA (Witten et al., 1999; Frank et al., 1999), a popular supervised baseline that adopts the traditional supervised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an instance for supervised keyphrase extraction can be broadly divided into two categories. 3.2.2.1 Within-Collection Features Within-collection features are computed based solely on the training documents. These features can be further divided into three types. Statistical features are computed based on statistical information gathered from the training documents. Three such features have been extensively used in supervised approaches. The first one, tf*idf (Salton and Buckley, 1988), is computed based on candid</context>
</contexts>
<marker>Song, Song, Hu, 2003</marker>
<rawString>Min Song, Il-Yeol Song, and Xiaohua Hu. 2003. KPSpotter: A flexible information gain-based keyphrase extraction system. In Proceedings of the 5th ACM International Workshop on Web Information and Data Management, pages 50–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International World Wide Web Conference,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="34968" citStr="Suchanek et al., 2007" startWordPosition="5601" endWordPosition="5604">nding gold keyphrase are semantically equivalent. In other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program. In our example, while Olympics and Olympic games refer to the same concept, only the former is annotated as keyphrase. Hence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent. 5.2 Recommendations We recommend that background knowledge be extracted from external lexical databases (e.g., YAGO2 (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), BabelNet (Navigli and Ponzetto, 2012)) to address the four types of errors discussed above. First, we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases. Note that if we can identify semantically equivalent candidates, then we can reduce redundancy errors. The question, then, is: can background knowledge be used to help us identify semantically equivalent candidates? To answer this question, note that Freebase, for instance, has over 40 million topics (i.e., realworld entities such as people,</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A core of semantic knowledge. In Proceedings of the 16th International World Wide Web Conference, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Tomokiyo</author>
<author>Matthew Hurst</author>
</authors>
<title>A language model approach to keyphrase extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="911" citStr="Tomokiyo and Hurst, 2003" startWordPosition="137" endWordPosition="140">xamined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved</context>
<context position="22859" citStr="Tomokiyo and Hurst (2003)" startWordPosition="3603" endWordPosition="3606">proach (i.e., bipartite S–W graphs) and TextRank (i.e., W– W graphs) and performs better than both of them. However, it has a weakness: like TextRank, it does not ensure that the extracted keyphrases will cover all the main topics. To address this problem, one can employ a topic clustering algorithm on the W– W graph to produce the topic clusters, and then ensure that keyphrases are chosen from every main topic cluster. 1266 3.3.4 Language Modeling Many existing approaches have a separate, heuristic module for extracting candidate keyphrases prior to keyphrase ranking/extraction. In contrast, Tomokiyo and Hurst (2003) propose an approach (henceforth LMA) that combines these two steps. LMA scores a candidate keyphrase based on two features, namely, phraseness (i.e., the extent to which a word sequence can be treated as a phrase) and informativeness (i.e., the extent to which a word sequence captures the central idea of the document it appears in). Intuitively, a phrase that has high scores for phraseness and informativeness is likely to be a keyphrase. These feature values are estimated using language models (LMs) trained on a foreground corpus and a background corpus. The foreground corpus is composed of t</context>
</contexts>
<marker>Tomokiyo, Hurst, 2003</marker>
<rawString>Takashi Tomokiyo and Matthew Hurst. 2003. A language model approach to keyphrase extraction. In Proceedings of the ACL Workshop on Multiword Expressions, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Learning to extract keyphrases from text.</title>
<date>1999</date>
<tech>Technical Report ERB-1057.</tech>
<institution>National Research Council Canada, Institute for Information Technology,</institution>
<contexts>
<context position="8478" citStr="Turney, 1999" startWordPosition="1311" endWordPosition="1312"> long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010</context>
</contexts>
<marker>Turney, 1999</marker>
<rawString>Peter Turney. 1999. Learning to extract keyphrases from text. National Research Council Canada, Institute for Information Technology, Technical Report ERB-1057.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction. Information Retrieval,</title>
<date>2000</date>
<pages>2--303</pages>
<contexts>
<context position="762" citStr="Turney, 2000" startWordPosition="111" endWordPosition="112">sity of Texas at Dallas Richardson, TX 75083-0688 {saidul,vince}@hlt.utdallas.edu Abstract While automatic keyphrase extraction has been examined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutw</context>
<context position="8514" citStr="Turney, 2000" startWordPosition="1317" endWordPosition="1318">of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jian</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2:303–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Coherent keyphrase extraction via web mining.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>434--439</pages>
<contexts>
<context position="6115" citStr="Turney, 2003" startWordPosition="949" endWordPosition="950">rd in time, and so do the keyphrases associated with a topic. One way to address this complication is to detect a topic change in conversational text (Kim and Baldwin, 2012). However, topic change detection is not always easy: while the topics listed in the form of an agenda at the beginning of formal meeting transcripts can be exploited, such clues are absent in casual conversations (e.g., chats). Topic correlation Another observation commonly exploited in keyphrase extraction from scientific articles and news articles is that the keyphrases in a document are typically related to each other (Turney, 2003; Mihalcea and Tarau, 2004). However, this observation does not necessarily hold for informal text (e.g., emails, chats, informal meetings, personal blogs), where people can talk about any number of potentially uncorrelated topics. The presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness and therefore increases the difficulty of keyphrase extraction. 3 Keyphrase Extraction Approaches A keyphrase extraction system typically operates in two steps: (1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3</context>
<context position="14823" citStr="Turney (2003)" startWordPosition="2329" endWordPosition="2330">tated with keyphrases and can work even if there is a mismatch between the training domain and the test domain. Yih et al. (2006) employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query. Terminological databases have been similarly exploited to encode the salience of candidate keyphrases in scientific papers (Lopez and Romary, 2010). While the aforementioned external resourcebased features attempt to encode how salient a candidate keyphrase is, Turney (2003) proposes features that encode the semantic relatedness between two candidate keyphrases. Noting that candidate keyphrases that are not semantically related to the predicted keyphrases are unlikely to be keyphrases in technical reports, Turney employs coherence features to identify such candidate keyphrases. Semantic relatedness is encoded in the coherence features as two candidate keyphrases’ pointwise mutual information, which Turney computes by using the Web as a corpus. 3.3 Unsupervised Approaches Existing unsupervised approaches to keyphrase extraction can be categorized into four groups.</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>Peter Turney. 2003. Coherent keyphrase extraction via web mining. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pages 434–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>CollabRank: Towards a collaborative approach to single-document keyphrase extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="4540" citStr="Wan and Xiao, 2008" startWordPosition="683" endWordPosition="686">oogle.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articl</context>
<context position="7379" citStr="Wan and Xiao, 2008" startWordPosition="1138" endWordPosition="1141">ate keyphrases are correct keyphrases using supervised (Section 3.2) or unsupervised (Section 3.3) approaches. 3.1 Selecting Candidate Words and Phrases As noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have bee</context>
<context position="16127" citStr="Wan and Xiao, 2008" startWordPosition="2525" endWordPosition="2528">rtant words and phrases from a document. Traditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected candidates. For each node, each of its edges is treated as a “vote” from the other node connected by the edge. A node’s score in </context>
<context position="29934" citStr="Wan and Xiao, 2008" startWordPosition="4771" endWordPosition="4774">, scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-theart keyphrase extractors, of which two are unsupervised (Wan and Xiao, 2008b; Liu et al., 2009b) and one is supervised (Medelyan et al., 2009). Our analysis reveals that the errors fall into four major types, each of which contributes significantly to the overall errors mad</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008a. CollabRank: Towards a collaborative approach to single-document keyphrase extraction. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 969–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Single document keyphrase extraction using neighborhood knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,</booktitle>
<pages>855--860</pages>
<contexts>
<context position="4540" citStr="Wan and Xiao, 2008" startWordPosition="683" endWordPosition="686">oogle.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articl</context>
<context position="7379" citStr="Wan and Xiao, 2008" startWordPosition="1138" endWordPosition="1141">ate keyphrases are correct keyphrases using supervised (Section 3.2) or unsupervised (Section 3.3) approaches. 3.1 Selecting Candidate Words and Phrases As noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have bee</context>
<context position="16127" citStr="Wan and Xiao, 2008" startWordPosition="2525" endWordPosition="2528">rtant words and phrases from a document. Traditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected candidates. For each node, each of its edges is treated as a “vote” from the other node connected by the edge. A node’s score in </context>
<context position="29934" citStr="Wan and Xiao, 2008" startWordPosition="4771" endWordPosition="4774">, scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adopted by the participating systems can be found in Kim et al. (2013). Dataset Approach and System Score [Supervised?] P R F Abstracts Topic clustering 35.0 66.0 45.7 (Inspec) (Liu et al., 2009b) [×] Blogs 35.1 61.5 44.7 Topic community detection (Grineva et al., 2009) [ × ] News Graph-based ranking 28.8 35.4 31.7 (DUC for extended neighborhood -2001) (Wan and Xiao, 2008b) [×] Papers Statistical, semantic, and 27.2 27.8 27.5 (SemEval distributional features -2010) (Lopez and Romary, 2010) [✓] Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-theart keyphrase extractors, of which two are unsupervised (Wan and Xiao, 2008b; Liu et al., 2009b) and one is supervised (Medelyan et al., 2009). Our analysis reveals that the errors fall into four major types, each of which contributes significantly to the overall errors mad</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008b. Single document keyphrase extraction using neighborhood knowledge. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pages 855–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>552--559</pages>
<contexts>
<context position="21203" citStr="Wan et al. (2007)" startWordPosition="3327" endWordPosition="3330"> recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo! term extractor. 3.3.3 Simultaneous Learning Since keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously. Zha (2002) proposes the first graph-based approach for simultaneous summarization and keyphrase extraction, motivated by a key observation: a sentence is important if it contains important words, and important words appear in important sentences. Wan et al. (2007) extend Zha’s work by adding two assumptions: (1) an important sentence is connected to other important sentences, and (2) an important word is linked to other important words, a TextRank-like assumption. Based on these assumptions, Wan et al. (2007) build three graphs to capture the association between the sentences (S) and the words (W) in an input document, namely, a S–S graph, a bipartite S–W graph, and a W–W graph. The weight of an edge connecting two sentence nodes in a S–S graph corresponds to their content similarity. An edge weight in a S–W graph denotes the word’s importance in the s</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Wang</author>
<author>Sujian Li</author>
</authors>
<title>CoRankBayes: Bayesian learning to rank under the co-training framework and its application in keyphrase extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>2241--2244</pages>
<contexts>
<context position="9701" citStr="Wang and Li, 2011" startWordPosition="1500" endWordPosition="1503">d support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011). Motivated by this observation, Jiang et al. (2009) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA (Witten et al., 1999; Frank et al., 1999), a popular supervised baseline that adopts the traditional supervised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an instance for supervised keyphrase extract</context>
</contexts>
<marker>Wang, Li, 2011</marker>
<rawString>Chen Wang and Sujian Li. 2011. CoRankBayes: Bayesian learning to rank under the co-training framework and its application in keyphrase extraction. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, pages 2241–2244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Gordon W Paynter</author>
<author>Eibe Frank</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>KEA: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4th ACM Conference on Digital Libraries,</booktitle>
<pages>254--255</pages>
<contexts>
<context position="4486" citStr="Witten et al., 1999" startWordPosition="673" endWordPosition="676">om/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list. 1262 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphra</context>
<context position="7551" citStr="Witten et al., 1999" startWordPosition="1167" endWordPosition="1170"> set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al</context>
<context position="8876" citStr="Witten et al., 1999" startWordPosition="1369" endWordPosition="1372"> on two issues: task reformulation and feature design. 1263 3.2.1 Task Reformulation Early supervised approaches to keyphrase extraction recast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c</context>
<context position="11490" citStr="Witten et al., 1999" startWordPosition="1784" endWordPosition="1787">where the candidate appears).2 The second one, the distance of a phrase, is defined as the number of words preceding its first occurrence normalized by the number of words in the document. Its usefulness stems from the fact that keyphrases tend to appear early in a document. The third one, supervised keyphraseness, encodes the number of times a phrase appears as a keyphrase in the training set. This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. These three features form the feature set of KEA (Witten et al., 1999; Frank et al., 1999), and have been shown to perform consistently well on documents from various sources (Yih et al., 2006; Kim et al., 2013). Other statistical features include phrase length and spread (i.e., the number of words between the first and last occurrences of a phrase in the document). Structural features encode how different instances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. In fact, features that encode how fre</context>
<context position="28964" citStr="Witten et al., 1999" startWordPosition="4613" endWordPosition="4616">ea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. 5 Analysis With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing. To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), scientific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we randomly selected 25 documents from each of these 3A more detailed analysis of the results of the SemEval2010 shared task and the approaches adop</context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl Gutwin, and Craig G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. In Proceedings of the 4th ACM Conference on Digital Libraries, pages 254–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Fang Brook Wu</author>
<author>Quanzhi Li</author>
<author>Razvan Stefan Bot</author>
<author>Xin Chen</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>283--284</pages>
<contexts>
<context position="7651" citStr="Wu et al., 2005" startWordPosition="1184" endWordPosition="1187">ules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) allowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009). Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has fo</context>
</contexts>
<marker>Wu, Li, Bot, Chen, 2005</marker>
<rawString>Yi-Fang Brook Wu, Quanzhi Li, Razvan Stefan Bot, and Xin Chen. 2005. Domain-specific keyphrase extraction. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, pages 283–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Tau Yih</author>
<author>Joshua Goodman</author>
<author>Vitor R Carvalho</author>
</authors>
<title>Finding advertising keywords on web pages.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th International Conference on World Wide Web,</booktitle>
<pages>213--222</pages>
<contexts>
<context position="3270" citStr="Yih et al., 2006" startWordPosition="508" endWordPosition="511">er search space (Hasan and Ng, 2010). Consequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles. Structural consistency In a structured document, there are certain locations where a keyphrase is most likely to appear. For instance, most of a scientific paper’s keyphrases should appear in the abstract and the introduction. While structural information has been exploited to extract keyphrases from scientific papers (e.g., title, section information) (Kim et al., 2013), web pages (e.g., metadata) (Yih et al., 2006), and chats (e.g., dialogue acts) (Kim and Baldwin, 2012), it is most useful when the documents from a source exhibit structural similarity. For this reason, structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports because of their standard format (i.e., standard sections such as abstract, introduction, conclusion, etc.). In contrast, the lack of structural consistency in other types of structured documents (e.g., web pages, which can be blogs, forums, or reviews) 1Many of the publicly available corpora can be found in http://github.com/s</context>
<context position="4634" citStr="Yih et al. (2006)" startWordPosition="701" endWordPosition="704">ociation for Computational Linguistics, pages 1262–1273, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Source Dataset/Contributor Statistics Documents Tokens/doc Keys/doc Paper abstracts Inspec (Hulth, 2003)∗ 2,000 &lt;200 10 Scientific papers NUS corpus (Nguyen and Kan, 2007)∗ 211 ≈8K 11 citeulike.org (Medelyan et al., 2009)∗ 180 - 5 SemEval-2010 (Kim et al., 2010b)∗ 284 &gt;5K 15 Technical reports NZDL (Witten et al., 1999)∗ 1,800 - - News articles DUC-2001 (Wan and Xiao, 2008b)∗ 308 ≈900 8 Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6 Web pages Yih et al. (2006) 828 - - Hammouda et al. (2005)∗ 312 ≈500 - Blogs (Grineva et al., 2009) 252 ≈1K 8 Meeting transcripts ICSI (Liu et al., 2009a) 161 ≈1.6K 4 Emails Enron corpus (Dredze et al., 2008)∗ 14,659 - - Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10 Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (∗). may render structural information less useful. Topic change An observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the beginning (Witten et al., 1999) but als</context>
<context position="9010" citStr="Yih et al., 2006" startWordPosition="1390" endWordPosition="1393">ecast this task as a binary classification problem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including naive Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bagging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010). Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c1 is more representative than another candidate phrase c2, c1 should be preferred to c2. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow u</context>
<context position="11613" citStr="Yih et al., 2006" startWordPosition="1806" endWordPosition="1809"> occurrence normalized by the number of words in the document. Its usefulness stems from the fact that keyphrases tend to appear early in a document. The third one, supervised keyphraseness, encodes the number of times a phrase appears as a keyphrase in the training set. This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. These three features form the feature set of KEA (Witten et al., 1999; Frank et al., 1999), and have been shown to perform consistently well on documents from various sources (Yih et al., 2006; Kim et al., 2013). Other statistical features include phrase length and spread (i.e., the number of words between the first and last occurrences of a phrase in the document). Structural features encode how different instances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. In fact, features that encode how frequently a candidate keyphrase occurs in various sections of a scientific paper (e.g., introduction, conclusion) (Nguyen and</context>
<context position="12867" citStr="Yih et al., 2006" startWordPosition="2023" endWordPosition="2026"> location of a candidate keyphrase in a web page (e.g., whether it appears in the title) (Chen et al., 2005; Yih et al., 2006) have been shown to be useful for the task. Syntactic features encode the syntactic patterns of a candidate keyphrase. For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence, which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studies conducted on web pages (Yih et al., 2006) and scientific articles 2A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches (Zhang et al., 2005; Dredze et al., 2008; Paukkeri et al., 2008; Grineva et al., 2009). 1264 (Kim and Kan, 2009) reveal that syntactic features are not useful for keyphrase extraction in the presence of other feature types. 3.2.2.2 External Resource-Based Features External resource-based features are computed based on information gathered from resources other than the training documents, such as lexical kn</context>
<context position="14339" citStr="Yih et al. (2006)" startWordPosition="2252" endWordPosition="2255">ikipedia-based keyphraseness is computed as a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears (Medelyan et al., 2009). This feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently as a link in Wikipedia. Unlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain. Yih et al. (2006) employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query. Terminological databases have been similarly exploited to encode the salience of candidate keyphrases in scientific papers (Lopez and Romary, 2010). While the aforementioned external resourcebased features attempt to encode how salient a candidate keyphrase is, Turney (2003) proposes features that encode the semantic relatedness between two candidate keyphrases. Noting that candidate keyp</context>
</contexts>
<marker>Yih, Goodman, Carvalho, 2006</marker>
<rawString>Wen-Tau Yih, Joshua Goodman, and Vitor R. Carvalho. 2006. Finding advertising keywords on web pages. In Proceedings of the 15th International Conference on World Wide Web, pages 213–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei You</author>
<author>Dominique Fontaine</author>
<author>Jean-Paul Barth`es</author>
</authors>
<title>Automatic keyphrase extraction with a refined candidate set.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,</booktitle>
<pages>576--579</pages>
<marker>You, Fontaine, Barth`es, 2009</marker>
<rawString>Wei You, Dominique Fontaine, and Jean-Paul Barth`es. 2009. Automatic keyphrase extraction with a refined candidate set. In Proceedings of the IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, pages 576–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Approximate matching for evaluating keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing</booktitle>
<pages>484--489</pages>
<contexts>
<context position="27586" citStr="Zesch and Gurevych, 2009" startWordPosition="4393" endWordPosition="4396"> detect a subset of the near-misses (Kim et al., 2010a). The second type of metrics focuses on how a system ranks its predictions. Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) (Liu et al., 2010) will award more credit to A than to B if the ranks of the correct predictions in A’s output are higher than those in B’s output. R-precision (Rp) is an 1267 IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates (Zesch and Gurevych, 2009). The motivation behind the design of Rp is simple: a system will achieve a perfect Rp value if it ranks all the keyphrases above the non-keyphrases. 4.2 The State of the Art Table 2 lists the best scores on some popular evaluation datasets and the corresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.3 Two points deserve mention. First, F-scores decrease as document length increases. These results are consistent with the observation we made in Section 2 that it is more difficult </context>
</contexts>
<marker>Zesch, Gurevych, 2009</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2009. Approximate matching for evaluating keyphrase extraction. In Proceedings of the International Conference on Recent Advances in Natural Language Processing 2009, pages 484–489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="20949" citStr="Zha (2002)" startWordPosition="3290" endWordPosition="3291">xtracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic. CommunityCluster yields much better recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo! term extractor. 3.3.3 Simultaneous Learning Since keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously. Zha (2002) proposes the first graph-based approach for simultaneous summarization and keyphrase extraction, motivated by a key observation: a sentence is important if it contains important words, and important words appear in important sentences. Wan et al. (2007) extend Zha’s work by adding two assumptions: (1) an important sentence is connected to other important sentences, and (2) an important word is linked to other important words, a TextRank-like assumption. Based on these assumptions, Wan et al. (2007) build three graphs to capture the association between the sentences (S) and the words (W) in an</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongzheng Zhang</author>
<author>Nur Zincir-Heywood</author>
<author>Evangelos Milios</author>
</authors>
<title>World Wide Web site summarization. Web Intelligence and Agent Systems,</title>
<date>2004</date>
<pages>2--39</pages>
<contexts>
<context position="1255" citStr="Zhang et al., 2004" startWordPosition="192" endWordPosition="195">rase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 2 Corpora Automatic keyphrase extr</context>
</contexts>
<marker>Zhang, Zincir-Heywood, Milios, 2004</marker>
<rawString>Yongzheng Zhang, Nur Zincir-Heywood, and Evangelos Milios. 2004. World Wide Web site summarization. Web Intelligence and Agent Systems, 2:39–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongzheng Zhang</author>
<author>Nur Zincir-Heywood</author>
<author>Evangelos Milios</author>
</authors>
<title>Narrative text classification for automatic key phrase extraction in web document corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 7th ACM International Workshop on Web Information and Data Management,</booktitle>
<pages>51--58</pages>
<contexts>
<context position="13077" citStr="Zhang et al., 2005" startWordPosition="2053" endWordPosition="2056"> patterns of a candidate keyphrase. For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence, which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studies conducted on web pages (Yih et al., 2006) and scientific articles 2A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches (Zhang et al., 2005; Dredze et al., 2008; Paukkeri et al., 2008; Grineva et al., 2009). 1264 (Kim and Kan, 2009) reveal that syntactic features are not useful for keyphrase extraction in the presence of other feature types. 3.2.2.2 External Resource-Based Features External resource-based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g., Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge. Below we give an overview of the external resource-based features that h</context>
</contexts>
<marker>Zhang, Zincir-Heywood, Milios, 2005</marker>
<rawString>Yongzheng Zhang, Nur Zincir-Heywood, and Evangelos Milios. 2005. Narrative text classification for automatic key phrase extraction in web document corpora. In Proceedings of the 7th ACM International Workshop on Web Information and Data Management, pages 51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Zhao</author>
<author>Jing Jiang</author>
<author>Jing He</author>
</authors>
<title>Yang Song, Palakorn Achanauparp, Ee-Peng Lim, and Xiaoming Li.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>379--388</pages>
<contexts>
<context position="969" citStr="Zhao et al., 2011" startWordPosition="149" endWordPosition="152"> still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. 1 Introduction Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011). Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and document indexing (Gutwin et al., 1999). Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is </context>
</contexts>
<marker>Zhao, Jiang, He, 2011</marker>
<rawString>Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achanauparp, Ee-Peng Lim, and Xiaoming Li. 2011. Topical keyphrase extraction from Twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 379–388.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>