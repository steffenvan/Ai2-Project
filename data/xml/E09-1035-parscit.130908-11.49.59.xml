<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000363">
<title confidence="0.969747">
Structural, Transitive and Latent Models for Biographic Fact Extraction
</title>
<author confidence="0.992068">
Nikesh Garera and David Yarowsky
</author>
<affiliation confidence="0.9983095">
Department of Computer Science, Johns Hopkins University
Human Language Technology Center of Excellence
</affiliation>
<address confidence="0.9484">
Baltimore MD, USA
</address>
<email confidence="0.999829">
{ngarera,yarowsky}@cs.jhu.edu
</email>
<sectionHeader confidence="0.997403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997825">
This paper presents six novel approaches
to biographic fact extraction that model
structural, transitive and latent proper-
ties of biographical data. The ensem-
ble of these proposed models substantially
outperforms standard pattern-based bio-
graphic fact extraction methods and per-
formance is further improved by modeling
inter-attribute correlations and distribu-
tions over functions of attributes, achiev-
ing an average extraction accuracy of 80%
over seven types of biographic attributes.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999208">
Extracting biographic facts such as “Birthdate”,
“Occupation”, “Nationality”, etc. is a critical step
for advancing the state of the art in information
processing and retrieval. An important aspect of
web search is to be able to narrow down search
results by distinguishing among people with the
same name leading to multiple efforts focusing
on web person name disambiguation in the liter-
ature (Mann and Yarowsky, 2003; Artiles et al.,
2007, Cucerzan, 2007). While biographic facts are
certainly useful for disambiguating person names,
they also allow for automatic extraction of ency-
lopedic knowledge that has been limited to man-
ual efforts such as Britannica, Wikipedia, etc.
Such encyploedic knowledge can advance verti-
cal search engines such as http://www.spock.com
that are focused on people searches where one can
get an enhanced search interface for searching by
various biographic attributes. Biographic facts are
also useful for powerful query mechanisms such
as finding what attributes are common between
two people (Auer and Lehmann, 2007).
</bodyText>
<figureCaption confidence="0.754195">
Figure 1: Goal: extracting attribute-value bio-
graphic fact pairs from biographic free-text
</figureCaption>
<bodyText confidence="0.99971825">
While there are a large quantity of biographic texts
available online, there are only a few biographic
fact databases available1, and most of them have
been created manually, are incomplete and are
available primarily in English.
This work presents multiple novel approaches
for automatically extracting biographic facts such
as “Birthdate”, “Occupation”, “Nationality”, and
“Religion”, making use of diverse sources of in-
formation present in biographies.
In particular, we have proposed and evaluated the
following 6 distinct original approaches to this
</bodyText>
<footnote confidence="0.988148">
1E.g.: http://www.nndb.com, http://www.biography.com,
Infoboxes in Wikipedia
</footnote>
<note confidence="0.962071">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.994742">
300
</page>
<bodyText confidence="0.767537">
task with large collective empirical gains:
</bodyText>
<listItem confidence="0.9803619">
1. An improvement to the Ravichandran and
Hovy (2002) algorithm based on Partially
Untethered Contextual Pattern Models
2. Learning a position-based model using ab-
solute and relative positions and sequential
order of hypotheses that satisfy the domain
model. For example, “Deathdate” very often
appears after “Birthdate” in a biography.
3. Using transitive models over attributes via
co-occurring entities. For example, other
people mentioned person’s biography page
tend to have similar attributes such as occu-
pation (See Figure 4).
4. Using latent wide-document-context models
to detect attributes that may not be mentioned
directly in the article (e.g. the words “song,
hits, album, recorded,..” all collectively indi-
cate the occupation of singer or musician in
the article.
5. Using inter-attribute correlations, for filter-
ing unlikely biographic attribute combina-
tions. For example, a tuple consisting of &lt;
“Nationality” = India, “Religion” = Hindu &gt;
has a higher probability than a tuple consist-
ing of &lt; “Nationality” = France, “Religion”
= Hindu &gt;.
6. Learning distributions over functions of at-
tributes, for example, using an age distri-
bution to filter tuples containing improbable
&lt;deathyear&gt;-&lt;birthyear&gt; lifespan values.
</listItem>
<bodyText confidence="0.997050333333333">
We propose and evaluate techniques for exploiting
all of the above classes of information in the next
sections.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.995066255813954">
The literature for biography extraction falls into
two major classes. The first one deals with iden-
tifying and extracting biographical sentences and
treats the problem as a summarization task (Cowie
et al., 2000, Schiffman et al., 2001, Zhou et
al., 2004). The second and more closely related
class deals with extracting specific facts such as
“birthplace”, “occupation”, etc. For this task,
the primary theme of work in the literature has
been to treat the task as a general semantic-class
learning problem where one starts with a few
seeds of the semantic relationship of interest and
learns contextual patterns such as “&lt;NAME&gt;
was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born
&lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; The-
len and Riloff, 2002; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002; Mann and
Yarowsky, 2003; Jijkoun et al., 2004; Mann and
Yarowsky, 2005; Alfonseca et al., 2006; Pasca et
al., 2006). There has also been some work on ex-
tracting biographic facts directly from Wikipedia
pages. Culotta et al. (2006) deal with learning
contextual patterns for extracting family relation-
ships from Wikipedia. Ruiz-Casado et al. (2006)
learn contextual patterns for biographic facts and
apply them to Wikipedia pages.
While the pattern-learning approach extends well
for a few biography classes, some of the bio-
graphic facts like “Gender” and “Religion” do not
have consistent contextual patterns, and only a
few of the explicit biographic attributes such as
“Birthdate”, “Deathdate”, “Birthplace” and “Oc-
cupation” have been shown to work well in the
pattern-learning framework (Mann and Yarowsky,
2005; Alfonesca, 2006; Pasca et al., 2006).
Secondly, there is a general lack of work that at-
tempts to utilize the typical information sequenc-
ing within biographic texts for fact extraction, and
we show how the information structure of biogra-
phies can be used to improve upon pattern based
models. Furthermore, we also present additional
novel models of attribute correlation and age dis-
tribution that aid the extraction process.
</bodyText>
<sectionHeader confidence="0.994723" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999895866666667">
We first implement the standard pattern-based ap-
proach for extracting biographic facts from the raw
prose in Wikipedia people pages. We then present
an array of novel techniques exploiting different
classes of information including partially-tethered
contextual patterns, relative attribute position and
sequence, transitive attributes of co-occurring en-
tities, broad-context topical profiles, inter-attribute
correlations and likely human age distributions.
For illustrative purposes, we motivate each tech-
nique using one or two attributes but in practice
they can be applied to a wide range of attributes
and empirical results in Table 4 show that they
give consistent performance gains across multiple
attributes.
</bodyText>
<page confidence="0.999244">
301
</page>
<sectionHeader confidence="0.997233" genericHeader="method">
4 Contextual Pattern-Based Model
</sectionHeader>
<bodyText confidence="0.95788668627451">
A standard model for extracting biographic facts
is to learn templatic contextual patterns such as
&lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such
templatic patterns can be learned using seed ex-
amples of the attribute in question and, there has
been a plethora of work in the seed-based boot-
strapping literature which addresses this problem
(Ravichandran and Hovy, 2002; Thelen and Riloff,
2002; Mann and Yarowsky, 2005; Alfonseca et al.,
2006; Pasca et al., 2006)
Thus for our baseline we implemented a stan-
dard Ravichandran and Hovy (2002) pattern
learning model using 100 seed2 examples from
an online biographic database called NNDB
(http://www.nndb.com) for each of the biographic
attributes: “Birthdate”, “Birthplace”, “Death-
date”, “Gender”, “Nationality”, “Occupation” and
“Religion”. Given the seed pairs, patterns for
each attribute were learned by searching for seed
&lt;Name,Attribute Value&gt; pairs in the Wikipedia
page and extracting the left, middle and right con-
texts as various contextual patterns3.
While the biographic text was obtained from
Wikipedia articles, all of the 7 attribute values
used as seed and test person names could not
be obtained from Wikipedia due to incomplete
and unnormalized (for attribute value format) in-
foboxes. Hence, the values for training/evaluation
were extracted from NNDB which provides a
cleaner set of gold truth, and is similar to an ap-
proach utilizing trained annotators for marking up
and extracting the factual information in a stan-
dard format. For consistency, only the people
names whose articles occur in Wikipedia where
selected as part of seed and test sets.
Given the attribute values of the seed names and
their text articles, the probability of a relationship
r(Attribute Name), given the surrounding context
“A1 p A2 q A3”, where p and q are &lt;NAME&gt;
and &lt;Attrib Val&gt; respectively, is given using the
rote extractor model probability as in (Ravichan-
dran and Hovy, 2002; Mann and Yarowsky 2005):
2The seed examples were chosen randomly, with a bias
against duplicate attribute values to increase training diver-
sity. Both the seed and test names and data will be made
available online to the research community for replication
and extension.
3We implemented a noisy model of coreference resolu-
tion by resolving any gender-correct pronoun used in the
Wikipedia page to the title person name of the article. Gender
is also extracted automatically as a biographic attribute.
</bodyText>
<equation confidence="0.984453">
Ex,y Er c(A1xA2yA3)
P(r(p, q) |A1pA2qA3) = Ex z c(A1xA2zA3)
</equation>
<bodyText confidence="0.997763363636364">
Thus, the probability for each contextual pattern
is based on how often it correctly predicts a re-
lationship in the seed set. And, each extracted
attribute value q using the given pattern can thus
be ranked according to the above probability. We
tested this approach for extracting values for each
of the seven attributes on a test set of 100 held-out
names and report Precision, Pseudo-recall and F-
score for each attribute which are computed in the
standard way as follows, for say Attribute “Birth-
place (bplace)”:
</bodyText>
<figure confidence="0.531336714285714">
# people with bplace correctly extracted
Precisionbplace = # of people with bplace extracted
# people with bplace correctly extracted
Pseudo-recbplace =
# of people with bplace in test set
2·Precisionbplace·Pseudo-recbplace
Precisionbplace + Pseudo-recbplace
</figure>
<bodyText confidence="0.994763380952381">
Since the true values of each attribute are obtained
from a cleaner and normalized person-database
(NNDB), not all the attribute values maybe present
in the Wikipedia article for a given name. Thus,
we also compute accuracy on the subset of names
for which the value of a given attribute is also ex-
plictly stated in the article. This is denoted as:
# people with bplace correctly extracted
Acctruth pres = # of people with true bplace stated in article
We further applied a domain model for each at-
tribute to filter noisy targets extracted from lex-
ical patterns. Our domain models of attributes
include lists of acceptable values (such as lists
of places, occupations and religions) and struc-
tural constraints such as possible date formats for
“Birthdate” and “Deathdate”. The rows with sub-
script “RH02”in Table 4 shows the performance
of this Ravichandran and Hovy (2002) model with
additional attribute domain modeling for each at-
tribute, and Table 3 shows the average perfor-
mance across all attributes.
</bodyText>
<sectionHeader confidence="0.78876" genericHeader="method">
5 Partially Untethered Templatic
Contextual Patterns
</sectionHeader>
<bodyText confidence="0.99997">
The pattern-learning literature for fact extraction
often consists of patterns with a “hook” and
“target” (Mann and Yarowsky, 2005). For ex-
ample, in the pattern “&lt;Name&gt; was born in
&lt;Birthplace&gt;”, “&lt;NAME&gt;” is the hook and
“&lt;Birthplace&gt;” is the target. The disadvantage
of this approach is that the intervening dually-
tethered patterns can be quite long and highly
variable, such as “&lt;NAME&gt; was highly influ-
</bodyText>
<equation confidence="0.685151">
F-scorebplace =
</equation>
<page confidence="0.974863">
302
</page>
<figureCaption confidence="0.9802355">
Figure 2: Distribution of the observed document
mentions of Deathdate, Nationality and Religion.
</figureCaption>
<bodyText confidence="0.999847642857143">
ential in his role as &lt;Occupation&gt;”. We over-
come this problem by modeling partially unteth-
ered variable-length ngram patterns adjacent to
only the target, with the only constraint being
that the hook entity appear somewhere in the sen-
tence4. Examples of these new contextual ngram
features include “his role as &lt;Occupation&gt;” and
‘role as &lt;Occupation&gt;”. The pattern probability
model here is essentially the same as in Ravichan-
dran and Hovy, 2002 and just the pattern repre-
sentation is changed. The rows with subscript
“RH02imp” in tables 4 and 3 show performance
gains using this improved templatic-pattern-based
model, yielding an absolute 21% gain in accuracy.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
6 Document-Position-Based Model
</sectionHeader>
<bodyText confidence="0.984713875">
One of the properties of biographic genres is that
primary biographic attributes5 tend to appear in
characteristic positions, often toward the begin-
ning of the article. Thus, the absolute position
(in percentage) can be modeled explicitly using a
Gaussian parametric model as follows for choos-
ing the best candidate value v* for a given attribute
A:
</bodyText>
<equation confidence="0.879131714285714">
*
v = argmaxvEdomain(A)f(posnv|A)
where,
f(posnv|A)
= N(posnv; µA,
1 −(posn„− µA)2�2 vA2
vA,/2π E
</equation>
<footnote confidence="0.925460666666667">
4This constraint is particularly viable in biographic text,
which tends to focus on the properties of a single individual.
5We use the hyperlinked phrases as potential values for all
attributes except “Gender”. For “Gender” we used pronouns
as potential values ranked according to the their distance from
the beginning of the page.
</footnote>
<bodyText confidence="0.99992825">
In the above equation, posnv is the absolute
position ratio (position/length) and µA, 0A2 are
the sample mean and variance based on the sam-
ple of correct position ratios of attribute values
in biographies with attribute A. Figure 2, for
example, shows the positional distribution of the
seed attribute values for deathdate, nationality and
religion in Wikipedia articles, fit to a Gaussian
distribution. Combining this empirically derived
position model with a domain model6 of accept-
able attribute values is effective enough to serve
as a stand-alone model.
</bodyText>
<table confidence="0.999652777777778">
Attribute Best rank P(Rank)
in seed set
Birthplace 1 0.61
Birthdate 1 0.98
Deathdate 2 0.58
Gender 1 1.0
Occupation 1 0.70
Nationality 1 0.83
Religion 1 0.80
</table>
<tableCaption confidence="0.99864">
Table 1: Majority rank of the correct attribute
</tableCaption>
<bodyText confidence="0.616283">
value in the Wikipedia pages of the seed names
used for learning relative ordering among at-
tributes satisfying the domain model
</bodyText>
<subsectionHeader confidence="0.9975815">
6.1 Learning Relative Ordering in the
Position-Based Model
</subsectionHeader>
<bodyText confidence="0.999995888888889">
In practice, for attributes such as birthdate, the
first text pattern satisfying the domain model is
often the correct answer for biographical articles.
Deathdate also tends to occur near the beginning
of the article, but almost always some point
after the birthdate. This motivates a second,
sequence-based position model based on the rank
of the attribute values among other values in the
domain of the attribute, as follows:
</bodyText>
<equation confidence="0.975801">
v* = argmaxvEdomain(A)P(rankv|A)
</equation>
<bodyText confidence="0.9992738">
where P(rankv|A) is the fraction of biographies
having attribute a with the correct value occuring
at rank rankv, where rank is measured according
to the relative order in which the values belonging
to the attribute domain occur from the beginning
</bodyText>
<footnote confidence="0.774176666666667">
6The domain model is the same as used in Section 4 and
remains constant across all the models developed in this paper
&apos;2�A)
</footnote>
<page confidence="0.995714">
303
</page>
<bodyText confidence="0.993334346153846">
of the article. We use the seed set to learn the rel-
ative positions between attributes, that is, in the
Wikipedia pages of seed names what is the rank of
the correct attribute.
Table 1 shows the most frequent rank of the correct
attribute value and Figure 3 shows the distribu-
tion of the correct ranks for a sample of attributes.
We can see that 61% of the time the first loca-
tion mentioned in a biography is the individuals’s
birthplace, while 58% of the time the 2nd date
in the article is the deathdate. Thus, “Deathdate”
often appears as the second date in a Wikipedia
page as expected. These empirical distributions
for the correct rank provide a direct vehicle for
scoring hypotheses, and the rows with “rel. posn”
as the subscript in Table 4 shows the improvement
in performance using the learned relative order-
ing. Averaging across different attributes, table
3 shows an absolute 11% average gain in accu-
racy of the position-sequence-based models rela-
tive to the improved Ravichandran and Hovy re-
sults achieved here.
Figure 3: Empirical distribution of the relative po-
sition of the correct (seed) answers among all text
phrases satisfying the domain model for “birth-
place” and “death date”.
</bodyText>
<sectionHeader confidence="0.997585" genericHeader="method">
7 Implicit Models
</sectionHeader>
<bodyText confidence="0.999765166666667">
Some of the biographic attributes such as “Nation-
ality”, “Occupation” and “Religion” can be ex-
tracted successfully even when the answer is not
directly mentioned in the biographic article. We
present two such models for doing so in the fol-
lowing subsections:
</bodyText>
<subsectionHeader confidence="0.985096">
7.1 Extracting Attributes Transitively using
Neighboring Person-Names
</subsectionHeader>
<bodyText confidence="0.999817125">
Attributes such as “Occupation” are transitive in
nature, that is, the people names appearing close
to the target name will tend to have the same
occupation as the target name. Based on this
intution, we implemented a transitive model that
predicts occupation based on consensus voting via
the extracted occupations of neighboring names7
as follows:
</bodyText>
<equation confidence="0.944083333333333">
v = argmax„EdomaZn(a)P(vJA, Sneighbors)
where,
P (v A, Sneighbors) =
</equation>
<bodyText confidence="0.996804384615385">
# neighboring names with attrib value v
# of neighboring names in the article
The set of neighboring names is represented
as Sneighbors and the best candidate value for
an attribute A is chosen based on the the fraction
of neighboring names having the same value
for the respective attribute. We rank candidates
according to this probability and the row labeled
“trans” in Table 4 shows that this model helps in
subsantially improving the recall of “Occupation”
and “Religion”, yielding a 7% and 3% average
improvement in F-measure respectively, on top of
the position model described in Section 6.
</bodyText>
<subsectionHeader confidence="0.9672375">
7.2 Latent Model based on Document-Wide
Context Profiles
</subsectionHeader>
<bodyText confidence="0.9999866">
In addition to modeling cross-entity attribute
transitively, attributes such as “Occupation” can
also be modeled successfully using a document-
wide context or topic model. For example, the
distribution of words occuring in a biography
</bodyText>
<footnote confidence="0.595169">
7We only use the neighboring names whose attribute
value can be obtained from an encylopedic database. Fur-
thermore, since we are dealing with biographic pages that
talk about a single person, all other person-names mentioned
in the article whose attributes are present in an encylopedia
were considered for consensus voting
</footnote>
<page confidence="0.998306">
304
</page>
<figureCaption confidence="0.996544">
Figure 4: Illustration of modeling “occupation” and “nationality” transitively via consensus from at-
tributes of neighboring names
</figureCaption>
<bodyText confidence="0.998772">
of a politician would be different from that of
a scientist. Thus, even if the occupation is not
explicitly mentioned in the article, one can infer
it using a bag-of-words topic profile learned from
the seed examples.
Given a value v, for an attribute A, (for ex-
ample v = “Politician” and A = “Occupation”),
we learn a centroid weight vector:
</bodyText>
<equation confidence="0.997842">
Cv = [w1,v, w2,v, ..., wn,v] where,
wt,v = N tft,v · log |t|EA�
</equation>
<bodyText confidence="0.963039692307692">
tft,v is the frequency of word t in the articles of People
having attribute A = v
|A |is the total number of values of attribute A
|t E A |is the total number of values of attribute A, such that
the articles of people having one of those values contain the
term t
N is the total number of People in the seed set
Given a biography article of a test name and
an attribute in question, we compute a similar
word weight vector C0 = [w0 1, w02, ..., w0n] for
the test name and measure its cosine similarity
to the centroid vector of each value of the given
attribute. Thus, the best value a∗ is chosen as:
</bodyText>
<equation confidence="0.7122125">
v∗ = wi·w1,v+w2·w2,v+....+wn.·wn,v
argmaxv
1/
w1+w2+...+wn Jw1 v+w2 v+...+wn,v
</equation>
<bodyText confidence="0.999938739130435">
Tables 3 and 4 show performance using the la-
tent document-wide-context model. We see that
this model by itself gives the top performance
on “Occupation”, outperforming the best alterna-
tive model by 9% absolute accuracy, indicating
the usefulness of implicit attribute modeling via
broad-context word frequencies.
This latent model can be further extended us-
ing the multilingual nature of Wikipedia. We
take the corresponding German pages of the train-
ing names and model the German word distribu-
tions characterizing each seed occupation. Table
4 shows that English attribute classification can be
successful using only the words in a parallel Ger-
man article. For some attributes, the performance
of latent model modeled via cross-language (noted
as latentCL) is close to that of English suggesting
potential future work by exploiting this multilin-
gual dimension.
It is interesting to note that both the transitive
model and the latent wide-context model do not
rely on the actual “Occupation” being explicitly
mentioned in the article, they still outperform ex-
</bodyText>
<page confidence="0.997807">
305
</page>
<table confidence="0.992454461538461">
Occupation Weight Vector
English
Physicist &lt;magnetic:32.7, electromagnetic:18.2, wire: 18.2, electricity: 17.7, optical:14.5, discovered:11.2&gt;
Singer &lt;song:40, hits:30.5, hit:29.6, reggae:23.6, album:17.1, francis:15.2, music:13.8, recorded:13.6, ...&gt;
Politician &lt;humphrey:367.4, soviet: 97.4, votes: 70.6, senate: 64.7, democratic: 57.2, kennedy: 55.9, ...&gt;
Painter &lt;mural:40.0, diego:14.7, paint:14.5, fresco:10.9. paintings:10.9, museum of modern art:8.83, ...&gt;
Auto racing &lt;renault:76.3, championship:32.7. schumacher:32.7, race:30.4, pole:29.1, driver:28.1 &gt;
German
Physicist &lt;faraday:25.4, chemie:7.3, vorlesungsserie:7.2, 1846:5.8, entdeckt:4.5, rotation:3.6 ...&gt;
Singer &lt;song:16.22, jamaikanischen:11.77, platz:7.3, hit: 6.7, solo¨unstler:4.5, album:4.1, widmet:4.0, ...&gt;
Politician &lt;konservativen:26.5, wahlkreis:26.5, romano:21.8, stimmen:18.6, gew¨ahlt:18.4, ...&gt;
Painter &lt;rivera:32.7, malerin:7.6, wandgem¨alde:7.3, kunst:6.75, 1940:5.8, maler:5.1, auftrag:4.5, ...&gt;
Auto racing &lt;team:29.4,mclaren:18.1,teamkollegen:18.1,sieg:11.7, meisterschaft:10.9, gegner:10.9, ...&gt;
</table>
<tableCaption confidence="0.999423">
Table 2: Sample of occupation weight vectors in English and German learned using the latent model.
</tableCaption>
<bodyText confidence="0.99975275">
plicit pattern-based and position-based models.
This implicit modeling also helps in improving the
recall of less-often directly mentioned attributes
such as a person’s “Religion”.
</bodyText>
<sectionHeader confidence="0.963782" genericHeader="method">
8 Model Combination
</sectionHeader>
<bodyText confidence="0.999307454545454">
While the pattern-based, position-based, transitive
and latent models are all stand-alone models, they
can complement each other in combination as they
provide relatively orthogonal sources of informa-
tion. To combine these models, we perform a sim-
ple backoff-based combination for each attribute
based on stand-alone model performance, and the
rows with subscript “combined” in Tables 3 and 4
shows an average 14% absolute performance gain
of the combined model relative to the improved
Ravichandran and Hovy 2002 model.
</bodyText>
<sectionHeader confidence="0.7871825" genericHeader="method">
9 Further Extensions: Reducing False
Positives
</sectionHeader>
<bodyText confidence="0.999655857142857">
Since the position-and-domain-based models will
almost always posit an answer, one of the prob-
lems is the high number of false positives yielded
by these algorithms. The following subsections in-
troduce further extensions using interesting prop-
erties of biographic attributes to reduce the effect
of false positives.
</bodyText>
<subsectionHeader confidence="0.999652">
9.1 Using Inter-Attribute Correlations
</subsectionHeader>
<bodyText confidence="0.979688857142857">
One of the ways to filter false positives is by
filtering empirically incompatible inter-attribute
pairings. The motivation here is that the at-
tributes are not independent of each other when
modeled for the same individual. For example,
P(Religion=Hindu I Nationality=India) is higher
than P(Religion=Hindu I Nationality=France) and
</bodyText>
<table confidence="0.999405555555556">
Model Fscore Acc
truth
pres
Ravichandran and Hovy, 2002 0.37 0.43
Improved RH02 Model 0.54 0.64
Position-Based Model 0.53 0.75
Combinedabove 3+trans+latent+cl 0.59 0.78
Combined + Age Dist + Corr 0.62 0.80
(+24%) (+37%)
</table>
<tableCaption confidence="0.980644">
Table 3: Average Performance of different models
across all biographic attributes
</tableCaption>
<bodyText confidence="0.9992082">
similarly we can find positive and negative cor-
relations among other attribute pairings. For im-
plementation, we consider all possible 3-tuples
of (“Nationality”, “Birthplace”, “Religion”)8 and
search on NNDB for the presence of the tuple for
any individual in the database (excluding the test
data of course). As an agressive but effective filter,
we filter the tuples for which no name in NNDB
was found containing the candidate 3-tuples. The
rows with label “combined+corr” in Table 4 and
Table 3 shows substantial performaance gains us-
ing inter-attribute correlations, such as the 7% ab-
solute average gain for Birthplace over the Section
8 combined models, and a 3% absolute gain for
Nationality and Religion.
</bodyText>
<subsectionHeader confidence="0.99973">
9.2 Using Age Distribution
</subsectionHeader>
<bodyText confidence="0.999606666666667">
Another way to filter out false positives is to con-
sider distributions on meta-attributes, for example:
while age is not explicitly extracted, we can use
the fact that age is a function of two extracted at-
tributes (&lt;Deathyear&gt;-&lt;Birthyear&gt;) and use the
age distribution to filter out false positives for
</bodyText>
<footnote confidence="0.9957235">
8The test of joint-presence between these three attributes
were used since they are strongly correlated
</footnote>
<page confidence="0.996144">
306
</page>
<figureCaption confidence="0.9957825">
Figure 5: Age distribution of famous people on the
web (from www.spock.com)
</figureCaption>
<bodyText confidence="0.999458555555556">
&lt;Birthdate&gt; and &lt;Deathdate&gt;. Based on the age
distribution for famous people9 on the web shown
in Figure 5, we can bias against unusual candi-
date lifespans and filter out completely those out-
side the range of 25-100, as most of the probabil-
ity mass is concentrated in this range. Rows with
subscript “comb + age dist” in Table 4 shows the
performance gains using this feature, yielding an
average 5% absolute accuracy gain for Birthdate.
</bodyText>
<sectionHeader confidence="0.995742" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999917222222222">
This paper has shown six successful novel ap-
proaches to biographic fact extraction using struc-
tural, transitive and latent properties of biographic
data. We first showed an improvement to the stan-
dard Ravichandran and Hovy (2002) model uti-
lizing untethered contextual pattern models, fol-
lowed by a document position and sequence-based
approach to attribute modeling.
Next we showed transitive models exploiting the
tendency for individuals occurring together in an
article to have related attribute values. We also
showed how latent models of wide document con-
text, both monolingually and translingually, can
capture facts that are not stated directly in a text.
Each of these models provide substantial per-
formance gain, and further performance gain is
achived via classifier combination. We also
showed how inter-attribution correlations can be
</bodyText>
<footnote confidence="0.98921275">
9Since all the seed and test examples were used from
nndb.com, we use the age distribution of famous people on
the web: http://blog.spock.com/2008/02/08/age-distribution-
of-people-on-the-web/
</footnote>
<table confidence="0.999981666666667">
Attribute Prec P-Rec Fscore Acc
truth
pres
BirthdateRH02 0.86 0.38 0.53 0.88
BirthdateRH02imp 0.52 0.52 0.52 0.67
Birthdaterel. posn 0.42 0.40 0.41 0.93
Birthdatecombined 0.58 0.58 0.58 0.95
Birthdatecomb+age dist 0.63 0.60 0.61 1.00
DeathdateRH02 0.80 0.19 0.30 0.36
DeathdateRH02imp 0.50 0.49 0.49 0.59
Deathdaterel. posn 0.46 0.44 0.45 0.86
Deathdatecombined 0.49 0.49 0.49 0.86
Deathdatecomb+age dist 0.51 0.49 0.50 0.86
BirthplaceRH02 0.42 0.38 0.40 0.42
BirthplaceRH02imp 0.41 0.41 0.41 0.45
Birthplacerel. posn 0.47 0.41 0.44 0.48
Birthplacecombined 0.44 0.44 0.44 0.48
Birthplacecombined+corr 0.53 0.50 0.51 0.55
OccupationRH02 0.54 0.18 0.27 0.26
OccupationRH02imp 0.38 0.34 0.36 0.48
Occupationrel. posn 0.48 0.35 0.40 0.50
Occupationtrans 0.49 0.46 0.47 0.50
Occupationlatent 0.48 0.48 0.48 0.59
OccupationlatentCL 0.48 0.48 0.48 0.54
Occupationcombined 0.48 0.48 0.48 0.59
NationalityRH02 0.40 0.25 0.31 0.27
NationalityRH02imp 0.75 0.75 0.75 0.81
Nationalityrel. posn 0.73 0.72 0.71 0.78
Nationalitytrans 0.51 0.48 0.49 0.49
Nationalitylatent 0.56 0.56 0.56 0.56
NationalitylatentCL 0.55 0.48 0.51 0.48
Nationalitycombined 0.75 0.75 0.75 0.81
Nationalitycomb+corr 0.77 0.77 0.77 0.84
GenderRH02 0.76 0.76 0.76 0.76
GenderRH02imp 0.99 0.99 0.99 0.99
Genderrel. posn 1.00 1.00 1.00 1.00
Gendertrans 0.79 0.75 0.77 0.75
Genderlatent 0.82 0.82 0.82 0.82
GenderlatentCL 0.83 0.72 0.77 0.72
Gendercombined 1.00 1.00 1.00 1.00
ReligionRH02 0.02 0.02 0.04 0.06
ReligionRH02imp 0.55 0.18 0.27 0.45
Religionrel. posn 0.49 0.24 0.32 0.73
Religiontrans 0.38 0.33 0.35 0.48
Religionlatent 0.36 0.36 0.36 0.45
ReligionlatentCL 0.30 0.26 0.28 0.22
Religioncombined 0.41 0.41 0.41 0.76
Religioncombined+corr 0.44 0.44 0.44 0.79
</table>
<tableCaption confidence="0.995628">
Table 4: Attribute-wise performance comparison
</tableCaption>
<bodyText confidence="0.9890154">
of all the models across several biographic at-
tributes.
modeled to filter unlikely attribute combinations,
and how models of functions over attributes, such
as deathdate-birthdate distributions, can further
constrain the candidate space. These approaches
collectively achieve 80% average accuracy on a
test set of 7 biographic attribute types, yielding a
37% absolute accuracy gain relative to a standard
algorithm on the same data.
</bodyText>
<page confidence="0.997164">
307
</page>
<sectionHeader confidence="0.996339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999911039215687">
E. Agichtein and L. Gravano. 2000. Snowball: ex-
tracting relations from large plain-text collections.
Proceedings of ICDL, pages 85–94.
E. Alfonseca, P. Castells, M. Okumura, and M. Ruiz-
Casado. 2006. A rote extractor with edit distance-
based generalisation and multi-corpora precision
calculation. Proceedings of COLING-ACL, pages
9–16.
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
semeval-2007 weps evaluation: Establishing a
benchmark for the web people search task. In Pro-
ceedings of SemEval, pages 64–69.
S. Auer and J. Lehmann. 2007. What have Innsbruck
and Leipzig in common? Extracting Semantics from
Wiki Content. Proceedings of ESWC, pages 503–
517.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. Proceedings of COLING-ACL, pages 79–
85.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. Pro-
ceedings of EACL, pages 3–7.
J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000.
Generating personal profiles. The International
Conference On MT And Multilingual NLP.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. Proceedings of
EMNLP-CoNLL, pages 708–716.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrat-
ing probabilistic extraction models and data mining
to discover relations and patterns in text. Proceed-
ings of HLT-NAACL, pages 296–303.
E. Filatova and J. Prager. 2005. Tell me what you do
and I’ll tell you what you are: Learning occupation-
related activities for biographies. Proceedings of
HLT-EMNLP, pages 113–120.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539–545.
V. Jijkoun, M. de Rijke, and J. Mur. 2004. Infor-
mation extraction for question answering: improv-
ing recall through syntactic patterns. Proceedings of
COLING, page 1284.
G.S. Mann and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings of
CoNLL, pages 33–40.
G.S. Mann and D. Yarowsky. 2005. Multi-field in-
formation extraction and cross-document fusion. In
Proceedings of ACL, pages 483–490.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. Proceedings of HLT-
NAACL companion volume, pages 70–72.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the World Wide
Web of Facts Step one: The One-Million Fact Ex-
traction Challenge. Proceedings of AAAI, pages
1400–1405.
D. Ravichandran and E. Hovy. 2002. Learning sur-
face text patterns for a question answering system.
Proceedings ofACL, pages 41–47.
Y. Ravin and Z. Kazi. 1999. Is Hillary Rodham Clin-
ton the President? Disambiguating Names across
Documents. Proceedings of ACL.
M. Remy. 2002. Wikipedia: The Free Encyclopedia.
Online Information Review Year, 26(6).
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. Proceedings of AAAI,
pages 1044–1049.
M. Ruiz-Casado, E. Alfonseca, and P. Castells.
2005. Automatic extraction of semantic relation-
ships for wordnet by means of pattern learning from
wikipedia. Proceedings of NLDB 2005.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2006.
From Wikipedia to semantic relationships: a semi-
automated annotation approach. Proceedings of
ESWC.
B. Schiffman, I. Mani, and K.J. Concepcion. 2001.
Producing biographical summaries: combining lin-
guistic knowledge with corpus statistics. Proceed-
ings of ACL, pages 458–465.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proceedings of EMNLP,
pages 14–21.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text. Proceedings of
ANLP, pages 202–208.
C. Walker, S. Strassel, J. Medero, and K. Maeda. 2006.
Ace 2005 multilingual training corpus. Linguistic
Data Consortium.
R. Weischedel, J. Xu, and A. Licuanan. 2004. A
Hybrid Approach to Answering Biographical Ques-
tions. New Directions In Question Answering, pages
59–70.
M. Wick, A. Culotta, and A. McCallum. 2006. Learn-
ing field compatibilities to extract database records
from unstructured text. In Proceedings of EMNLP,
pages 603–611.
L. Zhou, M. Ticrea, and E. Hovy. 2004. Multidoc-
ument biography summarization. Proceedings of
EMNLP, pages 434–441.
</reference>
<page confidence="0.998575">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957585">
<title confidence="0.999858">Structural, Transitive and Latent Models for Biographic Fact Extraction</title>
<author confidence="0.997282">Garera Yarowsky</author>
<affiliation confidence="0.987379">Department of Computer Science, Johns Hopkins University Human Language Technology Center of Excellence</affiliation>
<address confidence="0.995111">Baltimore MD, USA</address>
<abstract confidence="0.999102307692308">This paper presents six novel approaches to biographic fact extraction that model structural, transitive and latent properties of biographical data. The ensemble of these proposed models substantially outperforms standard pattern-based biographic fact extraction methods and performance is further improved by modeling inter-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>Proceedings of ICDL,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="4894" citStr="Agichtein and Gravano, 2000" startWordPosition="715" endWordPosition="718">ces and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do </context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: extracting relations from large plain-text collections. Proceedings of ICDL, pages 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Alfonseca</author>
<author>P Castells</author>
<author>M Okumura</author>
<author>M RuizCasado</author>
</authors>
<title>A rote extractor with edit distancebased generalisation and multi-corpora precision calculation.</title>
<date>2006</date>
<booktitle>Proceedings of COLING-ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="5019" citStr="Alfonseca et al., 2006" startWordPosition="735" endWordPosition="738"> more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as “Birthdate”, “Deathdate</context>
<context position="7365" citStr="Alfonseca et al., 2006" startWordPosition="1088" endWordPosition="1091">e applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. 301 4 Contextual Pattern-Based Model A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be learned using seed examples of the attribute in question and, there has been a plethora of work in the seed-based bootstrapping literature which addresses this problem (Ravichandran and Hovy, 2002; Thelen and Riloff, 2002; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006) Thus for our baseline we implemented a standard Ravichandran and Hovy (2002) pattern learning model using 100 seed2 examples from an online biographic database called NNDB (http://www.nndb.com) for each of the biographic attributes: “Birthdate”, “Birthplace”, “Deathdate”, “Gender”, “Nationality”, “Occupation” and “Religion”. Given the seed pairs, patterns for each attribute were learned by searching for seed &lt;Name,Attribute Value&gt; pairs in the Wikipedia page and extracting the left, middle and right contexts as various contextual patterns3. While the biographic text was o</context>
</contexts>
<marker>Alfonseca, Castells, Okumura, RuizCasado, 2006</marker>
<rawString>E. Alfonseca, P. Castells, M. Okumura, and M. RuizCasado. 2006. A rote extractor with edit distancebased generalisation and multi-corpora precision calculation. Proceedings of COLING-ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>64--69</pages>
<contexts>
<context position="1209" citStr="Artiles et al., 2007" startWordPosition="168" endWordPosition="171">r-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes. 1 Introduction Extracting biographic facts such as “Birthdate”, “Occupation”, “Nationality”, etc. is a critical step for advancing the state of the art in information processing and retrieval. An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature (Mann and Yarowsky, 2003; Artiles et al., 2007, Cucerzan, 2007). While biographic facts are certainly useful for disambiguating person names, they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as Britannica, Wikipedia, etc. Such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes. Biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two people (Auer and Lehm</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>J. Artiles, J. Gonzalo, and S. Sekine. 2007. The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task. In Proceedings of SemEval, pages 64–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Auer</author>
<author>J Lehmann</author>
</authors>
<title>What have Innsbruck and Leipzig in common? Extracting Semantics from Wiki Content.</title>
<date>2007</date>
<booktitle>Proceedings of ESWC,</booktitle>
<pages>503--517</pages>
<contexts>
<context position="1819" citStr="Auer and Lehmann, 2007" startWordPosition="258" endWordPosition="261"> et al., 2007, Cucerzan, 2007). While biographic facts are certainly useful for disambiguating person names, they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as Britannica, Wikipedia, etc. Such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes. Biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two people (Auer and Lehmann, 2007). Figure 1: Goal: extracting attribute-value biographic fact pairs from biographic free-text While there are a large quantity of biographic texts available online, there are only a few biographic fact databases available1, and most of them have been created manually, are incomplete and are available primarily in English. This work presents multiple novel approaches for automatically extracting biographic facts such as “Birthdate”, “Occupation”, “Nationality”, and “Religion”, making use of diverse sources of information present in biographies. In particular, we have proposed and evaluated the f</context>
</contexts>
<marker>Auer, Lehmann, 2007</marker>
<rawString>S. Auer and J. Lehmann. 2007. What have Innsbruck and Leipzig in common? Extracting Semantics from Wiki Content. Proceedings of ESWC, pages 503– 517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-Based CrossDocument Coreferencing Using the Vector Space Model.</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL,</booktitle>
<pages>79--85</pages>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-Based CrossDocument Coreferencing Using the Vector Space Model. Proceedings of COLING-ACL, pages 79– 85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>Proceedings of EACL,</booktitle>
<pages>3--7</pages>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. Proceedings of EACL, pages 3–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>S Nirenburg</author>
<author>H Molina-Salgado</author>
</authors>
<title>Generating personal profiles.</title>
<date>2000</date>
<booktitle>The International Conference On MT And Multilingual NLP.</booktitle>
<contexts>
<context position="4337" citStr="Cowie et al., 2000" startWordPosition="626" endWordPosition="629">du &gt; has a higher probability than a tuple consisting of &lt; “Nationality” = France, “Religion” = Hindu &gt;. 6. Learning distributions over functions of attributes, for example, using an age distribution to filter tuples containing improbable &lt;deathyear&gt;-&lt;birthyear&gt; lifespan values. We propose and evaluate techniques for exploiting all of the above classes of information in the next sections. 2 Related Work The literature for biography extraction falls into two major classes. The first one deals with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yar</context>
</contexts>
<marker>Cowie, Nirenburg, Molina-Salgado, 2000</marker>
<rawString>J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000. Generating personal profiles. The International Conference On MT And Multilingual NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>Proceedings of EMNLP-CoNLL,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="1226" citStr="Cucerzan, 2007" startWordPosition="172" endWordPosition="173">ns and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes. 1 Introduction Extracting biographic facts such as “Birthdate”, “Occupation”, “Nationality”, etc. is a critical step for advancing the state of the art in information processing and retrieval. An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature (Mann and Yarowsky, 2003; Artiles et al., 2007, Cucerzan, 2007). While biographic facts are certainly useful for disambiguating person names, they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as Britannica, Wikipedia, etc. Such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes. Biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two people (Auer and Lehmann, 2007). Figur</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. Proceedings of EMNLP-CoNLL, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>A McCallum</author>
<author>J Betz</author>
</authors>
<title>Integrating probabilistic extraction models and data mining to discover relations and patterns in text.</title>
<date>2006</date>
<booktitle>Proceedings of HLT-NAACL,</booktitle>
<pages>296--303</pages>
<contexts>
<context position="5155" citStr="Culotta et al. (2006)" startWordPosition="758" endWordPosition="761">e of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as “Birthdate”, “Deathdate”, “Birthplace” and “Occupation” have been shown to work well in the pattern-learning framework (Mann and Yarowsky, 2005; Alfonesca, 200</context>
</contexts>
<marker>Culotta, McCallum, Betz, 2006</marker>
<rawString>A. Culotta, A. McCallum, and J. Betz. 2006. Integrating probabilistic extraction models and data mining to discover relations and patterns in text. Proceedings of HLT-NAACL, pages 296–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>J Prager</author>
</authors>
<title>Tell me what you do and I’ll tell you what you are: Learning occupationrelated activities for biographies.</title>
<date>2005</date>
<booktitle>Proceedings of HLT-EMNLP,</booktitle>
<pages>113--120</pages>
<marker>Filatova, Prager, 2005</marker>
<rawString>E. Filatova and J. Prager. 2005. Tell me what you do and I’ll tell you what you are: Learning occupationrelated activities for biographies. Proceedings of HLT-EMNLP, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="4826" citStr="Hearst, 1992" startWordPosition="706" endWordPosition="707">s with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography cl</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
<author>J Mur</author>
</authors>
<title>Information extraction for question answering: improving recall through syntactic patterns.</title>
<date>2004</date>
<booktitle>Proceedings of COLING,</booktitle>
<pages>1284</pages>
<marker>Jijkoun, de Rijke, Mur, 2004</marker>
<rawString>V. Jijkoun, M. de Rijke, and J. Mur. 2004. Information extraction for question answering: improving recall through syntactic patterns. Proceedings of COLING, page 1284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1187" citStr="Mann and Yarowsky, 2003" startWordPosition="164" endWordPosition="167">improved by modeling inter-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes. 1 Introduction Extracting biographic facts such as “Birthdate”, “Occupation”, “Nationality”, etc. is a critical step for advancing the state of the art in information processing and retrieval. An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature (Mann and Yarowsky, 2003; Artiles et al., 2007, Cucerzan, 2007). While biographic facts are certainly useful for disambiguating person names, they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as Britannica, Wikipedia, etc. Such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes. Biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two</context>
<context position="4948" citStr="Mann and Yarowsky, 2003" startWordPosition="723" endWordPosition="726">et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a fe</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>G.S. Mann and D. Yarowsky. 2003. Unsupervised personal name disambiguation. In Proceedings of CoNLL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Multi-field information extraction and cross-document fusion.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="4995" citStr="Mann and Yarowsky, 2005" startWordPosition="731" endWordPosition="734">l., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as</context>
<context position="7341" citStr="Mann and Yarowsky, 2005" startWordPosition="1084" endWordPosition="1087">ut in practice they can be applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. 301 4 Contextual Pattern-Based Model A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be learned using seed examples of the attribute in question and, there has been a plethora of work in the seed-based bootstrapping literature which addresses this problem (Ravichandran and Hovy, 2002; Thelen and Riloff, 2002; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006) Thus for our baseline we implemented a standard Ravichandran and Hovy (2002) pattern learning model using 100 seed2 examples from an online biographic database called NNDB (http://www.nndb.com) for each of the biographic attributes: “Birthdate”, “Birthplace”, “Deathdate”, “Gender”, “Nationality”, “Occupation” and “Religion”. Given the seed pairs, patterns for each attribute were learned by searching for seed &lt;Name,Attribute Value&gt; pairs in the Wikipedia page and extracting the left, middle and right contexts as various contextual patterns3. While t</context>
<context position="8876" citStr="Mann and Yarowsky 2005" startWordPosition="1325" endWordPosition="1328">des a cleaner set of gold truth, and is similar to an approach utilizing trained annotators for marking up and extracting the factual information in a standard format. For consistency, only the people names whose articles occur in Wikipedia where selected as part of seed and test sets. Given the attribute values of the seed names and their text articles, the probability of a relationship r(Attribute Name), given the surrounding context “A1 p A2 q A3”, where p and q are &lt;NAME&gt; and &lt;Attrib Val&gt; respectively, is given using the rote extractor model probability as in (Ravichandran and Hovy, 2002; Mann and Yarowsky 2005): 2The seed examples were chosen randomly, with a bias against duplicate attribute values to increase training diversity. Both the seed and test names and data will be made available online to the research community for replication and extension. 3We implemented a noisy model of coreference resolution by resolving any gender-correct pronoun used in the Wikipedia page to the title person name of the article. Gender is also extracted automatically as a biographic attribute. Ex,y Er c(A1xA2yA3) P(r(p, q) |A1pA2qA3) = Ex z c(A1xA2zA3) Thus, the probability for each contextual pattern is based on h</context>
<context position="11378" citStr="Mann and Yarowsky, 2005" startWordPosition="1723" endWordPosition="1726"> domain models of attributes include lists of acceptable values (such as lists of places, occupations and religions) and structural constraints such as possible date formats for “Birthdate” and “Deathdate”. The rows with subscript “RH02”in Table 4 shows the performance of this Ravichandran and Hovy (2002) model with additional attribute domain modeling for each attribute, and Table 3 shows the average performance across all attributes. 5 Partially Untethered Templatic Contextual Patterns The pattern-learning literature for fact extraction often consists of patterns with a “hook” and “target” (Mann and Yarowsky, 2005). For example, in the pattern “&lt;Name&gt; was born in &lt;Birthplace&gt;”, “&lt;NAME&gt;” is the hook and “&lt;Birthplace&gt;” is the target. The disadvantage of this approach is that the intervening duallytethered patterns can be quite long and highly variable, such as “&lt;NAME&gt; was highly influF-scorebplace = 302 Figure 2: Distribution of the observed document mentions of Deathdate, Nationality and Religion. ential in his role as &lt;Occupation&gt;”. We overcome this problem by modeling partially untethered variable-length ngram patterns adjacent to only the target, with the only constraint being that the hook entity app</context>
</contexts>
<marker>Mann, Yarowsky, 2005</marker>
<rawString>G.S. Mann and D. Yarowsky. 2005. Multi-field information extraction and cross-document fusion. In Proceedings of ACL, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>K McKeown</author>
</authors>
<title>References to named entities: a corpus study.</title>
<date>2003</date>
<booktitle>Proceedings of HLTNAACL companion volume,</booktitle>
<pages>70--72</pages>
<marker>Nenkova, McKeown, 2003</marker>
<rawString>A. Nenkova and K. McKeown. 2003. References to named entities: a corpus study. Proceedings of HLTNAACL companion volume, pages 70–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Organizing and searching the World Wide Web of Facts Step one: The One-Million Fact Extraction Challenge.</title>
<date>2006</date>
<booktitle>Proceedings of AAAI,</booktitle>
<pages>1400--1405</pages>
<contexts>
<context position="5040" citStr="Pasca et al., 2006" startWordPosition="739" endWordPosition="742">ass deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as “Birthdate”, “Deathdate”, “Birthplace” and “</context>
<context position="7386" citStr="Pasca et al., 2006" startWordPosition="1092" endWordPosition="1095">e of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. 301 4 Contextual Pattern-Based Model A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be learned using seed examples of the attribute in question and, there has been a plethora of work in the seed-based bootstrapping literature which addresses this problem (Ravichandran and Hovy, 2002; Thelen and Riloff, 2002; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006) Thus for our baseline we implemented a standard Ravichandran and Hovy (2002) pattern learning model using 100 seed2 examples from an online biographic database called NNDB (http://www.nndb.com) for each of the biographic attributes: “Birthdate”, “Birthplace”, “Deathdate”, “Gender”, “Nationality”, “Occupation” and “Religion”. Given the seed pairs, patterns for each attribute were learned by searching for seed &lt;Name,Attribute Value&gt; pairs in the Wikipedia page and extracting the left, middle and right contexts as various contextual patterns3. While the biographic text was obtained from Wikipedi</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Organizing and searching the World Wide Web of Facts Step one: The One-Million Fact Extraction Challenge. Proceedings of AAAI, pages 1400–1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>Proceedings ofACL,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="2821" citStr="Ravichandran and Hovy (2002)" startWordPosition="399" endWordPosition="402">tomatically extracting biographic facts such as “Birthdate”, “Occupation”, “Nationality”, and “Religion”, making use of diverse sources of information present in biographies. In particular, we have proposed and evaluated the following 6 distinct original approaches to this 1E.g.: http://www.nndb.com, http://www.biography.com, Infoboxes in Wikipedia Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 300 task with large collective empirical gains: 1. An improvement to the Ravichandran and Hovy (2002) algorithm based on Partially Untethered Contextual Pattern Models 2. Learning a position-based model using absolute and relative positions and sequential order of hypotheses that satisfy the domain model. For example, “Deathdate” very often appears after “Birthdate” in a biography. 3. Using transitive models over attributes via co-occurring entities. For example, other people mentioned person’s biography page tend to have similar attributes such as occupation (See Figure 4). 4. Using latent wide-document-context models to detect attributes that may not be mentioned directly in the article (e.</context>
<context position="4923" citStr="Ravichandran and Hovy, 2002" startWordPosition="719" endWordPosition="722"> a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextua</context>
<context position="7291" citStr="Ravichandran and Hovy, 2002" startWordPosition="1076" endWordPosition="1079"> motivate each technique using one or two attributes but in practice they can be applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. 301 4 Contextual Pattern-Based Model A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be learned using seed examples of the attribute in question and, there has been a plethora of work in the seed-based bootstrapping literature which addresses this problem (Ravichandran and Hovy, 2002; Thelen and Riloff, 2002; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006) Thus for our baseline we implemented a standard Ravichandran and Hovy (2002) pattern learning model using 100 seed2 examples from an online biographic database called NNDB (http://www.nndb.com) for each of the biographic attributes: “Birthdate”, “Birthplace”, “Deathdate”, “Gender”, “Nationality”, “Occupation” and “Religion”. Given the seed pairs, patterns for each attribute were learned by searching for seed &lt;Name,Attribute Value&gt; pairs in the Wikipedia page and extracting the left, middle and right</context>
<context position="8851" citStr="Ravichandran and Hovy, 2002" startWordPosition="1320" endWordPosition="1324">tracted from NNDB which provides a cleaner set of gold truth, and is similar to an approach utilizing trained annotators for marking up and extracting the factual information in a standard format. For consistency, only the people names whose articles occur in Wikipedia where selected as part of seed and test sets. Given the attribute values of the seed names and their text articles, the probability of a relationship r(Attribute Name), given the surrounding context “A1 p A2 q A3”, where p and q are &lt;NAME&gt; and &lt;Attrib Val&gt; respectively, is given using the rote extractor model probability as in (Ravichandran and Hovy, 2002; Mann and Yarowsky 2005): 2The seed examples were chosen randomly, with a bias against duplicate attribute values to increase training diversity. Both the seed and test names and data will be made available online to the research community for replication and extension. 3We implemented a noisy model of coreference resolution by resolving any gender-correct pronoun used in the Wikipedia page to the title person name of the article. Gender is also extracted automatically as a biographic attribute. Ex,y Er c(A1xA2yA3) P(r(p, q) |A1pA2qA3) = Ex z c(A1xA2zA3) Thus, the probability for each context</context>
<context position="11060" citStr="Ravichandran and Hovy (2002)" startWordPosition="1677" endWordPosition="1680">es for which the value of a given attribute is also explictly stated in the article. This is denoted as: # people with bplace correctly extracted Acctruth pres = # of people with true bplace stated in article We further applied a domain model for each attribute to filter noisy targets extracted from lexical patterns. Our domain models of attributes include lists of acceptable values (such as lists of places, occupations and religions) and structural constraints such as possible date formats for “Birthdate” and “Deathdate”. The rows with subscript “RH02”in Table 4 shows the performance of this Ravichandran and Hovy (2002) model with additional attribute domain modeling for each attribute, and Table 3 shows the average performance across all attributes. 5 Partially Untethered Templatic Contextual Patterns The pattern-learning literature for fact extraction often consists of patterns with a “hook” and “target” (Mann and Yarowsky, 2005). For example, in the pattern “&lt;Name&gt; was born in &lt;Birthplace&gt;”, “&lt;NAME&gt;” is the hook and “&lt;Birthplace&gt;” is the target. The disadvantage of this approach is that the intervening duallytethered patterns can be quite long and highly variable, such as “&lt;NAME&gt; was highly influF-scorebp</context>
<context position="22390" citStr="Ravichandran and Hovy 2002" startWordPosition="3418" endWordPosition="3421">roving the recall of less-often directly mentioned attributes such as a person’s “Religion”. 8 Model Combination While the pattern-based, position-based, transitive and latent models are all stand-alone models, they can complement each other in combination as they provide relatively orthogonal sources of information. To combine these models, we perform a simple backoff-based combination for each attribute based on stand-alone model performance, and the rows with subscript “combined” in Tables 3 and 4 shows an average 14% absolute performance gain of the combined model relative to the improved Ravichandran and Hovy 2002 model. 9 Further Extensions: Reducing False Positives Since the position-and-domain-based models will almost always posit an answer, one of the problems is the high number of false positives yielded by these algorithms. The following subsections introduce further extensions using interesting properties of biographic attributes to reduce the effect of false positives. 9.1 Using Inter-Attribute Correlations One of the ways to filter false positives is by filtering empirically incompatible inter-attribute pairings. The motivation here is that the attributes are not independent of each other when</context>
<context position="25343" citStr="Ravichandran and Hovy (2002)" startWordPosition="3875" endWordPosition="3878">ution for famous people9 on the web shown in Figure 5, we can bias against unusual candidate lifespans and filter out completely those outside the range of 25-100, as most of the probability mass is concentrated in this range. Rows with subscript “comb + age dist” in Table 4 shows the performance gains using this feature, yielding an average 5% absolute accuracy gain for Birthdate. 10 Conclusion This paper has shown six successful novel approaches to biographic fact extraction using structural, transitive and latent properties of biographic data. We first showed an improvement to the standard Ravichandran and Hovy (2002) model utilizing untethered contextual pattern models, followed by a document position and sequence-based approach to attribute modeling. Next we showed transitive models exploiting the tendency for individuals occurring together in an article to have related attribute values. We also showed how latent models of wide document context, both monolingually and translingually, can capture facts that are not stated directly in a text. Each of these models provide substantial performance gain, and further performance gain is achived via classifier combination. We also showed how inter-attribution co</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. Proceedings ofACL, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ravin</author>
<author>Z Kazi</author>
</authors>
<title>Is Hillary Rodham Clinton the President? Disambiguating Names across Documents.</title>
<date>1999</date>
<booktitle>Proceedings of ACL.</booktitle>
<marker>Ravin, Kazi, 1999</marker>
<rawString>Y. Ravin and Z. Kazi. 1999. Is Hillary Rodham Clinton the President? Disambiguating Names across Documents. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Remy</author>
</authors>
<title>Wikipedia: The Free Encyclopedia.</title>
<date>2002</date>
<journal>Online Information Review Year,</journal>
<volume>26</volume>
<issue>6</issue>
<marker>Remy, 2002</marker>
<rawString>M. Remy. 2002. Wikipedia: The Free Encyclopedia. Online Information Review Year, 26(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>Proceedings of AAAI,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="4840" citStr="Riloff, 1996" startWordPosition="708" endWordPosition="709">ying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. Proceedings of AAAI, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ruiz-Casado</author>
<author>E Alfonseca</author>
<author>P Castells</author>
</authors>
<title>Automatic extraction of semantic relationships for wordnet by means of pattern learning from wikipedia.</title>
<date>2005</date>
<booktitle>Proceedings of NLDB</booktitle>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2005. Automatic extraction of semantic relationships for wordnet by means of pattern learning from wikipedia. Proceedings of NLDB 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ruiz-Casado</author>
<author>E Alfonseca</author>
<author>P Castells</author>
</authors>
<title>From Wikipedia to semantic relationships: a semiautomated annotation approach.</title>
<date>2006</date>
<booktitle>Proceedings of ESWC.</booktitle>
<contexts>
<context position="5272" citStr="Ruiz-Casado et al. (2006)" startWordPosition="774" endWordPosition="777">rts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as “Birthdate”, “Deathdate”, “Birthplace” and “Occupation” have been shown to work well in the pattern-learning framework (Mann and Yarowsky, 2005; Alfonesca, 2006; Pasca et al., 2006). Secondly, there is a general lack of work that attempts to utilize the typical information se</context>
</contexts>
<marker>Ruiz-Casado, Alfonseca, Castells, 2006</marker>
<rawString>M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2006. From Wikipedia to semantic relationships: a semiautomated annotation approach. Proceedings of ESWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schiffman</author>
<author>I Mani</author>
<author>K J Concepcion</author>
</authors>
<title>Producing biographical summaries: combining linguistic knowledge with corpus statistics.</title>
<date>2001</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>458--465</pages>
<contexts>
<context position="4361" citStr="Schiffman et al., 2001" startWordPosition="630" endWordPosition="633">obability than a tuple consisting of &lt; “Nationality” = France, “Religion” = Hindu &gt;. 6. Learning distributions over functions of attributes, for example, using an age distribution to filter tuples containing improbable &lt;deathyear&gt;-&lt;birthyear&gt; lifespan values. We propose and evaluate techniques for exploiting all of the above classes of information in the next sections. 2 Related Work The literature for biography extraction falls into two major classes. The first one deals with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et </context>
</contexts>
<marker>Schiffman, Mani, Concepcion, 2001</marker>
<rawString>B. Schiffman, I. Mani, and K.J. Concepcion. 2001. Producing biographical summaries: combining linguistic knowledge with corpus statistics. Proceedings of ACL, pages 458–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>14--21</pages>
<contexts>
<context position="4865" citStr="Thelen and Riloff, 2002" startWordPosition="710" endWordPosition="714">cting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts lik</context>
<context position="7316" citStr="Thelen and Riloff, 2002" startWordPosition="1080" endWordPosition="1083">g one or two attributes but in practice they can be applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. 301 4 Contextual Pattern-Based Model A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be learned using seed examples of the attribute in question and, there has been a plethora of work in the seed-based bootstrapping literature which addresses this problem (Ravichandran and Hovy, 2002; Thelen and Riloff, 2002; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006) Thus for our baseline we implemented a standard Ravichandran and Hovy (2002) pattern learning model using 100 seed2 examples from an online biographic database called NNDB (http://www.nndb.com) for each of the biographic attributes: “Birthdate”, “Birthplace”, “Deathdate”, “Gender”, “Nationality”, “Occupation” and “Religion”. Given the seed pairs, patterns for each attribute were learned by searching for seed &lt;Name,Attribute Value&gt; pairs in the Wikipedia page and extracting the left, middle and right contexts as various cont</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of EMNLP, pages 14–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Wacholder</author>
<author>Y Ravin</author>
<author>M Choi</author>
</authors>
<title>Disambiguation of proper names in text.</title>
<date>1997</date>
<booktitle>Proceedings of ANLP,</booktitle>
<pages>202--208</pages>
<marker>Wacholder, Ravin, Choi, 1997</marker>
<rawString>N. Wacholder, Y. Ravin, and M. Choi. 1997. Disambiguation of proper names in text. Proceedings of ANLP, pages 202–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Walker</author>
<author>S Strassel</author>
<author>J Medero</author>
<author>K Maeda</author>
</authors>
<title>multilingual training corpus. Linguistic Data Consortium.</title>
<date>2006</date>
<journal>Ace</journal>
<marker>Walker, Strassel, Medero, Maeda, 2006</marker>
<rawString>C. Walker, S. Strassel, J. Medero, and K. Maeda. 2006. Ace 2005 multilingual training corpus. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>J Xu</author>
<author>A Licuanan</author>
</authors>
<title>A Hybrid Approach to Answering Biographical Questions. New Directions In Question Answering,</title>
<date>2004</date>
<pages>59--70</pages>
<marker>Weischedel, Xu, Licuanan, 2004</marker>
<rawString>R. Weischedel, J. Xu, and A. Licuanan. 2004. A Hybrid Approach to Answering Biographical Questions. New Directions In Question Answering, pages 59–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wick</author>
<author>A Culotta</author>
<author>A McCallum</author>
</authors>
<title>Learning field compatibilities to extract database records from unstructured text.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>603--611</pages>
<marker>Wick, Culotta, McCallum, 2006</marker>
<rawString>M. Wick, A. Culotta, and A. McCallum. 2006. Learning field compatibilities to extract database records from unstructured text. In Proceedings of EMNLP, pages 603–611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>M Ticrea</author>
<author>E Hovy</author>
</authors>
<title>Multidocument biography summarization.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>434--441</pages>
<contexts>
<context position="4381" citStr="Zhou et al., 2004" startWordPosition="634" endWordPosition="637">onsisting of &lt; “Nationality” = France, “Religion” = Hindu &gt;. 6. Learning distributions over functions of attributes, for example, using an age distribution to filter tuples containing improbable &lt;deathyear&gt;-&lt;birthyear&gt; lifespan values. We propose and evaluate techniques for exploiting all of the above classes of information in the next sections. 2 Related Work The literature for biography extraction falls into two major classes. The first one deals with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and </context>
</contexts>
<marker>Zhou, Ticrea, Hovy, 2004</marker>
<rawString>L. Zhou, M. Ticrea, and E. Hovy. 2004. Multidocument biography summarization. Proceedings of EMNLP, pages 434–441.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>