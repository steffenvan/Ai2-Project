<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000546">
<title confidence="0.9970815">
[LVIC-LIMSI]: Using Syntactic Features and Multi-polarity Words for
Sentiment Analysis in Twitter
</title>
<author confidence="0.588006">
Morgane Marchand&apos;,&apos;, Alexandru Lucian Ginscal, Romaric Besanc¸on1, Olivier Mesnardl
</author>
<address confidence="0.7915414">
(1) CEA-LIST, DIASI, LVIC
CEA SACLAY - Nano-INNOV - Bt. 861 - Point courrier 173
91191 Gif-sur-Yvette Cedex, France
(2) LIMSI-CNRS
Bat 508, BP133,91403 Orsay Cedex
</address>
<email confidence="0.9892825">
morgane.marchand@cea.fr; alexandru.ginsca@cea.fr
romaric.besancon@cea.fr; olivier.mesnard@cea.fr
</email>
<sectionHeader confidence="0.995537" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999484181818182">
This paper presents the contribution of our
team at task 2 of SemEval 2013: Sentiment
Analysis in Twitter. We submitted a con-
strained run for each of the two subtasks. In the
Contextual Polarity Disambiguation subtask,
we use a sentiment lexicon approach combined
with polarity shift detection and tree kernel
based classifiers. In the Message Polarity Clas-
sification subtask, we focus on the influence
of domain information on sentiment classifica-
tion.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997">
In the past decade, new forms of communication,
such as microblogging and text messaging have
emerged and became ubiquitous. These short mes-
sages are often used to share opinions and sentiments.
The Sentiment Analysis in Twitter task promotes re-
search that will lead to a better understanding of how
sentiment is conveyed in tweets and texts. In this
paper, we describe our contribution at task 2 of Se-
mEval 2013 (Wilson et al., 2013). For the Contextual
Polarity Disambiguation subtask, covered in section
2, we use a system that combines a lexicon based
approach to sentiment detection with two types of
supervised learning methods, one used for polarity
shift identification and one for tweet segment classi-
fication in the absence of lexicon words. The third
section presents the Message Polarity Classification
subtask. We focus here on the influence of domain
information on sentiment classification by detecting
words that change their polarity across domains.
</bodyText>
<sectionHeader confidence="0.9990355" genericHeader="method">
2 Task A: Contextual Polarity
Disambiguation
</sectionHeader>
<bodyText confidence="0.999967714285714">
In this section we present our approach for the con-
textual polarity disambiguation task in which, given
a message containing a marked instance of a word or
a phrase, the system has to determine whether that
instance is positive, negative or neutral in that con-
text. For this task, we submitted a single run using
only the tweets provided by the organizers.
</bodyText>
<subsectionHeader confidence="0.969858">
2.1 System description
</subsectionHeader>
<bodyText confidence="0.999985923076923">
Based on the predominant strategy, sentiment anal-
ysis systems can be divided into those that focus on
sentiment lexicons together with a set of rules and
those that rely on machine learning techniques. For
this task, we use a mixed approach in which we first
filter the tweets based on the occurrences of words
from a sentiment lexicon and then apply different
supervised learning methods on the grounds of this
initial classification. In Figure 1 we detail the work-
flow of our system. We use the + , − and * symbols
to denote a positive, negative and neutral tweet seg-
ment, respectively. Also, we use the a --� b notation
when referring to a polarity shift from a to b.
</bodyText>
<subsectionHeader confidence="0.560986">
2.1.1 Data preprocessing
</subsectionHeader>
<bodyText confidence="0.99992775">
The language used in Twitter presents some partic-
ularities, such as the use of hashtags or user mentions.
In order to maximize the efficiency of language pro-
cessing methods, such as lemmatization and syntactic
parsing, we perform several normalization steps. We
remove the # symbol, all @ mentions and links and
perform lower case conversion. Also, if a vowel is
repeated more than 3 times in a word, we reduce it to
</bodyText>
<footnote confidence="0.424770666666667">
418
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 418–424, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</footnote>
<figureCaption confidence="0.999656">
Figure 1: Contextual polarity disambiguation task system description
</figureCaption>
<bodyText confidence="0.99671175">
a single occurrence and we reduce multiple consecu-
tive punctuation marks to a single one. Finally, we
lemmatize the normalized text.
Emoticons have been successfully used as senti-
ment indicators in tweets (Davidov et al., 2010). In
our approach, we map a set of positive emoticons to
the word good and a set of negative emoticons to the
word bad. We use the following sets of emoticons:
</bodyText>
<listItem confidence="0.999800666666667">
• Positive emoticons::) , :-) , :D , =) , :’) , :o) , :P
, &gt;:) , :”&gt;, &gt;:�, &lt;3 , ;&gt;, ;) , ;-) , ;&gt;, (: , (;
• Negative emoticons::( , : ( , :-( , :’( , :/ , :&lt;, ;(
</listItem>
<bodyText confidence="0.998443555555556">
Traits of informal language have been used as fea-
tures in Twitter sentiment classification tasks (Go
et al., 2009). In order to avoid the loss of possi-
ble useful information, we keep record of the per-
formed normalizations as binary features associated
to a tweet segment. We retain the following set of fea-
tures: hasPositiveEmoticon, hasNegativeEmoticon,
hasHashtag, hasAtSign, hasConsecutivePunctuation,
hasConsecutiveVowels, hasUpperCaseWords.
</bodyText>
<sectionHeader confidence="0.734184" genericHeader="method">
2.1.2 Classification methods
</sectionHeader>
<bodyText confidence="0.99972975">
In a first step, we select tweet segments that con-
tain at least one word from a lexicon and assign to
it the polarity of that word. If there are more than
one sentiment words with different polarities in the
segment, we keep the most frequent polarity and in
the few cases where there is an equal number of posi-
tive and negative words, we take the polarity of the
last one. Next, we look for negation indicators (e.g.
not, ’t) using a set of words and rules and replace
them with the NEG token. We then identify instances
where there is a shift between the polarity predicted
from the lexicon and the one from the ground truth.
In order to account for the unbalanced datasets(e.g.
192 instances where there is a + —* − shift and 3188
where the positive instance was correctly identified
from the lexicon) we use cost sensitive classifiers. We
define a cost matrix in which the cost of the classifier
making a false positive error is three times higher
than a false negative error. Using this approach we
guide the classifier to provide less but more confident
predictions for the existence of a polarity shift while
allowing it to make more errors when predicting the
absence of a shift. For these classifiers, we use a Bag
of Words representation of the lemmatized segments.
When a word from the sentiment lexicon does not
appear in the tweet segment, we use a one vs. all
classification approach with a SVM classifier and
tree kernels. The tree kernel is a function between
two trees that computes a normalized similarity score
in the range [0,1] (Culotta and Sorensen, 2004). For
our task, we use an implementation of tree kernels
for syntactic parse trees (Moschitti, 2006) that is built
on top of the SVM-Light library (Joachims, 1999) in
a similar manner to that presented in (Ginsca, 2012).
We build the syntactic parse trees with the Stanford
CoreNLP library (Klein and Manning, 2003).
</bodyText>
<subsectionHeader confidence="0.997993">
2.2 Evaluation and Results
</subsectionHeader>
<bodyText confidence="0.999981">
For the experiments presented in this section, we
merge the training and development datasets and for
the polarity shift and sentiment classification experi-
ments we report the results using a 5-fold cross vali-
dation technique over the resulting dataset.
</bodyText>
<page confidence="0.998123">
419
</page>
<subsectionHeader confidence="0.826143">
2.2.1 Lexicon choice influence
</subsectionHeader>
<bodyText confidence="0.997750736842105">
Considering that the selection of a lexicon plays
an important role on the performance of our system,
we tested 3 widely used sentiment lexicons: Sen-
tiWordNet 3 (Baccianella et al., 2010), Bing Liu’s
Opinion Lexicon (Hu and Liu, 2004) and MPQA
Subjectivity Lexicon (Wilson et al., 2005). Different
combinations of these lexicons were tried and in Ta-
ble 1 we present the top performing ones. Besides
the F-Measure for positive (Fp) and negative (Fn)
instances, we also list the percentage of instances in
which appears at least one word from the lexicon.
SentiWordnet appoints polarity weights to words,
ranging from 0 to 1. An important parameter is the
threshold over which a word is considered to have a
certain polarity. We tested several values (from 0.5 to
0.9 with a step of 0.05) and the best results in terms
of F-Measure were obtained for a threshold of 0.75.
Our finding is consistent with the value suggested in
(Chen et al., 2012).
</bodyText>
<table confidence="0.999775285714286">
Lexicon Found(%) Fp Fn
Liu 55.7 0.93 0.85
MPQA 61.4 0.89 0.76
SentWN 79.4 0.86 0.78
Liu+MPQA 67.1 0.89 0.78
Liu+SentWN 79.4 0.87 0.81
Liu+MPQA+SentWN 79.4 0.86 0.81
</table>
<tableCaption confidence="0.965772">
Table 1: Influence of lexicon on the F-Measure for positive
and negative segments
</tableCaption>
<subsubsectionHeader confidence="0.50259">
2.2.2 Polarity shift experiments
</subsubsectionHeader>
<bodyText confidence="0.999935888888889">
We tested several classifiers using the Weka toolkit
(Hall et al., 2009) and found that the best results
were obtained with the Sequential Minimal Optimiza-
tion (SMO) classifier. For instance, when classifying
+ —* − shifts, SMO correctly identified 91 out of
192 polarity shifts in contrast with 68 and 41 detected
by a Random Forests and a Naive Bayes classifier,
respectively. For the + --� * classification, the SMO
classifier finds 2 out of 34 shifts, for − --� +, 15 out
of 238 and for − --� *, 2 out of 32 shifts are found.
After changing the polarity of sentiment segments as
found by the 4 classifiers, we obtain an increase in
F-Measure from 0.930 to 0.947 for positive segments
and from 0.851 to 0.913 for negative segments. Our
choice of the Bag of Words model instead of a parse
tree representation for these classifiers is justified by
the poor performance of tree kernels when dealing
with unbalanced data.
</bodyText>
<table confidence="0.958366636363636">
2.2.3 Sentiment classification experiments
Model Class Avg. F-score
positive 0.780
Basic Tree negative 0.645
neutral 0.227
positive 0.768
Tree + Numeric negative 0.590
neutral 0.132
positive 0.801
Tree + Context 2 negative 0.676
neutral 0.231
</table>
<tableCaption confidence="0.989817">
Table 2: Comparison between different models used for
segment polarity classification
</tableCaption>
<bodyText confidence="0.999989055555556">
In a series of preliminary experiments, we tested
several classifiers trained on a Bag of Words model
and an SVM classifier with a tree kernel. We found
that the parse tree representation of a tweet segment
provided a higher accuracy. This shows that although
small, when a segment contains more than one word,
its syntactic structure becomes a relevant feature.
In Table 2 we compare the results of 3 tree based
models. In the Basic Tree model, we use only the
syntactic parse tree representation of a tweet seg-
ment. For the Tree + Numeric model, we use the
initial tree kernel together with a polynomial kernel
on the binary structure features presented in section
2.1.1. In the Tree + Context model, we include in the
parse tree, besides the given section, k tokens (words,
punctuation) from the whole tweet that surround the
selected segment. We performed tests with k from 1
to 5 and obtained the best results with a k value of 2.
</bodyText>
<subsectionHeader confidence="0.499093">
2.2.4 Competition results
</subsectionHeader>
<bodyText confidence="0.999962666666667">
For the Twitter dataset, we ranked 4th out of
23 groups that submitted constrained runs. When
combining the results of the constrained and uncon-
strained submissions, our run was ranked 5th out of
a total of 29 submissions. For the SMS dataset, we
ranked 5th out of a total of 18 groups for the con-
strained setting and our submission was ranked 5th
out of 24 combined runs. In Table 3, we detail the
results we obtained on the competition test datasets.
</bodyText>
<page confidence="0.980583">
420
</page>
<table confidence="0.999974428571428">
Class P R F-score
Twitter positive 0.8623 0.9140 0.8874
Twitter negative 0.8453 0.8086 0.8265
Twitter neutral 0.4127 0.1625 0.2332
SMS positive 0.7107 0.8945 0.7921
SMS negative 0.8687 0.7609 0.8112
SMS neutral 0.3684 0.0440 0.0787
</table>
<tableCaption confidence="0.98775">
Table 3: Competition results overview on the Twitter and
SMS datasets
</tableCaption>
<subsectionHeader confidence="0.955685">
2.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999955">
The robustness of our approach is proved by the
low standard deviation of the F-Measure scores ob-
tained over each of the the 5 folds used for evaluation
(0.026) but also by the small difference between the
results we obtained during the development phase
and those reported on the competition test dataset.
The choice of lexicons results in a trade-off between
the percentage of instances classified with either the
lexicon and polarity shift or the supervised learning
method. Although the first one yields better results
and it is apparently desirable to have a better cover-
age of lexicon terms, this would reduce the number of
instances for training a classifier leading to a poorer
performance of this approach.
</bodyText>
<sectionHeader confidence="0.97629" genericHeader="method">
3 Task B: Message Polarity Classification
</sectionHeader>
<bodyText confidence="0.999918">
In this section, we present our approach for the mes-
sage polarity classification task in which, given a
message, the system has to determine whether it ex-
presses a positive, negative, or neutral sentiment. As
for Task A, we submitted a single constrained run.
</bodyText>
<subsectionHeader confidence="0.999836">
3.1 Preprocessing of the corpora
</subsectionHeader>
<bodyText confidence="0.995595166666667">
We use as training corpora the training data, merged
with the development data. After the deletion of
tweets no longer available, our final training set con-
tains 10402 tweets: 3855 positive, 1633 negative and
4914 objective or neutral. In the preprocessing step,
we first remove the web addresses from the tweets to
reduce the noise. Then, we extract the emoticons and
create new features with the number of occurrences
of each type of emoticon. The different emoticons
types are presented in Table 4. Then, we lemmatize
the text using LIMA, a linguistic analyzer of CEA
LIST (Besanc¸on et al., 2010).
</bodyText>
<table confidence="0.999335333333333">
:-) :) =)X) x) Smile
:-( :( =( Sadness
:-D :D =D X-D XD x-D xD :’) Laugh
;-) ;) Wink
&lt; 3 Heart
:’-( :’( =’( Tear
</table>
<tableCaption confidence="0.999238">
Table 4: Common emoticon types
</tableCaption>
<subsectionHeader confidence="0.997921">
3.2 Boostexter baseline
</subsectionHeader>
<bodyText confidence="0.97881425">
To classify the tweets, we used the BoosTexter1 clas-
sifier (Schapire and Singer, 2000) in its discrete Ad-
aBoost.MH version, setting the number of iterations
to 1000. We used two types of features: a Bag of
Words of lemmatized uni-, bi- and tri-grams and the
number of occurrences of each emoticon type.
Bog of words features Emoticon type feature
wow lady gaga be great Smile 1
</bodyText>
<tableCaption confidence="0.981672">
Table 5: Example of tweet representation
</tableCaption>
<bodyText confidence="0.9999477">
Boostexter is designed to maximize the accuracy,
not the F-score, which is the chosen evaluation metric
for this task. As the training data contain few negative
examples, the classifier tends to under-detect this
class. In order to favour the negative class detection,
we balance the training corpora. So our final system
is trained on 4899 tweets (1633 of each class, chosen
randomly). The accuracy results are not presented
here. However, the gain between our baseline and
our final system has the same order of magnitude.
</bodyText>
<subsectionHeader confidence="0.99964">
3.3 Integration of domain information
</subsectionHeader>
<bodyText confidence="0.999969090909091">
Some words can change their polarity between two
different domains (Navigli, 2012; Yoshida et al.,
2011). For example, the word ”return” is positive
in ”I can’t wait to return to my book”. However, it is
often very negative when we are talking about some
electronics device, as in ”I had to return my phone
to the store”. This phenomenon happens even in
more closely related domains: ”I was laughing all the
time” is a good point for a comedy film but a bad one
for a horror film. We call such words or expressions
”multi-polarity words”. This phenomenon is different
</bodyText>
<footnote confidence="0.9896605">
1BoosTexter is a general purpose machine-learning program
based on boosting for building a classifier from text.
</footnote>
<page confidence="0.998681">
421
</page>
<bodyText confidence="0.999263111111111">
from polysemy, as a word can keep the same meaning
across domains while changing its polarity and it can
lead to classification error (Wilson et al., 2009). In
(Marchand, 2013), we have shown, on a corpus of re-
views, that a sensible amount of multi-polarity words
influences the results of common opinion classifiers.
Their deletion or their differentiation leads to better
classification results. Here, we test this approach on
a corpus of tweets.
</bodyText>
<subsectionHeader confidence="0.400885">
3.3.1 Domain generation with LDA
</subsectionHeader>
<bodyText confidence="0.9976253">
In order to apply our method, we need to assign
domains to tweets. For that purpose, we use Latent
Dirichlet Allocation (LDA) (Blei et al., 2003). We
used the Mallet LDA implementation (McCallum,
2002). The framework uses Gibbs sampling to con-
stitute the sample distributions that are exploited for
the creation of the topic models. The models are built
using the lemmatized tweets from the training and
development data. We performed tests with a number
of domains ranging from 5 to 25, with a step of 5.
Each LDA representation of a tweet is encoded by
inferring a domain distribution. For example, if a
model with 5 domains is used, we generate a vector
of length 5, where each the i-th value is the propor-
tion of terms belonging to the i-th domain.
Domain 1 tonight, watch, time, today
Domain 2 win, vote, obama, black
Domain 3 game, play, win, team
Domain 4 apple, international, sun, anderson
Domain 5 ticket, show, open, live
</bodyText>
<tableCaption confidence="0.8951785">
Table 6: Most representative words of each domain (5
domains version)
</tableCaption>
<bodyText confidence="0.9999595">
In first experiments with crossvalidation on train-
ing data, the 5 domains version, presented in Table 6,
appears to be the most efficient. Therefore, in the rest
of the paper, results are shown only for this version.
</bodyText>
<subsectionHeader confidence="0.978482">
3.3.2 Detection of multi-polarity words
</subsectionHeader>
<bodyText confidence="0.999991090909091">
For detecting the multi-polarity words, we use the
positive and negative labels of the training data. We
make the assumption that positive words will mostly
appear in positive tweets and negative words in neg-
ative tweets. Between two different corpora, we de-
termine words with different polarity across corpora
by using a X2 test on their profile of occurrence in
positive and negative tweets in both corpora. The risk
of false positive is set to 0.05. The words are also
selected only if they occur more often than a given
threshold. For the SemEval task B, we apply this
detection for each domain. Each time, we detect the
words that change their polarity between a specific
domain and all the others. For example, the word
”black” is detected as positive in the second domain,
related to the election of Barack Obama, and neutral
in the rest of the tweets. At the end of this procedure,
we have 5 collections of words which change their
polarity (one different collection for each domain).
These collections are rather small: from 21 to 61
multi-polarity words are detected depending on the
domain and the parameters.
</bodyText>
<subsectionHeader confidence="0.975911">
3.3.3 Differentiation of multi-polarity words
</subsectionHeader>
<bodyText confidence="0.999788">
We tested different strategies in order to integrate
the domain information in the Sentiment Classifica-
tion in Twitter task.
</bodyText>
<listItem confidence="0.9974365">
• Domain-specific: 5 different classifiers are
trained on the domain specific subpart of the
tweets, without change on the data.
• Diff-topic: 5 different classifiers are trained on
the whole corpus, where the detected multi-
polarity words are differentiated into ”word-
domainX” and ”word-other”.
• Change-all: only 1 classifier is trained. Similar
to the previous one, except all the differentia-
tions are made at the same time.
• Keep-topic: 5 different classifiers are trained.
The detected multi-polarity words are kept in-
side their domain and deleted in the others.
• Remove-all: 5 different classifiers are trained.
</listItem>
<bodyText confidence="0.954405111111111">
The detected multi-polarity words are deleted
inside and outside their domain.
For the change-all version, we use only one classifier:
all test tweets are classified using the same classifier.
In the other versions, we obtain 5 classifiers. For
each test tweet, we determine its domain profile us-
ing topic models of LDA. Then we use a mix of all
the classifiers with weighting according to the LDA
mixture2. The domain-specific version gives worse
</bodyText>
<footnote confidence="0.99633">
2The weight is the exponential of the LDA score.
</footnote>
<page confidence="0.995364">
422
</page>
<bodyText confidence="0.87708">
results than the baseline trained on the whole original
corpus and is not represented on the figures.
</bodyText>
<figureCaption confidence="0.986488">
Figure 2: Average F-measure results for the best set of
parameters for each method.
</figureCaption>
<bodyText confidence="0.998193375">
We tested all these versions with two training sets:
first, using all the training tweets to train the clas-
sifiers (Figure 2) and secondly, only the tweets for
which a domain can be confidently attributed (at least
a 75% score from the LDA model) (Figure 3). In this
case, the training set contains 2889 tweets. The run
submitted to SemEval corresponds to the change-all
version, trained with all the training tweets.
</bodyText>
<figureCaption confidence="0.9881325">
Figure 3: Average F-measure results for the best set of
parameters for each method.
</figureCaption>
<bodyText confidence="0.9999825">
Empirically, we set the threshold for the number
of occurrences to 10 in the first experiment and only
to 5 in the domain confident experiment, due to the
smallest size of the training corpora.
</bodyText>
<subsectionHeader confidence="0.999709">
3.4 Analysis of the result and discussion
</subsectionHeader>
<bodyText confidence="0.999980233333333">
Using a boosting method with lemma trigrams and
emoticons features is a good fully automatic baseline.
We are in the mid range of results of all the partici-
pants (19th out of 48 submissions for the tweets and
26th out of 42 submissions for the SMS). We try to
include domain information to improve the opinion
classification. As we don’t have a reference domain
differentiation for the tweets, we separate them us-
ing the LDA method. The domain-specific version,
which does not take into account the multi-polarity
words, degrades the performances(-1.85% in the first
experiment, -2.8% in the second). On the contrary,
all our versions which use multi-polarity words, es-
pecially remove-all version, improve the F-measure.
The final improvement is small but it has to be re-
lated to the small number of multi-polarity words we
have detected (in average, 36 words per domain). We
think that the tweet collection is too small for the X2
test to detect a lot of words with enough confidence.
For comparison, in our experiment on reviews, we
detected about 400 multi-polarity words per domain.
It is also worth noticing that for the domain confi-
dent experiment, the improvement is more sensible
(+1.46% versus +0.70%) even if the absolute value of
the score is not better, due to a much smaller training
data. It’s a good argument for our method. Another
question is about the method used to separate the
tweets into different domains. We plan to have more
control on the domains by using a more supervised
method based on the categories of Wikipedia.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999983884615384">
In this paper, we presented our contribution to Se-
mEval 2013 task 2: Sentiment Analysis in Twitter.
For the Contextual Polarity Disambiguation subtask,
we described a very efficient and robust method based
on a sentiment lexicon associated with a polarity shift
detector and a tree based classification. As for the
Message Polarity Classification, we focused on the
impact of domain information. With only 4899 train-
ing tweets, we achieve good performances and we
demonstrate that words with changing polarity can
influence the classification performance.
One of the challenges of this SemEval task was to
see how well sentiment analysis models trained us-
ing Twitter data would generalize to a SMS dataset.
Looking at our result but also at the submissions of
other participants, a drop of performance can be ob-
served between the results on the Twitter and SMS
test datasets. In (Hu et al., 2013), the authors per-
form a thorough study on the differences between the
language used on Twitter and that of SMS messages
and chat. They find that Twitter language is more
conservative and less informal than SMS and online
chat and that the language of Twitter can be seen as
a projection of a formal register in a restricted space.
This is a good indicator to the difficulty of using a
Twitter centered system on a SMS dataset.
</bodyText>
<page confidence="0.999068">
423
</page>
<sectionHeader confidence="0.998333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999246">
This work was partly supported by the MUCKE
project (http://ifs.tuwien.ac.at/˜mucke/) through a
grant from the French National Research Agency
(ANR), FP7 CHIST-ERA Programme (ANR-12-
CHRI-0007-04).
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99973228125">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the 7th conference on International Lan-
guage Resources and Evaluation (LREC10), Valletta,
Malta, May.
Romaric Besanc¸on, Ga¨el de Chalendar, Olivier Ferret,
Faiza Gara, Olivier Mesnard, Meriama Lab, and Nasre-
dine Semmar. 2010. Lima: A multilingual framework
for linguistic analysis and linguistic resources develop-
ment and evaluation. In Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner,
and Daniel Tapias, editors, Proceedings of LREC’10,
Valletta, Malta, may. European Language Resources
Association (ELRA).
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent dirichlet allocation. the Journal of machine
Learning research, 3:993–1022.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from twitter. Proceedings of ICWSM.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting on Association for Computa-
tional Linguistics, page 423. Association for Computa-
tional Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags and
smileys. In Proceedings of the 23rd International Con-
ference on Computational Linguistics: Posters, pages
241–249. Association for Computational Linguistics.
Alexandru Lucian Ginsca. 2012. Fine-grained opin-
ion mining as a relation classification problem. In
2012 Imperial College Computing Student Workshop,
volume 28, pages 56–61. Schloss Dagstuhl–Leibniz-
Zentrum fuer Informatik.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–12.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten. 2009.
The weka data mining software: an update. ACM
SIGKDD Explorations Newsletter, 11(1):10–18.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 168–177. ACM.
Yuheng Hu, Kartik Talamadupula, and Subbarao Kamb-
hampati. 2013. Dude, srsly?: The surprisingly formal
nature of twitters language. Proceedings of ICWSM.
Thorsten Joachims. 1999. Making large scale svm learn-
ing practical.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423–430. Association for
Computational Linguistics.
Morgane Marchand. 2013. Fouille dopinion: ces mots qui
changent de polarit´e selon le domaine. In Proceedings
of the 8e Rencontres Jeunes Chercheurs en Recherche
dInformation (RJCRI).
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings of
EACL, volume 6, pages 113–120.
R. Navigli. 2012. A quick tour of word sense disam-
biguation, induction and related approaches. SOFSEM
2012: Theory and Practice of Computer Science, pages
115–129.
Robert E Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine learning, 39(2-3):135–168.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, pages 347–354. As-
sociation for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009.
Recognizing contextual polarity: An exploration of
features for phrase-level sentiment analysis. Computa-
tional Linguistics, 35.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Yasuhisa Yoshida, Tsutomu Hirao, Tomoharu Iwata,
Masaaki Nagata, and Yuji Matsumoto. 2011. Transfer
learning for multiple-domain sentiment analysis - iden-
tifying domain dependent/independent word polaritys.
In AAAI.
</reference>
<page confidence="0.99908">
424
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.129549">
<title confidence="0.9895635">[LVIC-LIMSI]: Using Syntactic Features and Multi-polarity Words Sentiment Analysis in Twitter</title>
<author confidence="0.994772">Alexandru Lucian Romaric Olivier</author>
<note confidence="0.4222962">(1) CEA-LIST, DIASI, CEA SACLAY - Nano-INNOV - Bt. 861 - Point courrier 91191 Gif-sur-Yvette Cedex, (2) Bat 508, BP133,91403 Orsay</note>
<email confidence="0.966046">morgane.marchand@cea.fr;romaric.besancon@cea.fr;olivier.mesnard@cea.fr</email>
<abstract confidence="0.98708375">This paper presents the contribution of our team at task 2 of SemEval 2013: Sentiment Analysis in Twitter. We submitted a constrained run for each of the two subtasks. In the Contextual Polarity Disambiguation subtask, we use a sentiment lexicon approach combined with polarity shift detection and tree kernel based classifiers. In the Message Polarity Classification subtask, we focus on the influence of domain information on sentiment classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th conference on International Language Resources and Evaluation (LREC10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="7172" citStr="Baccianella et al., 2010" startWordPosition="1169" endWordPosition="1172">sented in (Ginsca, 2012). We build the syntactic parse trees with the Stanford CoreNLP library (Klein and Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 419 2.2.1 Lexicon choice influence Considering that the selection of a lexicon plays an important role on the performance of our system, we tested 3 widely used sentiment lexicons: SentiWordNet 3 (Baccianella et al., 2010), Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) and MPQA Subjectivity Lexicon (Wilson et al., 2005). Different combinations of these lexicons were tried and in Table 1 we present the top performing ones. Besides the F-Measure for positive (Fp) and negative (Fn) instances, we also list the percentage of instances in which appears at least one word from the lexicon. SentiWordnet appoints polarity weights to words, ranging from 0 to 1. An important parameter is the threshold over which a word is considered to have a certain polarity. We tested several values (from 0.5 to 0.9 with a step of 0.05) </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the 7th conference on International Language Resources and Evaluation (LREC10), Valletta, Malta, May.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Romaric Besanc¸on</author>
<author>Ga¨el de Chalendar</author>
</authors>
<title>Olivier Ferret, Faiza Gara, Olivier Mesnard, Meriama Lab, and Nasredine Semmar.</title>
<date>2010</date>
<booktitle>In Nicoletta Calzolari</booktitle>
<editor>(Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<marker>Besanc¸on, Ga¨el de Chalendar, 2010</marker>
<rawString>Romaric Besanc¸on, Ga¨el de Chalendar, Olivier Ferret, Faiza Gara, Olivier Mesnard, Meriama Lab, and Nasredine Semmar. 2010. Lima: A multilingual framework for linguistic analysis and linguistic resources development and evaluation. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of LREC’10, Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="15310" citStr="Blei et al., 2003" startWordPosition="2542" endWordPosition="2545">semy, as a word can keep the same meaning across domains while changing its polarity and it can lead to classification error (Wilson et al., 2009). In (Marchand, 2013), we have shown, on a corpus of reviews, that a sensible amount of multi-polarity words influences the results of common opinion classifiers. Their deletion or their differentiation leads to better classification results. Here, we test this approach on a corpus of tweets. 3.3.1 Domain generation with LDA In order to apply our method, we need to assign domains to tweets. For that purpose, we use Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We used the Mallet LDA implementation (McCallum, 2002). The framework uses Gibbs sampling to constitute the sample distributions that are exploited for the creation of the topic models. The models are built using the lemmatized tweets from the training and development data. We performed tests with a number of domains ranging from 5 to 25, with a step of 5. Each LDA representation of a tweet is encoded by inferring a domain distribution. For example, if a model with 5 domains is used, we generate a vector of length 5, where each the i-th value is the proportion of terms belonging to the i-th </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Chen</author>
<author>Wenbo Wang</author>
<author>Meenakshi Nagarajan</author>
<author>Shaojun Wang</author>
<author>Amit P Sheth</author>
</authors>
<title>Extracting diverse sentiment expressions with target-dependent polarity from twitter.</title>
<date>2012</date>
<booktitle>Proceedings of ICWSM.</booktitle>
<contexts>
<context position="7927" citStr="Chen et al., 2012" startWordPosition="1299" endWordPosition="1302">cons were tried and in Table 1 we present the top performing ones. Besides the F-Measure for positive (Fp) and negative (Fn) instances, we also list the percentage of instances in which appears at least one word from the lexicon. SentiWordnet appoints polarity weights to words, ranging from 0 to 1. An important parameter is the threshold over which a word is considered to have a certain polarity. We tested several values (from 0.5 to 0.9 with a step of 0.05) and the best results in terms of F-Measure were obtained for a threshold of 0.75. Our finding is consistent with the value suggested in (Chen et al., 2012). Lexicon Found(%) Fp Fn Liu 55.7 0.93 0.85 MPQA 61.4 0.89 0.76 SentWN 79.4 0.86 0.78 Liu+MPQA 67.1 0.89 0.78 Liu+SentWN 79.4 0.87 0.81 Liu+MPQA+SentWN 79.4 0.86 0.81 Table 1: Influence of lexicon on the F-Measure for positive and negative segments 2.2.2 Polarity shift experiments We tested several classifiers using the Weka toolkit (Hall et al., 2009) and found that the best results were obtained with the Sequential Minimal Optimization (SMO) classifier. For instance, when classifying + —* − shifts, SMO correctly identified 91 out of 192 polarity shifts in contrast with 68 and 41 detected by </context>
</contexts>
<marker>Chen, Wang, Nagarajan, Wang, Sheth, 2012</marker>
<rawString>Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P Sheth. 2012. Extracting diverse sentiment expressions with target-dependent polarity from twitter. Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6352" citStr="Culotta and Sorensen, 2004" startWordPosition="1038" endWordPosition="1041">r than a false negative error. Using this approach we guide the classifier to provide less but more confident predictions for the existence of a polarity shift while allowing it to make more errors when predicting the absence of a shift. For these classifiers, we use a Bag of Words representation of the lemmatized segments. When a word from the sentiment lexicon does not appear in the tweet segment, we use a one vs. all classification approach with a SVM classifier and tree kernels. The tree kernel is a function between two trees that computes a normalized similarity score in the range [0,1] (Culotta and Sorensen, 2004). For our task, we use an implementation of tree kernels for syntactic parse trees (Moschitti, 2006) that is built on top of the SVM-Light library (Joachims, 1999) in a similar manner to that presented in (Ginsca, 2012). We build the syntactic parse trees with the Stanford CoreNLP library (Klein and Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 41</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 423. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3977" citStr="Davidov et al., 2010" startWordPosition="617" endWordPosition="620">, if a vowel is repeated more than 3 times in a word, we reduce it to 418 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 418–424, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Figure 1: Contextual polarity disambiguation task system description a single occurrence and we reduce multiple consecutive punctuation marks to a single one. Finally, we lemmatize the normalized text. Emoticons have been successfully used as sentiment indicators in tweets (Davidov et al., 2010). In our approach, we map a set of positive emoticons to the word good and a set of negative emoticons to the word bad. We use the following sets of emoticons: • Positive emoticons::) , :-) , :D , =) , :’) , :o) , :P , &gt;:) , :”&gt;, &gt;:�, &lt;3 , ;&gt;, ;) , ;-) , ;&gt;, (: , (; • Negative emoticons::( , : ( , :-( , :’( , :/ , :&lt;, ;( Traits of informal language have been used as features in Twitter sentiment classification tasks (Go et al., 2009). In order to avoid the loss of possible useful information, we keep record of the performed normalizations as binary features associated to a tweet segment. We re</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 241–249. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandru Lucian Ginsca</author>
</authors>
<title>Fine-grained opinion mining as a relation classification problem.</title>
<date>2012</date>
<booktitle>In 2012 Imperial College Computing Student Workshop,</booktitle>
<volume>28</volume>
<pages>56--61</pages>
<contexts>
<context position="6571" citStr="Ginsca, 2012" startWordPosition="1077" endWordPosition="1078">ift. For these classifiers, we use a Bag of Words representation of the lemmatized segments. When a word from the sentiment lexicon does not appear in the tweet segment, we use a one vs. all classification approach with a SVM classifier and tree kernels. The tree kernel is a function between two trees that computes a normalized similarity score in the range [0,1] (Culotta and Sorensen, 2004). For our task, we use an implementation of tree kernels for syntactic parse trees (Moschitti, 2006) that is built on top of the SVM-Light library (Joachims, 1999) in a similar manner to that presented in (Ginsca, 2012). We build the syntactic parse trees with the Stanford CoreNLP library (Klein and Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 419 2.2.1 Lexicon choice influence Considering that the selection of a lexicon plays an important role on the performance of our system, we tested 3 widely used sentiment lexicons: SentiWordNet 3 (Baccianella et al., 2010</context>
</contexts>
<marker>Ginsca, 2012</marker>
<rawString>Alexandru Lucian Ginsca. 2012. Fine-grained opinion mining as a relation classification problem. In 2012 Imperial College Computing Student Workshop, volume 28, pages 56–61. Schloss Dagstuhl–LeibnizZentrum fuer Informatik.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="4414" citStr="Go et al., 2009" startWordPosition="713" endWordPosition="716">cutive punctuation marks to a single one. Finally, we lemmatize the normalized text. Emoticons have been successfully used as sentiment indicators in tweets (Davidov et al., 2010). In our approach, we map a set of positive emoticons to the word good and a set of negative emoticons to the word bad. We use the following sets of emoticons: • Positive emoticons::) , :-) , :D , =) , :’) , :o) , :P , &gt;:) , :”&gt;, &gt;:�, &lt;3 , ;&gt;, ;) , ;-) , ;&gt;, (: , (; • Negative emoticons::( , : ( , :-( , :’( , :/ , :&lt;, ;( Traits of informal language have been used as features in Twitter sentiment classification tasks (Go et al., 2009). In order to avoid the loss of possible useful information, we keep record of the performed normalizations as binary features associated to a tweet segment. We retain the following set of features: hasPositiveEmoticon, hasNegativeEmoticon, hasHashtag, hasAtSign, hasConsecutivePunctuation, hasConsecutiveVowels, hasUpperCaseWords. 2.1.2 Classification methods In a first step, we select tweet segments that contain at least one word from a lexicon and assign to it the polarity of that word. If there are more than one sentiment words with different polarities in the segment, we keep the most frequ</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<date>2009</date>
<contexts>
<context position="8281" citStr="Hall et al., 2009" startWordPosition="1356" endWordPosition="1359">ord is considered to have a certain polarity. We tested several values (from 0.5 to 0.9 with a step of 0.05) and the best results in terms of F-Measure were obtained for a threshold of 0.75. Our finding is consistent with the value suggested in (Chen et al., 2012). Lexicon Found(%) Fp Fn Liu 55.7 0.93 0.85 MPQA 61.4 0.89 0.76 SentWN 79.4 0.86 0.78 Liu+MPQA 67.1 0.89 0.78 Liu+SentWN 79.4 0.87 0.81 Liu+MPQA+SentWN 79.4 0.86 0.81 Table 1: Influence of lexicon on the F-Measure for positive and negative segments 2.2.2 Polarity shift experiments We tested several classifiers using the Weka toolkit (Hall et al., 2009) and found that the best results were obtained with the Sequential Minimal Optimization (SMO) classifier. For instance, when classifying + —* − shifts, SMO correctly identified 91 out of 192 polarity shifts in contrast with 68 and 41 detected by a Random Forests and a Naive Bayes classifier, respectively. For the + --� * classification, the SMO classifier finds 2 out of 34 shifts, for − --� +, 15 out of 238 and for − --� *, 2 out of 32 shifts are found. After changing the polarity of sentiment segments as found by the 4 classifiers, we obtain an increase in F-Measure from 0.930 to 0.947 for po</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009.</rawString>
</citation>
<citation valid="false">
<title>The weka data mining software: an update.</title>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<marker></marker>
<rawString>The weka data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7219" citStr="Hu and Liu, 2004" startWordPosition="1177" endWordPosition="1180">trees with the Stanford CoreNLP library (Klein and Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 419 2.2.1 Lexicon choice influence Considering that the selection of a lexicon plays an important role on the performance of our system, we tested 3 widely used sentiment lexicons: SentiWordNet 3 (Baccianella et al., 2010), Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) and MPQA Subjectivity Lexicon (Wilson et al., 2005). Different combinations of these lexicons were tried and in Table 1 we present the top performing ones. Besides the F-Measure for positive (Fp) and negative (Fn) instances, we also list the percentage of instances in which appears at least one word from the lexicon. SentiWordnet appoints polarity weights to words, ranging from 0 to 1. An important parameter is the threshold over which a word is considered to have a certain polarity. We tested several values (from 0.5 to 0.9 with a step of 0.05) and the best results in terms of F-Measure were</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuheng Hu</author>
<author>Kartik Talamadupula</author>
<author>Subbarao Kambhampati</author>
</authors>
<title>Dude, srsly?: The surprisingly formal nature of twitters language.</title>
<date>2013</date>
<booktitle>Proceedings of ICWSM.</booktitle>
<contexts>
<context position="22215" citStr="Hu et al., 2013" startWordPosition="3692" endWordPosition="3695">e based classification. As for the Message Polarity Classification, we focused on the impact of domain information. With only 4899 training tweets, we achieve good performances and we demonstrate that words with changing polarity can influence the classification performance. One of the challenges of this SemEval task was to see how well sentiment analysis models trained using Twitter data would generalize to a SMS dataset. Looking at our result but also at the submissions of other participants, a drop of performance can be observed between the results on the Twitter and SMS test datasets. In (Hu et al., 2013), the authors perform a thorough study on the differences between the language used on Twitter and that of SMS messages and chat. They find that Twitter language is more conservative and less informal than SMS and online chat and that the language of Twitter can be seen as a projection of a formal register in a restricted space. This is a good indicator to the difficulty of using a Twitter centered system on a SMS dataset. 423 Acknowledgments This work was partly supported by the MUCKE project (http://ifs.tuwien.ac.at/˜mucke/) through a grant from the French National Research Agency (ANR), FP7</context>
</contexts>
<marker>Hu, Talamadupula, Kambhampati, 2013</marker>
<rawString>Yuheng Hu, Kartik Talamadupula, and Subbarao Kambhampati. 2013. Dude, srsly?: The surprisingly formal nature of twitters language. Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large scale svm learning practical.</title>
<date>1999</date>
<contexts>
<context position="6515" citStr="Joachims, 1999" startWordPosition="1067" endWordPosition="1068">it to make more errors when predicting the absence of a shift. For these classifiers, we use a Bag of Words representation of the lemmatized segments. When a word from the sentiment lexicon does not appear in the tweet segment, we use a one vs. all classification approach with a SVM classifier and tree kernels. The tree kernel is a function between two trees that computes a normalized similarity score in the range [0,1] (Culotta and Sorensen, 2004). For our task, we use an implementation of tree kernels for syntactic parse trees (Moschitti, 2006) that is built on top of the SVM-Light library (Joachims, 1999) in a similar manner to that presented in (Ginsca, 2012). We build the syntactic parse trees with the Stanford CoreNLP library (Klein and Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 419 2.2.1 Lexicon choice influence Considering that the selection of a lexicon plays an important role on the performance of our system, we tested 3 widely used sent</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large scale svm learning practical.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6667" citStr="Klein and Manning, 2003" startWordPosition="1090" endWordPosition="1093">gments. When a word from the sentiment lexicon does not appear in the tweet segment, we use a one vs. all classification approach with a SVM classifier and tree kernels. The tree kernel is a function between two trees that computes a normalized similarity score in the range [0,1] (Culotta and Sorensen, 2004). For our task, we use an implementation of tree kernels for syntactic parse trees (Moschitti, 2006) that is built on top of the SVM-Light library (Joachims, 1999) in a similar manner to that presented in (Ginsca, 2012). We build the syntactic parse trees with the Stanford CoreNLP library (Klein and Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 419 2.2.1 Lexicon choice influence Considering that the selection of a lexicon plays an important role on the performance of our system, we tested 3 widely used sentiment lexicons: SentiWordNet 3 (Baccianella et al., 2010), Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) and MPQA Subjectivity Lexicon (Wilson et al., 2</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgane Marchand</author>
</authors>
<title>Fouille dopinion: ces mots qui changent de polarit´e selon le domaine.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8e Rencontres Jeunes Chercheurs en Recherche dInformation (RJCRI).</booktitle>
<contexts>
<context position="14859" citStr="Marchand, 2013" startWordPosition="2470" endWordPosition="2471">out some electronics device, as in ”I had to return my phone to the store”. This phenomenon happens even in more closely related domains: ”I was laughing all the time” is a good point for a comedy film but a bad one for a horror film. We call such words or expressions ”multi-polarity words”. This phenomenon is different 1BoosTexter is a general purpose machine-learning program based on boosting for building a classifier from text. 421 from polysemy, as a word can keep the same meaning across domains while changing its polarity and it can lead to classification error (Wilson et al., 2009). In (Marchand, 2013), we have shown, on a corpus of reviews, that a sensible amount of multi-polarity words influences the results of common opinion classifiers. Their deletion or their differentiation leads to better classification results. Here, we test this approach on a corpus of tweets. 3.3.1 Domain generation with LDA In order to apply our method, we need to assign domains to tweets. For that purpose, we use Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We used the Mallet LDA implementation (McCallum, 2002). The framework uses Gibbs sampling to constitute the sample distributions that are exploited</context>
</contexts>
<marker>Marchand, 2013</marker>
<rawString>Morgane Marchand. 2013. Fouille dopinion: ces mots qui changent de polarit´e selon le domaine. In Proceedings of the 8e Rencontres Jeunes Chercheurs en Recherche dInformation (RJCRI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="15366" citStr="McCallum, 2002" startWordPosition="2552" endWordPosition="2553">ile changing its polarity and it can lead to classification error (Wilson et al., 2009). In (Marchand, 2013), we have shown, on a corpus of reviews, that a sensible amount of multi-polarity words influences the results of common opinion classifiers. Their deletion or their differentiation leads to better classification results. Here, we test this approach on a corpus of tweets. 3.3.1 Domain generation with LDA In order to apply our method, we need to assign domains to tweets. For that purpose, we use Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We used the Mallet LDA implementation (McCallum, 2002). The framework uses Gibbs sampling to constitute the sample distributions that are exploited for the creation of the topic models. The models are built using the lemmatized tweets from the training and development data. We performed tests with a number of domains ranging from 5 to 25, with a step of 5. Each LDA representation of a tweet is encoded by inferring a domain distribution. For example, if a model with 5 domains is used, we generate a vector of length 5, where each the i-th value is the proportion of terms belonging to the i-th domain. Domain 1 tonight, watch, time, today Domain 2 wi</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<volume>6</volume>
<pages>113--120</pages>
<contexts>
<context position="6452" citStr="Moschitti, 2006" startWordPosition="1056" endWordPosition="1057">redictions for the existence of a polarity shift while allowing it to make more errors when predicting the absence of a shift. For these classifiers, we use a Bag of Words representation of the lemmatized segments. When a word from the sentiment lexicon does not appear in the tweet segment, we use a one vs. all classification approach with a SVM classifier and tree kernels. The tree kernel is a function between two trees that computes a normalized similarity score in the range [0,1] (Culotta and Sorensen, 2004). For our task, we use an implementation of tree kernels for syntactic parse trees (Moschitti, 2006) that is built on top of the SVM-Light library (Joachims, 1999) in a similar manner to that presented in (Ginsca, 2012). We build the syntactic parse trees with the Stanford CoreNLP library (Klein and Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 419 2.2.1 Lexicon choice influence Considering that the selection of a lexicon plays an important role</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL, volume 6, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>A quick tour of word sense disambiguation, induction and related approaches. SOFSEM 2012: Theory and Practice of Computer Science,</title>
<date>2012</date>
<pages>115--129</pages>
<contexts>
<context position="14079" citStr="Navigli, 2012" startWordPosition="2335" endWordPosition="2336">ize the accuracy, not the F-score, which is the chosen evaluation metric for this task. As the training data contain few negative examples, the classifier tends to under-detect this class. In order to favour the negative class detection, we balance the training corpora. So our final system is trained on 4899 tweets (1633 of each class, chosen randomly). The accuracy results are not presented here. However, the gain between our baseline and our final system has the same order of magnitude. 3.3 Integration of domain information Some words can change their polarity between two different domains (Navigli, 2012; Yoshida et al., 2011). For example, the word ”return” is positive in ”I can’t wait to return to my book”. However, it is often very negative when we are talking about some electronics device, as in ”I had to return my phone to the store”. This phenomenon happens even in more closely related domains: ”I was laughing all the time” is a good point for a comedy film but a bad one for a horror film. We call such words or expressions ”multi-polarity words”. This phenomenon is different 1BoosTexter is a general purpose machine-learning program based on boosting for building a classifier from text. </context>
</contexts>
<marker>Navigli, 2012</marker>
<rawString>R. Navigli. 2012. A quick tour of word sense disambiguation, induction and related approaches. SOFSEM 2012: Theory and Practice of Computer Science, pages 115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="13102" citStr="Schapire and Singer, 2000" startWordPosition="2173" endWordPosition="2176">tral. In the preprocessing step, we first remove the web addresses from the tweets to reduce the noise. Then, we extract the emoticons and create new features with the number of occurrences of each type of emoticon. The different emoticons types are presented in Table 4. Then, we lemmatize the text using LIMA, a linguistic analyzer of CEA LIST (Besanc¸on et al., 2010). :-) :) =)X) x) Smile :-( :( =( Sadness :-D :D =D X-D XD x-D xD :’) Laugh ;-) ;) Wink &lt; 3 Heart :’-( :’( =’( Tear Table 4: Common emoticon types 3.2 Boostexter baseline To classify the tweets, we used the BoosTexter1 classifier (Schapire and Singer, 2000) in its discrete AdaBoost.MH version, setting the number of iterations to 1000. We used two types of features: a Bag of Words of lemmatized uni-, bi- and tri-grams and the number of occurrences of each emoticon type. Bog of words features Emoticon type feature wow lady gaga be great Smile 1 Table 5: Example of tweet representation Boostexter is designed to maximize the accuracy, not the F-score, which is the chosen evaluation metric for this task. As the training data contain few negative examples, the classifier tends to under-detect this class. In order to favour the negative class detection</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E Schapire and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine learning, 39(2-3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7271" citStr="Wilson et al., 2005" startWordPosition="1185" endWordPosition="1188">d Manning, 2003). 2.2 Evaluation and Results For the experiments presented in this section, we merge the training and development datasets and for the polarity shift and sentiment classification experiments we report the results using a 5-fold cross validation technique over the resulting dataset. 419 2.2.1 Lexicon choice influence Considering that the selection of a lexicon plays an important role on the performance of our system, we tested 3 widely used sentiment lexicons: SentiWordNet 3 (Baccianella et al., 2010), Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) and MPQA Subjectivity Lexicon (Wilson et al., 2005). Different combinations of these lexicons were tried and in Table 1 we present the top performing ones. Besides the F-Measure for positive (Fp) and negative (Fn) instances, we also list the percentage of instances in which appears at least one word from the lexicon. SentiWordnet appoints polarity weights to words, ranging from 0 to 1. An important parameter is the threshold over which a word is considered to have a certain polarity. We tested several values (from 0.5 to 0.9 with a step of 0.05) and the best results in terms of F-Measure were obtained for a threshold of 0.75. Our finding is co</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<contexts>
<context position="14838" citStr="Wilson et al., 2009" startWordPosition="2465" endWordPosition="2468">ive when we are talking about some electronics device, as in ”I had to return my phone to the store”. This phenomenon happens even in more closely related domains: ”I was laughing all the time” is a good point for a comedy film but a bad one for a horror film. We call such words or expressions ”multi-polarity words”. This phenomenon is different 1BoosTexter is a general purpose machine-learning program based on boosting for building a classifier from text. 421 from polysemy, as a word can keep the same meaning across domains while changing its polarity and it can lead to classification error (Wilson et al., 2009). In (Marchand, 2013), we have shown, on a corpus of reviews, that a sensible amount of multi-polarity words influences the results of common opinion classifiers. Their deletion or their differentiation leads to better classification results. Here, we test this approach on a corpus of tweets. 3.3.1 Domain generation with LDA In order to apply our method, we need to assign domains to tweets. For that purpose, we use Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We used the Mallet LDA implementation (McCallum, 2002). The framework uses Gibbs sampling to constitute the sample distributio</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1355" citStr="Wilson et al., 2013" startWordPosition="195" endWordPosition="198"> detection and tree kernel based classifiers. In the Message Polarity Classification subtask, we focus on the influence of domain information on sentiment classification. 1 Introduction In the past decade, new forms of communication, such as microblogging and text messaging have emerged and became ubiquitous. These short messages are often used to share opinions and sentiments. The Sentiment Analysis in Twitter task promotes research that will lead to a better understanding of how sentiment is conveyed in tweets and texts. In this paper, we describe our contribution at task 2 of SemEval 2013 (Wilson et al., 2013). For the Contextual Polarity Disambiguation subtask, covered in section 2, we use a system that combines a lexicon based approach to sentiment detection with two types of supervised learning methods, one used for polarity shift identification and one for tweet segment classification in the absence of lexicon words. The third section presents the Message Polarity Classification subtask. We focus here on the influence of domain information on sentiment classification by detecting words that change their polarity across domains. 2 Task A: Contextual Polarity Disambiguation In this section we pre</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhisa Yoshida</author>
<author>Tsutomu Hirao</author>
<author>Tomoharu Iwata</author>
<author>Masaaki Nagata</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Transfer learning for multiple-domain sentiment analysis - identifying domain dependent/independent word polaritys.</title>
<date>2011</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="14102" citStr="Yoshida et al., 2011" startWordPosition="2337" endWordPosition="2340">y, not the F-score, which is the chosen evaluation metric for this task. As the training data contain few negative examples, the classifier tends to under-detect this class. In order to favour the negative class detection, we balance the training corpora. So our final system is trained on 4899 tweets (1633 of each class, chosen randomly). The accuracy results are not presented here. However, the gain between our baseline and our final system has the same order of magnitude. 3.3 Integration of domain information Some words can change their polarity between two different domains (Navigli, 2012; Yoshida et al., 2011). For example, the word ”return” is positive in ”I can’t wait to return to my book”. However, it is often very negative when we are talking about some electronics device, as in ”I had to return my phone to the store”. This phenomenon happens even in more closely related domains: ”I was laughing all the time” is a good point for a comedy film but a bad one for a horror film. We call such words or expressions ”multi-polarity words”. This phenomenon is different 1BoosTexter is a general purpose machine-learning program based on boosting for building a classifier from text. 421 from polysemy, as a</context>
</contexts>
<marker>Yoshida, Hirao, Iwata, Nagata, Matsumoto, 2011</marker>
<rawString>Yasuhisa Yoshida, Tsutomu Hirao, Tomoharu Iwata, Masaaki Nagata, and Yuji Matsumoto. 2011. Transfer learning for multiple-domain sentiment analysis - identifying domain dependent/independent word polaritys. In AAAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>