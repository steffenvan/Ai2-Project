<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000709">
<title confidence="0.996015">
Multi-Task Learning for Multiple Language Translation
</title>
<author confidence="0.990094">
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu and Haifeng Wang
</author>
<affiliation confidence="0.908422">
Baidu Inc, Beijing, China
</affiliation>
<email confidence="0.970324">
{dongdaxiang, wu hua, hewei06, yudianhai, wanghaifeng}@baidu.com
</email>
<sectionHeader confidence="0.993199" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999768227272727">
In this paper, we investigate the problem of
learning a machine translation model that
can simultaneously translate sentences
from one source language to multiple
target languages. Our solution is inspired
by the recently proposed neural machine
translation model which generalizes
machine translation as a sequence
learning problem. We extend the neural
machine translation to a multi-task
learning framework which shares source
language representation and separates
the modeling of different target language
translation. Our framework can be applied
to situations where either large amounts
of parallel data or limited parallel data
is available. Experiments show that
our multi-task learning model is able to
achieve significantly higher translation
quality over individually learned model in
both situations on the data sets publicly
available.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943214285715">
Translation from one source language to multiple
target languages at the same time is a difficult task
for humans. A person often needs to be familiar
with specific translation rules for different
language pairs. Machine translation systems
suffer from the same problems too. Under the
current classic statistical machine translation
framework, it is hard to share information across
different phrase tables among different language
pairs. Translation quality decreases rapidly when
the size of training corpus for some minority
language pairs becomes smaller. To conquer the
problems described above, we propose a
multi-task learning framework based on a
sequence learning model to conduct machine
translation from one source language to multiple
target languages, inspired by the recently
proposed neural machine translation(NMT)
framework proposed by Bahdanau et al. (2014).
Specifically, we extend the recurrent neural
network based encoder-decoder framework to a
multi-task learning model that shares an encoder
across all language pairs and utilize a different
decoder for each target language.
The neural machine translation approach has
recently achieved promising results in improving
translation quality. Different from conventional
statistical machine translation approaches, neural
machine translation approaches aim at learning
a radically end-to-end neural network model to
optimize translation performance by generalizing
machine translation as a sequence learning
problem. Based on the neural translation
framework, the lexical sparsity problem and the
long-range dependency problem in traditional
statistical machine translation can be alleviated
through neural networks such as long short-
term memory networks which provide great
lexical generalization and long-term sequence
memorization abilities.
The basic assumption of our proposed
framework is that many languages differ lexically
but are closely related on the semantic and/or the
syntactic levels. We explore such correlation
across different target languages and realize it
under a multi-task learning framework. We treat a
separate translation direction as a sub RNN
encode-decoder task in this framework which
shares the same encoder (i.e. the same source
language representation) across different
translation directions, and use a different decoder
for each specific target language. In this way, this
proposed multi-task learning model can make full
use of the source language corpora across
different language pairs. Since the encoder part
shares the same source language representation
</bodyText>
<page confidence="0.740063">
1723
</page>
<note confidence="0.967437333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1723–1732,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999793527777778">
across all the translation tasks, it may learn
semantic and structured predictive representations
that can not be learned with only a small amount
of data. Moreover, during training we jointly
model the alignment and the translation process
simultaneously for different language pairs under
the same framework. For example, when we
simultaneously translate from English into
Korean and Japanese, we can jointly learn latent
similar semantic and structure information across
Korea and Japanese because these two languages
share some common language structures.
The contribution of this work is three folds.
First, we propose a unified machine learning
framework to explore the problem of translating
one source language into multiple target
languages. To the best of our knowledge, this
problem has not been studied carefully in the
statistical machine translation field before.
Second, given large-scale training corpora for
different language pairs, we show that our
framework can improve translation quality on
each target language as compared with the neural
translation model trained on a single language
pair. Finally, our framework is able to alleviate
the data scarcity problem, using language pairs
with large-scale parallel training corpora to
improve the translation quality of those with few
parallel training corpus.
The following sections will be organized as
follows: in section 2, related work will be
described, and in section 3, we will describe our
multi-task learning method. Experiments that
demonstrate the effectiveness of our framework
will be described in section 4. Lastly, we will
conclude our work in section 5.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999828296875">
Statistical machine translation systems often rely
on large-scale parallel and monolingual training
corpora to generate translations of high quality.
Unfortunately, statistical machine translation
system often suffers from data sparsity problem
due to the fact that phrase tables are extracted from
the limited bilingual corpus. Much work has been
done to address the data sparsity problem such
as the pivot language approach (Wu and Wang,
2007; Cohn and Lapata, 2007) and deep learning
techniques (Devlin et al., 2014; Gao et al., 2014;
Sundermeyer et al., 2014; Liu et al., 2014).
On the problem of how to translate one source
language to many target languages within one
model, few work has been done in statistical
machine translation. A related work in SMT is
the pivot language approach for statistical machine
translation which uses a commonly used language
as a ”bridge” to generate source-target translation
for language pair with few training corpus. Pivot
based statistical machine translation is crucial in
machine translation for resource-poor language
pairs, such as Spanish to Chinese. Considering
the problem of translating one source language
to many target languages, pivot based SMT
approaches does work well given a large-scale
source language to pivot language bilingual corpus
and large-scale pivot language to target languages
corpus. However, in reality, language pairs
between English and many other target languages
may not be large enough, and pivot-based SMT
sometimes fails to handle this problem. Our
approach handles one to many target language
translation in a different way that we directly learn
an end to multi-end translation system that does
not need a pivot language based on the idea of
neural machine translation.
Neural Machine translation is a emerging
new field in machine translation, proposed
by several work recently (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014; Bahdanau
et al., 2014), aiming at end-to-end machine
translation without phrase table extraction and
language model training. Different from
traditional statistical machine translation, neural
machine translation encodes a variable-length
source sentence with a recurrent neural network
into a fixed-length vector representation and
decodes it with another recurrent neural network
from a fixed-length vector into variable-length
target sentence. A typical model is the RNN
encoder-decoder approach proposed by Bahdanau
et al. (2014), which utilizes a bidirectional
recurrent neural network to compress the source
sentence information and fits the conditional
probability of words in target languages with
a recurrent manner. Moreover, soft alignment
parameters are considered in this model. As a
specific example model in this paper, we adopt a
RNN encoder-decoder neural machine translation
model for multi-task learning, though all neural
network based model can be adapted in our
framework.
In the natural language processing field, a
</bodyText>
<page confidence="0.990118">
1724
</page>
<bodyText confidence="0.999968076923077">
notable work related with multi-task learning
was proposed by Collobert et al. (2011) which
shared common representation for input words
and solve different traditional NLP tasks such as
part-of-Speech tagging, name entity recognition
and semantic role labeling within one framework,
where the convolutional neural network model
was used. Hatori et al. (2012) proposed to
jointly train word segmentation, POS tagging and
dependency parsing, which can also be seen as
a multi-task learning approach. Similar idea has
also been proposed by Li et al. (2014) in Chinese
dependency parsing. Most of multi-task learning
or joint training frameworks can be summarized
as parameter sharing approaches proposed by
Ando and Zhang (2005) where they jointly trained
models and shared center parameters in NLP
tasks. Researchers have also explored similar
approaches (Sennrich et al., 2013; Cui et al., 2013)
in statistical machine translation which are often
refered as domain adaption. Our work explores the
possibility of machine translation under the multi-
task framework by using the recurrent neural
networks. To the best of our knowledge, this is the
first trial of end to end machine translation under
multi-task learning framework.
</bodyText>
<sectionHeader confidence="0.785855" genericHeader="method">
3 Multi-task Model for Multiple
Language Translation
</sectionHeader>
<bodyText confidence="0.999956555555556">
Our model is a general framework for translating
from one source language to many targets. The
model we build in this section is a recurrent
neural network based encoder-decoder model with
multiple target tasks, and each task is a specific
translation direction. Different tasks share the
same translation encoder across different language
pairs. We will describe model details in this
section.
</bodyText>
<subsectionHeader confidence="0.998807">
3.1 Objective Function
</subsectionHeader>
<bodyText confidence="0.999962882352941">
Given a pair of training sentence {x, y}, a
standard recurrent neural network based
encoder-decoder machine translation model fits a
parameterized model to maximize the conditional
probability of a target sentence y given a source
sentence x , i.e., argmaxp(y|x). We extend this
into multiple languages setting. In particular,
suppose we want to translate from English to
many different languages, for instance,
French(Fr), Dutch(Nl), Spanish(Es). Parallel
training data will be collected before training, i.e.
En-Fr, En-Nl, En-Es parallel sentences. Since the
English representation of the three language pairs
is shared in one encoder, the objective function
we optimize is the summation of several
conditional probability terms conditioned on
representation generated from the same encoder.
</bodyText>
<equation confidence="0.960095333333333">
1 Np
(N log p(yiTp  |xiTp; Θ))
p i
</equation>
<bodyText confidence="0.985296833333333">
where Θ = {Θsrc, ΘtrgTp , Tp = 1, 2, · · · , Tm},
Θsrc is a collection of parameters for source
encoder. And ΘtrgTp is the parameter set
of the Tpth target language. Np is the size
of parallel training corpus of the pth language
pair. For different target languages, the target
encoder parameters are seperated so we have Tm
decoders to optimize. This parameter sharing
strategy makes different language pairs maintain
the same semantic and structure information of the
source language and learn to translate into target
languages in different decoders.
</bodyText>
<subsectionHeader confidence="0.999859">
3.2 Model Details
</subsectionHeader>
<bodyText confidence="0.9722489">
Suppose we have several language pairs
(xTp, yTp) where Tp denotes the index of the Tpth
language pair. For a specific language pair, given
a sequence of source sentence input
(xTp
1 , xTp
2 , · · · , xTpn ), the goal is to jointly
maximize the conditional probability for each
generated target word. The probability of
generating the tth target word is estimated as:
</bodyText>
<equation confidence="0.954477666666667">
p(yTp
t |yTp
1 ,···,yTp
t−1, xTp) = g(yTp
t−1, sTp
t , cTp
</equation>
<bodyText confidence="0.93451575">
t )
where the function g is parameterized by a
feedforward neural network with a softmax output
layer. And g can be viewed as a probability
predictor with neural networks. sTp tis a recurrent
neural network hidden state at time t, which can
be estimated as:
TpTp Tp Tp 3
st = f (st−1, yt−1, ct ) ( )
the context vector cTp
t depends on a sequence of
annotations (h1, · · · , hL.) to which an encoder
maps the input sentence, where Lx is the
number of tokens in x. Each annotation hi
is a bidirectional recurrent representation with
forward and backward sequence information
</bodyText>
<equation confidence="0.988379428571429">
L(Θ) = argm� (E
Tp
1725
around the ith word.
L�
ctTp = aTp ijhj (4)
j=1
</equation>
<bodyText confidence="0.9163795">
where the weight aTp is a scalar computed by
tj
</bodyText>
<equation confidence="0.972026333333333">
exp(eTp
tj ) (5)
Tp
Ek=1 exp(e tkp)
eTp = 0(st−1Tp, hj) (6)
tj
</equation>
<bodyText confidence="0.979747">
aTp is a normalized score of etj which is a soft
tj
alignment model measuring how well the input
context around the jth word and the output word
in the tth position match. etj is modeled through a
perceptron-like function:
0(x, y) = vT tanh(Wx + Uy) (7)
To compute hj, a bidirectional recurrent neural
network is used. In the bidirectional recurrent
neural network, the representation of a forward
sequence and a backward sequence of the input
sentence is estimated and concatenated to be a
single vector. This concatenated vector can be
used to translate multiple languages during the test
time.
</bodyText>
<equation confidence="0.573511">
hj = [−→hj; ←− hj]T (8)
</equation>
<bodyText confidence="0.999927">
From a probabilistic perspective, our model is
able to learn the conditional distribution of several
target languages given the same source corpus.
Thus, the recurrent encoder-decoders are jointly
trained with several conditional probabilities
added together. As for the bidirectional recurrent
neural network module, we adopt the recently
proposed gated recurrent neural network (Cho
et al., 2014). The gated recurrent neural
network is shown to have promising results in
several sequence learning problem such as speech
recognition and machine translation where input
and output sequences are of variable length. It
is also shown that the gated recurrent neural
network has the ability to address the gradient
vanishing problem compared with the traditional
recurrent neural network, and thus the long-range
dependency problem in machine translation can
be handled well. In our multi-task learning
framework, the parameters of the gated recurrent
neural network in the encoder are shared, which is
formulated as follows.
</bodyText>
<equation confidence="0.99983625">
ht = (I − zt) O ht−1 + zt O ˆht (9)
zt = Q(Wzxt + Uzht−1) (10)
ˆht = tanh(Wxt + U(rt (D ht−1)) (11)
rt = Q(Wrxt + Urht−1) (12)
</equation>
<bodyText confidence="0.997154">
Where I is identity vector and O denotes element
wise product between vectors. tanh(x) and Q(x)
are nonlinear transformation functions that can be
applied element-wise on vectors. The recurrent
computation procedure is illustrated in 1, where
xt denotes one-hot vector for the tth word in a
sequence.
</bodyText>
<figureCaption confidence="0.69246">
Figure 1: Gated recurrent neural network
</figureCaption>
<bodyText confidence="0.98311725">
computation, where rt is a reset gate responsible
for memory unit elimination, and zt can be viewed
as a soft weight between current state information
and history information.
</bodyText>
<equation confidence="0.958272666666667">
ex − e−x
tanh(x) = (13)
ex + e−x
1
Q(x) = (14)
1 + e−x
</equation>
<bodyText confidence="0.999898666666667">
The overall model is illustrated in Figure 2
where the multi-task learning framework with
four target languages is demonstrated. The
soft alignment parameters Ai for each encoder-
decoder are different and only the bidirectional
recurrent neural network representation is shared.
</bodyText>
<subsectionHeader confidence="0.991115">
3.3 Optimization
</subsectionHeader>
<bodyText confidence="0.999985111111111">
The optimization approach we use is the
mini-batch stochastic gradient descent approach
(Bottou, 1991). The only difference between our
optimization and the commonly used stochastic
gradient descent is that we learn several mini-
batches within a fixed language pair for several
mini-batch iterations and then move onto the next
language pair. Our optimization procedure is
shown in Figure 3.
</bodyText>
<equation confidence="0.751665">
aTp
tj =
</equation>
<page confidence="0.903549">
1726
</page>
<figureCaption confidence="0.9997795">
Figure 2: Multi-task learning framework for multiple-target language translation
Figure 3: Optimization for end to multi-end model
</figureCaption>
<subsectionHeader confidence="0.996029">
3.4 Translation with Beam Search
</subsectionHeader>
<bodyText confidence="0.9999442">
Although parallel corpora are available for the
encoder and the decoder modeling in the training
phrase, the ground truth is not available during test
time. During test time, translation is produced by
finding the most likely sequence via beam search.
</bodyText>
<equation confidence="0.9962365">
Yˆ = argmax p(YTP|STP) (15)
Y
</equation>
<bodyText confidence="0.9999613">
Given the target direction we want to translate to,
beam search is performed with the shared encoder
and a specific target decoder where search space
belongs to the decoder Tp. We adopt beam search
algorithm similar as it is used in SMT system
(Koehn, 2004) except that we only utilize scores
produced by each decoder as features. The size
of beam is 10 in our experiments for speedup
consideration. Beam search is ended until the end-
of-sentence eos symbol is generated.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999993">
We conducted two groups of experiments to
show the effectiveness of our framework. The
goal of the first experiment is to show that
multi-task learning helps to improve translation
performance given enough training corpora for all
language pairs. In the second experiment, we
show that for some resource-poor language pairs
with a few parallel training data, their translation
performance could be improved as well.
</bodyText>
<subsectionHeader confidence="0.927986">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999987666666667">
The Europarl corpus is a multi-lingual corpus
including 21 European languages. Here we only
choose four language pairs for our experiments.
The source language is English for all language
pairs. And the target languages are Spanish
(Es), French (Fr), Portuguese (Pt) and Dutch
(Nl). To demonstrate the validity of our
learning framework, we do some preprocessing
on the training set. For the source language,
we use 30k of the most frequent words for
source language vocabulary which is shared
across different language pairs and 30k most
frequent words for each target language. Out-
of-vocabulary words are denoted as unknown
words, and we maintain different unknown word
labels for different languages. For test sets,
we also restrict all words in the test set to
be from our training vocabulary and mark the
OOV words as the corresponding labels as in
the training data. The size of training corpus in
experiment 1 and 2 is listed in Table 1 where
</bodyText>
<page confidence="0.982999">
1727
</page>
<table confidence="0.9995506">
Training Data Information
Lang En-Es En-Fr En-Nl En-Pt En-Nl-sub En-Pt-sub
Sent size 1,965,734 2,007,723 1,997,775 1,960,407 300,000 300,000
Src tokens 49,158,635 50,263,003 49,533,217 49,283,373 8,362,323 8,260,690
Trg tokens 51,622,215 52,525,000 50,661,711 54,996,139 8,590,245 8,334,454
</table>
<tableCaption confidence="0.999893">
Table 1: Size of training corpus for different language pairs
</tableCaption>
<bodyText confidence="0.9991677">
En-Nl-sub and En-Pt-sub are sub-sampled data
set of the full corpus. The full parallel training
corpus is available from the EuroParl corpus,
downloaded from EuroParl public websites1. We
mimic the situation that there are only a small-
scale parallel corpus available for some language
pairs by randomly sub-sampling the training data.
The parallel corpus of English-Portuguese and
English-Dutch are sub-sampled to approximately
15% of the full corpus size. We select two data
</bodyText>
<table confidence="0.955068">
Language pair En-Es En-Fr En-Nl En-Pt
Common test 1755 1755 1755 1755
WMT2013 3000 3000 - -
</table>
<tableCaption confidence="0.949389">
Table 2: Size of test set in EuroParl Common
testset and WMT2013
</tableCaption>
<bodyText confidence="0.998160333333333">
sets as our test data. One is the EuroParl Common
test set2 in European Parliament Corpus, the other
is WMT 2013 data set3. For WMT 2013, only
En-Fr, En-Es are available and we evaluate the
translation performance only on these two test
sets. Information of test sets is shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.992646">
4.2 Training Details
</subsectionHeader>
<bodyText confidence="0.999913666666667">
Our model is trained on Graphic Processing Unit
K40. Our implementation is based on the open
source deep learning package Theano (Bastien et
al., 2012) so that we do not need to take care
about gradient computations. During training, we
randomly shuffle our parallel training corpus for
each language pair at each epoch of our learning
process. The optimization algorithm and model
hyper parameters are listed below.
</bodyText>
<listItem confidence="0.9958168">
• Initialization of all parameters are from
uniform distribution between -0.01 and 0.01.
• We use stochastic gradient descent with
recently proposed learning rate decay
strategy Ada-Delta (Zeiler, 2012).
</listItem>
<footnote confidence="0.999983666666667">
1http:www.statmt.orgeuroparl
2http://www.statmt.org/wmt14/test.tgz
3http://matrix.statmt.org/test sets
</footnote>
<listItem confidence="0.999639222222222">
• Mini batch size in our model is set to 50 so
that the convergence speed is fast.
• We train 1000 mini batches of data in one
language pair before we switch to the next
language pair.
• For word representation dimensionality, we
use 1000 for both source language and target
language.
• The size of hidden layer is set to 1000.
</listItem>
<bodyText confidence="0.9999904">
We trained our multi-task model with a multi-
GPU implementation due to the limitation of
Graphic memory. And each target decoder is
trained within one GPU card, and we synchronize
our source encoder every 1000 batches among all
GPU card. Our model costs about 72 hours on full
large parallel corpora training until convergence
and about 24 hours on partial parallel corpora
training. During decoding, our implementation on
GPU costs about 0.5 second per sentence.
</bodyText>
<subsectionHeader confidence="0.988945">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999980473684211">
We evaluate the effectiveness of our method with
EuroParl Common testset and WMT 2013 dataset.
BLEU-4 (Papineni et al., 2002) is used as the
evaluation metric. We evaluate BLEU scores on
EuroParl Common test set with multi-task NMT
models and single NMT models to demonstrate
the validity of our multi-task learning framework.
On the WMT 2013 data sets, we compare
performance of separately trained NMT models,
multi-task NMT models and Moses. We use the
EuroParl Common test set as a development set in
both neural machine translation experiments and
Moses experiments. For single NMT models and
multi-task NMT models, we select the best model
with the highest BLEU score in the EuroParl
Common testset and apply it to the WMT 2013
dataset. Note that our experiment settings in NMT
is equivalent with Moses, considering the same
training corpus, development sets and test sets.
</bodyText>
<page confidence="0.979513">
1728
</page>
<subsectionHeader confidence="0.996392">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9994298">
We report our results of three experiments to
show the validity of our methods. In the first
experiment, we train multi-task learning model
jointly on all four parallel corpora and compare
BLEU scores with models trained separately on
each parallel corpora. In the second experiment,
we utilize the same training procedures as
Experiment 1, except that we mimic the situation
where some parallel corpora are resource-poor and
maintain only 15% data on two parallel training
corpora. In experiment 3, we test our learned
model from experiment 1 and experiment 2 on
WMT 2013 dataset. Table 3 and 4 show the
case-insensitive BLEU scores on the Europarl
common test data. Models learned from the multi-
task learning framework significantly outperform
the models trained separately. Table 4 shows
that given only 15% of parallel training corpus
of English-Dutch and English-Portuguese, it is
possible to improve translation performance on all
the target languages as well. This result makes
sense because the correlated languages benefit
from each other by sharing the same predictive
structure, e.g. French, Spanish and Portuguese, all
of which are from Latin. We also notice that even
though Dutch is from Germanic languages, it is
also possible to increase translation performance
under our multi-task learning framework which
demonstrates the generalization of our model to
multiple target languages.
</bodyText>
<table confidence="0.999331">
Lang-Pair En-Es En-Fr En-Nl En-Pt
Single NMT 26.65 21.22 28.75 20.27
Multi Task 28.03 22.47 29.88 20.75
Delta +1.38 +1.25 +1.13 +0.48
</table>
<tableCaption confidence="0.718921">
Table 3: Multi-task neural translation v.s. single
model given large-scale corpus in all language
pairs
</tableCaption>
<bodyText confidence="0.999710125">
We tested our selected model on the WMT 2013
dataset. Our results are shown in Table 5 where
Multi-Full is the model with Experiment 1 setting
and the model of Multi-Partial uses the same
setting in Experiment 2. The English-French
and English-Spanish translation performances are
improved significantly compared with models
trained separately on each language pair. Note
</bodyText>
<table confidence="0.99929675">
Lang-Pair En-Es En-Fr En-Nl* En-Pt*
Single NMT 26.65 21.22 26.59 18.26
Multi Task 28.29 21.89 27.85 19.32
Delta +1.64 +0.67 +1.26 +1.06
</table>
<tableCaption confidence="0.986225">
Table 4: Multi-task neural translation v.s. single
</tableCaption>
<bodyText confidence="0.9894146">
model with a small-scale training corpus on some
language pairs. * means that the language pair is
sub-sampled.
that this result is not comparable with the result
reported in (Bahdanau et al., 2014) as we use
much less training corpus. We also compare our
trained models with Moses. On the WMT 2013
data set, we utilize parallel corpora for Moses
training without any extra resource such as large-
scale monolingual corpus. From Table 5, it is
shown that neural machine translation models
have comparable BLEU scores with Moses. On
the WMT 2013 test set, multi-task learning model
outperforms both single model and Moses results
significantly.
</bodyText>
<subsectionHeader confidence="0.999961">
4.5 Model Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.999992333333334">
We try to make empirical analysis through
learning curves and qualitative results to explain
why multi-task learning framework works well in
multiple-target machine translation problem.
From the learning process, we observed that the
speed of model convergence under multi-task
learning is faster than models trained separately
especially when a model is trained for resource-
poor language pairs. The detailed learning curves
are shown in Figure 4. Here we study the
learning curve for resource-poor language pairs,
i.e. English-Dutch and En-Portuguese, for which
only 15% of the bilingual data is sampled for
training. The BLEU scores are evaluated on the
Europarl common test set. From Figure 4, it
can be seen that in the early stage of training,
given the same amount of training data for each
language pair, the translation performance of
the multi-task learning model is improved more
rapidly. And the multi-task models achieve better
translation quality than separately trained models
within three iterations of training. The reason
of faster and better convergence in performance
is that the encoder parameters are shared across
different language pairs, which can make full use
of all the source language training data across the
language pairs and improve the source language
</bodyText>
<page confidence="0.979249">
1729
</page>
<table confidence="0.991079333333333">
Nmt Baseline Nmt Multi-Full Nmt Multi-Partial Moses
En-Fr 23.89 26.02(+2.13) 25.01(+1.12) 23.83
En-Es 23.28 25.31(+2.03) 25.83(+2.55) 23.58
</table>
<tableCaption confidence="0.997239">
Table 5: Multi-task NMT v.s. single model v.s. moses on the WMT 2013 test set
</tableCaption>
<figureCaption confidence="0.994214">
Figure 4: Faster and Better convergence in Multi-task Learning in multiple language translation
</figureCaption>
<bodyText confidence="0.999493448275862">
representation.
The sharing of encoder parameters is useful
especially for the resource-poor language pairs.
In the multi-task learning framework, the amount
of the source language is not limited by the
resource-poor language pairs and we are able to
learn better representation for the source language.
Thus the representation of the source language
learned from the multi-task model is more stable,
and can be viewed as a constraint that leverages
translation performance of all language pairs.
Therefore, the overfitting problem and the data
scarcity problem can be alleviated for language
pairs with only a few training data. In Table 6,
we list the three nearest neighbors of some source
words whose similarity is computed by using
the cosine score of the embeddings both in the
multi-task learning framework (from Experiment
two ) and in the single model (the resource-
poor English-Portuguese model). Although the
nearest neighbors of the high-frequent words such
as numbers can be learned both in the multi-task
model and the single model, the overall quality of
the nearest neighbors learned by the resource-poor
single model is much poorer compared with the
multi-task model.
The multi-task learning framework also generates
translations of higher quality. Some examples are
shown in Table 7. The examples are from the
</bodyText>
<table confidence="0.967424588235294">
MultiTask Nearest neighbors
provide deliver 0.78, providing 0.74,
give 0.72
crime terrorism 0.66, criminal 0.65,
homelessness 0.65
regress condense 0.74, mutate 0.71,
evolve 0.70
six eight 0.98,seven 0.96, 12 0.94
Single-Resource-Poor Nearest Neighbors
provide though 0.67,extending 0.56,
parliamentarians 0.44
crime care 0.75, remember 0.56, three
0.53
regress committing 0.33, accuracy
0.30, longed-for 0.28
six eight 0.87, three 0.69, thirteen
0.65
</table>
<tableCaption confidence="0.984662">
Table 6: Source language nearest-neighbor comparison
between the multi-task model and the single model
</tableCaption>
<bodyText confidence="0.992721333333333">
WMT 2013 test set. The French and Spanish
translations generated by the multi-task learning
model and the single model are shown in the table.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999282">
In this paper, we investigate the problem of how to
translate one source language into several different
target languages within a unified translation
model. Our proposed solution is based on the
</bodyText>
<page confidence="0.92061">
1730
</page>
<table confidence="0.975794260869565">
English Students, meanwhile, say the course is
one of the most interesting around.
Reference-Fr Les ´etudiants, pour leur part, assurent
que le cours est l’ un des plus
int´eressants.
Single-Fr Les ´etudiants, entre-temps, disent
entendu l’ une des plus int´eressantes.
Multi-Fr Les ´etudiants, en attendant, disent qu’ il
est l’ un des sujets les plus int´eressants.
English In addition, they limited the right
of individuals and groups to provide
assistance to voters wishing to register.
Reference-Fr De plus, ils ont limit´e le droit de
personnes et de groupes de fournir
une assistance aux ´electeurs d´esirant s’
inscrire.
Single-Fr En outre, ils limitent le droit des
particuliers et des groupes pour fournir
l’ assistance aux ´electeurs.
Multi-Fr De plus, ils restreignent le droit des
individus et des groupes a` fournir une
assistance aux ´electeurs qui souhaitent
enregistrer.
</table>
<tableCaption confidence="0.7963165">
Table 7: Translation of different target languages
given the same input in our multi-task model.
</tableCaption>
<bodyText confidence="0.999839766666667">
recently proposed recurrent neural network based
encoder-decoder framework. We train a unified
neural machine translation model under the multi-
task learning framework where the encoder is
shared across different language pairs and each
target language has a separate decoder. To the
best of our knowledge, the problem of learning
to translate from one source to multiple targets
has seldom been studied. Experiments show that
given large-scale parallel training data, the multi-
task neural machine translation model is able
to learn good predictive structures in translating
multiple targets. Significant improvement can be
observed from our experiments on the data sets
publicly available. Moreover, our framework is
able to address the data scarcity problem of some
resource-poor language pairs by utilizing large-
scale parallel training corpora of other language
pairs to improve the translation quality. Our model
is efficient and gets faster and better convergence
for both resource-rich and resource-poor language
pair under the multi-task learning.
In the future, we would like to extend our
learning framework to more practical setting. For
example, train a multi-task learning model with
the same target language from different domains
to improve multiple domain translation within
one model. The correlation of different target
languages will also be considered in the future
work.
</bodyText>
<sectionHeader confidence="0.967438" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9979015">
This paper is supported by the 973 program
No. 2014CB340505. We would like to
thank anonymous reviewers for their insightful
comments.
</bodyText>
<sectionHeader confidence="0.999042" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99961780952381">
Rie Kubota Ando and Tong Zhang. 2005. A
framework for learning predictive structures from
multiple tasks and unlabeled data. Journal of
Machine Learning Research, 6:1817–1853.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud
Bergeron, Nicolas Bouchard, David Warde-Farley,
and Yoshua Bengio. 2012. Theano: new features
and speed improvements. CoRR, abs/1211.5590.
L´eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-Nimes
91, Nimes, France. EC2.
KyungHyun Cho, Bart van Merrienboer, Dzmitry
Bahdanau, and Yoshua Bengio. 2014. On the
properties of neural machine translation: Encoder-
decoder approaches. CoRR, abs/1409.1259.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use of
multi-parallel corpora. In Proc. ACL, pages 728–
735.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,
Mu Li, and Ming Zhou. 2013. Multi-domain
adaptation for SMT using multi-task learning. In
Proc. EMNLP, pages 1055–1065.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard M. Schwartz, and John Makhoul.
2014. Fast and robust neural network joint models
for statistical machine translation. In Proc. ACL,
pages 1370–1380.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase
representations for translation modeling. In Proc.
ACL, pages 699–709.
</reference>
<page confidence="0.812118">
1731
</page>
<reference confidence="0.999685795918367">
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2012. Incremental joint approach to
word segmentation, POS tagging, and dependency
parsing in chinese. In Proc. ACL, pages 1045–1053.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proc. EMNLP,
pages 1700–1709.
Philipp Koehn. 2004. Pharaoh: A beam
search decoder for phrase-based statistical machine
translation models. In Machine Translation:
From Real Users to Research, 6th Conference of
the Association for Machine Translation in the
Americas, AMTA 2004, Washington, DC, USA,
September 28-October 2, 2004, Proceedings, pages
115–124.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,
and Wenliang Chen. 2014. Joint optimization
for chinese POS tagging and dependency parsing.
IEEE/ACM Transactions on Audio, Speech &amp;
Language Processing, 22(1):274–286.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proc. ACL, pages 1491–
1500.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proc. ACL,
ACL 2002, pages 311–318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model framework
for statistical machine translation. In Proc. ACL,
pages 832–840.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling
with bidirectional recurrent neural networks. In
Proc. EMNLP, pages 14–25.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014. Sequence to sequence learning with neural
networks. In Advances in Neural Information
Processing Systems 27: Annual Conference on
Neural Information Processing Systems 2014,
December 8-13 2014, Montreal, Quebec, Canada,
pages 3104–3112.
Hua Wu and Haifeng Wang. 2007. Pivot
language approach for phrase-based statistical
machine translation. In Proc. ACL, pages 165–181.
Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.
</reference>
<page confidence="0.994226">
1732
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196470">
<title confidence="0.999915">Multi-Task Learning for Multiple Language Translation</title>
<author confidence="0.477217">Daxiang Dong</author>
<author confidence="0.477217">Hua Wu</author>
<author confidence="0.477217">Wei He</author>
<author confidence="0.477217">Dianhai Yu</author>
<author confidence="0.477217">Haifeng</author>
<affiliation confidence="0.334548">Baidu Inc, Beijing, China</affiliation>
<email confidence="0.723614">wuhua,hewei06,yudianhai,</email>
<abstract confidence="0.99699452173913">In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="9256" citStr="Ando and Zhang (2005)" startWordPosition="1339" endWordPosition="1342"> representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework. 3 Multi-task Model for Multiple Language Translation Our model is a general framework for translati</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2014</date>
<location>CoRR, abs/1409.0473.</location>
<contexts>
<context position="1952" citStr="Bahdanau et al. (2014)" startWordPosition="273" endWordPosition="276">he same problems too. Under the current classic statistical machine translation framework, it is hard to share information across different phrase tables among different language pairs. Translation quality decreases rapidly when the size of training corpus for some minority language pairs becomes smaller. To conquer the problems described above, we propose a multi-task learning framework based on a sequence learning model to conduct machine translation from one source language to multiple target languages, inspired by the recently proposed neural machine translation(NMT) framework proposed by Bahdanau et al. (2014). Specifically, we extend the recurrent neural network based encoder-decoder framework to a multi-task learning model that shares an encoder across all language pairs and utilize a different decoder for each target language. The neural machine translation approach has recently achieved promising results in improving translation quality. Different from conventional statistical machine translation approaches, neural machine translation approaches aim at learning a radically end-to-end neural network model to optimize translation performance by generalizing machine translation as a sequence learn</context>
<context position="7508" citStr="Bahdanau et al., 2014" startWordPosition="1087" endWordPosition="1090"> language to target languages corpus. However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem. Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation. Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase table extraction and language model training. Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence. A typical model is the RNN encoder-decoder approach proposed by Bahdanau et al. (2014), which utilizes a bidirectional recurrent neural network to compress the source sentence</context>
<context position="24299" citStr="Bahdanau et al., 2014" startWordPosition="3762" endWordPosition="3765">setting and the model of Multi-Partial uses the same setting in Experiment 2. The English-French and English-Spanish translation performances are improved significantly compared with models trained separately on each language pair. Note Lang-Pair En-Es En-Fr En-Nl* En-Pt* Single NMT 26.65 21.22 26.59 18.26 Multi Task 28.29 21.89 27.85 19.32 Delta +1.64 +0.67 +1.26 +1.06 Table 4: Multi-task neural translation v.s. single model with a small-scale training corpus on some language pairs. * means that the language pair is sub-sampled. that this result is not comparable with the result reported in (Bahdanau et al., 2014) as we use much less training corpus. We also compare our trained models with Moses. On the WMT 2013 data set, we utilize parallel corpora for Moses training without any extra resource such as largescale monolingual corpus. From Table 5, it is shown that neural machine translation models have comparable BLEU scores with Moses. On the WMT 2013 test set, multi-task learning model outperforms both single model and Moses results significantly. 4.5 Model Analysis and Discussion We try to make empirical analysis through learning curves and qualitative results to explain why multi-task learning frame</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>James Bergstra</author>
<author>Ian J Goodfellow</author>
<author>Arnaud Bergeron</author>
<author>Nicolas Bouchard</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: new features and speed improvements.</title>
<date>2012</date>
<location>CoRR, abs/1211.5590.</location>
<contexts>
<context position="19617" citStr="Bastien et al., 2012" startWordPosition="3028" endWordPosition="3031">ta Language pair En-Es En-Fr En-Nl En-Pt Common test 1755 1755 1755 1755 WMT2013 3000 3000 - - Table 2: Size of test set in EuroParl Common testset and WMT2013 sets as our test data. One is the EuroParl Common test set2 in European Parliament Corpus, the other is WMT 2013 data set3. For WMT 2013, only En-Fr, En-Es are available and we evaluate the translation performance only on these two test sets. Information of test sets is shown in Table 2. 4.2 Training Details Our model is trained on Graphic Processing Unit K40. Our implementation is based on the open source deep learning package Theano (Bastien et al., 2012) so that we do not need to take care about gradient computations. During training, we randomly shuffle our parallel training corpus for each language pair at each epoch of our learning process. The optimization algorithm and model hyper parameters are listed below. • Initialization of all parameters are from uniform distribution between -0.01 and 0.01. • We use stochastic gradient descent with recently proposed learning rate decay strategy Ada-Delta (Zeiler, 2012). 1http:www.statmt.orgeuroparl 2http://www.statmt.org/wmt14/test.tgz 3http://matrix.statmt.org/test sets • Mini batch size in our mo</context>
</contexts>
<marker>Bastien, Lamblin, Pascanu, Bergstra, Goodfellow, Bergeron, Bouchard, Warde-Farley, Bengio, 2012</marker>
<rawString>Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, and Yoshua Bengio. 2012. Theano: new features and speed improvements. CoRR, abs/1211.5590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Stochastic gradient learning in neural networks.</title>
<date>1991</date>
<booktitle>In Proceedings of Neuro-Nimes 91,</booktitle>
<location>Nimes,</location>
<contexts>
<context position="15553" citStr="Bottou, 1991" startWordPosition="2380" endWordPosition="2381">ere rt is a reset gate responsible for memory unit elimination, and zt can be viewed as a soft weight between current state information and history information. ex − e−x tanh(x) = (13) ex + e−x 1 Q(x) = (14) 1 + e−x The overall model is illustrated in Figure 2 where the multi-task learning framework with four target languages is demonstrated. The soft alignment parameters Ai for each encoderdecoder are different and only the bidirectional recurrent neural network representation is shared. 3.3 Optimization The optimization approach we use is the mini-batch stochastic gradient descent approach (Bottou, 1991). The only difference between our optimization and the commonly used stochastic gradient descent is that we learn several minibatches within a fixed language pair for several mini-batch iterations and then move onto the next language pair. Our optimization procedure is shown in Figure 3. aTp tj = 1726 Figure 2: Multi-task learning framework for multiple-target language translation Figure 3: Optimization for end to multi-end model 3.4 Translation with Beam Search Although parallel corpora are available for the encoder and the decoder modeling in the training phrase, the ground truth is not avai</context>
</contexts>
<marker>Bottou, 1991</marker>
<rawString>L´eon Bottou. 1991. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nimes 91, Nimes, France. EC2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>KyungHyun Cho</author>
<author>Bart van Merrienboer</author>
<author>Dzmitry Bahdanau</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the properties of neural machine translation: Encoderdecoder approaches.</title>
<date>2014</date>
<location>CoRR, abs/1409.1259.</location>
<marker>Cho, van Merrienboer, Bahdanau, Bengio, 2014</marker>
<rawString>KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoderdecoder approaches. CoRR, abs/1409.1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine translation by triangulation: Making effective use of multi-parallel corpora.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>728--735</pages>
<contexts>
<context position="6034" citStr="Cohn and Lapata, 2007" startWordPosition="859" endWordPosition="862">iments that demonstrate the effectiveness of our framework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pa</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine translation by triangulation: Making effective use of multi-parallel corpora. In Proc. ACL, pages 728– 735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="8615" citStr="Collobert et al. (2011)" startWordPosition="1243" endWordPosition="1246">d by Bahdanau et al. (2014), which utilizes a bidirectional recurrent neural network to compress the source sentence information and fits the conditional probability of words in target languages with a recurrent manner. Moreover, soft alignment parameters are considered in this model. As a specific example model in this paper, we adopt a RNN encoder-decoder neural machine translation model for multi-task learning, though all neural network based model can be adapted in our framework. In the natural language processing field, a 1724 notable work related with multi-task learning was proposed by Collobert et al. (2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing app</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Xilun Chen</author>
<author>Dongdong Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Multi-domain adaptation for SMT using multi-task learning.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1055--1065</pages>
<contexts>
<context position="9425" citStr="Cui et al., 2013" startWordPosition="1365" endWordPosition="1368">work, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework. 3 Multi-task Model for Multiple Language Translation Our model is a general framework for translating from one source language to many targets. The model we build in this section is a recurrent neural network based encoder-decoder model with multiple target tasks, and</context>
</contexts>
<marker>Cui, Chen, Zhang, Liu, Li, Zhou, 2013</marker>
<rawString>Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu, Mu Li, and Ming Zhou. 2013. Multi-domain adaptation for SMT using multi-task learning. In Proc. EMNLP, pages 1055–1065.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard M Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1370--1380</pages>
<contexts>
<context position="6084" citStr="Devlin et al., 2014" startWordPosition="867" endWordPosition="870">ework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the p</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard M. Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proc. ACL, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>699--709</pages>
<contexts>
<context position="6102" citStr="Gao et al., 2014" startWordPosition="871" endWordPosition="874">ed in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the problem of translat</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proc. ACL, pages 699–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint approach to word segmentation, POS tagging, and dependency parsing in chinese.</title>
<date>2012</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1045--1053</pages>
<contexts>
<context position="8889" citStr="Hatori et al. (2012)" startWordPosition="1282" endWordPosition="1285">s model. As a specific example model in this paper, we adopt a RNN encoder-decoder neural machine translation model for multi-task learning, though all neural network based model can be adapted in our framework. In the natural language processing field, a 1724 notable work related with multi-task learning was proposed by Collobert et al. (2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as d</context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2012</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2012. Incremental joint approach to word segmentation, POS tagging, and dependency parsing in chinese. In Proc. ACL, pages 1045–1053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="7460" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1079" endWordPosition="1082">to pivot language bilingual corpus and large-scale pivot language to target languages corpus. However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem. Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation. Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase table extraction and language model training. Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence. A typical model is the RNN encoder-decoder approach proposed by Bahdanau et al. (2014), which utilizes a bidirectional recurren</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proc. EMNLP, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Machine Translation: From Real Users to Research, 6th Conference of the Association for Machine Translation in the Americas, AMTA 2004,</booktitle>
<pages>115--124</pages>
<location>Washington, DC, USA,</location>
<contexts>
<context position="16559" citStr="Koehn, 2004" startWordPosition="2542" endWordPosition="2543">Optimization for end to multi-end model 3.4 Translation with Beam Search Although parallel corpora are available for the encoder and the decoder modeling in the training phrase, the ground truth is not available during test time. During test time, translation is produced by finding the most likely sequence via beam search. Yˆ = argmax p(YTP|STP) (15) Y Given the target direction we want to translate to, beam search is performed with the shared encoder and a specific target decoder where search space belongs to the decoder Tp. We adopt beam search algorithm similar as it is used in SMT system (Koehn, 2004) except that we only utilize scores produced by each decoder as features. The size of beam is 10 in our experiments for speedup consideration. Beam search is ended until the endof-sentence eos symbol is generated. 4 Experiments We conducted two groups of experiments to show the effectiveness of our framework. The goal of the first experiment is to show that multi-task learning helps to improve translation performance given enough training corpora for all language pairs. In the second experiment, we show that for some resource-poor language pairs with a few parallel training data, their transla</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Machine Translation: From Real Users to Research, 6th Conference of the Association for Machine Translation in the Americas, AMTA 2004, Washington, DC, USA, September 28-October 2, 2004, Proceedings, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Wenliang Chen</author>
</authors>
<title>Joint optimization for chinese POS tagging and dependency parsing.</title>
<date>2014</date>
<journal>IEEE/ACM Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="9084" citStr="Li et al. (2014)" startWordPosition="1314" endWordPosition="1317">r framework. In the natural language processing field, a 1724 notable work related with multi-task learning was proposed by Collobert et al. (2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first tria</context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, 2014</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, and Wenliang Chen. 2014. Joint optimization for chinese POS tagging and dependency parsing. IEEE/ACM Transactions on Audio, Speech &amp; Language Processing, 22(1):274–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1491--1500</pages>
<contexts>
<context position="6147" citStr="Liu et al., 2014" startWordPosition="879" endWordPosition="882">r work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the problem of translating one source language to many target langua</context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proc. ACL, pages 1491– 1500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL, ACL</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21121" citStr="Papineni et al., 2002" startWordPosition="3263" endWordPosition="3266">er is set to 1000. We trained our multi-task model with a multiGPU implementation due to the limitation of Graphic memory. And each target decoder is trained within one GPU card, and we synchronize our source encoder every 1000 batches among all GPU card. Our model costs about 72 hours on full large parallel corpora training until convergence and about 24 hours on partial parallel corpora training. During decoding, our implementation on GPU costs about 0.5 second per sentence. 4.3 Evaluation We evaluate the effectiveness of our method with EuroParl Common testset and WMT 2013 dataset. BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. We evaluate BLEU scores on EuroParl Common test set with multi-task NMT models and single NMT models to demonstrate the validity of our multi-task learning framework. On the WMT 2013 data sets, we compare performance of separately trained NMT models, multi-task NMT models and Moses. We use the EuroParl Common test set as a development set in both neural machine translation experiments and Moses experiments. For single NMT models and multi-task NMT models, we select the best model with the highest BLEU score in the EuroParl Common testset and apply it to the W</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proc. ACL, ACL 2002, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Holger Schwenk</author>
<author>Walid Aransa</author>
</authors>
<title>A multi-domain translation model framework for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>832--840</pages>
<contexts>
<context position="9406" citStr="Sennrich et al., 2013" startWordPosition="1361" endWordPosition="1364">beling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework. 3 Multi-task Model for Multiple Language Translation Our model is a general framework for translating from one source language to many targets. The model we build in this section is a recurrent neural network based encoder-decoder model with multipl</context>
</contexts>
<marker>Sennrich, Schwenk, Aransa, 2013</marker>
<rawString>Rico Sennrich, Holger Schwenk, and Walid Aransa. 2013. A multi-domain translation model framework for statistical machine translation. In Proc. ACL, pages 832–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation modeling with bidirectional recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>14--25</pages>
<contexts>
<context position="6128" citStr="Sundermeyer et al., 2014" startWordPosition="875" endWordPosition="878">astly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the problem of translating one source language to</context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proc. EMNLP, pages 14–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</booktitle>
<pages>3104--3112</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="7484" citStr="Sutskever et al., 2014" startWordPosition="1083" endWordPosition="1086">us and large-scale pivot language to target languages corpus. However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem. Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation. Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase table extraction and language model training. Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence. A typical model is the RNN encoder-decoder approach proposed by Bahdanau et al. (2014), which utilizes a bidirectional recurrent neural network to comp</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Pivot language approach for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>165--181</pages>
<contexts>
<context position="6010" citStr="Wu and Wang, 2007" startWordPosition="855" endWordPosition="858">rning method. Experiments that demonstrate the effectiveness of our framework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for r</context>
</contexts>
<marker>Wu, Wang, 2007</marker>
<rawString>Hua Wu and Haifeng Wang. 2007. Pivot language approach for phrase-based statistical machine translation. In Proc. ACL, pages 165–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>ADADELTA: an adaptive learning rate method.</title>
<date>2012</date>
<location>CoRR, abs/1212.5701.</location>
<contexts>
<context position="20085" citStr="Zeiler, 2012" startWordPosition="3101" endWordPosition="3102">del is trained on Graphic Processing Unit K40. Our implementation is based on the open source deep learning package Theano (Bastien et al., 2012) so that we do not need to take care about gradient computations. During training, we randomly shuffle our parallel training corpus for each language pair at each epoch of our learning process. The optimization algorithm and model hyper parameters are listed below. • Initialization of all parameters are from uniform distribution between -0.01 and 0.01. • We use stochastic gradient descent with recently proposed learning rate decay strategy Ada-Delta (Zeiler, 2012). 1http:www.statmt.orgeuroparl 2http://www.statmt.org/wmt14/test.tgz 3http://matrix.statmt.org/test sets • Mini batch size in our model is set to 50 so that the convergence speed is fast. • We train 1000 mini batches of data in one language pair before we switch to the next language pair. • For word representation dimensionality, we use 1000 for both source language and target language. • The size of hidden layer is set to 1000. We trained our multi-task model with a multiGPU implementation due to the limitation of Graphic memory. And each target decoder is trained within one GPU card, and we </context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>