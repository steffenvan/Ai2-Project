<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000290">
<title confidence="0.975884">
Answer Validation by Information Distance Calculation
</title>
<author confidence="0.933835">
Fangtao Li, Xian Zhang, Xiaoyan Zhu
</author>
<affiliation confidence="0.948970666666667">
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.9906">
zxy-dcs@mail.tsinghua.edu.cn
</email>
<sectionHeader confidence="0.997308" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995975">
In this paper,an information distance based
approach is proposed to perform answer
validation for question answering system.
To validate an answer candidate, the ap-
proach calculates the conditional informa-
tion distance between the question focus
and the candidate under certain condition
pattern set. Heuristic methods are de-
signed to extract question focus and gen-
erate proper condition patterns from ques-
tion. General search engines are employed
to estimate the Kolmogorov complexity,
hence the information distance. Experi-
mental results show that our approach is
stable and flexible, and outperforms tradi-
tional tfidf methods.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995688">
Question answering(QA) system aims at finding
exact answers to a natural language question. In
order to correctly answer a question, several com-
ponents are implemented including question clas-
sification, passage retrieval, answer candidates
generation, answer validation etc. Answer Vali-
dation is to decide whether the candidate answers
are correct or not, or even to determine the accu-
rate confidence score to them. Most of QA systems
employ answer validation as the last step to iden-
tify the correct answer. If this component fails, it
is impossible to enable the question to be correctly
answered.
Automatic techniques for answer validation are
of great interest among question answering re-
</bodyText>
<footnote confidence="0.90307">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999730710526316">
search. With automatic answer validation, the
system will carry out different refinements of its
searching criteria to check the relevance of new
candidate answers. In addition, since most of
QA systems rely on complex architectures and the
evaluation of their performances requires a huge
amount of work, the automatic assessment of can-
didates with respect to a given question will speed
up both algorithm refinement and testing.
Currently, answer validation is mainly viewed
as a classification problem or ranking problem.
Different models, such as Support Vector Ma-
chine (Shen and Klakow, 2006) and Maximum En-
tropy Model (Ittycheriah et al., 2001), are used to
integrate sophisticated linguistic features to deter-
mine the correctness of candidates. The answer
validation exercise (Penas et al. , 2007) aims at
developing systems able to decide whether the an-
swer is correct or not. They formulate answer val-
idation as a text entailment problem. These ap-
proaches are dependent on sophisticated linguis-
tic analysis of syntactic and semantic relations be-
tween question and candidates. It is quite expen-
sive to use deep analysis for automatic answer val-
idation, especially in large scale data set. Thus it
is appropriate to find an alternative solution to this
problem. Here, we just consider the English an-
swer validation task.
This paper proposes a novel approach based on
information retrieval on the Web. The answer val-
idation problem is reformulated as distance calcu-
lation from an answer candidate to a question. The
hypothesis is that, among all candidates, the cor-
rect answer has the smallest distance from ques-
tion. We employ conditional normalized min dis-
tance, which is based on Kolmogorov Complexity
theory (Li and Vitanyi, 1997), for this task. The
distance measures the relevance between question
</bodyText>
<page confidence="0.99041">
42
</page>
<note confidence="0.9945365">
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 42–49
Manchester, UK. August 2008
</note>
<bodyText confidence="0.999913333333333">
focus and candidates conditioned on a surface pat-
tern set. For distance calculation, we first ex-
tract the question focus, and then a hierarchical
pattern set is automatically constructed as condi-
tion. Since Kolmogrov Complexity can be approx-
imated through frequency counts. Two types of
search engine “Google” and “Altavista” are used
to approximate the distance.
The paper is organized as follows: Section 2
describes related work. The fundamental Kol-
mogorov Complexity theory is introduced in Sec-
tion 3. Section 4 presents our proposed answer val-
idation method based on information retrieval. In
Section 5, we describe the experiments and discus-
sions. The paper is concluded in Section 6.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999830771929825">
Answer Validation is an emerging topic in Ques-
tion Answering, where open domain systems are
often required to rank huge amounts of answer
candidates. This task can be viewed as a classi-
fication problem or re-ranking problem.
Early question answering systems focused on
employing surface text patterns (Subbotin and
Subbotin, 2001) for answer validation. Xu et
al. (2003) identified that pattern-based approaches
got bad performances due to poor system recall.
Some researchers exploited machine learning tech-
niques with rich syntactic or semantic features to
measure the similarity between question and an-
swer. Ittycheriah et al. (2001) used Maximum En-
tropy model to combine rich features and automat-
ically learn feature weights. These features in-
cluded query expansion features, focus features,
named entity features, dependency relation fea-
tures, pattern features et al. Shen and Klakow
(2006) presented three methods, including feature
vector, string kernel and tree kernel, to represent
surface text features and parse tree features in Sup-
port Vector Machines. Ko et al. (2007) pro-
posed a probabilistic graphical model to estimate
the probability of correctness for all candidate an-
swers. Four types of features were employed,
including knowledge-based features, data-driven
features, string distance feature and synonym fea-
tures.
Started in 2006, the annual Answer Validation
Exercise (Penas et al. , 2007) aims to develop sys-
tems to decide if the answer to a question is correct
or not. The English answer validation task is refor-
mulated as a Text Entailment problem. The triplet,
including question, answer and supporting text, is
given. The system determines if the supporting
text can entail the hypothesis, which is a reformu-
lation from the question and answer. All partici-
pants used lexical processing, including lemmati-
zation and part-of speech tagging. Some systems
used first order logic representations, performed
semantic analysis and took the validation decision
with a theorem proof.
The above approaches should process deep syn-
tactic and semantic analysis for either questions or
candidate answers. The annotated linguistic re-
source is hard to acquire for the supervised clas-
sification problem. Another alternative solution
for answer validation is to exploit the redundancy
of large scale data. Eric et al. (2007) devel-
oped AskMSR question answering system. They
focus on the Web as a gigantic data repository
with tremendous redundancy that can be exploited
to extract the correct answer. Lin (2007) im-
plemented another Web-based question answering
system, named ARANEA, which is used approxi-
mate tfidf method for answer validation.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="method">
3 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.999913">
3.1 Kolmogorov complexity
</subsectionHeader>
<bodyText confidence="0.9999486875">
Kolmogorov complexity , or algorithm entropy ,
K(x) of a string x is the length of the shortest bi-
nary program to compute x. It defines randomness
of an individual string. Kolmogorov complexity
has been widely accepted as an information theory
for individual objects parallel to that of Shannon’s
information theory which is defined on an ensem-
ble of objects. It has also found many applications
in computer science such as average case analysis
of algorithms (Li and Vitanyi, 1997). For a uni-
versal Turing machine U, the Kolmogorov com-
plexity of a binary string x condition to another
binary string y, KU(x|y), is the length of the short-
est (prefix-free) program for U that outputs x with
input y. It has been proved that for different uni-
versal Turing machine U&apos;, for all x, y
</bodyText>
<equation confidence="0.995134">
KU(x|y) = KU&apos;(x|y) + C,
</equation>
<bodyText confidence="0.9999434">
where the constant C depends only on U&apos;. Thus we
simply write KU(x|y) as K(x|y). Define K(x) =
K(x|E), where E is the empty string. For for-
mal definitions and a comprehensive study of Kol-
mogorov complexity, see (Li and Vitanyi, 1997).
</bodyText>
<page confidence="0.998645">
43
</page>
<subsectionHeader confidence="0.983863">
3.2 Information Distance
</subsectionHeader>
<bodyText confidence="0.999617545454545">
Based on the Kolmogovov complexity theory, in-
formation distance (Bennett et al., 1998) is a uni-
versal distance metric, which has been success-
fully applied to many applications. The informa-
tion distance D(x, y) is defined as the length of
a shortest binary program which can compute x
given y as well as compute y from x. It has been
proved that , up to an additive logarithmic term,
D(x, y) = max{K(x|y), K(y|x)}. The normal-
ized version of D(x, y), called the normalized in-
formation distance(NID), is defined as
</bodyText>
<equation confidence="0.939666">
max{K(x|y), K(y|x)}
dmax(x, y) = (1)
max{K(x), K(y)}
</equation>
<bodyText confidence="0.88153675">
Parallel to this, the min distance is proposed in
(Zhang et al. , 2007), defined as
Dmin(x, y) = min{K(x|y), K(y|x)}. (2)
And the normalized version is
</bodyText>
<equation confidence="0.9969675">
min{K(x|y), K(y|x)} (3)
dmin(x,y) = min{K(x), K(y)}
</equation>
<subsectionHeader confidence="0.952031">
3.3 Conditional Information Distance
</subsectionHeader>
<bodyText confidence="0.969541">
Conditional information distance is defined as
</bodyText>
<equation confidence="0.983985">
max{K(x|y, c), K(y|x, c)}
dm��(x, y|c) = (4)
max{K(x |c), K(y |c)}
min{K(x|y, c), K(y|x, c)}
dmin(x, y|c) = (5)
min{K(x |c), K(y |c)}
</equation>
<bodyText confidence="0.999181533333333">
where c is given in both x to y and y to x compu-
tation.
The information distance is proved to be uni-
versal (Zhang et al. , 2007), that is, if x and y
are “close” under any distance measure, they are
“close” under the measure of information distance.
However, it is not clear yet how to find out such
“closeness” in traditional information distance the-
ory. Now the conditional information distance pro-
vides a possible solution.Figure 1 gives a more in-
terpretable explanation: the condition c could map
the original concepts x and y into different x, and
y,, thus the variant “closeness” could be reflected
by the distance between x, and y,, as shown in
Figure1.
</bodyText>
<figureCaption confidence="0.99395">
Figure 1: Conditional information distances under different
conditions c’s
</figureCaption>
<bodyText confidence="0.99995475">
The Kolmogorov complexity is non-
computable, that is, to use the information
distance measures, we must estimate the K(x)
first. There are traditionally two ways to do
this: (1) by compression (Li et al. , 2001),
and (2) by frequency counting based on coding
theorem (Cilibrasi and Vitanyi, 2007). The second
approach is implemented in this paper.
</bodyText>
<sectionHeader confidence="0.9947945" genericHeader="method">
4 Answer Validation with Information
Distance
</sectionHeader>
<bodyText confidence="0.999951">
Given a question q and a candidate answer c, the
answer validation task can be considered as deter-
mining the degree of relevance of c with respect
to q. The intuition of our approach is that the dis-
tance between question and the correct answer is
smaller than other candidates. Take the question
“What is the capital of the USA?” as an example,
among all candidates, the correct answer “Wash-
ington” is closest to the question under some dis-
tance measure. Thus the answer validation prob-
lem is to determine a proper distance measure.
Fortunately, it has been proved that the informa-
tion distance (Bennett et al., 1998) is universal so
that the similarity between the question and the an-
swer can surely be discovered using this measure.
Direct calculation of the unconditional distance
is difficult and non-flexible. We find it possible
and convenient to estimate the conditional infor-
mation distance between question focus and the
answers, under certain context as the condition. As
explained previously, different conditions lead to
different distance. With the most proper condition
and the nearest distance, the best answer can be
identified out of previously determined candidates.
The conditional normalized min distance is em-
ployed for distance calculation, which is defined
</bodyText>
<page confidence="0.998342">
44
</page>
<figureCaption confidence="0.999627">
Figure 2: Sample of conditional information distance calculation.
</figureCaption>
<bodyText confidence="0.551775">
as:
</bodyText>
<equation confidence="0.999131">
dmin(x, y|c)
= K 3c(x,y)´
−max{K 3c(x,φ)´,K 3
min{K 3c(x,φ)´
3c(φ,y)´
,K }−K 3c(φ,φ)´
</equation>
<bodyText confidence="0.992214">
where x represents the answer candidates, y is
the question focus, and c is condition pattern. The
function c(x, y) will be described in the Distance
Calculation section.
Figure 2 shows the procedure of distance cal-
culation. Given a question and a set of candidates,
we calculate the min information distance between
question focus and candidates conditioned on sur-
face patterns. Obviously, in order to calculate in-
formation distance, there are three issues to be ad-
dressed:
</bodyText>
<listItem confidence="0.997486294117647">
1. Question Focus Extraction: since the question
answer distance is reformulated as the mea-
sure between question focus and answer con-
ditioned on the surface pattern, it is important
to extract some words or phrases as question
focus.
2. Condition Pattern Generation: Obviously, the
generation of the condition is the key part.
We have built a well revised algorithm, in
which proper conditions can be generated
from question sentence according to some
heuristic rules.
3. Distance Calculation: after question focus
and condition patterns are obtained, the last
step is calculating the conditional distance to
estimate the relevance between question and
answer candidates.
</listItem>
<subsectionHeader confidence="0.994177">
4.1 Question Focus Extraction
</subsectionHeader>
<bodyText confidence="0.9999716">
Most factoid questions refer to specific objects. A
question is asked to learn some knowledge for this
object from certain perspective. In our approach,
we take the key named entity or noun phrase, usu-
ally as the subject or the main object of the ques-
tion sentence as the reference object. Take the
question “What city is Lake Washington by” as ex-
ample, the specific object is “Lake Washington”.
The question focus is identified using some heuris-
tic rules as follows:
</bodyText>
<listItem confidence="0.995270461538462">
1. The question is processed by shallow parsing.
All the noun phrases(NP) are extracted as NP set.
2. All the named entities(NE) in the question are
extracted as NE set.
3. If only one same element is identified in both
NE and NP set, this element is considered as ques-
tion focus.
4. If step 3 fails, but two elements from NE and
NP set have overlap words, then choose the ele-
ment with more words as question focus.
5. If step 3 and 4 fail, choose the candidate,
which is nearest with verb phrase in dependency
tree, as question focus.
</listItem>
<subsectionHeader confidence="0.996951">
4.2 Condition Pattern Generation
</subsectionHeader>
<bodyText confidence="0.9998765">
A set of hierarchical patterns is automatically con-
structed for conditional min distance calculation.
</bodyText>
<subsubsectionHeader confidence="0.808244">
4.2.1 Condition Pattern Construction
</subsubsectionHeader>
<bodyText confidence="0.969829857142857">
Several operations are defined for patterns con-
struction from the original question sentence. We
describe pattern set construction with a sam-
ple question “What year was President Kennedy
killed?”:
1. With linguistic analysis, the question is
split into pieces of tokens. These tokens in-
</bodyText>
<page confidence="0.996804">
45
</page>
<bodyText confidence="0.9998236">
clude wh-word phrases, preposition phrases, noun
phrases, verb phrases, key verb, etc. The exam-
ple question is split into “What year”(wh-word
phrase), “was”(key verb) “President Kennedy”
(noun phrases), “killed”(verb phrase).
</bodyText>
<listItem confidence="0.8807147">
2. Replace the wh-word phrases with the candi-
date placeholder hci. Then the words “What year”
is replaced with placeholder hci.
3. Replace the question focus with the focus
placeholder hfi, and add this pattern to the pat-
tern set. The example question focus is identified
as “President Kennedy”. It is replaced with place-
holder hfi. The first pattern “hci was hfi killed?”
is generated.
4. Voice Transformation: with morphology
techniques, verbs are expanded with all their tense
forms ( i.e. present, past tense and past participle).
The tokens’ order is adjusted to transform between
active voice and passive voice. Both patterns are
added to the patterns set. For sample question,
the passive pattern is translated into active pattern,
“hci kill hfi”.
5. Preposition addition: for time and location
questions, the preposition (i.e. in, on and at) is
added before the candidate hci; Then the pattern
“hci was hfi killed” is reformulated as “(in |on)
hci was hfi killed”.
6. Tokens shift: preposition phrase token could
be shifted to the begin or the end of pattern, and
“key verb” must be shift before the “verb phrase”.
Then the pattern “(in |on) hci was hfi killed” can
be reformulated as “hfi was killed (in |on) hci”.
7. Definitional patterns: several heuristic pat-
terns, as introduced at (Hildebrandt et al. , 2004),
are added into our final pattern sets, such as “hci,
</listItem>
<bodyText confidence="0.97659875">
hfi
By such heuristic rules, the original pattern set is
obtained from question sentence. The patterns are
initially enclosed in quotation marks, which means
exact matching. However, by eliminating these
quotations, or reducing the scope that they cover,
the matching is relaxed as words co-occurrence.
The patterns are expanded into different strict-level
patterns by adding or removing quotation marks
for each tokens or adjacent tokens combination.
Several condition pattern samples are shown in Ta-
ble 1
</bodyText>
<tableCaption confidence="0.9882375">
Table 1: Sample condition patterns, ‘ “” ’ denotes exact
match in web query.
</tableCaption>
<equation confidence="0.9845176">
D “ &lt;f&gt;(was  |were) killed (in  |on) &lt;c&gt;”
© “ (in  |on) &lt;c&gt;, &lt;f&gt;(was  |were) killed”
OO “ (in  |on) &lt;c&gt;” &amp; “&lt;f&gt;(was  |were) killed”
® “ (in  |on) &lt;c&gt;” &amp; “&lt;f&gt;” &amp; “(was  |were) killed”
OO in  |on &lt;c&gt;&lt;f&gt;(was  |were) killed
</equation>
<bodyText confidence="0.9995172">
Each operation introduced above is given a pre-
defined confidence coefficient(cc). Then the con-
fidence coefficient of a pattern is defined as the
multiplication of cc for all performed operations
to generate this pattern.
</bodyText>
<subsubsectionHeader confidence="0.806805">
4.2.2 Condition Pattern Ranking
</subsubsectionHeader>
<bodyText confidence="0.999221875">
From the previous step, a set of condition pat-
terns and corresponding confidence coefficient are
obtained. Let pi denotes the ith pattern in the pat-
tern set, and cci is the confidence coefficient for the
ith pattern. The confidence coefficient estimation
in previous section contains much noise. And the
patterns with similar confidence coefficient make
little difference. Therefore, the exact confidence
coefficient value is not directly used. We cluster
the patterns into different priority groups. Cj de-
notes the pattern cluster with jth priority. Here,
the smaller j means higher priority. The condi-
tion patterns are ranked mainly based on confi-
dence coefficient and the number of double quo-
tation marks. The following algorithm shows each
step in detail:
</bodyText>
<tableCaption confidence="0.982397">
Table 2: patterns ranking algorithm
</tableCaption>
<bodyText confidence="0.427325">
Input patterns set C = {(pi, cci)}
Algorithm
</bodyText>
<listItem confidence="0.990369571428571">
(1) Initialize Cj = 0, j = 0
(2) if C is empty, end this algorithm
(3) Select (pmax, ccmax), where ccmax ≥
cci, (pi, cci) ∈ C
(4) if Cj is empty, add ccmax into Cj, jump to
(2)
(5) select the minimum confidence coefficient
</listItem>
<bodyText confidence="0.7207382">
(pmin, ccmin) from Cj, compare it with
(pmax, ccmax). if the number of double
quotes(“”) in pmin is equal to the number in
pmax, add pmax into Cj. otherwise, j =
j + 1, Cj = {pmax}.
</bodyText>
<listItem confidence="0.947721">
(6) jump to (2) and repeat
</listItem>
<subsectionHeader confidence="0.996158">
4.3 Distance Calculation
</subsectionHeader>
<bodyText confidence="0.951726666666667">
Conditional min distance dmin is used to mea-
sure the relevance between question and candidate.
From section 3, dmin is not computable, but ap-
proximated by frequency counts based on the cod-
ing theory:
”.
</bodyText>
<page confidence="0.90872">
46
</page>
<equation confidence="0.937726272727273">
dmin(x, y|c)
= K 3c(x,y)´
−max{K 3c(x,φ)´,K 3
3 ´ 3 ´ 3 ´
min{K c(x,φ) ,K c(φ,y) }−K c(φ,φ)
log f 3c(x,y)´
−min{log f3c(x,φ)´ 3c(φ,y)´
,log f }
max{log f 3c(x,φ)´
,log f 3c(φ,y)´
}−log f3c(φ,φ)´
</equation>
<bodyText confidence="0.99990325">
The function c(x, 0) means substituting (c) in c
by answer candidate x and removing placeholder
(f) if any. Similar definition applies to c(y, 0),
c(x, y). For example, given pattern “(f) was in-
vented in (c)”, question focus “the telegraph” and
a candidate “1867”. c(x, 0) is “was invented in
1867”. c(y, 0) is “the telegraph was invented”, and
c(x, y) is “the telegraph was invented in 1867”.
The frequency counts f(x) are estimated as the
number of returned pages by certain search en-
gine with respect to x . f(c(o, 0)) denote the to-
tal pages indexed in search engine. Two types of
search engines “Google” and “Altavista” are em-
ployed.
The patterns are selected in priority order to cal-
culate the information distance for each candidate.
</bodyText>
<sectionHeader confidence="0.995555" genericHeader="evaluation">
5 Experiment and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999583">
5.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.99571128125">
Data set: The standard QA test collection (Lin
and Katz, 2006) is employed in our experiments. It
consists of 109 factoid questions, covering several
domains including history, geography, physics, bi-
ology, economics, fashion knowledge, and etc.. 20
candidates are prepared for each questions. All an-
swer candidates are first extracted by the imple-
mented question answering system. Then we re-
view the candidate set for each question. If the cor-
rect answer is not in this set, it is manually added
into the set.
Performance Metric: The top 1 answer precision
and mean reciprocal rank (MRR) are used for per-
formance evaluation.The top 1 answer means the
correct answer ranks first with our distance calcu-
lation method, and MRR = n1 * Pi(1
ranki ), in
which the 1 is 1 if the correct answer occurs in
ranki
the first position; 0.5 if it firstly occurs in the sec-
ond position; 0.33 for the third, 0.25 for the fourth,
0.2 for the fifth and 0 if none of the first five an-
swers is correct.
The open source factoid QA system ARANEA
(downloaded from Jimmy Lin’s website in 2005)
is used for comparison, which implements an ap-
proximate tfidf algorithm for candidate scoring.
Both ARANEA and our proposed approaches use
the internet directly. Google is used as the search
engine for ARENEA, and our conditional normal-
ized min distance is calculated with Google and
Altavista respectively.
</bodyText>
<subsectionHeader confidence="0.999825">
5.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.9989077">
The performances of our proposed approach and
ARANEA are shown in Table 3. For top 1 an-
swer precision, our conditional min distance cal-
culation method through Google achieves 69.7%,
and Altavista is 66.1%, which make 56.6%
(69.7% v.s.42.2% ) and 50.0% (66.1% v.s 42.2%)
improvement compared with ARENEA’s tfidf
method. Our proposed methods achieve 0.756 and
0.772 compared with ARENEA’s 0.581 for MRR
measure.
</bodyText>
<tableCaption confidence="0.998875">
Table 3: Performance comparison, where dmin(G) denotes
</tableCaption>
<table confidence="0.881895166666667">
the distance calculation through “Google”, dmin(A) through
“Altavista”
tfidf dmin(G) dmin(A)
# of Top 1 46 72 69
% of Top 1 42.2 69.7 66.1
MRR 0.581 0.772 0.756
</table>
<tableCaption confidence="0.57264575">
Table 4 shows some correct answer validation
examples. the Google Condition(GC) and the Al-
tavista Condition(AC) columns are the employed
condition patterns for distance calculation. For
</tableCaption>
<bodyText confidence="0.996721666666667">
question 1400, the conditional normalized google
min distance calculates the distance between ques-
tion focus “the telegragh” and all 20 answer can-
didates. The minimum distance score is achieved
between “the telegraph” and “1837” with the con-
dition pattern “(f) was invented in (c)”. There-
fore, the candidate “1837” is validated as the cor-
rect answer. Meanwhile, the minimum value for
conditional normalized altavista min distance is
achieved on the same condition.
These results demonstrate that the distance cal-
culation method provides a feasible solution for
answer validation.
In discussion section, we will study three ques-
tions:
</bodyText>
<listItem confidence="0.944598">
1. What is the role of search engine?
2. What is the role of condition pattern?
3. What is the role of question focus?
</listItem>
<page confidence="0.999546">
47
</page>
<tableCaption confidence="0.990261">
Table 4: Question Examples in conditional information calculation through Google and Altavista. GC:Google Condition;
AC:Altavista Condition
</tableCaption>
<table confidence="0.9983685">
ID Question GC AC Answer Question focus
1400 When was the telegraph “?y was in- “?y was 1837 the telegraph
invented? vented in ?s” invented in
?x”
1401 What is the democratic “?y is ?x” “?y is ?x” the don- the democratic
party symbol? key party symbol
1411 What Spanish explorer “?x discov- “?x” “dis- Hernando the Mississippi
discovered the Missis- ered ?y” covered” de Soto River
sippi River?
1412 Who is the governor of “?y is ?x” “?y, ?x” Gov. Bill the governor of
Colorado? Ritter Colorado
1484 What college did Allen “?y attended “?x” “did Georgetown Allen Iverson at-
Iverson attend? ?x” ?y” Univer- tend
sity
</table>
<subsectionHeader confidence="0.9089175">
5.3 Discussions
5.3.1 Role of Search Engine
</subsectionHeader>
<bodyText confidence="0.999925789473684">
The rise of world-wide-web has enticed millions
of users to create billions of web pages. The re-
dundancy of web information is an important re-
source for question answering. Our Kolmogorov
Complexity based information distance is approx-
imated with query frequency obtained by search
engine. Two types of search engines “Google” and
“Altavista” are employed in this paper. The num-
ber of top 1 correct answer is 72 through “Google”
and 69 through “Altavista”. There is little differ-
ence between two numbers, which shows that the
information distance based on Kolmogorov Com-
plexity is independent of special search engine.
The performance didn’t vary much with the change
of search engine. Actually, if the local data is ac-
cumulated large enough, the information distance
can be approximated without the internet. The
quality and size of data set affect the experiment
performance.
</bodyText>
<subsectionHeader confidence="0.963619">
5.3.2 Role of Condition Pattern
</subsectionHeader>
<bodyText confidence="0.999924">
Pattern set offers convenient and flexible condi-
tion for information distance calculation. In the
experiment, there are 61 questions correctly an-
swered by both Google and Altavista. 46 ques-
tions of them employ different patterns. Consider-
ing Question 1412, the condition pattern in Google
is “(c) is (f)”, while in Altavista, it is “(f), (c)”.
However, the correct answer “Gov. Bill Ritter” is
identified by both methods. The information dis-
tance is stable over specific condition patterns.
</bodyText>
<subsectionHeader confidence="0.842566">
5.3.3 Role of Question Focus
</subsectionHeader>
<bodyText confidence="0.999972">
Question focus is considered as the discrimina-
tor for the question. The distance between a ques-
tion and a candidate is reformulated as the distance
between question focus and candidate conditioned
on a set of surface patterns. The proposed ap-
proach may not properly extract the question fo-
cus, but the answers can be correctly identified
when the condition pattern becomes loose enough.
Take the question 1484 “What college did Allen
Iverson attend?” as example, the verb “attend” is
tagged as “noun”, then question focus is mistak-
enly extracted as “Allen Iverson attend”, instead of
the correct “Allen Iverson”. The two conditional
information distance method still identify the cor-
rect answer “Georgetown University”. Because
they both employed the looser condition patterns
’“(c)” “(f)”’ and ’“(c)” did “(f)”’.Therefore, our
proposed distance answer validation methods are
robust to the question focus selection component.
From the discussion above, it can be seen that
our algorithm is stable and robust, not depending
on the specific search engine, condition pattern,
and question focus.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999993857142857">
We have presented a novel approach for answer
validation based on information distance. The an-
swer validation task is reformulated as distance
calculation between question focus and candidate
conditioned on a set of surface patterns. The ex-
periments show that our proposed answer valida-
tion method makes a great improvement compared
</bodyText>
<page confidence="0.996794">
48
</page>
<bodyText confidence="0.999968">
with ARANEA’s tfidf method. Furthermore, The
experiments show that our approach is stable and
robust, not depending on the specific search en-
gine, condition pattern, and question focus. In fu-
ture work, we will try to calculate information dis-
tance in the local constructed data set, and expand
this distance measure into other application fields.
</bodyText>
<sectionHeader confidence="0.993454" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<reference confidence="0.758321">
This work is supported by National Natural Sci-
ence Foundation of China (60572084, 60621062),
Hi-tech Research and Development Program of
China (2006AA02Z321), National Basic Research
Program of China (2007CB311003).
</reference>
<sectionHeader confidence="0.968437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999508616666666">
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, Ad-
wait Ratnaparkhi, and Richard J.Mammone. 2001.
Question answering using maximum entropy conm-
ponents. In Proceedings of the Second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language tecnologies.
Anselmo Penas, A. Rodrigo, F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2007.
Working Notes for the CLEF 2007 Workshop.
C.H. Bennett, P. Gacs, M. Li, P. Vit´anyi, W. Zurek..
1998. Information Distance. IEEE Trans. Inform.
Theory, 44:4, 1407–1423.
Eric Brill and Susan Dumais and Michele Banko. 2002.
An analysis of the AskMSR question-answering sys-
tem. EMNLP ’02: the ACL-02 conference on Em-
pirical methods in natural language processing.
Hildebrandt W., Katz B., and Lin J. 2004. Answer-
ing Definition Questions Using Multiple Knowledge
Sources. Proceedings of Human Language Technol-
ogy Conference. Boston, USA.
Jeongwoo Ko, Luo Si, Eric Nyberg. 2007. A Proba-
bilistic Graphical Model for Joint Answer Ranking
in Question Answering. In Proceedings of the 30th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
Jimmy Lin and Boris Katz. 2006. Building a reusable
test collection for question answering. J. Am. Soc.
Inf. Sci. Technol..
Jimmy Lin. 2007. An Exploration of the Principles Un-
derlying Redundancy-Based Factoid Question An-
swering. ACM Transactions on Information Sys-
tems, 27(2):1-55.
Jinxi Xu, Ana Licuanan and Ralph Weischedel. 2003.
Trec 2003 qa at bbn: Answering definitional ques-
tions. In Proceedings of the 12th Text REtrieval
Conference, Gaithersburgh, MD, USA.
Ming Li and Paul MB Vitanyi. 1997. An Introduc-
tion to Kolmogorov Complexity and Its Applications.
Working Notes for the CLEF 2007 Workshop.
M. Li, J. Badger, X. Chen, S. Kwong, P. Kearney,
H. Zhang.. 2001. An information-based sequence
distance and its application to whole mitochondrial
genome phylogeny. Bioinformatics, 17:2.
M. Subbotin and S. Subbotin. 2001. Patterns of Po-
tential Answer Expressions as Clues to the RightAn-
swers. In TREC-10 Notebook papers. Gaithesburg,
MD.
R. Cilibrasi, P.M.B. Vit´anyi. 2007. An Exploration of
the Principles Underlying Redundancy-Based Fac-
toid Question Answering. EEE Trans. Knowledge
and Data Engineering, 19:3, 370–383.
Shen, Dan and Dietrich Klakow . 2006. Exploring cor-
relation of dependency relation paths for answer ex-
traction. In Proceedings of COLING-ACL, Sydney,
Australia.
Xian Zhang, Yu Hao, Xiaoyan Zhu, and Ming Li. 2007.
Information Distance from a Question to an Answer.
In Proceedings of the 13th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
</reference>
<page confidence="0.99954">
49
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526033">
<title confidence="0.999312">Answer Validation by Information Distance Calculation</title>
<author confidence="0.988945">Fangtao Li</author>
<author confidence="0.988945">Xian Zhang</author>
<author confidence="0.988945">Xiaoyan</author>
<affiliation confidence="0.851356">State Key Laboratory on Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Science and Technology, Tsinghua University, Beijing 100084,</affiliation>
<email confidence="0.89155">zxy-dcs@mail.tsinghua.edu.cn</email>
<abstract confidence="0.9927265">In this paper,an information distance based approach is proposed to perform answer validation for question answering system. To validate an answer candidate, the approach calculates the conditional information distance between the question focus and the candidate under certain condition pattern set. Heuristic methods are designed to extract question focus and generate proper condition patterns from question. General search engines are employed to estimate the Kolmogorov complexity, hence the information distance. Experimental results show that our approach is stable and flexible, and outperforms tradi-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>This work is supported by National Natural Science Foundation of China (60572084,</title>
<date>2007</date>
<booktitle>60621062), Hi-tech Research and Development Program of China (2006AA02Z321), National Basic Research Program of</booktitle>
<contexts>
<context position="5602" citStr="(2007)" startWordPosition="843" endWordPosition="843">s exploited machine learning techniques with rich syntactic or semantic features to measure the similarity between question and answer. Ittycheriah et al. (2001) used Maximum Entropy model to combine rich features and automatically learn feature weights. These features included query expansion features, focus features, named entity features, dependency relation features, pattern features et al. Shen and Klakow (2006) presented three methods, including feature vector, string kernel and tree kernel, to represent surface text features and parse tree features in Support Vector Machines. Ko et al. (2007) proposed a probabilistic graphical model to estimate the probability of correctness for all candidate answers. Four types of features were employed, including knowledge-based features, data-driven features, string distance feature and synonym features. Started in 2006, the annual Answer Validation Exercise (Penas et al. , 2007) aims to develop systems to decide if the answer to a question is correct or not. The English answer validation task is reformulated as a Text Entailment problem. The triplet, including question, answer and supporting text, is given. The system determines if the support</context>
<context position="6854" citStr="(2007)" startWordPosition="1035" endWordPosition="1035"> reformulation from the question and answer. All participants used lexical processing, including lemmatization and part-of speech tagging. Some systems used first order logic representations, performed semantic analysis and took the validation decision with a theorem proof. The above approaches should process deep syntactic and semantic analysis for either questions or candidate answers. The annotated linguistic resource is hard to acquire for the supervised classification problem. Another alternative solution for answer validation is to exploit the redundancy of large scale data. Eric et al. (2007) developed AskMSR question answering system. They focus on the Web as a gigantic data repository with tremendous redundancy that can be exploited to extract the correct answer. Lin (2007) implemented another Web-based question answering system, named ARANEA, which is used approximate tfidf method for answer validation. 3 Preliminaries 3.1 Kolmogorov complexity Kolmogorov complexity , or algorithm entropy , K(x) of a string x is the length of the shortest binary program to compute x. It defines randomness of an individual string. Kolmogorov complexity has been widely accepted as an information </context>
</contexts>
<marker>2007</marker>
<rawString>This work is supported by National Natural Science Foundation of China (60572084, 60621062), Hi-tech Research and Development Program of China (2006AA02Z321), National Basic Research Program of China (2007CB311003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Martin Franz</author>
<author>Wei-Jing Zhu</author>
<author>Adwait Ratnaparkhi</author>
<author>Richard J Mammone</author>
</authors>
<title>Question answering using maximum entropy conmponents.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second meeting of the North American Chapter of the Association for Computational Linguistics on Language tecnologies.</booktitle>
<contexts>
<context position="2510" citStr="Ittycheriah et al., 2001" startWordPosition="356" endWordPosition="359">swer validation, the system will carry out different refinements of its searching criteria to check the relevance of new candidate answers. In addition, since most of QA systems rely on complex architectures and the evaluation of their performances requires a huge amount of work, the automatic assessment of candidates with respect to a given question will speed up both algorithm refinement and testing. Currently, answer validation is mainly viewed as a classification problem or ranking problem. Different models, such as Support Vector Machine (Shen and Klakow, 2006) and Maximum Entropy Model (Ittycheriah et al., 2001), are used to integrate sophisticated linguistic features to determine the correctness of candidates. The answer validation exercise (Penas et al. , 2007) aims at developing systems able to decide whether the answer is correct or not. They formulate answer validation as a text entailment problem. These approaches are dependent on sophisticated linguistic analysis of syntactic and semantic relations between question and candidates. It is quite expensive to use deep analysis for automatic answer validation, especially in large scale data set. Thus it is appropriate to find an alternative solutio</context>
<context position="5157" citStr="Ittycheriah et al. (2001)" startWordPosition="772" endWordPosition="775">dation is an emerging topic in Question Answering, where open domain systems are often required to rank huge amounts of answer candidates. This task can be viewed as a classification problem or re-ranking problem. Early question answering systems focused on employing surface text patterns (Subbotin and Subbotin, 2001) for answer validation. Xu et al. (2003) identified that pattern-based approaches got bad performances due to poor system recall. Some researchers exploited machine learning techniques with rich syntactic or semantic features to measure the similarity between question and answer. Ittycheriah et al. (2001) used Maximum Entropy model to combine rich features and automatically learn feature weights. These features included query expansion features, focus features, named entity features, dependency relation features, pattern features et al. Shen and Klakow (2006) presented three methods, including feature vector, string kernel and tree kernel, to represent surface text features and parse tree features in Support Vector Machines. Ko et al. (2007) proposed a probabilistic graphical model to estimate the probability of correctness for all candidate answers. Four types of features were employed, inclu</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, Mammone, 2001</marker>
<rawString>Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, Adwait Ratnaparkhi, and Richard J.Mammone. 2001. Question answering using maximum entropy conmponents. In Proceedings of the Second meeting of the North American Chapter of the Association for Computational Linguistics on Language tecnologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Penas</author>
<author>A Rodrigo</author>
<author>F Verdejo</author>
</authors>
<title>Overview of the Answer Validation Exercise</title>
<date>2007</date>
<note>Workshop.</note>
<marker>Penas, Rodrigo, Verdejo, 2007</marker>
<rawString>Anselmo Penas, A. Rodrigo, F. Verdejo. 2007. Overview of the Answer Validation Exercise 2007. Working Notes for the CLEF 2007 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Bennett</author>
<author>P Gacs</author>
<author>M Li</author>
<author>P Vit´anyi</author>
<author>W Zurek</author>
</authors>
<title>Information Distance.</title>
<date>1998</date>
<journal>IEEE Trans. Inform. Theory,</journal>
<volume>44</volume>
<pages>1407--1423</pages>
<marker>Bennett, Gacs, Li, Vit´anyi, Zurek, 1998</marker>
<rawString>C.H. Bennett, P. Gacs, M. Li, P. Vit´anyi, W. Zurek.. 1998. Information Distance. IEEE Trans. Inform. Theory, 44:4, 1407–1423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Susan Dumais</author>
<author>Michele Banko</author>
</authors>
<title>An analysis of the AskMSR question-answering system.</title>
<date>2002</date>
<booktitle>EMNLP ’02: the ACL-02 conference on Empirical methods in natural language processing.</booktitle>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>Eric Brill and Susan Dumais and Michele Banko. 2002. An analysis of the AskMSR question-answering system. EMNLP ’02: the ACL-02 conference on Empirical methods in natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hildebrandt</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>Answering Definition Questions Using Multiple Knowledge Sources.</title>
<date>2004</date>
<booktitle>Proceedings of Human Language Technology Conference.</booktitle>
<location>Boston, USA.</location>
<marker>Hildebrandt, Katz, Lin, 2004</marker>
<rawString>Hildebrandt W., Katz B., and Lin J. 2004. Answering Definition Questions Using Multiple Knowledge Sources. Proceedings of Human Language Technology Conference. Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeongwoo Ko</author>
<author>Luo Si</author>
<author>Eric Nyberg</author>
</authors>
<title>A Probabilistic Graphical Model for Joint Answer Ranking in Question Answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="5602" citStr="Ko et al. (2007)" startWordPosition="840" endWordPosition="843">researchers exploited machine learning techniques with rich syntactic or semantic features to measure the similarity between question and answer. Ittycheriah et al. (2001) used Maximum Entropy model to combine rich features and automatically learn feature weights. These features included query expansion features, focus features, named entity features, dependency relation features, pattern features et al. Shen and Klakow (2006) presented three methods, including feature vector, string kernel and tree kernel, to represent surface text features and parse tree features in Support Vector Machines. Ko et al. (2007) proposed a probabilistic graphical model to estimate the probability of correctness for all candidate answers. Four types of features were employed, including knowledge-based features, data-driven features, string distance feature and synonym features. Started in 2006, the annual Answer Validation Exercise (Penas et al. , 2007) aims to develop systems to decide if the answer to a question is correct or not. The English answer validation task is reformulated as a Text Entailment problem. The triplet, including question, answer and supporting text, is given. The system determines if the support</context>
</contexts>
<marker>Ko, Si, Nyberg, 2007</marker>
<rawString>Jeongwoo Ko, Luo Si, Eric Nyberg. 2007. A Probabilistic Graphical Model for Joint Answer Ranking in Question Answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Boris Katz</author>
</authors>
<title>Building a reusable test collection for question answering.</title>
<date>2006</date>
<journal>J. Am. Soc. Inf. Sci. Technol..</journal>
<contexts>
<context position="19680" citStr="Lin and Katz, 2006" startWordPosition="3166" endWordPosition="3169">legraph” and a candidate “1867”. c(x, 0) is “was invented in 1867”. c(y, 0) is “the telegraph was invented”, and c(x, y) is “the telegraph was invented in 1867”. The frequency counts f(x) are estimated as the number of returned pages by certain search engine with respect to x . f(c(o, 0)) denote the total pages indexed in search engine. Two types of search engines “Google” and “Altavista” are employed. The patterns are selected in priority order to calculate the information distance for each candidate. 5 Experiment and Discussion 5.1 Experiment Setup Data set: The standard QA test collection (Lin and Katz, 2006) is employed in our experiments. It consists of 109 factoid questions, covering several domains including history, geography, physics, biology, economics, fashion knowledge, and etc.. 20 candidates are prepared for each questions. All answer candidates are first extracted by the implemented question answering system. Then we review the candidate set for each question. If the correct answer is not in this set, it is manually added into the set. Performance Metric: The top 1 answer precision and mean reciprocal rank (MRR) are used for performance evaluation.The top 1 answer means the correct ans</context>
</contexts>
<marker>Lin, Katz, 2006</marker>
<rawString>Jimmy Lin and Boris Katz. 2006. Building a reusable test collection for question answering. J. Am. Soc. Inf. Sci. Technol..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
</authors>
<title>An Exploration of the Principles Underlying Redundancy-Based Factoid Question Answering.</title>
<date>2007</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>27--2</pages>
<contexts>
<context position="7041" citStr="Lin (2007)" startWordPosition="1064" endWordPosition="1065">tations, performed semantic analysis and took the validation decision with a theorem proof. The above approaches should process deep syntactic and semantic analysis for either questions or candidate answers. The annotated linguistic resource is hard to acquire for the supervised classification problem. Another alternative solution for answer validation is to exploit the redundancy of large scale data. Eric et al. (2007) developed AskMSR question answering system. They focus on the Web as a gigantic data repository with tremendous redundancy that can be exploited to extract the correct answer. Lin (2007) implemented another Web-based question answering system, named ARANEA, which is used approximate tfidf method for answer validation. 3 Preliminaries 3.1 Kolmogorov complexity Kolmogorov complexity , or algorithm entropy , K(x) of a string x is the length of the shortest binary program to compute x. It defines randomness of an individual string. Kolmogorov complexity has been widely accepted as an information theory for individual objects parallel to that of Shannon’s information theory which is defined on an ensemble of objects. It has also found many applications in computer science such as </context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>Jimmy Lin. 2007. An Exploration of the Principles Underlying Redundancy-Based Factoid Question Answering. ACM Transactions on Information Systems, 27(2):1-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Ana Licuanan</author>
<author>Ralph Weischedel</author>
</authors>
<title>Trec</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th Text REtrieval Conference,</booktitle>
<location>Gaithersburgh, MD, USA.</location>
<contexts>
<context position="4891" citStr="Xu et al. (2003)" startWordPosition="734" endWordPosition="737">Complexity theory is introduced in Section 3. Section 4 presents our proposed answer validation method based on information retrieval. In Section 5, we describe the experiments and discussions. The paper is concluded in Section 6. 2 Related Work Answer Validation is an emerging topic in Question Answering, where open domain systems are often required to rank huge amounts of answer candidates. This task can be viewed as a classification problem or re-ranking problem. Early question answering systems focused on employing surface text patterns (Subbotin and Subbotin, 2001) for answer validation. Xu et al. (2003) identified that pattern-based approaches got bad performances due to poor system recall. Some researchers exploited machine learning techniques with rich syntactic or semantic features to measure the similarity between question and answer. Ittycheriah et al. (2001) used Maximum Entropy model to combine rich features and automatically learn feature weights. These features included query expansion features, focus features, named entity features, dependency relation features, pattern features et al. Shen and Klakow (2006) presented three methods, including feature vector, string kernel and tree </context>
</contexts>
<marker>Xu, Licuanan, Weischedel, 2003</marker>
<rawString>Jinxi Xu, Ana Licuanan and Ralph Weischedel. 2003. Trec 2003 qa at bbn: Answering definitional questions. In Proceedings of the 12th Text REtrieval Conference, Gaithersburgh, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Li</author>
<author>Paul MB Vitanyi</author>
</authors>
<title>An Introduction to Kolmogorov Complexity and Its Applications. Working Notes for the CLEF</title>
<date>1997</date>
<note>Workshop.</note>
<contexts>
<context position="3600" citStr="Li and Vitanyi, 1997" startWordPosition="533" endWordPosition="536">analysis for automatic answer validation, especially in large scale data set. Thus it is appropriate to find an alternative solution to this problem. Here, we just consider the English answer validation task. This paper proposes a novel approach based on information retrieval on the Web. The answer validation problem is reformulated as distance calculation from an answer candidate to a question. The hypothesis is that, among all candidates, the correct answer has the smallest distance from question. We employ conditional normalized min distance, which is based on Kolmogorov Complexity theory (Li and Vitanyi, 1997), for this task. The distance measures the relevance between question 42 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 42–49 Manchester, UK. August 2008 focus and candidates conditioned on a surface pattern set. For distance calculation, we first extract the question focus, and then a hierarchical pattern set is automatically constructed as condition. Since Kolmogrov Complexity can be approximated through frequency counts. Two types of search engine “Google” and “Altavista” are used to approximate the distance. The paper is organize</context>
<context position="7699" citStr="Li and Vitanyi, 1997" startWordPosition="1166" endWordPosition="1169">on answering system, named ARANEA, which is used approximate tfidf method for answer validation. 3 Preliminaries 3.1 Kolmogorov complexity Kolmogorov complexity , or algorithm entropy , K(x) of a string x is the length of the shortest binary program to compute x. It defines randomness of an individual string. Kolmogorov complexity has been widely accepted as an information theory for individual objects parallel to that of Shannon’s information theory which is defined on an ensemble of objects. It has also found many applications in computer science such as average case analysis of algorithms (Li and Vitanyi, 1997). For a universal Turing machine U, the Kolmogorov complexity of a binary string x condition to another binary string y, KU(x|y), is the length of the shortest (prefix-free) program for U that outputs x with input y. It has been proved that for different universal Turing machine U&apos;, for all x, y KU(x|y) = KU&apos;(x|y) + C, where the constant C depends only on U&apos;. Thus we simply write KU(x|y) as K(x|y). Define K(x) = K(x|E), where E is the empty string. For formal definitions and a comprehensive study of Kolmogorov complexity, see (Li and Vitanyi, 1997). 43 3.2 Information Distance Based on the Kol</context>
</contexts>
<marker>Li, Vitanyi, 1997</marker>
<rawString>Ming Li and Paul MB Vitanyi. 1997. An Introduction to Kolmogorov Complexity and Its Applications. Working Notes for the CLEF 2007 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>J Badger</author>
<author>X Chen</author>
<author>S Kwong</author>
<author>P Kearney</author>
<author>H Zhang</author>
</authors>
<title>An information-based sequence distance and its application to whole mitochondrial genome phylogeny.</title>
<date>2001</date>
<journal>Bioinformatics,</journal>
<volume>17</volume>
<marker>Li, Badger, Chen, Kwong, Kearney, Zhang, 2001</marker>
<rawString>M. Li, J. Badger, X. Chen, S. Kwong, P. Kearney, H. Zhang.. 2001. An information-based sequence distance and its application to whole mitochondrial genome phylogeny. Bioinformatics, 17:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Subbotin</author>
<author>S Subbotin</author>
</authors>
<title>Patterns of Potential Answer Expressions as Clues to the RightAnswers.</title>
<date>2001</date>
<booktitle>In TREC-10 Notebook papers.</booktitle>
<location>Gaithesburg, MD.</location>
<contexts>
<context position="4851" citStr="Subbotin and Subbotin, 2001" startWordPosition="727" endWordPosition="730"> describes related work. The fundamental Kolmogorov Complexity theory is introduced in Section 3. Section 4 presents our proposed answer validation method based on information retrieval. In Section 5, we describe the experiments and discussions. The paper is concluded in Section 6. 2 Related Work Answer Validation is an emerging topic in Question Answering, where open domain systems are often required to rank huge amounts of answer candidates. This task can be viewed as a classification problem or re-ranking problem. Early question answering systems focused on employing surface text patterns (Subbotin and Subbotin, 2001) for answer validation. Xu et al. (2003) identified that pattern-based approaches got bad performances due to poor system recall. Some researchers exploited machine learning techniques with rich syntactic or semantic features to measure the similarity between question and answer. Ittycheriah et al. (2001) used Maximum Entropy model to combine rich features and automatically learn feature weights. These features included query expansion features, focus features, named entity features, dependency relation features, pattern features et al. Shen and Klakow (2006) presented three methods, including</context>
</contexts>
<marker>Subbotin, Subbotin, 2001</marker>
<rawString>M. Subbotin and S. Subbotin. 2001. Patterns of Potential Answer Expressions as Clues to the RightAnswers. In TREC-10 Notebook papers. Gaithesburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cilibrasi</author>
<author>P M B Vit´anyi</author>
</authors>
<title>An Exploration of the Principles Underlying Redundancy-Based Factoid Question Answering.</title>
<date>2007</date>
<journal>EEE Trans. Knowledge and Data Engineering,</journal>
<volume>19</volume>
<pages>370--383</pages>
<marker>Cilibrasi, Vit´anyi, 2007</marker>
<rawString>R. Cilibrasi, P.M.B. Vit´anyi. 2007. An Exploration of the Principles Underlying Redundancy-Based Factoid Question Answering. EEE Trans. Knowledge and Data Engineering, 19:3, 370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Dietrich Klakow</author>
</authors>
<title>Exploring correlation of dependency relation paths for answer extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2457" citStr="Shen and Klakow, 2006" startWordPosition="347" endWordPosition="350">). Some rights reserved. search. With automatic answer validation, the system will carry out different refinements of its searching criteria to check the relevance of new candidate answers. In addition, since most of QA systems rely on complex architectures and the evaluation of their performances requires a huge amount of work, the automatic assessment of candidates with respect to a given question will speed up both algorithm refinement and testing. Currently, answer validation is mainly viewed as a classification problem or ranking problem. Different models, such as Support Vector Machine (Shen and Klakow, 2006) and Maximum Entropy Model (Ittycheriah et al., 2001), are used to integrate sophisticated linguistic features to determine the correctness of candidates. The answer validation exercise (Penas et al. , 2007) aims at developing systems able to decide whether the answer is correct or not. They formulate answer validation as a text entailment problem. These approaches are dependent on sophisticated linguistic analysis of syntactic and semantic relations between question and candidates. It is quite expensive to use deep analysis for automatic answer validation, especially in large scale data set. </context>
<context position="5416" citStr="Shen and Klakow (2006)" startWordPosition="811" endWordPosition="814">oying surface text patterns (Subbotin and Subbotin, 2001) for answer validation. Xu et al. (2003) identified that pattern-based approaches got bad performances due to poor system recall. Some researchers exploited machine learning techniques with rich syntactic or semantic features to measure the similarity between question and answer. Ittycheriah et al. (2001) used Maximum Entropy model to combine rich features and automatically learn feature weights. These features included query expansion features, focus features, named entity features, dependency relation features, pattern features et al. Shen and Klakow (2006) presented three methods, including feature vector, string kernel and tree kernel, to represent surface text features and parse tree features in Support Vector Machines. Ko et al. (2007) proposed a probabilistic graphical model to estimate the probability of correctness for all candidate answers. Four types of features were employed, including knowledge-based features, data-driven features, string distance feature and synonym features. Started in 2006, the annual Answer Validation Exercise (Penas et al. , 2007) aims to develop systems to decide if the answer to a question is correct or not. Th</context>
</contexts>
<marker>Shen, Klakow, 2006</marker>
<rawString>Shen, Dan and Dietrich Klakow . 2006. Exploring correlation of dependency relation paths for answer extraction. In Proceedings of COLING-ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Zhang</author>
<author>Yu Hao</author>
<author>Xiaoyan Zhu</author>
<author>Ming Li</author>
</authors>
<title>Information Distance from a Question to an Answer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<marker>Zhang, Hao, Zhu, Li, 2007</marker>
<rawString>Xian Zhang, Yu Hao, Xiaoyan Zhu, and Ming Li. 2007. Information Distance from a Question to an Answer. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>