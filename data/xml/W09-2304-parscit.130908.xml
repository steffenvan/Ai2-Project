<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004096">
<title confidence="0.9990305">
Improving Phrase-Based Translation via Word Alignments
from Stochastic Inversion Transduction Grammars
</title>
<author confidence="0.992695">
Markus SAERS
</author>
<affiliation confidence="0.871457333333333">
Dept. of Linguistics and Philology
Uppsala University
Sweden
</affiliation>
<email confidence="0.987427">
markus.saers@lingfil.uu.se
</email>
<sectionHeader confidence="0.993531" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955419354839">
We argue that learning word alignments
through a compositionally-structured, joint
process yields higher phrase-based transla-
tion accuracy than the conventional heuris-
tic of intersecting conditional models.
Flawed word alignments can lead to flawed
phrase translations that damage translation
accuracy. Yet the IBM word alignments
usually used today are known to be flawed,
in large part because IBM models (1)
model reordering by allowing unrestricted
movement of words, rather than con-
strained movement of compositional units,
and therefore must (2) attempt to compen-
sate via directed, asymmetric distortion and
fertility models. The conventional heuris-
tics for attempting to recover from the re-
sulting alignment errors involve estimating
two directed models in opposite directions
and then intersecting their alignments – to
make up for the fact that, in reality, word
alignment is an inherently joint relation. A
natural alternative is provided by Inversion
Transduction Grammars, which estimate
the joint word alignment relation directly,
eliminating the need for any of the conven-
tional heuristics. We show that this align-
ment ultimately produces superior
translation accuracy on BLEU, NIST, and
METEOR metrics over three distinct lan-
guage pairs.
</bodyText>
<sectionHeader confidence="0.987754" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.7477905">
In this paper we argue that word alignments
learned through a compositionally-structured, joint
</bodyText>
<page confidence="0.982849">
28
</page>
<author confidence="0.780312">
Dekai WU
</author>
<affiliation confidence="0.8870975">
Human Language Technology Center
Dept. of Computer Science &amp; Engineering
</affiliation>
<email confidence="0.353912">
HKUST
</email>
<author confidence="0.839094">
Hong Kong
</author>
<email confidence="0.859234">
dekai@cs.ust.hk
</email>
<bodyText confidence="0.999898384615385">
process are able to significantly improve the train-
ing of phrase-based translation systems, leading to
higher translation accuracy than the conventional
heuristic of intersecting conditional models. To-
day, statistical machine translation (SMT) systems
perform at state-of-the-art levels; their ability to
weigh different translation hypotheses against each
other to find an optimal solution has proven to be a
great asset. What sets various SMT systems apart
are the models employed to determine what to con-
sider optimal. The most common systems today
consist of phrase-based models, where chunks of
texts are substituted and rearranged to produce the
output sentence.
Our premise is that certain flawed word align-
ments can lead to flawed phrase translations that in
turn damage translation accuracy, since word
alignment is the basis for learning phrase transla-
tions in phrase-based SMT systems. A critical part
of such systems is the word-level translation
model, which is estimated from aligned data. Cur-
rently, the standard way of computing a word
alignment is to estimate a function linking words in
one of the languages to words in the other. Func-
tions can only define many-to-one relations, but
word alignment is a many-to-many relation. The
solution is to combine two functions, one in each
direction, and harmonize them by means of some
heuristic. After that, phrases can be extracted from
the word alignments.
The problem is that the starting point for word
alignments is usually the IBM models (Brown et
al., 1993), which are known to produce flawed
alignments, in large part because they (1) model
reordering by allowing unrestricted movement of
words, rather than constrained movement of com-
positional units, and therefore must (2) attempt to
compensate via directed, asymmetric distortion and
fertility models.
</bodyText>
<note confidence="0.965074">
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 28–36,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999915265306123">
The conventional heuristics for attempting to re-
cover from the resulting alignment errors is to es-
timate two directed models in opposite directions
and then intersect their alignments – to make up
for the fact that, in reality, word alignment is an
inherently joint relation. It is unfortunate that such
a critical stage in the training process of an SMT
system relies on inaccurate heuristics, which have
been largely motivated by historical implementa-
tion factors, rather than principles explaining lan-
guage phenomena.
Inversion Transduction Grammar (ITG) models
provide a natural, alternative approach, by estimat-
ing the joint word alignment relation directly,
eliminating the need for any of the conventional
heuristics. A transduction grammar is a grammar
that generates sentences in two languages (L0 and
L1) simultaneously; i.e., one start symbol expands
into two strings, as for example in Figure 1(b). A
transduction grammar explains two languages si-
multaneously. ITGs model a class of transductions
(sets of sentence translations) with expressive
power and computational complexity falling be-
tween (a) finite-state transducers or FSTs and (b)
syntax-directed transduction grammars1 or SDTGs.
An ITG produces both a common structural form
for a sentence pairs, as well as relating the words –
aligning them. This could actually work as the
joint word alignment that is usually constructed by
heuristic function combination.
Yet despite the substantial body of literature on
word alignment, ITG based models, and phrase-
based SMT, the existing work has not assessed the
potential for improving phrase-based translation
quality by using joint ITG based word alignments
to replace the error-prone conditional IBM model
based word alignments and associated heuristics
for intersecting bidirectional IBM alignments.
On one hand, word alignment work is usually
evaluated not on actual translation quality, but
rather on artificial metrics like alignment error rate
(AER, Och &amp; Ney, 2003), which relies on a manu-
ally annotated gold standard word alignment.
There are some indications that ITG produces bet-
ter alignment then the standard method (Zhao &amp;
Vogel, 2003, Zhang &amp; Gildea 2005, Chao &amp; Li,
2007). There is, however, little inherent utility in
alignments – their value is determined by the SMT
systems one can build from them. In fact, recent
</bodyText>
<footnote confidence="0.436106">
1 Which “synchronous CFGs” are essentially identical to.
</footnote>
<bodyText confidence="0.9988124375">
studies have discredited the earlier assumption that
lower AER is correlated with improved translation
quality – the opposite can very well occur (Ayan &amp;
Dorr, 2006). Therefore it is essential to evaluate
the quality of the word alignment not in terms of
AER, but rather in terms of actual translation qual-
ity in a system built from it.
On the other hand, ITG models have been em-
ployed to improve translation quality as measured
by BLEU (Papineni et al., 2002), but still without
directly addressing the problem of dependence on
inaccurate IBM alignments. S‡nchez &amp; Bened’
(2006) construct an ITG from word alignments
computed by the conventional IBM model, which
does little to alleviate the problems. Sima’an &amp;
Mylonakis (2008) use an ITG to structure a prior
distribution to a phrase extraction system, which is
an altogether different approach. Cherry &amp; Lin
(2007) do use ITG to build word alignments, but
blur the lines by still mixing in the conventional
IBM method, and focus on phrase extraction.
The present work clearly demonstrates, for the
first time to our knowledge, that replacing the
widely-used heuristic of intersecting IBM word
alignments from two directed conditional models
instead with a single ITG alignment from a joint
model produces superior translation accuracy. The
experiments are performed on three distinct lan-
guage pairs: German–English, Spanish–English,
and French–English. Translation accuracy is re-
ported in terms of BLEU, NIST, and METEOR
metrics.
</bodyText>
<sectionHeader confidence="0.971036" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999665">
Statistical Machine Translation is a paradigm
where translation is considered as a code-breaking
problem. The goal is to find the most likely output
sentence (clear text message) given the supplied
input sentence (coded message), according to some
model.
To get a probabilistic model, large amounts of
training data are used. These data have to be
aligned so that an understanding of correspon-
dences between the languages is there to be learnt
from. Even if the data is assumed to be aligned at
sentence level, sub-sentence alignment is also
needed. This is usually carried out by training
some statistical model of a word-to-word function
(Brown et al., 1993), or a hidden Markov model
consuming input words and emitting output words
</bodyText>
<page confidence="0.99758">
29
</page>
<bodyText confidence="0.99889434375">
(Vogel, Ney &amp; Tillmann, 1996). The toolkit
GIZA++ (Och &amp; Ney, 2000) is freely available and
widely used to compute such word alignments.
All these models learn a directed translation
function that maps input words to output words.
Since these functions focus solely on surface phe-
nomena, they have no mechanisms for dealing with
the kind of structured reordering between lan-
guages that could account for, e.g., the difference
between SVO languages and SOV languages.
What emerges is in fact a rather flawed model of
how one language is rewritten into another. The
conventional way to alleviate this flaw is to train
an equally flawed model in the other direction, and
then intersect the two. This practice certainly alle-
viates some of the problems, but far from all.
To build a phrase-based SMT system, the word
alignment is used as a starting point to try to ac-
count for the entire sentence. This means that the
word alignment is gradually expanded, so that all
words in both sentences are accounted for, either
by words in the other language, or by the null
empty word ε. This process is called grow-diag-
final (Koehn, Och &amp; Marcu, 2003).
The grow-diag-final process does smooth over
some of the flaws still left in the word alignment,
but error analysis gives reason to doubt that it re-
pairs enough of the errors to avoid damaging trans-
lation accuracy. Thus, we are motivated to
investigate a completely different approach that
attempts to avoid the noisy directed alignments in
the first place.
</bodyText>
<subsectionHeader confidence="0.956635">
2.1 Inversion Transduction Grammars
</subsectionHeader>
<bodyText confidence="0.999901938461539">
A transduction is a set of sentence translation
pairs – just as a language is a set of sentences. The
set defines a relation between the input and output
languages.
In the generative view, a transduction gram-
mar generates a transduction, i.e., a set of sentence
translation pairs or bisentences – just as an ordi-
nary (monolingual) language grammar generates a
language, i.e., a set of sentences. In the recogni-
tion view, alternatively, a transduction grammar
biparses or accepts all sentence pairs of a trans-
duction – just as a language grammar parses or
accepts all sentences of a language. And in the
transduction view, a transduction grammar trans-
duces (translates) input sentences to output sen-
tences.
Two familiar classes of transductions have been
in widespread use for decades in many areas of
computer science and linguistics:
A syntax-directed transduction is a set of bisen-
tences generated by some syntax-directed transduc-
tion grammar or SDTG (Lewis &amp; Stearns, 1968;
Aho &amp; Ullman, 1969, 1972). A “synchronous CFG”
is equivalent to an SDTG.
A finite-state transduction is a set of bisentences
generated by some finite-state transducer or FST.
It is possible to describe finite-state transductions us-
ing SDTGs (or synchronous CFGs) by restricting
them alternatively to the special cases of either “right
regular SDTGs” or “left regular SDTGs”. However,
such characterizations rather misleadingly overlook
the key point – by severely limiting expressive
power, finite-state transductions are orders of magni-
tude cheaper to biparse, train, and induce than syn-
tax-directed transductions – and are often even more
accurate to induce.
More recently, an intermediate equivalence class
of transductions whose generative capacity and
computational complexity falls in between these
two has become widely used in state-of-the-art MT
systems – due to numerous empirical results indi-
cating significantly better fit to modeling transla-
tion between many human language pairs:
An inversion transduction is a set of bisentences
generated by some inversion transduction gram-
mar or ITG (Wu, 1995a, 1995b, 1997). As above
with finite-state transductions, it is possible to de-
scribe inversion transductions using SDTGs (or syn-
chronous CFGs) by restricting them alternatively to
the special cases of “binary SDTGs”, “ternary
SDTGs”, or “SDTGs whose transduction rules are
restricted to straight and inverted permutations only”.
Again however, as above, such characterizations
rather misleadingly overlook the key point – by se-
verely limiting expressive power, inversion transduc-
tions are orders of magnitude cheaper to biparse,
train, and induce than syntax-directed transductions –
and are often even more accurate to induce.
Any SDTG (or synchronous CFG) of binary
rank – i.e., that has at most two nonterminals on
the right-hand-side of any rule – is an ITG. (Simi-
larly, any SDTG (or synchronous CFG) that is
right regular is a finite-state transduction gram-
mar.) Thus, for example, any grammar computed
by the binarization algorithm of Zhang et al.
</bodyText>
<page confidence="0.990165">
30
</page>
<bodyText confidence="0.999123">
(2006) is an ITG. Similarly, any grammar induced
following the hierarchical phrase-based translation
method, which always yields a binary transduction
grammar (Chiang 2005), is an ITG.
Moreover, any SDTG (or synchronous CFG) of
ternary rank – i.e., that has at most three nontermi-
nals on the right-hand-side of any rule – is still
equivalent to an ITG. Of course, this does not hold
for SDTGs (or synchronous CFGs) in general,
which allow arbitrary rank (possibly exceeding
three) at the price of exponential complexity, as
summarized in Table 1.
</bodyText>
<table confidence="0.97972144">
monolingual bilingual
regular or regular or
finite-state finite-state
languages transductions
FSA FST
or or
CFG that is SDTG (or syn-
right regular chronous CFG)
or left regular that is
context-free right regular
languages or left regular
inversion
transductions
CFG ITG O(n6)
or
SDTG (or syn-
chronous CFG)
that is binary
or ternary
or inverting
syntax-directed
transductions
SDTG O(n2n+2)
(or synchro-
nous CFG)
</table>
<tableCaption confidence="0.878985">
Table 1: Summary comparison of computational
complexity for Viterbi and chart (bi)parsing, and
EM training algorithms for both monolingual and
bilingual hierarchies.
</tableCaption>
<bodyText confidence="0.9996316875">
Without loss of generality, any ITG can be con-
veniently written in a 2-normal form (Wu, 1995a,
1997). This cannot be done for SDTGs (or syn-
chronous CFGs) – unlike the monolingual case of
CFGs, which form an equivalence class of context-
free languages that can all be written in Chomsky’s
2-normal form. In the bilingual case, only ITGs
form an equivalence class of inversion transduc-
tions that can all be written in a 2-normal form.
Formally, an ITG in this 2-normal form, which
segregates syntactic versus lexical rules, consists
of a tuple where N is a set of non-
terminal symbols, V0 and V1 are the vocabularies of
L0 and L1 respectively, R is a set of transduction
rules, and is the start symbol. Each trans-
duction rule takes one of the following forms:
</bodyText>
<equation confidence="0.999873">
S → X
X → [Y Z]
X → &lt;Y Z&gt;
X → segmentL0/ε
X → ε/segmentL1
X → segmentL0/segmentL1
</equation>
<bodyText confidence="0.99968809375">
where X, Y and Z may be any nonterminal.
Aside from the start rule, there are two kinds of
syntactic transduction rules, namely straight and
inverted. In the above notation, straight transduc-
tion rules X → [Y Z] use square brackets,
whereas inverted rules X → &lt;Y Z&gt; use angled
brackets. The transductions generated by straight
nodes have the same order in both languages,
whereas the transduction generated by the inverted
nodes are inverted in one of the languages, mean-
ing that the children are read left-to-right in L0 and
right-to-left in L1. In Figure 1(b) for example, the
parse tree node instantiating an inverted transduc-
tion rule is marked with a horizontal bar. This
mechanism allows for a minimal amount of reor-
dering, while keeping the complexity down.
The last three forms are for lexical transduction
rules. Each segment comes from the vocabulary
of one of the languages, indicated by the subscript.
In the simplest case, the two ε-rule forms define
singletons, which insert “spurious” segments into
either language. Spurious segments lack any cor-
respondence in the other language – they are
“aligned to null” – and singletons are lexical rules
that associate a null-aligned segment in one of the
languages with an empty segment (ε) in the other.
On the other hand, the last rule form defines a
lexical translation pair that aligns the
word/phrase segmentL0 to its translation
segmentL1. Such rules can also be written com-
positionally as a pair of singletons, although it
reads less transparently:
</bodyText>
<equation confidence="0.836628">
X → segmentL0/ε ε/segmentL1
</equation>
<page confidence="0.996711">
31
</page>
<bodyText confidence="0.9757205">
Note that segments typically consist of multiple
tokens. Common examples include:
</bodyText>
<listItem confidence="0.997789142857143">
• Chinese word/phrase segments consisting of
multiple unsegmented character tokens
• Chinese word/phrase segments consisting of
multiple smaller, presegmented multi-
character word/phrase tokens
• English phrase/collocation segments consist-
ing of multiple word tokens (roller coaster)
</listItem>
<bodyText confidence="0.999505694444444">
ITGs inherently model phrasal translation – lin-
guistically speaking, ITGs assume the set of lexical
translation pairs constitutes a phrasal lexicon (just
as lexicographers assume in building ordinary eve-
ryday dictionaries). An advantage of this is that the
ITG biparsing and decoding algorithms perform
integrated translation-driven segmentation si-
multaneously with optimizing the parse (Wu,
1997; Wu &amp; Wong, 1998).
These properties allow an ITG to (1) insert and
delete words/phrases, which matches the ability of
the conventional methods for word alignment as
well as phrase alignment, and (2) account for the
reordering in a more principled and restricted way
than conventional alignment methods.
A stochastic ITG or SITG is an ITG where
every rule is associated with a probability. As with
a stochastic CFG (SCFG), the probabilities are
conditioned on the left-hand-side symbol, so that
the probability of rule X → χ is p(χ|X).
A bracketing ITG or BITG or BTG (Wu,
1995a) contains only one nonterminal symbol,
with syntactic transduction rules X → [X X] and
X → &lt;X X&gt;, which means that it produces a
bracketing rather than a labeled tree. With a sto-
chastic BITG (SBITG or SBTG) it is still possible
to determine an optimal tree, since inversion and
alignment are coupled: where inversions are
needed is decided by the translations, and vice
versa.
In Wu (1995b) algorithms for training a SITG
using expectation maximization, as well as finding
the optimal parse of a sentence pair given a SITG
are presented. These are polynomial time O(n6), as
seen in Table 1. Further pruning methods can also
be added, especially for longer sentences.
</bodyText>
<subsectionHeader confidence="0.992932">
2.2 Previous uses of ITG in alignment
</subsectionHeader>
<bodyText confidence="0.999943918918919">
There have been several attempts to use various
forms of ITGs in an alignment setting.
Zhao &amp; Vogel (2003) and S‡nchez &amp; Bened’
(2006) both use GIZA++ to establish their SITG.
Since they use GIZA++ to create their ITG, little
light is shed on the question of whether an ITG
produces better alignments than GIZA++.
Zhang &amp; Gildea (2005) compare lexicalized and
standard ITGs on an alignment task, and conclude
that both are superior to IBM models 1 and 4, and
that lexicalization helps. They also employ some
pruning techniques to speed up training. Chao &amp; Li
(2007) incorporate the reordering constraints im-
posed by an ITG to their discriminative word
aligner, and also note a lower alignment error rate
in their system. Since neither work evaluates re-
sults on a translation task, it is hard to know
whether better AER would translate into improved
translation quality, in light of Ayan &amp; Dorr (2006).
Sima’an &amp; Mylonakis (2008) use an ITG as the
basis of a prior distribution in their system that ex-
tracts all possible phrases rather than employing a
length cut-off, and report an increase in translation
quality as measured by the BLEU score (Papineni
et al., 2002). In this paper, it is not primarily pure
ITG that is being evaluated, but it lends some
credibility to our assumption that the ITG structure
helps when aligning.
Cherry &amp; Lin (2007) use an ITG to produce
phrase tables that are then used in a translation sys-
tem. However, to make their system outperform
GIZA++, they blend in a non-compositionality
constraint that is still based on GIZA++ word
alignments. We would very much like to clearly
see and understand the difference between ITG and
GIZA++ alignments, and the lines are somewhat
blurred in their work.
</bodyText>
<sectionHeader confidence="0.995991" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999908153846154">
First, the lexicon of the SBITG is initialized, by
extracting lexical transduction rules from cooccur-
rence data from the corpus. Each pair of tokens in
each sentence pair is initially considered equally
likely to be a lexical translation pair. Each token is
also considered to be a possible singleton. The two
syntactic transduction rules X → [X X] and X →
&lt;X X&gt; are initially assumed to be equally likely.
Then full expectation-maximization training
(Wu, 1995b) is carried out on the training data.
Instead of waiting for full convergence, the process
is halted when the increase in the training data’s
probability starts to decline.
</bodyText>
<page confidence="0.99127">
32
</page>
<figure confidence="0.9999775">
(a) (a) (a)
(b) (b) (b)
</figure>
<figureCaption confidence="0.999151444444444">
Figure 1: (a) Bidirectional IBM
alignments and their intersection
and (b) ITG alignments.
Figure 2: (a) Bidirectional IBM
alignments and their intersection
and (b) ITG alignments.
Figure 3: (a) Bidirectional IBM
alignments and their intersec-
tion and (b) ITG alignments.
</figureCaption>
<bodyText confidence="0.8978010625">
At this point, we extract the optimal parses from
the training data, and use the word alignment im-
posed by the ITG instead of the one computed by
GIZA++ (Och &amp; Ney, 2000). Training after this
point is carried out according to the guidelines for
the WSMT08 baseline system (see section 4.2). In
Figure 1(a) is an example of a sentence aligned
with GIZA++, and in Figure 1(b) is the same sen-
tence, aligned with ITG. In this case it is clearly
visible how the structured reordering constraints
that the ITG enforces results in a clear alignment,
whereas GIZA++ is unable to sort it out.
sentence pairs tokens
de-en 115,323 1,602,781
es-en 108,073 1,466,132
fr-en 95,990 1,340,718
</bodyText>
<tableCaption confidence="0.98698">
Table 2: Summary of training data.
</tableCaption>
<sectionHeader confidence="0.995384" genericHeader="method">
4 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.933188">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9999826">
We used a subset of the data provided for the Sec-
ond Workshop on Statistical Machine Translation2,
which consists mainly of texts from the Europarl
corpus (Koehn, 2005). We used the Europarl part
for the translation tasks: German–English (de-en),
Spanish–English (es-en), and French–English
(fr-en). Table 2 summarizes the datasets used for
training. For tuning and testing, the tuning and de-
velopment test sets provided for the workshop
were used – each measuring 2,000 sentence pairs.
</bodyText>
<subsectionHeader confidence="0.990381">
4.2 Baseline system
</subsectionHeader>
<bodyText confidence="0.989946666666667">
For baseline system we trained phrase-based SMT
models with GIZA++ (Och &amp; Ney, 2000), the
training scripts supplied with Moses (Koehn et al.,
</bodyText>
<footnote confidence="0.965704">
2 www.statmt.org/wmt08
</footnote>
<page confidence="0.999045">
33
</page>
<bodyText confidence="0.999797733333333">
2007), and minimum error rate training (MERT,
Och, 2003), all according to the WSMT08-
guidelines for baseline systems. This means that 5
iterations are carried out with IBM model 1 train-
ing, 5 iterations with HMM training, 3 iterations of
IBM model 3 training, and finally 3 iterations of
IBM model 4 training. After GIZA++ training, the
Moses training script extracts and scores phrases,
and establishes a lexicalized reordering model.
The WSMT08 guidelines call for the combina-
tion heuristic “grow-diag-final-and” (GDFA). We
also tried the “intersect” combination heuristic,
which simply calculates the intersection of align-
ment points in the two directed alignments pro-
vided by GIZA++.
</bodyText>
<subsectionHeader confidence="0.998419">
4.3 SBITG system
</subsectionHeader>
<bodyText confidence="0.999975461538461">
Since imposing an SBITG biparse on a sentence
pair forces a word alignment on the sentence pair,
word alignment under SBITG models is identical
to biparsing.
Expectation-maximization training was used to
induce a SBITG from the training data. Training is
halted when the EM-process started to converge. In
our experience, convergence typically requires no
more than 3 iterations or so. When EM training is
finished, we extracted the optimal biparses from
the training data, which then constitute the optimal
alignment given the grammar. This alignment was
then output in GIZA++ format. All singletons from
the SBITG alignment were converted to be null-
alignments in the GIZA++ formatted file. These
files could then be used instead of GIZA++ in the
remainder of the training process for the phrase-
based translation system.
Although the results from the ITG are inter-
preted as two directed alignments, they are identi-
cal, both with each other and the intersection.
Trying different combination heuristics for these
results always yields the same results.
The training process was identical save for the
fact that the word alignments were produced by
SBITGs rather than by GIZA++.
</bodyText>
<sectionHeader confidence="0.991991" genericHeader="method">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.99719465">
We trained a total of nine systems (three tasks and
three different alignments), which we evaluated
with three different measures: BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), and
METEOR (Lavie &amp; Agarwal 2007).
Figure 2 shows a sentence pair as it was aligned
with the two different models. Figure 2(a) shows
the GIZA++ alignment in both directions, and the
intersection between them, whereas Figure 2(b)
shows the SBITG alignment with its common
structure. The asymmetric reordering mechanism
of the IBM models is simply unable to relate the
two halves to one another. The segment zur
kenntnis genommen could certainly be said to
mean note, but as a verb, and not as a noun, which
is the current usage of the word. This is an inherent
problem of the asymmetry of the IBM models,
which is rectified by simultaneous alignment.
Figure 3 shows another sentence pair. Again,
Figure 3(a) was aligned with GIZA++ and Figure
3(b) with the SITG model. This shows a case with
perhaps even more structured reordering, where a
notion of constituency is definitely needed to get it
right. SITG handles constituency, and gets this is-
sue right. The IBM models do not, resulting in the
error of aligning either to aufgerufen.
As mentioned before, the GDFA heuristic is ap-
plied after the word alignment process, and it does
fix some of these problems. Therefore we opted to
evaluate this, not on alignments, but rather on
translation quality of phrase based SMT systems
derived from the alignments. Our empirical results
confirm that SBITG alignments do indeed lead to
better translation quality, as shown in Table 2.
We also tried the intersect combination heuris-
tic, and depending on language pair and evaluation
metric, the GDFA and intersect heuristics come out
on top. The ITG approach is, however, consistently
better than either of the heuristics applied to
GIZA++ output.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99998375">
There are of course fundamental differences be-
tween ITG and IBM models. The main difference
is that IBM models are directed and surface ori-
ented, whereas the ITG model is joint and struc-
tured. The directedness means that the IBM models
are unable to produce a word alignment that is op-
timal for a sentence pair; they can only produce
word alignments that are optimal when translating
from one language into the other. An ITG on the
other hand is capable of producing the optimal
alignment that explains both sentences in the pair.
We see this phenomenon clearly in Figures 1–3.
</bodyText>
<page confidence="0.996319">
34
</page>
<table confidence="0.999324666666667">
BLEU NIST METEOR
GIZA++ SBITG GIZA++ SBITG GIZA++ SBITG
GDFA inters. GDFA inters. GDFA inters.
de-en 20.59 20.69 21.13 5.8668 5.8623 5.9380 0.4969 0.4953 0.5029
es-en 25.97 26.33 26.63 6.6352 6.6793 6.7407 0.5599 0.5582 0.5612
fr-en 26.03 26.17 26.63 6.6907 6.7071 6.8151 0.5544 0.5560 0.5635
</table>
<tableCaption confidence="0.8586225">
Table 2: Results. The best result on each task/metric combination is in bold digits.
(The identical results for SBITG on Spanish–English and French–English are not typos.)
</tableCaption>
<bodyText confidence="0.999976012658228">
IBM models are also built to allow for fairly
“whimsical” reorderings, which are not modeled
very well to begin with. This allows for far too
many degrees of freedom to fit the model to the
data. Because natural languages are inherently
structural, this excess degree of freedom could hurt
performance. Some restraints are needed. ITGs on
the other hand only allow for compositionally
structured reordering, which corresponds better to
the reorderings between natural languages. There
are some issues with ITG as well, one of them be-
ing that all permutations are actually not allowed,
even if structured. This has led to some problems
when an a prior alignment or structure is forced
upon a sentence pair, but using unrestricted expec-
tation-maximization means that the sentence pair is
fitted to the grammar, and what the grammar can-
not express is not applied to the data. Even if ITG
proves to be too restrictive in the future, the fact
that it bases reordering on structure, rather than
unrestricted lexical movement, gives it an edge
over the IBM models. The benefits of structured
reordering as opposed to unrestricted are clearly
visible in Figures 1–3.
An argument to continue using IBM models is
that two directed alignments can be intersected and
heuristically grown to build a joint alignment, thus
compensating for the flaws in the original models.
But as we have seen in Figure 3, even the combi-
nation of two models contains errors that should
have been avoided. This approach is not able to
smooth over the flaws of the IBM models.
The results in this paper give credibility to the
claim that these limitations of the IBM models are
so serious that they hurt translation quality of sys-
tems built upon them; even after the phrase build-
ing heuristic has been applied. Systems built on
ITG alignment on the other hand fare better, on all
three evaluation metrics.
There is still more to be done. So far we have
only employed bracketing SITGs, which are not
able to distinguish one structure form another. The
structural changes that the SBITG is capable of are
dictated by the alignment of the leaves in the tree.
This seems impressive, given the information at
hand, but is really a logical conclusion of the fact
that the grammar can leverage different alignment
probabilities against each other, and as the align-
ment is coupled to the structure of the ITG parse,
the structure is constrained to the alignment. The
reverse is also true: the alignment is constrained by
the structure. This coupling is essential to the train-
ing of SITGs. For a SBITG, there is very little in-
formation in the structure, only the decision to read
the node as straight or inverted. This is not an in-
herent property of ITGs in general; more informa-
tion can be carried higher up in the tree by labeling
the nonterminals. There is great hope that adding
more information to the structuring, even better
alignments could be gained.
In this paper we have extracted the word align-
ments from ITG biparses, and inserted them into
the conventional phrase-based SMT pipeline. It is
feasible to extract phrases directly from the gram-
mar, as demonstrated by Cherry &amp; Lin (2007). Our
results suggest that augmenting other portions of
the phrase-based SMT framework with ITG struc-
tures might also be worth exploring, in particular
decoding. Recall that in the transduction view of
transduction grammars (as opposed to generative
or recognition views), an output translation can be
determined by parsing an input sentence with a
transduction grammar (Wu 1996; Wu &amp; Wong
1998). This kind of translation would also entail
the notion of structure that we have just witnessed
helping alignment. Phrase-based SMT currently
relies on unrestricted phrasal movement, which is a
lot better than unrestricted lexical movement, but
could probably use some structure as well.
</bodyText>
<page confidence="0.998927">
35
</page>
<sectionHeader confidence="0.99797" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999963884615385">
We have shown that learning word alignments
through a compositionally-structured, joint process
yields higher phrase-based translation accuracy
than the conventional heuristic of intersecting con-
ditional models.
The conventional method with IBM-models suf-
fers from their directionality. The asymmetry
causes bad alignments. We have instead introduced
an automatically induced ITG alignment that does
not suffer from this asymmetry, and is able to ex-
plain the two sentences simultaneously rather than
one in terms of the other. The IBM-models also
suffers from a simplified reordering model, which
relies on moving individual words. The hierarchi-
cal structure of ITGs means that even a BITG has
enough structural information to outperform the
IBM models. Previous work shows that these ad-
vantages translate into better alignments as meas-
ured against a manually annotated gold standard
using alignment error rate (AER). Previous work
also shows that AER is a poor indicator of whether
translation quality is increased. We have showed
that the increase in alignment quality actually
translates into an increase in translation quality in
this case, as measured by BLEU, NIST and
METEOR across three different language pairs.
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.634548777777778">
This material is based upon work supported in part by the Swedish
National Graduate School of Language Technology, the Olof Gjerd-
mans Travel Grant, the Defense Advanced Research Projects Agency
(DARPA) under GALE Contract No. HR0011-06-C-0023, and the
Hong Kong Research Grants Council (RGC) under research grants
GRF621008, DAG03/04.EG09, RGC6256/00E, and RGC6083/99E.
Any opinions, findings and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessarily
reflect the views of the Defense Advanced Research Projects Agency.
</bodyText>
<sectionHeader confidence="0.984272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999639744186046">
AHO, Alfred V. &amp; Jeffrey D. ULLMAN (1969) “Syntax-directed trans-
lations and the pushdown assembler” in Journal of Computer and
System Sciences 3: 37–56.
AHO, Alfred V. &amp; Jeffrey D. ULLMAN (1972) The Theory of Parsing,
Translation, and Compiling (Volumes 1 and 2). Englewood Cliffs,
NJ: Prentice-Hall.
AYAN, Necip Fazil &amp; Bonnie J. DORR (2006) “Going Beyond AER:
An Extensive Analysis of Word Alignments and Their Impact on
MT” in COLING-ACL’06, pp. 9–16, Sydney, Australia, July 2006.
BROWN, Peter F., Stephen A. DELLA PIETRA, Vincent J. DELLA
PIETRA &amp; Robert L. MERCER (1993) “The Mathematics of Statis-
tical Machine Translation” in Computational Linguistics 19(2):
263–311.
CHAO, Wen-Han &amp; Zhou-Jun LI (2007) “Incorporating Constituent
Structure Constraint into Discriminative Word Alignment” in MT
Summit XI, pp. 97–103, Copenhagen, Denmark.
CHERRY, Colin &amp; Dekang LIN (2007) “Inversion Transduction
Grammar for Joint Phrasal Translation Modeling” in Proceedings
of SSST, pp. 17–24, Rochester, New York, April 2007.
CHIANG, David (2005) “A Hierarchical Phrase-Based Model for Sta-
tistical Machine Translation” in ACL-2005, pp. 263–270, Ann Ar-
bor, MI, June 2005.
KOEHN, Philipp, Franz Josef OCH &amp; Daniel MARCU (2003) “Statisti-
cal Phrase-based Translation” in HLT-NAACL’03, pp. 127–133.
KOEHN, Philipp (2005) “Europarl: A Parallel Corpus for Statistical
Machine Translation” in MT Summit X, Phuket, Thailand, Septem-
ber 2005.
DODDINGTON, George (2002) “Automatic Evaluation of Machine
Translation Quality using n-gram Co-occurrence Statistics” in
HLT-2002. San Diego, California.
KOEHN, Philipp, Hieu HOANG, Alexandra BIRCH, Chris CALLISON-
BURCH, Marcello FEDERICO, Nicola BERTOLDI, Brooke COWAN,
Wade SHEN, Christine MORAN, Richard ZENS, Chris DYER, On-
drej BOJAR, Alexandra CONSTANTIN &amp; Evan HERBST (2007)
“Moses: Open Source Toolkit for Statistical Machine Translation”
in ACL’07, Prague, Czech Republic, June 2007.
LAVIE, Alon &amp; Abhaya AGARWAL (2007) “METEOR: An Automatic
Metric for MT Evaluation with High Levels of Correlation with
Human Judgment” in WSMT. Prague, Czech Republic, June 2007.
LEWIS, Philip M. &amp; Richard E. STEARNS. (1968) “Syntax-directed
transduction” in Journal of the ACM 15: 465–488.
OCH, Franz Josef &amp; Hermann NEY (2000) “Improved Statistical
Alignment Models” in ACL-2000, pp. 440–447, Hong Kong, Oc-
tober 2000.
OCH, Franz Josef (2003) “Minimum error rate training in statistical
machine translation” in ACL’03.
OCH, Franz Josef &amp; Hermann NEY (2003) “A Systematic Comparison
of Various Statistical Alignment Models” in Computational Lin-
guistics 29(1), pp. 19–52.
PAPINENI, Kishore, Salim ROUKOS, Todd WARD &amp; Wei-Jing ZHU
(2002) “BLEU: a Method for Automatic Evaluation of Machine
Translation” in ACL’02, pp 311-318. Philadelphia, Pennsylvania.
SçNCHEZ, J. A., J.M. BENEDê (2006) “Stochastic Inversion Transduc-
tion Grammars for Obtaining Word Phrases for Phrase-based Sta-
tistical Machine Translation” in WSMT, pp. 130–133, New York
City, June 2006.
SIMA’AN, Khalil &amp; Markos MYLONAKIS (2008) “Better Statistical
Estimation Can Benefit all Phrases in Phrase-based Statistical Ma-
chine Translation” in SLT 2008, pp. 237–240, Goa, India, Decem-
ber 2008.
VOGEL, Stephan, Hermann NEY &amp; Christoph TILLMANN (1996)
“HMM-based Word Alignment in Statistical Translation” in
COLING’96, pp. 836–841.
WU, Dekai (1995a) “An Algorithm for Simultaneously Bracketing
Parallel Texts by Aligning Words” in ACL’95, pp. 244–251, Cam-
bridge, Massachusetts, June 1995.
WU, Dekai (1995b) “Trainable Coarse Bilingual Grammars for Paral-
lel Text Bracketing” in WVLC-3, pp. 69–82, Cambridge, Massa-
chusetts, June 1995.
WU, Dekai (1996) “A polynomial-time algorithm for statistical ma-
chine translation” in ACL-96, Santa Cruz, CA: June 1996.
WU, Dekai (1997) “Stochastic Inversion Transduction Grammars and
Bilingual Parsing of Parallel Corpora” in Computational Linguis-
tics 23(3), pp 377–403.
WU, Dekai &amp; Hongsing WONG (1998) “Machine Translation with a
Stochastic Grammatical Channel” in COLING-ACL&apos;98, Montreal,
August 1998.
ZHANG, Hao &amp; Daniel GILDEA (2005) “Stochastic Lexicalized Inver-
sion Transduction Grammar for Alignment” in ACL’05, pp. 475–
482, Ann Arbor, June 2005.
ZHANG, Hao, Liang HUANG, Dan GILDEA &amp; Kevin KNIGHT (2006)
“Synchronous Binarization for Machine Translation” in
HLT/NAACL-2006, pp. 256–263, New York, June 2006.
ZHAO, Bing &amp; Stephan VOGEL (2003) “Word Alignment Based on
Bilingual Bracketing” in HLT-NAACL Workshop: Building and
Using Parallel Texts, pp. 15–18, Edmonton, May–June 2003.
</reference>
<page confidence="0.998942">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239933">
<title confidence="0.8887955">Improving Phrase-Based Translation via Word from Stochastic Inversion Transduction Grammars</title>
<affiliation confidence="0.668101">Dept. of Linguistics and Uppsala Sweden</affiliation>
<email confidence="0.941533">markus.saers@lingfil.uu.se</email>
<abstract confidence="0.99881715625">We argue that learning word alignments through a compositionally-structured, joint process yields higher phrase-based translation accuracy than the conventional heuristic of intersecting conditional models. Flawed word alignments can lead to flawed phrase translations that damage translation accuracy. Yet the IBM word alignments usually used today are known to be flawed, in large part because IBM models (1) model reordering by allowing unrestricted movement of words, rather than constrained movement of compositional units, and therefore must (2) attempt to compensate via directed, asymmetric distortion and fertility models. The conventional heuristics for attempting to recover from the resulting alignment errors involve estimating two directed models in opposite directions and then intersecting their alignments – to make up for the fact that, in reality, word alignment is an inherently joint relation. A natural alternative is provided by Inversion Transduction Grammars, which estimate the joint word alignment relation directly, eliminating the need for any of the conventional heuristics. We show that this alignment ultimately produces superior translation accuracy on BLEU, NIST, and METEOR metrics over three distinct language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V AHO</author>
<author>Jeffrey D ULLMAN</author>
</authors>
<title>Syntax-directed translations and the pushdown assembler”</title>
<date>1969</date>
<journal>in Journal of Computer and System Sciences</journal>
<volume>3</volume>
<pages>37--56</pages>
<marker>AHO, ULLMAN, 1969</marker>
<rawString>AHO, Alfred V. &amp; Jeffrey D. ULLMAN (1969) “Syntax-directed translations and the pushdown assembler” in Journal of Computer and System Sciences 3: 37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V AHO</author>
<author>Jeffrey D ULLMAN</author>
</authors>
<title>The Theory of Parsing,</title>
<date>1972</date>
<journal>Translation, and Compiling (Volumes</journal>
<volume>1</volume>
<publisher>Prentice-Hall.</publisher>
<location>Englewood Cliffs, NJ:</location>
<marker>AHO, ULLMAN, 1972</marker>
<rawString>AHO, Alfred V. &amp; Jeffrey D. ULLMAN (1972) The Theory of Parsing, Translation, and Compiling (Volumes 1 and 2). Englewood Cliffs, NJ: Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Necip Fazil AYAN</author>
<author>Bonnie J DORR</author>
</authors>
<title>Going Beyond AER: An Extensive Analysis of Word Alignments and Their</title>
<date>2006</date>
<booktitle>Impact on MT” in COLING-ACL’06,</booktitle>
<pages>9--16</pages>
<location>Sydney, Australia,</location>
<marker>AYAN, DORR, 2006</marker>
<rawString>AYAN, Necip Fazil &amp; Bonnie J. DORR (2006) “Going Beyond AER: An Extensive Analysis of Word Alignments and Their Impact on MT” in COLING-ACL’06, pp. 9–16, Sydney, Australia, July 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F BROWN</author>
<author>Stephen A DELLA PIETRA</author>
<author>Vincent J DELLA PIETRA</author>
<author>Robert L MERCER</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation” in Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>BROWN, PIETRA, PIETRA, MERCER, 1993</marker>
<rawString>BROWN, Peter F., Stephen A. DELLA PIETRA, Vincent J. DELLA PIETRA &amp; Robert L. MERCER (1993) “The Mathematics of Statistical Machine Translation” in Computational Linguistics 19(2): 263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Han CHAO</author>
<author>Zhou-Jun LI</author>
</authors>
<title>Incorporating Constituent Structure Constraint into Discriminative Word Alignment”</title>
<date>2007</date>
<booktitle>in MT Summit XI,</booktitle>
<pages>97--103</pages>
<location>Copenhagen,</location>
<marker>CHAO, LI, 2007</marker>
<rawString>CHAO, Wen-Han &amp; Zhou-Jun LI (2007) “Incorporating Constituent Structure Constraint into Discriminative Word Alignment” in MT Summit XI, pp. 97–103, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin CHERRY</author>
<author>Dekang LIN</author>
</authors>
<title>Inversion Transduction Grammar for Joint Phrasal Translation Modeling”</title>
<date>2007</date>
<booktitle>in Proceedings of SSST,</booktitle>
<pages>17--24</pages>
<location>Rochester, New York,</location>
<marker>CHERRY, LIN, 2007</marker>
<rawString>CHERRY, Colin &amp; Dekang LIN (2007) “Inversion Transduction Grammar for Joint Phrasal Translation Modeling” in Proceedings of SSST, pp. 17–24, Rochester, New York, April 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David CHIANG</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation”</title>
<date>2005</date>
<booktitle>in ACL-2005,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI,</location>
<marker>CHIANG, 2005</marker>
<rawString>CHIANG, David (2005) “A Hierarchical Phrase-Based Model for Statistical Machine Translation” in ACL-2005, pp. 263–270, Ann Arbor, MI, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp KOEHN</author>
<author>Franz Josef OCH</author>
<author>Daniel MARCU</author>
</authors>
<date>2003</date>
<booktitle>Statistical Phrase-based Translation” in HLT-NAACL’03,</booktitle>
<pages>127--133</pages>
<marker>KOEHN, OCH, MARCU, 2003</marker>
<rawString>KOEHN, Philipp, Franz Josef OCH &amp; Daniel MARCU (2003) “Statistical Phrase-based Translation” in HLT-NAACL’03, pp. 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp KOEHN</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation”</title>
<date>2005</date>
<booktitle>in MT Summit X,</booktitle>
<location>Phuket, Thailand,</location>
<marker>KOEHN, 2005</marker>
<rawString>KOEHN, Philipp (2005) “Europarl: A Parallel Corpus for Statistical Machine Translation” in MT Summit X, Phuket, Thailand, September 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George DODDINGTON</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality using n-gram Co-occurrence Statistics” in HLT-2002.</title>
<date>2002</date>
<location>San Diego, California.</location>
<marker>DODDINGTON, 2002</marker>
<rawString>DODDINGTON, George (2002) “Automatic Evaluation of Machine Translation Quality using n-gram Co-occurrence Statistics” in HLT-2002. San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp KOEHN</author>
<author>Hieu HOANG</author>
<author>Alexandra BIRCH</author>
<author>Chris CALLISONBURCH</author>
<author>Marcello FEDERICO</author>
<author>Nicola BERTOLDI</author>
<author>Brooke COWAN</author>
<author>Wade SHEN</author>
<author>Christine MORAN</author>
<author>Richard ZENS</author>
<author>Chris DYER</author>
<author>Ondrej BOJAR</author>
</authors>
<date>2007</date>
<booktitle>CONSTANTIN &amp; Evan HERBST (2007) “Moses: Open Source Toolkit for Statistical Machine Translation” in ACL’07,</booktitle>
<location>Alexandra</location>
<marker>KOEHN, HOANG, BIRCH, CALLISONBURCH, FEDERICO, BERTOLDI, COWAN, SHEN, MORAN, ZENS, DYER, BOJAR, 2007</marker>
<rawString>KOEHN, Philipp, Hieu HOANG, Alexandra BIRCH, Chris CALLISONBURCH, Marcello FEDERICO, Nicola BERTOLDI, Brooke COWAN, Wade SHEN, Christine MORAN, Richard ZENS, Chris DYER, Ondrej BOJAR, Alexandra CONSTANTIN &amp; Evan HERBST (2007) “Moses: Open Source Toolkit for Statistical Machine Translation” in ACL’07, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon LAVIE</author>
<author>Abhaya AGARWAL</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgment” in WSMT.</title>
<date>2007</date>
<location>Prague, Czech Republic,</location>
<marker>LAVIE, AGARWAL, 2007</marker>
<rawString>LAVIE, Alon &amp; Abhaya AGARWAL (2007) “METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgment” in WSMT. Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M LEWIS</author>
<author>Richard E STEARNS</author>
</authors>
<title>Syntax-directed transduction”</title>
<date>1968</date>
<journal>in Journal of the ACM</journal>
<volume>15</volume>
<pages>465--488</pages>
<marker>LEWIS, STEARNS, 1968</marker>
<rawString>LEWIS, Philip M. &amp; Richard E. STEARNS. (1968) “Syntax-directed transduction” in Journal of the ACM 15: 465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef OCH</author>
<author>Hermann NEY</author>
</authors>
<title>Improved Statistical Alignment Models”</title>
<date>2000</date>
<booktitle>in ACL-2000,</booktitle>
<pages>440--447</pages>
<location>Hong Kong,</location>
<marker>OCH, NEY, 2000</marker>
<rawString>OCH, Franz Josef &amp; Hermann NEY (2000) “Improved Statistical Alignment Models” in ACL-2000, pp. 440–447, Hong Kong, October 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef OCH</author>
</authors>
<title>Minimum error rate training in statistical machine translation”</title>
<date>2003</date>
<note>in ACL’03.</note>
<marker>OCH, 2003</marker>
<rawString>OCH, Franz Josef (2003) “Minimum error rate training in statistical machine translation” in ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef OCH</author>
<author>Hermann NEY</author>
</authors>
<date>2003</date>
<journal>A Systematic Comparison of Various Statistical Alignment Models” in Computational Linguistics</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pp.</pages>
<marker>OCH, NEY, 2003</marker>
<rawString>OCH, Franz Josef &amp; Hermann NEY (2003) “A Systematic Comparison of Various Statistical Alignment Models” in Computational Linguistics 29(1), pp. 19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore PAPINENI</author>
<author>Salim ROUKOS</author>
<author>Todd WARD</author>
<author>Wei-Jing ZHU</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of</title>
<date>2002</date>
<booktitle>Machine Translation” in ACL’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<marker>PAPINENI, ROUKOS, WARD, ZHU, 2002</marker>
<rawString>PAPINENI, Kishore, Salim ROUKOS, Todd WARD &amp; Wei-Jing ZHU (2002) “BLEU: a Method for Automatic Evaluation of Machine Translation” in ACL’02, pp 311-318. Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A SçNCHEZ</author>
<author>J M</author>
</authors>
<title>BENEDê (2006) “Stochastic Inversion Transduction Grammars for Obtaining Word Phrases for Phrase-based Statistical</title>
<date>2006</date>
<booktitle>Machine Translation” in WSMT,</booktitle>
<pages>130--133</pages>
<location>New York City,</location>
<marker>SçNCHEZ, M, 2006</marker>
<rawString>SçNCHEZ, J. A., J.M. BENEDê (2006) “Stochastic Inversion Transduction Grammars for Obtaining Word Phrases for Phrase-based Statistical Machine Translation” in WSMT, pp. 130–133, New York City, June 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil SIMA’AN</author>
<author>Markos MYLONAKIS</author>
</authors>
<title>Better Statistical Estimation Can Benefit all Phrases</title>
<date>2008</date>
<booktitle>in Phrase-based Statistical Machine Translation” in SLT</booktitle>
<pages>237--240</pages>
<location>Goa, India,</location>
<marker>SIMA’AN, MYLONAKIS, 2008</marker>
<rawString>SIMA’AN, Khalil &amp; Markos MYLONAKIS (2008) “Better Statistical Estimation Can Benefit all Phrases in Phrase-based Statistical Machine Translation” in SLT 2008, pp. 237–240, Goa, India, December 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan VOGEL</author>
<author>Hermann NEY</author>
<author>Christoph TILLMANN</author>
</authors>
<title>HMM-based Word Alignment</title>
<date>1996</date>
<booktitle>in Statistical Translation” in COLING’96,</booktitle>
<pages>836--841</pages>
<marker>VOGEL, NEY, TILLMANN, 1996</marker>
<rawString>VOGEL, Stephan, Hermann NEY &amp; Christoph TILLMANN (1996) “HMM-based Word Alignment in Statistical Translation” in COLING’96, pp. 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai WU</author>
</authors>
<title>An Algorithm for Simultaneously Bracketing Parallel Texts by Aligning Words”</title>
<date>1995</date>
<booktitle>in ACL’95,</booktitle>
<pages>244--251</pages>
<location>Cambridge, Massachusetts,</location>
<marker>WU, 1995</marker>
<rawString>WU, Dekai (1995a) “An Algorithm for Simultaneously Bracketing Parallel Texts by Aligning Words” in ACL’95, pp. 244–251, Cambridge, Massachusetts, June 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai WU</author>
</authors>
<title>Trainable Coarse Bilingual Grammars for Parallel Text Bracketing”</title>
<date>1995</date>
<booktitle>in WVLC-3,</booktitle>
<pages>69--82</pages>
<location>Cambridge, Massachusetts,</location>
<marker>WU, 1995</marker>
<rawString>WU, Dekai (1995b) “Trainable Coarse Bilingual Grammars for Parallel Text Bracketing” in WVLC-3, pp. 69–82, Cambridge, Massachusetts, June 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai WU</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation” in ACL-96,</title>
<date>1996</date>
<location>Santa Cruz, CA:</location>
<marker>WU, 1996</marker>
<rawString>WU, Dekai (1996) “A polynomial-time algorithm for statistical machine translation” in ACL-96, Santa Cruz, CA: June 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai WU</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing</title>
<date>1997</date>
<journal>of Parallel Corpora” in Computational Linguistics</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<marker>WU, 1997</marker>
<rawString>WU, Dekai (1997) “Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora” in Computational Linguistics 23(3), pp 377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai WU</author>
<author>Hongsing WONG</author>
</authors>
<title>Machine Translation with a Stochastic Grammatical Channel”</title>
<date>1998</date>
<booktitle>in COLING-ACL&apos;98,</booktitle>
<location>Montreal,</location>
<marker>WU, WONG, 1998</marker>
<rawString>WU, Dekai &amp; Hongsing WONG (1998) “Machine Translation with a Stochastic Grammatical Channel” in COLING-ACL&apos;98, Montreal, August 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao ZHANG</author>
<author>Daniel GILDEA</author>
</authors>
<title>Stochastic Lexicalized Inversion Transduction Grammar for Alignment”</title>
<date>2005</date>
<booktitle>in ACL’05,</booktitle>
<pages>475--482</pages>
<location>Ann Arbor,</location>
<marker>ZHANG, GILDEA, 2005</marker>
<rawString>ZHANG, Hao &amp; Daniel GILDEA (2005) “Stochastic Lexicalized Inversion Transduction Grammar for Alignment” in ACL’05, pp. 475– 482, Ann Arbor, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao ZHANG</author>
<author>Liang HUANG</author>
<author>Dan GILDEA</author>
<author>Kevin KNIGHT</author>
</authors>
<date>2006</date>
<booktitle>Synchronous Binarization for Machine Translation” in HLT/NAACL-2006,</booktitle>
<pages>256--263</pages>
<location>New York,</location>
<marker>ZHANG, HUANG, GILDEA, KNIGHT, 2006</marker>
<rawString>ZHANG, Hao, Liang HUANG, Dan GILDEA &amp; Kevin KNIGHT (2006) “Synchronous Binarization for Machine Translation” in HLT/NAACL-2006, pp. 256–263, New York, June 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing ZHAO</author>
<author>Stephan VOGEL</author>
</authors>
<title>Word Alignment Based on Bilingual Bracketing”</title>
<date>2003</date>
<booktitle>in HLT-NAACL Workshop: Building and Using Parallel Texts,</booktitle>
<pages>15--18</pages>
<location>Edmonton, May–June</location>
<marker>ZHAO, VOGEL, 2003</marker>
<rawString>ZHAO, Bing &amp; Stephan VOGEL (2003) “Word Alignment Based on Bilingual Bracketing” in HLT-NAACL Workshop: Building and Using Parallel Texts, pp. 15–18, Edmonton, May–June 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>