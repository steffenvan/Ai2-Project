<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044525">
<title confidence="0.972986">
USAAR-WLV: Hypernym Generation with Deep Neural Nets
</title>
<author confidence="0.988496">
Liling Tan, Rohit Gupta&amp;quot; and Josef van Genabitha
</author>
<affiliation confidence="0.982742">
Universit¨at des Saarlandes / Campus A2.2, Saarbr¨ucken, Germany
University of Wolverhampton&amp;quot; / Wulfruna Street, Wolverhampton, UK
</affiliation>
<address confidence="0.696638">
Deutsches Forschungszentrum f¨ur Knstliche Intelligenza /
Stuhlsatzenhausweg, Saarbr¨ucken, Germany
</address>
<email confidence="0.965735">
alvations@gmail.com, r.gupta@wlv.ac.uk,
josef.van genabith@dfki.de
</email>
<sectionHeader confidence="0.995284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901230769231">
This paper describes the USAAR-WLV tax-
onomy induction system that participated in
the Taxonomy Extraction Evaluation task of
SemEval-2015. We extend prior work on
using vector space word embedding models
for hypernym-hyponym extraction by simpli-
fying the means to extract a projection matrix
that transforms any hyponym to its hypernym.
This is done by making use of function words,
which are usually overlooked in vector space
approaches to NLP. Our system performs best
in the chemical domain and has achieved com-
petitive results in the overall evaluations.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997508">
Traditionally, broad-coverage semantic taxonomies
such as CYC (Lenat, 1995) and WordNet ontol-
ogy (Miller, 1995) have been manually created with
much effort and yet they suffer from coverage spar-
sity. This motivated the move towards unsupervised
approaches to extract structured relational knowl-
edge from texts (Lin and Pantel, 2001; Snow et al.,
2006; Velardi et al., 2013).1
Previous work in taxonomy extraction focused on
rule-based, clustering and graph-based approaches.
Although vector space approaches are popular in
current NLP researches, ontology induction studies
have yet to catch on the frenzy. Fu et al. (2014)
proposed a vector space approach to hypernym-
hyponym identification using word embeddings that
</bodyText>
<footnote confidence="0.902839666666667">
1For the rest of the paper, taxonomy and ontology will be
used interchangeably to refer to a hierarchically structure that
organizes a list of concepts.
</footnote>
<bodyText confidence="0.998974103448276">
trains a projection matrix2 that converts a hyponym
vector to its hypernym. However, their approach
requires an existing hypernym-hyponym pairs for
training before discovering new pairs.
Our system submitted to the SemEval-2015 tax-
onomy building task is most similar to the approach
by Fu et al. (2014) in using word embeddings pro-
jections to identify hypernym-hyponym pairs. As
opposed to previous method our method does not re-
quires prior taxonomical knowledge.
Instead of training a projection matrix, we capi-
talize on the fact that hypernym-hyponym pair often
occurs in a sentence with an ‘is a’ phrase, e.g. “The
goldfish (Carassius auratus auratus) is a freshwa-
ter fish”.3 Intuitively, if we single-tokenize the ‘is
a’ phrase prior to training a vector space, we can
make use of the vector that represents the phrase
in capturing a hypernym-hyponym pair as such
the multiplication of v(goldfish) and v(is-a)
will be similar to the cross product v(fish)
(v(goldfish)×v(is-a) ≈ v(fish)).
There is little or no previous work that manipu-
lates non-content word vectors in vector space mod-
els studies for natural language processing. Of-
ten, non-content words4 were implicitly incorpo-
rated into the vector space models by means of syn-
tactic frames (Sarmento et al., 2009) or syntactic
parses (Thater et al., 2010).
Our main contribution for ontological induction
</bodyText>
<footnote confidence="0.9937674">
2In this case, the projection matrix is a vector space feature
function.
3From http://en.wikipedia.org/wiki/Goldfish.
4Words that are not noun (entities/arguments), verbs (predi-
cates), adjectives or adverbs (adjuncts).
</footnote>
<page confidence="0.939448">
932
</page>
<bodyText confidence="0.848146125">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 932–937,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
using vector space models are primarily (i) the use
of non-content word vectors and (ii) simplifying a
previously complex process of learning a hypernym-
hyponym transition matrix. The implementation of
our ontological induction approach is open-sourced
and available on our GitHub repository.5
</bodyText>
<subsectionHeader confidence="0.999186">
1.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.99958896">
Similar to Fountain and Lapata (2012), the
SemEval-2015 Taxonomy Extraction Evaluation
(TaxEval) task addresses taxonomy learning without
the term discovery step, i.e. the terms for which to
create the taxonomy are given (Bordea et al., 2015).
The focus is on creating the hypernym-hyponym re-
lations.
In the TaxEval task, taxonomies are evaluated
through comparison with gold standard taxonomies.
There is no training corpus provided by the organ-
isers of the task and the participating systems are to
generate hyper-hyponyms pairs using a list of terms
from four different domains, viz. chemicals, equip-
ment, food and science.
The gold standards used in evaluation are the
ChEBI ontology for the chemical domain (Degt-
yarenko et al., 2008), the Material Handling Equip-
ment taxonomy6 for the equipment domain, the
Google product taxonomy7 for the food domain
and the Taxonomy of Fields and their Different
Sub-fields8 for the science domain. In addition,
all four domains are also evaluated against the
sub-hierarchies from the WordNet ontology that
subsumes the Suggested Upper Merged Ontology
(Pease et al., 2002).
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998745">
There are a variety of methods used in taxonomy
induction. They can be broadly categorized as (i)
pattern/rule based, (ii) clustering based, (iii) graph
based and (iv) vector space approaches.
</bodyText>
<footnote confidence="0.9989025">
5https://github.com/alvations/USAAR-SemEval-
2015/tree/master/task17-USAAR-WLV
6http://www.ise.ncsu.edu/kay/mhetax/index.htm
7http://www.google.com/basepages/producttype/taxonomy.en-
US.txt
8http://sites.nationalacademies.org/PGA/Resdoc/PGA 044522
</footnote>
<subsectionHeader confidence="0.998994">
2.1 Pattern/Rule Based Approaches
</subsectionHeader>
<bodyText confidence="0.9999705">
Hearst (1992) first introduced ontology learning by
exploiting lexico-syntactic patterns that explicitly
links a hypernym to its hyponym, e.g. “X and other
Ys” and “Ys such as X”. These patterns could be
manually constructed (Berland and Charniak, 1999;
Kozareva et al., 2008) or automatically bootstrapped
(Girju, 2003).
These methods rely on surface-level patterns and
incorrect items are frequently extracted because of
parsing errors, polysemy, idiomatic expressions, etc.
</bodyText>
<subsectionHeader confidence="0.999874">
2.2 Clustering Approaches
</subsectionHeader>
<bodyText confidence="0.999975214285714">
Clustering based approaches are mostly used to dis-
cover hypernym (is-a) and synonym (is-like) rela-
tions. For instance, to induce synonyms, Lin (1998)
clustered words based on the amount of informa-
tion needed to state the commonality between two
words.9
Contrary to most bottom-up clustering ap-
proaches for taxonomy induction (Caraballo, 2001;
Lin, 1998), Pantel and Ravichandran (2004) intro-
duced a top-down approach, assigning the hyper-
nyms to clusters using co-occurrence statistics and
then pruning the cluster by recalculating the pair-
wise similarity between every hyponym pair within
the cluster.
</bodyText>
<subsectionHeader confidence="0.993663">
2.3 Graph-based Approaches
</subsectionHeader>
<bodyText confidence="0.999964133333333">
In graph theory (Biggs et al., 1976), similar ideas are
conceived with a different jargon. In graph notation,
nodes/vertices form the atom units of the graph and
nodes are connected by directed edges. A graph, un-
like an ontology, regards the hierarchical structure of
a taxonomy as a by-product of the individual pairs of
nodes connected by a directed edges. In this regard,
a single root node is not guaranteed and to produce
a tree-like structure.
Disregarding the overall hierarchical structure,
the crux of graph induction focuses on the differ-
ent techniques of edge weighting between individ-
ual node pairs and graph pruning or edge collaps-
ing (Kozareva and Hovy, 2010; Navigli et al., 2011;
Fountain and Lapata, 2012; Tuan et al., 2014).
</bodyText>
<footnote confidence="0.922935">
9Commonly known as Lin information content measure.
</footnote>
<page confidence="0.995934">
933
</page>
<subsectionHeader confidence="0.988277">
2.4 Vector Space Approaches
</subsectionHeader>
<bodyText confidence="0.9999064375">
Semantic knowledge can be thought of as a two-
dimensional vector space where each word is rep-
resented as a point and semantic association is in-
dicated by word proximity. The vector space repre-
sentation for each word is constructed from the dis-
tribution of words across context, such that words
with similar meaning are found close to each other
in the space (Mitchell and Lapata, 2010; Tan, 2013).
Although vector space models have been used
widely in other NLP tasks, ontology/taxonomy in-
ducing using vector space models has not been pop-
ular. It is only since the recent advancement in
neural nets and word embeddings that vector space
models are gaining ground for ontology induction
and relation extraction (Saxe et al., 2013; Khashabi,
2013).
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99980975">
This section provides a brief overview of our sys-
tem’s approach to taxonomy induction. The full sys-
tem is released as open-source and contains docu-
mentation with additional implementation details.10
</bodyText>
<subsectionHeader confidence="0.9892245">
3.1 Projecting a Hyponym to its Hypernym
with Transition Matrix
</subsectionHeader>
<bodyText confidence="0.955190956521739">
Fu et al. (2014) discovered that hypernym-
hyponyms pairs have similar semantic properties as
the linguistics regularities discussed in Mikolov et
al. (2013b). For instance: v(shrimp)-v(prawn)
≈ v(fish)-v(goldfish).
Intuitively, the assumption is that all words can be
projected to their hypernyms based on a transition
matrix. That is, given a word x and its hypernym
y, a transition matrix (D exists such that y = (Dx, e.g.
v(goldfish) = (D×v(fish).
Fu et al. proposed two projection approaches to
identify hypernym-hyponym pairs, (i) uniform lin-
ear projection where (D is the same for all words and
(D is learnt by minimizing the mean squared error of
k(Dx-yk across all word-pairs (i.e. a domain inde-
pendent (D) and (ii) piecewise linear projection that
learns a separate projection for different word clus-
ters (i.e. a domain dependent (D, where a taxonomy’s
domain is bounded by its terms’ cluster(s)). In both
10https://github.com/alvations/USAAR-SemEval-
2015/blob/master/task17-USAAR-WLV/README.md
projections, hypernym-hyponym pairs are required
to train the transition matrix (D.
</bodyText>
<subsectionHeader confidence="0.999754">
3.2 Inducing a Hypernym with is-a Vector
</subsectionHeader>
<bodyText confidence="0.999939529411765">
Instead of learning a supervised transition matrix (D,
we propose a simpler unsupervised approach where
we learn a vector for the phrase “is-a”. We single-
tokenize the adjacent “is” and “a” tokens and learn
the word embeddings with is-a forming part of the
vocabulary in the input matrix.
Effectively, we hypothesize that (D can be re-
placed by the “is-a” vector. To achieve the piece-
wise projection effects of (D, we trained a differ-
ent deep neural net model for each TaxEval do-
main and assume that the “is-a” scales automati-
cally across domains. For instance, the multiplica-
tion of the v(tiramisu) and the v(is-afood)
vectors yields a proxy vector and we consider
the top ten word vectors that are most similar to
this proxy vector as the possible hypernyms, i.e.
v(tiramisu)×v(is-afood) ≈ v(cake).
</bodyText>
<sectionHeader confidence="0.998655" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.996875">
4.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999984166666667">
There is no specified training corpus released for the
SemEval-2015 TaxEval task. To produce a domain
specific corpus for each of the given domains in the
task, we used the Wikipedia dump and preprocessed
it using WikiExtractor11 and then extracted docu-
ments that contain the terms for each domain indi-
vidually.
We trained a skip-gram model phrasal word2vec
neural net (Mikolov et al., 2013a) using gensim
(ˇReh˚uˇrek and Sojka, 2010). The neural nets were
trained for 100 epochs with a window size of 5 for
all words in the corpus.12
</bodyText>
<subsectionHeader confidence="0.987532">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9912875">
For the TaxEval task, the multi-faceted evaluation
scheme presented in Navigli (2013) was adopted
to compare the overall structure of the taxonomy
against a gold standard, with an approach used for
comparing hierarchical clusters. The multi-faceted
11We use the same Wikipedia dump to text extraction process
from the SeedLing - Human Language Project (Emerson et al.,
2014).
12i.e. words with minimum count of 1; other parameters set
for the neural nets can be found on our GitHub repository.
</bodyText>
<page confidence="0.996309">
934
</page>
<table confidence="0.999626333333333">
|V ||E |#c.c cycles #VC %VC #EC %EC :NE
Chemical 13785 30392 302 YES 13784 0.7838 2427 0.0977 1.1268
Equipment 337 548 28 YES 336 0.549 227 0.3691 0.5219
Food 1118 2692 23 YES 948 0.6092 428 0.2696 1.4265
Science 355 952 14 YES 354 0.7831 173 0.3720 1.6752
WN Chemical 1173 3107 31 YES 1172 0.8675 532 0.3835 1.8566
WN Equipment 354 547 43 YES 353 0.7431 149 0.3072 0.8206
WN Food 1200 3465 23 YES 1199 0.8068 549 0.3581 1.9021
WN Science 307 892 8 YES 306 0.7132 156 0.3537 1.6689
</table>
<tableCaption confidence="0.987957">
Table 1: Structural Measures and Comparison against Gold Standards for USAAR-WLV. The labels of the columns
refer to no. of distinct vertices and edges in induced taxonomy (|V |and |E|), no. of connected components (#c.c),
whether the taxonomy is a Directed Acyclic Graph (cycles), vertex and edge coverage, i.e. proportion of gold standard
vertices and edges covered by system (%VC and %EC), no. of vertices and edges in common with gold standard
(#VC and #EC) and ratio of novel edges (:NE).
</tableCaption>
<table confidence="0.999749">
INRIASAC LT3 NTNU QASSIT TALN-UPF USAAR-WLV
Avg. F&amp;M 0.3270 0.4130 0.0580 0.3880 0.2630 0.0770
Avg. Precision 0.1721 0.3612 0.1754 0.1563 0.0720 0.2014
Avg. Recall 0.4279 0.6307 0.2756 0.1588 0.1165 0.3139
Avg. F-Score 0.2427 0.3886 0.2075 0.1575 0.0798 0.2377
Avg. Precision of NE 0.4800 0.5960 0.3530 0.2470 0.1020 0.4200
</table>
<tableCaption confidence="0.9921785">
Table 2: Averaged F&amp;M Measure, Precision, Recall, F-score for All Systems Outputs when Compared to Gold Stan-
dard and Manually Evaluated Average Precision of Novel Edges.
</tableCaption>
<bodyText confidence="0.999804266666667">
evaluation scheme evaluates (i) the structural mea-
sures of the induced taxonomy (left columns of Ta-
ble 1), (ii) the comparison against gold standard tax-
onomy (right columns of Table 1 and leftmost col-
umn of Table 2) and (iii) manual evaluation of novel
edges precision (last row of Table 2).
Regarding the two types of automatic evaluation
measures, the structural measures provides a gauge
of the system’s coverage and the ontology struc-
tural integrity, i.e. “tree-likeness” of the ontology
produced by the hypernym-hyponym pairs, and the
comparison against the gold standards gives an ob-
jective measure of the “human-likeness” of the sys-
tem in producing a taxonomy that is similar to the
manually-crafted taxonomy.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999890933333333">
Table 1 presents the evaluation scores for our system
in the TaxEval task, the %VC and %EC scores sum-
marize the performance of the system in replicating
the gold standard taxonomies.
In terms of vertex coverage, our system performs
best in the chemical and WordNet chemical domain.
Regarding edge coverage, our system achieves high-
est coverage for the science domain and WordNet
chemical domain. Having high edge and vertex cov-
erage significantly lowers false positive rate when
evaluating hypernym-hyponyms pairs with preci-
sion, recall and F-score.
We also note that the wikipedia corpus extracted
that we used to induce the vectors lacks coverage
for the food domain. In the other domains, we dis-
covered all terms in the wikipedia corpus plus the
domains’ root hypernym (i.e. |V |= #VC + 1).
Table 2 presents the comparative results between
the participating teams in the TaxEval task aver-
aged over all domains. We performed reasonable
well as compared to the other systems in all mea-
sures. While our system’s F&amp;M measure is low, it is
only representative of the clusters we have induced
as compared to the gold standard. To improve our
F&amp;M measure, we could reduce the number of re-
dundant novel edges by pruning our system outputs
and achieve comparable results to the other teams
given our relatively precision of novel edges.
A detailed evaluation on the results for the indi-
vidual domains is presented on Bordea et al. (2015).
</bodyText>
<page confidence="0.998273">
935
</page>
<sectionHeader confidence="0.997127" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999927307692308">
In this paper, we have described our submissions to
the Taxonomy Evaluation task for SemEval-2015.
We have simplified a previously complex process
of inducing a hypernym-hyponym ontology from a
neural net by using the word vector for the non-
content word text pattern, ”is a”.
Our system achieved modest results when com-
pared against other participating teams. Given the
simple approach to hypernym-hyponym relations, it
is possible that future research can apply the method
to other non-content words vectors to induce other
relations between entities. The implementation of
our system is released as open-source.
</bodyText>
<sectionHeader confidence="0.989382" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999806428571429">
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA grant
agreement no 317471. We would like to thank the
Daniel Cer and other anonymous reviewers for their
helpful suggestions and comments.
</bodyText>
<sectionHeader confidence="0.992987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9982816">
Matthew Berland and Eugene Charniak. 1999. Finding
Parts in Very Large Corpora. In Proceedings of the
37th annual meeting of the Association for Computa-
tional Linguistics on Computational Linguistics, pages
57–64.
Norman Biggs, E. Keith Lloyd, and Robin J. Wilson.
1976. Graph theory 1736-1936. Clarendon Press.
Georgeta Bordea, Paul Buitelaar, Stefano Faralli, and
Roberto Navigli. 2015. Semeval-2015 task 17: Tax-
onomy Extraction Evaluation. In Proceedings of the
9th International Workshop on Semantic Evaluation.
Sharon Ann Caraballo. 2001. Automatic Construction of
a Hypernym-labeled Noun Hierarchy from Text. Ph.D.
thesis, Providence, RI, USA. AAI3006696.
Kirill Degtyarenko, Paula De Matos, Marcus Ennis,
Janna Hastings, Martin Zbinden, Alan Mcnaught,
Rafael Alc´antara, Michael Darsow, Micka¨el Guedj,
and Michael Ashburner. 2008. ChEBI: A Database
and Ontology for Chemical Entities of Biological In-
terest. Nucleic acids research, 36(suppl 1):D344–
D350.
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis
Palmer, and Michaela Regneri. 2014. SeedLing:
Building and Using a Seed corpus for the Human Lan-
guage Project. In Proceedings of the 2014 Workshop
on the Use of Computational Methods in the Study of
Endangered Languages, pages 77–85.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy
Induction using Hierarchical Random Graphs. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 466–
476.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2014. Learning Semantic Hier-
archies via Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1199–1209.
Roxana Girju. 2003. Automatic Detection of Causal Re-
lations for Question Answering. In Proceedings of the
ACL 2003 workshop on Multilingual summarization
and question answering-Volume 12, pages 76–83.
Marti A Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings
of the 14th conference on Computational linguistics-
Volume 2, pages 539–545.
Daniel Khashabi. 2013. On the Recursive Neural Net-
works for Relation Extraction and Entity Recognition.
Technical report.
Zornitsa Kozareva and Eduard Hovy. 2010. A Semi-
Supervised Method to Learn and Construct Tax-
onomies using the Web. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1110–1118.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic Class Learning from the Web with Hyponym
Pattern Linkage Graphs. In Proceedings of ACL-08:
HLT, pages 1048–1056, Columbus, Ohio, June.
Douglas B Lenat. 1995. CYC: A Large-Scale Investment
in Knowledge Infrastructure. Communications of the
ACM, 38(11):33–38.
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question-Answering. Natural Lan-
guage Engineering, 7(04):343–360.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 17th in-
ternational conference on Computational linguistics-
Volume 2, pages 768–774.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013
</reference>
<page confidence="0.993386">
936
</page>
<reference confidence="0.999388360655738">
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 746–751.
George A Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39–
41.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive Sci-
ence, 34(8):1388–1439.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A Graph-Based Algorithm for Inducing Lexical
Taxonomies from Scratch. In IJCAI 2011, Proceed-
ings of the 22nd International Joint Conference on Ar-
tificial Intelligence, Barcelona, Catalonia, Spain, July
16-22, 2011, pages 1872–1877.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. In Proceedings
of the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004.
Adam Pease, Ian Niles, and John Li. 2002. The Sug-
gested Upper Merged Ontology: A Large Ontology
for the Semantic Web and its Applications. In Working
Notes of the AAAI-2002 Workshop on Ontologies and
the SemanticWeb, Edmonton, Canada.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May. ELRA.
Lu´ıs Sarmento, Paula Carvalho, and Eug´enio Oliveira.
2009. Exploring the Vector Space Model for Find-
ing Verb Synonyms in Portuguese. In Proceedings
of the International Conference RANLP-2009, pages
393–398.
Andrew M. Saxe, James L. McClelland, and Surya Gan-
guli. 2013. Learning Hierarchical Category Structure
in Deep Neural Networks. pages 1271–1276.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic Taxonomy Induction from Heterogenous
Evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 801–808.
Liling Tan. 2013. Examining crosslingual word sense
disambiguation. Master’s thesis, Nanyang Technolog-
ical University. pages 17-21.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing Semantic Representations us-
ing Syntactically Enriched Vector Models. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 948–957.
Luu Anh Tuan, Jung-jae Kim, and Kiong See Ng. 2014.
Taxonomy construction using syntactic contextual ev-
idence. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 810–819.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A Graph-based Algo-
rithm for Taxonomy Induction. Computational Lin-
guistics, 39(3):665–707.
</reference>
<page confidence="0.997578">
937
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.095355">
<title confidence="0.998685">USAAR-WLV: Hypernym Generation with Deep Neural Nets</title>
<author confidence="0.996069">Rohit Josef van Tan</author>
<note confidence="0.39637975">Universit¨at des Saarlandes / Campus A2.2, Saarbr¨ucken, of Wulfruna Street, Wolverhampton, Forschungszentrum f¨ur Knstliche Stuhlsatzenhausweg, Saarbr¨ucken,</note>
<email confidence="0.471047">alvations@gmail.com,josef.vangenabith@dfki.de</email>
<abstract confidence="0.999005857142857">This paper describes the USAAR-WLV taxonomy induction system that participated in the Taxonomy Extraction Evaluation task of SemEval-2015. We extend prior work on using vector space word embedding models for hypernym-hyponym extraction by simplifying the means to extract a projection matrix that transforms any hyponym to its hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding Parts in Very Large Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="5789" citStr="Berland and Charniak, 1999" startWordPosition="826" endWordPosition="829"> pattern/rule based, (ii) clustering based, (iii) graph based and (iv) vector space approaches. 5https://github.com/alvations/USAAR-SemEval2015/tree/master/task17-USAAR-WLV 6http://www.ise.ncsu.edu/kay/mhetax/index.htm 7http://www.google.com/basepages/producttype/taxonomy.enUS.txt 8http://sites.nationalacademies.org/PGA/Resdoc/PGA 044522 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding Parts in Very Large Corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman Biggs</author>
<author>E Keith Lloyd</author>
<author>Robin J Wilson</author>
</authors>
<title>Graph theory 1736-1936.</title>
<date>1976</date>
<publisher>Clarendon Press.</publisher>
<contexts>
<context position="6705" citStr="Biggs et al., 1976" startWordPosition="958" endWordPosition="961"> discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach, assigning the hypernyms to clusters using co-occurrence statistics and then pruning the cluster by recalculating the pairwise similarity between every hyponym pair within the cluster. 2.3 Graph-based Approaches In graph theory (Biggs et al., 1976), similar ideas are conceived with a different jargon. In graph notation, nodes/vertices form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of a taxonomy as a by-product of the individual pairs of nodes connected by a directed edges. In this regard, a single root node is not guaranteed and to produce a tree-like structure. Disregarding the overall hierarchical structure, the crux of graph induction focuses on the different techniques of edge weighting between individual node pairs and graph pruning or edge</context>
</contexts>
<marker>Biggs, Lloyd, Wilson, 1976</marker>
<rawString>Norman Biggs, E. Keith Lloyd, and Robin J. Wilson. 1976. Graph theory 1736-1936. Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgeta Bordea</author>
<author>Paul Buitelaar</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2015 task 17: Taxonomy Extraction Evaluation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="4178" citStr="Bordea et al., 2015" startWordPosition="608" endWordPosition="611">June 4-5, 2015. c�2015 Association for Computational Linguistics using vector space models are primarily (i) the use of non-content word vectors and (ii) simplifying a previously complex process of learning a hypernymhyponym transition matrix. The implementation of our ontological induction approach is open-sourced and available on our GitHub repository.5 1.1 Task Definition Similar to Fountain and Lapata (2012), the SemEval-2015 Taxonomy Extraction Evaluation (TaxEval) task addresses taxonomy learning without the term discovery step, i.e. the terms for which to create the taxonomy are given (Bordea et al., 2015). The focus is on creating the hypernym-hyponym relations. In the TaxEval task, taxonomies are evaluated through comparison with gold standard taxonomies. There is no training corpus provided by the organisers of the task and the participating systems are to generate hyper-hyponyms pairs using a list of terms from four different domains, viz. chemicals, equipment, food and science. The gold standards used in evaluation are the ChEBI ontology for the chemical domain (Degtyarenko et al., 2008), the Material Handling Equipment taxonomy6 for the equipment domain, the Google product taxonomy7 for t</context>
<context position="15168" citStr="Bordea et al. (2015)" startWordPosition="2337" endWordPosition="2340">the comparative results between the participating teams in the TaxEval task averaged over all domains. We performed reasonable well as compared to the other systems in all measures. While our system’s F&amp;M measure is low, it is only representative of the clusters we have induced as compared to the gold standard. To improve our F&amp;M measure, we could reduce the number of redundant novel edges by pruning our system outputs and achieve comparable results to the other teams given our relatively precision of novel edges. A detailed evaluation on the results for the individual domains is presented on Bordea et al. (2015). 935 6 Conclusion In this paper, we have described our submissions to the Taxonomy Evaluation task for SemEval-2015. We have simplified a previously complex process of inducing a hypernym-hyponym ontology from a neural net by using the word vector for the noncontent word text pattern, ”is a”. Our system achieved modest results when compared against other participating teams. Given the simple approach to hypernym-hyponym relations, it is possible that future research can apply the method to other non-content words vectors to induce other relations between entities. The implementation of our sy</context>
</contexts>
<marker>Bordea, Buitelaar, Faralli, Navigli, 2015</marker>
<rawString>Georgeta Bordea, Paul Buitelaar, Stefano Faralli, and Roberto Navigli. 2015. Semeval-2015 task 17: Taxonomy Extraction Evaluation. In Proceedings of the 9th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Ann Caraballo</author>
</authors>
<title>Automatic Construction of a Hypernym-labeled Noun Hierarchy from Text.</title>
<date>2001</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Providence, RI, USA.</location>
<contexts>
<context position="6381" citStr="Caraballo, 2001" startWordPosition="912" endWordPosition="913">land and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach, assigning the hypernyms to clusters using co-occurrence statistics and then pruning the cluster by recalculating the pairwise similarity between every hyponym pair within the cluster. 2.3 Graph-based Approaches In graph theory (Biggs et al., 1976), similar ideas are conceived with a different jargon. In graph notation, nodes/vertices form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of a taxonomy as a by-product of the individual</context>
</contexts>
<marker>Caraballo, 2001</marker>
<rawString>Sharon Ann Caraballo. 2001. Automatic Construction of a Hypernym-labeled Noun Hierarchy from Text. Ph.D. thesis, Providence, RI, USA. AAI3006696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirill Degtyarenko</author>
<author>Paula De Matos</author>
<author>Marcus Ennis</author>
<author>Janna Hastings</author>
<author>Martin Zbinden</author>
<author>Alan Mcnaught</author>
<author>Rafael Alc´antara</author>
<author>Michael Darsow</author>
<author>Micka¨el Guedj</author>
<author>Michael Ashburner</author>
</authors>
<title>ChEBI: A Database and Ontology for Chemical Entities of Biological Interest. Nucleic acids research, 36(suppl 1):D344– D350.</title>
<date>2008</date>
<marker>Degtyarenko, De Matos, Ennis, Hastings, Zbinden, Mcnaught, Alc´antara, Darsow, Micka¨el Guedj, Ashburner, 2008</marker>
<rawString>Kirill Degtyarenko, Paula De Matos, Marcus Ennis, Janna Hastings, Martin Zbinden, Alan Mcnaught, Rafael Alc´antara, Michael Darsow, Micka¨el Guedj, and Michael Ashburner. 2008. ChEBI: A Database and Ontology for Chemical Entities of Biological Interest. Nucleic acids research, 36(suppl 1):D344– D350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Emerson</author>
<author>Liling Tan</author>
<author>Susanne Fertmann</author>
<author>Alexis Palmer</author>
<author>Michaela Regneri</author>
</authors>
<title>SeedLing: Building and Using a Seed corpus for the Human Language Project.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages,</booktitle>
<pages>77--85</pages>
<contexts>
<context position="11416" citStr="Emerson et al., 2014" startWordPosition="1710" endWordPosition="1713"> We trained a skip-gram model phrasal word2vec neural net (Mikolov et al., 2013a) using gensim (ˇReh˚uˇrek and Sojka, 2010). The neural nets were trained for 100 epochs with a window size of 5 for all words in the corpus.12 4.2 Evaluation Metrics For the TaxEval task, the multi-faceted evaluation scheme presented in Navigli (2013) was adopted to compare the overall structure of the taxonomy against a gold standard, with an approach used for comparing hierarchical clusters. The multi-faceted 11We use the same Wikipedia dump to text extraction process from the SeedLing - Human Language Project (Emerson et al., 2014). 12i.e. words with minimum count of 1; other parameters set for the neural nets can be found on our GitHub repository. 934 |V ||E |#c.c cycles #VC %VC #EC %EC :NE Chemical 13785 30392 302 YES 13784 0.7838 2427 0.0977 1.1268 Equipment 337 548 28 YES 336 0.549 227 0.3691 0.5219 Food 1118 2692 23 YES 948 0.6092 428 0.2696 1.4265 Science 355 952 14 YES 354 0.7831 173 0.3720 1.6752 WN Chemical 1173 3107 31 YES 1172 0.8675 532 0.3835 1.8566 WN Equipment 354 547 43 YES 353 0.7431 149 0.3072 0.8206 WN Food 1200 3465 23 YES 1199 0.8068 549 0.3581 1.9021 WN Science 307 892 8 YES 306 0.7132 156 0.3537 1</context>
</contexts>
<marker>Emerson, Tan, Fertmann, Palmer, Regneri, 2014</marker>
<rawString>Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer, and Michaela Regneri. 2014. SeedLing: Building and Using a Seed corpus for the Human Language Project. In Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 77–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Fountain</author>
<author>Mirella Lapata</author>
</authors>
<title>Taxonomy Induction using Hierarchical Random Graphs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>466--476</pages>
<contexts>
<context position="3973" citStr="Fountain and Lapata (2012)" startWordPosition="578" endWordPosition="581">are not noun (entities/arguments), verbs (predicates), adjectives or adverbs (adjuncts). 932 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 932–937, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics using vector space models are primarily (i) the use of non-content word vectors and (ii) simplifying a previously complex process of learning a hypernymhyponym transition matrix. The implementation of our ontological induction approach is open-sourced and available on our GitHub repository.5 1.1 Task Definition Similar to Fountain and Lapata (2012), the SemEval-2015 Taxonomy Extraction Evaluation (TaxEval) task addresses taxonomy learning without the term discovery step, i.e. the terms for which to create the taxonomy are given (Bordea et al., 2015). The focus is on creating the hypernym-hyponym relations. In the TaxEval task, taxonomies are evaluated through comparison with gold standard taxonomies. There is no training corpus provided by the organisers of the task and the participating systems are to generate hyper-hyponyms pairs using a list of terms from four different domains, viz. chemicals, equipment, food and science. The gold s</context>
<context position="7390" citStr="Fountain and Lapata, 2012" startWordPosition="1070" endWordPosition="1073">aph notation, nodes/vertices form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of a taxonomy as a by-product of the individual pairs of nodes connected by a directed edges. In this regard, a single root node is not guaranteed and to produce a tree-like structure. Disregarding the overall hierarchical structure, the crux of graph induction focuses on the different techniques of edge weighting between individual node pairs and graph pruning or edge collapsing (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014). 9Commonly known as Lin information content measure. 933 2.4 Vector Space Approaches Semantic knowledge can be thought of as a twodimensional vector space where each word is represented as a point and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used widely in other NLP tasks, ontology/taxonomy inducing</context>
</contexts>
<marker>Fountain, Lapata, 2012</marker>
<rawString>Trevor Fountain and Mirella Lapata. 2012. Taxonomy Induction using Hierarchical Random Graphs. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 466– 476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiji Fu</author>
<author>Jiang Guo</author>
<author>Bing Qin</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning Semantic Hierarchies via Word Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1199--1209</pages>
<contexts>
<context position="1607" citStr="Fu et al. (2014)" startWordPosition="223" endWordPosition="226">onally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embeddings that 1For the rest of the paper, taxonomy and ontology will be used interchangeably to refer to a hierarchically structure that organizes a list of concepts. trains a projection matrix2 that converts a hyponym vector to its hypernym. However, their approach requires an existing hypernym-hyponym pairs for training before discovering new pairs. Our system submitted to the SemEval-2015 taxonomy building task is most similar to the approach by Fu et al. (2014) in using word embeddings projections to identify </context>
<context position="8535" citStr="Fu et al. (2014)" startWordPosition="1257" endWordPosition="1260">ls have been used widely in other NLP tasks, ontology/taxonomy inducing using vector space models has not been popular. It is only since the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym to its Hypernym with Transition Matrix Fu et al. (2014) discovered that hypernymhyponyms pairs have similar semantic properties as the linguistics regularities discussed in Mikolov et al. (2013b). For instance: v(shrimp)-v(prawn) ≈ v(fish)-v(goldfish). Intuitively, the assumption is that all words can be projected to their hypernyms based on a transition matrix. That is, given a word x and its hypernym y, a transition matrix (D exists such that y = (Dx, e.g. v(goldfish) = (D×v(fish). Fu et al. proposed two projection approaches to identify hypernym-hyponym pairs, (i) uniform linear projection where (D is the same for all words and (D is learnt by </context>
</contexts>
<marker>Fu, Guo, Qin, Che, Wang, Liu, 2014</marker>
<rawString>Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning Semantic Hierarchies via Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1199–1209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic Detection of Causal Relations for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering-Volume 12,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="5857" citStr="Girju, 2003" startWordPosition="837" endWordPosition="838">e approaches. 5https://github.com/alvations/USAAR-SemEval2015/tree/master/task17-USAAR-WLV 6http://www.ise.ncsu.edu/kay/mhetax/index.htm 7http://www.google.com/basepages/producttype/taxonomy.enUS.txt 8http://sites.nationalacademies.org/PGA/Resdoc/PGA 044522 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach,</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic Detection of Causal Relations for Question Answering. In Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering-Volume 12, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguisticsVolume 2,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="5550" citStr="Hearst (1992)" startWordPosition="792" endWordPosition="793">sub-hierarchies from the WordNet ontology that subsumes the Suggested Upper Merged Ontology (Pease et al., 2002). 2 Related Work There are a variety of methods used in taxonomy induction. They can be broadly categorized as (i) pattern/rule based, (ii) clustering based, (iii) graph based and (iv) vector space approaches. 5https://github.com/alvations/USAAR-SemEval2015/tree/master/task17-USAAR-WLV 6http://www.ise.ncsu.edu/kay/mhetax/index.htm 7http://www.google.com/basepages/producttype/taxonomy.enUS.txt 8http://sites.nationalacademies.org/PGA/Resdoc/PGA 044522 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For i</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of the 14th conference on Computational linguisticsVolume 2, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Khashabi</author>
</authors>
<title>On the Recursive Neural Networks for Relation Extraction and Entity Recognition.</title>
<date>2013</date>
<tech>Technical report.</tech>
<contexts>
<context position="8240" citStr="Khashabi, 2013" startWordPosition="1213" endWordPosition="1214">ciation is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used widely in other NLP tasks, ontology/taxonomy inducing using vector space models has not been popular. It is only since the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym to its Hypernym with Transition Matrix Fu et al. (2014) discovered that hypernymhyponyms pairs have similar semantic properties as the linguistics regularities discussed in Mikolov et al. (2013b). For instance: v(shrimp)-v(prawn) ≈ v(fish)-v(goldfish). Intuitively, the assumption is that all words can be projected to their hypernyms based on a transition mat</context>
</contexts>
<marker>Khashabi, 2013</marker>
<rawString>Daniel Khashabi. 2013. On the Recursive Neural Networks for Relation Extraction and Entity Recognition. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>A SemiSupervised Method to Learn and Construct Taxonomies using the Web.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1110--1118</pages>
<contexts>
<context position="7341" citStr="Kozareva and Hovy, 2010" startWordPosition="1062" endWordPosition="1065">as are conceived with a different jargon. In graph notation, nodes/vertices form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of a taxonomy as a by-product of the individual pairs of nodes connected by a directed edges. In this regard, a single root node is not guaranteed and to produce a tree-like structure. Disregarding the overall hierarchical structure, the crux of graph induction focuses on the different techniques of edge weighting between individual node pairs and graph pruning or edge collapsing (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014). 9Commonly known as Lin information content measure. 933 2.4 Vector Space Approaches Semantic knowledge can be thought of as a twodimensional vector space where each word is represented as a point and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used wide</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010. A SemiSupervised Method to Learn and Construct Taxonomies using the Web. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1110–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5813" citStr="Kozareva et al., 2008" startWordPosition="830" endWordPosition="833">ustering based, (iii) graph based and (iv) vector space approaches. 5https://github.com/alvations/USAAR-SemEval2015/tree/master/task17-USAAR-WLV 6http://www.ise.ncsu.edu/kay/mhetax/index.htm 7http://www.google.com/basepages/producttype/taxonomy.enUS.txt 8http://sites.nationalacademies.org/PGA/Resdoc/PGA 044522 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravicha</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proceedings of ACL-08: HLT, pages 1048–1056, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
</authors>
<title>CYC: A Large-Scale Investment in Knowledge Infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1059" citStr="Lenat, 1995" startWordPosition="141" endWordPosition="142">onomy induction system that participated in the Taxonomy Extraction Evaluation task of SemEval-2015. We extend prior work on using vector space word embedding models for hypernym-hyponym extraction by simplifying the means to extract a projection matrix that transforms any hyponym to its hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations. 1 Introduction Traditionally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Douglas B Lenat. 1995. CYC: A Large-Scale Investment in Knowledge Infrastructure. Communications of the ACM, 38(11):33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of Inference Rules for Question-Answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>04</issue>
<contexts>
<context position="1315" citStr="Lin and Pantel, 2001" startWordPosition="179" endWordPosition="182">atrix that transforms any hyponym to its hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations. 1 Introduction Traditionally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embeddings that 1For the rest of the paper, taxonomy and ontology will be used interchangeably to refer to a hierarchically structure that organizes a list of concepts. trains a projection matrix2 that converts a hyponym vector t</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of Inference Rules for Question-Answering. Natural Language Engineering, 7(04):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguisticsVolume 2,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="6189" citStr="Lin (1998)" startWordPosition="883" endWordPosition="884">arning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach, assigning the hypernyms to clusters using co-occurrence statistics and then pruning the cluster by recalculating the pairwise similarity between every hyponym pair within the cluster. 2.3 Graph-based Approaches In graph theory (Biggs et al., 1976), similar ideas are conceived with a different jargon. In graph notation, nodes/vert</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of the 17th international conference on Computational linguisticsVolume 2, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="8673" citStr="Mikolov et al. (2013" startWordPosition="1276" endWordPosition="1279">ce the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym to its Hypernym with Transition Matrix Fu et al. (2014) discovered that hypernymhyponyms pairs have similar semantic properties as the linguistics regularities discussed in Mikolov et al. (2013b). For instance: v(shrimp)-v(prawn) ≈ v(fish)-v(goldfish). Intuitively, the assumption is that all words can be projected to their hypernyms based on a transition matrix. That is, given a word x and its hypernym y, a transition matrix (D exists such that y = (Dx, e.g. v(goldfish) = (D×v(fish). Fu et al. proposed two projection approaches to identify hypernym-hyponym pairs, (i) uniform linear projection where (D is the same for all words and (D is learnt by minimizing the mean squared error of k(Dx-yk across all word-pairs (i.e. a domain independent (D) and (ii) piecewise linear projection tha</context>
<context position="10874" citStr="Mikolov et al., 2013" startWordPosition="1624" endWordPosition="1627">is-afood) vectors yields a proxy vector and we consider the top ten word vectors that are most similar to this proxy vector as the possible hypernyms, i.e. v(tiramisu)×v(is-afood) ≈ v(cake). 4 Experimental Setup 4.1 Training Data There is no specified training corpus released for the SemEval-2015 TaxEval task. To produce a domain specific corpus for each of the given domains in the task, we used the Wikipedia dump and preprocessed it using WikiExtractor11 and then extracted documents that contain the terms for each domain individually. We trained a skip-gram model phrasal word2vec neural net (Mikolov et al., 2013a) using gensim (ˇReh˚uˇrek and Sojka, 2010). The neural nets were trained for 100 epochs with a window size of 5 for all words in the corpus.12 4.2 Evaluation Metrics For the TaxEval task, the multi-faceted evaluation scheme presented in Navigli (2013) was adopted to compare the overall structure of the taxonomy against a gold standard, with an approach used for comparing hierarchical clusters. The multi-faceted 11We use the same Wikipedia dump to text extraction process from the SeedLing - Human Language Project (Emerson et al., 2014). 12i.e. words with minimum count of 1; other parameters s</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="8673" citStr="Mikolov et al. (2013" startWordPosition="1276" endWordPosition="1279">ce the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym to its Hypernym with Transition Matrix Fu et al. (2014) discovered that hypernymhyponyms pairs have similar semantic properties as the linguistics regularities discussed in Mikolov et al. (2013b). For instance: v(shrimp)-v(prawn) ≈ v(fish)-v(goldfish). Intuitively, the assumption is that all words can be projected to their hypernyms based on a transition matrix. That is, given a word x and its hypernym y, a transition matrix (D exists such that y = (Dx, e.g. v(goldfish) = (D×v(fish). Fu et al. proposed two projection approaches to identify hypernym-hyponym pairs, (i) uniform linear projection where (D is the same for all words and (D is learnt by minimizing the mean squared error of k(Dx-yk across all word-pairs (i.e. a domain independent (D) and (ii) piecewise linear projection tha</context>
<context position="10874" citStr="Mikolov et al., 2013" startWordPosition="1624" endWordPosition="1627">is-afood) vectors yields a proxy vector and we consider the top ten word vectors that are most similar to this proxy vector as the possible hypernyms, i.e. v(tiramisu)×v(is-afood) ≈ v(cake). 4 Experimental Setup 4.1 Training Data There is no specified training corpus released for the SemEval-2015 TaxEval task. To produce a domain specific corpus for each of the given domains in the task, we used the Wikipedia dump and preprocessed it using WikiExtractor11 and then extracted documents that contain the terms for each domain individually. We trained a skip-gram model phrasal word2vec neural net (Mikolov et al., 2013a) using gensim (ˇReh˚uˇrek and Sojka, 2010). The neural nets were trained for 100 epochs with a window size of 5 for all words in the corpus.12 4.2 Evaluation Metrics For the TaxEval task, the multi-faceted evaluation scheme presented in Navigli (2013) was adopted to compare the overall structure of the taxonomy against a gold standard, with an approach used for comparing hierarchical clusters. The multi-faceted 11We use the same Wikipedia dump to text extraction process from the SeedLing - Human Language Project (Emerson et al., 2014). 12i.e. words with minimum count of 1; other parameters s</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>41</pages>
<contexts>
<context position="1095" citStr="Miller, 1995" startWordPosition="147" endWordPosition="148">pated in the Taxonomy Extraction Evaluation task of SemEval-2015. We extend prior work on using vector space word embedding models for hypernym-hyponym extraction by simplifying the means to extract a projection matrix that transforms any hyponym to its hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations. 1 Introduction Traditionally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embedding</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11):39– 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in Distributional Models of Semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="7879" citStr="Mitchell and Lapata, 2010" startWordPosition="1153" endWordPosition="1156">een individual node pairs and graph pruning or edge collapsing (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014). 9Commonly known as Lin information content measure. 933 2.4 Vector Space Approaches Semantic knowledge can be thought of as a twodimensional vector space where each word is represented as a point and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used widely in other NLP tasks, ontology/taxonomy inducing using vector space models has not been popular. It is only since the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in Distributional Models of Semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
</authors>
<title>A Graph-Based Algorithm for Inducing Lexical Taxonomies from Scratch.</title>
<date>2011</date>
<booktitle>In IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1872--1877</pages>
<location>Barcelona, Catalonia, Spain,</location>
<contexts>
<context position="7363" citStr="Navigli et al., 2011" startWordPosition="1066" endWordPosition="1069">ifferent jargon. In graph notation, nodes/vertices form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of a taxonomy as a by-product of the individual pairs of nodes connected by a directed edges. In this regard, a single root node is not guaranteed and to produce a tree-like structure. Disregarding the overall hierarchical structure, the crux of graph induction focuses on the different techniques of edge weighting between individual node pairs and graph pruning or edge collapsing (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014). 9Commonly known as Lin information content measure. 933 2.4 Vector Space Approaches Semantic knowledge can be thought of as a twodimensional vector space where each word is represented as a point and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used widely in other NLP tasks,</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011. A Graph-Based Algorithm for Inducing Lexical Taxonomies from Scratch. In IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011, pages 1872–1877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically Labeling Semantic Classes.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<contexts>
<context position="6425" citStr="Pantel and Ravichandran (2004)" startWordPosition="916" endWordPosition="919">reva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach, assigning the hypernyms to clusters using co-occurrence statistics and then pruning the cluster by recalculating the pairwise similarity between every hyponym pair within the cluster. 2.3 Graph-based Approaches In graph theory (Biggs et al., 1976), similar ideas are conceived with a different jargon. In graph notation, nodes/vertices form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of a taxonomy as a by-product of the individual pairs of nodes connected by a directed edge</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically Labeling Semantic Classes. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pease</author>
<author>Ian Niles</author>
<author>John Li</author>
</authors>
<title>The Suggested Upper Merged Ontology: A Large Ontology for the Semantic Web and its Applications.</title>
<date>2002</date>
<booktitle>In Working Notes of the AAAI-2002 Workshop on Ontologies and the SemanticWeb,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5049" citStr="Pease et al., 2002" startWordPosition="744" endWordPosition="747">are to generate hyper-hyponyms pairs using a list of terms from four different domains, viz. chemicals, equipment, food and science. The gold standards used in evaluation are the ChEBI ontology for the chemical domain (Degtyarenko et al., 2008), the Material Handling Equipment taxonomy6 for the equipment domain, the Google product taxonomy7 for the food domain and the Taxonomy of Fields and their Different Sub-fields8 for the science domain. In addition, all four domains are also evaluated against the sub-hierarchies from the WordNet ontology that subsumes the Suggested Upper Merged Ontology (Pease et al., 2002). 2 Related Work There are a variety of methods used in taxonomy induction. They can be broadly categorized as (i) pattern/rule based, (ii) clustering based, (iii) graph based and (iv) vector space approaches. 5https://github.com/alvations/USAAR-SemEval2015/tree/master/task17-USAAR-WLV 6http://www.ise.ncsu.edu/kay/mhetax/index.htm 7http://www.google.com/basepages/producttype/taxonomy.enUS.txt 8http://sites.nationalacademies.org/PGA/Resdoc/PGA 044522 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a</context>
</contexts>
<marker>Pease, Niles, Li, 2002</marker>
<rawString>Adam Pease, Ian Niles, and John Li. 2002. The Suggested Upper Merged Ontology: A Large Ontology for the Semantic Web and its Applications. In Working Notes of the AAAI-2002 Workshop on Ontologies and the SemanticWeb, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<publisher>ELRA.</publisher>
<location>Valletta, Malta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu´ıs Sarmento</author>
<author>Paula Carvalho</author>
<author>Eug´enio Oliveira</author>
</authors>
<title>Exploring the Vector Space Model for Finding Verb Synonyms in Portuguese.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>393--398</pages>
<contexts>
<context position="3125" citStr="Sarmento et al., 2009" startWordPosition="463" endWordPosition="466">ratus) is a freshwater fish”.3 Intuitively, if we single-tokenize the ‘is a’ phrase prior to training a vector space, we can make use of the vector that represents the phrase in capturing a hypernym-hyponym pair as such the multiplication of v(goldfish) and v(is-a) will be similar to the cross product v(fish) (v(goldfish)×v(is-a) ≈ v(fish)). There is little or no previous work that manipulates non-content word vectors in vector space models studies for natural language processing. Often, non-content words4 were implicitly incorporated into the vector space models by means of syntactic frames (Sarmento et al., 2009) or syntactic parses (Thater et al., 2010). Our main contribution for ontological induction 2In this case, the projection matrix is a vector space feature function. 3From http://en.wikipedia.org/wiki/Goldfish. 4Words that are not noun (entities/arguments), verbs (predicates), adjectives or adverbs (adjuncts). 932 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 932–937, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics using vector space models are primarily (i) the use of non-content word vectors and (ii) simplifying a</context>
</contexts>
<marker>Sarmento, Carvalho, Oliveira, 2009</marker>
<rawString>Lu´ıs Sarmento, Paula Carvalho, and Eug´enio Oliveira. 2009. Exploring the Vector Space Model for Finding Verb Synonyms in Portuguese. In Proceedings of the International Conference RANLP-2009, pages 393–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew M Saxe</author>
<author>James L McClelland</author>
<author>Surya Ganguli</author>
</authors>
<date>2013</date>
<booktitle>Learning Hierarchical Category Structure in Deep Neural Networks.</booktitle>
<pages>1271--1276</pages>
<contexts>
<context position="8223" citStr="Saxe et al., 2013" startWordPosition="1209" endWordPosition="1212">t and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used widely in other NLP tasks, ontology/taxonomy inducing using vector space models has not been popular. It is only since the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym to its Hypernym with Transition Matrix Fu et al. (2014) discovered that hypernymhyponyms pairs have similar semantic properties as the linguistics regularities discussed in Mikolov et al. (2013b). For instance: v(shrimp)-v(prawn) ≈ v(fish)-v(goldfish). Intuitively, the assumption is that all words can be projected to their hypernyms based on</context>
</contexts>
<marker>Saxe, McClelland, Ganguli, 2013</marker>
<rawString>Andrew M. Saxe, James L. McClelland, and Surya Ganguli. 2013. Learning Hierarchical Category Structure in Deep Neural Networks. pages 1271–1276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Taxonomy Induction from Heterogenous Evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="1334" citStr="Snow et al., 2006" startWordPosition="183" endWordPosition="186">any hyponym to its hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations. 1 Introduction Traditionally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embeddings that 1For the rest of the paper, taxonomy and ontology will be used interchangeably to refer to a hierarchically structure that organizes a list of concepts. trains a projection matrix2 that converts a hyponym vector to its hypernym. How</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic Taxonomy Induction from Heterogenous Evidence. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
</authors>
<title>Examining crosslingual word sense disambiguation. Master’s thesis,</title>
<date>2013</date>
<pages>17--21</pages>
<institution>Nanyang Technological University.</institution>
<contexts>
<context position="7891" citStr="Tan, 2013" startWordPosition="1157" endWordPosition="1158">nd graph pruning or edge collapsing (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014). 9Commonly known as Lin information content measure. 933 2.4 Vector Space Approaches Semantic knowledge can be thought of as a twodimensional vector space where each word is represented as a point and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used widely in other NLP tasks, ontology/taxonomy inducing using vector space models has not been popular. It is only since the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym to its Hype</context>
</contexts>
<marker>Tan, 2013</marker>
<rawString>Liling Tan. 2013. Examining crosslingual word sense disambiguation. Master’s thesis, Nanyang Technological University. pages 17-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing Semantic Representations using Syntactically Enriched Vector Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing Semantic Representations using Syntactically Enriched Vector Models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luu Anh Tuan</author>
<author>Jung-jae Kim</author>
<author>Kiong See Ng</author>
</authors>
<title>Taxonomy construction using syntactic contextual evidence.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>810--819</pages>
<contexts>
<context position="7410" citStr="Tuan et al., 2014" startWordPosition="1074" endWordPosition="1077">s form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of a taxonomy as a by-product of the individual pairs of nodes connected by a directed edges. In this regard, a single root node is not guaranteed and to produce a tree-like structure. Disregarding the overall hierarchical structure, the crux of graph induction focuses on the different techniques of edge weighting between individual node pairs and graph pruning or edge collapsing (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014). 9Commonly known as Lin information content measure. 933 2.4 Vector Space Approaches Semantic knowledge can be thought of as a twodimensional vector space where each word is represented as a point and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across context, such that words with similar meaning are found close to each other in the space (Mitchell and Lapata, 2010; Tan, 2013). Although vector space models have been used widely in other NLP tasks, ontology/taxonomy inducing using vector space </context>
</contexts>
<marker>Tuan, Kim, Ng, 2014</marker>
<rawString>Luu Anh Tuan, Jung-jae Kim, and Kiong See Ng. 2014. Taxonomy construction using syntactic contextual evidence. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 810–819.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>OntoLearn Reloaded: A Graph-based Algorithm for Taxonomy Induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="1357" citStr="Velardi et al., 2013" startWordPosition="187" endWordPosition="190">hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations. 1 Introduction Traditionally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embeddings that 1For the rest of the paper, taxonomy and ontology will be used interchangeably to refer to a hierarchically structure that organizes a list of concepts. trains a projection matrix2 that converts a hyponym vector to its hypernym. However, their approach re</context>
</contexts>
<marker>Velardi, Faralli, Navigli, 2013</marker>
<rawString>Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013. OntoLearn Reloaded: A Graph-based Algorithm for Taxonomy Induction. Computational Linguistics, 39(3):665–707.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>