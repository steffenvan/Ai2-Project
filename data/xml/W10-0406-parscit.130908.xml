<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004977">
<title confidence="0.997325">
Learning Simple Wikipedia:
A Cogitation in Ascertaining Abecedarian Language
</title>
<author confidence="0.993444">
Courtney Napoles and Mark Dredze
</author>
<affiliation confidence="0.940158333333333">
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.95682">
Baltimore, MD 21211
</address>
<email confidence="0.999286">
courtneyn@jhu.edu, mdredze@cs.jhu.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998958">
Text simplification is the process of changing
vocabulary and grammatical structure to cre-
ate a more accessible version of the text while
maintaining the underlying information and
content. Automated tools for text simplifica-
tion are a practical way to make large corpora
of text accessible to a wider audience lacking
high levels of fluency in the corpus language.
In this work, we investigate the potential of
Simple Wikipedia to assist automatic text sim-
plification by building a statistical classifica-
tion system that discriminates simple English
from ordinary English. Most text simplifica-
tion systems are based on hand-written rules
(e.g., PEST (Carroll et al., 1999) and its mod-
ule SYSTAR (Canning et al., 2000)), and
therefore face limitations scaling and trans-
ferring across domains. The potential for us-
ing Simple Wikipedia for text simplification
is significant; it contains nearly 60,000 ar-
ticles with revision histories and aligned ar-
ticles to ordinary English Wikipedia. Us-
ing articles from Simple Wikipedia and ordi-
nary Wikipedia, we evaluated different classi-
fiers and feature sets to identify the most dis-
criminative features of simple English for use
across domains. These findings help further
understanding of what makes text simple and
can be applied as a tool to help writers craft
simple text.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896243243243">
The availability of large collections of electronic
texts is a boon to information seekers, however, ad-
vanced texts often require fluency in the language.
Text simplification (TS) is an emerging area of text-
to-text generation that focuses on increasing the
readability of a given text. Potential applications
can increase the accessibility of text, which has great
value in education, public health, and safety, and can
aid natural language processing tasks such as ma-
chine translation and text generation.
Corresponding to these applications, TS can be
broken down into two rough categories depending
on the target “reader.” The first type of TS aims to
increase human readability for people lacking high-
level language skills, either because of age, educa-
tion level, unfamiliarity with the language, or dis-
ability. Historically, generating this text has been
done by hand, which is time consuming and expen-
sive, especially when dealing with material that re-
quires expertise, such as legal documents. Most cur-
rent automatic TS systems rely on handwritten rules,
e.g., PEST (Carroll et al., 1999), its SYSTAR mod-
ule (Canning et al., 2000), and the method described
by Siddharthan (2006). Systems using handwritten
rules can be susceptible to changes in domains and
need to be modified for each new domain or lan-
guage. There has been some research into automat-
ically learning the rules for simplifying text using
aligned corpora (Daelemans et al., 2004; Yatskar et
al., 2010), but these have yet to match the perfor-
mance hand-crafted rule systems. An example of
a manually simplified sentence can be found in ta-
ble 1.
The second type of TS has the goal of increas-
ing the machine readability of text to aid tasks such
as information extraction, machine translation, gen-
erative summarization, and other text generation
</bodyText>
<page confidence="0.990407">
42
</page>
<note confidence="0.966557">
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 42–50,
Los Angeles, California, June 2010. @2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999335847457627">
tasks for selecting and evaluating the best candi-
date output text. In machine translation, the eval-
uation tool most commonly used for evaluating out-
put, the BLEU score (Papineni et al., 2001), rates the
“goodness” of output based on n-gram overlap with
human-generated text. However this metric has been
criticized for not accurately measuring the fluency
of text and there is active research into other met-
rics (Callison-Burch et al., 2006; Ye et al., 2007).
Previous studies suggest that text simplified for ma-
chine and human comprehension are categorically
different (Chae and Nenkova, 2009). Our research
considers text simplified for human readers, but the
findings can be used to identify features that dis-
criminate simple text for both applications.
The process of TS can be divided into three as-
pects: removing extraneous or superfluous text, sub-
stituting more complex lexical and syntactic forms,
and inserting information to offer further clarifica-
tion where needed (Aluisio et al., 2008). In this re-
gard, TS is related to several different natural lan-
guage processing tasks such as text summarization,
compression, machine translation, and paraphras-
ing.
While none of these tasks alone directly provide
a solution to text simplification, techniques can be
drawn from each. Summarization techniques can
be used to identify the crucial, most informative
parts of a text and compression can be used to re-
move superfluous words and phrases. In fact, in the
Wikipedia documents analyzed for this research, the
average length of a “simple” document is only 21%
the length of an “ordinary” English document (al-
though this may be an unintentional byproduct of
how articles were simplified, as discussed in section
6.1).
In this paper we study the properties of language
that differentiate simple from ordinary text for hu-
man readers. Specifically, we use statistical learn-
ing techniques to identify the most discriminative
features of simple English and “ordinary” English
using articles from Simple Wikipedia and English
Wikipedia. We use cognitively motivated features
as well as statistical measurements of a document’s
lexical, syntactic, and surface features. Our study
demonstrates the validity and potential benefits of
using Simple Wikipedia as a resource for TS re-
search.
Ordinary text
Every person has the right to a name, in which is
included a first name and surname. ... The alias
chosen for legal activities has the same protection
as given to the name.
Same text in simple language
Every person has the right to have a name, and
the law protects people’s names. Also, the law
protects a person’s alias. ... The name is made
up of a first name and a surname (name = first
name + surname).
</bodyText>
<tableCaption confidence="0.9880035">
Table 1: A text in ordinary and simple language from
Aluisio et al. (2008).
</tableCaption>
<sectionHeader confidence="0.576888" genericHeader="method">
2 Wikipedia as a Corpus
</sectionHeader>
<bodyText confidence="0.999900966666667">
Wikipedia is a unique resource for natural lan-
guage processing tasks due to its sheer size, acces-
sibility, language diversity, article structure, inter-
document links, and inter-language document align-
ments. Denoyer and Gallinari (2006) introduced
the Wikipedia XML Corpus, with 1.5 million doc-
uments in eight languages from Wikipedia, that
stored the rich structural information of Wikipedia
with XML. This corpus was designed specifically
for XML retrieval but has uses in natural language
processing, categorization, machine translation, en-
tity ranking, etc. YAWN (Schenkel et al., 2007), a
Wikipedia XML corpus with semantic tags, is an-
other example of exploiting Wikipedia’s structural
information. Wikipedia provides XML site dumps
every few weeks in all languages as well as static
HTML dumps.
A diverse array of NLP research in the past
few years has used Wikipedia, such as for word
sense disambiguation (Mihalcea, 2007), classifica-
tion (Gantner and Schmidt-Thieme, 2009), machine
translation (Smith et al., 2010), coreference resolu-
tion (Versley et al., 2008; Yang and Su, 2007), sen-
tence extraction for summarization (Biadsy et al.,
2008), information retrieval (M¨uller and Gurevych,
2008), and semantic role labeling (Ponzetto and
Strube, 2006), to name a few. However, except for
very recent work by Yatskar et al. (2010), to our
knowledge there has not been comparable research
in using Wikipedia for text simplification.
</bodyText>
<page confidence="0.997882">
43
</page>
<bodyText confidence="0.6013879">
Ordinary Wikipedia
Hawking was the Lucasian Professor of Mathe-
matics at the University of Cambridge for thirty
years, taking up the post in 1979 and retiring on 1
October 2009.
Simple Wikipedia
Hawking was a professor of mathematics at the
University of Cambridge (a position that Isaac
Newton once had). He retired on October 1st
2009.
</bodyText>
<tableCaption confidence="0.940028">
Table 2: Comparable sentences from the ordinary
Wikipedia and Simple Wikipedia entry for “Stephen
Hawking.”
</tableCaption>
<bodyText confidence="0.999774258064516">
What makes Wikipedia an excellent resource for
text simplification is the new Simple Wikipedia
project1, a collection of 58,000 English Wikipedia
articles that have been rewritten in Simple English,
which uses basic vocabulary and less complex gram-
mar to make the content of Wikipedia accessible to
students, children, adults with learning difficulties,
and non-native English speakers. In addition to be-
ing a large corpus, these articles are linked to their
ordinary Wikipedia counterparts, so for each article
both a simple and an ordinary version are available.
Furthermore, on inspection many articles in Simple
Wikipedia appear to be copied and edited from the
corresponding ordinary Wikipedia article. This in-
formation, together with revision history and flags
signifying unsimplified text, can provide a scale of
information on the text-simplification process previ-
ously unavailable. Example sentences from Simple
Wikipedia and ordinary Wikipedia are shown in ta-
ble 2.
We used articles from Simple Wikipedia and or-
dinary English Wikipedia to create a large cor-
pus of simple and ordinary articles for our exper-
iments. In order to experiment with models that
work across domains, the corpus includes articles
from nine of the primary categories identified in
Simple Wikipedia: Everyday Life, Geography, His-
tory, Knowledge, Literature, Media, People, Reli-
gion, and Science. A total of 55,433 ordinary and
42,973 simple articles were extracted and processed
from English Wikipedia and Simple Wikipedia, re-
</bodyText>
<footnote confidence="0.981731">
1http://simple.wikipedia.org/
</footnote>
<table confidence="0.568114285714286">
Coarse Tag Penn Treebank Tags
DT, PDT
JJ, JJR, JJS
NN, NNS, NP, NPS, PRP, FW
RB, RBR, RBS
VB, VBN, VBG, VBP, VBZ, MD
WDT, WP, WP$, WRB
</table>
<tableCaption confidence="0.9957565">
Table 3: A mapping of the Penn Treebank tags to a coarse
tagset used to generate features.
</tableCaption>
<bodyText confidence="0.999969777777778">
spectively. Each document contains at least two sen-
tences. Additionally, the corpus contains only the
main text body of each article and does not con-
sider info boxes, tables, lists, external and cross-
references, and other structural features. The exper-
iments that follow randomly extract documents and
sentences from this collection.
Before extracting features, we ran a series of nat-
ural language processing tools to preprocess the col-
lection. First, all of the XML and “wiki markup”
was removed. Each document was split into sen-
tences using the Punkt sentence tokenizer (Kiss and
Strunk, 2006) in NLTK (Bird and Loper, 2004). We
then parsed each sentence using the PCFG parser
of Huang and Harper (2009), a modified version
of the Berkeley parser (Petrov et al., 2006; Petrov
and Klein, 2007), for the tree structure and part-of-
speech tags.
</bodyText>
<sectionHeader confidence="0.987715" genericHeader="method">
3 Task Setup
</sectionHeader>
<bodyText confidence="0.999969461538462">
To evaluate the feasibility of learning simple and or-
dinary texts, we sought to identify text properties
that differentiated between these classes. Using the
two document collections, we constructed a simple
binary classification task: label a piece of text as ei-
ther simple or ordinary. The text was labeled ac-
cording to its source: simple or ordinary Wikipedia.
From each piece of text, we extracted a set of fea-
tures designed to capture differences between the
texts, using cognitively motivated features based on
a document’s lexical, syntactic, and surface features.
We first describe our features and then our experi-
mental setup.
</bodyText>
<figure confidence="0.904904166666667">
DET
ADJ
N
ADV
V
WH
</figure>
<page confidence="0.993834">
44
</page>
<sectionHeader confidence="0.998735" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999981604651163">
We began by examining the guidelines for writing
Simple Wikipedia pages.2 These guidelines suggest
that articles use only the 1000 most common and ba-
sic English words and contain simple grammar and
short sentences. Articles should be short but can be
longer if they need to explain vocabulary words nec-
essary to understand the topic. Additionally, words
should appear on lists of basic English words, such
as the Voice of America Special English words list
(Voice Of America, 2009) or the Ogden Basic En-
glish list (Ogden, 1930). Idioms should be avoided
as well as compounds and the passive voice as op-
posed to a single simple verb.
To capture these properties in the text, we created
four classes of features: lexical, part-of-speech, sur-
face, and parse. Several of our features have previ-
ously been used for measuring text fluency (Aluisio
et al., 2008; Chae and Nenkova, 2009; Feng et al.,
2009; Petersen and Ostendorf, 2007).
Lexical. Previous work by Feng et al. (2009) sug-
gests that the document vocabulary is a good predic-
tor of document readability. Simple texts are more
likely to use basic words more often as opposed to
more complicated, domain-specific words used in
ordinary texts. To capture these features we used a
unigram bag-of-words representation. We note that
lexical features are unlikely to be useful unless we
have access to a large training corpus that allowed
the estimation of the relative frequency of words
(Chae and Nenkova, 2009). Additionally, we can
expect lexical features to be very fragile for cross-
domain experiments as they are especially suscepti-
ble to changes in domain vocabulary. Nevertheless,
we include these features as a baseline in our exper-
iments.
Parts of speech. A clear focus of the simple text
guidelines is grammar and word type. One way
of representing this information is by measuring
the relative frequency of different types of parts
of speech. We consider simple unigram part-of-
speech tag information. We measured the nor-
malized counts and relative frequency of part-of-
speech tags and counts of bigram part-of-speech tags
</bodyText>
<footnote confidence="0.7893205">
2http://simple.wikipedia.org/wiki/Wikipedia:
Simple-English-Wikipedia
</footnote>
<table confidence="0.999837666666667">
Feature Simple Ordinary
Tokens 158 4332
Types 100 1446
Sentences 10 172
Average sentence length 15.80 25.19
Type-token ratio 0.63 0.33
Percent simple words 0.31 0.08
Not BE850 type-token ratio 0.65 0.30
BE850 type-token ratio 0.59 0.67
</table>
<tableCaption confidence="0.988218">
Table 4: A comparison of the article “Stephen Hawking”
from Simple and ordinary Wikipedia.
</tableCaption>
<bodyText confidence="0.999942827586207">
in each piece of text. Since Devlin and Unthank
(2006) has shown that word order (subject verb ob-
ject (SVO), object verb subject (OVS), etc.) is cor-
related with readability, we also included a reduced
tagset to capture grammatical patterns (table 3). We
also included normalized counts of these reduced
tags in the model.
Surface features. While lexical items may be im-
portant, more general properties can be extracted
from the lexical forms. We can also include fea-
tures that correspond to surface information in the
text. These features include document length, sen-
tence length, word length, numbers of lexical types
and tokens, and the ratio of types to tokens. All
words are labeled as basic or not basic according
to Ogden’s Basic English 850 (BE850) list (Ogden,
1930).3 In order to measure the lexical complexity
of a document, we include features for the number
of BE850 words, the ratio of BE850 words to total
words, and the type-token ratio of BE850 and non-
BE850 words. Investigating the frequency and pro-
ductivity of words not in the BE850 list will hope-
fully improve the flexibility of our model to work
across domains and not learn any particular jargon.
We also hope that the relative frequency and pro-
ductivity measures of simple and non-simple words
will codify the lexical choices of a sentence while
avoiding the aforementioned problems with includ-
ing specific lexical items.
</bodyText>
<footnote confidence="0.9972552">
3Wikipedia advocates using words that appear on the BE850
list. Ogden also provides extended Basic English vocabulary
lists, totaling 2000 Basic English words, but these words tend
to be more specialized or domain specific. For the purposes of
this study only words in BE850 were used.
</footnote>
<page confidence="0.99891">
45
</page>
<bodyText confidence="0.99990025">
Table 4 shows the difference in some surface
statistics in an aligned document from Simple and
ordinary Wikipedia. In this example, nearly one-
third of the words in the simple document are from
the BE850 while less than a tenth of the words in the
ordinary document are. Additionally, the productiv-
ity of words, particularly non-BE850 words, is much
higher in the ordinary document. There are also
clear differences in the length of the documents, and
on average documents from ordinary Wikipedia are
more than four times longer than documents from
Simple Wikipedia.
Syntactic parse. As previously mentioned, a
number of Wikipedia’s writing guidelines focus on
general grammatical rules of sentence structure. Ev-
idence of these rules may be captured in the syn-
tactic parse of the sentences in the text. Chae and
Nenkova (2009) studied text fluency in the context
of machine translation and found strong correlations
between parse tree structures and sentence fluency.
In order to represent the structural complexity of
the text, we collected extracted features from the
parse trees. Our features included the frequency and
length of noun phrases, verb phrases, prepositional
phrases, and relative clauses (including embedded
structures). We also considered relative ratios, such
as the ratio of noun to verb phrases, prepositional to
noun phrases, and relative clauses to noun phrases.
We used the length of the longest noun phrase as
a signal of complexity, and we also sought features
that measured how typical the sentences were of En-
glish text. We included some of the features from
the parser reranking work of Charniak and Johnson
(2005): the height of the parse tree and the number
of right branches from the root of the tree to the fur-
thest right leaf that is not punctuation.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998899555555556">
Using the feature sets described above, we evalu-
ated a simple/ordinary text classifier in several set-
tings on each category. First, we considered the task
of document classification, where a classifier deter-
mines whether a full Wikipedia article was from
ordinary English Wikipedia or Simple Wikipedia.
For each category of articles, we measured accu-
racy on this binary classification task using 10-fold
cross-validation. In the second setting, we consid-
</bodyText>
<table confidence="0.9993257">
Category Documents Sentences
Everyday Life 15,124 7,392
Geography 10,470 5,852
History 5,174 1,644
Literature 992 438
Media 502 429
People 4,326 1,562
Religion 1,863 1,581
Science 25,787 21,054
All 64,238 39,952
</table>
<tableCaption confidence="0.990807">
Table 5: The number of examples available in each cate-
gory. To compare experiments in each category we used
at most 2000 instances in each experiment.
</tableCaption>
<figure confidence="0.965828666666667">
Feature class Features
Lexical 522,153
Part of speech
tags
tag pairs
tags (reduced)
tag pairs (reduced)
Parse
Surface
</figure>
<tableCaption confidence="0.883124">
Table 6: The number of features in each feature class.
</tableCaption>
<bodyText confidence="0.997991714285714">
ered the performance of a sentence-level classifier.
The classifier labeled each sentence as either ordi-
nary or simple and we report results using 10-fold
cross-validation on a random split of the sentences.
For both settings we also evaluated a single classifier
trained on all categories.
We next considered cross-category performance:
how would a classifier trained to detect differences
between simple and ordinary examples from one
category do when tested on another category. In
this experiment, we trained a single classifier on data
from a single category and used the classifier to label
examples from each of the other categories. We re-
port the accuracy on each category in these transfer
experiments.
For learning we require a binary classifier train-
ing algorithm. We evaluated several learning algo-
rithms for classification and report results for each
one: a) MIRA—a large margin online learning al-
gorithm (Crammer et al., 2006). Online learning
algorithms observe examples sequentially and up-
</bodyText>
<page confidence="0.988366625">
2478
45
1972
22
484
11
9
46
</page>
<bodyText confidence="0.9999555">
date the current hypothesis after each observation; b)
Confidence Weighted (CW) learning—a probabilis-
tic large margin online learning algorithm (Dredze et
al., 2008); c) Maximum Entropy—a log-linear dis-
criminative classifier (Berger et al., 1996); and d)
Support Vector Machines (SVM)—a large margin
discriminator (Joachims, 1998).
For each experiment, we used default settings of
the parameters and 10 online iterations for the online
methods (MIRA, CW). To create a fair comparison
for each category, we limited the number of exam-
ples to a maximum of 2000.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999459625">
For the first task of document classification, we saw
at least 90% mean accuracy with each of the clas-
sifiers. Using all features, SVM and Maximum En-
tropy performed almost perfectly. The online clas-
sifiers, CW and MIRA, displayed similar preference
to the larger feature sets, lexical and part-of-speech
counts. When using just lexical counts, both CW
and MIRA were more accurate than the SVM and
Maximum Entropy (reporting 92.95% and 86.55%
versus 75.00% and 78.75%, respectively). For all
classifiers, the models using the counts of part-of-
speech tags did better than classifiers trained on the
surface features and on the parse features. This is
surprising, since we expected the surface features to
be robust predictors of the document class, mainly
because the average ordinary Wikipedia article in
our corpus is about four times longer than the av-
erage Simple Wikipedia article. We also expected
the syntactic features to be a strong predictor of the
document class since more complicated parse trees
correspond to more complex sentences.
For each classifier, we looked at its performance
without its less predictive feature categories, and
for CW the inclusion of the surface features de-
creased performance noticeably. The best CW
classifiers used either part-of-speech and lexical
features (95.95%) or just part-of-speech features
(95.80%). The parse features, which by themselves
only yielded 64.60% accuracy, when combined with
part-of-speech and lexical features showed high ac-
curacy as well (95.60%). MIRA also showed higher
accuracy when surface features were not included
(from 97.50% mean accuracy with all features to
97.75% with all but surface features).
The best SVM classifier used all four feature
classes, but had nearly as good accuracy with just
part-of-speech counts and surface features (99.85%
mean accuracy) and with surface and parse features
(also 99.85% accuracy). Maximum Entropy, on
the other hand, improved slightly when the lexical
and parse features were not included (from 99.45%
mean accuracy with all feature classes to 99.55%).
We examined the weights learned by the classi-
fiers to determine the features that were effective for
learning. We selected the features with the highest
absolute weight for a MIRA classifier trained on all
categories. The most predictive features for docu-
ment classification were the sentence length (shorter
favors Simple), the length of the longest NP (longer
favors ordinary), the number of sentences (more fa-
vors ordinary), the average number of prepositional
phrases and noun phrases per sentence, the height
of the parse tree, and the number of adjectives. The
most predictive features for sentence classification
were the ratio of different tree non-terminals (VP, S,
NP, S-Bar) to the number of words in the sentence,
the ratio of the total height of the productions in a
tree to the height of the tree, and the extent to which
the tree was right branching. These features are con-
sistent with the rules described above for simple text.
Next we looked at a pairwise comparison of how
the classifiers perform when trained on one category
and tested on another. Surprisingly, the results were
robust across categories, across classifiers. Using
the best feature class as determined in the first task,
the average drop in accuracy when trained on each
domain was very low across all classifiers (the mean
accuracy rate of each cross-category classification
was at least 90%). Table 6 shows the mean change in
accuracy from CW models trained and tested on the
same category to the models trained and tested on
different categories. When trained on the Everyday
Life category, the model actually showed a mean in-
crease in accuracy when predicting other categories.
In the final task, we trained binary classifiers to
identify simple sentences in isolation. The mean
accuracy was lower for this task than for the doc-
ument classification task, and we anticipated indi-
vidual sentences to be more difficult to classify be-
cause each sentence only carries a fraction of the
</bodyText>
<page confidence="0.997441">
47
</page>
<table confidence="0.9999114">
Classifier All features Lexical POS Surface Parse
CW 86.40% 92.95% 95.80% 69.80% 64.60%
MIRA 97.50% 86.55% 94.55% 79.65% 66.90%
MaxEnt 99.45% 78.75% 96.25% 86.90% 80.70%
SVM 99.90% 75.00% 96.60% 89.75% 82.70%
</table>
<tableCaption confidence="0.983292">
Table 7: Mean accuracy of all classifiers on the document classification task.
</tableCaption>
<table confidence="0.9999408">
Classifier All features POS Surface Parse
CW 73.20% 74.45% 57.40% 62.25%
MIRA 71.15% 72.65% 56.50% 56.45%
MaxEnt 80.80% 77.65% 71.30% 69.00%
SVM 77.00% 76.40% 72.55% 73.00%
</table>
<tableCaption confidence="0.999127">
Table 8: Mean accuracy of all classifiers on the sentence classification task.
</tableCaption>
<figure confidence="0.534898222222222">
Category Mean accuracy change
Everyday life
Geography
History
Literature
Media −0.56%
People
Religion
Science
</figure>
<tableCaption confidence="0.961183">
Table 9: Mean accuracy drop for a CW model trained on
one category and tested on all other categories. Negative
numbers indicate a decrease in performance.
</tableCaption>
<bodyText confidence="0.99994625">
information held in an entire document. It is com-
mon to have short, simple sentences as part of ordi-
nary English text, although they will not make up the
whole. However results were still promising, with
between 72% and 80% mean accuracy. With CW
and MIRA, the classifiers benefited from training on
all categories, while MaxEnt and SVM in-category
and all-category models achieved similar accuracy
levels, but the results on cross-category tests were
more variable than in the document classification.
There was also no consistency across features and
classifiers with regard to category-to-category clas-
sification. Overall the results of the sentence classi-
fication task are encouraging and show promise for
detecting individual simple sentences taken out of
context.
</bodyText>
<subsectionHeader confidence="0.968534">
6.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999981592592592">
The classifiers performed robustly for the document-
level classification task, although the corpus itself
may have biased the model due to the longer aver-
age length of ordinary documents, which we tried
to address by filtering out articles with only one
or two sentences. Cursory inspection suggests that
there is overlap between many Simple Wikipedia ar-
ticles and their corresponding ordinary English arti-
cles, since a large number of Simple Wikipedia doc-
uments appear to be generated directly from the En-
glish Wikipedia articles with more complicated sub-
sections of the documents omitted from the Simple
article.
The sentence classification task could be im-
proved by better labeling of sentences. In these ex-
periments, we assumed that every sentence in an or-
dinary document would be ordinary (i.e., not simple)
and vice versa for simple documents. However it is
not the case that ordinary English text contains only
complicated sentences. In future research we can
use human annotated sentences for building the clas-
sifiers. The features we used in this research suggest
that simple text is created from categorical lexical
and syntactic replacement, but more complicated,
technical, or detailed oriented text may require more
rewriting, and would be of more interest in future
research.
</bodyText>
<sectionHeader confidence="0.885796" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9997125">
We have demonstrated the ability to automatically
identify texts as either simple or ordinary at both
</bodyText>
<figure confidence="0.997603">
+1.42%
−4.29%
−1.01%
−1.84%
−0.20%
−0.56%
−2.50%
</figure>
<page confidence="0.997693">
48
</page>
<bodyText confidence="0.999961444444444">
the document and sentence levels using a variety of
features based on the word usage and grammatical
structures in text. Our statistical analysis has identi-
fied relevant features for this task accessible to com-
putational systems. Immediate applications of the
classifiers created in this research for text simplifi-
cation include editing tools that can identify parts of
a text that may be difficult to understand or for word
processors, in order to notify writers of complicated
sentences in real time.
Using this initial exploration of Simple
Wikipedia, we plan to continue working in a
number of directions. First, we will explore ad-
ditional robust indications of text difficulty. For
example, Alu´ısio et al. (2008) claim that sentences
that are easier to read are also easier to parse, so
the entropy of the parser or confidence in the output
may be indicative of a text’s difficulty. Additionally,
language models trained on large corpora can assign
probability scores to texts, which may indicate
text difficulty. Of particular interest are syntactic
language models that incorporate some of the
syntactic observations in this paper (Filimonov and
Harper, 2009).
Our next goal will be to look at parallel sentences
to learn rules for simplifying text. One of the ad-
vantages of the Wikipedia collection is the parallel
articles in ordinary English Wikipedia and Simple
Wikipedia. While the content of the articles can dif-
fer, these are excellent examples of comparable texts
that can be useful for learning simplification rules.
Such learning can draw from machine translation,
which learns rules that translate between languages.
The related task of paraphrase extraction could also
provide comparable phrases, one of which can be
identified as a simplified version of the other (Ban-
nard and Callison-Burch, 2005). An additional re-
source available in Simple Wikipedia is the flagging
of articles as not simple. By examining the revision
history of articles whose flags have been changed,
we can discover changes that simplified texts. Initial
work on this topic has automatically learned which
edits correspond to text simplifications (Yatskar et
al., 2010).
Text simplification may necessitate the removal of
whole phrases, sentences, or even paragraphs, as, ac-
cording to the writing guidelines for Wikipedia Sim-
ple (Wikipedia, 2009), the articles should not exceed
a specified length, and some concepts may not be
explainable using the lexicon of Basic English. In
some situations, adding new text to explain confus-
ing but crucial points may serve to aid the reader,
and text generation needs to be further investigated
to make text simplification an automatic process.
</bodyText>
<sectionHeader confidence="0.994739" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9981655">
The authors would like to thank Mary Harper for her
help in parsing our corpus.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997392805555556">
S.M. Alu´ısio, L. Specia, T.A.S. Pardo, E.G. Maziero, and
R.P.M. Fortes. 2008. Brazilian portuguese automatic
text simplification systems. In DocEng.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Associa-
tion for Computational Linguistics (ACL).
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational linguistics, 22(1):39–71.
F. Biadsy, J. Hirschberg, E. Filatova, and LLC InforS-
ense. 2008. An unsupervised approach to biography
production using Wikipedia. In Association for Com-
putational Linguistics (ACL).
S. Bird and E. Loper. 2004. NLTK: The natural lan-
guage toolkit. Proceedings of the ACL demonstration
session, pages 214–217.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In European Conference for Computational
Linguistics (EACL), volume 2006, pages 249–256.
Y. Canning, J. Tait, J. Archibald, and R. Crawley. 2000.
Cohesive generation of syntactically simplified news-
paper text. Lecture notes in computer science, pages
145–150.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,
and J. Tait. 1999. Simplifying text for language-
impaired readers. In European Conference for Com-
putational Linguistics (EACL), pages 269–270.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features. In European
Conference for Computational Linguistics (EACL),
pages 139–147.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In As-
sociation for Computational Linguistics (ACL), page
180. Association for Computational Linguistics.
</reference>
<page confidence="0.995758">
49
</page>
<reference confidence="0.997251371428571">
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
W. Daelemans, A. H¨othker, and E Tjong Kim Sang.
2004. Automatic sentence simplification for subtitling
in Dutch and English. In Conference on Language Re-
sources and Evaluation (LREC), pages 1045–1048.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
S. Devlin and G. Unthank. 2006. Helping aphasic peo-
ple process online information. In SIGACCESS Con-
ference on Computers and Accessibility.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
L. Feng, N. Elhadad, and M. Huenerfauth. 2009. Cog-
nitively motivated features for readability assessment.
In European Conference for Computational Linguis-
tics (EACL).
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
Empirical Methods in Natural Language Processing
(EMNLP).
Z. Gantner and L. Schmidt-Thieme. 2009. Automatic
content-based categorization of Wikipedia articles. In
Association for Computational Linguistics (ACL).
Z. Huang and M. Harper. 2009. Self-training pcfg
grammars with latent annotations across languages. In
Empirical Methods in Natural Language Processing
(EMNLP).
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learning
(ECML).
T. Kiss and J. Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguis-
tics, 32(4):485–525.
R. Mihalcea. 2007. Using Wikipedia for automatic
word sense disambiguation. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
C. M¨uller and I. Gurevych. 2008. Using Wikipedia
and Wiktionary in domain-specific information re-
trieval. In Working Notes of the Annual CLEF Meet-
ing. Springer.
C.K. Ogden. 1930. Basic English: A General Introduc-
tion with Rules and Grammar. Paul Treber &amp; Co., Ltd.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Association for Computational Linguis-
tics (ACL).
S.E. Petersen and M. Ostendorf. 2007. Text simplifi-
cation for language learners: A corpus analysis. In
The Speech and Language Technology for Education
Workshop, pages 69–72.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Association for Computa-
tional Linguistics (ACL).
S.P. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, WordNet and Wikipedia for coreference
resolution. In North American Chapter of the Associ-
ation for Computational Linguistics (NAACL).
R. Schenkel, F. Suchanek, and G. Kasneci. 2007.
YAWN: A semantically annotated Wikipedia XML
corpus. In Proceedings of GI-Fachtagung f¨ur
Datenbanksysteme in Business, Technologie und Web
(BTW2007).
A. Siddharthan. 2006. Syntactic simplification and text
cohesion. Research on Language &amp; Computation,
4(1):77–109.
Jason Smith, Chris Quirk, and Kristina Toutanova. 2010.
Extracting parallel sentences from comparable corpora
using document level alignment. In North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Association for Computational Linguistics (ACL)
Demo Session.
Voice Of America. 2009. Word book, 2009 edition.
www.voaspecialenglish.com, February.
Wikipedia. 2009. Simple Wikipedia English.
http://en.wikipedia.org/wiki/Citing Wikipedia, Octo-
ber.
X. Yang and J. Su. 2007. Coreference resolution using
semantic relatedness information from automatically
discovered patterns. In Association for Computational
Linguistics (ACL).
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Experiments with unsupervised extraction of lexi-
cal simplifications. In North American Chapter of the
Association for Computational Linguistics (NAACL).
Y. Ye, M. Zhou, and C.Y. Lin. 2007. Sentence level ma-
chine translation evaluation as a ranking problem: one
step aside from BLEU. In ACL Workshop on statisti-
cal machine translation.
</reference>
<page confidence="0.997687">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.223129">
<title confidence="0.991635">Learning Simple Wikipedia: A Cogitation in Ascertaining Abecedarian Language</title>
<affiliation confidence="0.625545">Napoles Center for Language and Speech Human Language Technology Center of Johns Hopkins</affiliation>
<address confidence="0.959445">Baltimore, MD</address>
<email confidence="0.999748">courtneyn@jhu.edu,mdredze@cs.jhu.edu</email>
<abstract confidence="0.999711290322581">Text simplification is the process of changing vocabulary and grammatical structure to create a more accessible version of the text while maintaining the underlying information and content. Automated tools for text simplification are a practical way to make large corpora of text accessible to a wider audience lacking high levels of fluency in the corpus language. In this work, we investigate the potential of Simple Wikipedia to assist automatic text simplification by building a statistical classificasystem that discriminates Most text simplification systems are based on hand-written rules (e.g., PEST (Carroll et al., 1999) and its module SYSTAR (Canning et al., 2000)), and therefore face limitations scaling and transferring across domains. The potential for using Simple Wikipedia for text simplification is significant; it contains nearly 60,000 articles with revision histories and aligned articles to ordinary English Wikipedia. Using articles from Simple Wikipedia and ordinary Wikipedia, we evaluated different classifiers and feature sets to identify the most discriminative features of simple English for use across domains. These findings help further understanding of what makes text simple and can be applied as a tool to help writers craft simple text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S M Alu´ısio</author>
<author>L Specia</author>
<author>T A S Pardo</author>
<author>E G Maziero</author>
<author>R P M Fortes</author>
</authors>
<title>Brazilian portuguese automatic text simplification systems.</title>
<date>2008</date>
<booktitle>In DocEng.</booktitle>
<marker>Alu´ısio, Specia, Pardo, Maziero, Fortes, 2008</marker>
<rawString>S.M. Alu´ısio, L. Specia, T.A.S. Pardo, E.G. Maziero, and R.P.M. Fortes. 2008. Brazilian portuguese automatic text simplification systems. In DocEng.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="29076" citStr="Bannard and Callison-Burch, 2005" startWordPosition="4606" endWordPosition="4610">l be to look at parallel sentences to learn rules for simplifying text. One of the advantages of the Wikipedia collection is the parallel articles in ordinary English Wikipedia and Simple Wikipedia. While the content of the articles can differ, these are excellent examples of comparable texts that can be useful for learning simplification rules. Such learning can draw from machine translation, which learns rules that translate between languages. The related task of paraphrase extraction could also provide comparable phrases, one of which can be identified as a simplified version of the other (Bannard and Callison-Burch, 2005). An additional resource available in Simple Wikipedia is the flagging of articles as not simple. By examining the revision history of articles whose flags have been changed, we can discover changes that simplified texts. Initial work on this topic has automatically learned which edits correspond to text simplifications (Yatskar et al., 2010). Text simplification may necessitate the removal of whole phrases, sentences, or even paragraphs, as, according to the writing guidelines for Wikipedia Simple (Wikipedia, 2009), the articles should not exceed a specified length, and some concepts may not </context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="19830" citStr="Berger et al., 1996" startWordPosition="3148" endWordPosition="3151">racy on each category in these transfer experiments. For learning we require a binary classifier training algorithm. We evaluated several learning algorithms for classification and report results for each one: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006). Online learning algorithms observe examples sequentially and up2478 45 1972 22 484 11 9 46 date the current hypothesis after each observation; b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998). For each experiment, we used default settings of the parameters and 10 online iterations for the online methods (MIRA, CW). To create a fair comparison for each category, we limited the number of examples to a maximum of 2000. 6 Results For the first task of document classification, we saw at least 90% mean accuracy with each of the classifiers. Using all features, SVM and Maximum Entropy performed almost perfectly. The online classifiers, CW and MIRA, displayed similar preference to the larger feature sets, </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Biadsy</author>
<author>J Hirschberg</author>
<author>E Filatova</author>
<author>LLC InforSense</author>
</authors>
<title>An unsupervised approach to biography production using Wikipedia.</title>
<date>2008</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7577" citStr="Biadsy et al., 2008" startWordPosition="1187" endWordPosition="1190">ranslation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathematics at the University of Cambridge (a position that Isaac Newton once had). He r</context>
</contexts>
<marker>Biadsy, Hirschberg, Filatova, InforSense, 2008</marker>
<rawString>F. Biadsy, J. Hirschberg, E. Filatova, and LLC InforSense. 2008. An unsupervised approach to biography production using Wikipedia. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>E Loper</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL demonstration session,</booktitle>
<pages>214--217</pages>
<contexts>
<context position="10703" citStr="Bird and Loper, 2004" startWordPosition="1679" endWordPosition="1682">ively. Each document contains at least two sentences. Additionally, the corpus contains only the main text body of each article and does not consider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From e</context>
</contexts>
<marker>Bird, Loper, 2004</marker>
<rawString>S. Bird and E. Loper. 2004. NLTK: The natural language toolkit. Proceedings of the ACL demonstration session, pages 214–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Reevaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In European Conference for Computational Linguistics (EACL),</booktitle>
<volume>volume</volume>
<pages>249--256</pages>
<contexts>
<context position="4063" citStr="Callison-Burch et al., 2006" startWordPosition="629" endWordPosition="632">n 42 Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 42–50, Los Angeles, California, June 2010. @2010 Association for Computational Linguistics tasks for selecting and evaluating the best candidate output text. In machine translation, the evaluation tool most commonly used for evaluating output, the BLEU score (Papineni et al., 2001), rates the “goodness” of output based on n-gram overlap with human-generated text. However this metric has been criticized for not accurately measuring the fluency of text and there is active research into other metrics (Callison-Burch et al., 2006; Ye et al., 2007). Previous studies suggest that text simplified for machine and human comprehension are categorically different (Chae and Nenkova, 2009). Our research considers text simplified for human readers, but the findings can be used to identify features that discriminate simple text for both applications. The process of TS can be divided into three aspects: removing extraneous or superfluous text, substituting more complex lexical and syntactic forms, and inserting information to offer further clarification where needed (Aluisio et al., 2008). In this regard, TS is related to several</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Reevaluating the role of BLEU in machine translation research. In European Conference for Computational Linguistics (EACL), volume 2006, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Canning</author>
<author>J Tait</author>
<author>J Archibald</author>
<author>R Crawley</author>
</authors>
<title>Cohesive generation of syntactically simplified newspaper text. Lecture notes in computer science,</title>
<date>2000</date>
<pages>145--150</pages>
<contexts>
<context position="1009" citStr="Canning et al., 2000" startWordPosition="144" endWordPosition="147">ore accessible version of the text while maintaining the underlying information and content. Automated tools for text simplification are a practical way to make large corpora of text accessible to a wider audience lacking high levels of fluency in the corpus language. In this work, we investigate the potential of Simple Wikipedia to assist automatic text simplification by building a statistical classification system that discriminates simple English from ordinary English. Most text simplification systems are based on hand-written rules (e.g., PEST (Carroll et al., 1999) and its module SYSTAR (Canning et al., 2000)), and therefore face limitations scaling and transferring across domains. The potential for using Simple Wikipedia for text simplification is significant; it contains nearly 60,000 articles with revision histories and aligned articles to ordinary English Wikipedia. Using articles from Simple Wikipedia and ordinary Wikipedia, we evaluated different classifiers and feature sets to identify the most discriminative features of simple English for use across domains. These findings help further understanding of what makes text simple and can be applied as a tool to help writers craft simple text. 1</context>
<context position="2760" citStr="Canning et al., 2000" startWordPosition="420" endWordPosition="423">ding to these applications, TS can be broken down into two rough categories depending on the target “reader.” The first type of TS aims to increase human readability for people lacking highlevel language skills, either because of age, education level, unfamiliarity with the language, or disability. Historically, generating this text has been done by hand, which is time consuming and expensive, especially when dealing with material that requires expertise, such as legal documents. Most current automatic TS systems rely on handwritten rules, e.g., PEST (Carroll et al., 1999), its SYSTAR module (Canning et al., 2000), and the method described by Siddharthan (2006). Systems using handwritten rules can be susceptible to changes in domains and need to be modified for each new domain or language. There has been some research into automatically learning the rules for simplifying text using aligned corpora (Daelemans et al., 2004; Yatskar et al., 2010), but these have yet to match the performance hand-crafted rule systems. An example of a manually simplified sentence can be found in table 1. The second type of TS has the goal of increasing the machine readability of text to aid tasks such as information extract</context>
</contexts>
<marker>Canning, Tait, Archibald, Crawley, 2000</marker>
<rawString>Y. Canning, J. Tait, J. Archibald, and R. Crawley. 2000. Cohesive generation of syntactically simplified newspaper text. Lecture notes in computer science, pages 145–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>D Pearce</author>
<author>Y Canning</author>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>Simplifying text for languageimpaired readers.</title>
<date>1999</date>
<booktitle>In European Conference for Computational Linguistics (EACL),</booktitle>
<pages>269--270</pages>
<contexts>
<context position="964" citStr="Carroll et al., 1999" startWordPosition="135" endWordPosition="138">ulary and grammatical structure to create a more accessible version of the text while maintaining the underlying information and content. Automated tools for text simplification are a practical way to make large corpora of text accessible to a wider audience lacking high levels of fluency in the corpus language. In this work, we investigate the potential of Simple Wikipedia to assist automatic text simplification by building a statistical classification system that discriminates simple English from ordinary English. Most text simplification systems are based on hand-written rules (e.g., PEST (Carroll et al., 1999) and its module SYSTAR (Canning et al., 2000)), and therefore face limitations scaling and transferring across domains. The potential for using Simple Wikipedia for text simplification is significant; it contains nearly 60,000 articles with revision histories and aligned articles to ordinary English Wikipedia. Using articles from Simple Wikipedia and ordinary Wikipedia, we evaluated different classifiers and feature sets to identify the most discriminative features of simple English for use across domains. These findings help further understanding of what makes text simple and can be applied a</context>
<context position="2718" citStr="Carroll et al., 1999" startWordPosition="412" endWordPosition="415">translation and text generation. Corresponding to these applications, TS can be broken down into two rough categories depending on the target “reader.” The first type of TS aims to increase human readability for people lacking highlevel language skills, either because of age, education level, unfamiliarity with the language, or disability. Historically, generating this text has been done by hand, which is time consuming and expensive, especially when dealing with material that requires expertise, such as legal documents. Most current automatic TS systems rely on handwritten rules, e.g., PEST (Carroll et al., 1999), its SYSTAR module (Canning et al., 2000), and the method described by Siddharthan (2006). Systems using handwritten rules can be susceptible to changes in domains and need to be modified for each new domain or language. There has been some research into automatically learning the rules for simplifying text using aligned corpora (Daelemans et al., 2004; Yatskar et al., 2010), but these have yet to match the performance hand-crafted rule systems. An example of a manually simplified sentence can be found in table 1. The second type of TS has the goal of increasing the machine readability of tex</context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin, and J. Tait. 1999. Simplifying text for languageimpaired readers. In European Conference for Computational Linguistics (EACL), pages 269–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chae</author>
<author>A Nenkova</author>
</authors>
<title>Predicting the fluency of text with shallow structural features.</title>
<date>2009</date>
<booktitle>In European Conference for Computational Linguistics (EACL),</booktitle>
<pages>139--147</pages>
<contexts>
<context position="4217" citStr="Chae and Nenkova, 2009" startWordPosition="652" endWordPosition="655">or Computational Linguistics tasks for selecting and evaluating the best candidate output text. In machine translation, the evaluation tool most commonly used for evaluating output, the BLEU score (Papineni et al., 2001), rates the “goodness” of output based on n-gram overlap with human-generated text. However this metric has been criticized for not accurately measuring the fluency of text and there is active research into other metrics (Callison-Burch et al., 2006; Ye et al., 2007). Previous studies suggest that text simplified for machine and human comprehension are categorically different (Chae and Nenkova, 2009). Our research considers text simplified for human readers, but the findings can be used to identify features that discriminate simple text for both applications. The process of TS can be divided into three aspects: removing extraneous or superfluous text, substituting more complex lexical and syntactic forms, and inserting information to offer further clarification where needed (Aluisio et al., 2008). In this regard, TS is related to several different natural language processing tasks such as text summarization, compression, machine translation, and paraphrasing. While none of these tasks alo</context>
<context position="12480" citStr="Chae and Nenkova, 2009" startWordPosition="1977" endWordPosition="1980">r if they need to explain vocabulary words necessary to understand the topic. Additionally, words should appear on lists of basic English words, such as the Voice of America Special English words list (Voice Of America, 2009) or the Ogden Basic English list (Ogden, 1930). Idioms should be avoided as well as compounds and the passive voice as opposed to a single simple verb. To capture these properties in the text, we created four classes of features: lexical, part-of-speech, surface, and parse. Several of our features have previously been used for measuring text fluency (Aluisio et al., 2008; Chae and Nenkova, 2009; Feng et al., 2009; Petersen and Ostendorf, 2007). Lexical. Previous work by Feng et al. (2009) suggests that the document vocabulary is a good predictor of document readability. Simple texts are more likely to use basic words more often as opposed to more complicated, domain-specific words used in ordinary texts. To capture these features we used a unigram bag-of-words representation. We note that lexical features are unlikely to be useful unless we have access to a large training corpus that allowed the estimation of the relative frequency of words (Chae and Nenkova, 2009). Additionally, we</context>
<context position="16588" citStr="Chae and Nenkova (2009)" startWordPosition="2639" endWordPosition="2642"> while less than a tenth of the words in the ordinary document are. Additionally, the productivity of words, particularly non-BE850 words, is much higher in the ordinary document. There are also clear differences in the length of the documents, and on average documents from ordinary Wikipedia are more than four times longer than documents from Simple Wikipedia. Syntactic parse. As previously mentioned, a number of Wikipedia’s writing guidelines focus on general grammatical rules of sentence structure. Evidence of these rules may be captured in the syntactic parse of the sentences in the text. Chae and Nenkova (2009) studied text fluency in the context of machine translation and found strong correlations between parse tree structures and sentence fluency. In order to represent the structural complexity of the text, we collected extracted features from the parse trees. Our features included the frequency and length of noun phrases, verb phrases, prepositional phrases, and relative clauses (including embedded structures). We also considered relative ratios, such as the ratio of noun to verb phrases, prepositional to noun phrases, and relative clauses to noun phrases. We used the length of the longest noun p</context>
</contexts>
<marker>Chae, Nenkova, 2009</marker>
<rawString>J. Chae and A. Nenkova. 2009. Predicting the fluency of text with shallow structural features. In European Conference for Computational Linguistics (EACL), pages 139–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17405" citStr="Charniak and Johnson (2005)" startWordPosition="2767" endWordPosition="2770">of the text, we collected extracted features from the parse trees. Our features included the frequency and length of noun phrases, verb phrases, prepositional phrases, and relative clauses (including embedded structures). We also considered relative ratios, such as the ratio of noun to verb phrases, prepositional to noun phrases, and relative clauses to noun phrases. We used the length of the longest noun phrase as a signal of complexity, and we also sought features that measured how typical the sentences were of English text. We included some of the features from the parser reranking work of Charniak and Johnson (2005): the height of the parse tree and the number of right branches from the root of the tree to the furthest right leaf that is not punctuation. 5 Experiments Using the feature sets described above, we evaluated a simple/ordinary text classifier in several settings on each category. First, we considered the task of document classification, where a classifier determines whether a full Wikipedia article was from ordinary English Wikipedia or Simple Wikipedia. For each category of articles, we measured accuracy on this binary classification task using 10-fold cross-validation. In the second setting,</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Association for Computational Linguistics (ACL), page 180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="19491" citStr="Crammer et al., 2006" startWordPosition="3098" endWordPosition="3101">egory performance: how would a classifier trained to detect differences between simple and ordinary examples from one category do when tested on another category. In this experiment, we trained a single classifier on data from a single category and used the classifier to label examples from each of the other categories. We report the accuracy on each category in these transfer experiments. For learning we require a binary classifier training algorithm. We evaluated several learning algorithms for classification and report results for each one: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006). Online learning algorithms observe examples sequentially and up2478 45 1972 22 484 11 9 46 date the current hypothesis after each observation; b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998). For each experiment, we used default settings of the parameters and 10 online iterations for the online methods (MIRA, CW). To create a fair comparison for each category, we l</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A H¨othker</author>
<author>E Tjong Kim Sang</author>
</authors>
<title>Automatic sentence simplification for subtitling in Dutch and English.</title>
<date>2004</date>
<booktitle>In Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1045--1048</pages>
<marker>Daelemans, H¨othker, Sang, 2004</marker>
<rawString>W. Daelemans, A. H¨othker, and E Tjong Kim Sang. 2004. Automatic sentence simplification for subtitling in Dutch and English. In Conference on Language Resources and Evaluation (LREC), pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Denoyer</author>
<author>Patrick Gallinari</author>
</authors>
<title>The Wikipedia XML Corpus.</title>
<date>2006</date>
<publisher>SIGIR Forum.</publisher>
<contexts>
<context position="6664" citStr="Denoyer and Gallinari (2006)" startWordPosition="1048" endWordPosition="1051"> activities has the same protection as given to the name. Same text in simple language Every person has the right to have a name, and the law protects people’s names. Also, the law protects a person’s alias. ... The name is made up of a first name and a surname (name = first name + surname). Table 1: A text in ordinary and simple language from Aluisio et al. (2008). 2 Wikipedia as a Corpus Wikipedia is a unique resource for natural language processing tasks due to its sheer size, accessibility, language diversity, article structure, interdocument links, and inter-language document alignments. Denoyer and Gallinari (2006) introduced the Wikipedia XML Corpus, with 1.5 million documents in eight languages from Wikipedia, that stored the rich structural information of Wikipedia with XML. This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in </context>
</contexts>
<marker>Denoyer, Gallinari, 2006</marker>
<rawString>Ludovic Denoyer and Patrick Gallinari. 2006. The Wikipedia XML Corpus. SIGIR Forum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Devlin</author>
<author>G Unthank</author>
</authors>
<title>Helping aphasic people process online information.</title>
<date>2006</date>
<booktitle>In SIGACCESS Conference on Computers and Accessibility.</booktitle>
<contexts>
<context position="14133" citStr="Devlin and Unthank (2006)" startWordPosition="2234" endWordPosition="2237">onsider simple unigram part-ofspeech tag information. We measured the normalized counts and relative frequency of part-ofspeech tags and counts of bigram part-of-speech tags 2http://simple.wikipedia.org/wiki/Wikipedia: Simple-English-Wikipedia Feature Simple Ordinary Tokens 158 4332 Types 100 1446 Sentences 10 172 Average sentence length 15.80 25.19 Type-token ratio 0.63 0.33 Percent simple words 0.31 0.08 Not BE850 type-token ratio 0.65 0.30 BE850 type-token ratio 0.59 0.67 Table 4: A comparison of the article “Stephen Hawking” from Simple and ordinary Wikipedia. in each piece of text. Since Devlin and Unthank (2006) has shown that word order (subject verb object (SVO), object verb subject (OVS), etc.) is correlated with readability, we also included a reduced tagset to capture grammatical patterns (table 3). We also included normalized counts of these reduced tags in the model. Surface features. While lexical items may be important, more general properties can be extracted from the lexical forms. We can also include features that correspond to surface information in the text. These features include document length, sentence length, word length, numbers of lexical types and tokens, and the ratio of types </context>
</contexts>
<marker>Devlin, Unthank, 2006</marker>
<rawString>S. Devlin and G. Unthank. 2006. Helping aphasic people process online information. In SIGACCESS Conference on Computers and Accessibility.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="19749" citStr="Dredze et al., 2008" startWordPosition="3137" endWordPosition="3140">lassifier to label examples from each of the other categories. We report the accuracy on each category in these transfer experiments. For learning we require a binary classifier training algorithm. We evaluated several learning algorithms for classification and report results for each one: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006). Online learning algorithms observe examples sequentially and up2478 45 1972 22 484 11 9 46 date the current hypothesis after each observation; b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998). For each experiment, we used default settings of the parameters and 10 online iterations for the online methods (MIRA, CW). To create a fair comparison for each category, we limited the number of examples to a maximum of 2000. 6 Results For the first task of document classification, we saw at least 90% mean accuracy with each of the classifiers. Using all features, SVM and Maximum Entropy performed almost perfectly. The online cl</context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Feng</author>
<author>N Elhadad</author>
<author>M Huenerfauth</author>
</authors>
<title>Cognitively motivated features for readability assessment.</title>
<date>2009</date>
<booktitle>In European Conference for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="12499" citStr="Feng et al., 2009" startWordPosition="1981" endWordPosition="1984">n vocabulary words necessary to understand the topic. Additionally, words should appear on lists of basic English words, such as the Voice of America Special English words list (Voice Of America, 2009) or the Ogden Basic English list (Ogden, 1930). Idioms should be avoided as well as compounds and the passive voice as opposed to a single simple verb. To capture these properties in the text, we created four classes of features: lexical, part-of-speech, surface, and parse. Several of our features have previously been used for measuring text fluency (Aluisio et al., 2008; Chae and Nenkova, 2009; Feng et al., 2009; Petersen and Ostendorf, 2007). Lexical. Previous work by Feng et al. (2009) suggests that the document vocabulary is a good predictor of document readability. Simple texts are more likely to use basic words more often as opposed to more complicated, domain-specific words used in ordinary texts. To capture these features we used a unigram bag-of-words representation. We note that lexical features are unlikely to be useful unless we have access to a large training corpus that allowed the estimation of the relative frequency of words (Chae and Nenkova, 2009). Additionally, we can expect lexical</context>
</contexts>
<marker>Feng, Elhadad, Huenerfauth, 2009</marker>
<rawString>L. Feng, N. Elhadad, and M. Huenerfauth. 2009. Cognitively motivated features for readability assessment. In European Conference for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>A joint language model with fine-grain syntactic tags.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="28424" citStr="Filimonov and Harper, 2009" startWordPosition="4504" endWordPosition="4507">ple Wikipedia, we plan to continue working in a number of directions. First, we will explore additional robust indications of text difficulty. For example, Alu´ısio et al. (2008) claim that sentences that are easier to read are also easier to parse, so the entropy of the parser or confidence in the output may be indicative of a text’s difficulty. Additionally, language models trained on large corpora can assign probability scores to texts, which may indicate text difficulty. Of particular interest are syntactic language models that incorporate some of the syntactic observations in this paper (Filimonov and Harper, 2009). Our next goal will be to look at parallel sentences to learn rules for simplifying text. One of the advantages of the Wikipedia collection is the parallel articles in ordinary English Wikipedia and Simple Wikipedia. While the content of the articles can differ, these are excellent examples of comparable texts that can be useful for learning simplification rules. Such learning can draw from machine translation, which learns rules that translate between languages. The related task of paraphrase extraction could also provide comparable phrases, one of which can be identified as a simplified ver</context>
</contexts>
<marker>Filimonov, Harper, 2009</marker>
<rawString>Denis Filimonov and Mary Harper. 2009. A joint language model with fine-grain syntactic tags. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Gantner</author>
<author>L Schmidt-Thieme</author>
</authors>
<title>Automatic content-based categorization of Wikipedia articles.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7408" citStr="Gantner and Schmidt-Thieme, 2009" startWordPosition="1161" endWordPosition="1164"> the rich structural information of Wikipedia with XML. This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in </context>
</contexts>
<marker>Gantner, Schmidt-Thieme, 2009</marker>
<rawString>Z. Gantner and L. Schmidt-Thieme. 2009. Automatic content-based categorization of Wikipedia articles. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Harper</author>
</authors>
<title>Self-training pcfg grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="10782" citStr="Huang and Harper (2009)" startWordPosition="1693" endWordPosition="1696">s contains only the main text body of each article and does not consider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From each piece of text, we extracted a set of features designed to capture differenc</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Z. Huang and M. Harper. 2009. Self-training pcfg grammars with latent annotations across languages. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="19914" citStr="Joachims, 1998" startWordPosition="3161" endWordPosition="3162">ssifier training algorithm. We evaluated several learning algorithms for classification and report results for each one: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006). Online learning algorithms observe examples sequentially and up2478 45 1972 22 484 11 9 46 date the current hypothesis after each observation; b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998). For each experiment, we used default settings of the parameters and 10 online iterations for the online methods (MIRA, CW). To create a fair comparison for each category, we limited the number of examples to a maximum of 2000. 6 Results For the first task of document classification, we saw at least 90% mean accuracy with each of the classifiers. Using all features, SVM and Maximum Entropy performed almost perfectly. The online classifiers, CW and MIRA, displayed similar preference to the larger feature sets, lexical and part-of-speech counts. When using just lexical counts, both CW and MIRA </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kiss</author>
<author>J Strunk</author>
</authors>
<title>Unsupervised multilingual sentence boundary detection.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="10672" citStr="Kiss and Strunk, 2006" startWordPosition="1673" endWordPosition="1676">used to generate features. spectively. Each document contains at least two sentences. Additionally, the corpus contains only the main text body of each article and does not consider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simpl</context>
</contexts>
<marker>Kiss, Strunk, 2006</marker>
<rawString>T. Kiss and J. Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4):485–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic word sense disambiguation.</title>
<date>2007</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="7357" citStr="Mihalcea, 2007" startWordPosition="1157" endWordPosition="1158">uages from Wikipedia, that stored the rich structural information of Wikipedia with XML. This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>R. Mihalcea. 2007. Using Wikipedia for automatic word sense disambiguation. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M¨uller</author>
<author>I Gurevych</author>
</authors>
<title>Using Wikipedia and Wiktionary in domain-specific information retrieval.</title>
<date>2008</date>
<booktitle>In Working Notes of the Annual CLEF Meeting.</booktitle>
<publisher>Springer.</publisher>
<marker>M¨uller, Gurevych, 2008</marker>
<rawString>C. M¨uller and I. Gurevych. 2008. Using Wikipedia and Wiktionary in domain-specific information retrieval. In Working Notes of the Annual CLEF Meeting. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Ogden</author>
</authors>
<title>Basic English: A General Introduction with Rules and Grammar. Paul Treber</title>
<date>1930</date>
<publisher>Co., Ltd.</publisher>
<contexts>
<context position="12129" citStr="Ogden, 1930" startWordPosition="1919" endWordPosition="1920">cribe our features and then our experimental setup. DET ADJ N ADV V WH 44 4 Features We began by examining the guidelines for writing Simple Wikipedia pages.2 These guidelines suggest that articles use only the 1000 most common and basic English words and contain simple grammar and short sentences. Articles should be short but can be longer if they need to explain vocabulary words necessary to understand the topic. Additionally, words should appear on lists of basic English words, such as the Voice of America Special English words list (Voice Of America, 2009) or the Ogden Basic English list (Ogden, 1930). Idioms should be avoided as well as compounds and the passive voice as opposed to a single simple verb. To capture these properties in the text, we created four classes of features: lexical, part-of-speech, surface, and parse. Several of our features have previously been used for measuring text fluency (Aluisio et al., 2008; Chae and Nenkova, 2009; Feng et al., 2009; Petersen and Ostendorf, 2007). Lexical. Previous work by Feng et al. (2009) suggests that the document vocabulary is a good predictor of document readability. Simple texts are more likely to use basic words more often as opposed</context>
<context position="14853" citStr="Ogden, 1930" startWordPosition="2355" endWordPosition="2356">eadability, we also included a reduced tagset to capture grammatical patterns (table 3). We also included normalized counts of these reduced tags in the model. Surface features. While lexical items may be important, more general properties can be extracted from the lexical forms. We can also include features that correspond to surface information in the text. These features include document length, sentence length, word length, numbers of lexical types and tokens, and the ratio of types to tokens. All words are labeled as basic or not basic according to Ogden’s Basic English 850 (BE850) list (Ogden, 1930).3 In order to measure the lexical complexity of a document, we include features for the number of BE850 words, the ratio of BE850 words to total words, and the type-token ratio of BE850 and nonBE850 words. Investigating the frequency and productivity of words not in the BE850 list will hopefully improve the flexibility of our model to work across domains and not learn any particular jargon. We also hope that the relative frequency and productivity measures of simple and non-simple words will codify the lexical choices of a sentence while avoiding the aforementioned problems with including spe</context>
</contexts>
<marker>Ogden, 1930</marker>
<rawString>C.K. Ogden. 1930. Basic English: A General Introduction with Rules and Grammar. Paul Treber &amp; Co., Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3814" citStr="Papineni et al., 2001" startWordPosition="590" endWordPosition="593">y simplified sentence can be found in table 1. The second type of TS has the goal of increasing the machine readability of text to aid tasks such as information extraction, machine translation, generative summarization, and other text generation 42 Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 42–50, Los Angeles, California, June 2010. @2010 Association for Computational Linguistics tasks for selecting and evaluating the best candidate output text. In machine translation, the evaluation tool most commonly used for evaluating output, the BLEU score (Papineni et al., 2001), rates the “goodness” of output based on n-gram overlap with human-generated text. However this metric has been criticized for not accurately measuring the fluency of text and there is active research into other metrics (Callison-Burch et al., 2006; Ye et al., 2007). Previous studies suggest that text simplified for machine and human comprehension are categorically different (Chae and Nenkova, 2009). Our research considers text simplified for human readers, but the findings can be used to identify features that discriminate simple text for both applications. The process of TS can be divided i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Petersen</author>
<author>M Ostendorf</author>
</authors>
<title>Text simplification for language learners: A corpus analysis.</title>
<date>2007</date>
<booktitle>In The Speech and Language Technology for Education Workshop,</booktitle>
<pages>69--72</pages>
<contexts>
<context position="12530" citStr="Petersen and Ostendorf, 2007" startWordPosition="1985" endWordPosition="1988">necessary to understand the topic. Additionally, words should appear on lists of basic English words, such as the Voice of America Special English words list (Voice Of America, 2009) or the Ogden Basic English list (Ogden, 1930). Idioms should be avoided as well as compounds and the passive voice as opposed to a single simple verb. To capture these properties in the text, we created four classes of features: lexical, part-of-speech, surface, and parse. Several of our features have previously been used for measuring text fluency (Aluisio et al., 2008; Chae and Nenkova, 2009; Feng et al., 2009; Petersen and Ostendorf, 2007). Lexical. Previous work by Feng et al. (2009) suggests that the document vocabulary is a good predictor of document readability. Simple texts are more likely to use basic words more often as opposed to more complicated, domain-specific words used in ordinary texts. To capture these features we used a unigram bag-of-words representation. We note that lexical features are unlikely to be useful unless we have access to a large training corpus that allowed the estimation of the relative frequency of words (Chae and Nenkova, 2009). Additionally, we can expect lexical features to be very fragile fo</context>
</contexts>
<marker>Petersen, Ostendorf, 2007</marker>
<rawString>S.E. Petersen and M. Ostendorf. 2007. Text simplification for language learners: A corpus analysis. In The Speech and Language Technology for Education Workshop, pages 69–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="10871" citStr="Petrov and Klein, 2007" startWordPosition="1708" endWordPosition="1711">es, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From each piece of text, we extracted a set of features designed to capture differences between the texts, using cognitively motivated features based on a document’s lexical,</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="10846" citStr="Petrov et al., 2006" startWordPosition="1704" endWordPosition="1707">ider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From each piece of text, we extracted a set of features designed to capture differences between the texts, using cognitively motivated features based</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="7685" citStr="Ponzetto and Strube, 2006" startWordPosition="1201" endWordPosition="1204">gs, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathematics at the University of Cambridge (a position that Isaac Newton once had). He retired on October 1st 2009. Table 2: Comparable sentences from the ordinary Wikipedia and Simple Wikipedia e</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>S.P. Ponzetto and M. Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schenkel</author>
<author>F Suchanek</author>
<author>G Kasneci</author>
</authors>
<title>YAWN: A semantically annotated Wikipedia XML corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of GI-Fachtagung f¨ur Datenbanksysteme in Business, Technologie und Web (BTW2007).</booktitle>
<contexts>
<context position="7018" citStr="Schenkel et al., 2007" startWordPosition="1100" endWordPosition="1103"> (2008). 2 Wikipedia as a Corpus Wikipedia is a unique resource for natural language processing tasks due to its sheer size, accessibility, language diversity, article structure, interdocument links, and inter-language document alignments. Denoyer and Gallinari (2006) introduced the Wikipedia XML Corpus, with 1.5 million documents in eight languages from Wikipedia, that stored the rich structural information of Wikipedia with XML. This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gure</context>
</contexts>
<marker>Schenkel, Suchanek, Kasneci, 2007</marker>
<rawString>R. Schenkel, F. Suchanek, and G. Kasneci. 2007. YAWN: A semantically annotated Wikipedia XML corpus. In Proceedings of GI-Fachtagung f¨ur Datenbanksysteme in Business, Technologie und Web (BTW2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2006</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="2808" citStr="Siddharthan (2006)" startWordPosition="429" endWordPosition="430">nto two rough categories depending on the target “reader.” The first type of TS aims to increase human readability for people lacking highlevel language skills, either because of age, education level, unfamiliarity with the language, or disability. Historically, generating this text has been done by hand, which is time consuming and expensive, especially when dealing with material that requires expertise, such as legal documents. Most current automatic TS systems rely on handwritten rules, e.g., PEST (Carroll et al., 1999), its SYSTAR module (Canning et al., 2000), and the method described by Siddharthan (2006). Systems using handwritten rules can be susceptible to changes in domains and need to be modified for each new domain or language. There has been some research into automatically learning the rules for simplifying text using aligned corpora (Daelemans et al., 2004; Yatskar et al., 2010), but these have yet to match the performance hand-crafted rule systems. An example of a manually simplified sentence can be found in table 1. The second type of TS has the goal of increasing the machine readability of text to aid tasks such as information extraction, machine translation, generative summarizati</context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>A. Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language &amp; Computation, 4(1):77–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="7450" citStr="Smith et al., 2010" startWordPosition="1167" endWordPosition="1170">This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simpl</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Versley</author>
<author>S P Ponzetto</author>
<author>M Poesio</author>
<author>V Eidelman</author>
<author>A Jern</author>
<author>J Smith</author>
<author>X Yang</author>
<author>A Moschitti</author>
</authors>
<title>BART: A modular toolkit for coreference resolution. In Association for Computational Linguistics (ACL) Demo Session.</title>
<date>2008</date>
<contexts>
<context position="7496" citStr="Versley et al., 2008" startWordPosition="1174" endWordPosition="1177"> retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathema</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman, A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008. BART: A modular toolkit for coreference resolution. In Association for Computational Linguistics (ACL) Demo Session.</rawString>
</citation>
<citation valid="true">
<title>Word book,</title>
<date>2009</date>
<institution>Voice Of America.</institution>
<note>edition. www.voaspecialenglish.com,</note>
<contexts>
<context position="10782" citStr="(2009)" startWordPosition="1696" endWordPosition="1696">he main text body of each article and does not consider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From each piece of text, we extracted a set of features designed to capture differenc</context>
<context position="12576" citStr="(2009)" startWordPosition="1996" endWordPosition="1996">on lists of basic English words, such as the Voice of America Special English words list (Voice Of America, 2009) or the Ogden Basic English list (Ogden, 1930). Idioms should be avoided as well as compounds and the passive voice as opposed to a single simple verb. To capture these properties in the text, we created four classes of features: lexical, part-of-speech, surface, and parse. Several of our features have previously been used for measuring text fluency (Aluisio et al., 2008; Chae and Nenkova, 2009; Feng et al., 2009; Petersen and Ostendorf, 2007). Lexical. Previous work by Feng et al. (2009) suggests that the document vocabulary is a good predictor of document readability. Simple texts are more likely to use basic words more often as opposed to more complicated, domain-specific words used in ordinary texts. To capture these features we used a unigram bag-of-words representation. We note that lexical features are unlikely to be useful unless we have access to a large training corpus that allowed the estimation of the relative frequency of words (Chae and Nenkova, 2009). Additionally, we can expect lexical features to be very fragile for crossdomain experiments as they are especial</context>
<context position="16588" citStr="(2009)" startWordPosition="2642" endWordPosition="2642">a tenth of the words in the ordinary document are. Additionally, the productivity of words, particularly non-BE850 words, is much higher in the ordinary document. There are also clear differences in the length of the documents, and on average documents from ordinary Wikipedia are more than four times longer than documents from Simple Wikipedia. Syntactic parse. As previously mentioned, a number of Wikipedia’s writing guidelines focus on general grammatical rules of sentence structure. Evidence of these rules may be captured in the syntactic parse of the sentences in the text. Chae and Nenkova (2009) studied text fluency in the context of machine translation and found strong correlations between parse tree structures and sentence fluency. In order to represent the structural complexity of the text, we collected extracted features from the parse trees. Our features included the frequency and length of noun phrases, verb phrases, prepositional phrases, and relative clauses (including embedded structures). We also considered relative ratios, such as the ratio of noun to verb phrases, prepositional to noun phrases, and relative clauses to noun phrases. We used the length of the longest noun p</context>
</contexts>
<marker>2009</marker>
<rawString>Voice Of America. 2009. Word book, 2009 edition. www.voaspecialenglish.com, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<title>Simple Wikipedia English. http://en.wikipedia.org/wiki/Citing Wikipedia,</title>
<date>2009</date>
<marker>Wikipedia, 2009</marker>
<rawString>Wikipedia. 2009. Simple Wikipedia English. http://en.wikipedia.org/wiki/Citing Wikipedia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
</authors>
<title>Coreference resolution using semantic relatedness information from automatically discovered patterns.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7516" citStr="Yang and Su, 2007" startWordPosition="1178" endWordPosition="1181">s in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathematics at the Universi</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>X. Yang and J. Su. 2007. Coreference resolution using semantic relatedness information from automatically discovered patterns. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Experiments with unsupervised extraction of lexical simplifications.</title>
<date>2010</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="3096" citStr="Yatskar et al., 2010" startWordPosition="475" endWordPosition="478">as been done by hand, which is time consuming and expensive, especially when dealing with material that requires expertise, such as legal documents. Most current automatic TS systems rely on handwritten rules, e.g., PEST (Carroll et al., 1999), its SYSTAR module (Canning et al., 2000), and the method described by Siddharthan (2006). Systems using handwritten rules can be susceptible to changes in domains and need to be modified for each new domain or language. There has been some research into automatically learning the rules for simplifying text using aligned corpora (Daelemans et al., 2004; Yatskar et al., 2010), but these have yet to match the performance hand-crafted rule systems. An example of a manually simplified sentence can be found in table 1. The second type of TS has the goal of increasing the machine readability of text to aid tasks such as information extraction, machine translation, generative summarization, and other text generation 42 Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 42–50, Los Angeles, California, June 2010. @2010 Association for Computational Linguistics tasks for selecting and evaluating the best candidate output text. In mac</context>
<context position="7763" citStr="Yatskar et al. (2010)" startWordPosition="1216" endWordPosition="1219">provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. 43 Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathematics at the University of Cambridge (a position that Isaac Newton once had). He retired on October 1st 2009. Table 2: Comparable sentences from the ordinary Wikipedia and Simple Wikipedia entry for “Stephen Hawking.” What makes Wikipedia an excellent resource for tex</context>
<context position="29420" citStr="Yatskar et al., 2010" startWordPosition="4660" endWordPosition="4663">rning can draw from machine translation, which learns rules that translate between languages. The related task of paraphrase extraction could also provide comparable phrases, one of which can be identified as a simplified version of the other (Bannard and Callison-Burch, 2005). An additional resource available in Simple Wikipedia is the flagging of articles as not simple. By examining the revision history of articles whose flags have been changed, we can discover changes that simplified texts. Initial work on this topic has automatically learned which edits correspond to text simplifications (Yatskar et al., 2010). Text simplification may necessitate the removal of whole phrases, sentences, or even paragraphs, as, according to the writing guidelines for Wikipedia Simple (Wikipedia, 2009), the articles should not exceed a specified length, and some concepts may not be explainable using the lexicon of Basic English. In some situations, adding new text to explain confusing but crucial points may serve to aid the reader, and text generation needs to be further investigated to make text simplification an automatic process. Acknowledgements The authors would like to thank Mary Harper for her help in parsing </context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Experiments with unsupervised extraction of lexical simplifications. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ye</author>
<author>M Zhou</author>
<author>C Y Lin</author>
</authors>
<title>Sentence level machine translation evaluation as a ranking problem: one step aside from BLEU.</title>
<date>2007</date>
<booktitle>In ACL Workshop on statistical machine translation.</booktitle>
<contexts>
<context position="4081" citStr="Ye et al., 2007" startWordPosition="633" endWordPosition="636"> HLT 2010 Workshop on Computational Linguistics and Writing, pages 42–50, Los Angeles, California, June 2010. @2010 Association for Computational Linguistics tasks for selecting and evaluating the best candidate output text. In machine translation, the evaluation tool most commonly used for evaluating output, the BLEU score (Papineni et al., 2001), rates the “goodness” of output based on n-gram overlap with human-generated text. However this metric has been criticized for not accurately measuring the fluency of text and there is active research into other metrics (Callison-Burch et al., 2006; Ye et al., 2007). Previous studies suggest that text simplified for machine and human comprehension are categorically different (Chae and Nenkova, 2009). Our research considers text simplified for human readers, but the findings can be used to identify features that discriminate simple text for both applications. The process of TS can be divided into three aspects: removing extraneous or superfluous text, substituting more complex lexical and syntactic forms, and inserting information to offer further clarification where needed (Aluisio et al., 2008). In this regard, TS is related to several different natural</context>
</contexts>
<marker>Ye, Zhou, Lin, 2007</marker>
<rawString>Y. Ye, M. Zhou, and C.Y. Lin. 2007. Sentence level machine translation evaluation as a ranking problem: one step aside from BLEU. In ACL Workshop on statistical machine translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>