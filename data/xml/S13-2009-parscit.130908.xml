<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008828">
<title confidence="0.996023">
ManTIME: Temporal expression identification and normalization in the
TempEval-3 challenge
</title>
<author confidence="0.999557">
Michele Filannino, Gavin Brown, Goran Nenadic
</author>
<affiliation confidence="0.999755">
The University of Manchester
School of Computer Science
</affiliation>
<address confidence="0.995268">
Manchester, M13 9PL, UK
</address>
<email confidence="0.99858">
{m.filannino, g.brown, g.nenadic}@cs.man.ac.uk
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956541666667">
This paper describes a temporal expression
identification and normalization system, Man-
TIME, developed for the TempEval-3 chal-
lenge. The identification phase combines
the use of conditional random fields along
with a post-processing identification pipeline,
whereas the normalization phase is carried out
using NorMA, an open-source rule-based tem-
poral normalizer. We investigate the perfor-
mance variation with respect to different fea-
ture types. Specifically, we show that the use
of WordNet-based features in the identifica-
tion task negatively affects the overall perfor-
mance, and that there is no statistically sig-
nificant difference in using gazetteers, shal-
low parsing and propositional noun phrases
labels on top of the morphological features.
On the test data, the best run achieved 0.95
(P), 0.85 (R) and 0.90 (F1) in the identifica-
tion phase. Normalization accuracies are 0.84
(type attribute) and 0.77 (value attribute). Sur-
prisingly, the use of the silver data (alone or in
addition to the gold annotated ones) does not
improve the performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984052631579">
Temporal information extraction (Verhagen et al.,
2007; Verhagen et al., 2010) is pivotal for many Nat-
ural Language Processing (NLP) applications such
as question answering, text summarization and ma-
chine translation. Recently the topic aroused in-
creasing interest also in the medical domain (Sun et
al., 2013; Kova´cevi´c et al., 2013).
Following the work of Ahn et al. (2005), the
temporal expression extraction task is now conven-
tionally divided into two main steps: identification
and normalization. In the former step, the effort
is concentrated on how to detect the right bound-
ary of temporal expressions in the text. In the nor-
malization step, the aim is to interpret and repre-
sent the temporal meaning of the expressions using
TimeML (Pustejovsky et al., 2003) format. In the
TempEval-3 challenge (UzZaman et al., 2012) the
normalization task is focused only on two temporal
attributes: type and value.
</bodyText>
<sectionHeader confidence="0.869431" genericHeader="introduction">
2 System architecture
</sectionHeader>
<bodyText confidence="0.992104">
ManTIME mainly consists of two components, one
for the identification and one for the normalization.
</bodyText>
<subsectionHeader confidence="0.927236">
2.1 Identification
</subsectionHeader>
<bodyText confidence="0.999830882352941">
We tackled the problem of identification as a se-
quencing labeling task leading to the choice of Lin-
ear Conditional Random Fields (CRF) (Lafferty et
al., 2001). We trained the system using both human-
annotated data (TimeBank and AQUAINT corpora)
and silver data (TE3Silver corpus) provided by the
organizers of the challenge in order to investigate the
importance of the silver data.
Because the silver data are far more numerous
(660K tokens vs. 95K), our main goal was to rein-
force the human-annotated data, under the assump-
tion that they are more informative with respect to
the training phase. Similarly to the approach pro-
posed by Adafre and de Rijke (2005), we developed
a post-processing pipeline on top of the CRF se-
quence labeler to boost the results. Below we de-
scribe each component in detail.
</bodyText>
<page confidence="0.978417">
53
</page>
<bodyText confidence="0.8458235">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 53–57, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.903167">
2.1.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.988317857142857">
The success of applying CRFs mainly depends on
three factors: the labeling scheme (BI, BIO, BIOE
or BIOEU), the topology of the factor graph and
the quality of the features used. We used the BIO
format in all the experiments performed during this
research. The factor graph has been generated us-
ing the following topology: (w0), (w−1), (w−2),
</bodyText>
<equation confidence="0.987725333333333">
(w+1), (w+2), (w−2∧w−1), (w−1∧w0), (w0∧w+1),
(w−1 ∧w0 ∧w+1), (w0 ∧w+1 ∧w+2), (w+1 ∧w+2),
(w−2 ∧ w−1 ∧ w0), (w−1 ∧ w+1) and (w−2 ∧ w+2).
</equation>
<bodyText confidence="0.999811666666667">
The system tokenizes each document in the cor-
pus and extracts 94 features. These belong to the
following four disjoint categories:
</bodyText>
<listItem confidence="0.973737">
• Morphological: This set includes a compre-
hensive list of features typical of Named En-
tity Recognition (NER) tasks, such as the word
as it is, lemma, stem, pattern (e.g. ’Jan-2003’:
</listItem>
<bodyText confidence="0.998441782608696">
’Xxx-dddd’), collapsed pattern (e.g. ’Jan-
2003’: ’Xx-d’), first 3 characters, last 3 charac-
ters, upper first character, presence of ’s’ as last
character, word without letters, word without
letters or numbers, and verb tense. For lemma
and POS tags we use TreeTagger (Schmid,
1994). Boolean values are included, indicating
if the word is lower-case, alphabetic, digit, al-
phanumeric, titled, capitalized, acronym (cap-
italized with dots), number, decimal number,
number with dots or stop-word. Additionally,
there are features specifically crafted to han-
dle temporal expressions in the form of regu-
lar expression matching: cardinal and ordinal
numbers, times, dates, temporal periods (e.g.
morning, noon, nightfall), day of the week, sea-
sons, past references (e.g. ago, recent, before),
present references (e.g. current, now), future
references (e.g. tomorrow, later, ahead), tem-
poral signals (e.g. since, during), fuzzy quan-
tifiers (e.g. about, few, some), modifiers, tem-
poral adverbs (e.g. daily, earlier), adjectives,
conjunctions and prepositions.
</bodyText>
<listItem confidence="0.9955015">
• Syntactic: Chunks and propositional noun
phrases belong to this category. Both are
extracted using the shallow parsing software
MBSP1.
</listItem>
<footnote confidence="0.506015">
1http://www.clips.ua.ac.be/software/mbsp-for-python
</footnote>
<listItem confidence="0.931078210526316">
• Gazetteers: These features are expressed us-
ing the BIO format because they can include
expressions longer than one word. The inte-
grated gazetteers are: male and female names,
U.S. cities, nationalities, world festival names
and ISO countries.
• WordNet: For each word we use the number of
senses associated to the word, the first and the
second sense name, the first 4 lemmas, the first
4 entailments for verbs, the first 4 antonyms,
the first 4 hypernyms and the first 4 hyponyms.
Each of them is defined as a separate feature.
The features mentioned above have been com-
bined in 4 different models:
• Model 1: Morphological only
• Model 2: Morphological + syntactic
• Model 3: Morphological + gazetteers
• Model 4: Morphological + gazetteers + Word-
Net
</listItem>
<bodyText confidence="0.892546666666667">
All the experiments have been carried out using
CRF++ 0.572 with parameters C = 1, q = 0.0001
and L2-regularization function.
</bodyText>
<subsectionHeader confidence="0.530088">
2.1.2 Model selection
</subsectionHeader>
<bodyText confidence="0.999984444444444">
The model selection was performed over the
entire training corpus. Silver data and human-
annotated data were merged, shuffled at sentence-
level (seed = 490) and split into two sets: 80% as
cross-validation set and 20% as real-world test set.
The cross-validation set was shuffled 5 times, and
for each of these, the 10-fold cross validation tech-
nique was applied.
The analysis is statistically significant (p =
0.0054 with ANOVA test) and provides two impor-
tant outcomes: (i) the set of WordNet features nega-
tively affects the overall classification performance,
as suggested by Rigo et al. (2011). We believe this is
due to the sparseness of the labels: many tokens did
not have any associated WordNet sense. (ii) There
is no statistically significant difference among the
first three models, despite the presence of apparently
important information such as chunks, propositional
</bodyText>
<footnote confidence="0.977949">
2https://code.google.com/p/crfpp/
</footnote>
<page confidence="0.995768">
54
</page>
<figureCaption confidence="0.9912465">
Figure 1: Differences among models using 5x10-fold
cross-validation
</figureCaption>
<bodyText confidence="0.950986277777778">
noun phrases and gazetteers. The Figure 1 shows the
box plots for each model.
In virtue of this analysis, we opted for the smallest
feature set (Model 1) to prevent overfitting.
In order to get a reliable estimation of the perfor-
mance of the selected model on the real world data,
we trained it on the entire cross-validation set and
tested it against the real-word test set. The results
for all the models are shown in the following table:
System Pre. Rec. Fβ=1
Model 1 83.20 85.22 84.50
Model 2 83.57 85.12 84.33
Model 3 83.51 85.12 84.31
Model 4 83.15 84.44 83.79
Precision, Recall and F,a=1 score are computed
using strict matching.
The models used for the challenge have been
trained using the entire training set.
</bodyText>
<subsectionHeader confidence="0.773085">
2.1.3 Post-processing identification pipeline
</subsectionHeader>
<bodyText confidence="0.999975538461539">
Although CRFs already provide reasonable per-
formance, equally balanced in terms of precision
and recall, we focused on boosting the baseline per-
formance through a post-processing pipeline. For
this purpose, we introduced 3 different modules.
Probabilistic correction module averages the
probabilities from the trained CRFs model with the
ones extracted from human-annotated data only. For
each token, we extracted: (i) the conditional proba-
bility for each label to be assigned (B, I or O), and
(ii) the prior probability of the labels in the human-
annotated data only. The two probabilities are aver-
aged for every label of each token. The list of tokens
extracted in the human-annotated data was restricted
to those that appeared within the span of temporal
expressions at least twice. The application of this
module in some cases has the effect of changing the
most likely label leading to an improvement of re-
call, although its major advantage is making CRFs
predictions less strict.
BIO fixer fixes wrong label sequences. For the
BIO labeling scheme, the sequence O-I is necessar-
ily wrong. We identified B-I as the appropriate sub-
stitution. This is the case in which the first token
has been incorrectly annotated (e.g. “Three/O days/I
ago/I ./O” is converted into “Three/B days/I ago/I
./O”). We also merged close expressions such as B-
B or I-B, because different temporal expressions are
generally divided at least by a symbol or a punctu-
ation character (e.g. “Wednesday/B morning/B” is
converted into “Wednesday/B morning/I”).
Threshold-based label switcher uses the prob-
abilities extracted from the human-annotated data.
When the most likely label (in the human-annotated
data) has a prior probability greater than a certain
threshold, the module changes the CRFs predicted
label to the most likely one. This leads to force
the probabilities learned from the human-annotated
data.
Through repeated empirical experiments on a
small sub-set of the training data, we found an
optimal threshold value (0.87) and an optimal se-
quence of pipeline components (Probabilistic cor-
rection module, BIO fixer, Threshold-based label
switcher, BIO fixer).
We analyzed the effectiveness of the post-
processing identification pipeline using a 10-fold
cross-validation over the 4 models. The difference
between CRFs and CRFs + post-processing pipeline
is statistically significant (p = 3.51 x 10−23 with
paired T-test) and the expected average increment is
2.27% with respect to the strict FO=1 scores.
</bodyText>
<subsectionHeader confidence="0.984685">
2.2 Normalization
</subsectionHeader>
<bodyText confidence="0.999868">
The normalization component is an updated version
of NorMA (Filannino, 2012), an open-source rule-
based system.
</bodyText>
<page confidence="0.994889">
55
</page>
<table confidence="0.9981183">
# Training data Identification Normalization Overall
run (post-processing) score
Strict matching Lenient matching Accuracy
Pre. Rec. Fβ_1 Pre. Rec. �Fβ_1 Type Value
1 Human&amp;Silver (no) 78.57 63.77 70.40 97.32 78.99 87.20 88.99 77.06 67.20
2 Human&amp;Silver (yes) 79.82 65.94 72.22 97.37 80.43 88.10 87.38 75.68 66.67
3 Human (no) 76.07 64.49 69.80 94.87 80.43 87.06 87.39 77.48 67.45
4 Human (yes) 78.86 70.29 74.33 95.12 84.78 89.66 86.31 76.92 68.97
5 Silver (no) 77.68 63.04 69.60 97.32 78.99 87.20 88.99 77.06 67.20
6 Silver (yes) 81.98 65.94 73.09 98.20 78.99 87.55 90.83 77.98 68.27
</table>
<tableCaption confidence="0.999909">
Table 1: Performance on the TempEval-3 test set.
</tableCaption>
<sectionHeader confidence="0.997493" genericHeader="background">
3 Results and Discussion 4 Conclusions
</sectionHeader>
<bodyText confidence="0.999950433962264">
We submitted six runs as combinations of different
training sets and the use of the post-processing iden-
tification pipeline. The results are shown in Table 1
where the overall score is computed as multiplica-
tion between lenient F�_i score and the value accu-
racy.
In all the runs, recall is lower than precision. This
is an indication of a moderate lexical difference be-
tween training data and test data. The relatively low
type accuracy testifies the normalizer’s inability to
recognize new lexical patterns. Among the correctly
typed temporal expressions, there is still about 10%
of them for which an incorrect value is provided.
The normalization task is proved to be challenging.
The training of the system by using human-
annotated data only, in addition to the post-
processing pipeline, provided the best results, al-
though not the highest normalization accuracy. Sur-
prisingly, the silver data do not improve the per-
formance, both when used alone or in addition
to human-annotated data (regardless of the post-
processing pipeline usage).
The post-processing pipeline produces the high-
est precision when applied to the silver data only.
In this case, the pipeline acts as a reinforcement of
the human-annotated data. As expected, the post-
processing pipeline boosts the performance of both
precision and recall. We registered the best improve-
ment with the human-annotated data.
Due to the small number of temporal expressions
in the test set (138), further analysis is required to
draw more general conclusions.
We described the overall architecture of ManTIME,
a temporal expression extraction pipeline, in the
context of TempEval-3 challenge.
This research shows, in the limits of its general-
ity, the primary and exhaustive importance of mor-
phological features to the detriment of syntactic fea-
tures, as well as gazetteer and WordNet-related ones.
In particular, while syntactic and gazetteer-related
features do not affect the performance, WordNet-
related features affect it negatively.
The research also proves the use of a post-
processing identification pipeline to be promising
for both precision and recall enhancement.
Finally, we found out that the silver data do not
improve the performance, although we consider the
test set too small for this result to be generalizable.
To aid replicability of this work, the system
code, machine learning pre-trained models, statis-
tical validation details and an online DEMO are
available at: http://www.cs.man.ac.uk/
˜filannim/projects/tempeval-3/
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999764142857143">
We would like to thank the organizers of the
TempEval-3 challenge. The first author would like
also to acknowledge Marilena Di Bari, Joseph Mel-
lor and Daniel Jamieson for their support and the UK
Engineering and Physical Science Research Coun-
cil for its support in the form of a doctoral training
grant.
</bodyText>
<page confidence="0.995799">
56
</page>
<sectionHeader confidence="0.984433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.931634258064516">
Sisay Fissaha Adafre and Maarten de Rijke. 2005. Fea-
ture engineering and post-processing for temporal ex-
pression recognition using conditional random fields.
In Proceedings of the ACL Workshop on Feature En-
gineering for Machine Learning in Natural Language
Processing, FeatureEng ’05, pages 9–16, Stroudsburg,
PA, USA. Association for Computational Linguistics.
David Ahn, Sisay Fissaha Adafre, and Maarten de Ri-
jke. 2005. Towards task-based temporal extraction
and recognition. In Graham Katz, James Pustejovsky,
and Frank Schilder, editors, Annotating, Extracting
and Reasoning about Time and Events, number 05151
in Dagstuhl Seminar Proceedings, Dagstuhl, Germany.
Internationales Begegnungs- und Forschungszentrum
f¨ur Informatik (IBFI), Schloss Dagstuhl, Germany.
Michele Filannino. 2012. Temporal expression
normalisation in natural language texts. CoRR,
abs/1206.2010.
Aleksandar Kova´cevi´c, Azad Dehghan, Michele Filan-
nino, John A Keane, and Goran Nenadic. 2013. Com-
bining rules and machine learning for extraction of
temporal expressions and events from clinical narra-
tives. Journal of American Medical Informatics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282–289.
James Pustejovsky, Jos´e Casta˜no, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. Timeml: Robust specification of event
and temporal expressions in text. In in Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5.
Stefan Rigo and Alberto Lavelli. 2011. Multisex - a
multi-language timex sequential extractor. In Tempo-
ral Representation and Reasoning (TIME), 2011 Eigh-
teenth International Symposium on, pages 163–170.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012
i2b2 challenge. Journal of the American Medical In-
formatics Association.
Naushad UzZaman, Hector Llorens, James F. Allen,
Leon Derczynski, Marc Verhagen, and James Puste-
jovsky. 2012. Tempeval-3: Evaluating events,
time expressions, and temporal relations. CoRR,
abs/1206.5333.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 75–
80, Prague.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, SemEval
’10, pages 57–62, Stroudsburg, PA, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.99914">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.796581">
<title confidence="0.886261">ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge</title>
<author confidence="0.999715">Michele Filannino</author>
<author confidence="0.999715">Gavin Brown</author>
<author confidence="0.999715">Goran</author>
<affiliation confidence="0.999597">The University of School of Computer</affiliation>
<address confidence="0.986634">Manchester, M13 9PL,</address>
<email confidence="0.986408">g.brown,</email>
<abstract confidence="0.999223">This paper describes a temporal expression identification and normalization system, Man- TIME, developed for the TempEval-3 challenge. The identification phase combines the use of conditional random fields along with a post-processing identification pipeline, whereas the normalization phase is carried out using NorMA, an open-source rule-based temporal normalizer. We investigate the performance variation with respect to different feature types. Specifically, we show that the use of WordNet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. On the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identification phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sisay Fissaha Adafre</author>
<author>Maarten de Rijke</author>
</authors>
<title>Feature engineering and post-processing for temporal expression recognition using conditional random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing, FeatureEng ’05,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Adafre, de Rijke, 2005</marker>
<rawString>Sisay Fissaha Adafre and Maarten de Rijke. 2005. Feature engineering and post-processing for temporal expression recognition using conditional random fields. In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing, FeatureEng ’05, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Ahn</author>
</authors>
<title>Sisay Fissaha Adafre, and Maarten de Rijke.</title>
<date>2005</date>
<booktitle>Annotating, Extracting and Reasoning about Time and Events, number 05151 in Dagstuhl Seminar Proceedings, Dagstuhl, Germany. Internationales Begegnungs- und Forschungszentrum f¨ur Informatik (IBFI), Schloss Dagstuhl,</booktitle>
<editor>Graham Katz, James Pustejovsky, and Frank Schilder, editors,</editor>
<location>Germany.</location>
<marker>Ahn, 2005</marker>
<rawString>David Ahn, Sisay Fissaha Adafre, and Maarten de Rijke. 2005. Towards task-based temporal extraction and recognition. In Graham Katz, James Pustejovsky, and Frank Schilder, editors, Annotating, Extracting and Reasoning about Time and Events, number 05151 in Dagstuhl Seminar Proceedings, Dagstuhl, Germany. Internationales Begegnungs- und Forschungszentrum f¨ur Informatik (IBFI), Schloss Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Filannino</author>
</authors>
<title>Temporal expression normalisation in natural language texts.</title>
<date>2012</date>
<location>CoRR, abs/1206.2010.</location>
<contexts>
<context position="10749" citStr="Filannino, 2012" startWordPosition="1672" endWordPosition="1673">we found an optimal threshold value (0.87) and an optimal sequence of pipeline components (Probabilistic correction module, BIO fixer, Threshold-based label switcher, BIO fixer). We analyzed the effectiveness of the postprocessing identification pipeline using a 10-fold cross-validation over the 4 models. The difference between CRFs and CRFs + post-processing pipeline is statistically significant (p = 3.51 x 10−23 with paired T-test) and the expected average increment is 2.27% with respect to the strict FO=1 scores. 2.2 Normalization The normalization component is an updated version of NorMA (Filannino, 2012), an open-source rulebased system. 55 # Training data Identification Normalization Overall run (post-processing) score Strict matching Lenient matching Accuracy Pre. Rec. Fβ_1 Pre. Rec. �Fβ_1 Type Value 1 Human&amp;Silver (no) 78.57 63.77 70.40 97.32 78.99 87.20 88.99 77.06 67.20 2 Human&amp;Silver (yes) 79.82 65.94 72.22 97.37 80.43 88.10 87.38 75.68 66.67 3 Human (no) 76.07 64.49 69.80 94.87 80.43 87.06 87.39 77.48 67.45 4 Human (yes) 78.86 70.29 74.33 95.12 84.78 89.66 86.31 76.92 68.97 5 Silver (no) 77.68 63.04 69.60 97.32 78.99 87.20 88.99 77.06 67.20 6 Silver (yes) 81.98 65.94 73.09 98.20 78.99 </context>
</contexts>
<marker>Filannino, 2012</marker>
<rawString>Michele Filannino. 2012. Temporal expression normalisation in natural language texts. CoRR, abs/1206.2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleksandar Kova´cevi´c</author>
<author>Azad Dehghan</author>
<author>Michele Filannino</author>
<author>John A Keane</author>
<author>Goran Nenadic</author>
</authors>
<title>Combining rules and machine learning for extraction of temporal expressions and events from clinical narratives.</title>
<date>2013</date>
<journal>Journal of American Medical Informatics.</journal>
<marker>Kova´cevi´c, Dehghan, Filannino, Keane, Nenadic, 2013</marker>
<rawString>Aleksandar Kova´cevi´c, Azad Dehghan, Michele Filannino, John A Keane, and Goran Nenadic. 2013. Combining rules and machine learning for extraction of temporal expressions and events from clinical narratives. Journal of American Medical Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2554" citStr="Lafferty et al., 2001" startWordPosition="382" endWordPosition="385"> temporal expressions in the text. In the normalization step, the aim is to interpret and represent the temporal meaning of the expressions using TimeML (Pustejovsky et al., 2003) format. In the TempEval-3 challenge (UzZaman et al., 2012) the normalization task is focused only on two temporal attributes: type and value. 2 System architecture ManTIME mainly consists of two components, one for the identification and one for the normalization. 2.1 Identification We tackled the problem of identification as a sequencing labeling task leading to the choice of Linear Conditional Random Fields (CRF) (Lafferty et al., 2001). We trained the system using both humanannotated data (TimeBank and AQUAINT corpora) and silver data (TE3Silver corpus) provided by the organizers of the challenge in order to investigate the importance of the silver data. Because the silver data are far more numerous (660K tokens vs. 95K), our main goal was to reinforce the human-annotated data, under the assumption that they are more informative with respect to the training phase. Similarly to the approach proposed by Adafre and de Rijke (2005), we developed a post-processing pipeline on top of the CRF sequence labeler to boost the results.</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e Casta˜no</author>
<author>Robert Ingria</author>
<author>Roser Sauri</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
</authors>
<title>Timeml: Robust specification of event and temporal expressions in text.</title>
<date>2003</date>
<booktitle>In in Fifth International Workshop on Computational Semantics (IWCS5.</booktitle>
<marker>Pustejovsky, Casta˜no, Ingria, Sauri, Gaizauskas, Setzer, Katz, 2003</marker>
<rawString>James Pustejovsky, Jos´e Casta˜no, Robert Ingria, Roser Sauri, Robert Gaizauskas, Andrea Setzer, and Graham Katz. 2003. Timeml: Robust specification of event and temporal expressions in text. In in Fifth International Workshop on Computational Semantics (IWCS5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Rigo</author>
<author>Alberto Lavelli</author>
</authors>
<title>Multisex - a multi-language timex sequential extractor.</title>
<date>2011</date>
<booktitle>In Temporal Representation and Reasoning (TIME), 2011 Eighteenth International Symposium on,</booktitle>
<pages>163--170</pages>
<marker>Rigo, Lavelli, 2011</marker>
<rawString>Stefan Rigo and Alberto Lavelli. 2011. Multisex - a multi-language timex sequential extractor. In Temporal Representation and Reasoning (TIME), 2011 Eighteenth International Symposium on, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="4556" citStr="Schmid, 1994" startWordPosition="708" endWordPosition="709">) and (w−2 ∧ w+2). The system tokenizes each document in the corpus and extracts 94 features. These belong to the following four disjoint categories: • Morphological: This set includes a comprehensive list of features typical of Named Entity Recognition (NER) tasks, such as the word as it is, lemma, stem, pattern (e.g. ’Jan-2003’: ’Xxx-dddd’), collapsed pattern (e.g. ’Jan2003’: ’Xx-d’), first 3 characters, last 3 characters, upper first character, presence of ’s’ as last character, word without letters, word without letters or numbers, and verb tense. For lemma and POS tags we use TreeTagger (Schmid, 1994). Boolean values are included, indicating if the word is lower-case, alphabetic, digit, alphanumeric, titled, capitalized, acronym (capitalized with dots), number, decimal number, number with dots or stop-word. Additionally, there are features specifically crafted to handle temporal expressions in the form of regular expression matching: cardinal and ordinal numbers, times, dates, temporal periods (e.g. morning, noon, nightfall), day of the week, seasons, past references (e.g. ago, recent, before), present references (e.g. current, now), future references (e.g. tomorrow, later, ahead), tempora</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiyi Sun</author>
<author>Anna Rumshisky</author>
<author>Ozlem Uzuner</author>
</authors>
<title>Evaluating temporal relations in clinical text: 2012 i2b2 challenge.</title>
<date>2013</date>
<journal>Journal of the American Medical Informatics Association.</journal>
<contexts>
<context position="1652" citStr="Sun et al., 2013" startWordPosition="237" endWordPosition="240">ta, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identification phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance. 1 Introduction Temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010) is pivotal for many Natural Language Processing (NLP) applications such as question answering, text summarization and machine translation. Recently the topic aroused increasing interest also in the medical domain (Sun et al., 2013; Kova´cevi´c et al., 2013). Following the work of Ahn et al. (2005), the temporal expression extraction task is now conventionally divided into two main steps: identification and normalization. In the former step, the effort is concentrated on how to detect the right boundary of temporal expressions in the text. In the normalization step, the aim is to interpret and represent the temporal meaning of the expressions using TimeML (Pustejovsky et al., 2003) format. In the TempEval-3 challenge (UzZaman et al., 2012) the normalization task is focused only on two temporal attributes: type and value</context>
</contexts>
<marker>Sun, Rumshisky, Uzuner, 2013</marker>
<rawString>Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2 challenge. Journal of the American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>James F Allen</author>
<author>Leon Derczynski</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Tempeval-3: Evaluating events, time expressions, and temporal relations.</title>
<date>2012</date>
<location>CoRR, abs/1206.5333.</location>
<contexts>
<context position="2170" citStr="UzZaman et al., 2012" startWordPosition="322" endWordPosition="325">ranslation. Recently the topic aroused increasing interest also in the medical domain (Sun et al., 2013; Kova´cevi´c et al., 2013). Following the work of Ahn et al. (2005), the temporal expression extraction task is now conventionally divided into two main steps: identification and normalization. In the former step, the effort is concentrated on how to detect the right boundary of temporal expressions in the text. In the normalization step, the aim is to interpret and represent the temporal meaning of the expressions using TimeML (Pustejovsky et al., 2003) format. In the TempEval-3 challenge (UzZaman et al., 2012) the normalization task is focused only on two temporal attributes: type and value. 2 System architecture ManTIME mainly consists of two components, one for the identification and one for the normalization. 2.1 Identification We tackled the problem of identification as a sequencing labeling task leading to the choice of Linear Conditional Random Fields (CRF) (Lafferty et al., 2001). We trained the system using both humanannotated data (TimeBank and AQUAINT corpora) and silver data (TE3Silver corpus) provided by the organizers of the challenge in order to investigate the importance of the silve</context>
</contexts>
<marker>UzZaman, Llorens, Allen, Derczynski, Verhagen, Pustejovsky, 2012</marker>
<rawString>Naushad UzZaman, Hector Llorens, James F. Allen, Leon Derczynski, Marc Verhagen, and James Pustejovsky. 2012. Tempeval-3: Evaluating events, time expressions, and temporal relations. CoRR, abs/1206.5333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>75--80</pages>
<location>Prague.</location>
<contexts>
<context position="1397" citStr="Verhagen et al., 2007" startWordPosition="197" endWordPosition="200">res in the identification task negatively affects the overall performance, and that there is no statistically significant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. On the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identification phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance. 1 Introduction Temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010) is pivotal for many Natural Language Processing (NLP) applications such as question answering, text summarization and machine translation. Recently the topic aroused increasing interest also in the medical domain (Sun et al., 2013; Kova´cevi´c et al., 2013). Following the work of Ahn et al. (2005), the temporal expression extraction task is now conventionally divided into two main steps: identification and normalization. In the former step, the effort is concentrated on how to detect the right boundary of temporal expressions in the text. In the normalization step, the</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75– 80, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 13: Tempeval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>57--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1421" citStr="Verhagen et al., 2010" startWordPosition="201" endWordPosition="204">on task negatively affects the overall performance, and that there is no statistically significant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. On the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identification phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance. 1 Introduction Temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010) is pivotal for many Natural Language Processing (NLP) applications such as question answering, text summarization and machine translation. Recently the topic aroused increasing interest also in the medical domain (Sun et al., 2013; Kova´cevi´c et al., 2013). Following the work of Ahn et al. (2005), the temporal expression extraction task is now conventionally divided into two main steps: identification and normalization. In the former step, the effort is concentrated on how to detect the right boundary of temporal expressions in the text. In the normalization step, the aim is to interpret and</context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. Semeval-2010 task 13: Tempeval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 57–62, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>