<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019112">
<title confidence="0.998271">
A Platform for Automated Acoustic Analysis for Assistive Technology
</title>
<author confidence="0.997802">
Suzanne Boyce Harriet Fell
</author>
<affiliation confidence="0.9853145">
Department of Communication Sciences and College of Computer and Information
Disorders Science
University of Cincinnati Northeastern University
Cincinnati, Ohio, 45267, USA Boston, Massachusetts, 02115, USA
</affiliation>
<email confidence="0.998168">
boycese@ucmail.uc.edu fell@ccs.neu.edu
</email>
<author confidence="0.999643">
Joel MacAuslan Lorin Wilde
</author>
<affiliation confidence="0.988079">
Speech Technology and Applied Research Boston University
</affiliation>
<address confidence="0.558714">
54 Middlesex Turnpike
Bedford, Massachusetts, , USA Boston, Massachusetts, 02180, USA
</address>
<email confidence="0.991657">
joelm@staranalyticalservices.com wildercom@gmail.com
</email>
<sectionHeader confidence="0.992914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999573785714286">
The use of speech production data has been
limited by a steep learning curve and the need
for laborious hand measurement. We are
building a tool set that provides summary sta-
tistics for measures designed by clinicians to
screen, diagnose or provide training to assis-
tive technology users. This will be achieved
by extending an existing shareware software
platform with “plug-ins” that perform specific
measures and report results to the user. The
common underlying basis for this tool set is a
Stevens’ paradigm of landmarks, points in an
utterance around which information about ar-
ticulatory events can be extracted.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995275">
To date, the use of speech production data has been
limited by a steep learning curve and the need for
laborious hand measurement. Many speech-related
studies result in voluminous acoustic data. Many
clinicians who design and use assistive technology
would like to incorporate acoustic analysis, but
have been discouraged because of these technical
challenges. We are in the process of developing a
set of tools that considerably streamlines the proc-
ess of analyzing speech production details.
We are building a tool set to provide summary sta-
tistics for measures designed by clinicians to
screen, diagnose or provide training to patients.
This will be achieved by extending an existing
shareware software platform with “plug-ins” that
perform specific measures and report results to the
user. At present, our goal is to use the existing
shareware software tool Wavesurfer (Wavesurfer,
2005). The new modules will be set up to report
data from a single audio file, or groups of audio
files in a standard table format, for easy input to
statistical or other analysis software. For example,
the data may be imported into a program that cor-
relates speech data with scalp electrode and medi-
cation data.
Our tool will include alternative and independently
tested algorithms for clinically relevant measures,
as well as guidance as to what the speech data may
mean.
The common underlying basis for this tool set is a
focused set of landmarks derived from Stevens’
Lexical Access from Features (LAFF) paradigm
(Stevens, 1992, 2002; Liu, 1995; Slifka et al.,
2004). In this approach, landmarks are points in an
utterance around which information about articula-
tory events can be extracted.
</bodyText>
<page confidence="0.994529">
37
</page>
<note confidence="0.756403">
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 37–43,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999955">
In what follows, we will describe (1) the theoreti-
cal rationale of landmarks, (2) the general utility of
landmark processing and several examples of clin-
ically related measures, and (3) our current work
on developing tools to make landmark analysis
more widely available.
</bodyText>
<sectionHeader confidence="0.878066" genericHeader="introduction">
2 Landmarks reflect articulation
</sectionHeader>
<bodyText confidence="0.999866771428571">
Landmark analysis is based on the fact that differ-
ent sounds produce different patterns of abrupt
changes in the acoustic signal simultaneously
across wide frequency ranges. For instance, the
abrupt increase in amplitude for a broad range of
frequencies above 3 kHz can be used to indicate
the onset of bursts. Likewise, an abrupt decrease
in the same frequency bands can be used to indi-
cate the end of frication. The use of onset and off-
set data in other frequency bands can be used to
indicate sonorancy; i.e., intervals when the oral
cavity is relatively unconstricted. Examples based
on Liu [1995] are listed below.
g(lottis): marks the onset (+g) or offset (-g) of
voicing.
s(yllabicity): marks the onset (+s) or offset (-s) of
syllabicity, i.e. onsets and releases of voiced sono-
rant consonants such as /l/ or /r/, vocal tract clo-
sures due to voiced stop consonants such as /b/ or
/d/.
b(urst): marks the onset (+b) of the burst of air
following stop or affricate consonant release, or the
onset of frication noise for fricative consonants.
Offsets (-b) mark points where aspiration or frica-
tion noise ends abruptly due to a stop closure.
V(owel): marks points of peak amplitude in a so-
norant region—that is, a region where voicing is
evident [Howitt, 2000].
Although much of the past work using landmark
processing has been focused on employing a wide
variety of landmarks to recognize the lexical con-
tent of speech [Juneja and Espy-Wilson 2003, Slif-
ka, et al. 2004], the power of these measures is
even more apparent when applied to non-lexical
attributes.
</bodyText>
<sectionHeader confidence="0.9372905" genericHeader="method">
3 Applications of Landmark Analysis to
Assistive Technology
</sectionHeader>
<subsectionHeader confidence="0.999969">
3.1 Tracking Articulatory Precision
</subsectionHeader>
<bodyText confidence="0.999996121212121">
Measuring articulatory precision is important to
evaluating efficacy of a treatment or in monitoring
disease progression, e.g. in Parkinson’s disease.
Given that landmarks reflect articulation, tools
based on landmarks may be useful for measuring
and monitoring articulatory precision [Boyce et al.
2005, 2007]. The technique relies on setting em-
pirically derived thresholds for the detection of
abrupt acoustic changes in specified frequency
bands. Recall that changes in the acoustic signal
occur simultaneously across wide frequency
ranges. When the onset of energy does not exceed
threshold in a particular frequency band, i.e., not
quite abrupt enough to trigger the detection of a
landmark, then no landmark may be assigned.
However, since different sounds produce different
patterns, changes detected in other bands at that
point in time are either a) assigned to a different
landmark, or b) considered to be extraneous. Thus,
small acoustic differences in the way speech is
produced can be tracked as different patterns of
landmarks.
In addition to requirements that a tool for general
clinical use must be fast and robust, it must be able
to handle a wide variety of speaking styles, dia-
lects, and voices. By focusing on landmarks that
specify syllable structure and broad phoneme
classes, distinctive differences between phonemes
can be ignored. Therefore, the tool is less likely to
break down due to problems recognizing specific
vocabulary while remaining sensitive to changes in
the acoustic signal that reflect articulatory preci-
sion of speech.
</bodyText>
<subsectionHeader confidence="0.999829">
3.2 Evaluating phonological complexity
</subsectionHeader>
<bodyText confidence="0.9999575">
Development of speech in early infancy includes
the ability to produce increasingly complex
phonological structure. Patterns of syllable struc-
ture in speech output can be tracked using land-
marks, again without reference to specific
phonemes or words. In Fell et al. [2002], land-
marks were grouped into standard syllable patterns
and syllables were grouped into utterances. Statis-
</bodyText>
<page confidence="0.995322">
38
</page>
<bodyText confidence="0.999979">
tics based on these patterns were then reported to
the clinician for various uses in training, screening
or diagnosis. Patterns of syllable complexity were
used to compute a &amp;quot;vocalization age.&amp;quot; This was
used in turn to derive screening rules that clinically
distinguish infants who may be at risk for later
communication or other developmental problems
from typically developing infants.
</bodyText>
<subsectionHeader confidence="0.9956135">
3.3 Measuring and Evaluating “Clear
Speech”
</subsectionHeader>
<bodyText confidence="0.9999555">
“Clear Speech” is an intelligibility-enhancing style
of speech that is used to improve communication
outcomes. Listeners with hearing impairment de-
rive significant benefit from being addressed with
clearly articulated speech. Speech that is more
clearly articulated contains more abrupt acoustic
changes. The result is that speech with different
levels of intelligibility shows different numbers
and combinations of landmarks [Boyce et al. 2005,
2007].
</bodyText>
<subsectionHeader confidence="0.942825">
3.4 Other Applications
</subsectionHeader>
<bodyText confidence="0.999950947368421">
In the UCARE project [1995], Cress reported ana-
lyzing 40 hours of pre-existing [2005] videotaped
sessions of children with physical or neurological
impairments using landmark-based tools.
Fell et al. [2004] reported using landmark analysis
to follow the progress of several children with se-
vere speech delays. In this project, 10-minute, in-
home audio recordings were processed in real-time
on a 2002-era PC laptop.
Wade and Möbius [2007] used automated land-
mark analysis to study speaking rate effects as a
measure of disease progression in Parkinson&apos;s dis-
ease.
DiCicco and Patel [2008] used automatic landmark
analysis on dysarthric speech. This study provides
quantitative support for the hypothesis [Deller
1991] that dysarthric speech includes erroneous
additional acoustic cues, not only malformed or
missing ones.
</bodyText>
<sectionHeader confidence="0.978684" genericHeader="method">
4 Potential Benefits of Landmark Appli-
cations
</sectionHeader>
<bodyText confidence="0.9999888">
In a small study, Warner-Czyz and Davis [2010]
compared consonant–vowel syllable accuracy in
early words of children with normal hearing and
children with hearing loss who received cochlear
implantation. They found and evaluated, via man-
ual coding, approximately 4000 syllables from 48
hours of recordings. This is a project where auto-
matic landmark analysis might have greatly re-
duced the effort.
Similarly, in a study on tongue-twisters, Matthew
Goldrick (Northwestern University) collected 100
hours of data comprising 20,000 tokens in less than
three weeks, but found that it required another 600
hours merely to segment and label the data for fur-
ther analysis. In personal correspondence about
another study on single words, he stated:
A major ‘choke point’ for speech production
research is the need to manually analyze
speech data. Given that many thousands of
data points are typically required to gain accu-
rate estimates of probability density functions
along phonetic dimensions, hundreds of per-
son-hours are typically required to analyze da-
ta from a single simple experiment.... If we
could gain access to reliable, highly accurate
automated tools, we could change the speed of
research by an order of magnitude.
Researchers who currently want to use speech
analysis as a tool must accept long periods of hand
measurements. This discourages researchers who
may be more interested in a particular neurological
disease or process than in speech research per se.
It is notoriously difficult to quantify projects not
undertaken, or papers not written, but it is telling
that, although each of the studies cited above re-
ported positive results from a study of speech ar-
ticulation, they exist as relative islands in their
respective disciplines. We contend that this situa-
tion exists largely because of barriers to entry; that
is, we believe that many scientists would like to
use speech assessment as part of their research, but
elect not to do for lack of a convenient tool. The
existence of a convenient tool to detect, measure
and track subtle changes in speech articulation
would constitute an enabling technology.
</bodyText>
<page confidence="0.998245">
39
</page>
<bodyText confidence="0.777804666666667">
For this version, we are implementing user controls
(“widgets”) to produce automated measures or
types of analyses for speech research such as:
</bodyText>
<figure confidence="0.784982666666667">
• Voice-onset time, VOT.
5 Tools
5.1 Description
</figure>
<bodyText confidence="0.999494571428571">
In our own work, we have developed an automatic
tool for detecting, counting and analyzing acoustic
events in the speech signal that are commonly used
by scientists to measure differences in speech ar-
ticulation.
We are now integrating our system with Wave-
surfer for certain researchers (linguists, speech-
language pathologists, certain engineering and
cognitive-science researchers) with a primary in-
terest in inspecting and interpreting the articula-
tion-related features in the waveforms of a corpus:
e.g., the placement of landmarks of each type, pat-
terns of clustering, or identification of non-speech
sounds to be excised. (See Figure 1).)
</bodyText>
<listItem confidence="0.995056">
• Detection of non-harmonic (and harmonic)
voicing.
• Identification and suppression or removal of
stray sounds, i.e., non-speech.
• Grouping of landmarks into syllable-like clus-
ters.
</listItem>
<bodyText confidence="0.992103">
(Note that Wavesurfer already provides a general
pitch-tracking capability for harmonic voicing.)
The Wavesurfer plug-in will also allow the user to
output information about an audio file or a direc-
tory of audio files, e.g. all the recordings of a child.
This information will be in a tab-delimited text file
or a spreadsheet. This will allow the speech scien-
tist
</bodyText>
<figureCaption confidence="0.928227">
Figure 1: Wavesurfer with landmarks/waveform pane filtered to show only +/-g landmarks, and tran-
scription pane (top) with +/-g and +/-s landmarks
</figureCaption>
<page confidence="0.995297">
40
</page>
<bodyText confidence="0.999967">
This information will be in a tab-delimited text file
or a spreadsheet. This will allow the speech scien-
tist to analyze the output and, for example, to
summarize and compare the typically developing
children to those diagnosed with autism.
</bodyText>
<subsectionHeader confidence="0.999482">
5.2 User Testing
</subsectionHeader>
<bodyText confidence="0.9999458125">
We are currently recruiting potential users to test
the system including graduate students and senior
researchers in neurosciences and speech-related
sciences. So that these users can test the system on
a realistic problem, we will provide them with a
corpus of annotated, de-identified recordings of
children with and without a diagnosis of autism.
This will provide context for specific training tasks
that we ask of the users and enable them to formu-
late their own appropriate, if small, research ques-
tions that the system can help to answer. We will
probe their experiences by logging the questions
they have about the system, watching their actions
as they attempt to answer the research questions,
and asking their opinions of the experience after-
ward.
</bodyText>
<sectionHeader confidence="0.993593" genericHeader="method">
6 Requested Features
</sectionHeader>
<bodyText confidence="0.999150263157895">
In an early trial of our Waversurfer plug-in, a user
requested the VOT (voice-onset time) measure. In
response to this request, we are now adding a
VOT-transcription pane to display the automati-
cally computed voice onset times aligned with the
waveform, spectrogram, and displayed informa-
tion. The information in this pane is also auto-
matically saved to a text file that can be analyzed
with other software.
This request also led us to include a popup window
to show the vowel-space in a recording. Vowel-
space measures are conventionally labor-intensive,
thus limited to a few instances of specific vowels,
and require that the researcher first identify spe-
cific instances of these vowels. On the other hand,
vocalic landmarks identify the instants where for-
mant frequencies may be reliably estimated, so our
tools can quickly and automatically evaluate the
full vowel space of a passage. (See Figure 2.)
</bodyText>
<figureCaption confidence="0.946821">
Figure 2: Automatic Vowel-Space Evaluation. Com-
</figureCaption>
<bodyText confidence="0.989329">
puting the resonant frequencies (formants) at vowel
landmarks allows plotting the vowel space, i.e., the scat-
ter of the first two formants against each other. In this
case, a female read the complete Rainbow Passage (a
standard passage of 3 paragraphs, approx. 90 sec of
reading). The system automatically identified all the
consonantal and vocalic landmarks, evaluated the for-
mants at ~ 140 stressed vowels, and computed the con-
vex hull (“rubber-band”) area, 0.88 kHz2. Total
computation time on a commodity 3 GHz PC was 143
sec (and is directly proportional to the duration of the
passage).
</bodyText>
<sectionHeader confidence="0.994554" genericHeader="method">
7 Challenges for Software development,
</sectionHeader>
<subsectionHeader confidence="0.696279">
Challenges for availability
</subsectionHeader>
<bodyText confidence="0.999948375">
Our algorithms are implemented in MATLAB.
Though toolkits that run in MATLAB might be
available free, or for a modest price, the MATLAB
platform itself is costly, especially for non-
academic users. On the other hand, shareware or
freeware may have minimal documentation; sup-
port that depends entirely on the presence (or ab-
sence!) of a knowledgeable user community; and
variable standards for testing, correctness, and per-
formance.
A critical hidden cost for any system is the learn-
ing curve. For those systems with little documen-
tation and training, this can dwarf the overt costs.
Our goal is to make learning easier by creating
landmark-processing plug-ins that people can use
within software that they already employ.
</bodyText>
<page confidence="0.998033">
41
</page>
<bodyText confidence="0.9999133125">
Such a plan requires a careful balance between the
flexibility of a general, extensible system and the
simplicity of a small, fixed set of easily docu-
mented plug-in capabilities. Our project therefore
includes both a small set of simple functions, such
as VOT, and software design centered on the needs
identified by users from the appropriate research
communities. Our design relies on an iterative
process of structured interviews and web-based
surveys, combined with observations of user expe-
riences with our plug-ins.
This user study extends beyond the matter of func-
tionality and documentation. It also addresses the
expectations or requirements for convenient avail-
ability, training, and support, and the costs that
these imply.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="discussions">
8 Future work
</sectionHeader>
<subsectionHeader confidence="0.649037">
8.1 R – statistical analysis system
</subsectionHeader>
<bodyText confidence="0.999980888888889">
We will integrate our software with R
(http://www.r-project.org/) for those with a pri-
mary interest instead in the derived articulatory-
precision information: e.g., syllable production
rate, fraction of syllables of a given complexity, or
range of vowels.
For this platform, we will implement further user-
level functions, with corresponding graphical user
interfaces as appropriate, to produce:
</bodyText>
<listItem confidence="0.997445625">
• Number of landmarks, optionally excluding
those that are automatically detected as noise-
related.
• Syllable complexity and statistics of same.
• Utterance complexity.
• Syllable production rate.
• Articulatory precision.
• Vowel space measures.
</listItem>
<subsectionHeader confidence="0.987746">
8.2 Other Platforms
</subsectionHeader>
<bodyText confidence="0.99985375">
We plan to expand our work to include plugins or
packages for integration with a wider (and more
powerful) collection of research tools, for example
PRAAT, CSL, or even Excel.
</bodyText>
<subsectionHeader confidence="0.98805">
8.3 Other Features
</subsectionHeader>
<bodyText confidence="0.999853666666667">
We are soliciting input from user communities
about the features they would like to see in these
tools.
</bodyText>
<sectionHeader confidence="0.995489" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982965">
This work was funded in part by NIH grant R43
DC010104.
</bodyText>
<sectionHeader confidence="0.998512" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988713860465116">
Suzanne Boyce, Joel MacAuslan, Ann Bradlow, and
Rajka Smiljanič. 2007. Automatic Detection of Dif-
ferences Between Clear &amp; Conversational Speech,
poster presented at American Speech-Language-
Hearing Convention.
Suzanne Boyce, Ann Bradlow, and Joel MacAuslan.
2005. Landmark analysis of clear and conversational
speaking styles, 150th meeting of the Acoustical So-
ciety of America.
Thomas DiCicco and Rupal Patel. 2008. Automatic
Landmark Analysis of Dysarthric Speech, Journal of
Medical Speech-Language Pathology, 16(4):213-
219.
Cynthia J. Cress, S. Unrein, A. Weber, S. Krings, H.
Fell, J. MacAuslan, and J. Gong. 2005. Vocal Devel-
opment Patterns in Children at Risk for Being Non-
speaking. ASHA 2005.
Cynthia J. Cress. 1995. Communicative and symbolic
precursors of AAC, Unpublished NIH CIDA Grant:
University of Nebraska-Lincoln.
Jack R. Deller, D. Hsu, and Linda J. Ferrier. 1991. On
the Use of Hidden Markov Modeling for Recognition
of Dysarthric Speech, Computer Methods and Pro-
grams in Biomedicine. (35)2:125-139.
Harriet J. Fell, Joel MacAuslan, Linda J. Ferrier, Susan
G. Worst, and Karen Chenausky. 2002. Vocalization
Age as a Clinical Tool. Procroceedings of the Inter-
national Conference on Speech and Language Proc-
essing.
Harriet J. Fell, Joel MacAuslan, Cynthia. Cress, Linda J.
Ferrier. 2004. visiBabble for Reinforcement of Early
Vocalization, Proceedings of ASSETS 2004. 161-168.
Wilson Howitt. 2000. Unpublished Ph.D. dissertation,
Massachusetts Institute of Technology.
Amit Juneja and Carol Espy-Wilson. 2003, Speech
Segmentation Using Probabilistic Phonetic Feature
Hierarchy and Support Vector Machines. Proceed-
ings of the International Joint Conference on Neural
Networks.
Sharlene A. Liu. 1995. Landmark Detection for Distinc-
tive Feature-Hyphen Based Speech Recognition,
M.I.T. Doctoral Thesis.
R, http://www.r-project.org/
</reference>
<page confidence="0.98958">
42
</page>
<reference confidence="0.998016789473684">
Janet Slifka, Kenneth N. Stevens, Sharon Manuel, and
Stefanie Shattuck-Hufnagel. 2004. A Landmark-
Based Model of Speech Perception: History and Re-
cent Developments. From Sound to Sense, 85-90.
Kenneth N. Stevens, 2000. Acoustic Phonetics, The
MIT Press, Cambridge, Massachusetts.
Kenneth N. Stevens. 2002. Toward a model for lexical
access based on acoustic landmarks and distinctive
features, Journal of the Acoustic Society of America.
111(4):1872-1891.
Kenneth N. Stevens, Sharon Manuel, Stefanie Shattuck-
Hufnagel, and Sharlene Liu. 1992. Implementation of
a model for lexical access based on features, Proced-
ings ICSLP (Int. Conf. on Speech &amp; Language Proc-
essing). 499-502.
Travis Wade, Bernd Möbius. 2007. Speaking rate ef-
fects in a landmark-based phonetic exemplar model,
Interspeech 2007. 402-405.
Wavesurfer.2005. http://www.speech.kth.se/wavesurfer/
</reference>
<page confidence="0.999824">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964447">
<title confidence="0.995917">A Platform for Automated Acoustic Analysis for Assistive Technology</title>
<author confidence="0.998471">Suzanne Boyce Harriet Fell</author>
<affiliation confidence="0.999682666666667">Department of Communication Sciences and College of Computer and Information Disorders Science University of Cincinnati Northeastern University</affiliation>
<address confidence="0.999961">Cincinnati, Ohio, 45267, USA Boston, Massachusetts, 02115, USA</address>
<email confidence="0.999695">boycese@ucmail.uc.edu</email>
<author confidence="0.999784">Joel MacAuslan Lorin Wilde</author>
<affiliation confidence="0.999308">Speech Technology and Applied Research Boston</affiliation>
<address confidence="0.99913">54 Middlesex Turnpike Bedford, Massachusetts, , USA Boston, Massachusetts, 02180, USA</address>
<email confidence="0.999359">joelm@staranalyticalservices.comwildercom@gmail.com</email>
<abstract confidence="0.998247933333333">The use of speech production data has been limited by a steep learning curve and the need for laborious hand measurement. We are building a tool set that provides summary statistics for measures designed by clinicians to screen, diagnose or provide training to assistive technology users. This will be achieved by extending an existing shareware software platform with “plug-ins” that perform specific measures and report results to the user. The common underlying basis for this tool set is a Stevens’ paradigm of landmarks, points in an utterance around which information about articulatory events can be extracted.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Suzanne Boyce</author>
<author>Joel MacAuslan</author>
<author>Ann Bradlow</author>
<author>Rajka Smiljanič</author>
</authors>
<title>Automatic Detection of Differences Between Clear &amp; Conversational Speech, poster presented at American Speech-LanguageHearing Convention.</title>
<date>2007</date>
<marker>Boyce, MacAuslan, Bradlow, Smiljanič, 2007</marker>
<rawString>Suzanne Boyce, Joel MacAuslan, Ann Bradlow, and Rajka Smiljanič. 2007. Automatic Detection of Differences Between Clear &amp; Conversational Speech, poster presented at American Speech-LanguageHearing Convention.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Boyce</author>
<author>Ann Bradlow</author>
<author>Joel MacAuslan</author>
</authors>
<title>Landmark analysis of clear and conversational speaking styles, 150th meeting of the Acoustical Society of America.</title>
<date>2005</date>
<contexts>
<context position="5363" citStr="Boyce et al. 2005" startWordPosition="826" endWordPosition="829">ying a wide variety of landmarks to recognize the lexical content of speech [Juneja and Espy-Wilson 2003, Slifka, et al. 2004], the power of these measures is even more apparent when applied to non-lexical attributes. 3 Applications of Landmark Analysis to Assistive Technology 3.1 Tracking Articulatory Precision Measuring articulatory precision is important to evaluating efficacy of a treatment or in monitoring disease progression, e.g. in Parkinson’s disease. Given that landmarks reflect articulation, tools based on landmarks may be useful for measuring and monitoring articulatory precision [Boyce et al. 2005, 2007]. The technique relies on setting empirically derived thresholds for the detection of abrupt acoustic changes in specified frequency bands. Recall that changes in the acoustic signal occur simultaneously across wide frequency ranges. When the onset of energy does not exceed threshold in a particular frequency band, i.e., not quite abrupt enough to trigger the detection of a landmark, then no landmark may be assigned. However, since different sounds produce different patterns, changes detected in other bands at that point in time are either a) assigned to a different landmark, or b) cons</context>
<context position="7917" citStr="Boyce et al. 2005" startWordPosition="1212" endWordPosition="1215">istinguish infants who may be at risk for later communication or other developmental problems from typically developing infants. 3.3 Measuring and Evaluating “Clear Speech” “Clear Speech” is an intelligibility-enhancing style of speech that is used to improve communication outcomes. Listeners with hearing impairment derive significant benefit from being addressed with clearly articulated speech. Speech that is more clearly articulated contains more abrupt acoustic changes. The result is that speech with different levels of intelligibility shows different numbers and combinations of landmarks [Boyce et al. 2005, 2007]. 3.4 Other Applications In the UCARE project [1995], Cress reported analyzing 40 hours of pre-existing [2005] videotaped sessions of children with physical or neurological impairments using landmark-based tools. Fell et al. [2004] reported using landmark analysis to follow the progress of several children with severe speech delays. In this project, 10-minute, inhome audio recordings were processed in real-time on a 2002-era PC laptop. Wade and Möbius [2007] used automated landmark analysis to study speaking rate effects as a measure of disease progression in Parkinson&apos;s disease. DiCicc</context>
</contexts>
<marker>Boyce, Bradlow, MacAuslan, 2005</marker>
<rawString>Suzanne Boyce, Ann Bradlow, and Joel MacAuslan. 2005. Landmark analysis of clear and conversational speaking styles, 150th meeting of the Acoustical Society of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas DiCicco</author>
<author>Rupal Patel</author>
</authors>
<title>Automatic Landmark Analysis of Dysarthric Speech,</title>
<date>2008</date>
<journal>Journal of Medical Speech-Language Pathology,</journal>
<pages>16--4</pages>
<marker>DiCicco, Patel, 2008</marker>
<rawString>Thomas DiCicco and Rupal Patel. 2008. Automatic Landmark Analysis of Dysarthric Speech, Journal of Medical Speech-Language Pathology, 16(4):213-219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia J Cress</author>
<author>S Unrein</author>
<author>A Weber</author>
<author>S Krings</author>
<author>H Fell</author>
<author>J MacAuslan</author>
<author>J Gong</author>
</authors>
<title>Vocal Development Patterns in Children at Risk for Being Nonspeaking.</title>
<date>2005</date>
<location>ASHA</location>
<marker>Cress, Unrein, Weber, Krings, Fell, MacAuslan, Gong, 2005</marker>
<rawString>Cynthia J. Cress, S. Unrein, A. Weber, S. Krings, H. Fell, J. MacAuslan, and J. Gong. 2005. Vocal Development Patterns in Children at Risk for Being Nonspeaking. ASHA 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia J Cress</author>
</authors>
<title>Communicative and symbolic precursors of AAC, Unpublished NIH CIDA Grant:</title>
<date>1995</date>
<institution>University of Nebraska-Lincoln.</institution>
<marker>Cress, 1995</marker>
<rawString>Cynthia J. Cress. 1995. Communicative and symbolic precursors of AAC, Unpublished NIH CIDA Grant: University of Nebraska-Lincoln.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack R Deller</author>
<author>D Hsu</author>
<author>Linda J Ferrier</author>
</authors>
<date>1991</date>
<booktitle>On the Use of Hidden Markov Modeling for Recognition of Dysarthric Speech, Computer Methods and Programs in Biomedicine.</booktitle>
<pages>35--2</pages>
<marker>Deller, Hsu, Ferrier, 1991</marker>
<rawString>Jack R. Deller, D. Hsu, and Linda J. Ferrier. 1991. On the Use of Hidden Markov Modeling for Recognition of Dysarthric Speech, Computer Methods and Programs in Biomedicine. (35)2:125-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harriet J Fell</author>
<author>Joel MacAuslan</author>
<author>Linda J Ferrier</author>
<author>Susan G Worst</author>
<author>Karen Chenausky</author>
</authors>
<date>2002</date>
<booktitle>Vocalization Age as a Clinical Tool. Procroceedings of the International Conference on Speech and Language Processing.</booktitle>
<marker>Fell, MacAuslan, Ferrier, Worst, Chenausky, 2002</marker>
<rawString>Harriet J. Fell, Joel MacAuslan, Linda J. Ferrier, Susan G. Worst, and Karen Chenausky. 2002. Vocalization Age as a Clinical Tool. Procroceedings of the International Conference on Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linda J Ferrier Cress</author>
</authors>
<title>visiBabble for Reinforcement of Early Vocalization,</title>
<date>2004</date>
<booktitle>Proceedings of ASSETS</booktitle>
<pages>161--168</pages>
<marker>Cress, 2004</marker>
<rawString>Harriet J. Fell, Joel MacAuslan, Cynthia. Cress, Linda J. Ferrier. 2004. visiBabble for Reinforcement of Early Vocalization, Proceedings of ASSETS 2004. 161-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson Howitt</author>
</authors>
<date>2000</date>
<institution>Massachusetts Institute of Technology.</institution>
<note>Unpublished Ph.D. dissertation,</note>
<contexts>
<context position="4661" citStr="Howitt, 2000" startWordPosition="724" endWordPosition="725">) or offset (-g) of voicing. s(yllabicity): marks the onset (+s) or offset (-s) of syllabicity, i.e. onsets and releases of voiced sonorant consonants such as /l/ or /r/, vocal tract closures due to voiced stop consonants such as /b/ or /d/. b(urst): marks the onset (+b) of the burst of air following stop or affricate consonant release, or the onset of frication noise for fricative consonants. Offsets (-b) mark points where aspiration or frication noise ends abruptly due to a stop closure. V(owel): marks points of peak amplitude in a sonorant region—that is, a region where voicing is evident [Howitt, 2000]. Although much of the past work using landmark processing has been focused on employing a wide variety of landmarks to recognize the lexical content of speech [Juneja and Espy-Wilson 2003, Slifka, et al. 2004], the power of these measures is even more apparent when applied to non-lexical attributes. 3 Applications of Landmark Analysis to Assistive Technology 3.1 Tracking Articulatory Precision Measuring articulatory precision is important to evaluating efficacy of a treatment or in monitoring disease progression, e.g. in Parkinson’s disease. Given that landmarks reflect articulation, tools b</context>
</contexts>
<marker>Howitt, 2000</marker>
<rawString>Wilson Howitt. 2000. Unpublished Ph.D. dissertation, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Juneja</author>
<author>Carol Espy-Wilson</author>
</authors>
<title>Speech Segmentation Using Probabilistic Phonetic Feature Hierarchy and Support Vector Machines.</title>
<date>2003</date>
<booktitle>Proceedings of the International Joint Conference on Neural Networks.</booktitle>
<contexts>
<context position="4850" citStr="Juneja and Espy-Wilson 2003" startWordPosition="753" endWordPosition="756">al tract closures due to voiced stop consonants such as /b/ or /d/. b(urst): marks the onset (+b) of the burst of air following stop or affricate consonant release, or the onset of frication noise for fricative consonants. Offsets (-b) mark points where aspiration or frication noise ends abruptly due to a stop closure. V(owel): marks points of peak amplitude in a sonorant region—that is, a region where voicing is evident [Howitt, 2000]. Although much of the past work using landmark processing has been focused on employing a wide variety of landmarks to recognize the lexical content of speech [Juneja and Espy-Wilson 2003, Slifka, et al. 2004], the power of these measures is even more apparent when applied to non-lexical attributes. 3 Applications of Landmark Analysis to Assistive Technology 3.1 Tracking Articulatory Precision Measuring articulatory precision is important to evaluating efficacy of a treatment or in monitoring disease progression, e.g. in Parkinson’s disease. Given that landmarks reflect articulation, tools based on landmarks may be useful for measuring and monitoring articulatory precision [Boyce et al. 2005, 2007]. The technique relies on setting empirically derived thresholds for the detecti</context>
</contexts>
<marker>Juneja, Espy-Wilson, 2003</marker>
<rawString>Amit Juneja and Carol Espy-Wilson. 2003, Speech Segmentation Using Probabilistic Phonetic Feature Hierarchy and Support Vector Machines. Proceedings of the International Joint Conference on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharlene A Liu</author>
</authors>
<title>Landmark Detection for Distinctive Feature-Hyphen Based Speech Recognition, M.I.T. Doctoral Thesis.</title>
<date>1995</date>
<contexts>
<context position="2740" citStr="Liu, 1995" startWordPosition="411" endWordPosition="412">o report data from a single audio file, or groups of audio files in a standard table format, for easy input to statistical or other analysis software. For example, the data may be imported into a program that correlates speech data with scalp electrode and medication data. Our tool will include alternative and independently tested algorithms for clinically relevant measures, as well as guidance as to what the speech data may mean. The common underlying basis for this tool set is a focused set of landmarks derived from Stevens’ Lexical Access from Features (LAFF) paradigm (Stevens, 1992, 2002; Liu, 1995; Slifka et al., 2004). In this approach, landmarks are points in an utterance around which information about articulatory events can be extracted. 37 Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 37–43, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics In what follows, we will describe (1) the theoretical rationale of landmarks, (2) the general utility of landmark processing and several examples of clinically related measures, and (3) our current work on developing tools to make landmark analys</context>
</contexts>
<marker>Liu, 1995</marker>
<rawString>Sharlene A. Liu. 1995. Landmark Detection for Distinctive Feature-Hyphen Based Speech Recognition, M.I.T. Doctoral Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www r-project org Janet Slifka R</author>
<author>Kenneth N Stevens</author>
<author>Sharon Manuel</author>
<author>Stefanie Shattuck-Hufnagel</author>
</authors>
<title>A LandmarkBased Model of Speech Perception: History and Recent Developments. From Sound to Sense,</title>
<date>2004</date>
<pages>85--90</pages>
<marker>R, Stevens, Manuel, Shattuck-Hufnagel, 2004</marker>
<rawString>R, http://www.r-project.org/ Janet Slifka, Kenneth N. Stevens, Sharon Manuel, and Stefanie Shattuck-Hufnagel. 2004. A LandmarkBased Model of Speech Perception: History and Recent Developments. From Sound to Sense, 85-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth N Stevens</author>
</authors>
<title>Acoustic Phonetics,</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Stevens, 2000</marker>
<rawString>Kenneth N. Stevens, 2000. Acoustic Phonetics, The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth N Stevens</author>
</authors>
<title>Toward a model for lexical access based on acoustic landmarks and distinctive features,</title>
<date>2002</date>
<journal>Journal of the Acoustic Society of America.</journal>
<pages>111--4</pages>
<marker>Stevens, 2002</marker>
<rawString>Kenneth N. Stevens. 2002. Toward a model for lexical access based on acoustic landmarks and distinctive features, Journal of the Acoustic Society of America. 111(4):1872-1891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth N Stevens</author>
<author>Sharon Manuel</author>
<author>Stefanie ShattuckHufnagel</author>
<author>Sharlene Liu</author>
</authors>
<title>Implementation of a model for lexical access based on features,</title>
<date>1992</date>
<booktitle>Procedings ICSLP (Int. Conf. on Speech &amp; Language Processing).</booktitle>
<pages>499--502</pages>
<marker>Stevens, Manuel, ShattuckHufnagel, Liu, 1992</marker>
<rawString>Kenneth N. Stevens, Sharon Manuel, Stefanie ShattuckHufnagel, and Sharlene Liu. 1992. Implementation of a model for lexical access based on features, Procedings ICSLP (Int. Conf. on Speech &amp; Language Processing). 499-502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Travis Wade</author>
<author>Bernd Möbius</author>
</authors>
<title>Speaking rate effects in a landmark-based phonetic exemplar model, Interspeech</title>
<date>2007</date>
<pages>402--405</pages>
<marker>Wade, Möbius, 2007</marker>
<rawString>Travis Wade, Bernd Möbius. 2007. Speaking rate effects in a landmark-based phonetic exemplar model, Interspeech 2007. 402-405.</rawString>
</citation>
<citation valid="false">
<note>Wavesurfer.2005. http://www.speech.kth.se/wavesurfer/</note>
<marker></marker>
<rawString>Wavesurfer.2005. http://www.speech.kth.se/wavesurfer/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>