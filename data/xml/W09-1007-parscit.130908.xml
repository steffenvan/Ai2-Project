<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030629">
<title confidence="0.996804">
Language models for contextual error detection and correction
</title>
<author confidence="0.997702">
Herman Stehouwer
</author>
<affiliation confidence="0.988917333333333">
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
</affiliation>
<email confidence="0.989229">
j.h.stehouwer@uvt.nl
</email>
<author confidence="0.787094">
Menno van Zaanen
</author>
<affiliation confidence="0.967520333333333">
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
</affiliation>
<email confidence="0.99507">
mvzaanen@uvt.nl
</email>
<sectionHeader confidence="0.993816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997195">
The problem of identifying and correcting
confusibles, i.e. context-sensitive spelling
errors, in text is typically tackled using
specifically trained machine learning clas-
sifiers. For each different set of con-
fusibles, a specific classifier is trained and
tuned.
In this research, we investigate a more
generic approach to context-sensitive con-
fusible correction. Instead of using spe-
cific classifiers, we use one generic clas-
sifier based on a language model. This
measures the likelihood of sentences with
different possible solutions of a confusible
in place. The advantage of this approach
is that all confusible sets are handled by
a single model. Preliminary results show
that the performance of the generic clas-
sifier approach is only slightly worse that
that of the specific classifier approach.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981">
When writing texts, people often use spelling
checkers to reduce the number of spelling mis-
takes in their texts. Many spelling checkers con-
centrate on non-word errors. These errors can be
easily identified in texts because they consist of
character sequences that are not part of the lan-
guage. For example, in English woord is is not
part of the language, hence a non-word error. A
possible correction would be word.
Even when a text does not contain any non-
word errors, there is no guarantee that the text is
error-free. There are several types of spelling er-
rors where the words themselves are part of the
language, but are used incorrectly in their context.
Note that these kinds of errors are much harder
to recognize, as information from the context in
which they occur is required to recognize and cor-
rect these errors. In contrast, non-word errors can
be recognized without context.
One class of such errors, called confusibles,
consists of words that belong to the language, but
are used incorrectly with respect to their local,
sentential context. For example, She owns to cars
contains the confusible to. Note that this word is
a valid token and part of the language, but used
incorrectly in the context. Considering the con-
text, a correct and very likely alternative would be
the word two. Confusibles are grouped together in
confusible sets. Confusible sets are sets of words
that are similar and often used incorrectly in con-
text. Too is the third alternative in this particular
confusible set.
The research presented here is part of a
larger project, which focusses on context-sensitive
spelling mistakes in general. Within this project
all classes of context-sensitive spelling errors are
tackled. For example, in addition to confusibles,
a class of pragmatically incorrect words (where
words are incorrectly used within the document-
wide context) is considered as well. In this arti-
cle we concentrate on the problem of confusibles,
where the context is only as large as a sentence.
</bodyText>
<sectionHeader confidence="0.993537" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999480307692308">
A typical approach to the problem of confusibles
is to train a machine learning classifier to a specific
confusible set. Most of the work in this area has
concentrated on confusibles due to homophony
(to, too, two) or similar spelling (desert, dessert).
However, some research has also touched upon in-
flectional or derivational confusibles such as I ver-
sus me (Golding and Roth, 1999). For instance,
when word forms are homophonic, they tend to
get confused often in writing (cf. the situation with
to, too, and two, affect and effect, or there, their,
and they’re in English) (Sandra et al., 2001; Van
den Bosch and Daelemans, 2007).
</bodyText>
<note confidence="0.32711">
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41–48,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999422">
41
</page>
<bodyText confidence="0.999982988235294">
Most work on confusible disambiguation using
machine learning concentrates on hand-selected
sets of notorious confusibles. The confusible sets
are typically very small (two or three elements)
and the machine learner will only see training
examples of the members of the confusible set.
This approach is similar to approaches used in ac-
cent restoration (Yarowsky, 1994; Golding, 1995;
Mangu and Brill, 1997; Wu et al., 1999; Even-
Zohar and Roth, 2000; Banko and Brill, 2001;
Huang and Powers, 2001; Van den Bosch, 2006).
The task of the machine learner is to decide, us-
ing features describing information from the con-
text, which word taken from the confusible set re-
ally belongs in the position of the confusible. Us-
ing the example above, the classifier has to decide
which word belongs on the position of the X in
She owns X cars, where the possible answers for
X are to, too, or two. We call X, the confusible
that is under consideration, the focus word.
Another way of looking at the problem of con-
fusible disambiguation is to see it as a very spe-
cialized case of word prediction. The problem is
then to predict which word belongs at a specific
position. Using similarities between these cases,
we can use techniques from the field of language
modeling to solve the problem of selecting the best
alternative from confusible sets. We will investi-
gate this approach in this article.
Language models assign probabilities to se-
quences of words. Using this information, it
is possible to predict the most likely word in
a certain context. If a language model gives
us the probability for a sequence of n words
PLM(w1, ... , wn), we can use this to predict the
most likely word w following a sequence of n − 1
words arg maxw PLM(w1, ... , wn−1, w). Obvi-
ously, a similar approach can be taken with w in
the middle of the sequence.
Here, we will use a language model as a classi-
fier to predict the correct word in a context. Since
a language model models the entire language, it is
different from a regular machine learning classifier
trained on a specific set of confusibles. The advan-
tage of this approach to confusible disambiguation
is that the language model can handle all potential
confusibles without any further training and tun-
ing. With the language model it is possible to take
the words from any confusible set and compute the
probabilities of those words in the context. The
element from the confusible set that has the high-
est probability according to the language model is
then selected. Since the language model assigns
probabilities to all sequences of words, it is pos-
sible to define new confusible sets on the fly and
let the language model disambiguate them with-
out any further training. Obviously, this is not
possible for a specialized machine learning clas-
sifier approach, where a classifier is fine-tuned to
the features and classes of a specific confusible set.
The expected disadvantage of the generic (lan-
guage model) classifier approach is that the accu-
racy is expected to be less than that of the specific
(specialized machine learning classifier) approach.
Since the specific classifiers are tuned to each spe-
cific confusible set, the weights for each of the
features may be different for each set. For in-
stance, there may be confusibles for which the cor-
rect word is easily identified by words in a specific
position. If a determiner, like the, occurs in the po-
sition directly before the confusible, to or too are
very probably not the correct answers. The spe-
cific approach can take this into account by assign-
ing specific weights to part-of-speech and position
combinations, whereas the generic approach can-
not do this explicitly for specific cases; the weights
follow automatically from the training corpus.
In this article, we will investigate whether it is
possible to build a confusible disambiguation sys-
tem that is generic for all sets of confusibles using
language models as generic classifiers and investi-
gate in how far this approach is useful for solving
the confusible problem. We will compare these
generic classifiers against specific classifiers that
are trained for each confusible set independently.
</bodyText>
<sectionHeader confidence="0.999907" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999986083333333">
To measure the effectiveness of the generic clas-
sifier approach to confusible disambiguation, and
to compare it against a specific classifier approach
we have implemented several classification sys-
tems. First of these is a majority class baseline sys-
tem, which selects the word from the confusible
set that occurs most often in the training data.1
We have also implemented several generic classi-
fiers based on different language models. We com-
pare these against two machine learning classi-
fiers. The machine learning classifiers are trained
separately for each different experiment, whereas
</bodyText>
<footnote confidence="0.652883666666667">
1This baseline system corresponds to the simplest lan-
guage model classifier. In this case, it only uses n-grams with
n = 1.
</footnote>
<page confidence="0.984986">
42
</page>
<bodyText confidence="0.999427666666667">
the parameters and the training material of the lan-
guage model are kept fixed throughout all the ex-
periments.
</bodyText>
<subsectionHeader confidence="0.998847">
3.1 System description
</subsectionHeader>
<bodyText confidence="0.999901105263158">
There are many different approaches that can be
taken to develop language models. A well-known
approach is to use n-grams, or Markov models.
These models take into account the probability
that a word occurs in the context of the previous
n − 1 words. The probabilities can be extracted
from the occurrences of words in a corpus. Proba-
bilities are computed by taking the relative occur-
rence count of the n words in sequence.
In the experiments described below, we will use
a tri-gram-based language model and where re-
quired this model will be extended with bi-gram
and uni-gram language models. The probability
of a sequence is computed as the combination of
the probabilities of the tri-grams that are found in
the sequence.
Especially when n-grams with large n are used,
data sparseness becomes an issue. The training
data may not contain any occurrences of the par-
ticular sequence of n symbols, even though the
sequence is correct. In that case, the probability
extracted from the training data will be zero, even
though the correct probability should be non-zero
(albeit small). To reduce this problem we can ei-
ther use back-off or smoothing when the probabil-
ity of an n-gram is zero. In the case of back-off,
the probabilities of lower order n-grams are taken
into account when needed. Alternatively, smooth-
ing techniques (Chen and Goodman, 1996) redis-
tribute the probabilities, taking into account previ-
ously unseen word sequences.
Even though the language models provide us
with probabilities of entire sequences, we are
only interested in the n-grams directly around the
confusible when using the language models in
the context of confusible disambiguation. The
probabilities of the rest of the sequence will re-
main the same whichever alternative confusible
is inserted in the focus word position. Fig-
ure 1 illustrates that the probability of for example
P(analysts had expected) is irrelevant for the de-
cision between then and than because it occurs in
both sequences.
The different language models we will consider
here are essentially the same. The differences lie
in how they handle sequences that have zero prob-
ability. Since the probabilities of the n-grams are
multiplied, having a n-gram probability of zero re-
sults in a zero probability for the entire sequence.
There may be two reasons for an n-gram to have
probability zero: there is not enough training data,
so this sequence has not been seen yet, or this se-
quence is not valid in the language.
When it is known that a sequence is not valid
in the language, this information can be used to
decide which word from the confusible set should
be selected. However, when the sequence simply
has not been seen in the training data yet, we can-
not rely on this information. To resolve the se-
quences with zero probability, we can use smooth-
ing. However, this assumes that the sequence is
valid, but has not been seen during training. The
other solution, back-off, tries not to make this as-
sumption. It checks whether subsequences of the
sequence are valid, i.e. have non-zero probabili-
ties. Because of this, we will not use smoothing to
reach non-zero probabilities in the current exper-
iments, although this may be investigated further
in the future.
The first language model that we will investi-
gate here is a linear combination of the differ-
ent n-grams. The probability of a sequence is
computed by a linear combination of weighted n-
gram probabilities. We will report on two different
weight settings, one system using uniform weight-
ing, called uniform linear, and one where uni-
grams receive weight 1, bi-grams weight 138, and
tri-grams weight 437.2 These weights are normal-
ized to yield a final probability for the sequence,
resulting in the second system called weighted lin-
ear.
The third system uses the probabilities of the
different n-grams separately, instead of using the
probabilities of all n-grams at the same time as is
done in the linear systems. The continuous back-
off method uses only one of the probabilities at
each position, preferring the higher-level probabil-
ities. This model provides a step-wise back-off.
The probability of a sequence is that of the tri-
grams contained in that sequence. However, if the
probability of a trigram is zero, a back-off to the
probabilities of the two bi-grams of the sequence
is used. If that is still zero, the uni-gram probabil-
ity at that position is used. Note that this uni-gram
probability is exactly what the baseline system
</bodyText>
<footnote confidence="0.977726">
2These weights are selected by computing the accuracy of
all combinations of weights on a held out set.
</footnote>
<page confidence="0.99966">
43
</page>
<figure confidence="0.9189006">
... much stronger most analysts had expected.
than then
P(much stronger than) P(much stronger then)
xP(stronger than most) xP(stronger then most)
xP(than most analysts) xP(then most analysts)
</figure>
<figureCaption confidence="0.999962">
Figure 1: Computation of probabilities using the language model.
</figureCaption>
<bodyText confidence="0.999912052631579">
uses. With this approach it may be the case that
the probability for one word in the confusible set
is computed based on tri-grams, whereas the prob-
ability of another word in the set of confusibles is
based on bi-grams or even the uni-gram probabil-
ity. Effectively, this means that different kinds of
probabilities are compared. The same weights as
in the weighted linear systems are used.
To resolve the problem of unbalanced probabil-
ities, a fourth language model, called synchronous
back-off, is proposed. Whereas in the case of the
continuous back-off model, two words from the
confusible set may be computed using probabil-
ities of different level n-grams, the synchronous
back-off model uses probabilities of the same level
of n-grams for all words in the confusible set, with
n being the highest value for which at least one of
the words has a non-zero probability. For instance,
when word a has a tri-gram probability of zero and
word b has a non-zero tri-gram probability, b is se-
lected. When both have a zero tri-gram probabil-
ity, a back-off to bi-grams is performed for both
words. This is in line with the idea that if a proba-
bility is zero, the training data is sufficient, hence
the sequence is not in the language.
To implement the specific classifiers, we used
the TiMBL implementation of a k-NN classifier
(Daelemans et al., 2007). This implementation of
the k-NN algorithm is called IB1. We have tuned
the different parameter settings for the k-NN clas-
sifier using Paramsearch (Van den Bosch, 2004),
which resulted in a k of 35.3 To describe the in-
stances, we try to model the data as similar as pos-
sible to the data used by the generic classifier ap-
proach. Since the language model approaches use
n-grams with n = 3 as the largest n, the features
for the specific classifier approach use words one
and two positions left and right of the focus word.
</bodyText>
<footnote confidence="0.8697745">
3We note that k is handled slightly differently in TiMBL
than usual, k denotes the number of closest distances consid-
ered. So if there are multiple instances that have the same
(closest) distance they are all considered.
</footnote>
<bodyText confidence="0.999985595238095">
The focus word becomes the class that needs to
be predicted. We show an example of both train-
ing and testing in figure 2. Note that the features
for the machine learning classifiers could be ex-
panded with, for instance, part-of-speech tags, but
in the current experiments only the word forms are
used as features.
In addition to the k-NN classifier, we also run
the experiments using the IGTree classifier, which
is denoted IGTree in the rest of the article, which is
also contained in the TiMBL distribution. IGTree
is a fast, trie based, approximation of k-nearest
neighbor classification (Knuth, 1973; Daelemans
et al., 1997). IGTree allows for fast training and
testing even with millions of examples. IGTree
compresses a set of labeled examples into a deci-
sion tree structure similar to the classic C4.5 algo-
rithm (Quinlan, 1993), except that throughout one
level in the IGTree decision tree, the same feature
is tested. Classification in IGTree is a simple pro-
cedure in which the decision tree is traversed from
the root node down, and one path is followed that
matches the actual values of the new example to
be classified. If a leaf is found, the outcome stored
at the leaf of the IGTree is returned as the clas-
sification. If the last node is not a leaf node, but
there are no outgoing arcs that match a feature-
value combination of the instance, the most likely
outcome stored at that node is produced as the re-
sulting classification. This outcome is computed
by collating the outcomes of all leaf nodes that can
be reached from the node.
IGTree is typically able to compress a large
example set into a lean decision tree with high
compression factors. This is done in reasonably
short time, comparable to other compression al-
gorithms. More importantly, IGTree’s classifica-
tion time depends only on the number of features
(O(f)). Indeed, in our experiments we observe
high compression rates. One of the unique char-
acteristics of IGTree compared to basic k-NN is
its resemblance to smoothing of a basic language
</bodyText>
<page confidence="0.998113">
44
</page>
<bodyText confidence="0.99802525">
Training ... much stronger than most analysts had expected.
(much, stronger, most, analysts) Athan
Testing ... much stronger most analysts had expected.
(much, stronger, most, analysts) A?
</bodyText>
<figureCaption confidence="0.839360333333333">
Figure 2: During training, a classified instance (in this case for the confusible pair {then, than}) are
generated from a sentence. During testing, a similar instance is generated. The classifier decides what
the corresponding class, and hence, which word should be the focus word.
</figureCaption>
<bodyText confidence="0.9941465">
model (Zavrel and Daelemans, 1997), while still
being a generic classifier that supports any number
and type of features. For these reasons, IGTree is
also included in the experiments.
</bodyText>
<subsectionHeader confidence="0.996023">
3.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999936068181818">
The probabilities used in the language models of
the generic classifiers are computed by looking at
occurrences of n-grams. These occurrences are
extracted from a corpus. The training instances
used in the specific machine learning classifiers
are also extracted from the same data set. For
training purposes, we used the Reuters news cor-
pus RCV1 (Lewis et al., 2004). The Reuters cor-
pus contains about 810,000 categorized newswire
stories as published by Reuters in 1996 and 1997.
This corpus contains around 130 million tokens.
For testing purposes, we used the Wall Street
Journal part of the Penn Treebank corpus (Marcus
et al., 1993). This well-known corpus contains ar-
ticles from the Wall Street Journal in 1987 to 1989.
We extract our test-instances from this corpus in
the same way as we extract our training data from
the Reuters corpus. There are minor tokenization
differences between the corpora. The data is cor-
rected for these differences.
Both corpora are in the domain of English lan-
guage news texts, so we expect them to have simi-
lar properties. However, they are different corpora
and hence are slightly different. This means that
there are also differences between the training and
testing set. We have selected this division to cre-
ate a more realistic setting. This should allow for a
more to real-world use comparison than when both
training and testing instances are extracted from
the same corpus.
For the specific experiments, we selected a
number of well-known confusible sets to test
the different approaches. In particular, we
look at {then, than}, {its, it’s}, {your, you’re},
{their, there, they’re}. To compare the difficulty
of these problems, we also selected two words at
random and used them as a confusible set.
The random category consists of two words that
where randomly selected from all words in the
Reuters corpus that occurred more than a thousand
times. The words that where chosen, and used for
all experiments here are refugees and effect. They
occur around 27 thousand times in the Reuters cor-
pus.
</bodyText>
<subsectionHeader confidence="0.999434">
3.3 Empirical results
</subsectionHeader>
<bodyText confidence="0.999858607142857">
Table 1 sums up the results we obtained with the
different systems. The baseline scores are gen-
erally very high, which tells us that the distribu-
tion of classes in a single confusible set is severely
skewed, up to a ten to one ratio. This also makes
the task hard. There are many examples for one
word in the set, but only very few training in-
stances for the other(s). However, it is especially
important to recognize the important aspects of the
minority class.
The results clearly show that the specific clas-
sifier approaches outperform the other systems.
For instance, on the first task ({then, than}) the
classifier achieves an accuracy slightly over 98%,
whereas the language model systems only yield
around 96%. This is as expected. The classifier
is trained on just one confusible task and is there-
fore able to specialize on that task.
Comparing the two specific classifiers, we see
that the accuracy achieved by IB1 and IGTree is
quite similar. In general, IGTree performs a bit
worse than IB1 on all confusible sets, which is
as expected. However, in general it is possible
for IGTree to outperform IB1 on certain tasks. In
our experience this mainly happens on tasks where
the usage of IGTree, allowing for more compact
internal representations, allows one to use much
more training data. IGTree also leads to improved
</bodyText>
<page confidence="0.99845">
45
</page>
<table confidence="0.999083555555556">
{then, than} {its, it’s} {your, you’re} {their, there, they’re} random
Baseline 82.63 92.42 78.55 68.36 93.16
IB1 98.01 98.67 96.36 97.12 97.89
IGTree 97.07 96.75 96.00 93.02 95.79
Uniform linear 68.27 50.70 31.64 32.72 38.95
Weighted linear 94.43 92.88 93.09 93.25 88.42
Continuous back-off 81.49 83.22 74.18 86.01 63.68
Synchronous back-off 96.42 94.10 92.36 93.06 87.37
Number of cases 2,458 4,830 275 3,053 190
</table>
<tableCaption confidence="0.957925">
Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%).
The Number of cases denotes the number of instances in the testset.
</tableCaption>
<bodyText confidence="0.999588037735849">
performance in cases where the features have a
strong, absolute ordering of importance with re-
spect to the classification problem at hand.
The generic language model approaches per-
form reasonably well. However, there are clear
differences between the approaches. For instance
the weighted linear and synchronous back-off ap-
proaches work well, but uniform linear and con-
tinuous back-off perform much worse. Especially
the synchronous back-off approach achieves de-
cent results, regardless of the confusible problem.
It is not very surprising to see that the contin-
uous back-off method performs worse than the
synchronous back-off method. Remember that
the continuous back-off method always uses lower
level n-grams when zero probabilities are found.
This is done independently of the probabilities of
the other words in the confusible set. The contin-
uous back-off method prefers n-grams with larger
n, however it does not penalize backing off to an
n-gram with smaller n. Combine this with the fact
that n-gram probabilities with large n are compar-
atively lower than those for n-grams with smaller
n and it becomes likely that a bi-gram contributes
more to the erroneous option than the correct tri-
gram does to the correct option. Tri-grams are
more sparse than bi-grams, given the same data.
The weighted linear approach outperforms the
uniform linear approach by a large margin on all
confusible sets. It is likely that the contribution
from the n-grams with large n overrules the prob-
abilities of the n-grams with smaller n in the uni-
form linear method. This causes a bias towards the
more frequent words, compounded by the fact that
bi-grams, and uni-grams even more so, are less
sparse and therefore contribute more to the total
probability.
We see that the both generic and specific clas-
sifier approaches perform consistently across the
different confusible sets. The synchronous back-
off approach is the best performing generic clas-
sifier approach we tested. It consistently outper-
forms the baseline, and overall performs better
than the weighted linear approach.
The experiments show that generic classifiers
based on language model can be used in the con-
text of confusible disambiguation. However, the
n in the different n-grams is of major importance.
Exactly which n grams should be used to com-
pute the probability of a sequence requires more
research. The experiments also show that ap-
proaches that concentrate on n-grams with larger
n yield more encouraging results.
</bodyText>
<sectionHeader confidence="0.982721" genericHeader="conclusions">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999876714285714">
Confusibles are spelling errors that can only be de-
tected within their sentential context. This kind
of errors requires a completely different approach
compared to non-word errors (errors that can be
identified out of context, i.e. sequences of char-
acters that do not belong to the language). In
practice, most confusible disambiguation systems
are based on machine learning classification tech-
niques, where for each type of confusible, a new
classifier is trained and tuned.
In this article, we investigate the use of language
models in the context of confusible disambigua-
tion. This approach works by selecting the word
in the set of confusibles that has the highest prob-
ability in the sentential context according to the
language model. Any kind of language model can
be used in this approach.
The main advantage of using language models
as generic classifiers is that it is easy to add new
sets of confusibles without retraining or adding ad-
ditional classifiers. The entire language is mod-
</bodyText>
<page confidence="0.997692">
46
</page>
<bodyText confidence="0.999917693877551">
eled, which means that all the information on
words in their context is inherently present.
The experiments show that using generic clas-
sifiers based on simple n-gram language models
yield slightly worse results compared to the spe-
cific classifier approach, where each classifier is
specifically trained on one confusible set. How-
ever, the advantage of the generic classifier ap-
proach is that only one system has to be trained,
compared to different systems for each confusible
in the specific classifier case. Also, the exact com-
putation of the probabilities using the n-grams, in
particular the means of backing-off, has a large
impact on the results.
As future work, we would like to investigate the
accuracy of more complex language models used
as classifiers. The n-gram language models de-
scribed here are relatively simple, but more com-
plex language models could improve performance.
In particular, instead of back-off, smoothing tech-
niques could be investigated to reduce the impact
of zero probability problems (Chen and Goodman,
1996). This assumes that the training data we are
currently working with is not enough to properly
describe the language.
Additionally, language models that concentrate
on more structural descriptions of the language,
for instance, using grammatical inference tech-
niques (de la Higuera, 2005), or models that ex-
plicitly take long distance dependencies into ac-
count (Griffiths et al., 2005) can be investigated.
This leads to much richer language models that
could, for example, check whether there is already
a verb in the sentence (which helps in cases such
as {its, it’s}).
A different route which we would also like to in-
vestigate is the usage of a specific classifier, such
as TiMBL’s IGTree, as a language model. If a
classifier is trained to predict the next word in the
sentence or to predict the word at a given position
with both left and right context as features, it can
be used to estimate the probability of the words in
a confusible set, just like the language models we
have looked at so far. Another type of classifier
might estimate the perplexity at a position, or pro-
vide some other measure of “surprisedness”. Ef-
fectively, these approaches all take a model of the
entire language (as described in the training data)
into account.
</bodyText>
<sectionHeader confidence="0.987153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995136245283019">
Banko, M. and Brill, E. (2001). Scaling to very very
large corpora for natural language disambiguation.
In Proceedings ofthe 39th Annual Meeting ofthe As-
sociation for Computational Linguistics, pages 26–
33. Association for Computational Linguistics.
Chen, S. and Goodman, J. (1996). An empirical study
of smoothing techniques for language modelling. In
Proceedings of the 34th Annual Meeting of the ACL,
pages 310–318. ACL.
Daelemans, W., Van den Bosch, A., and Weijters, A.
(1997). IGTree: using trees for compression and
classification in lazy learning algorithms. Artificial
Intelligence Review, 11:407–423.
Daelemans, W., Zavrel, J., Van der Sloot, K., and Van
den Bosch, A. (2007). TiMBL: Tilburg Memory
Based Learner, version 6.1, reference guide. Techni-
cal Report ILK 07-07, ILK Research Group, Tilburg
University.
de la Higuera, C. (2005). A bibliographical study
of grammatical inference. Pattern Recognition,
38(9):1332 – 1348. Grammatical Inference.
Even-Zohar, Y. and Roth, D. (2000). A classification
approach to word prediction. In Proceedings of the
First North-American Conference on Computational
Linguistics, pages 124–131, New Brunswick, NJ.
ACL.
Golding, A. and Roth, D. (1999). A Winnow-Based
Approach to Context-Sensitive Spelling Correction.
Machine Learning, 34(1–3):107–130.
Golding, A. R. (1995). A Bayesian hybrid method for
context-sensitive spelling correction. In Proceed-
ings of the 3rd workshop on very large corpora,
ACL-95.
Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-
baum, J. B. (2005). Integrating topics and syntax. In
In Advances in Neural Information Processing Sys-
tems 17, pages 537–544. MIT Press.
Huang, J. H. and Powers, D. W. (2001). Large scale ex-
periments on correction of confused words. In Aus-
tralasian Computer Science Conference Proceed-
ings, pages 77–82, Queensland AU. Bond Univer-
sity.
Knuth, D. E. (1973). The art of computer program-
ming, volume 3: Sorting and searching. Addison-
Wesley, Reading, MA.
Lewis, D. D., Yang, Y., Rose, T. G., Dietterich, G., Li,
F., and Li, F. (2004). Rcv1: A new benchmark col-
lection for text categorization research. Journal of
Machine Learning Research, 5:361–397.
Mangu, L. and Brill, E. (1997). Automatic rule ac-
quisition for spelling correction. In Proceedings of
the International Conference on Machine Learning,
pages 187–194.
</reference>
<page confidence="0.986743">
47
</page>
<reference confidence="0.998470540540541">
Marcus, M., Santorini, S., and Marcinkiewicz, M.
(1993). Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
Quinlan, J. (1993). C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Sandra, D., Daems, F., and Frisson, S. (2001). Zo
helder en toch zoveel fouten! wat leren we uit psy-
cholinguistisch onderzoek naar werkwoordfouten
bij ervaren spellers? Tijdschrift van de Vereniging
voor het Onderwijs in het Nederlands, 30(3):3–20.
Van den Bosch, A. (2004). Wrapped progressive
sampling search for optimizing learning algorithm
parameters. In Verbrugge, R., Taatgen, N., and
Schomaker, L., editors, Proceedings of the Sixteenth
Belgian-Dutch Conference on Artificial Intelligence,
pages 219–226, Groningen, The Netherlands.
Van den Bosch, A. (2006). Scalable classification-
based word prediction and confusible correction.
Traitement Automatique des Langues, 46(2):39–63.
Van den Bosch, A. and Daelemans, W. (2007). Tussen
Taal, Spelling en Onderwijs, chapter Dat gebeurd
mei niet: Computationele modellen voor verwarbare
homofonen, pages 199–210. Academia Press.
Wu, D., Sui, Z., and Zhao, J. (1999). An information-
based method for selecting feature types for word
prediction. In Proceedings of the Sixth European
Conference on Speech Communication and Technol-
ogy, EUROSPEECH’99, Budapest.
Yarowsky, D. (1994). Decision lists for lexical ambi-
guity resolution: application to accent restoration in
Spanish and French. In Proceedings of the Annual
Meeting of the ACL, pages 88–95.
Zavrel, J. and Daelemans, W. (1997). Memory-based
learning: Using similarity for smoothing. In Pro-
ceedings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 436–443.
</reference>
<page confidence="0.999346">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.263261">
<title confidence="0.999884">Language models for contextual error detection and correction</title>
<author confidence="0.995603">Herman</author>
<affiliation confidence="0.837521">Tilburg Centre for Creative Tilburg</affiliation>
<address confidence="0.972335">Tilburg, The</address>
<email confidence="0.990275">j.h.stehouwer@uvt.nl</email>
<author confidence="0.899931">Menno van</author>
<affiliation confidence="0.769303">Tilburg Centre for Creative Tilburg</affiliation>
<address confidence="0.860685">Tilburg, The</address>
<email confidence="0.990915">mvzaanen@uvt.nl</email>
<abstract confidence="0.999361952380953">The problem of identifying and correcting confusibles, i.e. context-sensitive spelling errors, in text is typically tackled using specifically trained machine learning classifiers. For each different set of confusibles, a specific classifier is trained and tuned. In this research, we investigate a more generic approach to context-sensitive confusible correction. Instead of using specific classifiers, we use one generic classifier based on a language model. This measures the likelihood of sentences with different possible solutions of a confusible in place. The advantage of this approach is that all confusible sets are handled by a single model. Preliminary results show that the performance of the generic classifier approach is only slightly worse that that of the specific classifier approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings ofthe 39th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4435" citStr="Banko and Brill, 2001" startWordPosition="697" endWordPosition="700">tational Linguistic Aspects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; EvenZohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006). The task of the machine learner is to decide, using features describing information from the context, which word taken from the confusible set really belongs in the position of the confusible. Using the example above, the classifier has to decide which word belongs on the position of the X in She owns X cars, where the possible answers for X are to, too, or two. We call X, the confusible that is under consideration, the focus word. Another way of looking at the problem of confusible disambiguation is to see it as a very specialized case of word p</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, M. and Brill, E. (2001). Scaling to very very large corpora for natural language disambiguation. In Proceedings ofthe 39th Annual Meeting ofthe Association for Computational Linguistics, pages 26– 33. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modelling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>310--318</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10317" citStr="Chen and Goodman, 1996" startWordPosition="1689" endWordPosition="1692">pecially when n-grams with large n are used, data sparseness becomes an issue. The training data may not contain any occurrences of the particular sequence of n symbols, even though the sequence is correct. In that case, the probability extracted from the training data will be zero, even though the correct probability should be non-zero (albeit small). To reduce this problem we can either use back-off or smoothing when the probability of an n-gram is zero. In the case of back-off, the probabilities of lower order n-grams are taken into account when needed. Alternatively, smoothing techniques (Chen and Goodman, 1996) redistribute the probabilities, taking into account previously unseen word sequences. Even though the language models provide us with probabilities of entire sequences, we are only interested in the n-grams directly around the confusible when using the language models in the context of confusible disambiguation. The probabilities of the rest of the sequence will remain the same whichever alternative confusible is inserted in the focus word position. Figure 1 illustrates that the probability of for example P(analysts had expected) is irrelevant for the decision between then and than because it</context>
<context position="27095" citStr="Chen and Goodman, 1996" startWordPosition="4463" endWordPosition="4466">rained, compared to different systems for each confusible in the specific classifier case. Also, the exact computation of the probabilities using the n-grams, in particular the means of backing-off, has a large impact on the results. As future work, we would like to investigate the accuracy of more complex language models used as classifiers. The n-gram language models described here are relatively simple, but more complex language models could improve performance. In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996). This assumes that the training data we are currently working with is not enough to properly describe the language. Additionally, language models that concentrate on more structural descriptions of the language, for instance, using grammatical inference techniques (de la Higuera, 2005), or models that explicitly take long distance dependencies into account (Griffiths et al., 2005) can be investigated. This leads to much richer language models that could, for example, check whether there is already a verb in the sentence (which helps in cases such as {its, it’s}). A different route which we wo</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Chen, S. and Goodman, J. (1996). An empirical study of smoothing techniques for language modelling. In Proceedings of the 34th Annual Meeting of the ACL, pages 310–318. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>A Weijters</author>
</authors>
<title>IGTree: using trees for compression and classification in lazy learning algorithms.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<pages>11--407</pages>
<marker>Daelemans, Van den Bosch, Weijters, 1997</marker>
<rawString>Daelemans, W., Van den Bosch, A., and Weijters, A. (1997). IGTree: using trees for compression and classification in lazy learning algorithms. Artificial Intelligence Review, 11:407–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 6.1, reference guide.</title>
<date>2007</date>
<tech>Technical Report ILK 07-07,</tech>
<institution>ILK Research Group, Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2007</marker>
<rawString>Daelemans, W., Zavrel, J., Van der Sloot, K., and Van den Bosch, A. (2007). TiMBL: Tilburg Memory Based Learner, version 6.1, reference guide. Technical Report ILK 07-07, ILK Research Group, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de la Higuera</author>
<author>C</author>
</authors>
<title>A bibliographical study of grammatical inference.</title>
<date>2005</date>
<journal>Pattern Recognition,</journal>
<volume>38</volume>
<issue>9</issue>
<pages>1348</pages>
<note>Grammatical Inference.</note>
<marker>Higuera, C, 2005</marker>
<rawString>de la Higuera, C. (2005). A bibliographical study of grammatical inference. Pattern Recognition, 38(9):1332 – 1348. Grammatical Inference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Even-Zohar</author>
<author>D Roth</author>
</authors>
<title>A classification approach to word prediction.</title>
<date>2000</date>
<booktitle>In Proceedings of the First North-American Conference on Computational Linguistics,</booktitle>
<pages>124--131</pages>
<publisher>ACL.</publisher>
<location>New Brunswick, NJ.</location>
<marker>Even-Zohar, Roth, 2000</marker>
<rawString>Even-Zohar, Y. and Roth, D. (2000). A classification approach to word prediction. In Proceedings of the First North-American Conference on Computational Linguistics, pages 124–131, New Brunswick, NJ. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow-Based Approach to Context-Sensitive Spelling Correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="3517" citStr="Golding and Roth, 1999" startWordPosition="554" endWordPosition="557">matically incorrect words (where words are incorrectly used within the documentwide context) is considered as well. In this article we concentrate on the problem of confusibles, where the context is only as large as a sentence. 2 Approach A typical approach to the problem of confusibles is to train a machine learning classifier to a specific confusible set. Most of the work in this area has concentrated on confusibles due to homophony (to, too, two) or similar spelling (desert, dessert). However, some research has also touched upon inflectional or derivational confusibles such as I versus me (Golding and Roth, 1999). For instance, when word forms are homophonic, they tend to get confused often in writing (cf. the situation with to, too, and two, affect and effect, or there, their, and they’re in English) (Sandra et al., 2001; Van den Bosch and Daelemans, 2007). Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typica</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>Golding, A. and Roth, D. (1999). A Winnow-Based Approach to Context-Sensitive Spelling Correction. Machine Learning, 34(1–3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
</authors>
<title>A Bayesian hybrid method for context-sensitive spelling correction.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd workshop on very large corpora, ACL-95.</booktitle>
<contexts>
<context position="4346" citStr="Golding, 1995" startWordPosition="682" endWordPosition="683">an den Bosch and Daelemans, 2007). Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; EvenZohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006). The task of the machine learner is to decide, using features describing information from the context, which word taken from the confusible set really belongs in the position of the confusible. Using the example above, the classifier has to decide which word belongs on the position of the X in She owns X cars, where the possible answers for X are to, too, or two. We call X, the confusible that is under consideration, the focus word. Another way of looking at t</context>
</contexts>
<marker>Golding, 1995</marker>
<rawString>Golding, A. R. (1995). A Bayesian hybrid method for context-sensitive spelling correction. In Proceedings of the 3rd workshop on very large corpora, ACL-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="27479" citStr="Griffiths et al., 2005" startWordPosition="4521" endWordPosition="4524">re are relatively simple, but more complex language models could improve performance. In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996). This assumes that the training data we are currently working with is not enough to properly describe the language. Additionally, language models that concentrate on more structural descriptions of the language, for instance, using grammatical inference techniques (de la Higuera, 2005), or models that explicitly take long distance dependencies into account (Griffiths et al., 2005) can be investigated. This leads to much richer language models that could, for example, check whether there is already a verb in the sentence (which helps in cases such as {its, it’s}). A different route which we would also like to investigate is the usage of a specific classifier, such as TiMBL’s IGTree, as a language model. If a classifier is trained to predict the next word in the sentence or to predict the word at a given position with both left and right context as features, it can be used to estimate the probability of the words in a confusible set, just like the language models we have</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenenbaum, J. B. (2005). Integrating topics and syntax. In In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Huang</author>
<author>D W Powers</author>
</authors>
<title>Large scale experiments on correction of confused words.</title>
<date>2001</date>
<booktitle>In Australasian Computer Science Conference Proceedings,</booktitle>
<pages>77--82</pages>
<publisher>Bond University.</publisher>
<location>Queensland AU.</location>
<contexts>
<context position="4459" citStr="Huang and Powers, 2001" startWordPosition="701" endWordPosition="704">ects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; EvenZohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006). The task of the machine learner is to decide, using features describing information from the context, which word taken from the confusible set really belongs in the position of the confusible. Using the example above, the classifier has to decide which word belongs on the position of the X in She owns X cars, where the possible answers for X are to, too, or two. We call X, the confusible that is under consideration, the focus word. Another way of looking at the problem of confusible disambiguation is to see it as a very specialized case of word prediction. The problem i</context>
</contexts>
<marker>Huang, Powers, 2001</marker>
<rawString>Huang, J. H. and Powers, D. W. (2001). Large scale experiments on correction of confused words. In Australasian Computer Science Conference Proceedings, pages 77–82, Queensland AU. Bond University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>The art of computer programming, volume 3: Sorting and searching.</title>
<date>1973</date>
<publisher>AddisonWesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="16468" citStr="Knuth, 1973" startWordPosition="2729" endWordPosition="2730">idered. The focus word becomes the class that needs to be predicted. We show an example of both training and testing in figure 2. Note that the features for the machine learning classifiers could be expanded with, for instance, part-of-speech tags, but in the current experiments only the word forms are used as features. In addition to the k-NN classifier, we also run the experiments using the IGTree classifier, which is denoted IGTree in the rest of the article, which is also contained in the TiMBL distribution. IGTree is a fast, trie based, approximation of k-nearest neighbor classification (Knuth, 1973; Daelemans et al., 1997). IGTree allows for fast training and testing even with millions of examples. IGTree compresses a set of labeled examples into a decision tree structure similar to the classic C4.5 algorithm (Quinlan, 1993), except that throughout one level in the IGTree decision tree, the same feature is tested. Classification in IGTree is a simple procedure in which the decision tree is traversed from the root node down, and one path is followed that matches the actual values of the new example to be classified. If a leaf is found, the outcome stored at the leaf of the IGTree is retu</context>
</contexts>
<marker>Knuth, 1973</marker>
<rawString>Knuth, D. E. (1973). The art of computer programming, volume 3: Sorting and searching. AddisonWesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T G Rose</author>
<author>G Dietterich</author>
<author>F Li</author>
<author>F Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="18936" citStr="Lewis et al., 2004" startWordPosition="3134" endWordPosition="3137">ce, which word should be the focus word. model (Zavrel and Daelemans, 1997), while still being a generic classifier that supports any number and type of features. For these reasons, IGTree is also included in the experiments. 3.2 Experimental settings The probabilities used in the language models of the generic classifiers are computed by looking at occurrences of n-grams. These occurrences are extracted from a corpus. The training instances used in the specific machine learning classifiers are also extracted from the same data set. For training purposes, we used the Reuters news corpus RCV1 (Lewis et al., 2004). The Reuters corpus contains about 810,000 categorized newswire stories as published by Reuters in 1996 and 1997. This corpus contains around 130 million tokens. For testing purposes, we used the Wall Street Journal part of the Penn Treebank corpus (Marcus et al., 1993). This well-known corpus contains articles from the Wall Street Journal in 1987 to 1989. We extract our test-instances from this corpus in the same way as we extract our training data from the Reuters corpus. There are minor tokenization differences between the corpora. The data is corrected for these differences. Both corpora </context>
</contexts>
<marker>Lewis, Yang, Rose, Dietterich, Li, Li, 2004</marker>
<rawString>Lewis, D. D., Yang, Y., Rose, T. G., Dietterich, G., Li, F., and Li, F. (2004). Rcv1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>187--194</pages>
<contexts>
<context position="4369" citStr="Mangu and Brill, 1997" startWordPosition="684" endWordPosition="687">d Daelemans, 2007). Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; EvenZohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006). The task of the machine learner is to decide, using features describing information from the context, which word taken from the confusible set really belongs in the position of the confusible. Using the example above, the classifier has to decide which word belongs on the position of the X in She owns X cars, where the possible answers for X are to, too, or two. We call X, the confusible that is under consideration, the focus word. Another way of looking at the problem of confusibl</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>Mangu, L. and Brill, E. (1997). Automatic rule acquisition for spelling correction. In Proceedings of the International Conference on Machine Learning, pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>S Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="19207" citStr="Marcus et al., 1993" startWordPosition="3178" endWordPosition="3181">used in the language models of the generic classifiers are computed by looking at occurrences of n-grams. These occurrences are extracted from a corpus. The training instances used in the specific machine learning classifiers are also extracted from the same data set. For training purposes, we used the Reuters news corpus RCV1 (Lewis et al., 2004). The Reuters corpus contains about 810,000 categorized newswire stories as published by Reuters in 1996 and 1997. This corpus contains around 130 million tokens. For testing purposes, we used the Wall Street Journal part of the Penn Treebank corpus (Marcus et al., 1993). This well-known corpus contains articles from the Wall Street Journal in 1987 to 1989. We extract our test-instances from this corpus in the same way as we extract our training data from the Reuters corpus. There are minor tokenization differences between the corpora. The data is corrected for these differences. Both corpora are in the domain of English language news texts, so we expect them to have similar properties. However, they are different corpora and hence are slightly different. This means that there are also differences between the training and testing set. We have selected this di</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., Santorini, S., and Marcinkiewicz, M. (1993). Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="16699" citStr="Quinlan, 1993" startWordPosition="2767" endWordPosition="2768">part-of-speech tags, but in the current experiments only the word forms are used as features. In addition to the k-NN classifier, we also run the experiments using the IGTree classifier, which is denoted IGTree in the rest of the article, which is also contained in the TiMBL distribution. IGTree is a fast, trie based, approximation of k-nearest neighbor classification (Knuth, 1973; Daelemans et al., 1997). IGTree allows for fast training and testing even with millions of examples. IGTree compresses a set of labeled examples into a decision tree structure similar to the classic C4.5 algorithm (Quinlan, 1993), except that throughout one level in the IGTree decision tree, the same feature is tested. Classification in IGTree is a simple procedure in which the decision tree is traversed from the root node down, and one path is followed that matches the actual values of the new example to be classified. If a leaf is found, the outcome stored at the leaf of the IGTree is returned as the classification. If the last node is not a leaf node, but there are no outgoing arcs that match a featurevalue combination of the instance, the most likely outcome stored at that node is produced as the resulting classif</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sandra</author>
<author>F Daems</author>
<author>S Frisson</author>
</authors>
<title>Zo helder en toch zoveel fouten! wat leren we uit psycholinguistisch onderzoek naar werkwoordfouten bij ervaren spellers? Tijdschrift van de Vereniging voor het Onderwijs in het</title>
<date>2001</date>
<journal>Nederlands,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="3730" citStr="Sandra et al., 2001" startWordPosition="591" endWordPosition="594"> sentence. 2 Approach A typical approach to the problem of confusibles is to train a machine learning classifier to a specific confusible set. Most of the work in this area has concentrated on confusibles due to homophony (to, too, two) or similar spelling (desert, dessert). However, some research has also touched upon inflectional or derivational confusibles such as I versus me (Golding and Roth, 1999). For instance, when word forms are homophonic, they tend to get confused often in writing (cf. the situation with to, too, and two, affect and effect, or there, their, and they’re in English) (Sandra et al., 2001; Van den Bosch and Daelemans, 2007). Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 199</context>
</contexts>
<marker>Sandra, Daems, Frisson, 2001</marker>
<rawString>Sandra, D., Daems, F., and Frisson, S. (2001). Zo helder en toch zoveel fouten! wat leren we uit psycholinguistisch onderzoek naar werkwoordfouten bij ervaren spellers? Tijdschrift van de Vereniging voor het Onderwijs in het Nederlands, 30(3):3–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Wrapped progressive sampling search for optimizing learning algorithm parameters.</title>
<date>2004</date>
<booktitle>Proceedings of the Sixteenth Belgian-Dutch Conference on Artificial Intelligence,</booktitle>
<pages>219--226</pages>
<editor>In Verbrugge, R., Taatgen, N., and Schomaker, L., editors,</editor>
<location>Groningen, The Netherlands.</location>
<marker>Van den Bosch, 2004</marker>
<rawString>Van den Bosch, A. (2004). Wrapped progressive sampling search for optimizing learning algorithm parameters. In Verbrugge, R., Taatgen, N., and Schomaker, L., editors, Proceedings of the Sixteenth Belgian-Dutch Conference on Artificial Intelligence, pages 219–226, Groningen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Scalable classificationbased word prediction and confusible correction.</title>
<date>2006</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<pages>46--2</pages>
<marker>Van den Bosch, 2006</marker>
<rawString>Van den Bosch, A. (2006). Scalable classificationbased word prediction and confusible correction. Traitement Automatique des Langues, 46(2):39–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
<author>W Daelemans</author>
</authors>
<title>Tussen Taal, Spelling en Onderwijs, chapter Dat gebeurd mei niet: Computationele modellen voor verwarbare homofonen,</title>
<date>2007</date>
<pages>199--210</pages>
<publisher>Academia Press.</publisher>
<marker>Van den Bosch, Daelemans, 2007</marker>
<rawString>Van den Bosch, A. and Daelemans, W. (2007). Tussen Taal, Spelling en Onderwijs, chapter Dat gebeurd mei niet: Computationele modellen voor verwarbare homofonen, pages 199–210. Academia Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>Z Sui</author>
<author>J Zhao</author>
</authors>
<title>An informationbased method for selecting feature types for word prediction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixth European Conference on Speech Communication and Technology, EUROSPEECH’99,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="4386" citStr="Wu et al., 1999" startWordPosition="688" endWordPosition="691">ceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; EvenZohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006). The task of the machine learner is to decide, using features describing information from the context, which word taken from the confusible set really belongs in the position of the confusible. Using the example above, the classifier has to decide which word belongs on the position of the X in She owns X cars, where the possible answers for X are to, too, or two. We call X, the confusible that is under consideration, the focus word. Another way of looking at the problem of confusible disambiguation </context>
</contexts>
<marker>Wu, Sui, Zhao, 1999</marker>
<rawString>Wu, D., Sui, Z., and Zhao, J. (1999). An informationbased method for selecting feature types for word prediction. In Proceedings of the Sixth European Conference on Speech Communication and Technology, EUROSPEECH’99, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the Annual Meeting of the ACL,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="4331" citStr="Yarowsky, 1994" startWordPosition="680" endWordPosition="681"> et al., 2001; Van den Bosch and Daelemans, 2007). Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41–48, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 41 Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; EvenZohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006). The task of the machine learner is to decide, using features describing information from the context, which word taken from the confusible set really belongs in the position of the confusible. Using the example above, the classifier has to decide which word belongs on the position of the X in She owns X cars, where the possible answers for X are to, too, or two. We call X, the confusible that is under consideration, the focus word. Another way </context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>Yarowsky, D. (1994). Decision lists for lexical ambiguity resolution: application to accent restoration in Spanish and French. In Proceedings of the Annual Meeting of the ACL, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Memory-based learning: Using similarity for smoothing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>436--443</pages>
<contexts>
<context position="18392" citStr="Zavrel and Daelemans, 1997" startWordPosition="3048" endWordPosition="3051">One of the unique characteristics of IGTree compared to basic k-NN is its resemblance to smoothing of a basic language 44 Training ... much stronger than most analysts had expected. (much, stronger, most, analysts) Athan Testing ... much stronger most analysts had expected. (much, stronger, most, analysts) A? Figure 2: During training, a classified instance (in this case for the confusible pair {then, than}) are generated from a sentence. During testing, a similar instance is generated. The classifier decides what the corresponding class, and hence, which word should be the focus word. model (Zavrel and Daelemans, 1997), while still being a generic classifier that supports any number and type of features. For these reasons, IGTree is also included in the experiments. 3.2 Experimental settings The probabilities used in the language models of the generic classifiers are computed by looking at occurrences of n-grams. These occurrences are extracted from a corpus. The training instances used in the specific machine learning classifiers are also extracted from the same data set. For training purposes, we used the Reuters news corpus RCV1 (Lewis et al., 2004). The Reuters corpus contains about 810,000 categorized </context>
</contexts>
<marker>Zavrel, Daelemans, 1997</marker>
<rawString>Zavrel, J. and Daelemans, W. (1997). Memory-based learning: Using similarity for smoothing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 436–443.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>