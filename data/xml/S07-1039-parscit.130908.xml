<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000965">
<title confidence="0.742308">
ILK: Machine learning of semantic relations with shallow features
and almost no data
Iris Hendrickx
CNTS / Language Technology Group
</title>
<author confidence="0.717477">
Roser Morante, Caroline Sporleder,
Antal van den Bosch
</author>
<affiliation confidence="0.780532">
Uversity of Antwerp,
</affiliation>
<address confidence="0.8682005">
Universiteitsplein 1
2610 Wilrijk, Belgium
</address>
<email confidence="0.995878">
iris.hendrickx@ua.ac.be
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999826545454545">
This paper summarizes our approach to the
Semeval 2007 shared task on “Classifica-
tion of Semantic Relations between Nom-
inals”. Our overall strategy is to develop
machine-learning classifiers making use of
a few easily computable and effective fea-
tures, selected independently for each clas-
sifier in wrapper experiments. We train two
types of classifiers for each of the seven re-
lations: with and without WordNet informa-
tion.
</bodyText>
<sectionHeader confidence="0.974662" genericHeader="introduction">
1 Introduction
</sectionHeader>
<note confidence="0.348306333333333">
ILK / Communication and Information Sciences
Tilburg University, P.O. Box 90153,
5000 LE Tilburg, The Netherlands
</note>
<email confidence="0.8321805">
{R.Morante,C.Sporleder,
Antal.vdnBosch}@uvt.nl
</email>
<bodyText confidence="0.999845909090909">
generated semantic clusters (Decadt and Daelemans,
2004) to model the semantic classes of the entities.
For both systems we trained seven binary clas-
sifiers; one for each relation. From a pool of eas-
ily computable features, we selected feature subsets
for each classifier in a number of wrapper exper-
iments, i.e. repeated cross-validation experiments
on the training set to test out subset selections sys-
tematically. Along with feature subsets we also
chose the machine-learning method independently
for each classifier.
Section 2 presents the system description, Sec-
tion 3, the results, and Section 4, the conclusions.
We interpret the task of determining semantic rela-
tions between nominals as a classification problem
that can be solved, per relation, by machine learning
algorithms. We aim at using straightforward features
that are easy to compute and relevant to preferably
all of the seven relations central to the task.
The starting conditions of the task provide us with
a very small amount of training data, which further
stresses the need for robust, generalizable features,
that generalize beyond surface words. We there-
fore hypothesize that generic information on the lex-
ical semantics of the entities involved in the rela-
tion is crucial. We developed two systems, based
on two sources of semantic information. Since the
entities in the provided data were word-sense dis-
ambiguated, an obvious way to model their lexical
semantics was by utilizing WordNet3.0 (Fellbaum,
1998) (WN). One of the systems followed this route.
We also entered a second system, which did not
rely on WN but instead made use of automatically
</bodyText>
<sectionHeader confidence="0.978516" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999511">
The development of the system consists of a prepro-
cessing phase to extract the features, and the classi-
fication phase.
</bodyText>
<subsectionHeader confidence="0.992928">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999655071428571">
Each sentence is preprocessed automatically in the
following steps. First, the sentence is tokenized with
a rule-based tokenizer. Next a part-of-speech tag-
ger and text chunker that use the memory-based tag-
ger MBT (Daelemans et al., 1996) produces part-
of-speech tags and NP chunk labels for each token.
Then a memory-based shallow parser predicts gram-
matical relations between verbs and NP chunks such
as subject, object or modifier (Buchholz, 2002). The
tagger, chunker and parser were all trained on the
WSJ Corpus (Marcus et al., 1993). We also use
a memory-based lemmatizer (Van den Bosch et al.,
1996) trained on Celex (Baayen et al., 1993) to pre-
dict the lemma of each word.
</bodyText>
<page confidence="0.984145">
187
</page>
<bodyText confidence="0.982079720930233">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190,
Prague, June 2007. c�2007 Association for Computational Linguistics
The features extracted are of three types: seman-
tic, lexical, and morpho-syntactic. The features that
apply to the entities in a relation (e1,e2) are extracted
for term 1 (t1) and term 2 (t2) of the relation, where
t1 is the first term in the relation name, and t2 is the
second term. For example, in the relation CAUSE–
EFFECT, t1 is CAUSE and t2 is EFFECT.
The semantic features are the following:
WN semantic class of t1 and t2. The WN seman-
tic class of each entity in the relation. For the WN-
based system, we determined the semantic class of
the entities on the basis of the lexicographer file
numbers (LFN) in WN3.0. The LFN are encoded in
the synset number provided in the annotation of the
data. For nouns there are 25 file numbers that corre-
spond to suitably abstract semantic classes, namely:
noun.Tops(top concepts for nouns), act, animal, artifact, at-
tribute, body, cognition, communication event, feeling, food,
group, location, motive, object, person, phenomenon, plant,
possession, process, quantity, relation, shape, state, substance,
time.
Is container (is C). Exclusively for the
CONTENT–CONTAINER relation we furthermore
included two binary features that test whether the
two entities in the relation are hyponyms of the
synset container in WN. For the PART–WHOLE
relation we also experimented with binary features
expressing whether the two entities in the relation
have some type of meronym and holonym relation,
but these features did not prove to be predictive.
Cluster class of t1 and t2. A cluster class iden-
tifier for each entity in the relation. This informa-
tion is drawn from automatically generated clusters
of semantically similar nouns (Decadt and Daele-
mans, 2004) generated on the British National Cor-
pus (Clear, 1993). The corpus was first prepro-
cessed by a lemmatizer and the memory-based shal-
low parser, and the found verb–object relations were
used to cluster nouns in groups. We used the top-
5000 lemmatized nouns, that are clustered into 250
groups. This is an example of two of these clusters:
</bodyText>
<listItem confidence="0.7971732">
• {can pot basin tray glass container bottle tin pan mug cup
jar bowl bucket plate jug vase kettle}
• {booth restaurant bath kitchen hallway toilet bedroom
hall suite bathroom interior lounge shower compartment
oven lavatory room}
</listItem>
<bodyText confidence="0.990354346153846">
The lexical features are the following:
Lemma of t1 and t2 (lem1, lem2). The lemmas of
the entities involved in the relation. In case an entity
consisted of multiple words (e.g. storage room) we
use the lemma of the head noun (i.e. room).
Main verb (verb). The main verb of the sentence
in which the entities involved in the relation appear,
as predicted by the shallow parser.
The morpho-syntactic features are:
GramRel (gr1, gr2). The grammatical relation
tags of the entities.
Suffixes of t1 and t2 (suf1, suf2). The suffixes of
the entity lemmas. We implemented a rule-based
suffix guesser, which determines whether the nouns
involved in the relation end in a derivational suffix,
such as -ee, -ment etc. Suffixes often provide cues
for semantic properties of the entities. For exam-
ple, the suffix -ee usually indicates animate (and typ-
ically human) referents (e.g. detainee etc.), whereas
(-ment) points at abstract entities (e.g. statement).
While the features were selected independently
for all relations, the seven classifiers in the WN-
based system all make use of the WN semantic class
features; in the system that did not use WN, the
seven classifiers make use of the cluster class fea-
tures instead.
</bodyText>
<subsectionHeader confidence="0.984274">
2.2 Classification
</subsectionHeader>
<bodyText confidence="0.999844">
We experimented with several machine learning
frameworks and different feature (sub-)sets. For
rapid testing of different learners and feature sets,
and given the size of the training data (140 exam-
ples for each relation), we made use of the Weka ma-
chine learning software1 (Witten and Frank, 1999).
We systematically tested the following algorithms:
NaiveBayes (NB) (Langley et al., 1992), BayesNet
(BN) (Cooper and Herskovits, 1992), J48 (Quinlan,
1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al.,
1991), LWL (Atkeson et al., 1997), and Decision-
Stumps (DS) (Iba and Langley, 1992), all with de-
fault algorithm settings.
The classifiers for all seven relations were opti-
mized independently in a number of 10-fold cross-
validation (CV) experiments on the provided train-
</bodyText>
<footnote confidence="0.991907">
1http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<page confidence="0.993311">
188
</page>
<bodyText confidence="0.999241454545455">
ing sets. The feature sets and learning algorithms
which were found to obtain the highest accuracies
for each relation were then used when applying the
classifiers to the unseen test data.
The classifiers of the cluster-based system (A) all
use the two cluster class features. The other se-
lected features and the chosen algorithms (CL) are
displayed in Table 1. Knowledge of the identity of
the lemmas was found to be beneficial for all clas-
sifiers. With respect to the machine learning frame-
work, Naive Bayes was selected most frequently.
</bodyText>
<figure confidence="0.538397">
Relation CL lem1 lem2 verb gr1 gr2 suf1 suf2
Cause-Effect DS + + + + + + +
Instr-Agency LWL + + + + +
Product-Producer NB + + + + + + +
Origin-Entity IBk + + + + + +
Theme-Tool NB + + + + +
Part-Whole NB + + + + + +
Content-Container NB + + + + + +
</figure>
<tableCaption confidence="0.8241355">
Table 1: The final selected algorithms and features
for each relation by the cluster-based system (A).
</tableCaption>
<bodyText confidence="0.999860818181818">
The classifiers of the WN-based system (B) all
use at least the WN semantic class features. Ta-
ble 2 shows the other selected features and algorithm
for each relation. None of the classifiers use all the
features. For the part-whole relation no extra fea-
tures besides the WN class are selected. Also the
classifiers for the relations cause-effect and content-
container only use two additional features. The list
of best found algorithms shows that —like with the
cluster-based system— a Bayesian approach is fa-
vorable, as it is selected in four of seven cases.
</bodyText>
<table confidence="0.97789475">
Relation CL lem1 lem2 verb gr1 gr2 suf1 suf2 is C
Cause-Effect BN + +
Instr-Agency NB + + +
Product-Producer IB1 + + + +
Origin-Entity IBk + + + + +
Theme-Tool NB + + + + + +
Part-Whole J48
Content-Container BN + +
</table>
<tableCaption confidence="0.757373333333333">
Table 2: The final selected algorithms and features
for each relation by the WN-based system (B). (is C
is the CONTENT-CONTAINER specific feature.)
</tableCaption>
<sectionHeader confidence="0.999935" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999610142857143">
In Table 3 we first present the best results computed
on the training set using 10-fold CV for the cluster-
based system (A) and the WN-based system (B).
These results are generally higher than the official
test set results, shown in Tables 4 and 5, possibly
showing a certain amount of overfitting on the train-
ing sets.
</bodyText>
<table confidence="0.998363555555556">
Relation A B
Cause-Effect 56.4 72.9
Instrument-Agency 71.4 75.7
Product-Producer 65.0 67.9
Origin-Entity 70.7 78.6
Theme-Tool 75.7 79.3
Part-Whole 65.7 73.6
Content-Container 70.0 75.4
Avg 67.9 74.8
</table>
<tableCaption confidence="0.888007333333333">
Table 3: Average accuracy on the training set com-
puted in 10-fold CV experiments of the cluster-
based system (A) and the WN-based system (B).
</tableCaption>
<bodyText confidence="0.997538222222222">
The official scores on the test set are computed
by the task organizers: accuracy, precision, recall
and Fl score. Table 4 presents the results of the
cluster-based system. Table 5 presents the results
of the WN-based system. (The column Total shows
the number of instances in the test set.) Markable is
the high accuracy for the PART-WHOLE relation as
the classifier was only trained on two features cod-
ing the WN classes.
</bodyText>
<table confidence="0.998301333333333">
A4 Pre Rec F Acc Total
Cause–Effect 53.3 97.6 69.0 55.0 80
Instrument–Agency 56.1 60.5 58.2 57.7 78
Product–Producer 69.1 75.8 72.3 61.3 93
Origin–Entity 60.7 47.2 53.1 63.0 81
Theme–Tool 64.5 69.0 66.7 71.8 71
Part–Whole 48.4 57.7 52.6 62.5 72
Content–Container 71.4 78.9 75.0 73.0 74
Avg 60.5 69.5 63.8 63.5 78.4
</table>
<tableCaption confidence="0.992058">
Table 4: Test scores for the seven relations of the
cluster-based system trained on 140 examples (A4).
</tableCaption>
<bodyText confidence="0.9938439">
The system using all training data with WordNet
features, B4 (Table 5), performs better in terms of F-
score on six out of the seven subtasks as compared
to the system that does not use the WordNet features
but the semantic cluster information instead, A4 (Ta-
ble 4). This is largely due to a lower precision of the
A4 system. The WordNet features appear to be di-
rectly responsible for a relatively higher precision.
In contrast, the semantic cluster features of sys-
tem A sometimes boost recall. A4’s recall on the
</bodyText>
<page confidence="0.995677">
189
</page>
<table confidence="0.996886222222222">
B4 Pre Rec F Acc Total
Cause–Effect 69.0 70.7 69.9 68.8 80
Instrument–Agency 69.8 78.9 74.1 73.1 78
Product–Producer 79.7 75.8 77.7 71.0 93
Origin–Entity 71.0 61.1 65.7 71.6 81
Theme–Tool 69.0 69.0 69.0 74.6 71
Part–Whole 73.1 73.1 73.1 80.6 72
Content–Container 78.1 65.8 71.4 73.0 74
Avg 72.8 70.6 71.5 73.2 78.4
</table>
<tableCaption confidence="0.9922545">
Table 5: Test scores for the seven relations of the
WN-based system trained on 140 examples (B4).
</tableCaption>
<bodyText confidence="0.97914475">
CAUSE–EFFECT relation is 97.6% (the classifier pre-
dicts the class ’true’ for 75 of the 80 examples),
and on CONTENT–CONTAINER the system attains
78.9%, markedly better than B4.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987206896552">
We have shown that a machine learning approach us-
ing shallow and easily computable features performs
quite well on this task. The system using Word-
Net features based on the provided disambiguated
word senses outperforms the cluster-based system.
It would be interesting to compare both systems to a
more realistic WN-based system that uses predicted
word senses by a Word Sense Disambiguation sys-
tem.
However we end by noting that the amount of
training and test data in this shared task should be
considered too small to base any reliable conclu-
sions on. In a realistic scenario (e.g. when high-
precision relation classification would be needed as
a component of a question-answering system), more
training material would have been gathered, and the
examples would not have been seeded by a limited
number of queries – especially the negative exam-
ples are very artificial now due to their similarity to
the positive cases, and the fact that they are down-
sampled very unrealistically. Rather, the focus of the
task should be on detecting positive instances of the
relations in vast amounts of text (i.e. vast amounts of
implicit negative examples). Positive training exam-
ples should be as randomly sampled from raw text
as possible. The seven relations are common enough
to warrant a focused effort to annotate a reasonable
amount of randomly selected text, gathering several
hundreds of positive cases of each relation.
</bodyText>
<sectionHeader confidence="0.989802" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721549019608">
D. W. Aha, D. Kibler, M. Albert. 1991. Instance-based
learning algorithms. Machine Learning, 6:37–66.
C. Atkeson, A. Moore, S. Schaal. 1997. Locally
weighted learning. Artificial Intelligence Review,
11(1–5):11–73.
R. H. Baayen, R. Piepenbrock, H. van Rijn. 1993. The
CELEX lexical data base on CD-ROM. Linguistic
Data Consortium, Philadelphia, PA.
S. Buchholz. 2002. Memory-Based Grammatical Rela-
tion Finding. PhD thesis, University of Tilburg.
J. H. Clear. 1993. The British national corpus. MIT
Press, Cambridge, MA, USA.
W. Cohen. 1995. Fast effective rule induction. In Pro-
ceedings of the 12th International Conference on Ma-
chine Learning, 115–123. Morgan Kaufmann.
G. F. Cooper, E. Herskovits. 1992. A bayesian method
for the induction of probabilistic networks from data.
Machine Learning, 9(4):309–347.
W. Daelemans, J. Zavrel, P. Berck, S. Gillis. 1996.
Mbt: A memory-based part of speech tagger genera-
tor. In Proceedings of the 4th ACL/SIGDAT Workshop
on Very Large Corpora, 14–27.
B. Decadt, W. Daelemans. 2004. Verb classification -
machine learning experiments in classifying verbs into
semantic classes. In Proceedings of the LREC 2004
Workshop Beyond Named Entity Recognition: Seman-
tic Labeling for NLP Tasks, 25–30, Lisbon, Portugal.
C. Fellbaum, ed. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
W. Iba, P. Langley. 1992. Induction of one-level decision
trees. Proceedings of the Ninth International Confer-
ence on Machine Learning, 233–240.
P. Langley, W. Iba, K. Thompson. 1992. An analysis of
Bayesian classifiers. In Proceedings of the Tenth An-
nual Conference on Artificial Intelligence, 223–228.
AAAI Press and MIT Press.
M. Marcus, S. Santorini, M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics,
19(2):313–330.
J. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA.
A. Van den Bosch, W. Daelemans, A. Weijters. 1996.
Morphological analysis as classification: an inductive-
learning approach. In K. Oflazer, H. Somers, eds.,
Proceedings of the Second International Conference
on New Methods in Natural Language Processing,
NeMLaP-2, Ankara, Turkey, 79–89.
I. H. Witten, E. Frank. 1999. Data Mining: Practical
Machine Learning Tools and Techniques with Java Im-
plementations. Morgan Kaufman, San Francisco, CA.
</reference>
<page confidence="0.997866">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.509381">
<title confidence="0.965533">ILK: Machine learning of semantic relations with shallow features and almost no data</title>
<author confidence="0.993988">Iris Hendrickx</author>
<affiliation confidence="0.93764">CNTS / Language Technology Group</affiliation>
<author confidence="0.8676515">Roser Morante</author>
<author confidence="0.8676515">Caroline Sporleder</author>
<author confidence="0.8676515">Antal van_den_Bosch</author>
<affiliation confidence="0.999978">Uversity of Antwerp,</affiliation>
<address confidence="0.971758">Universiteitsplein 1 2610 Wilrijk, Belgium</address>
<email confidence="0.987542">iris.hendrickx@ua.ac.be</email>
<abstract confidence="0.981265333333333">This paper summarizes our approach to the Semeval 2007 shared task on “Classification of Semantic Relations between Nominals”. Our overall strategy is to develop machine-learning classifiers making use of a few easily computable and effective features, selected independently for each classifier in wrapper experiments. We train two types of classifiers for each of the seven relations: with and without WordNet information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="7526" citStr="Aha et al., 1991" startWordPosition="1191" endWordPosition="1194">that did not use WN, the seven classifiers make use of the cluster class features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highest accuracies for each relation were then used when applying the classifiers to the unseen test data. The classifiers of the cluster-based system (A) all use the two cluster class features. The other selected </context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. W. Aha, D. Kibler, M. Albert. 1991. Instance-based learning algorithms. Machine Learning, 6:37–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Atkeson</author>
<author>A Moore</author>
<author>S Schaal</author>
</authors>
<title>Locally weighted learning.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<pages>11--1</pages>
<contexts>
<context position="7554" citStr="Atkeson et al., 1997" startWordPosition="1196" endWordPosition="1199"> seven classifiers make use of the cluster class features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highest accuracies for each relation were then used when applying the classifiers to the unseen test data. The classifiers of the cluster-based system (A) all use the two cluster class features. The other selected features and the chosen algo</context>
</contexts>
<marker>Atkeson, Moore, Schaal, 1997</marker>
<rawString>C. Atkeson, A. Moore, S. Schaal. 1997. Locally weighted learning. Artificial Intelligence Review, 11(1–5):11–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX lexical data base on CD-ROM. Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>R. H. Baayen, R. Piepenbrock, H. van Rijn. 1993. The CELEX lexical data base on CD-ROM. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
</authors>
<title>Memory-Based Grammatical Relation Finding.</title>
<date>2002</date>
<tech>PhD thesis,</tech>
<institution>University of Tilburg.</institution>
<contexts>
<context position="3122" citStr="Buchholz, 2002" startWordPosition="476" endWordPosition="477">atically 2 System Description The development of the system consists of a preprocessing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, Prague, June 2007. c�2007 Association for Computational Linguistics The features extracted are of three types: semantic, lexical, and morpho-syntactic. The features that apply to the entities in a relation (e1,e2) are extracted for term 1 (t1) and term 2 (t2) of </context>
</contexts>
<marker>Buchholz, 2002</marker>
<rawString>S. Buchholz. 2002. Memory-Based Grammatical Relation Finding. PhD thesis, University of Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Clear</author>
</authors>
<title>The British national corpus.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="5276" citStr="Clear, 1993" startWordPosition="826" endWordPosition="827">e included two binary features that test whether the two entities in the relation are hyponyms of the synset container in WN. For the PART–WHOLE relation we also experimented with binary features expressing whether the two entities in the relation have some type of meronym and holonym relation, but these features did not prove to be predictive. Cluster class of t1 and t2. A cluster class identifier for each entity in the relation. This information is drawn from automatically generated clusters of semantically similar nouns (Decadt and Daelemans, 2004) generated on the British National Corpus (Clear, 1993). The corpus was first preprocessed by a lemmatizer and the memory-based shallow parser, and the found verb–object relations were used to cluster nouns in groups. We used the top5000 lemmatized nouns, that are clustered into 250 groups. This is an example of two of these clusters: • {can pot basin tray glass container bottle tin pan mug cup jar bowl bucket plate jug vase kettle} • {booth restaurant bath kitchen hallway toilet bedroom hall suite bathroom interior lounge shower compartment oven lavatory room} The lexical features are the following: Lemma of t1 and t2 (lem1, lem2). The lemmas of </context>
</contexts>
<marker>Clear, 1993</marker>
<rawString>J. H. Clear. 1993. The British national corpus. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the 12th International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="7494" citStr="Cohen, 1995" startWordPosition="1186" endWordPosition="1187">ss features; in the system that did not use WN, the seven classifiers make use of the cluster class features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highest accuracies for each relation were then used when applying the classifiers to the unseen test data. The classifiers of the cluster-based system (A) all use the two cluster cla</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. Cohen. 1995. Fast effective rule induction. In Proceedings of the 12th International Conference on Machine Learning, 115–123. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F Cooper</author>
<author>E Herskovits</author>
</authors>
<title>A bayesian method for the induction of probabilistic networks from data.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="7453" citStr="Cooper and Herskovits, 1992" startWordPosition="1178" endWordPosition="1181">in the WNbased system all make use of the WN semantic class features; in the system that did not use WN, the seven classifiers make use of the cluster class features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highest accuracies for each relation were then used when applying the classifiers to the unseen test data. The classifiers of the cluster-bas</context>
</contexts>
<marker>Cooper, Herskovits, 1992</marker>
<rawString>G. F. Cooper, E. Herskovits. 1992. A bayesian method for the induction of probabilistic networks from data. Machine Learning, 9(4):309–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>Mbt: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th ACL/SIGDAT Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<contexts>
<context position="2911" citStr="Daelemans et al., 1996" startWordPosition="441" endWordPosition="444">ous way to model their lexical semantics was by utilizing WordNet3.0 (Fellbaum, 1998) (WN). One of the systems followed this route. We also entered a second system, which did not rely on WN but instead made use of automatically 2 System Description The development of the system consists of a preprocessing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, Prague, June 2007. c�2007 Association for Computatio</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, S. Gillis. 1996. Mbt: A memory-based part of speech tagger generator. In Proceedings of the 4th ACL/SIGDAT Workshop on Very Large Corpora, 14–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Decadt</author>
<author>W Daelemans</author>
</authors>
<title>Verb classification -machine learning experiments in classifying verbs into semantic classes.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC</booktitle>
<location>25–30, Lisbon, Portugal.</location>
<contexts>
<context position="942" citStr="Decadt and Daelemans, 2004" startWordPosition="128" endWordPosition="131">oach to the Semeval 2007 shared task on “Classification of Semantic Relations between Nominals”. Our overall strategy is to develop machine-learning classifiers making use of a few easily computable and effective features, selected independently for each classifier in wrapper experiments. We train two types of classifiers for each of the seven relations: with and without WordNet information. 1 Introduction ILK / Communication and Information Sciences Tilburg University, P.O. Box 90153, 5000 LE Tilburg, The Netherlands {R.Morante,C.Sporleder, Antal.vdnBosch}@uvt.nl generated semantic clusters (Decadt and Daelemans, 2004) to model the semantic classes of the entities. For both systems we trained seven binary classifiers; one for each relation. From a pool of easily computable features, we selected feature subsets for each classifier in a number of wrapper experiments, i.e. repeated cross-validation experiments on the training set to test out subset selections systematically. Along with feature subsets we also chose the machine-learning method independently for each classifier. Section 2 presents the system description, Section 3, the results, and Section 4, the conclusions. We interpret the task of determining</context>
<context position="5221" citStr="Decadt and Daelemans, 2004" startWordPosition="814" endWordPosition="818">r (is C). Exclusively for the CONTENT–CONTAINER relation we furthermore included two binary features that test whether the two entities in the relation are hyponyms of the synset container in WN. For the PART–WHOLE relation we also experimented with binary features expressing whether the two entities in the relation have some type of meronym and holonym relation, but these features did not prove to be predictive. Cluster class of t1 and t2. A cluster class identifier for each entity in the relation. This information is drawn from automatically generated clusters of semantically similar nouns (Decadt and Daelemans, 2004) generated on the British National Corpus (Clear, 1993). The corpus was first preprocessed by a lemmatizer and the memory-based shallow parser, and the found verb–object relations were used to cluster nouns in groups. We used the top5000 lemmatized nouns, that are clustered into 250 groups. This is an example of two of these clusters: • {can pot basin tray glass container bottle tin pan mug cup jar bowl bucket plate jug vase kettle} • {booth restaurant bath kitchen hallway toilet bedroom hall suite bathroom interior lounge shower compartment oven lavatory room} The lexical features are the fol</context>
</contexts>
<marker>Decadt, Daelemans, 2004</marker>
<rawString>B. Decadt, W. Daelemans. 2004. Verb classification -machine learning experiments in classifying verbs into semantic classes. In Proceedings of the LREC 2004 Workshop Beyond Named Entity Recognition: Semantic Labeling for NLP Tasks, 25–30, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
<author>ed</author>
</authors>
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Fellbaum, ed, 1998</marker>
<rawString>C. Fellbaum, ed. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Iba</author>
<author>P Langley</author>
</authors>
<title>Induction of one-level decision trees.</title>
<date>1992</date>
<booktitle>Proceedings of the Ninth International Conference on Machine Learning,</booktitle>
<pages>233--240</pages>
<contexts>
<context position="7603" citStr="Iba and Langley, 1992" startWordPosition="1204" endWordPosition="1207"> features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highest accuracies for each relation were then used when applying the classifiers to the unseen test data. The classifiers of the cluster-based system (A) all use the two cluster class features. The other selected features and the chosen algorithms (CL) are displayed in Table 1. Knowledge o</context>
</contexts>
<marker>Iba, Langley, 1992</marker>
<rawString>W. Iba, P. Langley. 1992. Induction of one-level decision trees. Proceedings of the Ninth International Conference on Machine Learning, 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langley</author>
<author>W Iba</author>
<author>K Thompson</author>
</authors>
<title>An analysis of Bayesian classifiers.</title>
<date>1992</date>
<booktitle>In Proceedings of the Tenth Annual Conference on Artificial Intelligence,</booktitle>
<pages>223--228</pages>
<publisher>AAAI Press and MIT Press.</publisher>
<contexts>
<context position="7408" citStr="Langley et al., 1992" startWordPosition="1172" endWordPosition="1175"> all relations, the seven classifiers in the WNbased system all make use of the WN semantic class features; in the system that did not use WN, the seven classifiers make use of the cluster class features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highest accuracies for each relation were then used when applying the classifiers to the unseen </context>
</contexts>
<marker>Langley, Iba, Thompson, 1992</marker>
<rawString>P. Langley, W. Iba, K. Thompson. 1992. An analysis of Bayesian classifiers. In Proceedings of the Tenth Annual Conference on Artificial Intelligence, 223–228. AAAI Press and MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>S Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="3211" citStr="Marcus et al., 1993" startWordPosition="490" endWordPosition="493">ing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, Prague, June 2007. c�2007 Association for Computational Linguistics The features extracted are of three types: semantic, lexical, and morpho-syntactic. The features that apply to the entities in a relation (e1,e2) are extracted for term 1 (t1) and term 2 (t2) of the relation, where t1 is the first term in the relation name, and t2 is the second term.</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, S. Santorini, M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="7474" citStr="Quinlan, 1993" startWordPosition="1183" endWordPosition="1184">of the WN semantic class features; in the system that did not use WN, the seven classifiers make use of the cluster class features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highest accuracies for each relation were then used when applying the classifiers to the unseen test data. The classifiers of the cluster-based system (A) all use</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
<author>W Daelemans</author>
<author>A Weijters</author>
</authors>
<title>Morphological analysis as classification: an inductivelearning approach. In</title>
<date>1996</date>
<booktitle>Proceedings of the Second International Conference on New Methods in Natural Language Processing, NeMLaP-2,</booktitle>
<pages>79--89</pages>
<editor>K. Oflazer, H. Somers, eds.,</editor>
<location>Ankara, Turkey,</location>
<marker>Van den Bosch, Daelemans, Weijters, 1996</marker>
<rawString>A. Van den Bosch, W. Daelemans, A. Weijters. 1996. Morphological analysis as classification: an inductivelearning approach. In K. Oflazer, H. Somers, eds., Proceedings of the Second International Conference on New Methods in Natural Language Processing, NeMLaP-2, Ankara, Turkey, 79–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan</publisher>
<location>Kaufman, San Francisco, CA.</location>
<contexts>
<context position="7317" citStr="Witten and Frank, 1999" startWordPosition="1160" endWordPosition="1163">nts at abstract entities (e.g. statement). While the features were selected independently for all relations, the seven classifiers in the WNbased system all make use of the WN semantic class features; in the system that did not use WN, the seven classifiers make use of the cluster class features instead. 2.2 Classification We experimented with several machine learning frameworks and different feature (sub-)sets. For rapid testing of different learners and feature sets, and given the size of the training data (140 examples for each relation), we made use of the Weka machine learning software1 (Witten and Frank, 1999). We systematically tested the following algorithms: NaiveBayes (NB) (Langley et al., 1992), BayesNet (BN) (Cooper and Herskovits, 1992), J48 (Quinlan, 1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al., 1991), LWL (Atkeson et al., 1997), and DecisionStumps (DS) (Iba and Langley, 1992), all with default algorithm settings. The classifiers for all seven relations were optimized independently in a number of 10-fold crossvalidation (CV) experiments on the provided train1http://www.cs.waikato.ac.nz/ml/weka/ 188 ing sets. The feature sets and learning algorithms which were found to obtain the highe</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>I. H. Witten, E. Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufman, San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>