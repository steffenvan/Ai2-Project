<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000102">
<title confidence="0.995653">
FBK-HLT: An Effective System for Paraphrase Identification
and Semantic Similarity in Twitter
</title>
<author confidence="0.9906">
Ngoc Phuoc An Vo
</author>
<affiliation confidence="0.8266625">
University of Trento,
Fondazione Bruno Kessler
</affiliation>
<address confidence="0.734637">
Trento, Italy
</address>
<email confidence="0.997589">
ngoc@fbk.eu
</email>
<author confidence="0.997648">
Simone Magnolini
</author>
<affiliation confidence="0.8272035">
University of Brescia,
Fondazione Bruno Kessler
</affiliation>
<address confidence="0.735">
Trento, Italy
</address>
<email confidence="0.997506">
magnolini@fbk.eu
</email>
<author confidence="0.521785">
Octavian Popescu
</author>
<affiliation confidence="0.27573">
IBM Research, T.J. Watson
</affiliation>
<address confidence="0.408238">
Yorktown, US
</address>
<email confidence="0.990465">
o.popescu@us.ibm.com
</email>
<sectionHeader confidence="0.995555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999311538461538">
This paper reports the description and perfor-
mance of our system, FBK-HLT, participating
in the SemEval 2015, Task #1 &amp;quot;Paraphrase and
Semantic Similarity in Twitter&amp;quot;, for both sub-
tasks. We submitted two runs with different
classifiers in combining typical features (lexi-
cal similarity, string similarity, word n-grams,
etc) with machine translation metrics and edit
distance features. We outperform the baseline
system and achieve a very competitive result to
the best system on the first subtask. Eventually,
we are ranked 4th out of 18 teams participating
in subtask &amp;quot;Paraphrase Identification&amp;quot;.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999137785714286">
Paraphrase identification/recognition is an important
task that can be used as a feature to improve many
other NLP tasks as Information Retrieval, Machine
Translation Evaluation, Text Summarization, Ques-
tion and Answering, and others. Besides this, analyz-
ing social data like tweets of social network Twitter
is a field of growing interest for different purposes.
The interesting combination of these two tasks was
brought forward as Shared Task #1 in the SemEval
2015 campaign for &amp;quot;Paraphrase and Semantic Simi-
larity in Twitter&amp;quot; (Xu et al., 2015). In this task, given
a set of sentence pairs, which are not necessarily full
tweets, their topic and the same sentences with part-
of-speech and named entity tags; participating sys-
tem is required to predict for each pair of sentences
is a paraphrase (Subtask 1) and optionally compute
a graded score between 0 and 1 for their semantic
equivalence (Subtask 2). We participate in this shared
task with a system combining different features us-
ing a binary classifier. We are interested in finding
out whether semantic similarity, textual entailment
and machine translation evaluation techniques could
increase the accuracy of our system. This paper is
organized as follows: Section 2 presents the System
Description, Section 3 describes the Experiment Set-
tings, Section 4 reports the Evaluations, Section 5
shows the Error Analysis, and finally Section 6 is the
Conclusions and Future Work.
</bodyText>
<sectionHeader confidence="0.975003" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999990692307692">
In order to build our system, we extract and select sev-
eral different linguistic features ranging from simple
(word/string similarity, edit distance) to more com-
plex ones (machine translation evaluation metrics),
then we consolidate them by a binary classifier. More-
over, different features can be used independently or
together with others to measure the semantic similar-
ity and recognize the paraphrase of given sentence
pair as well as to evaluate the significance of each
feature to the accuracy of system’s predictions. On
top of this, the system is expandable and scalable for
adopting more useful features aiming for improving
the accuracy.
</bodyText>
<subsectionHeader confidence="0.992419">
2.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999951">
In order to optimizing the system performance, we
carefully analyze the given data and notice that
Tweets’ topic is a sentence part that is always present
in both sentences; this redundant similarity in the
pairs does not give any information about paraphrase
as two sentences can always have a same topic, yet
they are may be paraphrase or not. Hence, we remove
</bodyText>
<page confidence="0.9843">
29
</page>
<bodyText confidence="0.934845166666667">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 29–33,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
the topic from the sentences, and we did the same in
the pairs with Part-of-Speech (POS) and named entity
tags. We have not try our system with the topic inside
tweets. As being suggested by the guideline of the
task, we remove all the pairs with uncertain judgment,
such as &amp;quot;debatable&amp;quot; (2, 3). After this data process-
ing, we obtain two smaller datasets with very short
texts, sometime reduced to a single word and with
very poor syntactic structure. We split the original
dataset into two subsets, in which one is composed
by sentence pairs and the other one is composed by
pairs with POS and named entity tags. Because of the
simple structure of given datasets, after undergoing
the preprocessing, we decide to focus on exploiting
the lexical and string similarity information, rather
than syntactic information.
</bodyText>
<subsectionHeader confidence="0.99776">
2.2 Lexical and String Similarity
</subsectionHeader>
<bodyText confidence="0.999984545454546">
Firstly, for computing the lexical and string similarity
between two sentences, we take advantage from the
task baseline (Das and Smith, 2009) which is a sys-
tem using a logistic regression model with eighteen
features based on n-grams. This baseline system uses
precision, recall and F1-score of 1-gram, 2-grams
and 3-grams of tokens and stems from sentence pair
to build a binary classification model for identifying
paraphrase. We extract these eighteen features from
baseline system, without modifications, to use in our
classification model.
</bodyText>
<subsectionHeader confidence="0.994723">
2.3 Machine Translation Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999998375">
Other than similarity features, we also use evalua-
tion metrics for machine translation as suggested in
(Madnani et al., 2012) for paraphrase recognition
on Microsoft Research paraphrase corpus (MSRP)
(Dolan et al., 2004). In machine translation, the eval-
uation metric scores the hypotheses by aligning them
to one or more reference translations. We take into
consideration to use all the eight metrics proposed,
but we find that adding some of them without a care-
ful process of training on the dataset may decrease
the performance of the system. Thus, we use two met-
rics for word alignment in our system, the METEOR
and BLEU. We actually also take into consideration
the metric TERp (Snover et al., 2009), but it does
not make any improvement on system performance,
hence, we exclude it.
</bodyText>
<subsectionHeader confidence="0.554719">
2.3.1 METEOR (Metric for Evaluation of
</subsectionHeader>
<bodyText confidence="0.96240275">
Translation with Explicit ORdering)
We use the latest version of METEOR (Denkowski
and Lavie, 2014) that find alignments between sen-
tences based on exact, stem, synonym and paraphrase
matches between words and phrases. We used the
system as distributed on its website, using only the
&amp;quot;norm&amp;quot; option that tokenizes and normalizes punctu-
ation and lowercase as suggested by documentation.1
We compute the word alignment scores on sentences
and on sentences with part-of-speech and named en-
tity tags, as our idea is that if two sentences are simi-
lar, their tagged version also should be similar.
</bodyText>
<subsectionHeader confidence="0.605783">
2.3.2 BLEU (Bilingual Evaluation Understudy)
</subsectionHeader>
<bodyText confidence="0.9999592">
We use another metric for machine translation
BLEU (Papineni et al., 2002) that is one of the most
commonly used and because of that has an high re-
liability. It is computed as the amount of n-gram
overlap, for different values of n=1,2,3, and 4, be-
tween the system output and the reference translation,
in our case between sentence pairs. The score is tem-
pered by a penalty for translations that might be too
short. BLEU relies on exact matching and has no
concept of synonymy or paraphrasing.
</bodyText>
<subsectionHeader confidence="0.978135">
2.4 Edit Distance
</subsectionHeader>
<bodyText confidence="0.999980153846154">
We use the edit distance between sentences as a fea-
ture; for that we used the Excitement Open Platform
(EOP) (Magnini et al., 2014). To obtain the edit dis-
tance, we use EDITS Entailment Decision Algorithm
(EDITS EDA), this algorithm classifies the pairs on
the base of their edit distance, we take only this one
without considering the entailment or not entailment
decision. We configure the system to use lemmas and
synonyms as identical words to compute sentence
distance, the system normalizes the score on the num-
ber of token of the shortest sentence. We choose this
configuration because it returns the best performance
evaluated on training and development data.
</bodyText>
<subsectionHeader confidence="0.965776">
2.5 Classification Algorithms
</subsectionHeader>
<bodyText confidence="0.9997484">
We build two systems for the task with different clas-
sifiers, to optimize the Accuracy and F1-score. We
use WEKA (Hall et al., 2009) to obtain robust and
efficient implementation of the classifiers. We try
several classification algorithms in WEKA, among
</bodyText>
<footnote confidence="0.998353">
1http://www.cs.cmu.edu/ alavie/METEOR/index.html
</footnote>
<page confidence="0.959523">
30
</page>
<table confidence="0.999912625">
Classifier / Features Baseline Baseline Baseline Baseline Baseline
features +METEOR +METEOR +METEOR +METEOR
(n-grams) +TERp +BLEU +BLEU
+EditDistance
Baseline (Das and Smith, 2009) 72.4
EOP EditDistance 73.3
VotedPerceptron 73.7 75.6 75.5 75.8 76.2
MultiLayerPerceptron 73.9 75.6 75.3 75.4 76.1
</table>
<tableCaption confidence="0.999961">
Table 1: Accuracy obtained on development dataset using different classifiers with different features.
</tableCaption>
<bodyText confidence="0.999717">
others, we find that the VotedPerceptron (with expo-
nent 0.8) and MultilayerPerceptron (with learn rate
0.1; momentum 0.3 and N 10000) return the best
performance for the evaluation on training and devel-
opment data.
</bodyText>
<sectionHeader confidence="0.997917" genericHeader="method">
3 Experiment Settings
</sectionHeader>
<bodyText confidence="0.999382">
For Subtask 1, we train two models with different fea-
ture settings using the VotedPerceptron and Multilay-
erPerception classification algorithms on the training
dataset and we evaluate these models on the devel-
opment dataset. Finally, we use the same models for
the evaluation on the test dataset. In table 1, we re-
port the Accuracy results obtained by using different
classifiers with different features. Our chosen classi-
fication algorithms outperform the baseline and EOP
EditDistance (standalone setting). Table 2 shows
F1-score obtained with different classifiers on our
best set of features, and our classification algorithms
again perform much better the baseline and EOP Ed-
itDistance.
For Subtask 2, due to no training data is given
for computing the semantic similarity, a different ap-
proach is needed. We do not use a classifier, our
similarity score is simply the average between ME-
TEOR score and edit distance score.
</bodyText>
<table confidence="0.999147666666667">
Subtask1 Subtask2
Team Prec Rec F1 Pearson
Baseline(logistic reg) .679 .520 .589 .511
Baseline(WTMF) .450 .663 .536 .350
Baseline(random) .192 .434 .266 .017
ASOBEK(1st Subtask1) .680 .669 .674 .475
MITRE(1st Subtask2) .569 .806 .667 .619
FBK-HLT(voted) .685 .634 .659 .462
FBK-HLT(multilayer) .676 .549 .606 .480
</table>
<tableCaption confidence="0.999671">
Table 3: Paraphrase and Semantic Similarity Results.
</tableCaption>
<sectionHeader confidence="0.996298" genericHeader="method">
4 Evaluations
</sectionHeader>
<bodyText confidence="0.997516166666667">
We submit two runs using two models described in
the Section 3 for both subtasks. In the Table 3, we re-
port the performance of our two runs against the base-
lines and best systems in each subtask. In Subtask 1,
our runs outperform all three baselines and achieve
very competitive results to the best system ASOBEK.
In the run FBK-HLT(voted), we even achieve a better
Precision than the best system. In Subtask 2, though
we apply a simple computation method for semantic
similarity by averaging the word alignment score and
EditDistance, we still have better results than two our
of three baselines.
</bodyText>
<table confidence="0.9962786">
Classifier F1
Baseline (Das and Smith, 2009) .502
EOP EditDistance .609
VotedPerceptron .746
MultiLayerPerceptron .741
</table>
<tableCaption confidence="0.944935666666667">
Table 2: F1-score obtained using different classifiers on
the best set of features (baseline + METEOR + BLEU +
EditDistance).
</tableCaption>
<sectionHeader confidence="0.998613" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.99995925">
In this section, we conduct an analysis of the mis-
classifications that our best system, FBK-HLT(voted),
makes on test dataset. We extract and show some
randomly selected examples in which our system
classifies incorrectly, both false positive or false neg-
ative; and then we analyze the possible causes for
the misclassification. This inspection yields not only
the top sources of error for our approach but also
</bodyText>
<page confidence="0.999872">
31
</page>
<tableCaption confidence="0.740921">
uncovers sources of unclear annotations in dataset.
</tableCaption>
<table confidence="0.996791333333333">
True True False False
Positive Negative Positive Negative
111 612 51 64
</table>
<tableCaption confidence="0.999301">
Table 4: Error Analysis.
</tableCaption>
<subsectionHeader confidence="0.99846">
5.1 False positive
</subsectionHeader>
<bodyText confidence="0.944815217391304">
[1357] omg Family Guy is killing me right now -
OMG we were quoting family guy
[1357] family guy is trending in the US - Family guy
is so racist or maybe they just point out the racism in
America
[4135] hahaha that sounds like me - That sounds
totally reasonable to me
[5211] The world ofjenks is such a real show - Jenks
from the World of Jenks is such a good person
[128] Anyone trying to see After Earth sometime
soon - Me and my son went to see After Earth last
night
Though all these sentence pairs share many word
similarity/matching and alignments, they are anno-
tated as non-paraphrase. For example, the sentence
pair [4135] has very high word matching and align-
ment after removing the common topic &amp;quot;sounds&amp;quot;, but
the important words &amp;quot;like&amp;quot; and &amp;quot;reasonable&amp;quot; which
differ the meaning between two sentences, are not
really semantically captured and distinguished by our
system. As our system does not use any semantic
feature, this kind of semantic difference is difficult to
distinguish,leading to false positive case.
</bodyText>
<subsectionHeader confidence="0.999141">
5.2 False negative
</subsectionHeader>
<bodyText confidence="0.765792">
[4220] Hell yeah Star Wars is on - Star Wars and
lord of the rings on tv
</bodyText>
<figure confidence="0.332600666666667">
[785] Chris Davis is putting the team on his back -
Chris Davis doing what he does
[400] Rafa Benitez deserves a hell of a thank you -
</figure>
<construct confidence="0.444218">
Any praise for Benitez from my Chelsea followers
[2832] Classy gesture by the Mets for Mariano - real
class shown by The Mets Mo Rivera is a legend
[4062] Shonda is a freaking genius - THAT LADY IS
AMAZING I LOVE SHONDA
</construct>
<bodyText confidence="0.9991277">
This case is opposite to the previous case, even
though these sentence pairs do not share many word
similarity and alignment, they are annotated as para-
phrase. We can possibly propose some hypothesis as
follows:
Extra information Though the pairs [4220] and
[400] may not be paraphrase according to the para-
phrase definition in the literature (Bhagat and Hovy,
2013), they are annotated as paraphrase in the gold-
standard labels. We notice that as one sentence con-
tains more extra information than the other one, it
leads to low word similarity and alignment, which
makes our system make wrong classification.
Specific knowledge-base In this case, the pairs
[785] and [2832] require a specific knowledge-base,
which is about baseball, to recognize the paraphrase;
hence, even for human without any related knowl-
edge, it might be difficult detect the paraphrase.
Common sense Though both sentences of the pair
[4062] do not share any word similarity/alignment,
they have a positive polarity that may allow iden-
tifying the paraphrase. This case may be easy for
human to identify the paraphrase, yet it is difficult
for machine to capture the same perception.
Table 4 shows that we can improve our system
performance by reducing the false positive and false
negative. In other word, we need to exploit more se-
mantic features to make correct classification. How-
ever, according to our analysis for the false negative,
it is difficult to cover these cases.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999001875">
In this paper, we describe a system participating in
the SemEval 2015, Task #1 &amp;quot;Paraphrase and Seman-
tic Similarity in Twitter&amp;quot;, for both subtasks. We
present a supervised system which considers mul-
tiple features at low level, such as lexical, string
similarities, word alignment and edit distance. The
performance of our runs is much better than the base-
lines and very competitive to the best system; we are
ranked 4th of total 18 teams in Subtask 1.
A lower result was obtained in Subtask 2, as the cho-
sen features have not really acquired the semantic
similarity judgment. Hence, we expect to study more
useful features (e.g the POS information, semantic
word similarity) to improve our system performance
on both identifying paraphrase and computing seman-
tic similarity scores.
</bodyText>
<page confidence="0.998628">
32
</page>
<sectionHeader confidence="0.990293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999279723404255">
Rahul Bhagat and Eduard Hovy. 2013. What is a para-
phrase? Computational Linguistics, 39(3):463–472.
Dipanjan Das and Noah A Smith. 2009. Paraphrase iden-
tification as probabilistic quasi-synchronous recogni-
tion. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 1-Volume 1, pages 468–476.
Michael Denkowski and Alon Lavie. 2014. Meteor uni-
versal: Language specific translation evaluation for any
target language. In Proceedings of the EACL 2014
Workshop on Statistical Machine Translation.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, page 350.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten. 2009.
The WEKA data mining software: an update. ACM
SIGKDD explorations newsletter, 11(1):10–18.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 182–190.
Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin
Eichler, Günter Neumann, Tae-Gil Noh, Sebastian
Pado, Asher Stern, and Omer Levy. 2014. The ex-
citement open platform for textual inferences. In Pro-
ceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL 2014).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting on association for computational
linguistics, pages 311–318.
Matthew G Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to translation edit
rate. Machine Translation, 23(2-3):117–127.
Wei Xu, Chris Callison-Burch, and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and seman-
tic similarity in Twitter (PIT). In Proceedings of the
9th International Workshop on Semantic Evaluation
(SemEval).
</reference>
<page confidence="0.999334">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.076816">
<title confidence="0.9734415">FBK-HLT: An Effective System for Paraphrase and Semantic Similarity in Twitter</title>
<author confidence="0.895854">Ngoc Phuoc An</author>
<affiliation confidence="0.87279">University of Fondazione Bruno</affiliation>
<address confidence="0.563032">Trento,</address>
<email confidence="0.995669">ngoc@fbk.eu</email>
<author confidence="0.975497">Simone</author>
<affiliation confidence="0.837339666666667">University of Fondazione Bruno Trento,</affiliation>
<email confidence="0.993749">magnolini@fbk.eu</email>
<author confidence="0.713013">Octavian</author>
<affiliation confidence="0.77952">IBM Research, T.J. Yorktown,</affiliation>
<email confidence="0.999864">o.popescu@us.ibm.com</email>
<abstract confidence="0.992326357142857">This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #1 &amp;quot;Paraphrase and Semantic Similarity in Twitter&amp;quot;, for both subtasks. We submitted two runs with different classifiers in combining typical features (lexical similarity, string similarity, word n-grams, etc) with machine translation metrics and edit distance features. We outperform the baseline system and achieve a very competitive result to the best system on the first subtask. Eventually, are ranked out of 18 teams participating in subtask &amp;quot;Paraphrase Identification&amp;quot;.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Eduard Hovy</author>
</authors>
<title>What is a paraphrase?</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="13296" citStr="Bhagat and Hovy, 2013" startWordPosition="2119" endWordPosition="2122">ez deserves a hell of a thank you - Any praise for Benitez from my Chelsea followers [2832] Classy gesture by the Mets for Mariano - real class shown by The Mets Mo Rivera is a legend [4062] Shonda is a freaking genius - THAT LADY IS AMAZING I LOVE SHONDA This case is opposite to the previous case, even though these sentence pairs do not share many word similarity and alignment, they are annotated as paraphrase. We can possibly propose some hypothesis as follows: Extra information Though the pairs [4220] and [400] may not be paraphrase according to the paraphrase definition in the literature (Bhagat and Hovy, 2013), they are annotated as paraphrase in the goldstandard labels. We notice that as one sentence contains more extra information than the other one, it leads to low word similarity and alignment, which makes our system make wrong classification. Specific knowledge-base In this case, the pairs [785] and [2832] require a specific knowledge-base, which is about baseball, to recognize the paraphrase; hence, even for human without any related knowledge, it might be difficult detect the paraphrase. Common sense Though both sentences of the pair [4062] do not share any word similarity/alignment, they ha</context>
</contexts>
<marker>Bhagat, Hovy, 2013</marker>
<rawString>Rahul Bhagat and Eduard Hovy. 2013. What is a paraphrase? Computational Linguistics, 39(3):463–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>468--476</pages>
<contexts>
<context position="4644" citStr="Das and Smith, 2009" startWordPosition="716" endWordPosition="719">exts, sometime reduced to a single word and with very poor syntactic structure. We split the original dataset into two subsets, in which one is composed by sentence pairs and the other one is composed by pairs with POS and named entity tags. Because of the simple structure of given datasets, after undergoing the preprocessing, we decide to focus on exploiting the lexical and string similarity information, rather than syntactic information. 2.2 Lexical and String Similarity Firstly, for computing the lexical and string similarity between two sentences, we take advantage from the task baseline (Das and Smith, 2009) which is a system using a logistic regression model with eighteen features based on n-grams. This baseline system uses precision, recall and F1-score of 1-gram, 2-grams and 3-grams of tokens and stems from sentence pair to build a binary classification model for identifying paraphrase. We extract these eighteen features from baseline system, without modifications, to use in our classification model. 2.3 Machine Translation Evaluation Metrics Other than similarity features, we also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition </context>
<context position="8252" citStr="Das and Smith, 2009" startWordPosition="1290" endWordPosition="1293">this configuration because it returns the best performance evaluated on training and development data. 2.5 Classification Algorithms We build two systems for the task with different classifiers, to optimize the Accuracy and F1-score. We use WEKA (Hall et al., 2009) to obtain robust and efficient implementation of the classifiers. We try several classification algorithms in WEKA, among 1http://www.cs.cmu.edu/ alavie/METEOR/index.html 30 Classifier / Features Baseline Baseline Baseline Baseline Baseline features +METEOR +METEOR +METEOR +METEOR (n-grams) +TERp +BLEU +BLEU +EditDistance Baseline (Das and Smith, 2009) 72.4 EOP EditDistance 73.3 VotedPerceptron 73.7 75.6 75.5 75.8 76.2 MultiLayerPerceptron 73.9 75.6 75.3 75.4 76.1 Table 1: Accuracy obtained on development dataset using different classifiers with different features. others, we find that the VotedPerceptron (with exponent 0.8) and MultilayerPerceptron (with learn rate 0.1; momentum 0.3 and N 10000) return the best performance for the evaluation on training and development data. 3 Experiment Settings For Subtask 1, we train two models with different feature settings using the VotedPerceptron and MultilayerPerception classification algorithms o</context>
<context position="10658" citStr="Das and Smith, 2009" startWordPosition="1667" endWordPosition="1670">ng two models described in the Section 3 for both subtasks. In the Table 3, we report the performance of our two runs against the baselines and best systems in each subtask. In Subtask 1, our runs outperform all three baselines and achieve very competitive results to the best system ASOBEK. In the run FBK-HLT(voted), we even achieve a better Precision than the best system. In Subtask 2, though we apply a simple computation method for semantic similarity by averaging the word alignment score and EditDistance, we still have better results than two our of three baselines. Classifier F1 Baseline (Das and Smith, 2009) .502 EOP EditDistance .609 VotedPerceptron .746 MultiLayerPerceptron .741 Table 2: F1-score obtained using different classifiers on the best set of features (baseline + METEOR + BLEU + EditDistance). 5 Error Analysis In this section, we conduct an analysis of the misclassifications that our best system, FBK-HLT(voted), makes on test dataset. We extract and show some randomly selected examples in which our system classifies incorrectly, both false positive or false negative; and then we analyze the possible causes for the misclassification. This inspection yields not only the top sources of er</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor universal: Language specific translation evaluation for any target language.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="6017" citStr="Denkowski and Lavie, 2014" startWordPosition="934" endWordPosition="937">g them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we use two metrics for word alignment in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not make any improvement on system performance, hence, we exclude it. 2.3.1 METEOR (Metric for Evaluation of Translation with Explicit ORdering) We use the latest version of METEOR (Denkowski and Lavie, 2014) that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the &amp;quot;norm&amp;quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.1 We compute the word alignment scores on sentences and on sentences with part-of-speech and named entity tags, as our idea is that if two sentences are similar, their tagged version also should be similar. 2.3.2 BLEU (Bilingual Evaluation Understudy) We use another metric for machine translation BLEU (Papineni et</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<contexts>
<context position="5311" citStr="Dolan et al., 2004" startWordPosition="815" endWordPosition="818">odel with eighteen features based on n-grams. This baseline system uses precision, recall and F1-score of 1-gram, 2-grams and 3-grams of tokens and stems from sentence pair to build a binary classification model for identifying paraphrase. We extract these eighteen features from baseline system, without modifications, to use in our classification model. 2.3 Machine Translation Evaluation Metrics Other than similarity features, we also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we use two metrics for word alignment in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not make any improvement on system performance, hence, we exclude it. 2.3.1 METEOR (Metric for Evaluat</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="7897" citStr="Hall et al., 2009" startWordPosition="1247" endWordPosition="1250">DA), this algorithm classifies the pairs on the base of their edit distance, we take only this one without considering the entailment or not entailment decision. We configure the system to use lemmas and synonyms as identical words to compute sentence distance, the system normalizes the score on the number of token of the shortest sentence. We choose this configuration because it returns the best performance evaluated on training and development data. 2.5 Classification Algorithms We build two systems for the task with different classifiers, to optimize the Accuracy and F1-score. We use WEKA (Hall et al., 2009) to obtain robust and efficient implementation of the classifiers. We try several classification algorithms in WEKA, among 1http://www.cs.cmu.edu/ alavie/METEOR/index.html 30 Classifier / Features Baseline Baseline Baseline Baseline Baseline features +METEOR +METEOR +METEOR +METEOR (n-grams) +TERp +BLEU +BLEU +EditDistance Baseline (Das and Smith, 2009) 72.4 EOP EditDistance 73.3 VotedPerceptron 73.7 75.6 75.5 75.8 76.2 MultiLayerPerceptron 73.9 75.6 75.3 75.4 76.1 Table 1: Accuracy obtained on development dataset using different classifiers with different features. others, we find that the Vo</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="5216" citStr="Madnani et al., 2012" startWordPosition="802" endWordPosition="805">tage from the task baseline (Das and Smith, 2009) which is a system using a logistic regression model with eighteen features based on n-grams. This baseline system uses precision, recall and F1-score of 1-gram, 2-grams and 3-grams of tokens and stems from sentence pair to build a binary classification model for identifying paraphrase. We extract these eighteen features from baseline system, without modifications, to use in our classification model. 2.3 Machine Translation Evaluation Metrics Other than similarity features, we also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we use two metrics for word alignment in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not mak</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Roberto Zanoli</author>
<author>Ido Dagan</author>
<author>Kathrin Eichler</author>
<author>Günter Neumann</author>
<author>Tae-Gil Noh</author>
<author>Sebastian Pado</author>
<author>Asher Stern</author>
<author>Omer Levy</author>
</authors>
<title>The excitement open platform for textual inferences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="7197" citStr="Magnini et al., 2014" startWordPosition="1134" endWordPosition="1137">for machine translation BLEU (Papineni et al., 2002) that is one of the most commonly used and because of that has an high reliability. It is computed as the amount of n-gram overlap, for different values of n=1,2,3, and 4, between the system output and the reference translation, in our case between sentence pairs. The score is tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. 2.4 Edit Distance We use the edit distance between sentences as a feature; for that we used the Excitement Open Platform (EOP) (Magnini et al., 2014). To obtain the edit distance, we use EDITS Entailment Decision Algorithm (EDITS EDA), this algorithm classifies the pairs on the base of their edit distance, we take only this one without considering the entailment or not entailment decision. We configure the system to use lemmas and synonyms as identical words to compute sentence distance, the system normalizes the score on the number of token of the shortest sentence. We choose this configuration because it returns the best performance evaluated on training and development data. 2.5 Classification Algorithms We build two systems for the tas</context>
</contexts>
<marker>Magnini, Zanoli, Dagan, Eichler, Neumann, Noh, Pado, Stern, Levy, 2014</marker>
<rawString>Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin Eichler, Günter Neumann, Tae-Gil Noh, Sebastian Pado, Asher Stern, and Omer Levy. 2014. The excitement open platform for textual inferences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6628" citStr="Papineni et al., 2002" startWordPosition="1031" endWordPosition="1034">avie, 2014) that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the &amp;quot;norm&amp;quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.1 We compute the word alignment scores on sentences and on sentences with part-of-speech and named entity tags, as our idea is that if two sentences are similar, their tagged version also should be similar. 2.3.2 BLEU (Bilingual Evaluation Understudy) We use another metric for machine translation BLEU (Papineni et al., 2002) that is one of the most commonly used and because of that has an high reliability. It is computed as the amount of n-gram overlap, for different values of n=1,2,3, and 4, between the system output and the reference translation, in our case between sentence pairs. The score is tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. 2.4 Edit Distance We use the edit distance between sentences as a feature; for that we used the Excitement Open Platform (EOP) (Magnini et al., 2014). To obtain the edit distance, </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TER-Plus: paraphrase, semantic, and alignment enhancements to translation edit rate.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="5795" citStr="Snover et al., 2009" startWordPosition="899" endWordPosition="902">on as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we use two metrics for word alignment in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not make any improvement on system performance, hence, we exclude it. 2.3.1 METEOR (Metric for Evaluation of Translation with Explicit ORdering) We use the latest version of METEOR (Denkowski and Lavie, 2014) that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the &amp;quot;norm&amp;quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.1 We compute the word alignment scores on sentences and on sentences with part-of-speech and </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew G Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. TER-Plus: paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation, 23(2-3):117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<title>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT).</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="1526" citStr="Xu et al., 2015" startWordPosition="218" endWordPosition="221">participating in subtask &amp;quot;Paraphrase Identification&amp;quot;. 1 Introduction Paraphrase identification/recognition is an important task that can be used as a feature to improve many other NLP tasks as Information Retrieval, Machine Translation Evaluation, Text Summarization, Question and Answering, and others. Besides this, analyzing social data like tweets of social network Twitter is a field of growing interest for different purposes. The interesting combination of these two tasks was brought forward as Shared Task #1 in the SemEval 2015 campaign for &amp;quot;Paraphrase and Semantic Similarity in Twitter&amp;quot; (Xu et al., 2015). In this task, given a set of sentence pairs, which are not necessarily full tweets, their topic and the same sentences with partof-speech and named entity tags; participating system is required to predict for each pair of sentences is a paraphrase (Subtask 1) and optionally compute a graded score between 0 and 1 for their semantic equivalence (Subtask 2). We participate in this shared task with a system combining different features using a binary classifier. We are interested in finding out whether semantic similarity, textual entailment and machine translation evaluation techniques could in</context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>