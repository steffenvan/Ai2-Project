<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000460">
<title confidence="0.99201">
Online Multitask Learning for Machine Translation Quality Estimation
</title>
<author confidence="0.45671">
Jos´e G. C. de Souza(1,2), Matteo Negri(1), Elisa Ricci(1), Marco Turchi(1)
</author>
<affiliation confidence="0.5399185">
(1) FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2) University of Trento, Italy
</affiliation>
<email confidence="0.979738">
{desouza,negri,eliricci,turchi}@fbk.eu
</email>
<sectionHeader confidence="0.993296" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999275">
We present a method for predicting ma-
chine translation output quality geared to
the needs of computer-assisted translation.
These include the capability to: i) con-
tinuously learn and self-adapt to a stream
of data coming from multiple translation
jobs, ii) react to data diversity by ex-
ploiting human feedback, and iii) leverage
data similarity by learning and transferring
knowledge across domains. To achieve
these goals, we combine two supervised
machine learning paradigms, online and
multitask learning, adapting and unifying
them in a single framework. We show
the effectiveness of our approach in a re-
gression task (HTER prediction), in which
online multitask learning outperforms the
competitive online single-task and pooling
methods used for comparison. This in-
dicates the feasibility of integrating in a
CAT tool a single QE component capa-
ble to simultaneously serve (and continu-
ously learn from) multiple translation jobs
involving different domains and users.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910333333333">
Even if not perfect, machine translation (MT) is
now getting reliable enough to support and speed-
up human translation. Thanks to this progress,
the work of professional translators is gradually
shifting from full translation from scratch to MT
post-editing. Advanced computer-assisted trans-
lation (CAT) tools1 provide a natural framework
for this activity by proposing, for each segment in
a source document, one or more suggestions ob-
tained either from a translation memory (TM) or
from an MT engine. In both cases, accurate mech-
anisms to indicate the reliability of a suggestion
</bodyText>
<footnote confidence="0.686226">
1See for instance the open source MateCat tool (Federico
et al., 2014).
</footnote>
<bodyText confidence="0.998503947368421">
are extremely useful to let the user decide whether
to post-edit a given suggestion or ignore it and
translate the source segment from scratch. How-
ever, while scoring TM matches relies on standard
methods based on fuzzy matching, predicting the
quality of MT suggestions at run-time and without
references is still an open issue.
This is the goal of MT quality estimation (QE),
which aims to predict the quality of an automatic
translation as a function of the estimated number
of editing operations or the time required for man-
ual correction (Specia et al., 2009; Soricut and
Echihabi, 2010; Bach et al., 2011; Mehdad et al.,
2012). So far, QE has been mainly approached
in controlled settings where homogeneous train-
ing and test data is used to learn and evaluate static
predictors. Cast in this way, however, it does not
fully reflect (nor exploit) the working conditions
posed by the CAT framework, in which:
</bodyText>
<listItem confidence="0.832266555555556">
1. The QE module is exposed to a continuous
stream of data. The amount of such data and
the tight schedule of multiple, simultaneous
translation jobs prevents from (theoretically
feasible but impractical) complete re-training
procedures in a batch fashion and advocate
for continuous learning methods.
2. The input data can be diverse in nature. Con-
tinuous learning should be sensitive to such
differences, in a way that each translation job
and user is supported by a reactive model that
is robust to variable working conditions.
3. The input data can show similarities with
previous observations. Continuous learning
should leverage such similarities, so that QE
can capitalize from all the previously pro-
cessed segments even if they come from dif-
ferent domains, genres or users.
</listItem>
<bodyText confidence="0.880678">
While previous QE research disregarded these
challenges or addressed them in isolation, our
</bodyText>
<page confidence="0.986821">
219
</page>
<note confidence="0.978268">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 219–228,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999784555555556">
work tackles them in a single unifying framework
based on the combination of two paradigms: on-
line and multitask learning. The former provides
continuous learning capabilities that allow the QE
model to be robust and self-adapt to a stream of
potentially diverse data. The latter provides the
model with the capability to exploit the similari-
ties between data coming from different sources.
Along this direction our contributions are:
</bodyText>
<listItem confidence="0.6589105">
• The first application of online multitask
learning to QE, geared to the challenges
posed by CAT technology. In this framework,
our models are trained to predict MT quality
in terms of HTER (Snover et al., 2006).2
• The extension of current online multitask
learning methods to regression. Prior works
in the machine learning field applied this
paradigm to classification problems, but its
use for HTER estimation requires real-valued
predictions. To this aim, we propose a new
regression algorithm that, at the same time,
handles positive and negative transfer and
performs online weight updates.
• A comparison between online multitask and
alternative, state-of-the-art online learning
strategies. Our experiments, carried out in a
realistic scenario involving a stream of data
from four domains, lead to consistent results
that prove the effectiveness of our approach.
</listItem>
<sectionHeader confidence="0.999417" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999914909090909">
In recent years, sentence-level QE has been
mainly investigated in controlled evaluation sce-
narios such as those proposed by the shared tasks
organized within the WMT workshop on SMT
(Callison-Burch et al., 2012; Bojar et al., 2013;
Bojar et al., 2014). In this framework, systems
trained from a collection of (source, target, label)
instances are evaluated based on their capability
to predict the correct label3 for new, unseen test
items. Compared to our application scenario, the
shared tasks setting differs in two main aspects.
</bodyText>
<footnote confidence="0.985786777777778">
2The HTER is the minimum edit distance between a trans-
lation suggestion and its manually post-edited version in the
[0,1] interval. Edit distance is calculated as the number of
edits (word insertions, deletions, substitutions, and shifts) di-
vided by the number of words in the reference.
3Possible label types include post-editing effort scores
(e.g. 1-5 Likert scores indicating the estimated percentage
of MT output that has to be corrected), HTER values, and
post-editing time (e.g. seconds per word).
</footnote>
<bodyText confidence="0.999562509803922">
First, the data used are substantially homogeneous
(usually they come from the same domain, and tar-
get translations are produced by the same MT sys-
tem). Second, training and test are carried out as
distinct, sequential phases. Instead, in the CAT en-
vironment, a QE component should ideally serve,
adapt to and continuously learn from simultaneous
translation jobs involving different MT engines,
domains, genres and users (Turchi et al., 2013).
These challenges have been separately ad-
dressed from different perspectives in few recent
works. Huang et al. (2014) proposed a method
to adaptively train a QE model for document-
specific MT post-editing. Adaptability, however,
is achieved in a batch fashion, by re-training an ad
hoc QE component for each document to be trans-
lated. The adaptive approach proposed by Turchi
et al. (2014) overcomes the limitations of batch
methods by applying an online learning protocol
to continuously learn from a stream of (potentially
heterogeneous) data. Experimental results suggest
the effectiveness of online learning as a way to ex-
ploit user feedback to tailor QE predictions to their
quality standards and to cope with the heterogene-
ity of data coming from different domains. How-
ever, though robust to user and domain changes,
the method is solely driven by the distance com-
puted between predicted and true labels, and it
does not exploit any notion of similarity between
tasks (e.g. domains, users, MT engines).
On the other way round, task relatedness is suc-
cessfully exploited by Cohn and Specia (2013),
who apply multitask learning to jointly learn from
data obtained from several annotators with differ-
ent levels of expertise and reliability. A similar ap-
proach is adopted by de Souza et al. (2014a), who
apply multitask learning to cope with situations in
which a QE model has to be trained with scarce
data from multiple domains/genres, different from
the actual test domain. The two methods signifi-
cantly outperform both individual single-task (in-
domain) models and single pooled models. How-
ever, operating in batch learning mode, none of
them provides the continuous learning capabilities
desirable in the CAT framework.
The idea that online and multitask learning can
complement each other if combined is suggested
by (de Souza et al., 2014b), who compared the two
learning paradigms in the same experimental set-
ting. So far, however, empirical evidence of this
complementarity is still lacking.
</bodyText>
<page confidence="0.996876">
220
</page>
<sectionHeader confidence="0.995951" genericHeader="method">
3 Online Multitask Learning for QE
</sectionHeader>
<bodyText confidence="0.999851096385543">
Online learning takes place in a stepwise fash-
ion. At each step, the learner processes an instance
(in our case a feature vector extracted from source
and target sentences) and predicts a label for it (in
our case an HTER value). After the prediction, the
learner receives the “true” label (in our case the ac-
tual HTER computed from a human post-edition)
and computes a loss that indicates the distance be-
tween the predicted and the true label. Before go-
ing to the next step, the weights are updated ac-
cording to the suffered loss.
Multitask learning (MTL) aims to simultane-
ously learn models for a set of possibly related
tasks by exploiting their relationships. By do-
ing this, improved generalization capabilities are
obtained over models trained on the different
tasks in isolation (single-task learning – STL).
The relationships among tasks are provided by a
shared structure, which can encode three types
of relationships based on their correlation (Zhang
and Yeung, 2010). Positive correlation indicates
that the tasks are related and knowledge transfer
should lead to similar model parameters. Negative
correlation indicates that the tasks are likely to be
unrelated and knowledge transfer should force an
increase in the distance between model parame-
ters. No correlation indicates that the tasks are in-
dependent and no knowledge transfer should take
place. In our case, a task is a set of (instance, la-
bel) pairs obtained from source sentences coming
from different translation jobs, together with their
translations produced by several MT systems and
the relative post-editions from various translators.
In this paper the terms task and domain are used
interchangeably.
Early MTL methods model only positive cor-
relation (Caruana, 1997; Argyriou et al., 2008),
which results in a positive knowledge transfer be-
tween all the tasks, with the risk of impairing each
other’s performance when they are unrelated or
negatively correlated. Other methods (Jacob et
al., 2009; Zhong and Kwok, 2012; Yan et al.,
2014) cluster tasks into different groups and share
knowledge only among those in the same cluster,
thus implicitly identifying outlier tasks. A third
class of algorithms considers all the three types of
relationships by learning task interaction via the
covariance of task-specific weights (Bonilla et al.,
2008; Zhang and Yeung, 2010). All these meth-
ods, however, learn the task relationships in batch
mode. To overcome this limitation, recent works
propose the “lifelong learning” paradigm (Eaton
and Ruvolo, 2013; Ruvolo and Eaton, 2014), in
which all the instances of a task are given to
the learner sequentially and the previously learned
tasks are leveraged to improve generalization for
future tasks. This approach, however, is not ap-
plicable to our scenario as it assumes that all the
instances of each task are processed as separate
blocks.
In this paper we propose a novel MTL algorithm
for QE that learns the structure shared by differ-
ent tasks in an online fashion and from an input
stream of instances from all the tasks. To this aim,
we extend the online passive aggressive (PA) al-
gorithm (Crammer et al., 2006) to the multitask
scenario, learning a set of task-specific regression
models. The multitask component of our method
is given by an “interaction matrix” that defines to
which extent each encoded task can “borrow” and
“lend” knowledge from and to the other tasks. Op-
posite to previous methods (Cavallanti et al., 2010)
that assume fixed dependencies among tasks, we
propose to learn the interaction matrix instance-
by-instance from the data. To this aim we follow
the recent work of Saha et al. (2011), extending it
to a regression setting. The choice of PA is mo-
tivated by practical reasons. Indeed, by provid-
ing the best trade-off between accuracy and com-
putational time (He and Wang, 2012) compared
to other algorithms such as OnlineSVR (Parrella,
2007), it represents a good solution to meet the de-
mand of efficiency posed by the CAT framework.
</bodyText>
<subsectionHeader confidence="0.999535">
3.1 Passive Aggressive Algorithm
</subsectionHeader>
<bodyText confidence="0.9999818">
PA follows the typical online learning proto-
col. At each round t the learner receives an in-
stance, xt E Rd (d is the number of features),
and predicts the label ˆyt according to a function
parametrized by a set weights wt E Rd. Next,
the learner receives the true label yt, computes the
E-insensitive loss, `E, measuring the deviation be-
tween the prediction ˆyt and the true label yt and
updates the weights. The weights are updated by
solving the optimization problem:
</bodyText>
<equation confidence="0.9793385">
CPA(w) + Cξ (1)
s.t. f,(w, (xt, yt)) ≤ ξ and ξ ≥ 0
</equation>
<bodyText confidence="0.9995735">
where CPA(w) = 12||w − wt−1||2 and `E is the
E-insensitive hinge loss defined as:
</bodyText>
<equation confidence="0.9518874">
wt = arg min
W
221
`E(w, (x, y)) = { |y − w · x |I e, otherwise
(2)
</equation>
<bodyText confidence="0.999164375">
The loss is zero when the absolute difference be-
tween the prediction and the true label is smaller
or equal to c, and grows linearly with this differ-
ence otherwise. The c parameter is given as input
and regulates the sensitivity to mistakes. The slack
variable ξ acts as an upper-bound to the loss, while
the C parameter is introduced to control the ag-
gressiveness of the weights update. High C values
lead to more aggressive weight updates. However,
when the labels present some degree of noise (a
common situation in MT QE), they might cause
the learner to drastically change the weight vector
in a wrong direction. In these situations, setting C
to small values is desirable. As shown in (Cram-
mer et al., 2006), a closed form solution for the
weights update in Eq.1 can be derived as:
</bodyText>
<equation confidence="0.909212666666667">
wt = wt−1 + sgn(yt − ˆyt)τtxt (3)
with τt = min(C,  |X
It ||2) and `t = `,(w, (xt, yt)).
</equation>
<subsectionHeader confidence="0.998235">
3.2 Passive Aggressive MTL Algorithm
</subsectionHeader>
<bodyText confidence="0.986980192307692">
Our Passive Aggressive Multitask Learning
(PAMTL) algorithm extends the traditional PA for
regression to multitask learning. Our approach is
inspired by the Online Task Relationship Learning
algorithm proposed by Saha et al. (2011) which,
however, is only defined for classification.
The learning process considers one instance at
each round t. The random sequence of instances
belongs to a fixed set of K tasks and the goal of the
algorithm is to learn K linear models, one for each
task, parametrized by weight vectors Wt,k, k E
11, ... , K}. Moreover, the algorithm also learns
a positive semidefinite matrix n E RK×K, mod-
eling the relationship among tasks. Algorithm 1
summarizes our approach. At each round t, the
learner receives a pair (xt, it) where xt E Rd is an
instance and it E 11, ... , K} is the task identifier.
Each incoming instance is transformed to a com-
pound vector φt = [0,... , 0, xt, 0, ... , 0] E RKd.
Then, the algorithm predicts the HTER score cor-
responding to the label yˆ by using the weight vec-
tor Wt. The weight vector is a compound vector
Rd , k E 11, ... , K}. Next, the learner receives
Wt = [Wt,1, ... , Wt,K] E RKd, where Wt,k E
the true HTER label y and computes the loss `,
(Eq. 2) for round t.
</bodyText>
<figure confidence="0.86141328">
Algorithm 1 PA Multitask Learning (PAMTL)
Input: instances from K tasks, number of rounds R &gt; 0,
e&gt;0,C&gt;0
Output: w and Ω, learned after T rounds
Initialization: Ω = 1 K × Ik, w = 0
fort= 1 to T do
receive instance (xt, it)
compute Ot from xt
predict HTER ˆyt = (�wTt · Ot)
receive true HTER label yt
compute `t (Eq. 2)
compute τt = min(C, let
||φt||2 )
/* update weights */
1
wt = wt−1 + sgn(yt − ˆyt)τt(Ωt−1 ⊗ Id) Ot
/* update task matrix */
if t &gt; R then
update Ωt with Eq. 6 or Eq. 7
end if
end for
We propose to update the weights by solving:
�wt, Ωt = argmin CMTL(w, Ω) + Cξ + D(Ω, Ωt−1)
w,Ωr0
s.t. `E(w, (xt, yt)) ≤ ξ, ξ ≥ 0 (4)
</figure>
<bodyText confidence="0.917444095238095">
The first term models the joint dependencies
between the task weights and the interaction
2(w matrix and it is defined as CMTL(w, n) =
− �wt)T n⊗(w − �wt), where n⊗ = n �
1
Id. The function D(·) represents the diver-
gence between a pair of positive definite matri-
ces. Similar to (Saha et al., 2011), to define
D(·) we also consider the family of Bregman di-
vergences and specifically the LogDet and the
Von Neumann divergences. Given two matri-
ces X, Y E Rn×n, the LogDet divergence is
DLD(X,Y) = tr(XY−1) − log IXY−11 − n,
while the Von Neumann divergence is computed
as DV N(X, Y) = tr(X log X−Y log Y−X+Y).
The optimization process to solve Eq.4 is per-
formed with an alternate scheme: first, with a
fixed n, we compute w; then, given w we opti-
mize for n. The closed-form solution for updating
w, which we derived similarly to the PA update
(Crammer et al., 2006), becomes:
</bodyText>
<equation confidence="0.998623">
wt = wt−1 + sgn(yt − ˆyt)τt(Ωt−1 ⊗ Id)−1Ot (5)
</equation>
<bodyText confidence="0.999938166666667">
In practice, the interaction matrix works as a learn-
ing rate when updating the weights of each task.
Similarly, following previous works (Tsuda et al.,
2005), the update steps for the interaction matrix
n can be easily derived. For the Log-Det diver-
gence we have:
</bodyText>
<equation confidence="0.990375">
Ωt = (Ωt−1 + η sym( �WTt−1�Wt−1))−1 (6)
</equation>
<page confidence="0.972076">
222
</page>
<bodyText confidence="0.884844">
while for the Von Neumann we obtain:
</bodyText>
<equation confidence="0.932513">
Dt = exp(log Dt−1 − η sym(�WTt−1 Wt−1)) (7)
</equation>
<bodyText confidence="0.994926888888889">
where Wt ∈ Rd×K is a matrix obtained by
column-wise reshaping the weight vector wt,
sym(X) = (X + XT )/2 and q is the learning
rate parameter. The sequence of steps to compute
SZt and wt is summarized in Algorithm 1. Impor-
tantly, the weight vector is updated at each round
t, while SZt is initialized to a diagonal matrix and
it is only computed after R iterations. In this way,
at the beginning, the tasks are assumed to be in-
dependent and the task-specific regression mod-
els are learned in isolation. Then, after R rounds,
the interaction matrix is updated and the weights
are refined considering tasks dependencies. This
leads to a progressive increase in the correlation
of weight vectors of related tasks. In the follow-
ing, PAMTLvn refers to PAMTL with the Von
Neumann updates and PAMTLld to PAMTL with
LogDet updates.
</bodyText>
<sectionHeader confidence="0.998321" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<bodyText confidence="0.99992484">
In this section, we describe the data used in our ex-
periments, the features extracted from the source
and target sentences, the evaluation metric and the
baselines used for comparison.
Data. We experiment with English-French
datasets coming from Technology Entertainment
Design talks (TED), Information Technology
manuals (IT) and Education Material (EM). All
datasets provide a set of tuples composed by
(source, translation and post-edited translation).
The TED dataset is distributed in the Trace cor-
pus4 and includes, as source sentences, the sub-
titles of several talks spanning a range of topics
presented in the TED conferences. Translations
were generated by two different MT systems: a
phrase-based statistical MT system and a commer-
cial rule-based system. Post-editions were col-
lected from four different translators, as described
by Wisniewski et al. (2013).
The IT manuals data come from two language
service providers, henceforth LSP1 and LSP2.
The ITLSP1 tuples belong to a software manual
translated by an SMT system trained using the
Moses toolkit (Koehn et al., 2007). The post-
editions were produced by one professional trans-
</bodyText>
<footnote confidence="0.9765415">
4http://anrtrace.limsi.fr/trace_
postedit.tar.bz2
</footnote>
<table confidence="0.9998871">
Domain No. Vocab. Avg. Snt.
tokens Size Length
TED src 20,048 3,452 20
TED tgt 21,565 3,940 22
ITLSP1 src 12,791 2,013 13
ITLSP1 tgt 13,626 2,321 13
EM src 15,327 3,200 15
EM tgt 17,857 3,149 17
ITLSP2 src 15,128 2,105 13
ITLSP2 tgt 17,109 2,104 14
</table>
<tableCaption confidence="0.999895">
Table 1: Data statistics for each domain.
</tableCaption>
<bodyText confidence="0.999717657142857">
lator. The ITLSP2 data includes a software man-
ual from the automotive industry; its source sen-
tences are translated with an adaptive proprietary
MT system and post-edited by several profes-
sional translators. The EM corpus is also pro-
vided by LSP2 and regards educational material
(e.g. courseware and assessments) of various text
styles. The translations and post-editions are pro-
duced in the same way as for ITLSP2. The ITLSP2
and the EM datasets are derived from the Au-
todesk Post-Editing Data corpus.5
In total, we end up with four domains (TED,
ITLSP1, EM and ITLSP2), which allows us to eval-
uate the PAMTL algorithm in realistic conditions
where the QE component is exposed to a contin-
uous stream of heterogeneous data. Each domain
is composed by 1,000 tuples formed by: i) the En-
glish source sentence, ii) its automatic translation
in French, and iii) a real-valued quality label ob-
tained by computing the HTER between the trans-
lation and the post-edition with the TERCpp open
source tool.6
Table 1 reports some macro-indicators (num-
ber of tokens, vocabulary size, average sentence
length) that give an idea about the similarities and
differences between domains. Although they con-
tain data from different software manuals, similar
vocabulary size and sentence lengths for the two
IT domains seem to reflect some commonalities in
their technical style and jargon. Larger values for
TED and EM evidence a higher lexical variability
in the topics that compose these domains and the
expected stylistic differences featured by speech
transcriptions and non-technical writing. Over-
all, these numbers suggest a possible dissimilar-
</bodyText>
<footnote confidence="0.99912075">
5https://autodesk.app.box.com/
Autodesk-PostEditing
6http://sourceforge.net/projects/
tercpp/
</footnote>
<page confidence="0.998436">
223
</page>
<figureCaption confidence="0.999844">
Figure 1: Validation curves for the R parameter.
</figureCaption>
<bodyText confidence="0.999941342857143">
ity between ITLSP1 and ITLSP2 and the other two
domains, which might make knowledge transfer
across them more difficult and QE model reactiv-
ity to domain changes particularly important.
Features. Our models are trained using the 17
baseline features proposed in (Specia et al., 2009),
extracted with the online version of the QuEst fea-
ture extractor (Shah et al., 2014). These features
take into account the complexity of the source sen-
tence (e.g. number of tokens, number of transla-
tions per source word) and the fluency of the trans-
lation (e.g. language model probabilities). Their
description is available in (Callison-Burch et al.,
2012). The results of previous WMT QE shared
tasks have shown that these features are particu-
larly competitive in the HTER prediction task.
Baselines. We compare the performance of
PAMTL against three baselines: i) pooling mean,
ii) pooling online single task learning (STLpool)
and iii) in-domain online single task learning
(STLz,,,). The pooling mean is obtained by assign-
ing a fixed prediction value to each test point. This
value is the average HTER computed on the entire
pool of training data. Although assigning the same
prediction to each test instance would be useless
in real applications, we compare against the mean
baseline since it is often hard to beat in regression
tasks, especially when dealing with heterogeneous
data distributions (Rubino et al., 2013).
The two online single task baselines implement
the PA algorithm described in Section 3.1. The
choice of PA is to make them comparable to our
method, so that we can isolate more precisely the
contribution of multitask learning. STLpool results
are obtained by a single model trained on the entire
</bodyText>
<figureCaption confidence="0.974446">
Figure 2: Learning curves for all the domains,
computed by calculating the mean MAE (↓) of the
four domains.
</figureCaption>
<bodyText confidence="0.953220105263158">
pool of available training data presented in random
order. STLz,,, results are obtained by separately
training one model for each domain. These repre-
sent two alternative strategies for the integration of
QE in the CAT framework. The former would al-
low a single model to simultaneously support mul-
tiple translation jobs in different domains, without
any notion about their relations. The latter would
lead to a more complex architecture, organized as
a pool of independent, specialized QE modules.
Evaluation metric. The performance of our re-
gression models is evaluated in terms of mean ab-
solute error (MAE), a standard error measure for
regression problems commonly used also for QE
(Callison-Burch et al., 2012). The MAE is the av-
erage of the absolute errors ez = |ˆyz − yz|, where
ˆyz is the prediction of the model and yz is the true
value for the ith instance. As it is an error mea-
sure, lower values indicate better performance (↓).
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999653916666666">
In this Section we evaluate the proposed PAMTL
algorithm. First, by analyzing how the number of
rounds R impacts on the performance of our ap-
proach, we empirically find the value that will be
used to train the model. Then, the learned model
is run on test data and compared against the base-
lines. Performance is analyzed both by averag-
ing the MAE results computed on all the domains,
and by separately discussing in-domain behavior.
Finally, the capability of the algorithm to learn
task correlations and, in turn, transfer knowledge
across them, is analysed by presenting the correla-
</bodyText>
<page confidence="0.998669">
224
</page>
<figureCaption confidence="0.999644">
Figure 3: Learning curves showing MAE (↓) variations for each domain.
</figureCaption>
<bodyText confidence="0.9991172">
tion matrix of the task weights.
For the evaluation, we uniformly sample 700 in-
stances from each domain for training, leaving the
remaining 300 instances for test. The training sets
of all the domains are concatenated and shuffled
to create a random sequence of points. To inves-
tigate the impact of different amounts of data on
the learning process, we create ten subsets of 10
to 100% of the training data. We optimize the pa-
rameters of all the models with a grid search pro-
cedure using 5-fold cross-validation. This process
is repeated for 30 different train/test splits over the
whole data. Results are presented with 95% confi-
dence bands.7
Analysis of the R parameter. We empirically
study the influence of the number of instances re-
quired to start updating the interaction matrix (the
R parameter in Algorithm 1). For that, we per-
form a set of experiments where R is initialized
with nine different values (expressed as percent-
age of training data). Figure 1 shows the val-
idation curves obtained in cross-validation over
the training data using the LogDet and Von Neu-
mann updates. The curves report the performance
(MAE) difference between STLin and PAMTLld
</bodyText>
<footnote confidence="0.888558">
7Confidence bands are used to show whether performance
differences between the models are statistically significant.
</footnote>
<bodyText confidence="0.999708481481482">
(black curve) and STLin and PAMTLvn (grey
curve). The higher the difference, the better. The
PAMTLvn curve differs from PAMTLld one only
for small values of R (&lt; 20), showing that the two
divergences are substantially equivalent. It is in-
teresting to note that with only 20% of the training
data (R = 20), PAMTL is able to find a stable
set of weights and to effectively update the inter-
action matrix. Larger values of R harm the perfor-
mance, indicating that the interaction matrix up-
dates require a reasonable amount of points to reli-
ably transfer knowledge across tasks. We use this
observation to set R for our final experiment, in
which we evaluate the methods over the test data.
Evaluation on test data. Global evaluation re-
sults are summarized in Figure 2, which shows
five curves: one for each baseline (Mean, STLin,
STLpool) and two for the proposed online mul-
titask method (PAMTLvn and PAMTLld). The
curves are computed by calculating the average
MAE achieved with different amounts of data on
each domain’s test set.
The results show that PAMTLld and PAMTLvn
have similar trends (confirming the substantial
equivalence previously observed), and that both
outperform all the baselines in a statistically sig-
nificant manner. This holds for all the training set
</bodyText>
<page confidence="0.996167">
225
</page>
<bodyText confidence="0.996286490196079">
sizes we experimented with. The maximum im-
provement over the baselines (+1.3 MAE) is ob-
served with 60% of the training data when com-
paring PAMTL„,,, with STLZ,,,. Even if this is the
best baseline, also with 100% of the data its results
are not competitive and of limited interest with re-
spect to our application scenario (the integration of
effective QE models in the CAT framework). In-
deed, despite the STLZ,,, downward error trend, it’s
worth remarking that an increased competitive-
ness would come at the cost of: i) collecting large
amounts of annotated data and ii) integrating the
model in a complex CAT architecture organized
as a pool of independent QE components. Under
the tested conditions, it is also evident that the al-
ternative strategy of using a single QE component
to simultaneously serve multiple translation jobs is
not viable. Indeed, STLpool is the worst perform-
ing baseline, with a constant distance of around 2
MAE points from the best PAMTL model for al-
most all the training set sizes. The fact that, with
increasing amounts of data, the STLpool predic-
tions get close to those of the simple mean base-
line indicates its limitations to cope with the noise
introduced by a continuous stream of diverse data.
The capability to handle such stream by exploit-
ing task relationships makes PAMTL a much bet-
ter solution for our purposes.
Per-domain analysis. Figure 3 shows the MAE
results achieved on each target domain by the most
competitive baseline (STLZ,,,) and the proposed on-
line multitask method (PAMTL„,,,, PAMTLld).
For all the domains, the behavior of PAMTLld
and PAMTL„,,, is consistent and almost identi-
cal. With both divergences, the improvement of
PAMTL over online single task learning becomes
statistically significant when using more than 30%
of the training data (210 instances). Interestingly,
in all the plots, with 20% of the training data
(140 instances for each domain, i.e. a total of
560 instances adding data from all the domains),
PATML results are comparable to those achieved
by STLZ,,, with 80% of the training data (i.e. 560
in-domain instances). This confirms that PATML
can effectively leverage data heterogeneity, and
that a limited amount of in-domain data is suf-
ficient to make it competitive. Nevertheless, for
all domains except EM, the PATML and STLZ,,,
curves converge to comparable performance when
trained with 100% of the data. This is not surpris-
ing if we consider that EM has a varied vocabulary
</bodyText>
<figureCaption confidence="0.8647355">
Figure 4: Correlation among the weights predicted
by PATML„,,, using all the training data.
</figureCaption>
<bodyText confidence="0.999952576923077">
(see Table 1), which may be evidence of the pres-
ence of different topics, increasing its similarity
with other domains. The same assumption should
also hold for TED, given that its source sentences
belong to talks about different topics. The results
for the TED domain, however, do not present the
same degree of improvement as for EM.
To better understand the relationships learned
by the PAMTL models, we compute the corre-
lation between the weights inferred for each do-
main (as performed by Saha et al. (2011)). Fig-
ure 4 shows the correlations computed on the task
weights learned by PATML„,,, with all the train-
ing data. In the matrix, EM is the domain that
presents the highest correlation with all the others.
Instead, TED and ITLSP2 are the less correlated
with the other domains (even though, being close
to the other IT domain, ITLSP2 can share knowl-
edge with it). This explains why the improvement
measured on TED is smaller compared to EM. Al-
though there is no canonical way to measure cor-
relation among domains, the weights correlation
matrix and the improvements achieved by PAMTL
show the capability of the method to identify task
relationships and exploit them to improve the gen-
eralization properties of the model.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999678125">
We addressed the problem of developing qual-
ity estimation models suitable for integration in
computer-assisted translation technology. In this
framework, on-the-fly MT quality prediction for a
stream of heterogeneous data coming from differ-
ent domains/users/MT systems represents a major
challenge. On one side, processing such stream
calls for supervised solutions that avoid the bot-
</bodyText>
<page confidence="0.994091">
226
</page>
<bodyText confidence="0.999983517241379">
tleneck of periodically retraining the QE models
in a batch fashion. On the other side, handling
data heterogeneity requires the capability to lever-
age data similarities and dissimilarities. While
previous works addressed these two problems in
isolation, by proposing approaches respectively
based on online and multitask learning, our so-
lution unifies the two paradigms in a single on-
line multitask approach. To this aim, we devel-
oped a novel regression algorithm, filling a gap
left by current online multitask learning methods
that only operate in classification mode. Our ap-
proach, which is based on the passive aggressive
algorithm, has been successfully evaluated against
strong online single-task competitors in a scenario
involving four domains. Our future objective is
to extend our evaluation to streams of data com-
ing from a larger number of domains. Finding
reasonably-sized datasets for this purpose is cur-
rently difficult. However, we are confident that the
gradual shift of the translation industry towards
human MT post-editing will not only push for fur-
ther research on these problems, but also provide
data for larger scale evaluations in a short time.
To allow for replicability of our results and
promote further research on QE, the features ex-
tracted from our data, the computed labels and
the source code of the method are available at
https://github.com/jsouza/pamtl.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999363">
This work has been partially supported by the EC-
funded H2020 project QT21 (grant agreement no.
645452). The authors would like to thank Dr.
Ventsislav Zhechev for his support with the Au-
todesk Post-Editing Data corpus.
</bodyText>
<sectionHeader confidence="0.998597" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999711308823529">
Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Massimo Pontil. 2008. Convex multi-task
feature learning. Machine Learning, 73(3):243–
272, January.
Nguyen Bach, F. Huang, and Y. Al-Onaizan. 2011.
Goodness: A method for measuring machine trans-
lation confidence. In 49th Annual Meeting of the
Association for Computational Linguistics.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Eighth
Workshop on Statistical Machine Translation, pages
1–44, Sofia, Bulgaria, August.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 Workshop
on Statistical Machine Translation. In Proceedings
of the Ninth Workshop on Statistical Machine Trans-
lation, pages 12–58, Baltimore, USA, June.
Edwin Bonilla, Kian Ming Chai, and Christopher
Williams. 2008. Multi-task Gaussian Process Pre-
diction. In Advances in Neural Information Process-
ing Systems 20: NIPS’08.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada, June.
Rich Caruana. 1997. Multitask learning. In Machine
Learning, pages 41–75.
Giovanni Cavallanti, N Cesa-Bianchi, and C Gentile.
2010. Linear algorithms for online multitask clas-
sification. The Journal of Machine Learning Re-
search, 11:2901–2934.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An application to Machine Translation Quality Es-
timation. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 32–42, Sofia, Bulgaria, August.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms. The Journal of Ma-
chine Learning Research, 7:551–585.
Jos´e G. C. de Souza, Marco Turchi, and Matteo Ne-
gri. 2014a. Machine Translation Quality Estima-
tion Across Domains. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 409–
420, Dublin, Ireland, August.
Jos´e G. C. de Souza, Marco Turchi, and Matteo Negri.
2014b. Towards a Combination of Online and Mul-
titask Learning for MT Quality Estimation: a Pre-
liminary Study. In Proceedings of Workshop on In-
teractive and Adaptive Machine Translation in 2014
(IAMT 2014), Vancouver, BC, Canada, October.
Eric Eaton and PL Ruvolo. 2013. ELLA: An efficient
lifelong learning algorithm. In Proceedings of the
30th International Conference on Machine Learn-
ing, pages 507–515, Atlanta, Georgia, USA, June.
Marcello Federico, Nicola Bertoldi, Mauro Cettolo,
Matteo Negri, Marco Turchi, Marco Trombetti,
Alessandro Cattelan, Antonio Farina, Domenico
</reference>
<page confidence="0.967573">
227
</page>
<reference confidence="0.999899660714286">
Lupinetti, Andrea Martines, Alberto Massidda, Hol-
ger Schwenk, Loic Barrault, Frederic Blain, Philipp
Koehn, Christian Buck, and Ulrich Germann. 2014.
THE MATECAT TOOL. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: System Demonstrations,
pages 129–132, Dublin, Ireland, August.
Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, and
Salim Roukos. 2014. Adaptive HTER Estimation
for Document-Specific MT Post-Editing. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1:
Long Papers), pages 861–870, Baltimore, Maryland,
June.
Laurent Jacob, Jean-philippe Vert, Francis R Bach, and
Jean-philippe Vert. 2009. Clustered Multi-Task
Learning: A Convex Formulation. In D Koller,
D Schuurmans, Y Bengio, and L Bottou, editors,
Advances in Neural Information Processing Systems
21, pages 745–752. Curran Associates, Inc.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zenz, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007 Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012. Match without a Referee: Eval-
uating MT Adequacy without Reference Transla-
tions. In Proceedings of the Machine Translation
Workshop (WMT2012), pages 171–180, Montr´eal,
Canada, June.
Francesco Parrella. 2007. Online support vector re-
gression. Master’s Thesis, Department of Informa-
tion Science, University of Genoa, Italy.
Raphael Rubino, Jos´e G. C. de Souza, and Lucia Spe-
cia. 2013. Topic Models for Translation Quality
Estimation for Gisting Purposes. In Machine Trans-
lation Summit XIV, pages 295–302.
Paul Ruvolo and Eric Eaton. 2014. Online Multi-Task
Learning via Sparse Dictionary Optimization. In
Proceedings of the 28th AAAI Conference on Arti-
ficial Intelligence (AAAI-14), Qu´ebec City, Qu´ebec,
Canada, July.
Avishek Saha, Piyush Rai, Hal Daum´e, and Suresh
Venkatasubramanian. 2011. Online Learning of
Multiple Tasks and their Relationships. In Proceed-
ings of the 14th International Conference on Ar-
tificial Intelligence and Statistics (AISTATS), Fort
Lauderdale, FL, USA, April.
Kashif Shah, Marco Turchi, and Lucia Specia. 2014.
An Efficient and User-friendly Tool for Machine
Translation Quality Estimation. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation, Reykjavik, Iceland, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Association for Machine Translation in
the Americas, Cambridge, MA, USA, August.
Radu Soricut and A Echihabi. 2010. Trustrank: In-
ducing trust in automatic translations via ranking. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, number July,
pages 612–621.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the Sentence-Level Quality of Machine Transla-
tion Systems. In Proceedings of the 13th Annual
Conference of the EAMT, pages 28–35, Barcelona,
Spain, May.
Koji Tsuda, Gunnar R¨atsch, and Manfred K Warmuth.
2005. Matrix exponentiated gradient updates for on-
line learning and bregman projection. In Journal of
Machine Learning Research, pages 995–1018.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the Subjectivity of Human
Judgements in MT Quality Estimation. In Proceed-
ings of the Eighth Workshop on Statistical Machine
Translation (WMT), pages 240–251, Sofia, Bulgaria,
August.
Marco Turchi, Antonios Anastasopoulos, Jos´e G. C. de
Souza, and Matteo Negri. 2014. Adaptive Qual-
ity Estimation for Machine Translation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 710–720, Baltimore, Maryland, USA,
June.
Guillaume Wisniewski, Anil Kumar Singh, Natalia Se-
gal, and Franc¸ois Yvon. 2013. Design and Anal-
ysis of a Large Corpus of Post-Edited Translations:
Quality Estimation, Failure Analysis and the Vari-
ability of Post-Edition. In Machine Translation
Summit XIV, pages 117–124.
Yan Yan, Elisa Ricci, Ramanathan Subramanian,
Gaowen Liu, and Nicu Sebe. 2014. Multitask lin-
ear discriminant analysis for view invariant action
recognition. IEEE Transactions on Image Process-
ing, 23(12):5599–5611.
Yu Zhang and Dit-yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Twenty-Sixth Con-
ference Annual Conference on Uncertainty in Artifi-
cial Intelligence (UAI-10), pages 733–742, Catalina
Island, CA, USA, July.
Leon Wenliang Zhong and James T. Kwok. 2012.
Convex multitask learning with flexible task clus-
ters. In Proceedings of the 29 th International Con-
ference on Machine Learning, Edinburgh, Scotland,
June.
</reference>
<page confidence="0.997663">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911780">
<title confidence="0.999963">Online Multitask Learning for Machine Translation Quality Estimation</title>
<author confidence="0.998735">G C de_Matteo Elisa Marco</author>
<affiliation confidence="0.9504385">(1)FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, (2)University of Trento,</affiliation>
<abstract confidence="0.99971864">We present a method for predicting machine translation output quality geared to the needs of computer-assisted translation. include the capability to: continuously learn and self-adapt to a stream of data coming from multiple translation to data diversity by exhuman feedback, and data similarity by learning and transferring knowledge across domains. To achieve these goals, we combine two supervised machine learning paradigms, online and multitask learning, adapting and unifying them in a single framework. We show the effectiveness of our approach in a regression task (HTER prediction), in which online multitask learning outperforms the competitive online single-task and pooling methods used for comparison. This indicates the feasibility of integrating in a CAT tool a single QE component capable to simultaneously serve (and continuously learn from) multiple translation jobs involving different domains and users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andreas Argyriou</author>
<author>Theodoros Evgeniou</author>
<author>Massimiliano Massimo Pontil</author>
</authors>
<date>2008</date>
<booktitle>Convex multi-task feature learning. Machine Learning,</booktitle>
<volume>73</volume>
<issue>3</issue>
<pages>272</pages>
<contexts>
<context position="10605" citStr="Argyriou et al., 2008" startWordPosition="1658" endWordPosition="1661">s are likely to be unrelated and knowledge transfer should force an increase in the distance between model parameters. No correlation indicates that the tasks are independent and no knowledge transfer should take place. In our case, a task is a set of (instance, label) pairs obtained from source sentences coming from different translation jobs, together with their translations produced by several MT systems and the relative post-editions from various translators. In this paper the terms task and domain are used interchangeably. Early MTL methods model only positive correlation (Caruana, 1997; Argyriou et al., 2008), which results in a positive knowledge transfer between all the tasks, with the risk of impairing each other’s performance when they are unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, </context>
</contexts>
<marker>Argyriou, Evgeniou, Pontil, 2008</marker>
<rawString>Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Massimo Pontil. 2008. Convex multi-task feature learning. Machine Learning, 73(3):243– 272, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>F Huang</author>
<author>Y Al-Onaizan</author>
</authors>
<title>Goodness: A method for measuring machine translation confidence.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2550" citStr="Bach et al., 2011" startWordPosition="389" endWordPosition="392">2014). are extremely useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: 1. The QE module is exposed to a continuous stream of data. The amount of such data and the tight schedule of multiple, simultaneous translation jobs prevents from (theoretically feasible but impractical) complete re-training procedures in a batch fashion and advocate for continuous learning meth</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>Nguyen Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness: A method for measuring machine translation confidence. In 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5546" citStr="Bojar et al., 2013" startWordPosition="853" endWordPosition="856">hat, at the same time, handles positive and negative transfer and performs online weight updates. • A comparison between online multitask and alternative, state-of-the-art online learning strategies. Our experiments, carried out in a realistic scenario involving a stream of data from four domains, lead to consistent results that prove the effectiveness of our approach. 2 Related Work In recent years, sentence-level QE has been mainly investigated in controlled evaluation scenarios such as those proposed by the shared tasks organized within the WMT workshop on SMT (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). In this framework, systems trained from a collection of (source, target, label) instances are evaluated based on their capability to predict the correct label3 for new, unseen test items. Compared to our application scenario, the shared tasks setting differs in two main aspects. 2The HTER is the minimum edit distance between a translation suggestion and its manually post-edited version in the [0,1] interval. Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the reference. 3Possible</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
</authors>
<title>Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<booktitle>Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>12--58</pages>
<location>Baltimore, USA,</location>
<contexts>
<context position="5567" citStr="Bojar et al., 2014" startWordPosition="857" endWordPosition="860">e, handles positive and negative transfer and performs online weight updates. • A comparison between online multitask and alternative, state-of-the-art online learning strategies. Our experiments, carried out in a realistic scenario involving a stream of data from four domains, lead to consistent results that prove the effectiveness of our approach. 2 Related Work In recent years, sentence-level QE has been mainly investigated in controlled evaluation scenarios such as those proposed by the shared tasks organized within the WMT workshop on SMT (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). In this framework, systems trained from a collection of (source, target, label) instances are evaluated based on their capability to predict the correct label3 for new, unseen test items. Compared to our application scenario, the shared tasks setting differs in two main aspects. 2The HTER is the minimum edit distance between a translation suggestion and its manually post-edited version in the [0,1] interval. Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the reference. 3Possible label types include </context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin Bonilla</author>
<author>Kian Ming Chai</author>
<author>Christopher Williams</author>
</authors>
<title>Multi-task Gaussian Process Prediction.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20:</booktitle>
<pages>08</pages>
<contexts>
<context position="11160" citStr="Bonilla et al., 2008" startWordPosition="1745" endWordPosition="1748"> only positive correlation (Caruana, 1997; Argyriou et al., 2008), which results in a positive knowledge transfer between all the tasks, with the risk of impairing each other’s performance when they are unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, however, learn the task relationships in batch mode. To overcome this limitation, recent works propose the “lifelong learning” paradigm (Eaton and Ruvolo, 2013; Ruvolo and Eaton, 2014), in which all the instances of a task are given to the learner sequentially and the previously learned tasks are leveraged to improve generalization for future tasks. This approach, however, is not applicable to our scenario as it assumes that all the instances of each task are processed as separate blocks. In this paper we propose a novel MTL algorithm for QE that le</context>
</contexts>
<marker>Bonilla, Chai, Williams, 2008</marker>
<rawString>Edwin Bonilla, Kian Ming Chai, and Christopher Williams. 2008. Multi-task Gaussian Process Prediction. In Advances in Neural Information Processing Systems 20: NIPS’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5526" citStr="Callison-Burch et al., 2012" startWordPosition="849" endWordPosition="852"> a new regression algorithm that, at the same time, handles positive and negative transfer and performs online weight updates. • A comparison between online multitask and alternative, state-of-the-art online learning strategies. Our experiments, carried out in a realistic scenario involving a stream of data from four domains, lead to consistent results that prove the effectiveness of our approach. 2 Related Work In recent years, sentence-level QE has been mainly investigated in controlled evaluation scenarios such as those proposed by the shared tasks organized within the WMT workshop on SMT (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). In this framework, systems trained from a collection of (source, target, label) instances are evaluated based on their capability to predict the correct label3 for new, unseen test items. Compared to our application scenario, the shared tasks setting differs in two main aspects. 2The HTER is the minimum edit distance between a translation suggestion and its manually post-edited version in the [0,1] interval. Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the </context>
<context position="22361" citStr="Callison-Burch et al., 2012" startWordPosition="3694" endWordPosition="3697">TLSP1 and ITLSP2 and the other two domains, which might make knowledge transfer across them more difficult and QE model reactivity to domain changes particularly important. Features. Our models are trained using the 17 baseline features proposed in (Specia et al., 2009), extracted with the online version of the QuEst feature extractor (Shah et al., 2014). These features take into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the translation (e.g. language model probabilities). Their description is available in (Callison-Burch et al., 2012). The results of previous WMT QE shared tasks have shown that these features are particularly competitive in the HTER prediction task. Baselines. We compare the performance of PAMTL against three baselines: i) pooling mean, ii) pooling online single task learning (STLpool) and iii) in-domain online single task learning (STLz,,,). The pooling mean is obtained by assigning a fixed prediction value to each test point. This value is the average HTER computed on the entire pool of training data. Although assigning the same prediction to each test instance would be useless in real applications, we c</context>
<context position="24249" citStr="Callison-Burch et al., 2012" startWordPosition="3998" endWordPosition="4001">ed by separately training one model for each domain. These represent two alternative strategies for the integration of QE in the CAT framework. The former would allow a single model to simultaneously support multiple translation jobs in different domains, without any notion about their relations. The latter would lead to a more complex architecture, organized as a pool of independent, specialized QE modules. Evaluation metric. The performance of our regression models is evaluated in terms of mean absolute error (MAE), a standard error measure for regression problems commonly used also for QE (Callison-Burch et al., 2012). The MAE is the average of the absolute errors ez = |ˆyz − yz|, where ˆyz is the prediction of the model and yz is the true value for the ith instance. As it is an error measure, lower values indicate better performance (↓). 5 Results and Discussion In this Section we evaluate the proposed PAMTL algorithm. First, by analyzing how the number of rounds R impacts on the performance of our approach, we empirically find the value that will be used to train the model. Then, the learned model is run on test data and compared against the baselines. Performance is analyzed both by averaging the MAE re</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<title>Multitask learning.</title>
<date>1997</date>
<booktitle>In Machine Learning,</booktitle>
<pages>41--75</pages>
<contexts>
<context position="10581" citStr="Caruana, 1997" startWordPosition="1656" endWordPosition="1657">s that the tasks are likely to be unrelated and knowledge transfer should force an increase in the distance between model parameters. No correlation indicates that the tasks are independent and no knowledge transfer should take place. In our case, a task is a set of (instance, label) pairs obtained from source sentences coming from different translation jobs, together with their translations produced by several MT systems and the relative post-editions from various translators. In this paper the terms task and domain are used interchangeably. Early MTL methods model only positive correlation (Caruana, 1997; Argyriou et al., 2008), which results in a positive knowledge transfer between all the tasks, with the risk of impairing each other’s performance when they are unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 20</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. In Machine Learning, pages 41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Cavallanti</author>
<author>N Cesa-Bianchi</author>
<author>C Gentile</author>
</authors>
<title>Linear algorithms for online multitask classification.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--2901</pages>
<contexts>
<context position="12291" citStr="Cavallanti et al., 2010" startWordPosition="1934" endWordPosition="1937">e processed as separate blocks. In this paper we propose a novel MTL algorithm for QE that learns the structure shared by different tasks in an online fashion and from an input stream of instances from all the tasks. To this aim, we extend the online passive aggressive (PA) algorithm (Crammer et al., 2006) to the multitask scenario, learning a set of task-specific regression models. The multitask component of our method is given by an “interaction matrix” that defines to which extent each encoded task can “borrow” and “lend” knowledge from and to the other tasks. Opposite to previous methods (Cavallanti et al., 2010) that assume fixed dependencies among tasks, we propose to learn the interaction matrix instanceby-instance from the data. To this aim we follow the recent work of Saha et al. (2011), extending it to a regression setting. The choice of PA is motivated by practical reasons. Indeed, by providing the best trade-off between accuracy and computational time (He and Wang, 2012) compared to other algorithms such as OnlineSVR (Parrella, 2007), it represents a good solution to meet the demand of efficiency posed by the CAT framework. 3.1 Passive Aggressive Algorithm PA follows the typical online learnin</context>
</contexts>
<marker>Cavallanti, Cesa-Bianchi, Gentile, 2010</marker>
<rawString>Giovanni Cavallanti, N Cesa-Bianchi, and C Gentile. 2010. Linear algorithms for online multitask classification. The Journal of Machine Learning Research, 11:2901–2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling Annotator Bias with Multi-task Gaussian Processes: An application to Machine Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>32--42</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7898" citStr="Cohn and Specia (2013)" startWordPosition="1226" endWordPosition="1229">tinuously learn from a stream of (potentially heterogeneous) data. Experimental results suggest the effectiveness of online learning as a way to exploit user feedback to tailor QE predictions to their quality standards and to cope with the heterogeneity of data coming from different domains. However, though robust to user and domain changes, the method is solely driven by the distance computed between predicted and true labels, and it does not exploit any notion of similarity between tasks (e.g. domains, users, MT engines). On the other way round, task relatedness is successfully exploited by Cohn and Specia (2013), who apply multitask learning to jointly learn from data obtained from several annotators with different levels of expertise and reliability. A similar approach is adopted by de Souza et al. (2014a), who apply multitask learning to cope with situations in which a QE model has to be trained with scarce data from multiple domains/genres, different from the actual test domain. The two methods significantly outperform both individual single-task (indomain) models and single pooled models. However, operating in batch learning mode, none of them provides the continuous learning capabilities desirab</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An application to Machine Translation Quality Estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 32–42, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="11974" citStr="Crammer et al., 2006" startWordPosition="1883" endWordPosition="1886">uvolo, 2013; Ruvolo and Eaton, 2014), in which all the instances of a task are given to the learner sequentially and the previously learned tasks are leveraged to improve generalization for future tasks. This approach, however, is not applicable to our scenario as it assumes that all the instances of each task are processed as separate blocks. In this paper we propose a novel MTL algorithm for QE that learns the structure shared by different tasks in an online fashion and from an input stream of instances from all the tasks. To this aim, we extend the online passive aggressive (PA) algorithm (Crammer et al., 2006) to the multitask scenario, learning a set of task-specific regression models. The multitask component of our method is given by an “interaction matrix” that defines to which extent each encoded task can “borrow” and “lend” knowledge from and to the other tasks. Opposite to previous methods (Cavallanti et al., 2010) that assume fixed dependencies among tasks, we propose to learn the interaction matrix instanceby-instance from the data. To this aim we follow the recent work of Saha et al. (2011), extending it to a regression setting. The choice of PA is motivated by practical reasons. Indeed, b</context>
<context position="14238" citStr="Crammer et al., 2006" startWordPosition="2285" endWordPosition="2289">maller or equal to c, and grows linearly with this difference otherwise. The c parameter is given as input and regulates the sensitivity to mistakes. The slack variable ξ acts as an upper-bound to the loss, while the C parameter is introduced to control the aggressiveness of the weights update. High C values lead to more aggressive weight updates. However, when the labels present some degree of noise (a common situation in MT QE), they might cause the learner to drastically change the weight vector in a wrong direction. In these situations, setting C to small values is desirable. As shown in (Crammer et al., 2006), a closed form solution for the weights update in Eq.1 can be derived as: wt = wt−1 + sgn(yt − ˆyt)τtxt (3) with τt = min(C, |X It ||2) and `t = `,(w, (xt, yt)). 3.2 Passive Aggressive MTL Algorithm Our Passive Aggressive Multitask Learning (PAMTL) algorithm extends the traditional PA for regression to multitask learning. Our approach is inspired by the Online Task Relationship Learning algorithm proposed by Saha et al. (2011) which, however, is only defined for classification. The learning process considers one instance at each round t. The random sequence of instances belongs to a fixed set</context>
<context position="17167" citStr="Crammer et al., 2006" startWordPosition="2856" endWordPosition="2859">sitive definite matrices. Similar to (Saha et al., 2011), to define D(·) we also consider the family of Bregman divergences and specifically the LogDet and the Von Neumann divergences. Given two matrices X, Y E Rn×n, the LogDet divergence is DLD(X,Y) = tr(XY−1) − log IXY−11 − n, while the Von Neumann divergence is computed as DV N(X, Y) = tr(X log X−Y log Y−X+Y). The optimization process to solve Eq.4 is performed with an alternate scheme: first, with a fixed n, we compute w; then, given w we optimize for n. The closed-form solution for updating w, which we derived similarly to the PA update (Crammer et al., 2006), becomes: wt = wt−1 + sgn(yt − ˆyt)τt(Ωt−1 ⊗ Id)−1Ot (5) In practice, the interaction matrix works as a learning rate when updating the weights of each task. Similarly, following previous works (Tsuda et al., 2005), the update steps for the interaction matrix n can be easily derived. For the Log-Det divergence we have: Ωt = (Ωt−1 + η sym( �WTt−1�Wt−1))−1 (6) 222 while for the Von Neumann we obtain: Dt = exp(log Dt−1 − η sym(�WTt−1 Wt−1)) (7) where Wt ∈ Rd×K is a matrix obtained by column-wise reshaping the weight vector wt, sym(X) = (X + XT )/2 and q is the learning rate parameter. The sequen</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>Machine Translation Quality Estimation Across Domains.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>409--420</pages>
<location>Dublin, Ireland,</location>
<marker>de Souza, Turchi, Negri, 2014</marker>
<rawString>Jos´e G. C. de Souza, Marco Turchi, and Matteo Negri. 2014a. Machine Translation Quality Estimation Across Domains. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 409– 420, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>Towards a Combination of Online and Multitask Learning for MT Quality Estimation: a Preliminary Study.</title>
<date>2014</date>
<booktitle>In Proceedings of Workshop on Interactive and Adaptive Machine Translation in 2014 (IAMT 2014),</booktitle>
<location>Vancouver, BC, Canada,</location>
<marker>de Souza, Turchi, Negri, 2014</marker>
<rawString>Jos´e G. C. de Souza, Marco Turchi, and Matteo Negri. 2014b. Towards a Combination of Online and Multitask Learning for MT Quality Estimation: a Preliminary Study. In Proceedings of Workshop on Interactive and Adaptive Machine Translation in 2014 (IAMT 2014), Vancouver, BC, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Eaton</author>
<author>PL Ruvolo</author>
</authors>
<title>ELLA: An efficient lifelong learning algorithm.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning,</booktitle>
<pages>507--515</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="11364" citStr="Eaton and Ruvolo, 2013" startWordPosition="1776" endWordPosition="1779"> unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, however, learn the task relationships in batch mode. To overcome this limitation, recent works propose the “lifelong learning” paradigm (Eaton and Ruvolo, 2013; Ruvolo and Eaton, 2014), in which all the instances of a task are given to the learner sequentially and the previously learned tasks are leveraged to improve generalization for future tasks. This approach, however, is not applicable to our scenario as it assumes that all the instances of each task are processed as separate blocks. In this paper we propose a novel MTL algorithm for QE that learns the structure shared by different tasks in an online fashion and from an input stream of instances from all the tasks. To this aim, we extend the online passive aggressive (PA) algorithm (Crammer et </context>
</contexts>
<marker>Eaton, Ruvolo, 2013</marker>
<rawString>Eric Eaton and PL Ruvolo. 2013. ELLA: An efficient lifelong learning algorithm. In Proceedings of the 30th International Conference on Machine Learning, pages 507–515, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<location>Matteo Negri, Marco Turchi, Marco Trombetti, Alessandro Cattelan, Antonio Farina, Domenico</location>
<marker>Federico, Bertoldi, Cettolo, </marker>
<rawString>Marcello Federico, Nicola Bertoldi, Mauro Cettolo, Matteo Negri, Marco Turchi, Marco Trombetti, Alessandro Cattelan, Antonio Farina, Domenico</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Martines Lupinetti</author>
<author>Alberto Massidda</author>
<author>Holger Schwenk</author>
<author>Loic Barrault</author>
<author>Frederic Blain</author>
<author>Philipp Koehn</author>
<author>Christian Buck</author>
<author>Ulrich Germann</author>
</authors>
<title>THE MATECAT TOOL.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,</booktitle>
<pages>129--132</pages>
<location>Dublin, Ireland,</location>
<marker>Lupinetti, Massidda, Schwenk, Barrault, Blain, Koehn, Buck, Germann, 2014</marker>
<rawString>Lupinetti, Andrea Martines, Alberto Massidda, Holger Schwenk, Loic Barrault, Frederic Blain, Philipp Koehn, Christian Buck, and Ulrich Germann. 2014. THE MATECAT TOOL. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, pages 129–132, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Jian-Ming Xu</author>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Adaptive HTER Estimation for Document-Specific MT Post-Editing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>861--870</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="6915" citStr="Huang et al. (2014)" startWordPosition="1066" endWordPosition="1069">ER values, and post-editing time (e.g. seconds per word). First, the data used are substantially homogeneous (usually they come from the same domain, and target translations are produced by the same MT system). Second, training and test are carried out as distinct, sequential phases. Instead, in the CAT environment, a QE component should ideally serve, adapt to and continuously learn from simultaneous translation jobs involving different MT engines, domains, genres and users (Turchi et al., 2013). These challenges have been separately addressed from different perspectives in few recent works. Huang et al. (2014) proposed a method to adaptively train a QE model for documentspecific MT post-editing. Adaptability, however, is achieved in a batch fashion, by re-training an ad hoc QE component for each document to be translated. The adaptive approach proposed by Turchi et al. (2014) overcomes the limitations of batch methods by applying an online learning protocol to continuously learn from a stream of (potentially heterogeneous) data. Experimental results suggest the effectiveness of online learning as a way to exploit user feedback to tailor QE predictions to their quality standards and to cope with the</context>
</contexts>
<marker>Huang, Xu, Ittycheriah, Roukos, 2014</marker>
<rawString>Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, and Salim Roukos. 2014. Adaptive HTER Estimation for Document-Specific MT Post-Editing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861–870, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Jacob</author>
<author>Jean-philippe Vert</author>
<author>Francis R Bach</author>
<author>Jean-philippe Vert</author>
</authors>
<title>Clustered Multi-Task Learning: A Convex Formulation. In</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 21,</booktitle>
<pages>745--752</pages>
<editor>D Koller, D Schuurmans, Y Bengio, and L Bottou, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="10812" citStr="Jacob et al., 2009" startWordPosition="1691" endWordPosition="1694">e place. In our case, a task is a set of (instance, label) pairs obtained from source sentences coming from different translation jobs, together with their translations produced by several MT systems and the relative post-editions from various translators. In this paper the terms task and domain are used interchangeably. Early MTL methods model only positive correlation (Caruana, 1997; Argyriou et al., 2008), which results in a positive knowledge transfer between all the tasks, with the risk of impairing each other’s performance when they are unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, however, learn the task relationships in batch mode. To overcome this limitation, recent works propose the “lifelong learning” paradigm (Eaton and Ruvolo, 2013; Ruvolo and Eaton, 2014), in which all the inst</context>
</contexts>
<marker>Jacob, Vert, Bach, Vert, 2009</marker>
<rawString>Laurent Jacob, Jean-philippe Vert, Francis R Bach, and Jean-philippe Vert. 2009. Clustered Multi-Task Learning: A Convex Formulation. In D Koller, D Schuurmans, Y Bengio, and L Bottou, editors, Advances in Neural Information Processing Systems 21, pages 745–752. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL 2007 Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zenz, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="19544" citStr="Koehn et al., 2007" startWordPosition="3251" endWordPosition="3254">dataset is distributed in the Trace corpus4 and includes, as source sentences, the subtitles of several talks spanning a range of topics presented in the TED conferences. Translations were generated by two different MT systems: a phrase-based statistical MT system and a commercial rule-based system. Post-editions were collected from four different translators, as described by Wisniewski et al. (2013). The IT manuals data come from two language service providers, henceforth LSP1 and LSP2. The ITLSP1 tuples belong to a software manual translated by an SMT system trained using the Moses toolkit (Koehn et al., 2007). The posteditions were produced by one professional trans4http://anrtrace.limsi.fr/trace_ postedit.tar.bz2 Domain No. Vocab. Avg. Snt. tokens Size Length TED src 20,048 3,452 20 TED tgt 21,565 3,940 22 ITLSP1 src 12,791 2,013 13 ITLSP1 tgt 13,626 2,321 13 EM src 15,327 3,200 15 EM tgt 17,857 3,149 17 ITLSP2 src 15,128 2,105 13 ITLSP2 tgt 17,109 2,104 14 Table 1: Data statistics for each domain. lator. The ITLSP2 data includes a software manual from the automotive industry; its source sentences are translated with an adaptive proprietary MT system and post-edited by several professional transl</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zenz, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007 Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Match without a Referee: Evaluating MT Adequacy without Reference Translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Machine Translation Workshop (WMT2012),</booktitle>
<pages>171--180</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2572" citStr="Mehdad et al., 2012" startWordPosition="393" endWordPosition="396">y useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: 1. The QE module is exposed to a continuous stream of data. The amount of such data and the tight schedule of multiple, simultaneous translation jobs prevents from (theoretically feasible but impractical) complete re-training procedures in a batch fashion and advocate for continuous learning methods. 2. The input data</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee: Evaluating MT Adequacy without Reference Translations. In Proceedings of the Machine Translation Workshop (WMT2012), pages 171–180, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Parrella</author>
</authors>
<title>Online support vector regression.</title>
<date>2007</date>
<tech>Master’s Thesis,</tech>
<institution>Department of Information Science, University of Genoa, Italy.</institution>
<contexts>
<context position="12728" citStr="Parrella, 2007" startWordPosition="2009" endWordPosition="2010">ction matrix” that defines to which extent each encoded task can “borrow” and “lend” knowledge from and to the other tasks. Opposite to previous methods (Cavallanti et al., 2010) that assume fixed dependencies among tasks, we propose to learn the interaction matrix instanceby-instance from the data. To this aim we follow the recent work of Saha et al. (2011), extending it to a regression setting. The choice of PA is motivated by practical reasons. Indeed, by providing the best trade-off between accuracy and computational time (He and Wang, 2012) compared to other algorithms such as OnlineSVR (Parrella, 2007), it represents a good solution to meet the demand of efficiency posed by the CAT framework. 3.1 Passive Aggressive Algorithm PA follows the typical online learning protocol. At each round t the learner receives an instance, xt E Rd (d is the number of features), and predicts the label ˆyt according to a function parametrized by a set weights wt E Rd. Next, the learner receives the true label yt, computes the E-insensitive loss, `E, measuring the deviation between the prediction ˆyt and the true label yt and updates the weights. The weights are updated by solving the optimization problem: CPA(</context>
</contexts>
<marker>Parrella, 2007</marker>
<rawString>Francesco Parrella. 2007. Online support vector regression. Master’s Thesis, Department of Information Science, University of Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Jos´e G C de Souza</author>
<author>Lucia Specia</author>
</authors>
<title>Topic Models for Translation Quality Estimation for Gisting Purposes.</title>
<date>2013</date>
<booktitle>In Machine Translation Summit XIV,</booktitle>
<pages>295--302</pages>
<marker>Rubino, de Souza, Specia, 2013</marker>
<rawString>Raphael Rubino, Jos´e G. C. de Souza, and Lucia Specia. 2013. Topic Models for Translation Quality Estimation for Gisting Purposes. In Machine Translation Summit XIV, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ruvolo</author>
<author>Eric Eaton</author>
</authors>
<title>Online Multi-Task Learning via Sparse Dictionary Optimization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI-14),</booktitle>
<location>Qu´ebec City, Qu´ebec, Canada,</location>
<contexts>
<context position="11389" citStr="Ruvolo and Eaton, 2014" startWordPosition="1780" endWordPosition="1783"> correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, however, learn the task relationships in batch mode. To overcome this limitation, recent works propose the “lifelong learning” paradigm (Eaton and Ruvolo, 2013; Ruvolo and Eaton, 2014), in which all the instances of a task are given to the learner sequentially and the previously learned tasks are leveraged to improve generalization for future tasks. This approach, however, is not applicable to our scenario as it assumes that all the instances of each task are processed as separate blocks. In this paper we propose a novel MTL algorithm for QE that learns the structure shared by different tasks in an online fashion and from an input stream of instances from all the tasks. To this aim, we extend the online passive aggressive (PA) algorithm (Crammer et al., 2006) to the multita</context>
</contexts>
<marker>Ruvolo, Eaton, 2014</marker>
<rawString>Paul Ruvolo and Eric Eaton. 2014. Online Multi-Task Learning via Sparse Dictionary Optimization. In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI-14), Qu´ebec City, Qu´ebec, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avishek Saha</author>
<author>Piyush Rai</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Online Learning of Multiple Tasks and their Relationships.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<location>Fort Lauderdale, FL, USA,</location>
<marker>Saha, Rai, Daum´e, Venkatasubramanian, 2011</marker>
<rawString>Avishek Saha, Piyush Rai, Hal Daum´e, and Suresh Venkatasubramanian. 2011. Online Learning of Multiple Tasks and their Relationships. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), Fort Lauderdale, FL, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Marco Turchi</author>
<author>Lucia Specia</author>
</authors>
<title>An Efficient and User-friendly Tool for Machine Translation Quality Estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation,</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="22089" citStr="Shah et al., 2014" startWordPosition="3651" endWordPosition="3654"> speech transcriptions and non-technical writing. Overall, these numbers suggest a possible dissimilar5https://autodesk.app.box.com/ Autodesk-PostEditing 6http://sourceforge.net/projects/ tercpp/ 223 Figure 1: Validation curves for the R parameter. ity between ITLSP1 and ITLSP2 and the other two domains, which might make knowledge transfer across them more difficult and QE model reactivity to domain changes particularly important. Features. Our models are trained using the 17 baseline features proposed in (Specia et al., 2009), extracted with the online version of the QuEst feature extractor (Shah et al., 2014). These features take into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the translation (e.g. language model probabilities). Their description is available in (Callison-Burch et al., 2012). The results of previous WMT QE shared tasks have shown that these features are particularly competitive in the HTER prediction task. Baselines. We compare the performance of PAMTL against three baselines: i) pooling mean, ii) pooling online single task learning (STLpool) and iii) in-domain online single task learning (STLz,,</context>
</contexts>
<marker>Shah, Turchi, Specia, 2014</marker>
<rawString>Kashif Shah, Marco Turchi, and Lucia Specia. 2014. An Efficient and User-friendly Tool for Machine Translation Quality Estimation. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Association for Machine Translation in the Americas,</booktitle>
<location>Cambridge, MA, USA,</location>
<contexts>
<context position="4639" citStr="Snover et al., 2006" startWordPosition="717" endWordPosition="720">le unifying framework based on the combination of two paradigms: online and multitask learning. The former provides continuous learning capabilities that allow the QE model to be robust and self-adapt to a stream of potentially diverse data. The latter provides the model with the capability to exploit the similarities between data coming from different sources. Along this direction our contributions are: • The first application of online multitask learning to QE, geared to the challenges posed by CAT technology. In this framework, our models are trained to predict MT quality in terms of HTER (Snover et al., 2006).2 • The extension of current online multitask learning methods to regression. Prior works in the machine learning field applied this paradigm to classification problems, but its use for HTER estimation requires real-valued predictions. To this aim, we propose a new regression algorithm that, at the same time, handles positive and negative transfer and performs online weight updates. • A comparison between online multitask and alternative, state-of-the-art online learning strategies. Our experiments, carried out in a realistic scenario involving a stream of data from four domains, lead to cons</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas, Cambridge, MA, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>A Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, number July,</booktitle>
<pages>612--621</pages>
<contexts>
<context position="2531" citStr="Soricut and Echihabi, 2010" startWordPosition="385" endWordPosition="388">eCat tool (Federico et al., 2014). are extremely useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: 1. The QE module is exposed to a continuous stream of data. The amount of such data and the tight schedule of multiple, simultaneous translation jobs prevents from (theoretically feasible but impractical) complete re-training procedures in a batch fashion and advocate for conti</context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and A Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, number July, pages 612–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the Sentence-Level Quality of Machine Translation Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the EAMT,</booktitle>
<pages>28--35</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2503" citStr="Specia et al., 2009" startWordPosition="381" endWordPosition="384">e the open source MateCat tool (Federico et al., 2014). are extremely useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: 1. The QE module is exposed to a continuous stream of data. The amount of such data and the tight schedule of multiple, simultaneous translation jobs prevents from (theoretically feasible but impractical) complete re-training procedures in a batch fa</context>
<context position="22003" citStr="Specia et al., 2009" startWordPosition="3636" endWordPosition="3639">the topics that compose these domains and the expected stylistic differences featured by speech transcriptions and non-technical writing. Overall, these numbers suggest a possible dissimilar5https://autodesk.app.box.com/ Autodesk-PostEditing 6http://sourceforge.net/projects/ tercpp/ 223 Figure 1: Validation curves for the R parameter. ity between ITLSP1 and ITLSP2 and the other two domains, which might make knowledge transfer across them more difficult and QE model reactivity to domain changes particularly important. Features. Our models are trained using the 17 baseline features proposed in (Specia et al., 2009), extracted with the online version of the QuEst feature extractor (Shah et al., 2014). These features take into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the translation (e.g. language model probabilities). Their description is available in (Callison-Burch et al., 2012). The results of previous WMT QE shared tasks have shown that these features are particularly competitive in the HTER prediction task. Baselines. We compare the performance of PAMTL against three baselines: i) pooling mean, ii) pooling online</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the EAMT, pages 28–35, Barcelona, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Tsuda</author>
<author>Gunnar R¨atsch</author>
<author>Manfred K Warmuth</author>
</authors>
<title>Matrix exponentiated gradient updates for online learning and bregman projection.</title>
<date>2005</date>
<journal>In Journal of Machine Learning Research,</journal>
<pages>995--1018</pages>
<marker>Tsuda, R¨atsch, Warmuth, 2005</marker>
<rawString>Koji Tsuda, Gunnar R¨atsch, and Manfred K Warmuth. 2005. Matrix exponentiated gradient updates for online learning and bregman projection. In Journal of Machine Learning Research, pages 995–1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Coping with the Subjectivity of Human Judgements in MT Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>240--251</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6797" citStr="Turchi et al., 2013" startWordPosition="1048" endWordPosition="1051">ng effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values, and post-editing time (e.g. seconds per word). First, the data used are substantially homogeneous (usually they come from the same domain, and target translations are produced by the same MT system). Second, training and test are carried out as distinct, sequential phases. Instead, in the CAT environment, a QE component should ideally serve, adapt to and continuously learn from simultaneous translation jobs involving different MT engines, domains, genres and users (Turchi et al., 2013). These challenges have been separately addressed from different perspectives in few recent works. Huang et al. (2014) proposed a method to adaptively train a QE model for documentspecific MT post-editing. Adaptability, however, is achieved in a batch fashion, by re-training an ad hoc QE component for each document to be translated. The adaptive approach proposed by Turchi et al. (2014) overcomes the limitations of batch methods by applying an online learning protocol to continuously learn from a stream of (potentially heterogeneous) data. Experimental results suggest the effectiveness of onli</context>
</contexts>
<marker>Turchi, Negri, Federico, 2013</marker>
<rawString>Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the Subjectivity of Human Judgements in MT Quality Estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT), pages 240–251, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Antonios Anastasopoulos</author>
<author>Jos´e G C de Souza</author>
<author>Matteo Negri</author>
</authors>
<title>Adaptive Quality Estimation for Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>710--720</pages>
<location>Baltimore, Maryland, USA,</location>
<marker>Turchi, Anastasopoulos, de Souza, Negri, 2014</marker>
<rawString>Marco Turchi, Antonios Anastasopoulos, Jos´e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estimation for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 710–720, Baltimore, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
<author>Anil Kumar Singh</author>
<author>Natalia Segal</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Design and Analysis of a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-Edition.</title>
<date>2013</date>
<booktitle>In Machine Translation Summit XIV,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="19328" citStr="Wisniewski et al. (2013)" startWordPosition="3215" endWordPosition="3218">om Technology Entertainment Design talks (TED), Information Technology manuals (IT) and Education Material (EM). All datasets provide a set of tuples composed by (source, translation and post-edited translation). The TED dataset is distributed in the Trace corpus4 and includes, as source sentences, the subtitles of several talks spanning a range of topics presented in the TED conferences. Translations were generated by two different MT systems: a phrase-based statistical MT system and a commercial rule-based system. Post-editions were collected from four different translators, as described by Wisniewski et al. (2013). The IT manuals data come from two language service providers, henceforth LSP1 and LSP2. The ITLSP1 tuples belong to a software manual translated by an SMT system trained using the Moses toolkit (Koehn et al., 2007). The posteditions were produced by one professional trans4http://anrtrace.limsi.fr/trace_ postedit.tar.bz2 Domain No. Vocab. Avg. Snt. tokens Size Length TED src 20,048 3,452 20 TED tgt 21,565 3,940 22 ITLSP1 src 12,791 2,013 13 ITLSP1 tgt 13,626 2,321 13 EM src 15,327 3,200 15 EM tgt 17,857 3,149 17 ITLSP2 src 15,128 2,105 13 ITLSP2 tgt 17,109 2,104 14 Table 1: Data statistics fo</context>
</contexts>
<marker>Wisniewski, Singh, Segal, Yvon, 2013</marker>
<rawString>Guillaume Wisniewski, Anil Kumar Singh, Natalia Segal, and Franc¸ois Yvon. 2013. Design and Analysis of a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-Edition. In Machine Translation Summit XIV, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Yan</author>
<author>Elisa Ricci</author>
<author>Ramanathan Subramanian</author>
<author>Gaowen Liu</author>
<author>Nicu Sebe</author>
</authors>
<title>Multitask linear discriminant analysis for view invariant action recognition.</title>
<date>2014</date>
<journal>IEEE Transactions on Image Processing,</journal>
<volume>23</volume>
<issue>12</issue>
<contexts>
<context position="10853" citStr="Yan et al., 2014" startWordPosition="1699" endWordPosition="1702">instance, label) pairs obtained from source sentences coming from different translation jobs, together with their translations produced by several MT systems and the relative post-editions from various translators. In this paper the terms task and domain are used interchangeably. Early MTL methods model only positive correlation (Caruana, 1997; Argyriou et al., 2008), which results in a positive knowledge transfer between all the tasks, with the risk of impairing each other’s performance when they are unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, however, learn the task relationships in batch mode. To overcome this limitation, recent works propose the “lifelong learning” paradigm (Eaton and Ruvolo, 2013; Ruvolo and Eaton, 2014), in which all the instances of a task are given to the learner </context>
</contexts>
<marker>Yan, Ricci, Subramanian, Liu, Sebe, 2014</marker>
<rawString>Yan Yan, Elisa Ricci, Ramanathan Subramanian, Gaowen Liu, and Nicu Sebe. 2014. Multitask linear discriminant analysis for view invariant action recognition. IEEE Transactions on Image Processing, 23(12):5599–5611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Zhang</author>
<author>Dit-yan Yeung</author>
</authors>
<title>A Convex Formulation for Learning Task Relationships in Multi-Task Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10),</booktitle>
<pages>733--742</pages>
<location>Catalina Island, CA, USA,</location>
<contexts>
<context position="9815" citStr="Zhang and Yeung, 2010" startWordPosition="1537" endWordPosition="1540">and computes a loss that indicates the distance between the predicted and the true label. Before going to the next step, the weights are updated according to the suffered loss. Multitask learning (MTL) aims to simultaneously learn models for a set of possibly related tasks by exploiting their relationships. By doing this, improved generalization capabilities are obtained over models trained on the different tasks in isolation (single-task learning – STL). The relationships among tasks are provided by a shared structure, which can encode three types of relationships based on their correlation (Zhang and Yeung, 2010). Positive correlation indicates that the tasks are related and knowledge transfer should lead to similar model parameters. Negative correlation indicates that the tasks are likely to be unrelated and knowledge transfer should force an increase in the distance between model parameters. No correlation indicates that the tasks are independent and no knowledge transfer should take place. In our case, a task is a set of (instance, label) pairs obtained from source sentences coming from different translation jobs, together with their translations produced by several MT systems and the relative post</context>
<context position="11184" citStr="Zhang and Yeung, 2010" startWordPosition="1749" endWordPosition="1752">tion (Caruana, 1997; Argyriou et al., 2008), which results in a positive knowledge transfer between all the tasks, with the risk of impairing each other’s performance when they are unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, however, learn the task relationships in batch mode. To overcome this limitation, recent works propose the “lifelong learning” paradigm (Eaton and Ruvolo, 2013; Ruvolo and Eaton, 2014), in which all the instances of a task are given to the learner sequentially and the previously learned tasks are leveraged to improve generalization for future tasks. This approach, however, is not applicable to our scenario as it assumes that all the instances of each task are processed as separate blocks. In this paper we propose a novel MTL algorithm for QE that learns the structure share</context>
</contexts>
<marker>Zhang, Yeung, 2010</marker>
<rawString>Yu Zhang and Dit-yan Yeung. 2010. A Convex Formulation for Learning Task Relationships in Multi-Task Learning. In Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10), pages 733–742, Catalina Island, CA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Wenliang Zhong</author>
<author>James T Kwok</author>
</authors>
<title>Convex multitask learning with flexible task clusters.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29 th International Conference on Machine Learning,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="10834" citStr="Zhong and Kwok, 2012" startWordPosition="1695" endWordPosition="1698">, a task is a set of (instance, label) pairs obtained from source sentences coming from different translation jobs, together with their translations produced by several MT systems and the relative post-editions from various translators. In this paper the terms task and domain are used interchangeably. Early MTL methods model only positive correlation (Caruana, 1997; Argyriou et al., 2008), which results in a positive knowledge transfer between all the tasks, with the risk of impairing each other’s performance when they are unrelated or negatively correlated. Other methods (Jacob et al., 2009; Zhong and Kwok, 2012; Yan et al., 2014) cluster tasks into different groups and share knowledge only among those in the same cluster, thus implicitly identifying outlier tasks. A third class of algorithms considers all the three types of relationships by learning task interaction via the covariance of task-specific weights (Bonilla et al., 2008; Zhang and Yeung, 2010). All these methods, however, learn the task relationships in batch mode. To overcome this limitation, recent works propose the “lifelong learning” paradigm (Eaton and Ruvolo, 2013; Ruvolo and Eaton, 2014), in which all the instances of a task are gi</context>
</contexts>
<marker>Zhong, Kwok, 2012</marker>
<rawString>Leon Wenliang Zhong and James T. Kwok. 2012. Convex multitask learning with flexible task clusters. In Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>