<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.939312">
Deep Syntax Language Models and Statistical Machine Translation
</title>
<author confidence="0.569421">
Yvette Graham Josef van Genabith
</author>
<affiliation confidence="0.7146335">
NCLT CNGL
Dublin City University Dublin City University
</affiliation>
<email confidence="0.995643">
ygraham@computing.dcu.ie josef@computing.dcu.ie
</email>
<sectionHeader confidence="0.993802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99965962962963">
Hierarchical Models increase the re-
ordering capabilities of MT systems
by introducing non-terminal symbols to
phrases that map source language (SL)
words/phrases to the correct position
in the target language (TL) translation.
Building translations via discontiguous
TL phrases increases the difficulty of lan-
guage modeling, however, introducing the
need for heuristic techniques such as cube
pruning (Chiang, 2005), for example.
An additional possibility to aid language
modeling in hierarchical systems is to use
a language model that models fluency of
words not using their local context in the
string, as in traditional language models,
but instead using the deeper context of
a word. In this paper, we explore the
potential of deep syntax language mod-
els providing an interesting comparison
with the traditional string-based language
model. We include an experimental evalu-
ation that compares the two kinds of mod-
els independently of any MT system to in-
vestigate the possible potential of integrat-
ing a deep syntax language model into Hi-
erarchical SMT systems.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941609756097">
In Phrase-Based Models of Machine Translation
all phrases consistent with the word alignment
are extracted (Koehn et al., 2003), with shorter
phrases needed for high coverage of unseen data
and longer phrases providing improved fluency in
target language translations. Hierarchical Mod-
els (Chiang, 2007; Chiang, 2005) build on Phrase-
Based Models by relaxing the constraint that
phrases must be contiguous sequences of words
and allow a short phrase (or phrases) nested within
a longer phrase to be replaced by a non-terminal
symbol forming a new hierarchical phrase. Tra-
ditional language models use the local context of
words to estimate the probability of the sentence
and introducing hierarchical phrases that generate
discontiguous sequences of TL words increases
the difficulty of computing language model proba-
bilities during decoding and require sophisticated
heuristic language modeling techniques (Chiang,
2007; Chiang, 2005).
Leaving aside heuristic language modeling for
a moment, the difficulty of integrating a tradi-
tional string-based language model into the de-
coding process in a hierarchical system, highlights
a slight incongruity between the translation model
and language model in Hierarchical Models. Ac-
cording to the translation model, the best way to
build a fluent TL translation is via discontiguous
phrases, while the language model can only pro-
vide information about the fluency of contiguous
sequences of words. Intuitively, a language model
that models fluency between discontiguous words
may be well-suited to hierarchical models. Deep
syntax language models condition the probability
of a word on its deep context, i.e. words linked to
it via dependency relations, as opposed to preced-
ing words in the string. During decoding in Hi-
erarchical Models, words missing a context in the
string due to being preceded by a non-terminal,
might however be in a dependency relation with
a word that is already present in the string and
</bodyText>
<page confidence="0.957276">
118
</page>
<note confidence="0.982039">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 118–126,
COLING 2010, Beijing, August 2010.
</note>
<bodyText confidence="0.999601833333333">
this context could add useful information about
the fluency of the hypothesis as its constructed.
In addition, using the deep context of a word
provides a deeper notion of fluency than the lo-
cal context provides on its own and this might be
useful to improve such things as lexical choice in
SMT systems. Good lexical choice is very im-
portant and the deeper context of a word, if avail-
able, may provide more meaningful information
and result in better lexical choice. Integrating
such a model into a Hierarchical SMT system is
not straightforward, however, and we believe be-
fore embarking on this its worthwhile to evalu-
ate the model independently of any MT system.
We therefore provide an experimental evaluation
of the model and in order to provide an interesting
comparison, we evaluate a traditional string-based
language model on the same data.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99956">
The idea of using a language model based on deep
syntax is not new to SMT. Shen et al. (2008) use
a dependency-based language model in a string
to dependency tree SMT system for Chinese-
English translation, using information from the
deeper structure about dependency relations be-
tween words, in addition to the position of the
words in the string, including information about
whether context words were positioned on the left
or right of a word. Bojar and Hajiˇc (2008) use a
deep syntax language model in an English-Czech
dependency tree-to-tree transfer system, and in-
clude three separate bigram language models: a
reverse, direct and joint model. The model in our
evaluation is similar to their direct bigram model,
but is not restricted to bigrams.
Riezler and Maxwell (2006) use a trigram deep
syntax language model in German-English depen-
dency tree-to-tree transfer to re-rank decoder out-
put. The language model of Riezler and Maxwell
(2006) is similar to the model in our evaluation,
but differs in that it is restricted to a trigram model
trained on LFG f-structures. In addition, as lan-
guage modeling is not the main focus of their
work, they provide little detail on the language
model they use, except to say that it is based on
“log-probability of strings ofpredicates from root
to frontier of target f-structure, estimated from
predicate trigrams in English f-structures” (Rie-
zler and Maxwell, 2006). An important prop-
erty of LFG f-structures (and deep syntactic struc-
tures in general) was possibly overlooked here.
F-structures can contain more than one path of
predicates from the root to a frontier that in-
clude the same ngram, and this occurs when the
underlying graph includes unary branching fol-
lowed by branching with arity greater than one.
In such cases, the language model probability as
described in Riezler and Maxwell (2006) is incor-
rect as the probability of these ngrams will be in-
cluded multiple times. In our definition of a deep
syntax language model, we ensure that such du-
plicate ngrams are omitted in training and testing.
In addition, Wu (1998) use a bigram deep syntax
language model in a stochastic inversion transduc-
tion grammar for English to Chinese. None of the
related research we discuss here has included an
evaluation of the deep syntax language model they
employ in isolation from the MT system, however.
</bodyText>
<sectionHeader confidence="0.985336" genericHeader="method">
3 Deep Syntax
</sectionHeader>
<bodyText confidence="0.973817458333333">
The deep syntax language model we describe is
not restricted to any individual theory of deep
syntax. For clarity, however, we restrict our ex-
amples to LFG, which is also the deep syntax
theory we use for our evaluation. The Lexical
Functional Grammar (LFG) (Kaplan and Bres-
nan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrym-
ple, 2001) functional structure (f-structure) is an
attribute-value encoding of bi-lexical labeled de-
pendencies, such as subject, object and adjunct
for example, with morpho-syntactic atomic at-
tributes encoding information such as mood and
tense of verbs, and person, number and case for
nouns. Figure 1 shows the LFG f-structure for En-
glish sentence “Today congress passed Obama’s
health care bill.”1
Encoded within the f-structure is a directed
graph and our language model uses a simplified
acyclic unlabeled version of this graph. Figure
1(b) shows the graph structure encoded within the
f-structure of Figure 1(a). We discuss the simpli-
fication procedure later in Section 5.
1Morpho-syntactic information/ atomic features are omit-
ted from the diagram.
</bodyText>
<page confidence="0.994438">
119
</page>
<figure confidence="0.999916608695652">
(a) �PRED pass
� SUBJ [PRED congress]
� �
�PRED brill
�
� �
�
OBJ
P
ADJ [
RED today]
SPEC [FOSS
[PRED Obama]]
PRED care
MOD MOD [PRED health]
1
1
(b) &lt;s&gt;
pass
today congress bill
&lt;/s&gt; &lt;/s&gt; obama care
&lt;/s&gt; health
&lt;/s&gt;
</figure>
<figureCaption confidence="0.999948">
Figure 1: “Today congress passed Obama’s health care bill.”
</figureCaption>
<bodyText confidence="0.9902646">
word in the structure. For example, a trigram deep
guage model conditions the probability
of each word on the sequence of words consisting
of the head of the head of the word followed by
the head of the word as follows:
</bodyText>
<equation confidence="0.980242">
de) = �l P(wi|wm(m(i)), wm(i)) (3)
i=1
quence of words from
to
w1
wi−1, as shown in
Equation 1.
p(w1, w2, ..., wl) = p(wi|w1,...,wi−1)(1)
</equation>
<bodyText confidence="0.9963992">
structure, the sequence of words fr
om wr, the
head of the sentence, to wm(i), as shown in Equa-
tion 2, with function m used to map the index of a
word in the structure to the index of its head. 2
</bodyText>
<equation confidence="0.999704">
de) = �l p(wi|wr, ..., wm(m(i))wm(i)) (2)
i=1
</equation>
<bodyText confidence="0.9645335">
syntax lan
deep syntax lan
guage model probability is com-
puted for the example sentence in Figure 1(a).
when estimating the probability of each
refer to the lexicalized nodes in the dependency
structure as words, alternatively the term predicate can
2We
</bodyText>
<equation confidence="0.5437395">
be
used.
</equation>
<sectionHeader confidence="0.995624" genericHeader="method">
4 Language Model
</sectionHeader>
<bodyText confidence="0.999964428571429">
We use a simplified approximation of the deep
syntactic structure, de, that encodes the unlabeled
dependencies between the words of the sentence,
to estimate a deep syntax language model prob-
ability. Traditional string-based language mod-
els combine the probability of each word in the
sentence, wi, given its preceding context, the se-
</bodyText>
<equation confidence="0.833119">
�l
i=1
</equation>
<bodyText confidence="0.955454529411765">
In a similar way, a deep syntax language model
probability combines the probability of each word
in the structure, wi, given its context within the
p(
In order to combat data sparseness, we apply
the Markov assumption, as is done in traditional
string-based language modeling, and simplify the
probability by only including a limited length of
history
p(
In addition, similar to string-based language
modeling, we add a start symbol, &lt;s&gt;, at the
root of the structure and end symbols, &lt;/s&gt;, at
the leaves to include the probability of a word be-
ing the head of the sentence and the probability
of words occurring as leaf nodes in the structure.
Figure 2(a) shows an example of how a trigram
</bodyText>
<sectionHeader confidence="0.941375" genericHeader="method">
5 Simplified Approximation of the Deep
</sectionHeader>
<subsectionHeader confidence="0.734458">
Syntactic Representation
</subsectionHeader>
<bodyText confidence="0.999084">
We describe the deep syntactic structure, de, as
an approximation since a parser is employed to
automatically produce it and there is therefore no
certainty that we use the actual/correct deep syn-
tactic representation for the sentence. In addi-
tion, the function m requires that each node in the
structure has exactly one head, however, structure-
sharing can occur within deep syntactic structures
resulting in a single word legitimately having two
heads. In such cases we use a simplification of
the graph in the deep syntactic structure. Fig-
ure 3shows an
f-structure in which the subject
</bodyText>
<page confidence="0.901251">
120
</page>
<figure confidence="0.841683615384615">
(a) Deep Syntax LM (b) Traditional LM
p(e) ≈ p( pass  |&lt;s&gt;)∗ p(e) ≈ p( passed  |today congress )∗
p( today  |&lt;s&gt; pass )∗ p( today  |&lt;s&gt;)∗
p(&lt;/s&gt;  |pass today )∗ p( congress  |&lt;s&gt; today )∗
p( congress  |&lt;s&gt; pass )∗
p(&lt;/s&gt;  |pass congress )∗ p( bill  |health care )∗
p( bill  |&lt;s&gt; pass )∗ p( obama  |congress passed )∗
p( obama  |pass bill )∗ p( care  |s health )∗
p(&lt;/s&gt;  |bill obama )∗ p( health  |’ s )∗
p( care  |pass bill )∗ p( ’  |passed Obama )∗
p( health  |bill care )∗ p( s  |obama ’ )∗
p(&lt;/s&gt;  |care health ) p( .  |care bill )∗
p(&lt;/s&gt;  |bill . )
</figure>
<figureCaption confidence="0.999854">
Figure 2: Example Comparison of Deep Syntax and Traditional Language Models
</figureCaption>
<bodyText confidence="0.999912375">
of both like, be and president is hillary. In our
simplified structure, the dependency relations be-
tween be and hillary and president and hillary are
dropped. We discuss how we do this later in Sec-
tion 6. Similar to our simplification for structure
sharing, we also simplify structures that contain
cycles by discarding edges that cause loops in the
structure.
</bodyText>
<sectionHeader confidence="0.99675" genericHeader="method">
6 Implementation
</sectionHeader>
<bodyText confidence="0.999908">
SRILM (Stolcke, 2002) can be used to compute
a language model from ngram counts (the -read
option of the ngram-count command). Implemen-
tation to train the language model, therefore, sim-
ply requires accurately extracting counts from the
deep syntax parsed training corpus. To simplify
the structures to acyclic graphs, nodes are labeled
with an increasing index number via a depth first
traversal. This allows each arc causing a loop in
the graph or argument sharing to be identified by
a simple comparison of index numbers, as the in-
dex number of its start node will be greater than
that of its end node. The algorithm we use to
extract ngrams from the dependency structures is
straightforward: we simply carry out a depth-first
traversal of the graph to construct paths of words
that stretch from the root of the graph to words
</bodyText>
<equation confidence="0.831869777777778">
PRED like
 1:�PRED Hillary~
 SUBJ
 
PRED be
  
 SUBJ 1
�PRED president~
 
 
  XCOMP-PRED SUBJ 1
   XCOMP 

  PRED at
  #
&amp;quot; 
  
  ADJ  PRED U.N. 
</equation>
<figure confidence="0.95841925">
OBJ SPEC�PRED the~
&lt;s&gt;
like
hillary be
&lt;/s&gt; president at
&lt;/s&gt; U.N.
the
&lt;/s&gt;
</figure>
<figureCaption confidence="0.6404955">
Figure 3: “Hillary liked being president at the
U.N.”
</figureCaption>
<figure confidence="0.997728071428571">

 
   










 
</figure>
<page confidence="0.712632">
121
</page>
<equation confidence="0.836531142857143">
�
� �
� � �
� �
� �
� � �
� � �
� �� � �
� � �
� � �
�� � � �
�PRED agree
� SUBJ [PRED nobody
� � �
�PRED with
� �
� �PRED point
� �
�XCOMP COORD and
OBJ ADJ [PRED two�,
I [PRED three]}
</equation>
<figureCaption confidence="0.980161">
Figure 4: “Nobody agreed with points two and
three.”
</figureCaption>
<bodyText confidence="0.99984052631579">
at the leaves and then extract the required order
ngrams from each path. As mentioned earlier,
some ngrams can belong to more than one path.
Figure 4 shows an example structure containing
unary branching followed by binary branching in
which the sequence of symbols and words “&lt;s&gt;
agree with point and” belong to the path ending
in two &lt;/s&gt; and three &lt;/s&gt;. In order to ensure
that only distinct ngrams are extracted we assign
each word in the structure a unique id number
and include this in the extracted ngrams. Paths
are split into ngrams and duplicate ngrams result-
ing from their occurrence in more than one path
are discarded. Its also possible for ngrams to le-
gitimately be repeated in a deep structure, and in
such cases we do not discard these ngrams. Legit-
imately repeating ngrams are easily identified as
the id numbers attached to words will be differ-
ent.
</bodyText>
<sectionHeader confidence="0.9930315" genericHeader="method">
7 Deep Syntax and Lexical Choice in
SMT
</sectionHeader>
<bodyText confidence="0.99586048">
Correct lexical choice in machine translation is
extremely important and PB-SMT systems rely
on the language model to ensure, that when two
phrases are combined with each other, that the
model can rank combined phrases that are flu-
ent higher than less fluent combinations. Con-
ditioning the probability of each word on its
deep context has the potential to provide a
more meaningful context than the local context
within the string. A comparison of the proba-
bilities of individual words in the deep syntax
model and traditional language model in Figure
2 clearly shows this. For instance, let us con-
sider how the language model in a German to
English SMT system is used to help rank the
following two translations today congress passed
... and today convention passed ... (the word
Kongress in German can be translated into ei-
ther congress or convention in English). In
the deep syntax model, the important compet-
ing probabilities are (i) p(congressj&lt;s&gt;pass)
and (ii) p(conventionj&lt;s&gt;pass), where (i)
can be interpreted as the probability of the
word congress modifying pass when pass is
the head of the entire sentence and, simi-
larly (ii) the probability of the word conven-
tion modifying pass when pass is the head of
the entire sentence. In the traditional string-
based language model, the equivalent compet-
ing probabilities are (i) p(congressj&lt;s&gt;today),
the probability of congress following today when
today is the start of the sentence and (ii)
p(conventionj&lt;s&gt;today), probability of con-
vention following today when today is the start
of the sentence, showing that the deep syntax
language model is able to use more meaningful
context for good lexical choice when estimating
the probability of words congress and convention
compared to the traditional language model.
In addition, the deep syntax language model
will encounter less data sparseness problems for
some words than a string-based language model.
In many languages words occur that can legiti-
mately be moved to different positions within the
string without any change to dependencies be-
tween words. For example, sentential adverbs
in English, can legitimately change position in
a sentence, without affecting the underlying de-
pendencies between words. The word today in
“Today congress passed Obama’s health bill”
</bodyText>
<figure confidence="0.999695285714286">
&lt;s&gt;
agree
nobody with
&lt;/s&gt; point
and
two three
&lt;/s&gt; &lt;/s&gt;
</figure>
<page confidence="0.989024">
122
</page>
<bodyText confidence="0.999892381818182">
can appear as “Congress passed Obama’s health
bill today” and “Congress today passed Obama’s
health bill”. Any sentence in the training cor-
pus in which the word pass is modified by today
will result in a bigram being counted for the two
words, regardless of the position of today within
each sentence.
In addition, some surface form words such as
auxiliary verbs for example, are not represented
as predicates in the deep syntactic structure. For
lexical choice, its not really the choice of auxiliary
verbs that is most important, but rather the choice
of an appropriate lexical item for the main verb
(that belongs to the auxiliary verb). Omitting aux-
iliary verbs during language modeling could aid
good lexical choice, by focusing on the choice of
a main verb without the effect of what auxiliary
verb is used with it.
For some words, however, the probability in the
string-based language model provides as good if
not better context than the deep syntax model, but
only for the few words that happen to be preceded
by words that are important to its lexical choice,
and this reinforces the idea that SMT systems can
benefit from using both a deep syntax and string-
based language model. For example, the proba-
bility of bill in Figures 2(a) and 2(b) is computed
in the deep syntax model as p(bill |&lt;s&gt; pass)
and in the string-based model using p(bill|health
care), and for this word the local context seems to
provide more important information than the deep
context when it comes to lexical choice. The deep
model nevertheless adds some useful information,
as it includes the probability of bill being an argu-
ment ofpass when pass is the head of a sentence.
In traditional language modeling, the special
start symbol is added at the beginning of a sen-
tence so that the probability of the first word ap-
pearing as the first word of a sentence can be
included when estimating the probability. With
similar motivation, we add a start symbol to the
deep syntactic representation so that the probabil-
ity of the head of the sentence occurring as the
head of a sentence can be included. For exam-
ple, p(be |&lt;s&gt;) will have a high probability as
the verb be is the head of many sentences of En-
glish, whereas p(colorless |&lt;s&gt;) will have a low
probability since it is unlikely to occur as the head.
We also add end symbols at the leaf nodes in the
structure to include the probability of these words
appearing at that position in a structure. For in-
stance, a noun followed by its determiner such as
p(&lt;/s&gt; |attorney a) would have a high probabil-
ity compared to a conjunction followed by a verb
p(&lt;/s&gt; |and be).
</bodyText>
<sectionHeader confidence="0.988777" genericHeader="evaluation">
8 Evaluation
</sectionHeader>
<bodyText confidence="0.999995954545454">
We carry out an experimental evaluation to inves-
tigate the potential of the deep syntax language
model we describe in this paper independently of
any machine translation system. We train a 5-
gram deep syntax language model on 7M English
f-structures, and evaluate it by computing the per-
plexity and ngram coverage statistics on a held-
out test set of parsed fluent English sentences. In
order to provide an interesting comparison, we
also train a traditional string-based 5-gram lan-
guage model on the same training data and test
it on the same held-out test set of English sen-
tences. A deep syntax language model comes with
the obvious disadvantage that any data it is trained
on must be in-coverage of the parser, whereas a
string-based language model can be trained on any
available data of the appropriate language. Since
parser coverage is not the focus of our work, we
eliminate its effects from the evaluation by select-
ing the training and test data for both the string-
based and deep syntax language models on the ba-
sis that they are in fact in-coverage of the parser.
</bodyText>
<subsectionHeader confidence="0.947814">
8.1 Language Model Training
</subsectionHeader>
<bodyText confidence="0.999951333333334">
Our training data consists of English sentences
from the WMT09 monolingual training corpus
with sentence length range of 5-20 words that are
in coverage of the parsing resources (Kaplan et al.,
2004; Riezler et al., 2002) resulting in approxi-
mately 7M sentences. Preparation of training and
test data for the traditional language model con-
sisted of tokenization and lower casing. Parsing
was carried out with XLE (Kaplan et al., 2002)
and an English LFG grammar (Kaplan et al.,
2004; Riezler et al., 2002). The parser produces
a packed representation of all possible parses ac-
cording to the LFG grammar and we select only
the single best parse for language model training
by means of a disambiguation model (Kaplan et
</bodyText>
<page confidence="0.996006">
123
</page>
<table confidence="0.99532375">
Corpus Tokens Ave. Tokens Vocab
per Sent.
strings 138.6M 19 345K
LFG lemmas/predicates 118.4M 16 280K
</table>
<tableCaption confidence="0.998982">
Table 1: Language model statistics for string-based and deep syntax language models, statistics are for
</tableCaption>
<bodyText confidence="0.628865571428571">
string tokens and LFG lemmas for the same set of 7.29M English sentences
al., 2004; Riezler et al., 2002). Ngrams were auto-
matically extracted from the f-structures and low-
ercased. SRILM (Stolcke, 2002) was used to com-
pute both language models. Table 1 shows statis-
tics on the number of words and lemmas used to
train each model.
</bodyText>
<subsectionHeader confidence="0.997811">
8.2 Testing
</subsectionHeader>
<bodyText confidence="0.999993777777778">
The test set consisted of 789 sentences selected
from WMT09 additional development sets3 con-
taining English Europarl text and again was se-
lected on the basis of sentences being in-coverage
of the parsing resources. SRILM (Stolcke, 2002)
was used to compute test set perplexity and ngram
coverage statistics for each order model.
Since the deep syntax language model adds end
of sentence markers to leaf nodes in the structures,
the number of (so-called) end of sentence markers
in the test set for the deep syntax model is much
higher than in the string-based model. We there-
fore also compute statistics for each model when
end of sentence markers are omitted from training
and testing. 4 In addition, since the vast majority
of punctuation is not represented as predicates in
LFG f-structures, we also test the string-based lan-
guage model when punctuation has been removed.
</bodyText>
<subsectionHeader confidence="0.831">
8.3 Results
</subsectionHeader>
<bodyText confidence="0.999962">
Table 2 shows perplexity scores and ngram cover-
age statistics for each order and type of language
model. Note that perplexity scores for the string-
based and deep syntax language models are not
directly comparable because each model has a dif-
ferent vocabulary. Although both models train on
an identical set of sentences, the data is in a dif-
ferent format for each model, as the string-based
</bodyText>
<footnote confidence="0.91174525">
3test2006.en and test2007.en
4When we include end of sentence marker probabilities
we also include them for normalization, and omit them from
normalization when their probabilities are omitted.
</footnote>
<bodyText confidence="0.99975224">
model is trained on surface form tokens, whereas
the deep syntax model uses lemmas. Ngram cov-
erage statistics provide a better comparison.
Unigram coverage for all models is high with
all models achieving close to 100% coverage on
the held-out test set. Bigram coverage is high-
est for the deep syntax language model when eos
markers are included (94.71%) with next high-
est coverage achieved by the string-based model
that includes eos markers (93.09%). When eos
markers are omitted bigram coverage goes down
slightly to 92.44% for the deep syntax model and
to 92.83% for the string-based model, and when
punctuation is also omitted from the string-based
model, coverage goes down again to 91.57%.
Trigram coverage statistics for the test set main-
tain the same rank between models as in the bi-
gram coverage, from highest to lowest as follows:
DS+eos at 64.71%, SB+eos at 58.75%, SB-eos
at 56.89%, DS-eos at 53.67%, SB-eos-punc at
53.45%. For 4-gram and 5-gram coverage a sim-
ilar coverage ranking is seen, but with DS-eos
(4gram at 17.17%, 5gram at 3.59%) and SB-eos-
punc (4gram at 20.24%, 5gram at 5.76%) swap-
ping rank position.
</bodyText>
<subsectionHeader confidence="0.791069">
8.4 Discussion
</subsectionHeader>
<bodyText confidence="0.992211666666667">
Ngram coverage statistics for the DS-eos and
SB-eos-punc models provide the fairest com-
parison, with the deep syntax model achiev-
ing higher coverage than the string-based model
for bigrams (+0.87%) and trigrams (+0.22%),
marginally lower coverage coverage of unigrams
(-0.02%) and lower coverage of 4-grams (-3.07%)
and 5-grams (2.17%) compared to the string-
based model.
Perplexity scores for the deep syntax model
when eos symbols are included are low (79 for the
5gram model) and this is caused by eos markers
</bodyText>
<page confidence="0.992255">
124
</page>
<table confidence="0.999465">
1-gram 2-gram 3-gram 4-gram 5-gram
cov. ppl cov. ppl cov. ppl cov. ppl cov. ppl
SB-eos 99.61% 1045 92.83% 297 56.89% 251 23.32% 268 7.19% 279
SB-eos-punc 99.58% 1357 91.57% 382 53.45% 327 20.24% 348 5.76% 360
DS-eos 99.56% 1005 92.44% 422 53.67% 412 17.17% 446 3.59% 453
SB+eos 99.63% 900 93.09% 227 58.75% 194 25.48% 207 8.35% 215
DS+eos 99.70% 211 94.71% 77 64.71% 73 29.86% 78 8.75% 79
</table>
<tableCaption confidence="0.86427">
Table 2: Ngram coverage and perplexity (ppl) on held-out test set. Note: DS = deep syntax, SB string-
based, eos = end of sentence markers
</tableCaption>
<bodyText confidence="0.999669277777778">
in the test set in general being assigned relatively
high probabilities by the model, and since several
occur per sentence, the perplexity increases when
the are omitted (453 for the 5gram model).
Tables 3 and 4 show the most frequently en-
countered trigrams in the test data for each type
of model. A comparison shows how different the
two models are and highlights the potential of the
deep syntax language model to aid lexical choice
in SMT systems. Many of the most frequently oc-
curring trigram probabilities for the deep syntax
model are for arguments of the main verb of the
sentence, conditioned on the main verb, and in-
cluding such probabilities in a system could im-
prove fluency by using information about which
words are in a dependency relation together ex-
plicitely in the model. In addition, a frequent tri-
gram in the held-out data is &lt;s&gt; be also, where
the word also is a sentential adverb modifying
be. Trigrams for sentential adverbs are likely to
be less effected by data sparseness in the deep
syntax model compared to the string-based model
which could result in the deep syntax model im-
proving fluency with respect to combinations of
main verbs and their modifying adverbs. The most
frequent trigram in the deep syntax test set is &lt;s&gt;
and be, in which the head of the sentence is the
conjunction and with argument be. In this type of
syntactic construction in English, its often the case
that the conjunction and verb will be distant from
each other in the sentence, for example: Nobody
was there except the old lady and without thinking
we quickly left. (where was and and are in a de-
pendency relation). Using a deep syntax language
model could therefore improve lexical choice for
such words, since they are too distant for a string-
</bodyText>
<table confidence="0.964669714285714">
3-gram No.Occ. Prob.
&lt;s&gt; and be 42 0.1251
&lt;s&gt; be this 21 0.0110
&lt;s&gt; must we 19 0.0347
&lt;s&gt; would i 19 0.0414
&lt;s&gt; be in 17 0.0326
&lt;s&gt; be that 14 0.0122
be debate the 13 0.0947
&lt;s&gt; be debate 13 0.0003
&lt;s&gt; can not 12 0.0348
&lt;s&gt; and president 11 0.0002
&lt;s&gt; would like 11 0.0136
&lt;s&gt; would be 11 0.0835
&lt;s&gt; be also 10 0.0075
</table>
<tableCaption confidence="0.9881125">
Table 3: Most frequent trigrams in test set for deep
syntax model
</tableCaption>
<bodyText confidence="0.798103">
based model.
</bodyText>
<sectionHeader confidence="0.997759" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999995333333333">
We presented a comparison of a deep syntax
language and traditional string-based language
model. Results showed that the deep syntax lan-
guage model achieves similar ngram coverage to
the string-based model on a held out test set.
We highlighted the potential of integrating such
a model into SMT systems for improving lexical
choice by using a deeper context for probabilities
of words compared to a string-based model.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994112333333333">
Bojar, Ondˇrej, Jan Hajiˇc. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the third Work-
</reference>
<page confidence="0.994259">
125
</page>
<bodyText confidence="0.987391388888889">
3-gram No.Occ. Prob.
mr president , 40 0.5385
&lt;s&gt; this is 25 0.1877
by the european 20 0.0014
the european union 18 0.1096
&lt;s&gt; it is 16 0.1815
the european parliament 15 0.0252
would like to 15 0.4944
&lt;s&gt; i would 15 0.0250
&lt;s&gt; that is 14 0.1094
i would like 14 0.0335
and gentlemen , 13 0.1005
ladies and gentlemen 13 0.2834
&lt;s&gt; we must 12 0.0120
should like to 12 0.1304
i should like 11 0.0089
, ladies and 11 0.5944
, it is 10 0.1090
</bodyText>
<tableCaption confidence="0.7905725">
Table 4: Most frequent trigrams in test set for
string-based model
</tableCaption>
<reference confidence="0.999564615384616">
shop on Statistical Machine Translation, Columbus,
Ohio.
Bresnan, Joan. 2001. Lexical-Functional Syntax.,
Blackwell Oxford.
Chiang, David. 2007. Hierarchical Phrase-based
Models of Translation In Computational Linguis-
tics, No. 33:2.
Chiang, David. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263-270,
Ann Arbor, Michigan.
Dalrymple, Mary. 2001. Lexical Functional Gram-
mar, Academic Press, San Diego, CA; London.
Kaplan, Ronald, Stefan Riezler, Tracy H. King, John
T. Maxwell, Alexander Vasserman. 2004. Speed
and Accuracy in Shallow and Deep Stochastic Pars-
ing. In Proceedings of Human Language Tech-
nology Conference/North American Chapter of the
Association for Computational Linguistics Meeting,
Boston, MA.
Kaplan, Ronald M., Tracy H. King, John T. Maxwell.
2002. Adapting Existing Grammars: the XLE Ex-
perience. In Proceedings of the 19th International
Conference on Computational Linguistics (COL-
ING) 2002, Taipei, Taiwan.
Kaplan, Ronald M. 1995. The Formal Architecture of
Lexical Functional Grammar. In Formal Issues in
Lexical Functional Grammar, ed. Mary Dalrymple,
pages 7-28, CSLI Publications, Stanford, CA.
Kaplan, Ronald M., Joan Bresnan. 1982. Lexical
Functional Grammar, a Formal System for Gram-
matical Represenation. In J. Bresnan, editor, The
Mental Representation of Grammatical Relations,
173-281, MIT Press, Cambridge, MA.
Koehn, Philipp, Hieu Hoang. 2007. Factored Trans-
lation Models. Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, 868-876.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicoli Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
Annual Meeting of the Association for Computa-
tional Linguistics, demonstration session
Koehn, Philipp 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the tenth Machine Translation Summit.
Koehn, Philipp, Franz Josef Och, Daniel Marcu. 2003.
Statistical Phrase-based Translation. In Proceed-
ings of Human Language Technology and North
American Chapter of the Association for Computa-
tional Linguistics Conference, 48-54.
Riezler, Stefan, John T. Maxwell III. 2006. Grammat-
ical Machine Translation. In Proceedings of HLT-
ACL, pages 248-255, New York.
Riezler, Stefan, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, Mark Johnson.
2002. Parsing the Wall Street Journal using Lexical
Functional Grammar and Discriminitive Estimation
Techniques . (grammar version 2005) In Proceed-
ings of the 40th ACL, Philadelphia.
Shen, Libin, Jinxi Xu, Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. Proceedings of ACL-08: HLT, pages 577-
585.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, Denver, Colorado.
Dekai, Wu, Hongsing Wong. 1998. Machine Trans-
lation with a Stochastic Grammatical Channel. In
Proceedings of the 36th ACL and 17th COLING,
Montreal, Quebec.
</reference>
<page confidence="0.998524">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444696">
<title confidence="0.999139">Deep Syntax Language Models and Statistical Machine Translation</title>
<author confidence="0.999818">Yvette Graham Josef van_Genabith</author>
<affiliation confidence="0.997426">NCLT CNGL Dublin City University Dublin City University</affiliation>
<email confidence="0.595865">ygraham@computing.dcu.iejosef@computing.dcu.ie</email>
<abstract confidence="0.990825928571429">Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the third Workshop on Statistical Machine Translation,</booktitle>
<location>Columbus, Ohio.</location>
<marker>Bojar, Hajiˇc, 2008</marker>
<rawString>Bojar, Ondˇrej, Jan Hajiˇc. 2008. Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation. In Proceedings of the third Workshop on Statistical Machine Translation, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Syntax.,</title>
<date>2001</date>
<location>Blackwell Oxford.</location>
<contexts>
<context position="6932" citStr="Bresnan, 2001" startWordPosition="1100" endWordPosition="1101">tion, Wu (1998) use a bigram deep syntax language model in a stochastic inversion transduction grammar for English to Chinese. None of the related research we discuss here has included an evaluation of the deep syntax language model they employ in isolation from the MT system, however. 3 Deep Syntax The deep syntax language model we describe is not restricted to any individual theory of deep syntax. For clarity, however, we restrict our examples to LFG, which is also the deep syntax theory we use for our evaluation. The Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrymple, 2001) functional structure (f-structure) is an attribute-value encoding of bi-lexical labeled dependencies, such as subject, object and adjunct for example, with morpho-syntactic atomic attributes encoding information such as mood and tense of verbs, and person, number and case for nouns. Figure 1 shows the LFG f-structure for English sentence “Today congress passed Obama’s health care bill.”1 Encoded within the f-structure is a directed graph and our language model uses a simplified acyclic unlabeled version of this graph. Figure 1(b) shows the graph structure encoded within the </context>
</contexts>
<marker>Bresnan, 2001</marker>
<rawString>Bresnan, Joan. 2001. Lexical-Functional Syntax., Blackwell Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-based Models of Translation</title>
<date>2007</date>
<booktitle>In Computational Linguistics, No. 33:2.</booktitle>
<contexts>
<context position="1593" citStr="Chiang, 2007" startWordPosition="232" endWordPosition="233">n interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems. 1 Introduction In Phrase-Based Models of Machine Translation all phrases consistent with the word alignment are extracted (Koehn et al., 2003), with shorter phrases needed for high coverage of unseen data and longer phrases providing improved fluency in target language translations. Hierarchical Models (Chiang, 2007; Chiang, 2005) build on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase. Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discontiguous sequences of TL words increases the difficulty of computing language model probabilities during decoding and require sophisticated heuristic language modeling techniqu</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>Chiang, David. 2007. Hierarchical Phrase-based Models of Translation In Computational Linguistics, No. 33:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="625" citStr="Chiang, 2005" startWordPosition="80" endWordPosition="81">Language Models and Statistical Machine Translation Yvette Graham Josef van Genabith NCLT CNGL Dublin City University Dublin City University ygraham@computing.dcu.ie josef@computing.dcu.ie Abstract Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep sy</context>
<context position="2224" citStr="Chiang, 2005" startWordPosition="325" endWordPosition="326">ild on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase. Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discontiguous sequences of TL words increases the difficulty of computing language model probabilities during decoding and require sophisticated heuristic language modeling techniques (Chiang, 2007; Chiang, 2005). Leaving aside heuristic language modeling for a moment, the difficulty of integrating a traditional string-based language model into the decoding process in a hierarchical system, highlights a slight incongruity between the translation model and language model in Hierarchical Models. According to the translation model, the best way to build a fluent TL translation is via discontiguous phrases, while the language model can only provide information about the fluency of contiguous sequences of words. Intuitively, a language model that models fluency between discontiguous words may be well-suite</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263-270, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
</authors>
<title>Lexical Functional Grammar,</title>
<date>2001</date>
<publisher>Academic Press,</publisher>
<location>San Diego, CA; London.</location>
<contexts>
<context position="6950" citStr="Dalrymple, 2001" startWordPosition="1102" endWordPosition="1104"> use a bigram deep syntax language model in a stochastic inversion transduction grammar for English to Chinese. None of the related research we discuss here has included an evaluation of the deep syntax language model they employ in isolation from the MT system, however. 3 Deep Syntax The deep syntax language model we describe is not restricted to any individual theory of deep syntax. For clarity, however, we restrict our examples to LFG, which is also the deep syntax theory we use for our evaluation. The Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrymple, 2001) functional structure (f-structure) is an attribute-value encoding of bi-lexical labeled dependencies, such as subject, object and adjunct for example, with morpho-syntactic atomic attributes encoding information such as mood and tense of verbs, and person, number and case for nouns. Figure 1 shows the LFG f-structure for English sentence “Today congress passed Obama’s health care bill.”1 Encoded within the f-structure is a directed graph and our language model uses a simplified acyclic unlabeled version of this graph. Figure 1(b) shows the graph structure encoded within the f-structure of Fig</context>
</contexts>
<marker>Dalrymple, 2001</marker>
<rawString>Dalrymple, Mary. 2001. Lexical Functional Grammar, Academic Press, San Diego, CA; London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell</author>
<author>Alexander Vasserman</author>
</authors>
<title>Speed and Accuracy in Shallow and Deep Stochastic Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics Meeting,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="20113" citStr="Kaplan et al., 2004" startWordPosition="3415" endWordPosition="3418">st be in-coverage of the parser, whereas a string-based language model can be trained on any available data of the appropriate language. Since parser coverage is not the focus of our work, we eliminate its effects from the evaluation by selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens Ave. Tokens Vocab per Sent. strings 138.6M 19 345K LFG lemmas/</context>
</contexts>
<marker>Kaplan, Riezler, King, Maxwell, Vasserman, 2004</marker>
<rawString>Kaplan, Ronald, Stefan Riezler, Tracy H. King, John T. Maxwell, Alexander Vasserman. 2004. Speed and Accuracy in Shallow and Deep Stochastic Parsing. In Proceedings of Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics Meeting, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Tracy H King</author>
<author>John T Maxwell</author>
</authors>
<title>Adapting Existing Grammars: the XLE Experience.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING)</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="20349" citStr="Kaplan et al., 2002" startWordPosition="3453" endWordPosition="3456">y selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens Ave. Tokens Vocab per Sent. strings 138.6M 19 345K LFG lemmas/predicates 118.4M 16 280K Table 1: Language model statistics for string-based and deep syntax language models, statistics are for string tokens and LFG lemmas for the same set of 7.29M English sentences al., 2004; Riezler et al., 2002).</context>
</contexts>
<marker>Kaplan, King, Maxwell, 2002</marker>
<rawString>Kaplan, Ronald M., Tracy H. King, John T. Maxwell. 2002. Adapting Existing Grammars: the XLE Experience. In Proceedings of the 19th International Conference on Computational Linguistics (COLING) 2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
</authors>
<title>The Formal Architecture of Lexical Functional Grammar.</title>
<date>1995</date>
<booktitle>In Formal Issues in Lexical Functional Grammar, ed. Mary Dalrymple,</booktitle>
<pages>7--28</pages>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="6917" citStr="Kaplan, 1995" startWordPosition="1098" endWordPosition="1099">sting. In addition, Wu (1998) use a bigram deep syntax language model in a stochastic inversion transduction grammar for English to Chinese. None of the related research we discuss here has included an evaluation of the deep syntax language model they employ in isolation from the MT system, however. 3 Deep Syntax The deep syntax language model we describe is not restricted to any individual theory of deep syntax. For clarity, however, we restrict our examples to LFG, which is also the deep syntax theory we use for our evaluation. The Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrymple, 2001) functional structure (f-structure) is an attribute-value encoding of bi-lexical labeled dependencies, such as subject, object and adjunct for example, with morpho-syntactic atomic attributes encoding information such as mood and tense of verbs, and person, number and case for nouns. Figure 1 shows the LFG f-structure for English sentence “Today congress passed Obama’s health care bill.”1 Encoded within the f-structure is a directed graph and our language model uses a simplified acyclic unlabeled version of this graph. Figure 1(b) shows the graph structure enco</context>
</contexts>
<marker>Kaplan, 1995</marker>
<rawString>Kaplan, Ronald M. 1995. The Formal Architecture of Lexical Functional Grammar. In Formal Issues in Lexical Functional Grammar, ed. Mary Dalrymple, pages 7-28, CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical Functional Grammar, a Formal System for Grammatical Represenation. In</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>J. Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6903" citStr="Kaplan and Bresnan, 1982" startWordPosition="1093" endWordPosition="1097">omitted in training and testing. In addition, Wu (1998) use a bigram deep syntax language model in a stochastic inversion transduction grammar for English to Chinese. None of the related research we discuss here has included an evaluation of the deep syntax language model they employ in isolation from the MT system, however. 3 Deep Syntax The deep syntax language model we describe is not restricted to any individual theory of deep syntax. For clarity, however, we restrict our examples to LFG, which is also the deep syntax theory we use for our evaluation. The Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrymple, 2001) functional structure (f-structure) is an attribute-value encoding of bi-lexical labeled dependencies, such as subject, object and adjunct for example, with morpho-syntactic atomic attributes encoding information such as mood and tense of verbs, and person, number and case for nouns. Figure 1 shows the LFG f-structure for English sentence “Today congress passed Obama’s health care bill.”1 Encoded within the f-structure is a directed graph and our language model uses a simplified acyclic unlabeled version of this graph. Figure 1(b) shows the graph </context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Kaplan, Ronald M., Joan Bresnan. 1982. Lexical Functional Grammar, a Formal System for Grammatical Represenation. In J. Bresnan, editor, The Mental Representation of Grammatical Relations, 173-281, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models.</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>868--876</pages>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, Philipp, Hieu Hoang. 2007. Factored Translation Models. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 868-876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicoli Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. Annual Meeting of the Association for Computational Linguistics, demonstration session</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicoli Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. Annual Meeting of the Association for Computational Linguistics, demonstration session</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the tenth Machine Translation Summit.</booktitle>
<marker>Koehn, 2005</marker>
<rawString>Koehn, Philipp 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the tenth Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1418" citStr="Koehn et al., 2003" startWordPosition="205" endWordPosition="208">t in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems. 1 Introduction In Phrase-Based Models of Machine Translation all phrases consistent with the word alignment are extracted (Koehn et al., 2003), with shorter phrases needed for high coverage of unseen data and longer phrases providing improved fluency in target language translations. Hierarchical Models (Chiang, 2007; Chiang, 2005) build on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase. Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discon</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Josef Och, Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference, 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>Grammatical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTACL,</booktitle>
<pages>248--255</pages>
<location>New York.</location>
<contexts>
<context position="5034" citStr="Riezler and Maxwell (2006)" startWordPosition="780" endWordPosition="783">ency tree SMT system for ChineseEnglish translation, using information from the deeper structure about dependency relations between words, in addition to the position of the words in the string, including information about whether context words were positioned on the left or right of a word. Bojar and Hajiˇc (2008) use a deep syntax language model in an English-Czech dependency tree-to-tree transfer system, and include three separate bigram language models: a reverse, direct and joint model. The model in our evaluation is similar to their direct bigram model, but is not restricted to bigrams. Riezler and Maxwell (2006) use a trigram deep syntax language model in German-English dependency tree-to-tree transfer to re-rank decoder output. The language model of Riezler and Maxwell (2006) is similar to the model in our evaluation, but differs in that it is restricted to a trigram model trained on LFG f-structures. In addition, as language modeling is not the main focus of their work, they provide little detail on the language model they use, except to say that it is based on “log-probability of strings ofpredicates from root to frontier of target f-structure, estimated from predicate trigrams in English f-struct</context>
</contexts>
<marker>Riezler, Maxwell, 2006</marker>
<rawString>Riezler, Stefan, John T. Maxwell III. 2006. Grammatical Machine Translation. In Proceedings of HLTACL, pages 248-255, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using Lexical Functional Grammar and Discriminitive Estimation Techniques . (grammar version</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="20136" citStr="Riezler et al., 2002" startWordPosition="3419" endWordPosition="3422">the parser, whereas a string-based language model can be trained on any available data of the appropriate language. Since parser coverage is not the focus of our work, we eliminate its effects from the evaluation by selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens Ave. Tokens Vocab per Sent. strings 138.6M 19 345K LFG lemmas/predicates 118.4M 16 28</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Riezler, Stefan, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell, Mark Johnson. 2002. Parsing the Wall Street Journal using Lexical Functional Grammar and Discriminitive Estimation Techniques . (grammar version 2005) In Proceedings of the 40th ACL, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="4348" citStr="Shen et al. (2008)" startWordPosition="671" endWordPosition="674">nt and the deeper context of a word, if available, may provide more meaningful information and result in better lexical choice. Integrating such a model into a Hierarchical SMT system is not straightforward, however, and we believe before embarking on this its worthwhile to evaluate the model independently of any MT system. We therefore provide an experimental evaluation of the model and in order to provide an interesting comparison, we evaluate a traditional string-based language model on the same data. 2 Related Work The idea of using a language model based on deep syntax is not new to SMT. Shen et al. (2008) use a dependency-based language model in a string to dependency tree SMT system for ChineseEnglish translation, using information from the deeper structure about dependency relations between words, in addition to the position of the words in the string, including information about whether context words were positioned on the left or right of a word. Bojar and Hajiˇc (2008) use a deep syntax language model in an English-Czech dependency tree-to-tree transfer system, and include three separate bigram language models: a reverse, direct and joint model. The model in our evaluation is similar to t</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Shen, Libin, Jinxi Xu, Ralph Weischedel. 2008. A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. Proceedings of ACL-08: HLT, pages 577-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="11553" citStr="Stolcke, 2002" startWordPosition="1899" endWordPosition="1900">are |pass bill )∗ p( ’ |passed Obama )∗ p( health |bill care )∗ p( s |obama ’ )∗ p(&lt;/s&gt; |care health ) p( . |care bill )∗ p(&lt;/s&gt; |bill . ) Figure 2: Example Comparison of Deep Syntax and Traditional Language Models of both like, be and president is hillary. In our simplified structure, the dependency relations between be and hillary and president and hillary are dropped. We discuss how we do this later in Section 6. Similar to our simplification for structure sharing, we also simplify structures that contain cycles by discarding edges that cause loops in the structure. 6 Implementation SRILM (Stolcke, 2002) can be used to compute a language model from ngram counts (the -read option of the ngram-count command). Implementation to train the language model, therefore, simply requires accurately extracting counts from the deep syntax parsed training corpus. To simplify the structures to acyclic graphs, nodes are labeled with an increasing index number via a depth first traversal. This allows each arc causing a loop in the graph or argument sharing to be identified by a simple comparison of index numbers, as the index number of its start node will be greater than that of its end node. The algorithm we</context>
<context position="21045" citStr="Stolcke, 2002" startWordPosition="3569" endWordPosition="3570">r produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens Ave. Tokens Vocab per Sent. strings 138.6M 19 345K LFG lemmas/predicates 118.4M 16 280K Table 1: Language model statistics for string-based and deep syntax language models, statistics are for string tokens and LFG lemmas for the same set of 7.29M English sentences al., 2004; Riezler et al., 2002). Ngrams were automatically extracted from the f-structures and lowercased. SRILM (Stolcke, 2002) was used to compute both language models. Table 1 shows statistics on the number of words and lemmas used to train each model. 8.2 Testing The test set consisted of 789 sentences selected from WMT09 additional development sets3 containing English Europarl text and again was selected on the basis of sentences being in-coverage of the parsing resources. SRILM (Stolcke, 2002) was used to compute test set perplexity and ngram coverage statistics for each order model. Since the deep syntax language model adds end of sentence markers to leaf nodes in the structures, the number of (so-called) end of</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wu Dekai</author>
</authors>
<title>Hongsing Wong.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th ACL and 17th COLING,</booktitle>
<location>Montreal, Quebec.</location>
<marker>Dekai, 1998</marker>
<rawString>Dekai, Wu, Hongsing Wong. 1998. Machine Translation with a Stochastic Grammatical Channel. In Proceedings of the 36th ACL and 17th COLING, Montreal, Quebec.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>