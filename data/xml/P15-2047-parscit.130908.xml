<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022471">
<title confidence="0.998403">
A Dependency-Based Neural Network for Relation Classification
</title>
<author confidence="0.998437">
Yang Liu1,2∗ Furu Wei3 Sujian Li1,2 Heng Ji4 Ming Zhou3 Houfeng Wang1,2
</author>
<affiliation confidence="0.88733775">
1Key Laboratory of Computational Linguistics, Peking University, MOE, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China
3Microsoft Research, Beijing, China
4Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA
</affiliation>
<email confidence="0.9858275">
{cs-ly, lisujian, wanghf}@pku.edu.cn
{furu, mingzhou}@microsoft.com jih@rpi.edu
</email>
<sectionHeader confidence="0.993876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99974485">
Previous research on relation classification
has verified the effectiveness of using de-
pendency shortest paths or subtrees. In
this paper, we further explore how to make
full use of the combination of these de-
pendency information. We first propose
a new structure, termed augmented de-
pendency path (ADP), which is composed
of the shortest dependency path between
two entities and the subtrees attached to
the shortest path. To exploit the semantic
representation behind the ADP structure,
we develop dependency-based neural net-
works (DepNN): a recursive neural net-
work designed to model the subtrees, and
a convolutional neural network to capture
the most important features on the shortest
path. Experiments on the SemEval-2010
dataset show that our proposed method
achieves state-of-art results.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997441706896552">
Relation classification aims to classify the seman-
tic relations between two entities in a sentence. It
plays a vital role in robust knowledge extraction
from unstructured texts and serves as an interme-
diate step in a variety of natural language process-
ing applications. Most existing approaches follow
the machine learning based framework and focus
on designing effective features to obtain better
classification performance.
The effectiveness of using dependency relation-
s between entities for relation classification has
been reported in previous approaches (Bach and
Badaskar, 2007). For example, Suchanek et al.
(2006) carefully selected a set of features from
tokenization and dependency parsing, and extend-
ed some of them to generate high order features
∗Contribution during internship at Microsoft Research.
in different ways. Culotta and Sorensen (2004)
designed a dependency tree kernel and attached
more information including Part-of-Speech tag,
chunking tag of each node in the tree. Interesting-
ly, Bunescu and Mooney (2005) provided an im-
portant insight that the shortest path between two
entities in a dependency graph concentrates most
of the information for identifying the relation be-
tween them. Nguyen et al. (2007) developed these
ideas by analyzing multiple subtrees with the guid-
ance of pre-extracted keywords. Previous work
showed that the most useful dependency informa-
tion in relation classification includes the shortest
dependency path and dependency subtrees. These
two kinds of information serve different functions
and their collaboration can boost the performance
of relation classification (see Section 2 for detailed
examples). However, how to uniformly and ef-
ficiently combine these two components is still
an open problem. In this paper, we propose a
novel structure named Augmented Dependency
Path (ADP) which attaches dependency subtrees
to words on a shortest dependency path and focus
on exploring the semantic representation behind
the ADP structure.
Recently, deep learning techniques have been
widely used in exploring semantic representation-
s behind complex structures. This provides us
an opportunity to model the ADP structure in a
neural network framework. Thus, we propose a
dependency-based framework where two neural
networks are used to model shortest dependency
paths and dependency subtrees separately. One
convolutional neural network (CNN) is applied
over the shortest dependency path, because CNN
is suitable for capturing the most useful features in
a flat structure. A recursive neural network (RN-
N) is used for extracting semantic representations
from the dependency subtrees, since RNN is good
at modeling hierarchical structures. To connect
these two networks, each word on the shortest
</bodyText>
<page confidence="0.887877">
285
</page>
<note confidence="0.372444">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 285–290,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999799">
Figure 1: Sentences and their dependency trees.
</figureCaption>
<figure confidence="0.9991735">
(a) Augmented dependency path in S1.
(b) Augmented dependency path in S2.
</figure>
<figureCaption confidence="0.999894">
Figure 2: Augmented dependency paths.
</figureCaption>
<bodyText confidence="0.998799166666667">
path is combined with a representation generated
from its subtree, strengthening the semantic rep-
resentation of the shortest path. In this way, the
augmented dependency path is represented as a
continuous semantic vector which can be further
used for relation classification.
</bodyText>
<sectionHeader confidence="0.792785" genericHeader="method">
2 Problem Definition and Motivation
</sectionHeader>
<bodyText confidence="0.999973918918919">
The task of relation classification can be defined
as follows. Given a sentence S with a pair of
entities e1 and e2 annotated, the task is to identify
the semantic relation between e1 and e2 in ac-
cordance with a set of predefined relation classes
(e.g., Content-Container, Cause-Effect). For ex-
ample, in Figure 2, the relation between two en-
tities e1=thief and e2=screwdriver is Instrument-
Agency.
Bunescu and Mooney (2005) first used short-
est dependency paths between two entities to
capture the predicate-argument sequences (e.g.,
“thief+—broke—*screwdriver” in Figure 2), which
provide strong evidence for relation classification.
As we observe, the shortest paths contain more
information and the subtrees attached to each node
on the shortest path are not exploited enough. For
example, Figure 2a and 2b show two instances
which have similar shortest dependency paths but
belong to different relation classes. Methods only
using the path will fail in this case. However, we
can distinguish these two paths by virtue of the
attached subtrees such as “dobj—*commandment”
and “dobj—*ignition”. Based on many observa-
tions like this, we propose the idea that combines
the subtrees and the shortest path to form a more
precise structure for classifying relations. This
combined structure is called “augmented depen-
dency path (ADP)”, as illustrated in Figure 2.
Next, our goal is to capture the semantic repre-
sentation of the ADP structure between two enti-
ties. We first adopt a recursive neural network to
model each word according to its attached depen-
dency subtree. Based on the semantic information
of each word, we design a convolutional neural
network to obtain salient semantic features on the
shortest dependency path.
</bodyText>
<sectionHeader confidence="0.990158" genericHeader="method">
3 Dependency-Based Neural Networks
</sectionHeader>
<bodyText confidence="0.999993263157895">
In this section, we will introduce how we use neu-
ral network techniques and dependency informa-
tion to explore the semantic connection between
two entities. We dub our architecture of model-
ing ADP structures as dependency-based neural
networks (DepNN). Figure 3 illustrates DepNN
with a concrete example. First, we associate each
word w and dependency relation r with a vector
representation x,,,, xr E Rdim. For each word
w on the shortest dependency path, we develop
an RNN from its leaf words up to the root to
generate a subtree embedding c,,, and concatenate
c,,, with x,,, to serve as the final representation of
w. Next, a CNN is designed to model the shortest
dependency path based on the representation of
its words and relations. Finally our framework
can efficiently represent the semantic connection
between two entities with consideration of more
comprehensive dependency information.
</bodyText>
<subsectionHeader confidence="0.999951">
3.1 Modeling Dependency Subtree
</subsectionHeader>
<bodyText confidence="0.99997875">
The goal of modeling dependency subtrees is to
find an appropriate representation for the words on
the shortest path. We assume that each word w
can be interpreted by itself and its children on the
dependency subtree. Then, for each word w on the
subtree, its word embedding x,,, E Rdim and sub-
tree representation c,,, E Rdimc are concatenated
to form its final representation p,,, E Rdim+dimc.
For a word that does not have a subtree, we set
its subtree representation as cLEAF. The subtree
representation of a word is derived through trans-
forming the representations of its children words.
</bodyText>
<figure confidence="0.996647833333333">
nsubj
prep-with
rcmod
det
xcomp
dobj
dobj
A thief who tried to steal the truck broke the ignition with screwdriver.
det
nsubj
aux
det
det
prep-on
det nsubj
dobj
prep-with
S2: On the Sabbath the priests broke the commandment with priestly work.
amod
det
ignition
det
the
A
det
dobj
thief nsubj
broke prep-with screwdriver
priests nsubj
the
det
commandment
det
the
dobj
broke prep-with work
prep-on
Sabbath
the
det
priestly
amod
</figure>
<page confidence="0.773727">
286
</page>
<figureCaption confidence="0.9193295">
Figure 3: Illustration of Dependency-based Neural
Networks.
</figureCaption>
<bodyText confidence="0.999464166666667">
During the bottom-up construction of the subtree,
each word is associated with a dependency rela-
tion such as dobj as in Figure 3. For each depen-
dency relation r, we set a transformation matrix
Wr E Rdimcx(dim+dimc) which is learned during
training. Then we can get,
</bodyText>
<equation confidence="0.998248">
�cw = f( WR(w,q) · pq + b) (1)
qEChildren(w)
pq = [xq, cq] (2)
</equation>
<bodyText confidence="0.99995575">
where R(w,q) denotes the dependency relation be-
tween word w and its child word q. This process
continues recursively up to the root word on the
shortest path.
</bodyText>
<subsectionHeader confidence="0.998898">
3.2 Modeling Shortest Dependency Path
</subsectionHeader>
<bodyText confidence="0.9998934375">
To classify the semantic relation between two en-
tities, we further explore the semantic representa-
tion behind their shortest dependency path, which
can be seen as a sequence of words and dependen-
cy relations as the bold-font part in Figure 2. As
the convolutional neural network (CNN) is good
at capturing the salient features from a sequence
of objects, we design a CNN to tackle the shortest
dependency path.
A CNN contains a convolution operation over
a window of object representations, followed by
a pooling operation. As we know, a word w
on the shortest path is associated with the repre-
sentation pw through modeling the subtree. For
a dependency relation r on the shortest path,
we set its representation as a vector xr E
Rdim. As a sliding window is applied on the
sequence, we set the window size as k. For
example, when k = 3, the sliding windows of
a shortest dependency path with n words are:
{[rs w1 r1], [r1 w2 r2],..., [rn_1 wn re]} where
rs and re are used to denote the beginning and
end of a shortest dependency path between two
entities.
We concatenate k neighboring words (or de-
pendency relations) representations into a new
vector. Assume Xi E Rdim·k+dimc·nw as the
concatenated representation of the i-th window,
where nw is the number of words in one window.
A convolution operation involves a filter W1 E
Rlx(dim·k+dimc·nw), which operates on Xi to pro-
duce a new feature vector Li with l dimensions,
</bodyText>
<equation confidence="0.958792">
Li = W1Xi (3)
</equation>
<bodyText confidence="0.980547666666667">
where the bias term is ignored for simplicity.
Then W1 is applied to each possible window
in the shortest dependency path to produce a
feature map: [L0, L1, L2, · · · ]. Next, we adop-
t the widely-used max-over-time pooling opera-
tion (Collobert et al., 2011), which can retain
the most important features, to obtain the final
representation L from the feature map. That is,
L = max(L0, L1, L2,... ).
</bodyText>
<subsectionHeader confidence="0.997745">
3.3 Learning
</subsectionHeader>
<bodyText confidence="0.999979125">
Like other relation classification systems, we al-
so incorporate some lexical level features such
as named entity tags and WordNet hypernyms,
which prove useful to this task. We concatenate
them with the ADP representation L to produce
a combined vector M. We then pass M to a
fully connected softmax layer whose output is
the probability distribution y over relation labels.
</bodyText>
<equation confidence="0.9999735">
M = [L, LEX] (4)
y = softmax(W2M + b2) (5)
</equation>
<bodyText confidence="0.9998186">
Then, the optimization objective is to minimize
the cross-entropy error between the ground-truth
label vector and the softmax output. Pa-
rameters are learned using the back-propagation
method (Rumelhart et al., 1988).
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999881">
We compare DepNN against multiple baselines on
SemEval-2010 dataset (Hendrickx et al., 2010).
The training set includes 8000 sentences, and
the test set includes 2717 sentences. There are 9
</bodyText>
<figure confidence="0.998229518518518">
priests nsubj broke prep_with work
Shortest
Path
Window
Processing
Subtree
Embeddings
the
Wdet Wdobj Wprep-on Wamod
comman-
dament
the
Wdet
Sabbath
the
Wdet
priestly
Word
Embedding
Subtree
Embedding
Recursive
Neural Network
Convolutional
Neural Network
W1
Max Over Time
</figure>
<page confidence="0.991243">
287
</page>
<bodyText confidence="0.999984285714286">
relation types, and each type has two directions.
Instances which don’t fall in any of these classes
are labeled as Other. The official evaluation metric
is the macro-averaged F1-score (excluding Other)
and the direction is considered. We use dependen-
cy trees generated by the Stanford Parser (Klein
and Manning, 2003) with the collapsed option.
</bodyText>
<subsectionHeader confidence="0.999568">
4.1 Contributions of different components
</subsectionHeader>
<bodyText confidence="0.999240666666667">
We first show the contributions from different
components of DepNN. Two different kinds of
word embeddings for initialization are used in the
experiments. One is the 50-d embeddings pro-
vided by SENNA (Collobert et al., 2011). The
second is the 200-d embeddings used in (Yu et
al., 2014), trained on Gigaword with word2vec1.
All the hyperparameters are set with 5-fold cross-
validation.
</bodyText>
<table confidence="0.9978415">
Model F1
50-d 200-d
baseline (Path words) 73.8 75.5
+Depedency relations 80.3 81.8
+Attached subtrees 81.2 82.8
+Lexical features 82.7 83.6
</table>
<tableCaption confidence="0.9210415">
Table 1: Performance of DepNN with different
components.
</tableCaption>
<bodyText confidence="0.999951545454546">
We start with a baseline model using a CNN
with only the words on the shortest path. We then
add dependency relations and attached subtrees.
The results indicate that both parts are effective
for relation classification. The rich linguistic in-
formation embedded in the dependency relations
and subtrees can on one hand, help distinguish dif-
ferent functions of the same word, and on the other
hand infer an unseen word’s role in the sentence.
Finally, the lexical features are added and DepNN
achieves state-of-the-art results.
</bodyText>
<subsectionHeader confidence="0.999883">
4.2 Comparison with Baselines
</subsectionHeader>
<bodyText confidence="0.998476555555555">
In this subsection, we compare DepNN with sev-
eral baseline relation classification approaches.
Here, DepNN and the baselines are all based on
the 200-d embeddings trained on Gigaword due to
the larger corpus and higher dimensions.
SVM (Rink and Harabagiu, 2010): This is the
top performed system in SemEval-2010. It utilizes
many external corpora to extract features from the
sentence to build an SVM classifier.
</bodyText>
<footnote confidence="0.98501">
1https://code.google.com/p/word2vec/
</footnote>
<table confidence="0.999919">
Model Additional Features F1
POS, PropBank, morphological
SVM WordNet, TextRunner, FrameNet 82.2
dependency parse, etc.
MV-RNN POS, NER, WordNet 81.82
CNN WordNet 82.7
FCM NER 83.0
DT-RNN NER 73.1
DepNN WordNet 83.0
NER 83.6
</table>
<tableCaption confidence="0.986146">
Table 2: Results on SemEval-2010 dataset with
Gigaword embeddings.
</tableCaption>
<bodyText confidence="0.993828108108108">
MV-RNN (Socher et al., 2012): This model
finds the path between the two entities in the con-
stituent parse tree and then learns the distributed
representation of its highest node with a matrix for
each word to make the compositions specific.
CNN: Zeng et al. (2014) build a convolutional
model over the tokens of a sentence to learn the
sentence level feature vector. It uses a special
position vector that indicates the relative distances
of current input word to two marked entities.
FCM (Yu et al., 2014): FCM decomposes the
sentence into substructures and extracts features
for each of them, forming substructure embed-
dings. These embeddings are combined by sum-
pooling and input into a softmax classifier.
DT-RNN (Socher et al., 2014) : This is an
RNN for modeling dependency trees. It combines
node’s word embedding with its children through
a linear combination but not a subtree embedding.
We adapt the augmented dependency path into a
dependency subtree and apply DT-RNN.
As shown in Table 2, DepNN achieves the best
result (83.6) using NER features. WordNet fea-
tures can also improve the performance of DepN-
N, but not as obvious as NER. Yu et al. (2014)
had similar observations, since the larger number
of WordNet tags may cause overfitting. SVM
achieves a comparable result, though the quality
of feature engineering highly relies on human ex-
perience and external NLP resources. MV-RNN
models the constituent parse trees with a recursive
procedure and its F1-score is about 1.8 percent
lower than DepNN. Meanwhile, MVR-NN is very
slow to train, since each word is associated with a
matrix. Both CNN and FCM use features from the
whole sentence and achieve similar performance.
DT-RNN is the worst of all baselines, though it
</bodyText>
<footnote confidence="0.9091135">
2MV-RNN achieves a higher F1-score (82.7) on SENNA
embeddings reported in the original paper.
</footnote>
<page confidence="0.994414">
288
</page>
<bodyText confidence="0.999959928571428">
also considers the information from shortest de-
pendency paths and attached subtrees. As we ana-
lyze, shortest dependency paths and subtrees play
different roles in relation classification. However,
we can see that DT-RNN does not distinguish the
modeling processes of shortest paths and subtrees.
This phenomenon is also seen in a kernel-based
method (Wang, 2008), where the tree kernel per-
forms worse than the shortest path kernel. We also
look into the DepNN model and find it can identify
different patterns of words and the dependency
relations. For example, in the Instrument-Agency
relation, the word “using” and the dependency re-
lation “prep with” are found playing a major role.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99994075">
In this paper, we propose to classify relations
between entities by modeling the augmented de-
pendency path in a neural network framework.
We present a novel approach, DepNN, to taking
advantages of both convolutional neural network
and recursive neural network to model this struc-
ture. Experiment results demonstrate that DepNN
achieves state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99986275">
We thank all the anonymous reviewers for their
insightful comments. This work was partially sup-
ported by National Key Basic Research Program
of China (No. 2014CB340504), National Natural
Science Foundation of China (No. 61273278 and
61370117), and National Social Science Fund of
China (No: 12&amp;ZD227). The correspondence
author of this paper is Sujian Li.
</bodyText>
<sectionHeader confidence="0.992447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992000188405797">
Nguyen Bach and Sameer Badaskar. 2007. A survey
on relation extraction. Language Technologies Insti-
tute, Carnegie Mellon University.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
A Shortest Path Dependency Kernel for Relation
Extraction. In North American Chapter of the As-
sociation for Computational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Aron Culotta and Jeffrey S. Sorensen. 2004. De-
pendency Tree Kernels for Relation Extraction. In
Meeting of the Association for Computational Lin-
guistics, pages 423–429.
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Se-
bastian Pad ok, Marco Pennacchiotti, Lorenza Ro-
mano, and Stan Szpakowicz. 2010. SemEval-2010
Task 8: Multi-Way Classification of Semantic Rela-
tions Between Pairs of Nominals.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Meeting of the As-
sociation for Computational Linguistics, pages 423–
430.
Dat PT Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka.
2007. Relation extraction from wikipedia using
subtree mining. In Proceedings of the National Con-
ference on Artificial Intelligence, volume 22, page
1414. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Bryan Rink and Sanda Harabagiu. 2010. Utd: Clas-
sifying semantic relations by combining lexical and
semantic resources. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
256–259, Uppsala, Sweden, July. Association for
Computational Linguistics.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. Cognitive modeling, 5.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211, Jeju Island, Korea, July. Association for
Computational Linguistics.
Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014. Ground-
ed compositional semantics for finding and describ-
ing images with sentences. Transactions of the
Association for Computational Linguistics, 2:207–
218.
Fabian M Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 712–717. ACM.
Mengqiu Wang. 2008. A re-examination of dependen-
cy path kernels for relation extraction. In IJCNLP,
pages 841–846.
Mo Yu, Matthew Gormley, and Mark Dredze. 2014.
Factor-based compositional embedding models. In
NIPS Workshop on Learning Semantics.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via
convolutional deep neural network. In Proceedings
</reference>
<page confidence="0.977838">
289
</page>
<reference confidence="0.9975704">
of COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344, Dublin, Ireland, August. Dublin
City University and Association for Computational
Linguistics.
</reference>
<page confidence="0.997106">
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495552">
<title confidence="0.999948">A Dependency-Based Neural Network for Relation Classification</title>
<author confidence="0.979258">Furu Sujian Heng Ming Houfeng</author>
<affiliation confidence="0.776324">Laboratory of Computational Linguistics, Peking University, MOE, Innovation Center for Language Ability, Xuzhou, Jiangsu,</affiliation>
<address confidence="0.9720115">Research, Beijing, China Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA</address>
<email confidence="0.9880365">lisujian,jih@rpi.edu</email>
<abstract confidence="0.999508761904762">Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Sameer Badaskar</author>
</authors>
<title>A survey on relation extraction.</title>
<date>2007</date>
<institution>Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="1889" citStr="Bach and Badaskar, 2007" startWordPosition="261" endWordPosition="264">s state-of-art results. 1 Introduction Relation classification aims to classify the semantic relations between two entities in a sentence. It plays a vital role in robust knowledge extraction from unstructured texts and serves as an intermediate step in a variety of natural language processing applications. Most existing approaches follow the machine learning based framework and focus on designing effective features to obtain better classification performance. The effectiveness of using dependency relations between entities for relation classification has been reported in previous approaches (Bach and Badaskar, 2007). For example, Suchanek et al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to generate high order features ∗Contribution during internship at Microsoft Research. in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying </context>
</contexts>
<marker>Bach, Badaskar, 2007</marker>
<rawString>Nguyen Bach and Sameer Badaskar. 2007. A survey on relation extraction. Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A Shortest Path Dependency Kernel for Relation Extraction.</title>
<date>2005</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2339" citStr="Bunescu and Mooney (2005)" startWordPosition="328" endWordPosition="331">on performance. The effectiveness of using dependency relations between entities for relation classification has been reported in previous approaches (Bach and Badaskar, 2007). For example, Suchanek et al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to generate high order features ∗Contribution during internship at Microsoft Research. in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 </context>
<context position="5223" citStr="Bunescu and Mooney (2005)" startWordPosition="759" endWordPosition="762">of the shortest path. In this way, the augmented dependency path is represented as a continuous semantic vector which can be further used for relation classification. 2 Problem Definition and Motivation The task of relation classification can be defined as follows. Given a sentence S with a pair of entities e1 and e2 annotated, the task is to identify the semantic relation between e1 and e2 in accordance with a set of predefined relation classes (e.g., Content-Container, Cause-Effect). For example, in Figure 2, the relation between two entities e1=thief and e2=screwdriver is InstrumentAgency. Bunescu and Mooney (2005) first used shortest dependency paths between two entities to capture the predicate-argument sequences (e.g., “thief+—broke—*screwdriver” in Figure 2), which provide strong evidence for relation classification. As we observe, the shortest paths contain more information and the subtrees attached to each node on the shortest path are not exploited enough. For example, Figure 2a and 2b show two instances which have similar shortest dependency paths but belong to different relation classes. Methods only using the path will fail in this case. However, we can distinguish these two paths by virtue of</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A Shortest Path Dependency Kernel for Relation Extraction. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="10794" citStr="Collobert et al., 2011" startWordPosition="1688" endWordPosition="1691">k neighboring words (or dependency relations) representations into a new vector. Assume Xi E Rdim·k+dimc·nw as the concatenated representation of the i-th window, where nw is the number of words in one window. A convolution operation involves a filter W1 E Rlx(dim·k+dimc·nw), which operates on Xi to produce a new feature vector Li with l dimensions, Li = W1Xi (3) where the bias term is ignored for simplicity. Then W1 is applied to each possible window in the shortest dependency path to produce a feature map: [L0, L1, L2, · · · ]. Next, we adopt the widely-used max-over-time pooling operation (Collobert et al., 2011), which can retain the most important features, to obtain the final representation L from the feature map. That is, L = max(L0, L1, L2,... ). 3.3 Learning Like other relation classification systems, we also incorporate some lexical level features such as named entity tags and WordNet hypernyms, which prove useful to this task. We concatenate them with the ADP representation L to produce a combined vector M. We then pass M to a fully connected softmax layer whose output is the probability distribution y over relation labels. M = [L, LEX] (4) y = softmax(W2M + b2) (5) Then, the optimization obje</context>
<context position="12672" citStr="Collobert et al., 2011" startWordPosition="1983" endWordPosition="1986">r Time 287 relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 Contributions of different components We first show the contributions from different components of DepNN. Two different kinds of word embeddings for initialization are used in the experiments. One is the 50-d embeddings provided by SENNA (Collobert et al., 2011). The second is the 200-d embeddings used in (Yu et al., 2014), trained on Gigaword with word2vec1. All the hyperparameters are set with 5-fold crossvalidation. Model F1 50-d 200-d baseline (Path words) 73.8 75.5 +Depedency relations 80.3 81.8 +Attached subtrees 81.2 82.8 +Lexical features 82.7 83.6 Table 1: Performance of DepNN with different components. We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic i</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey S Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="2165" citStr="Culotta and Sorensen (2004)" startWordPosition="302" endWordPosition="305">l language processing applications. Most existing approaches follow the machine learning based framework and focus on designing effective features to obtain better classification performance. The effectiveness of using dependency relations between entities for relation classification has been reported in previous approaches (Bach and Badaskar, 2007). For example, Suchanek et al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to generate high order features ∗Contribution during internship at Microsoft Research. in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey S. Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. In Meeting of the Association for Computational Linguistics, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sebastian Pad ok</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals.</booktitle>
<contexts>
<context position="11690" citStr="Hendrickx et al., 2010" startWordPosition="1833" endWordPosition="1836">and WordNet hypernyms, which prove useful to this task. We concatenate them with the ADP representation L to produce a combined vector M. We then pass M to a fully connected softmax layer whose output is the probability distribution y over relation labels. M = [L, LEX] (4) y = softmax(W2M + b2) (5) Then, the optimization objective is to minimize the cross-entropy error between the ground-truth label vector and the softmax output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 priests nsubj broke prep_with work Shortest Path Window Processing Subtree Embeddings the Wdet Wdobj Wprep-on Wamod commandament the Wdet Sabbath the Wdet priestly Word Embedding Subtree Embedding Recursive Neural Network Convolutional Neural Network W1 Max Over Time 287 relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is consid</context>
</contexts>
<marker>Hendrickx, Kozareva, Nakov, ok, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Sebastian Pad ok, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="12378" citStr="Klein and Manning, 2003" startWordPosition="1938" endWordPosition="1941">ludes 2717 sentences. There are 9 priests nsubj broke prep_with work Shortest Path Window Processing Subtree Embeddings the Wdet Wdobj Wprep-on Wamod commandament the Wdet Sabbath the Wdet priestly Word Embedding Subtree Embedding Recursive Neural Network Convolutional Neural Network W1 Max Over Time 287 relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 Contributions of different components We first show the contributions from different components of DepNN. Two different kinds of word embeddings for initialization are used in the experiments. One is the 50-d embeddings provided by SENNA (Collobert et al., 2011). The second is the 200-d embeddings used in (Yu et al., 2014), trained on Gigaword with word2vec1. All the hyperparameters are set with 5-fold crossvalidation. Model F1 50-d 200-d baseline (Path words) 73.8 75.5 +Depedency relations 80.3 81.8 +Attached subtrees 81.2 82.8 +Lexical features 82.7 83.6 Table</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Meeting of the Association for Computational Linguistics, pages 423– 430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dat PT Nguyen</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Relation extraction from wikipedia using subtree mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<volume>22</volume>
<pages>1414</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="2536" citStr="Nguyen et al. (2007)" startWordPosition="360" endWordPosition="363">al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to generate high order features ∗Contribution during internship at Microsoft Research. in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path </context>
</contexts>
<marker>Nguyen, Matsuo, Ishizuka, 2007</marker>
<rawString>Dat PT Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Relation extraction from wikipedia using subtree mining. In Proceedings of the National Conference on Artificial Intelligence, volume 22, page 1414. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Utd: Classifying semantic relations by combining lexical and semantic resources.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>256--259</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="13848" citStr="Rink and Harabagiu, 2010" startWordPosition="2168" endWordPosition="2171">or relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. 4.2 Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1https://code.google.com/p/word2vec/ Model Additional Features F1 POS, PropBank, morphological SVM WordNet, TextRunner, FrameNet 82.2 dependency parse, etc. MV-RNN POS, NER, WordNet 81.82 CNN WordNet 82.7 FCM NER 83.0 DT-RNN NER 73.1 DepNN WordNet 83.0 NER 83.6 Table 2: Results on SemEval-2010 dataset with Gigaword embeddings. MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then</context>
</contexts>
<marker>Rink, Harabagiu, 2010</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2010. Utd: Classifying semantic relations by combining lexical and semantic resources. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 256–259, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1988</date>
<booktitle>Cognitive modeling,</booktitle>
<pages>5</pages>
<contexts>
<context position="11582" citStr="Rumelhart et al., 1988" startWordPosition="1818" endWordPosition="1821"> relation classification systems, we also incorporate some lexical level features such as named entity tags and WordNet hypernyms, which prove useful to this task. We concatenate them with the ADP representation L to produce a combined vector M. We then pass M to a fully connected softmax layer whose output is the probability distribution y over relation labels. M = [L, LEX] (4) y = softmax(W2M + b2) (5) Then, the optimization objective is to minimize the cross-entropy error between the ground-truth label vector and the softmax output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 priests nsubj broke prep_with work Shortest Path Window Processing Subtree Embeddings the Wdet Wdobj Wprep-on Wamod commandament the Wdet Sabbath the Wdet priestly Word Embedding Subtree Embedding Recursive Neural Network Convolutional Neural Network W1 Max Over Time 287 relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other.</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by backpropagating errors. Cognitive modeling, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="14357" citStr="Socher et al., 2012" startWordPosition="2241" endWordPosition="2244">d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1https://code.google.com/p/word2vec/ Model Additional Features F1 POS, PropBank, morphological SVM WordNet, TextRunner, FrameNet 82.2 dependency parse, etc. MV-RNN POS, NER, WordNet 81.82 CNN WordNet 82.7 FCM NER 83.0 DT-RNN NER 73.1 DepNN WordNet 83.0 NER 83.6 Table 2: Results on SemEval-2010 dataset with Gigaword embeddings. MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. </context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>218</pages>
<contexts>
<context position="15066" citStr="Socher et al., 2014" startWordPosition="2358" endWordPosition="2361">en learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a softmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepNN, but not as obvious as NER. Yu et al. (2014) had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly rel</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207– 218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>Combining linguistic and statistical analysis to extract relations from web documents.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>712--717</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1926" citStr="Suchanek et al. (2006)" startWordPosition="267" endWordPosition="270">Relation classification aims to classify the semantic relations between two entities in a sentence. It plays a vital role in robust knowledge extraction from unstructured texts and serves as an intermediate step in a variety of natural language processing applications. Most existing approaches follow the machine learning based framework and focus on designing effective features to obtain better classification performance. The effectiveness of using dependency relations between entities for relation classification has been reported in previous approaches (Bach and Badaskar, 2007). For example, Suchanek et al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to generate high order features ∗Contribution during internship at Microsoft Research. in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et </context>
</contexts>
<marker>Suchanek, Ifrim, Weikum, 2006</marker>
<rawString>Fabian M Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. Combining linguistic and statistical analysis to extract relations from web documents. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 712–717. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
</authors>
<title>A re-examination of dependency path kernels for relation extraction. In</title>
<date>2008</date>
<booktitle>IJCNLP,</booktitle>
<pages>841--846</pages>
<contexts>
<context position="16524" citStr="Wang, 2008" startWordPosition="2596" endWordPosition="2597"> with a matrix. Both CNN and FCM use features from the whole sentence and achieve similar performance. DT-RNN is the worst of all baselines, though it 2MV-RNN achieves a higher F1-score (82.7) on SENNA embeddings reported in the original paper. 288 also considers the information from shortest dependency paths and attached subtrees. As we analyze, shortest dependency paths and subtrees play different roles in relation classification. However, we can see that DT-RNN does not distinguish the modeling processes of shortest paths and subtrees. This phenomenon is also seen in a kernel-based method (Wang, 2008), where the tree kernel performs worse than the shortest path kernel. We also look into the DepNN model and find it can identify different patterns of words and the dependency relations. For example, in the Instrument-Agency relation, the word “using” and the dependency relation “prep with” are found playing a major role. 5 Conclusion In this paper, we propose to classify relations between entities by modeling the augmented dependency path in a neural network framework. We present a novel approach, DepNN, to taking advantages of both convolutional neural network and recursive neural network to</context>
</contexts>
<marker>Wang, 2008</marker>
<rawString>Mengqiu Wang. 2008. A re-examination of dependency path kernels for relation extraction. In IJCNLP, pages 841–846.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Factor-based compositional embedding models.</title>
<date>2014</date>
<booktitle>In NIPS Workshop on Learning Semantics.</booktitle>
<contexts>
<context position="12734" citStr="Yu et al., 2014" startWordPosition="1995" endWordPosition="1998">s which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 Contributions of different components We first show the contributions from different components of DepNN. Two different kinds of word embeddings for initialization are used in the experiments. One is the 50-d embeddings provided by SENNA (Collobert et al., 2011). The second is the 200-d embeddings used in (Yu et al., 2014), trained on Gigaword with word2vec1. All the hyperparameters are set with 5-fold crossvalidation. Model F1 50-d 200-d baseline (Path words) 73.8 75.5 +Depedency relations 80.3 81.8 +Attached subtrees 81.2 82.8 +Lexical features 82.7 83.6 Table 1: Performance of DepNN with different components. We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees c</context>
<context position="14835" citStr="Yu et al., 2014" startWordPosition="2323" endWordPosition="2326">T-RNN NER 73.1 DepNN WordNet 83.0 NER 83.6 Table 2: Results on SemEval-2010 dataset with Gigaword embeddings. MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a softmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performanc</context>
</contexts>
<marker>Yu, Gormley, Dredze, 2014</marker>
<rawString>Mo Yu, Matthew Gormley, and Mark Dredze. 2014. Factor-based compositional embedding models. In NIPS Workshop on Learning Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation classification via convolutional deep neural network.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>2335--2344</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="14593" citStr="Zeng et al. (2014)" startWordPosition="2282" endWordPosition="2285">build an SVM classifier. 1https://code.google.com/p/word2vec/ Model Additional Features F1 POS, PropBank, morphological SVM WordNet, TextRunner, FrameNet 82.2 dependency parse, etc. MV-RNN POS, NER, WordNet 81.82 CNN WordNet 82.7 FCM NER 83.0 DT-RNN NER 73.1 DepNN WordNet 83.0 NER 83.6 Table 2: Results on SemEval-2010 dataset with Gigaword embeddings. MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a softmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combinati</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335–2344, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>