<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008636">
<title confidence="0.8053385">
Learn to speak and to write, learn to use your mind
The relevance of automatic text generation research for people
</title>
<author confidence="0.750128">
Michael Zock
</author>
<address confidence="0.646298">
LIMSI-CNRS, B.P.133,
91403 Orsay, France
</address>
<email confidence="0.997584">
zock@limsi.fr
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998951">
The aim of this talk is to show to what
extent the work on text generation by
computer (TGBC) does not address
some of the fundamental problems
people struggle with when generating
language (TGBP). We will substantiate
this claim by taking two tasks on
which a lot of research has been
carried out during the last 15 years:
discourse planning and lexicalisation.
</bodyText>
<sectionHeader confidence="0.95921" genericHeader="method">
1 Discourse planning
</sectionHeader>
<bodyText confidence="0.999471833333333">
While a tremendous amount of work has been
done on the generation of coherent discourse,
little if any has been devoted to writing. As a
result, many fundamental problems have been
overlooked or have been dealt with on the basis
of wrong assumptions. Also, little, if any of the
results achieved in the TGBC framework can be
reused in the classroom or in the context of an
intelligent writing-aid (tools for assisting the
writer to structure her/his thoughts: outline
planning). Let us consider some of the reasons
why this is so.
</bodyText>
<listItem confidence="0.963878181818182">
• Top-down processing: in the TGBC-
community texts are generally processed top to
bottom. Given some goal one looks for data
(messages) and structures which integrate them.
While this is a clever way to handle the
problem, it does not give a precise reflection of
the writers’ situation. First of all, it is not true
that content and structure are always determined
simultaneously, an assumption accepted since
Moore &amp; Paris (1993). Secondly, writers gene-
rally switch between data-driven (brain-
storming) and structure-driven processing (out-
lining). Thirdly, there is a triangular relationship
between messages, structures and goals (or
effects), changing any of them can affect the
others. Yet, at present we do not have the fain-
test idea what effect(s) a specific propositional
or conceptual configuration (order of messages)
might produce.
• Lack of a Conceptual Structure Theory
(CST): messages tend come to our mind in any
order and without exhibiting their potential
</listItem>
<bodyText confidence="0.93777025">
links. We have to discover these later, and to
reorganize the former in order to reveal the
structure to the reader. Writing is thinking.
These last three points are crucial, yet none of
the existing theories (schema, RST) is really
able to take them into account. Just imagine
how complex it is to recognize the fact that
there is a causal link between two events. We
don’t have a solid theory of causality, leave
alone a method of operationalizing it (i.e. infer
this kind of link solely on the basis of the
intrinsic features of the events involved).
</bodyText>
<listItem confidence="0.5181429">
• Interaction: As we all know, texts have
structure. This latter is generally the result of
discourse planning (schemata or RST-based) or
reasoning (chain of inferencing), in which case
the structure emerges as a side effect. The major
shortcoming of all these techniques is that they
do not model the interaction between the
conceptual data (ideas, messages), the text
structure and the rhetorical effects: (all) the data
to be communicated and the global discourse
</listItem>
<bodyText confidence="0.9997028">
goal are generally given with the input.1 The
problem of reconciling mismatches between
data and structure,2 and the problem of variable
rhetorical effects/goals as a function of various
linearization strategies is not addressed at all.3
</bodyText>
<sectionHeader confidence="0.984597" genericHeader="method">
2 Lexicalisation
</sectionHeader>
<bodyText confidence="0.981747735294118">
Lexicalisation amounts mainly to searching and
choosing: one has to find lemmata, matching a
given conceptual chunk, and then one has to
choose among them. While much emphasis has
been given to the notion of choice, far less
attention has been paid to the search mecha-
nisms (or access strategies). I will present
during my talk some preliminary results
concerning a system that is meant to help people
to overcome the tip-of-the tongue problem, a
well known stumbling block in real-time
processing: we know what we want to say, we
know that we do know the word, yet we cannot
access it (Brown and Mc Neill, 1966).
If the fundamental role of a dictionary in
NLG is obvious, it is less evident as to the
principles governing its compilation. A good
dictionary is a place with a lot of information,
structured in such a way that the relevant
information is easily accessible when needed. In
other words, what counts is &apos;what is in the
dictionary&apos; (content) and &apos;how the information is
organized (meaning, form, sound). These two
factors are not sufficient though: access depends
not only on the structure of the lexicon
(organisation), but also on the efficiency of
1 While in Moore &amp; Paris (1993), the messages are
not given, the goal is : it cannot emerge as a side
effect.
2 What shall we do if not all the data can be
integrated, or if we lack data for filling all the
slots of a chosen structure? Shall we keep the
structure and look for more data, or use a dif-
ferent structure as it integrates more of the data?
3 One of the reasons for this is that we do not have
a clear understanding concerning the mapping
between different conceptual configurations and
their corresponding rhetorical effect(s). If we did,
we could use them bidirectionally (for analysis
and generation).
search strategies, an issue not addressed at all
by the generation community. As a matter of
fact, from a strict computational linguistic point
of view, the whole matter may be a non-issue.
However, the problem does become relevant
when we look at generation as a machine-me-
diated process (people using a word processor
for writing) or from a psycholinguistic point of
view: word access in writing or spontaneous
discourse.
• The speaker’s problem: choosing words,
finding them or both ? Obviously, there is more
to lexicalisation than just choosing words: one
has to find them to begin with. No matter how
rich a lexical database may be, it is of little use
if one cannot access the relevant information in
time. Access is probably THE major problem that
we have to cope with when trying to produce
language in real-time (in spoken or written
form). As I will show during my talk, this is
precisely a point where computers can be of
considerable help.
Work on memory has shown that access
depends crucially on the way information is
organized, yet the latter can vary to a great
extent. From speech error literature we learn,
that ease of access depends not only on meaning
relations,—, i.e. the way words are organized in
our mind),— but also on linguistic form (letters,
phonemes). Researchers collecting speech errors
have offered countless examples of phono-
logical errors in which segments (phonemes,
syllables or words) are added, deleted, anti-
cipated or exchanged (Fromkin, 1993). The data
clearly show that knowing the meaning of
words does not guarantee their access.
The work on speech errors also reveals that
words are stored in at least two modes, by
meaning and by form (written, spoken), and it is
often this latter which inhibits finding the right
token: having inadvertently recombined the
components of a given word (syllable scramb-
ling), one may end up producing a word, which
either does not exist or is simply different from
the one in mind. This kind of recombination,
resulting from bookkeeping problems (due to
time pressure), parallel processing and infor-
mation overload, may disturb or prevent the
access of the right word. Hence the usefulness
of a tool which allows the process to be
reversed. In order to allow this to be done, it is
necessary to represent words not only in terms
of their meaning, but also in terms of their
written and spoken form. The fact that words are
indexed both by meaning and by sound could
now be used to our advantage. The phonetic co-
ding of words allows the recombination of their
segments (syllables), hence the presentation of
new candidates, among which the user should
find the one s/he is looking for.4 The fact that
words are coded semantically keeps the number
of candidates to be presented small.
</bodyText>
<sectionHeader confidence="0.93047" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999923285714286">
I have tried to illustrate briefly to what extent
we have neglected the human factor in our
work. I have also attempted to show how a
simple computational method (combinatorics
and filtering) can be used to bridge (one of) the
gap(s) between TGBC and TGBP: text generation
by people.
</bodyText>
<sectionHeader confidence="0.999079" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984133733333333">
Roger Brown and David Mc Neill. 1966. The tip
of the tongue phenomenon. Journal of Verbal
Learning and Verbal Behavior, 5, 325-337
Viktoria Fromkin. 1993. Speech Production. In
Psycholinguistics edited by Jean Berko-Gleason
&amp; Nan Bernstein Ratner. Fort Worth, TX:
Harcourt, Brace, Jovanovich
Johanna Moore and Cecile Paris. 1993.
Planning text for advisory dialogues: capturing
intentional and rhetorical information.
Computational Linguistics, 19(4).
4 The assumption is that speakers produce words
that formwise are reasonably close to the target
word. A fact that is supported by psycholinguistic
evidence.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.115362">
<title confidence="0.7698715">to to learn to relevance of text generation research people</title>
<author confidence="0.950712">Michael</author>
<affiliation confidence="0.986255">LIMSI-CNRS,</affiliation>
<address confidence="0.998829">91403 Orsay,</address>
<email confidence="0.998729">zock@limsi.fr</email>
<abstract confidence="0.999481825136613">The aim of this talk is to show to what the work on does not address some of the fundamental problems with when generating We will substantiate this claim by taking two tasks on which a lot of research has been carried out during the last 15 years: planning 1 Discourse planning While a tremendous amount of work has been on the generation of if any has been devoted to As a result, many fundamental problems have been overlooked or have been dealt with on the basis of wrong assumptions. Also, little, if any of the achieved in the can be reused in the classroom or in the context of an intelligent writing-aid (tools for assisting the to structure her/his thoughts: Let us consider some of the reasons why this is so. Top-down processing: the community texts are generally processed top to Given some goal one looks for and integrate them. While this is a clever way to handle the problem, it does not give a precise reflection of the writers’ situation. First of all, it is not true always determined simultaneously, an assumption accepted since Moore &amp; Paris (1993). Secondly, writers geneswitch between (brainand (outlining). Thirdly, there is a triangular relationship changing any of them can affect the others. Yet, at present we do not have the fainidea what specific propositional or conceptual configuration (order of messages) might produce. • Lack of a Conceptual Structure Theory messages tend come to our mind in any and We have to later, and to the former in order to to the reader. is These last three points are crucial, yet none of the existing theories (schema, RST) is really able to take them into account. Just imagine how complex it is to recognize the fact that is a between two events. We have a solid theory of leave alone a method of operationalizing it (i.e. infer this kind of link solely on the basis of the intrinsic features of the events involved). • As we all know, texts have structure. This latter is generally the result of planning or RST-based) or of inferencing), in which case the structure emerges as a side effect. The major shortcoming of all these techniques is that they not model the the data messages), the the (all) the data to be communicated and the global discourse are generally given with the The problem of reconciling mismatches between and the problem of variable effects/goals a function of various strategies is not addressed at 2 Lexicalisation amounts mainly to has to matching a given conceptual chunk, and then one has to them. While much emphasis has given to the notion of far less has been paid to the mechanisms (or access strategies). I will present during my talk some preliminary results concerning a system that is meant to help people overcome the tongue a well known stumbling block in real-time we know want to say, we that we the word, yet we cannot (Brown and Mc Neill, 1966). If the fundamental role of a dictionary in NLG is obvious, it is less evident as to the principles governing its compilation. A good is a place with a of information, such a way that the relevant is easily needed. In words, what counts is and information is form, sound). These two are not sufficient though: only on the the lexicon but also on the 1While in Moore &amp; Paris (1993), the messages are not given, the goal is : it cannot emerge as a side effect. 2What shall we do if not all the data can be integrated, or if we lack data for filling all the slots of a chosen structure? Shall we keep the structure and look for more data, or use a different structure as it integrates more of the data? 3One of the reasons for this is that we do not have a clear understanding concerning the mapping different configurations corresponding If we did, we could use them bidirectionally (for analysis and generation). an issue not addressed at all by the generation community. As a matter of from a strict linguistic point the whole matter may be a non-issue. However, the problem does become relevant we look at generation as a machine-meprocess using a word processor writing) or from a of view: word access in writing or spontaneous discourse. The speaker’s problem: or Obviously, there is more lexicalisation than just one to to begin with. No matter how rich a lexical database may be, it is of little use if one cannot access the relevant information in probably problem that we have to cope with when trying to produce language in real-time (in spoken or written form). As I will show during my talk, this is precisely a point where computers can be of considerable help. on shown that depends crucially on the way information is organized, yet the latter can vary to a great From error literature learn, ease of access depends not only on i.e. the way words are mind),— but also on form phonemes). Researchers collecting speech errors offered countless examples of phonoerrors which segments (phonemes, syllables or words) are added, deleted, anticipated or exchanged (Fromkin, 1993). The data show that knowing the does not guarantee their The work on speech errors also reveals that are at least two modes, by by (written, and it is often this latter which inhibits finding the right token: having inadvertently recombined the components of a given word (syllable scrambone may end up producing a which either does not exist or is simply different from one in mind. This kind of resulting from bookkeeping problems (due to time pressure), parallel processing and information overload, may disturb or prevent the access of the right word. Hence the usefulness of a tool which allows the process to be reversed. In order to allow this to be done, it is necessary to represent words not only in terms their but also in terms of their and spoken The fact that words are both by by be used to our advantage. The coding of words allows the recombination of their segments (syllables), hence the presentation of new candidates, among which the user should the one s/he is looking The fact that are coded the number of candidates to be presented small. Conclusion I have tried to illustrate briefly to what extent we have neglected the human factor in our work. I have also attempted to show how a simple computational method (combinatorics and filtering) can be used to bridge (one of) the between text generation by people.</abstract>
<note confidence="0.7847255">References Brown and David Mc Neill. 1966. The the tongue Journal of Verbal Learning and Verbal Behavior, 5, 325-337</note>
<author confidence="0.82589475">Speech Production In by Jean Berko-Gleason</author>
<author confidence="0.82589475">Nan Bernstein Ratner Fort Worth</author>
<author confidence="0.82589475">TX Harcourt</author>
<author confidence="0.82589475">Jovanovich Brace</author>
<abstract confidence="0.891588">Johanna Moore and Cecile Paris. 1993. Planning text for advisory dialogues: capturing and rhetorical Computational Linguistics, 19(4). 4The assumption is that speakers produce words reasonably close to the target word. A fact that is supported by psycholinguistic evidence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roger Brown</author>
<author>David Mc Neill</author>
</authors>
<title>The tip of the tongue phenomenon.</title>
<date>1966</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>5</volume>
<pages>325--337</pages>
<marker>Brown, Neill, 1966</marker>
<rawString>Roger Brown and David Mc Neill. 1966. The tip of the tongue phenomenon. Journal of Verbal Learning and Verbal Behavior, 5, 325-337</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktoria Fromkin</author>
</authors>
<title>Speech Production. In Psycholinguistics edited by Jean Berko-Gleason &amp;</title>
<date>1993</date>
<publisher>Harcourt,</publisher>
<location>Nan Bernstein Ratner. Fort Worth, TX:</location>
<contexts>
<context position="6619" citStr="Fromkin, 1993" startWordPosition="1104" endWordPosition="1105"> my talk, this is precisely a point where computers can be of considerable help. Work on memory has shown that access depends crucially on the way information is organized, yet the latter can vary to a great extent. From speech error literature we learn, that ease of access depends not only on meaning relations,—, i.e. the way words are organized in our mind),— but also on linguistic form (letters, phonemes). Researchers collecting speech errors have offered countless examples of phonological errors in which segments (phonemes, syllables or words) are added, deleted, anticipated or exchanged (Fromkin, 1993). The data clearly show that knowing the meaning of words does not guarantee their access. The work on speech errors also reveals that words are stored in at least two modes, by meaning and by form (written, spoken), and it is often this latter which inhibits finding the right token: having inadvertently recombined the components of a given word (syllable scrambling), one may end up producing a word, which either does not exist or is simply different from the one in mind. This kind of recombination, resulting from bookkeeping problems (due to time pressure), parallel processing and information</context>
</contexts>
<marker>Fromkin, 1993</marker>
<rawString>Viktoria Fromkin. 1993. Speech Production. In Psycholinguistics edited by Jean Berko-Gleason &amp; Nan Bernstein Ratner. Fort Worth, TX: Harcourt, Brace, Jovanovich</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
<author>Cecile Paris</author>
</authors>
<title>Planning text for advisory dialogues: capturing intentional and rhetorical information.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="1537" citStr="Moore &amp; Paris (1993)" startWordPosition="252" endWordPosition="255">assroom or in the context of an intelligent writing-aid (tools for assisting the writer to structure her/his thoughts: outline planning). Let us consider some of the reasons why this is so. • Top-down processing: in the TGBCcommunity texts are generally processed top to bottom. Given some goal one looks for data (messages) and structures which integrate them. While this is a clever way to handle the problem, it does not give a precise reflection of the writers’ situation. First of all, it is not true that content and structure are always determined simultaneously, an assumption accepted since Moore &amp; Paris (1993). Secondly, writers generally switch between data-driven (brainstorming) and structure-driven processing (outlining). Thirdly, there is a triangular relationship between messages, structures and goals (or effects), changing any of them can affect the others. Yet, at present we do not have the faintest idea what effect(s) a specific propositional or conceptual configuration (order of messages) might produce. • Lack of a Conceptual Structure Theory (CST): messages tend come to our mind in any order and without exhibiting their potential links. We have to discover these later, and to reorganize t</context>
<context position="4548" citStr="Moore &amp; Paris (1993)" startWordPosition="748" endWordPosition="751">t access it (Brown and Mc Neill, 1966). If the fundamental role of a dictionary in NLG is obvious, it is less evident as to the principles governing its compilation. A good dictionary is a place with a lot of information, structured in such a way that the relevant information is easily accessible when needed. In other words, what counts is &apos;what is in the dictionary&apos; (content) and &apos;how the information is organized (meaning, form, sound). These two factors are not sufficient though: access depends not only on the structure of the lexicon (organisation), but also on the efficiency of 1 While in Moore &amp; Paris (1993), the messages are not given, the goal is : it cannot emerge as a side effect. 2 What shall we do if not all the data can be integrated, or if we lack data for filling all the slots of a chosen structure? Shall we keep the structure and look for more data, or use a different structure as it integrates more of the data? 3 One of the reasons for this is that we do not have a clear understanding concerning the mapping between different conceptual configurations and their corresponding rhetorical effect(s). If we did, we could use them bidirectionally (for analysis and generation). search strategi</context>
</contexts>
<marker>Moore, Paris, 1993</marker>
<rawString>Johanna Moore and Cecile Paris. 1993. Planning text for advisory dialogues: capturing intentional and rhetorical information. Computational Linguistics, 19(4).</rawString>
</citation>
<citation valid="false">
<title>4 The assumption is that speakers produce words that formwise are reasonably close to the target word. A fact that is supported by psycholinguistic evidence.</title>
<marker></marker>
<rawString>4 The assumption is that speakers produce words that formwise are reasonably close to the target word. A fact that is supported by psycholinguistic evidence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>