<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<bodyText confidence="0.219158333333333">
A STOCHASTIC APPROACH TO SENTENCE PARSING
Tetsunosuke Fujisaki
Science Institute, IBM Japan, Ltd.
No. 36 Kowa Building
5-19 Sanbancho,Chiyoda-ku
Tokyo 102, Japan
</bodyText>
<sectionHeader confidence="0.818213" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999965333333333">
A description will be given of a procedure to assign
the most likely probabilities to each of the rules
of a given context-free grammar. The grammar devel-
oped by S. Kuno at Harvard University was picked as
the basis and was successfully augmented with rule
probabilities. A brief exposition of the method
with some preliminary results, when used as a device
for disambiguating parsing English texts picked from
natural corpus, will be given.
</bodyText>
<sectionHeader confidence="0.994883" genericHeader="method">
I. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.994347896103896">
To prepare a grammar which can parse arbitrary sen-
tences taken from a natural corpus is a difficult
task. One of the most serious problems is the poten-
tially unbounded number of ambiguities. Pure syn-
tactic analysis with an imprudent grammar will
sometimes result in hundreds of parses.
With prepositional phrase attachments and conjunc-
tions, for example, it is known that the actual
growth of ambiguities can be approximated by a Cat-
lan number [Knuth], the number of ways to insert
parentheses into a formula of N terms: 1, 2, 5, 14,
42, 132, 469, 1430, 4892, ... The five ambiguities
in the following sentence with three ambiguous con-
structions can be well explained with this number.
I saw a man in a park with a scope.
This Catalan number is essentially exponential and
[Martin] reported a syntactically ambiguous sentence
with 455 parses:
List the sales of products produced in 1973
with the products produced in 1972.
On the other hand, throughout the long history of
natural language understanding work, semantic and
pragmatic constraints are known to be indispensable
and are recommended to be represented in some formal
way and to be referred to during or after the syntac-
tic analysis process.
However, to represent semantic and pragmatic con-
straints, (which are usually domain sensitive) in a
well-formed way is a very difficult and expensive
task. A lot of effort in that direction has been
expended, especially in Artificial Intelligence,
using semantic networks, frame theory, etc. Howev-
er, to our knowledge no one has ever succeeded in
preparing them except in relatively small restricted
domains. [Winograd, Sibuya].
Faced with this situation, we propose in this paper
to use statistics as a device for reducing ambigui-
ties. In other words, we propose a scheme for gram-
matical inference as defined by [Fu], a stochastic
augmentation of a given grammar; furthermore, we
propose to use the resultant statistics as a device
for semantic and pragmatic constraints. Within this
stochastic framework, semantic and pragmatic con-
straints are expected to be coded implicitly in the
statistics. A simple bottom-up parse referring to
the grammar rules as well as the statistics will
assign relative probabilities among ambiguous deri-
vations. And these relative probabilities should be
useful for filtering meaningless garbage parses
because high probabilities will be assigned to the
parse trees corresponding to meaningful interpreta-
tions and low probabilities, hopefully 0.0, to other
parse trees which are grammatically correct but are
not meaningful.
Most importantly, stochastic augmentation of a gram-
mar will be done automatically by feeding a set of
sentences as samples from the relevant domain in
which we are interested, while the preparation of
semantic and pragmatic constraints in the form of
usual semantic network, for example, should be done
by human experts for each specific domain.
This paper first introduces the basic ideas of auto-
matic training process of statistics from given
example sentences, and then shows how it works wit
experimental results.
II. GRAMMATICAL INFERENCE OF A STOCHASTIC GRAMMAR
A. Estimation of Markov Parameters for sample texts
Assume a Markov source model as a collection of
states connected to one another by transitions which
produce symbols from a finite alphabet. To each
transition, t from a state s, is associated a proba-
bility c(s,t), which is the probability that t will
be chosen next when s is reached.
When output sentences CB(i), from this markov model
are observed, we can estimate the transition proba-
bilities Cci(s,tl) through an iteration process in
the following way:
</bodyText>
<footnote confidence="0.606535">
1. Make an initial guess of Cci(s,t)l.
</footnote>
<page confidence="0.997174">
16
</page>
<bodyText confidence="0.904795571428571">
can be reached from the state S by applying a rule
S-&gt;NP VP, the state ART.NOUN.VP can be reached from
the state NP.VP by applying the rule NP-&gt;ART NOUN to
the first NP of the state NP.VP, and so on.
2. Parse each output sentence 8(i). Let d(i,j) be
a j-th derivation of the i-th output sentence
8(i).
</bodyText>
<listItem confidence="0.749382444444444">
3. Then the probability p(d(i,j)) of each deriva-
tion d(i,j) can be defined in the following way:
p(d(i,j)) is the product of probability of all
the transitions ci(s,t) which contribute to that
derivation d(i,j).
4. From this p(d(i,j)), the Bayes a posteriori
estimate of the count c(s,t,i,j), how many times
the transition t from state $ is used on the der-
ivation d(i,j), can be estimated as follows:
</listItem>
<equation confidence="0.384577">
n(s,t,i,j) x p(d(i,j))
E
p(d(i,j))
</equation>
<bodyText confidence="0.9699094">
where n(s,t,i,j) is a number of times the tran-
sition t from state s is used in the derivation
d(i,j).
Obviously, c(s,t,i,j) becomes n(s,t,i,j) in an
unambiguous case.
</bodyText>
<listItem confidence="0.9554165">
5. From this c(s,t,i,j), new estimate of the proba-
bilities f(s,t) can be calculated.
</listItem>
<sectionHeader confidence="0.615918" genericHeader="method">
EE c(s,t,i,j)
</sectionHeader>
<bodyText confidence="0.992963171428571">
ii
Since the derivations correspond to sequences of
state transitions among the states defined above,
parsing over the set of sentences given as training
data will enable us to count how many times each
transition is fired from the given sample sentences.
For example, transitions from the state S to the
state NP.VP may occur for almost every sentence
because the corresponding rule, &apos;S-&gt;NP VP&apos;, must be
used to derive the most frequent declarative sen-
tences; the transition from state ART.NOUN.VP to the
state &apos;every&apos;.NOUN.VP may happen 103 times; etc. If
we associate each grammar rule with an a priori
probability as an initial guess, then the Bayes a
posteriori estimate of the number of times each
transition will be traversed can be calculated from
the initial probabilities and the actual counts
observed as described above.
Since each production is expected to occur Independ-
ently of the context, the new estimate of the proba-
bility for a rule will be calculated at each
iteration step by masking the contexts. That is,
the Bayes estimate counts from all of the transi-
tions which correspond to a single context free
rule; all transitions between states like xxx.A.yyy
and xxx.B.C.yyy correspond to the production rule
&apos;A-&gt;B C&apos; regardless of the contents of xxx and yyy;
are tied together to get the new probability esti-
mate of the corresponding rule.
c(s,t,i,j ) =
f(s,t) =
EXEc(s,t,i,j) Renewing the probabilities of the rules with new
ii t estimates, the same steps will be repeated until
they converge.
III. EXPERIMENTATION
</bodyText>
<listItem confidence="0.9807555">
6. Replace Cd(s,t)) with this new estimate Cf(s,t))
and repeat from step 2.
</listItem>
<bodyText confidence="0.990891">
Through this process, asymptotic convergence will
hold in the entropy of Cci(s,t)) which is defined as:
</bodyText>
<subsectionHeader confidence="0.629248">
Entoropy = EE -q(s,t)xlog(q(s,t))
St
</subsectionHeader>
<bodyText confidence="0.975259518518519">
and the Cq(s,t)) will approach the real transition
probability [Baum-1970,1792].
Further optimized versions of this algorithm can be
found in [Bahl-1983] and have been successfully used
for estimating parameters of various Markov models
which approximate speech processes [Bahl - 1978,
1980].
B. Extension to context-free grammar&apos;
This procedure for automatically estimating Markov
source parameters can easily be extended to con-
text-free grammars in the following manner.
Assume that each state in the Markov model corre-
sponds to a possible sentential form based on a giv-
en context-free grammar. Then each transition
corresponds to the application of a context-free
production rule to the previous state, i.e. previ-
ous sentential form. For example, the state NP.VP
A. Base Grammar
As the basis of this research, the grammar developed
by Prof. S. Kuno in the 1960&apos;s for the machine trans-
lation project at Harvard University [Kuno-1963,
19661 was chosen, with few modifications. The set
of grammar specifications in that grammar, which are
in Greibach normal form, were translated into a form
which is favorable to our method. 2118 rules of the
original rules were rewritten as 5241 rules in Chom-
sky normal form.
</bodyText>
<subsectionHeader confidence="0.41098">
B. Parser
</subsectionHeader>
<bodyText confidence="0.999577125">
A bottom-up context-free parser based on Cocke-Kasa-
mi-Young algorithm was developed especially for this
purpose. Special emphasis was put on the design of
the parser to get better performance in highly
ambiguous cases. That is, alternative-links, the
dotted link shown in the figure below, are intro-
duced to reduce the number of intermediate substruc-
ture as far as possible.
</bodyText>
<page confidence="0.997247">
17
</page>
<sectionHeader confidence="0.591834" genericHeader="method">
C. Test Corpus
</sectionHeader>
<bodyText confidence="0.999726">
Training sentences were selected from the magazines,
31 articles from Reader&apos;s Digest and Datamation, and
from IBM correspondence. Among 5528 selected sen-
tences from the magazine articles, 3582 sentences
were successfully parsed with 0.89 seconds of CPU
time ( IBM 3033-UP ) and with 48.5 ambiguities per a
sentence. The average word lengths were 10.85 words
from this corpus.
From the corpus of IBM correspondence, 1001 sen-
tences, 12.65 words in length in average, were cho-
sen and 624 sentences were successfully parsed with
an average of 13.5 ambiguities.
</bodyText>
<sectionHeader confidence="0.759198" genericHeader="method">
D. Resultant Stochastic Context-free Grammar
</sectionHeader>
<bodyText confidence="0.981095684210526">
After a certain number of iterations, probabilities
were successfully associated to all of the grammar
rules and the lexical rules as shown below:
In the above list, (a) means that &amp;quot;HELP&amp;quot; will be gen-
erated from part-of-speech &amp;quot;IT4&amp;quot; with the probabili-
ty 0.98788, and (b) means that &amp;quot;SEE&amp;quot; will be
generated from part-of-speech &amp;quot;IT4&amp;quot; with the proba-
bility 0.00931. (c) means that the non-terminal &amp;quot;SE
(sentence)&amp;quot; will generate the sequence, &amp;quot;PRN (pro-
noun)&amp;quot;, &amp;quot;VX (predicate)&amp;quot; and &amp;quot;PD (period or post
sentential modifiers followed by period)&amp;quot; with the
probability 0.28754. (d) means that &amp;quot;SE&amp;quot; will gener-
ate the sequence, &amp;quot;AAA (article, adjective, etc.)&amp;quot; ,
&amp;quot;4X (subject noun phrase)&amp;quot;, &amp;quot;VX&amp;quot; and &amp;quot;PD&amp;quot; with the
probability 0.25530. The remaining lines are to be
interpreted similarly.
E. Parse Trees with Probabilities
Parse trees were printed as shown below including
relative probabilities of each parse.
</bodyText>
<table confidence="0.9902848">
WE DO NOT UTILIZE OUTSIDE ART SERVICES DIRECTLY .
** total ambiguity is : 3
*: SENTENCE
*: PRONOUN &apos;we&apos;
*: PREDICATE
*: AUXILIARY &apos;do&apos;
*: INFINITE VERB PHRASE
• ADVERB TYPE1 &apos;not&apos;
A: 0.356 INFINITE VERB PHRASE
*: VERB TYPE ITl&apos;utilize&apos;
*: OBJECT
*: NOUN &apos;outside&apos;
*: ADJ CLAUSE
*: NOUN &apos;art&apos;
*: PRED. WITH NO OBJECT
*: VERB TYPE VTI &apos;services&apos;
B 0.003 INFINITE VERB PHRASE
*: VERB TYPE ITl&apos;utilizel
*: OBJECT
*: PREPOSITION &apos;outside&apos;
*: NOUN OBJECT
*: NOUN &apos;art&apos;
*: OBJECT
*: NOUN &apos;services&apos;
C 0.641 INFINITE VERB PHRASE
*: VERB TYPE ITl&apos;utilize&apos;
*: OBJECT
*: NOUN &apos;outside&apos;
*: OBJECT MASTER
*: NOUN &apos;art&apos;
*: OBJECT MASTER
NOUN &apos;services&apos;
*: PERIOD
*: ADVERB TYPE1 &apos;directly&apos;
*: PRD V
</table>
<bodyText confidence="0.999549333333333">
This example shows that the sentence &apos;We do not uti-
lize outside art services directly.&apos; was parsed in
three different ways. The differences are shown as
the difference of the sub-trees identified by A, B
and C in the figure.
The numbers following the identifiers are the rela-
tive probabilities. As shown in this case, the cor-
rect parse, the third one, got the highest relative
probability, as was expected.
</bodyText>
<subsectionHeader confidence="0.824139">
F. Result
</subsectionHeader>
<bodyText confidence="0.9997644">
63 ambiguous sentences from magazine corpus and 21
ambiguous sentences from IBM correspondence were
chosen at random from the sample sentences and their
parse trees with probabilities were manually exam-
ined as shown in the table below:
</bodyText>
<table confidence="0.647355965517241">
*IT4
0.98788 HELP ---(a)
0.00931 SEE ---(b)
0.00141 HEAR
0.00139 WATCH
0.00000 HAVE
0.00000 FEEL
* SE
0.28754
0.25530
0.14856
0.13567
0.04006
0.02693
0.01714
0.01319
PRN VX PD ---(c)
AAA 4X VX PD ---(d)
NNN VX PD
AV1 SE
PRE NQ SE
AV4 IX MX PD
NUM 4X VX PD
IT1 N2 PD
VX 0.16295 VT1 N2
0.14372 VII
0.11963 AUX BV
0.10174 PRE NQ VX
0.09460 BE3 PA
</table>
<page confidence="0.740681">
18
</page>
<table confidence="0.947105">
a. Corpus Magazine IBM
b. Number of sentences 63 21
checked manually
c. Number of sentences 4 2
with no correct parse
d. Number of sentences 54 18
which got highest prob.
on most natural parse
e. Number of sentences 5 1
which did not get the
highest prob. on the
most natural parse
f. Success ratio d/(d+e) .915 .947
</table>
<bodyText confidence="0.99484105">
Taking into consideration that the grammar is not
tailored for this experiment in any way, the result
is quite satisfactory.
The only erroneous case of the IBM corpus is due to a
grammar problem. That is, in this grammar, such
modifier phrases as TO-infinitives, prepositional
phrases, adverbials, etc. after the main verb will
be derived from the &apos;end marker&apos; of the sentence,
i.e. period, rather than from the relevant constitu-
ent being modified. The parse tree in the previous
figure is a typical example, that is, the adverb
&apos;DIRECTLY&apos; is derived from the &apos;PERIOD&apos; rather than
from the verb &apos;UTILIZE&apos;. This simplified handling
of dependencies will not keep information between
modifying and modified phrases and as a result, will
cause problems where the dependencies have crucial
roles in the analysis. This error occurred in a sen-
tence &apos; ... is going to work out&apos;, where the two
interpretations for the phrase &apos;to work&apos; exist:
&apos;to work&apos; modifies &apos;period&apos; as:
</bodyText>
<listItem confidence="0.993128">
1. A TO-infinitive phrase
2. A prepositional phrase
</listItem>
<bodyText confidence="0.9999204">
Ignoring the relationship to the previous context
&apos;is going&apos;, the second interpretation got the higher
probability because prepositional phrases occur more
frequently than TO-infinitive phrases if the context
is not taken into account.
</bodyText>
<sectionHeader confidence="0.918125" genericHeader="method">
IV. CONCLUSION
</sectionHeader>
<bodyText confidence="0.999986947368421">
The result from the trials suggests the strong
potential of this method. And this also suggests
some application possibility of this method such as:
refining, minimizing, and optimizing a given con-
text-free grammar. It will be also useful for giv-
ing a disambiguation capability to a given ambiguous
context-free grammar.
In this experiment, an existing grammar was picked
with few modifications, therefore, only statistics
due to the syntactic differences of the sub-struc-
tured units were gathered. Applying this method to
the collection of statistics which relate more to
semantics should be investigated as the next step of
this project. Introduction into the grammar of a
dependency relationship among sub-structured units,
semantically categorized parts-of-speech, head word
inheritance among sub-structured units, etc. might
be essential for this purpose. More investigation
should be done on this direction.
</bodyText>
<sectionHeader confidence="0.9676" genericHeader="method">
V. ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.987107833333333">
This work was carried out when the author was in the
Computer Science Department of the IBM Thomas J.
Watson Research Center.
The author would like to thank Dr. John Cocke, Dr. F.
Jelinek, Dr. B. Mercer, Dr. L. Bahl of the IBM Thomas
J. Watson Research Center, and Prof. S. Kuno, of
Harvard University for their encouragement and valu-
able technical suggestions.
Also the author is indebted to Mr. E. Black, Mr. B.
Green and Mr. J Lutz for their assistance and dis-
cussions.
VII. REFERENCES
</bodyText>
<listItem confidence="0.764910195121951">
• Bahl,L.,Jelinek,F., and Mercer,R.,A Maximum
Likelihood Approarch to Continuous Speech Recog-
nition,Vol. PAMI-5,No.2, IEEE Trans. Pattern
Analysis and Machine Intelligence,1983
• Bahl,L.,et.al.,Automatic Recognition of Contin-
uously Spoken Sentences from a finite state
grammar, Proc. IEEE Int. Conf. Acoust., Speech,
Signal Processing, TUlsa, OK, Apr. 1978
• Bahl,L.,et.al.,Further results on the recogni-
tion of a continuousl read natural corpus, Proc.
IEEE Int. Conf. Acoust., Speech,Signal Process-
ing,Denver,CO,Apr. 1980
• Baum,L.E.,A Maximazation Technique occurring in
the Statistical Analysis of Probablistic Func-
tions of Markov Chains, Vol. 41, No.1, The
Annals of Mathematical Statistics, 1970
• Baum,L.E.,An Inequality and Associated Maximi-
zation Technique in Statistical Estimation for
ProbablisticFunctions.of Markov Processes, Ine-
qualities, Vol. 3, Academic Press, 1972
• Fu,K.S.,Syntactic Methods in Pattern Recogni-
tion,Vol 112, Mathematics in science and Engi-
neering, Academic Press, 1974
• Knuth,D.,Fundamental Algorithms,Vol 1. in The
Art of Computer Programming, Addison Wesley,
1975
• Kuno,S.,The Augmented Predictive Analyzer for
Context-free Languages-Its Relative Efficiency,
Vol. 9, No. 11, CACM, 1966
• Kuno,S.,Oettinger,A.G.,Syntactic Structure and
Ambiguity of English, Proc. FJCC, AFIPS, 1963
• Martin,W., et.al.,Preliminary Analysis of a
Breadth-First Parsing Algorithm: Theoretical and
Experimental Results, MIT LCS report TR-261, MIT
1981
• Sibuya,M.,Fujisaki,T. and Takao,Y.,Noun-Phrase
Model and Natural Query Language, Vol 22, No
5,IBM J. Res. Day. 1978
• Winograd,T.,Understanding Natural Language,
Academic Press, 1972
• Woods,W.,The Lunar Sciences Natural Language
</listItem>
<reference confidence="0.868634">
Information System, BBN Report No. 2378, Bolt,
Beranek and Newman
</reference>
<page confidence="0.999238">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.736542">
<title confidence="0.999538">A STOCHASTIC APPROACH TO SENTENCE PARSING</title>
<author confidence="0.983628">Tetsunosuke Fujisaki</author>
<affiliation confidence="0.997822">Science Institute, IBM Japan, Ltd.</affiliation>
<address confidence="0.928211666666667">No. 36 Kowa Building 5-19 Sanbancho,Chiyoda-ku Tokyo 102, Japan</address>
<abstract confidence="0.9850535">A description will be given of a procedure to assign the most likely probabilities to each of the rules of a given context-free grammar. The grammar developed by S. Kuno at Harvard University was picked as the basis and was successfully augmented with rule probabilities. A brief exposition of the method with some preliminary results, when used as a device for disambiguating parsing English texts picked from natural corpus, will be given.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Information System</author>
</authors>
<tech>BBN Report No. 2378,</tech>
<location>Bolt, Beranek and Newman</location>
<marker>System, </marker>
<rawString>Information System, BBN Report No. 2378, Bolt, Beranek and Newman</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>