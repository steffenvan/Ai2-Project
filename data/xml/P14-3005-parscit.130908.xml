<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9826335">
A Mapping-Based Approach for General Formal
Human Computer Interaction Using Natural Language
</title>
<author confidence="0.81841">
Vincent Letard
</author>
<affiliation confidence="0.275612">
LIMSI CNRS
</affiliation>
<email confidence="0.83702">
letard@limsi.fr
</email>
<note confidence="0.3757">
Sophie Rosset
LIMSI CNRS
</note>
<email confidence="0.861525">
rosset@limsi.fr
</email>
<note confidence="0.34627">
Gabriel Illouz
LIMSI CNRS
</note>
<email confidence="0.945341">
illouz@limsi.fr
</email>
<sectionHeader confidence="0.996453" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999525">
We consider the problem of mapping nat-
ural language written utterances express-
ing operational instructions1 to formal lan-
guage expressions, applied to French and
the R programming language. Developing
a learning operational assistant requires
the means to train and evaluate it, that is,
a baseline system able to interact with the
user. After presenting the guidelines of
our work, we propose a model to repre-
sent the problem and discuss the fit of di-
rect mapping methods to our task. Finally,
we show that, while not resulting in excel-
lent scores, a simple approach seems to be
sufficient to provide a baseline for an in-
teractive learning system.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896333333333">
Technical and theoretical advances allow achiev-
ing more and more powerful and efficient opera-
tions with the help of computers. However, this
does not necessarily make it easier to work with
the machine. Recent supervised learning work
(Allen et al., 2007; Volkova et al., 2013) exploited
the richness of human-computer interaction for
improving the efficiency of a human performed
task with the help of the computer.
Contrary to most of what was proposed so far,
our long term goal is to build an assistant system
learning from interaction to construct a correct for-
mal language (FL) command for a given natural
language (NL) utterance, see Table 1. However,
designing such a system requires data collection,
and early attempts highlighted the importance of
usability for the learning process: a system that is
hard to use (eg. having very poor performance)
</bodyText>
<footnote confidence="0.7647295">
1We call operational instruction the natural language ex-
pression of a command in any programming language.
</footnote>
<bodyText confidence="0.998808794117647">
would prevent from extracting useful learning ex-
amples from the interaction. We thus need to pro-
vide the system with a basis of abilities and knowl-
edge to allow both incremental design and to keep
the interest of the users, without which data turn
to be way more tedious to collect. We assume that
making the system usable requires the ability to
provide help to the user more often than it needs
help from him/her, that is an accuracy over 50%.
We hypothesize that a parametrized direct
mapping between the NL utterances and the FL
commands can reach that score. A knowledge set
K is built from parametrized versions of the asso-
ciations shown in Table 1. The NL utterance Ubest
from K that is the closest to the request-utterance
according to a similarity measure is chosen and its
associated command C(Ubest) is adapted to the
parameters of the request-utterance and returned.
For example, given the request-utterance UreQ:
”Load the file data.csv”, the system should rank
the utterances of K by similarity with UreQ. Con-
sidering the associations represented in Table 1,
the first utterance should be the best ranked, and
the system should return the command:
”var1 &lt;- read.csv(&amp;quot;data.csv&amp;quot;)”.
Note that several commands can be proposed at
the same time to give the user alternate choices.
We use Jaccard, tf-idf, and BLEU similarity
measures, and consider different selection strate-
gies. We highlight that the examined similarity
measures show enough complementarity to permit
the use of combination methods, like vote or sta-
tistical classification, to improve a posteriori the
efficiency of the retrieval.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.98853">
2.1 Mapping Natural Language to Formal
Language
</subsectionHeader>
<bodyText confidence="0.9212085">
Related problems have been previously processed
using different learning methods. Branavan (2009,
</bodyText>
<page confidence="0.991572">
34
</page>
<note confidence="0.505176">
Proceedings of the ACL 2014 Student Research Workshop, pages 34–40,
</note>
<affiliation confidence="0.462046">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
</affiliation>
<table confidence="0.998058875">
NL utterances FL commands (in R)
1 Charge les donn´ees depuis ” &amp;quot; var1=read.csv(&amp;quot;res.csv&amp;quot;)
res.csv
Load the data from ”res.csv”
2 p l o t (hist (tab [ [ 2 ] ]) )
Trace l’histogramme de la colonne 2 de tab
Draw a bar chart with column 2 of tab
3 p l o t (hist (tab [ [ 3 ] ]) )
Dessine la r´epartition de la colonne 3 de tab
Draw the distribution of column 3 of tab
4 var2=c (sum (tab [ 3 ]) ,sum (tab [ 4 ]) )
Somme les colonnes 3 et 4 de tab
Compute the sum of columns 3 and 4 of tab
5 var3=sum (c (tab [ [ 3 ] ] ,tab [ [ 4 ] ]) )
Somme les colonnes 3 et 4 de tab
Compute the sum of columns 3 and 4 of tab
</table>
<tableCaption confidence="0.999898">
Table 1: A sample of NL utterances to FL commands mapping
</tableCaption>
<bodyText confidence="0.999961066666667">
These examples specify the expected command to be returned for each utterance. The tokens in bold
font are linked with the commands parameters, cf. section 4. Note that the relation between utterances
and commands is a n to n. Several utterances can be associated to the same command and conversely.
2010) uses reinforcement learning to map En-
glish NL instructions to a sequence of FL com-
mands. The mapping takes high-level instructions
and their constitution into account. The scope
of usable commands is yet limited to graphical
interaction possibilities. As a result, the learn-
ing does not produce highly abstract schemes. In
the problematic of interactive continuous learning,
Artzi and Zettlemoyer (2011) build by learning a
semantic NL parser based on combinatory cate-
gorial grammars (CCG). Kushman and Barzilay
(2013) also use CCG in order to generate regu-
lar expressions corresponding to their NL descrip-
tions. This constructive approach by translation
allows to generalize over learning examples, while
the expressive power of regular expressions cor-
respond to the type-3 grammars of the Chomsky
hierarchy. This is not the case for the program-
ming languages since they are at least of type-2.
Yu and Siskind (2013) use hidden Markov mod-
els to learn a mapping between object tracks from
a video sequence and predicates extracted from
a NL description. The goal of their approach is
different from ours but the underlying problem of
finding a map between objects can be compared.
The matched objects constitute here a FL expres-
sion instead of a video sequence track.
</bodyText>
<subsectionHeader confidence="0.996785">
2.2 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999121466666667">
Machine translation usually refers to transforming
a NL sentence from a source language to another
sentence of the same significance in another natu-
ral language, called target language. This task is
achieved by building an intermediary representa-
tion of the sentence structure at a given level of
abstraction, and then encoding the obtained object
into the target language. While following a dif-
ferent goal, one of the tasks of the XLike project
(Marko Tadi´c et al., 2012) was to examine the
possibility of translating statements from NL (En-
glish) to FL (Cycl). Adapting such an approach
to operational formal target language can be inter-
esting to investigate, but we will not focus on that
track for our early goal.
</bodyText>
<subsectionHeader confidence="0.996747">
2.3 Information Retrieval
</subsectionHeader>
<bodyText confidence="0.999929304347826">
The issue of information retrieval systems can be
compared with the operational assistant’s (OA),
when browsing its knowledge. Question an-
swering systems in particular (Hirschman and
Gaizauskas, 2001), turn out to be similar to OA
since both types of systems have to respond to a
NL utterance of the user by generating an accu-
rate reaction (which is respectively a NL utterance
containing the wanted information, or the execu-
tion of a piece of FL code). However, as in (Toney
et al., 2008), questions answering systems usually
rely on text mining to retrieve the right informa-
tion. Such a method demands large sets of anno-
tated textual data (either by hand or using an au-
tomatic annotator). Yet, tutorials, courses or man-
uals which could be used in order to look for re-
sponses for operational assistant systems are het-
erogeneous and include complex or implicit ref-
erences to operational knowledge. This makes
the annotation of such data difficult. Text min-
ing methods are thus not yet applicable to oper-
ational assistant systems but could be considered
once some annotated data is collected.
</bodyText>
<page confidence="0.998601">
35
</page>
<sectionHeader confidence="0.990836" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.9999538125">
As we introduced in the first section, we represent
the knowledge K as a set of examples of a binary
relation R : NL → FL associating a NL utter-
ance to a FL command. If we consider the simple
case of a functional and injective relation, each
utterance is associated to exactly one command.
This is not realistic since it is possible to reformu-
late nearly any NL sentence. The case of a non in-
jective relation covers better the usual cases: each
command can be associated with one or more ut-
terances, this situation is illustrated by the second
and third examples of Table 1. Yet, the real-life
case should be a non injective nor functional rela-
tion. Not only multiple utterances can refer to a
same command, but one single utterance can also
stand for several distinct commands (see the fourth
and fifth examples2 in Table 1). We must consider
all these associations when matching a request-
utterance UTeq for command retrieval in K.
At this point, several strategies can be used to
determine what to return, with the help of the sim-
ilarity measure Q : NL × NL → R between two
NL utterances. Basically, we must determine if
a response should be given, and if so how many
commands to return. To do this, two potential
strategies can be considered for selecting the as-
sociated utterances in K.
The first choice focuses on the number of re-
sponses that are given for each request-utterance.
The n first commands according to the rankings of
their associated utterances in K are returned. The
rank r of a given utterance U is computed with:
</bodyText>
<equation confidence="0.998756">
r(U|Ureq) = I{U′ E K : v(Ureq, U′) &gt; v(Ureq, U)}I
(1)
</equation>
<bodyText confidence="0.999622666666667">
The second strategy choice can be done by de-
termining an absolute similarity threshold below
which the candidate utterances from K and their
associated sets of commands are considered too
different to match. The resulting set of commands
is given by:
</bodyText>
<equation confidence="0.873034">
Res = {C E FL : (U, C) E K, v(Ureq, U) &lt; t} (2)
</equation>
<bodyText confidence="0.966042857142857">
with t the selected threshold. Once selected the
set of commands to be given as response, if there
are more than one, the choice of the one to execute
can be done interactively with the help of the user.
2The command 4 returns a vector of the sums of each col-
umn, while the command 5 returns the sum of the columns as
a single integer.
</bodyText>
<sectionHeader confidence="0.994297" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999994071428571">
We are given a simple parsing result of both the ut-
terance and the command. The first step to address
is the acquisition of examples and the way to up-
date the knowledge. Then we examine the meth-
ods for retrieving a command from the knowledge
and a given request-utterance.
Correctly mapping utterances to commands re-
quires at least to take their respective parameters
into account (variable names, numeric values, and
quoted strings). We build generic representations
of utterances and commands by identifying the pa-
rameters in the knowledge example pair (see Ta-
ble 1), and use them to reconstruct the command
with the parameters of the request-utterance.
</bodyText>
<subsectionHeader confidence="0.990519">
4.1 Retrieving the Commands
</subsectionHeader>
<bodyText confidence="0.996688857142857">
We applied three textual similarity measures to
our model in order to compare their strengths and
weaknesses on our task: the Jaccard similarity co-
efficient (Jaccard index), a tf-idf (Term frequency-
inverse document frequency) aggregation, and the
BLEU (Bilingual Evaluation Understudy) mea-
sure.
</bodyText>
<subsubsectionHeader confidence="0.53691">
4.1.1 Jaccard index
</subsubsectionHeader>
<bodyText confidence="0.999845142857143">
The Jaccard index measures a similarity between
two sets valued in the same superset. For the
present case, we compare the set of words of the
input NL instruction and the one of the compared
candidate instruction, valued in the set of possible
tokens. The adapted formula for two sentences S1
and S2 results in:
</bodyText>
<equation confidence="0.9999055">
J(s1, s2) = |W (s1) ∩ W (s2)|(3)
|W(s1) ∪ W(s2)|
</equation>
<bodyText confidence="0.9998142">
where W(S) stands for the set of words of the
sentence S. The Jaccard index is a baseline to
compare co-occurences of unigrams, and should
be efficient mainly with corpora containing few
ambiguous examples.
</bodyText>
<subsectionHeader confidence="0.662749">
4.1.2 tf-idf
</subsectionHeader>
<bodyText confidence="0.9999335">
The tf-idf measure permits, given a word, to clas-
sify documents on its importance in each one, re-
garding its importance in the whole set. This mea-
sure should be helpful to avoid noise bias when it
comes from frequent terms in the corpus. Here,
the documents are the NL utterances from K, and
they are classified regarding the whole request-
utterance, or input sentence si. We then use the
</bodyText>
<page confidence="0.991644">
36
</page>
<bodyText confidence="0.997912">
following aggregation of the tf-idf values for each
word of si.
</bodyText>
<equation confidence="0.9713125">
tfidfS(si, sc) = 1 E tfidf(w, sc, S) (4)
|W(si) |w∈W(si)
</equation>
<bodyText confidence="0.815447333333333">
with S = {s|(s, com) E K}, where si is the input
sentence, sc E S is the compared sentence, and
where the tf-idf is given by:
</bodyText>
<equation confidence="0.9983735">
tfidf(w, sc, S) = f(w, sc)idf(w, S) (5)
idf (w, S) = log „|fs E S |I E s}|« (6)
</equation>
<bodyText confidence="0.999982285714286">
where at last f(w, s) is the frequency of the word
w in the sentence s. As we did for the Jaccard in-
dex, we performed the measures on both raw and
lemmatized words. On the other hand, getting rid
of the function words and closed class words is not
here mandatory since the tf-idf measure already
takes the global word frequency into account.
</bodyText>
<subsectionHeader confidence="0.374907">
4.1.3 The BLEU measure
</subsectionHeader>
<bodyText confidence="0.9995805">
The bilingual evaluation understudy algorithm
(Papineni et al., 2002) focuses on n-grams co-
occurrences. This algorithm can be used to dis-
card examples where the words ordering is too far
from the candidate. It computes a modified pre-
cision based on the ratio of the co-occurring n-
grams within candidate and reference sentences,
on the total size of the candidate normalized by n.
</bodyText>
<equation confidence="0.851946166666667">
maxsc∈S occ(grn, sc) (7)
grams(si, n)
where grams(s, n) = |s |− (n − 1) is the number
of n-grams in the sentence s and occ(grn,s) =
E
grn′∈s [grn = grn′] is the number of occur-
</equation>
<bodyText confidence="0.999945">
rences of the n-gram grn in s. BLEU also uses
a brevity penalty to prevent long sentences from
being too disadvantaged by the n-gram based pre-
cision formula. Yet, the scale of the length of the
instructions in our corpus is sufficiently reduced
not to require its use.
</bodyText>
<subsectionHeader confidence="0.986544">
4.2 Optimizing the similarity measure
</subsectionHeader>
<bodyText confidence="0.968181466666667">
We applied several combinations of filters to the
utterances compared before evaluating their sim-
ilarity. We can change the set of words taken
into account, discarding or not the non open-class
words3. Identified non-lexical references such as
3Open-class words include nouns, verbs, adjectives, ad-
verbs and interjections.
variable names, quoted character strings and nu-
meric values can also be discarded or transformed
to standard substitutes. Finally, we can apply or
not a lemmatization4 on lexical tokens.By discard-
ing non open-class words, keeping non-lexical ref-
erences and applying the lemmatization, the sec-
ond utterance of Table 1 would then become:
draw bar chart column xxVALxx xxVARxx
</bodyText>
<sectionHeader confidence="0.996905" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.983733">
5.1 Parsing
</subsectionHeader>
<bodyText confidence="0.998928">
The NL utterances first pass through an arith-
metic expression finder to completely tag them be-
fore the NL analyzer. They are then parsed us-
ing WMATCH, a generic rule-based engine for
language analysis developed by Olivier Galibert
(2009). This system is modular and dispose of
rules sets for both French and English. As an ex-
ample, the simplified parsing result of the first ut-
terance of Table 1 looks like:
</bodyText>
<figure confidence="0.942002571428571">
&lt;_operation&gt;
&lt;_action&gt; charge|_-V &lt;/_action&gt;
&lt;_det&gt; les &lt;/_det&gt;
&lt;_subs&gt; donn´ees|_-N &lt;/_subs&gt;
&lt;_prep&gt; depuis &lt;/_prep&gt;
&lt;_unk&gt; &amp;quot;res.csv&amp;quot; &lt;/_unk&gt;
&lt;/_operation&gt;
</figure>
<bodyText confidence="0.9939900625">
Words tagged as unknown are considered as po-
tential variable or function names. We also added
a preliminary rule to identify character strings and
count them among the possibly linked features of
the utterance. The commands are normalized by
inserting spaces between every non semantically
linked character pair and we identify numeric val-
ues, variable/function names and character strings
as features.
Only generative forms of the commands are
associated to utterances in the knowledge. This
form consists in a normalized command with unre-
solved references for every parameter linked with
the learning utterance. These references are re-
solved at the retrieving phase by matching with the
tokens of the request-utterance.
</bodyText>
<subsectionHeader confidence="0.997437">
5.2 Corpus Constitution
</subsectionHeader>
<bodyText confidence="0.981907333333333">
Our initial corpus consists in 605 associations be-
tween 553 unique NL utterances in French and
240 unique R commands.
</bodyText>
<footnote confidence="0.9800025">
4Lemmatization is the process of transforming a word to
its canonical form, or lemma, ignoring the inflections. It can
be performed with a set of rules or with a dictionary. The
developed system uses a dictionary.
</footnote>
<equation confidence="0.962611">
EPBLEU(si, S) =
grn∈si
</equation>
<page confidence="0.989394">
37
</page>
<bodyText confidence="0.999969333333333">
The low number of documents describing a
majority of R commands and their heterogeneity
make automatic example gathering not yet achiev-
able. These documentations are written for human
readers having global references on the task. Thus,
we added each example pair manually, making
sure that the element render all the example infor-
mation and that the format correspond to the cor-
pus specifications. Those specifications are meant
to be the least restrictive, that is: a NL utterance
must be written as to ask for the execution of the
associated R task. It therefore should be mostly
in the imperative form and reflect, for experienced
people, a usual way they would express the con-
cerned operation for non specialists.
</bodyText>
<subsectionHeader confidence="0.975963">
5.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99972175">
The measures that can contribute to a relevant
evaluation of the system depend on its purpose.
Precision and recall values of information retrieval
systems are computed as follows:
</bodyText>
<equation confidence="0.976808833333333">
# correct responses
P _ (8)
# responses given
# correct responses
R _ (9)
# responses in K
</equation>
<bodyText confidence="0.924706086956522">
Note that the recall value is not as important as for
information retrieval: assuming that the situation
showed by the fourth and fifth associations of Ta-
ble 1 are not usual5, there should be few different
valid commands for a given request-utterance, and
most of them should be equivalent. Moreover, the
number of responses given is fixed (so is the num-
ber of responses in K), the recall thus gives the
same information as the precision, with a linear
coefficient variation.
These formulae can be applied to the ”command
level”, that is measuring the accuracy of the sys-
tem in terms of its good command ratio. However,
the user satisfaction can be better measured at the
”utterance level” since it represents the finest gran-
ularity for the user experience. We define the ut-
terance precision uP as:
# correct utterances
uP _ (10)
# responses given
where ”# correct utterances” stands for the num-
ber of request-utterances for which the system pro-
vided at least one good command.
</bodyText>
<footnote confidence="0.742196">
5Increasing the tasks covering of the corpus will make
these collisions more frequent, but this hypothesis seems rea-
sonable for a first approach.
</footnote>
<sectionHeader confidence="0.994489" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999970666666667">
The system was tested on 10% of the corpus (61
associations). The set of known associations K
contains 85% of the corpus (514 associations), in-
stead of 90% in order to allow several distinct
drawings (40 were tested), and thus avoid too
much noise.
</bodyText>
<subsectionHeader confidence="0.99988">
6.1 Comparing similarity measures
</subsectionHeader>
<bodyText confidence="0.999354">
As shown in Table 2 the tf-idf measure outper-
forms the Jaccard and BLEU measures, whichever
filter combination is applied. The form of the ut-
terances in the corpus causes indeed the repetition
of a small set of words across the associations.
This can explain why the inverse document fre-
quency is that better.
</bodyText>
<table confidence="0.9834675">
non-lexical included not included
lemmatize yes no yes no
Jaccard 36.5 36.5 21.2 23.0
tf-idf 48.0 51.9 36.5 40.4
BLEU 30.8 32.7 26.9 30.8
chance 1.9
</table>
<tableCaption confidence="0.969991">
Table 2: Scores of precision by utterance (uP),
</tableCaption>
<bodyText confidence="0.979876857142857">
providing 3 responses for each request-utterance.
The lemmatization and the inclusion of non
open-class words (not shown here) does not seem
to have a clear influence on uP, whereas including
the non-lexical tokens allows a real improvement.
This behaviour must result from the low length av-
erage (7.5 words) of the utterances in the corpus.
</bodyText>
<figure confidence="0.988888444444445">
0.7
0.6
measure
tfidf
tfidf_inl
0.4
0.3
1 2 3 4 5 6 7 8 9
Number of responses
</figure>
<figureCaption confidence="0.998586">
Figure 1: Utterance precision (uP) for a fixed
</figureCaption>
<bodyText confidence="0.98438">
number of responses by utterance. The tfidf inl
curve includes the non-lexical tokens.
Note that uP is obtained with Equation 10, which
explains the increase of the precision along the
number of responses.
</bodyText>
<figure confidence="0.6718775">
Utterance precision (uP)
0.5
</figure>
<page confidence="0.997172">
38
</page>
<bodyText confidence="0.99857675">
Figure 1 shows the precision obtained with tfidf
while increasing the number of commands given
for each request-utterance. It comes out that it
is useful to propose at least 3 commands to the
user. It would not be interesting, though, to offer a
choice of more than 5 items, because the gain on
uP would be offset by the time penalty for retriev-
ing the good command among the proposals.
</bodyText>
<subsectionHeader confidence="0.999713">
6.2 Allowing silence
</subsectionHeader>
<bodyText confidence="0.99998875">
We also tested the strategy of fixing an absolute
threshold to decide between response and silence.
Given a request-utterance and an associated order-
ing of K according to Q, the system will remain
silent if the similarity of the best example in K is
below the defined threshold.
Surprisingly, it turned out that for every mea-
sure, the 6 best similar responses at least were all
wrong. This result seems to be caused by the ex-
istence, in the test set of commands uncovered by
K, of some very short utterances that contain only
one or two lexical tokens.
</bodyText>
<subsectionHeader confidence="0.940672">
6.3 Combinations
</subsectionHeader>
<bodyText confidence="0.95881240625">
Figure 2: Comparison of the combinations with
the tf-idf inl method. Oracle and actual vote are
done using tf-idf, Jaccard, and BLEU, with and
without non-lexical tokens. The training set for
learning is the result of a run on K.
Having tested several methods giving differ-
ent results, combining these methods can be very
interesting depending on their complementarity.
The oracle vote using the best response among
the 6 best methods shows an encouraging progres-
sion margin (cf. Figure 2). The actual vote it-
self outperforms the best method for giving up to
3 responses (reaching 50% for only 2 responses).
However, the curve position is less clear for more
responses, and tests must be performed on other
drawings of K to measure the noise influence.
The complementarity of the methods can also
be exploited by training a classification model to
identify when a method is better than the others.
We used the similarity values as features and the
measure that gave a good response as the refer-
ence class label (best similarity if multiple, and
”none” class if no good response). This setup was
tested with the support vector machines using lib-
svm (Chang and Lin, 2011) and results are shown
in Figure 2. As expected, machine learning per-
forms poorly on our tiny corpus. The accuracy
is under 20% and the system only learned when
to use the best method, and when to give no re-
sponse. Still, it manages to be competitive with
the best method and should be tested again with
more data and multiple drawings of K.
</bodyText>
<sectionHeader confidence="0.99737" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999987">
The simple mapping methods based on similar-
ity ranking showed up to 60% of utterance pre-
cision6 remaining below a reasonable level of user
sollicitation, which validate our prior hypothesis.
A lot of approaches can enhance that score, such
as adding or developing more suitable similarity
measures (Achananuparp et al., 2008), combining
learning and vote or learning to rerank utterances.
However, while usable as a baseline, these
methods only allow poor generalization and really
need more corpus to perform well. As we pointed
out, the non-functionality of the mapping relation
also introduces ambiguities that cannot be solved
using the only knowledge of the system.
Thanks to this baseline method, we are now able
to collect more data by developing an interactive
agent that can be both an intelligent assistant and
a crowdsourcing platform. We are currently de-
veloping a web interface for this purpose. Finally,
situated human computer interaction will allow the
real-time resolving of ambiguities met in the re-
trieval with the help of the user or with the use of
contextual information from the dialogue.
</bodyText>
<sectionHeader confidence="0.938718" genericHeader="acknowledgments">
Aknowledgements
</sectionHeader>
<bodyText confidence="0.99995075">
The authors are grateful to every internal and ex-
ternal reviewer for their valuable advices. We also
would like to thank Google for the financial sup-
port for the authors participation to the conference.
</bodyText>
<footnote confidence="0.818396">
6The corpus will soon be made available.
</footnote>
<figure confidence="0.997596307692308">
0.8
method
vote
tfidf_inl
vote_oracle
learning
Utterance precision (uP)
0.6
0.4
0.2
0.0
1 2 3 4 5 6 7 8 9
Number of responses
</figure>
<page confidence="0.997038">
39
</page>
<sectionHeader confidence="0.998167" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735646153846">
Palakorn Achananuparp, Xiaohua Hu, and Xiajiong
Shen. 2008. The Evaluation of Sentence Similar-
ity Measures. In Data Warehousing and Knowledge
Discovery, Springer.
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Tayson. 2007. PLOW: A Collaborative
Task Learning Agent. In Proceedings of the 22nd
National Conference on Artificial Intelligence.
Yoav Artzi, and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsersfrom conversations. Pro-
ceedings of the conference on empirical methods in
natural language processing.
S.R.K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading Between the Lines: Learn-
ing to Map High-level Instructions to Commands. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP.
Chih-Chung Chang, and Chih-Jen Lin. 2011. LIB-
SVM: A Libraryfor Support Vector Machines. ACM
Transactions on Intelligent Systems and Technology
Olivier Galibert. 2009. Approches et m´ethodologies
pour la r´eponse automatique a` des questions
adapt´ees a` un cadre interactif en domaine ouvert.
Doctoral dissertation, Universit´e Paris Sud XI.
Lynette Hirschman, and Robert Gaizauskas. 2001.
Natural language question answering: The view
from here. Natural Language Engineering 7. Cam-
bridge University Press.
Nate Kushman, and Regina Barzilay. 2013. Using Se-
mantic Unification to Generate Regular Expressions
from Natural Language. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Marko Tadi´c, Boˇzo Bekavac, ˇZeljko Agi´c, Matea
Srebaˇci´c, Daˇsa Berovi´c, and Danijela Merkler.
2012. Early machine translation based semantic an-
notation prototype XLike project www.xlike.org .
Dave Toney, Sophie Rosset, Aur´elien Max, Olivier
Galibert, and ´eric Billinski. 2008. An Evaluation of
Spoken and Textual Interaction on the RITEL Inter-
active Question Answering System In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation.
Svitlana Volkova, Pallavi Choudhury, Chris Quirk, Bill
Dolan, and Luke Zettlemoyer. 2013. Lightly Su-
pervised Learning of Procedural Dialog System In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics.
Haonan Yu, and Jeffrey Mark Siskind. 2013.
Grounded Language Learning from Video De-
scribed with Sentences. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998633">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.051079">
<title confidence="0.9903165">A Mapping-Based Approach for General Formal Human Computer Interaction Using Natural Language</title>
<author confidence="0.889466">Vincent</author>
<affiliation confidence="0.466398">LIMSI</affiliation>
<email confidence="0.797267">letard@limsi.fr</email>
<author confidence="0.499107">Sophie</author>
<affiliation confidence="0.30069">LIMSI</affiliation>
<email confidence="0.749954">rosset@limsi.fr</email>
<author confidence="0.908495">Gabriel</author>
<affiliation confidence="0.526976">LIMSI</affiliation>
<email confidence="0.903109">illouz@limsi.fr</email>
<abstract confidence="0.999400352941177">We consider the problem of mapping natural language written utterances expressoperational to formal language expressions, applied to French and the R programming language. Developing a learning operational assistant requires the means to train and evaluate it, that is, a baseline system able to interact with the user. After presenting the guidelines of our work, we propose a model to represent the problem and discuss the fit of direct mapping methods to our task. Finally, we show that, while not resulting in excellent scores, a simple approach seems to be sufficient to provide a baseline for an interactive learning system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Palakorn Achananuparp</author>
<author>Xiaohua Hu</author>
<author>Xiajiong Shen</author>
</authors>
<title>The Evaluation of Sentence Similarity Measures.</title>
<date>2008</date>
<booktitle>In Data Warehousing and Knowledge Discovery,</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="22573" citStr="Achananuparp et al., 2008" startWordPosition="3836" endWordPosition="3839">orms poorly on our tiny corpus. The accuracy is under 20% and the system only learned when to use the best method, and when to give no response. Still, it manages to be competitive with the best method and should be tested again with more data and multiple drawings of K. 7 Conclusion and Future Work The simple mapping methods based on similarity ranking showed up to 60% of utterance precision6 remaining below a reasonable level of user sollicitation, which validate our prior hypothesis. A lot of approaches can enhance that score, such as adding or developing more suitable similarity measures (Achananuparp et al., 2008), combining learning and vote or learning to rerank utterances. However, while usable as a baseline, these methods only allow poor generalization and really need more corpus to perform well. As we pointed out, the non-functionality of the mapping relation also introduces ambiguities that cannot be solved using the only knowledge of the system. Thanks to this baseline method, we are now able to collect more data by developing an interactive agent that can be both an intelligent assistant and a crowdsourcing platform. We are currently developing a web interface for this purpose. Finally, situate</context>
</contexts>
<marker>Achananuparp, Hu, Shen, 2008</marker>
<rawString>Palakorn Achananuparp, Xiaohua Hu, and Xiajiong Shen. 2008. The Evaluation of Sentence Similarity Measures. In Data Warehousing and Knowledge Discovery, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Nathanael Chambers</author>
<author>George Ferguson</author>
<author>Lucian Galescu</author>
<author>Hyuckchul Jung</author>
<author>Mary Swift</author>
<author>William Tayson</author>
</authors>
<title>PLOW: A Collaborative Task Learning Agent.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1146" citStr="Allen et al., 2007" startWordPosition="175" endWordPosition="178">ystem able to interact with the user. After presenting the guidelines of our work, we propose a model to represent the problem and discuss the fit of direct mapping methods to our task. Finally, we show that, while not resulting in excellent scores, a simple approach seems to be sufficient to provide a baseline for an interactive learning system. 1 Introduction Technical and theoretical advances allow achieving more and more powerful and efficient operations with the help of computers. However, this does not necessarily make it easier to work with the machine. Recent supervised learning work (Allen et al., 2007; Volkova et al., 2013) exploited the richness of human-computer interaction for improving the efficiency of a human performed task with the help of the computer. Contrary to most of what was proposed so far, our long term goal is to build an assistant system learning from interaction to construct a correct formal language (FL) command for a given natural language (NL) utterance, see Table 1. However, designing such a system requires data collection, and early attempts highlighted the importance of usability for the learning process: a system that is hard to use (eg. having very poor performan</context>
</contexts>
<marker>Allen, Chambers, Ferguson, Galescu, Jung, Swift, Tayson, 2007</marker>
<rawString>James Allen, Nathanael Chambers, George Ferguson, Lucian Galescu, Hyuckchul Jung, Mary Swift, and William Tayson. 2007. PLOW: A Collaborative Task Learning Agent. In Proceedings of the 22nd National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>Bootstrapping semantic parsersfrom conversations.</title>
<date>2011</date>
<booktitle>Proceedings of the conference on empirical methods in natural language processing.</booktitle>
<contexts>
<context position="5162" citStr="Artzi and Zettlemoyer (2011)" startWordPosition="864" endWordPosition="867">e tokens in bold font are linked with the commands parameters, cf. section 4. Note that the relation between utterances and commands is a n to n. Several utterances can be associated to the same command and conversely. 2010) uses reinforcement learning to map English NL instructions to a sequence of FL commands. The mapping takes high-level instructions and their constitution into account. The scope of usable commands is yet limited to graphical interaction possibilities. As a result, the learning does not produce highly abstract schemes. In the problematic of interactive continuous learning, Artzi and Zettlemoyer (2011) build by learning a semantic NL parser based on combinatory categorial grammars (CCG). Kushman and Barzilay (2013) also use CCG in order to generate regular expressions corresponding to their NL descriptions. This constructive approach by translation allows to generalize over learning examples, while the expressive power of regular expressions correspond to the type-3 grammars of the Chomsky hierarchy. This is not the case for the programming languages since they are at least of type-2. Yu and Siskind (2013) use hidden Markov models to learn a mapping between object tracks from a video sequen</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Yoav Artzi, and Luke S. Zettlemoyer. 2011. Bootstrapping semantic parsersfrom conversations. Proceedings of the conference on empirical methods in natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reading Between the Lines: Learning to Map High-level Instructions to Commands.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S.R.K. Branavan, Luke S. Zettlemoyer, and Regina Barzilay. 2010. Reading Between the Lines: Learning to Map High-level Instructions to Commands. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement Learning for Mapping Instructions to Actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. 2009. Reinforcement Learning for Mapping Instructions to Actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Libraryfor Support Vector Machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology</journal>
<contexts>
<context position="21877" citStr="Chang and Lin, 2011" startWordPosition="3716" endWordPosition="3719">ing up to 3 responses (reaching 50% for only 2 responses). However, the curve position is less clear for more responses, and tests must be performed on other drawings of K to measure the noise influence. The complementarity of the methods can also be exploited by training a classification model to identify when a method is better than the others. We used the similarity values as features and the measure that gave a good response as the reference class label (best similarity if multiple, and ”none” class if no good response). This setup was tested with the support vector machines using libsvm (Chang and Lin, 2011) and results are shown in Figure 2. As expected, machine learning performs poorly on our tiny corpus. The accuracy is under 20% and the system only learned when to use the best method, and when to give no response. Still, it manages to be competitive with the best method and should be tested again with more data and multiple drawings of K. 7 Conclusion and Future Work The simple mapping methods based on similarity ranking showed up to 60% of utterance precision6 remaining below a reasonable level of user sollicitation, which validate our prior hypothesis. A lot of approaches can enhance that s</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang, and Chih-Jen Lin. 2011. LIBSVM: A Libraryfor Support Vector Machines. ACM Transactions on Intelligent Systems and Technology</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Galibert</author>
</authors>
<title>Approches et m´ethodologies pour la r´eponse automatique a` des questions adapt´ees a` un cadre interactif en domaine ouvert. Doctoral dissertation,</title>
<date>2009</date>
<institution>Universit´e Paris Sud XI.</institution>
<contexts>
<context position="14641" citStr="Galibert (2009)" startWordPosition="2504" endWordPosition="2505">ter strings and numeric values can also be discarded or transformed to standard substitutes. Finally, we can apply or not a lemmatization4 on lexical tokens.By discarding non open-class words, keeping non-lexical references and applying the lemmatization, the second utterance of Table 1 would then become: draw bar chart column xxVALxx xxVARxx 5 Experimental Setup 5.1 Parsing The NL utterances first pass through an arithmetic expression finder to completely tag them before the NL analyzer. They are then parsed using WMATCH, a generic rule-based engine for language analysis developed by Olivier Galibert (2009). This system is modular and dispose of rules sets for both French and English. As an example, the simplified parsing result of the first utterance of Table 1 looks like: &lt;_operation&gt; &lt;_action&gt; charge|_-V &lt;/_action&gt; &lt;_det&gt; les &lt;/_det&gt; &lt;_subs&gt; donn´ees|_-N &lt;/_subs&gt; &lt;_prep&gt; depuis &lt;/_prep&gt; &lt;_unk&gt; &amp;quot;res.csv&amp;quot; &lt;/_unk&gt; &lt;/_operation&gt; Words tagged as unknown are considered as potential variable or function names. We also added a preliminary rule to identify character strings and count them among the possibly linked features of the utterance. The commands are normalized by inserting spaces between every</context>
</contexts>
<marker>Galibert, 2009</marker>
<rawString>Olivier Galibert. 2009. Approches et m´ethodologies pour la r´eponse automatique a` des questions adapt´ees a` un cadre interactif en domaine ouvert. Doctoral dissertation, Universit´e Paris Sud XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Natural language question answering: The view from here. Natural Language Engineering 7.</title>
<date>2001</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6995" citStr="Hirschman and Gaizauskas, 2001" startWordPosition="1160" endWordPosition="1163"> then encoding the obtained object into the target language. While following a different goal, one of the tasks of the XLike project (Marko Tadi´c et al., 2012) was to examine the possibility of translating statements from NL (English) to FL (Cycl). Adapting such an approach to operational formal target language can be interesting to investigate, but we will not focus on that track for our early goal. 2.3 Information Retrieval The issue of information retrieval systems can be compared with the operational assistant’s (OA), when browsing its knowledge. Question answering systems in particular (Hirschman and Gaizauskas, 2001), turn out to be similar to OA since both types of systems have to respond to a NL utterance of the user by generating an accurate reaction (which is respectively a NL utterance containing the wanted information, or the execution of a piece of FL code). However, as in (Toney et al., 2008), questions answering systems usually rely on text mining to retrieve the right information. Such a method demands large sets of annotated textual data (either by hand or using an automatic annotator). Yet, tutorials, courses or manuals which could be used in order to look for responses for operational assista</context>
</contexts>
<marker>Hirschman, Gaizauskas, 2001</marker>
<rawString>Lynette Hirschman, and Robert Gaizauskas. 2001. Natural language question answering: The view from here. Natural Language Engineering 7. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Kushman</author>
<author>Regina Barzilay</author>
</authors>
<title>Using Semantic Unification to Generate Regular Expressions from Natural Language.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5277" citStr="Kushman and Barzilay (2013)" startWordPosition="882" endWordPosition="885">ces and commands is a n to n. Several utterances can be associated to the same command and conversely. 2010) uses reinforcement learning to map English NL instructions to a sequence of FL commands. The mapping takes high-level instructions and their constitution into account. The scope of usable commands is yet limited to graphical interaction possibilities. As a result, the learning does not produce highly abstract schemes. In the problematic of interactive continuous learning, Artzi and Zettlemoyer (2011) build by learning a semantic NL parser based on combinatory categorial grammars (CCG). Kushman and Barzilay (2013) also use CCG in order to generate regular expressions corresponding to their NL descriptions. This constructive approach by translation allows to generalize over learning examples, while the expressive power of regular expressions correspond to the type-3 grammars of the Chomsky hierarchy. This is not the case for the programming languages since they are at least of type-2. Yu and Siskind (2013) use hidden Markov models to learn a mapping between object tracks from a video sequence and predicates extracted from a NL description. The goal of their approach is different from ours but the underl</context>
</contexts>
<marker>Kushman, Barzilay, 2013</marker>
<rawString>Nate Kushman, and Regina Barzilay. 2013. Using Semantic Unification to Generate Regular Expressions from Natural Language. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12880" citStr="Papineni et al., 2002" startWordPosition="2209" endWordPosition="2212">m) E K}, where si is the input sentence, sc E S is the compared sentence, and where the tf-idf is given by: tfidf(w, sc, S) = f(w, sc)idf(w, S) (5) idf (w, S) = log „|fs E S |I E s}|« (6) where at last f(w, s) is the frequency of the word w in the sentence s. As we did for the Jaccard index, we performed the measures on both raw and lemmatized words. On the other hand, getting rid of the function words and closed class words is not here mandatory since the tf-idf measure already takes the global word frequency into account. 4.1.3 The BLEU measure The bilingual evaluation understudy algorithm (Papineni et al., 2002) focuses on n-grams cooccurrences. This algorithm can be used to discard examples where the words ordering is too far from the candidate. It computes a modified precision based on the ratio of the co-occurring ngrams within candidate and reference sentences, on the total size of the candidate normalized by n. maxsc∈S occ(grn, sc) (7) grams(si, n) where grams(s, n) = |s |− (n − 1) is the number of n-grams in the sentence s and occ(grn,s) = E grn′∈s [grn = grn′] is the number of occurrences of the n-gram grn in s. BLEU also uses a brevity penalty to prevent long sentences from being too disadvan</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marko Tadi´c</author>
</authors>
<title>Boˇzo Bekavac, ˇZeljko Agi´c, Matea Srebaˇci´c, Daˇsa Berovi´c, and Danijela Merkler.</title>
<date>2012</date>
<marker>Tadi´c, 2012</marker>
<rawString>Marko Tadi´c, Boˇzo Bekavac, ˇZeljko Agi´c, Matea Srebaˇci´c, Daˇsa Berovi´c, and Danijela Merkler. 2012. Early machine translation based semantic annotation prototype XLike project www.xlike.org .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Toney</author>
<author>Sophie Rosset</author>
<author>Aur´elien Max</author>
<author>Olivier Galibert</author>
<author>´eric Billinski</author>
</authors>
<date>2008</date>
<booktitle>An Evaluation of Spoken and Textual Interaction on the RITEL Interactive Question Answering System In Proceedings of the Sixth International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="7284" citStr="Toney et al., 2008" startWordPosition="1215" endWordPosition="1218">get language can be interesting to investigate, but we will not focus on that track for our early goal. 2.3 Information Retrieval The issue of information retrieval systems can be compared with the operational assistant’s (OA), when browsing its knowledge. Question answering systems in particular (Hirschman and Gaizauskas, 2001), turn out to be similar to OA since both types of systems have to respond to a NL utterance of the user by generating an accurate reaction (which is respectively a NL utterance containing the wanted information, or the execution of a piece of FL code). However, as in (Toney et al., 2008), questions answering systems usually rely on text mining to retrieve the right information. Such a method demands large sets of annotated textual data (either by hand or using an automatic annotator). Yet, tutorials, courses or manuals which could be used in order to look for responses for operational assistant systems are heterogeneous and include complex or implicit references to operational knowledge. This makes the annotation of such data difficult. Text mining methods are thus not yet applicable to operational assistant systems but could be considered once some annotated data is collecte</context>
</contexts>
<marker>Toney, Rosset, Max, Galibert, Billinski, 2008</marker>
<rawString>Dave Toney, Sophie Rosset, Aur´elien Max, Olivier Galibert, and ´eric Billinski. 2008. An Evaluation of Spoken and Textual Interaction on the RITEL Interactive Question Answering System In Proceedings of the Sixth International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Pallavi Choudhury</author>
<author>Chris Quirk</author>
<author>Bill Dolan</author>
<author>Luke Zettlemoyer</author>
</authors>
<date>2013</date>
<booktitle>Lightly Supervised Learning of Procedural Dialog System In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1169" citStr="Volkova et al., 2013" startWordPosition="179" endWordPosition="182">ct with the user. After presenting the guidelines of our work, we propose a model to represent the problem and discuss the fit of direct mapping methods to our task. Finally, we show that, while not resulting in excellent scores, a simple approach seems to be sufficient to provide a baseline for an interactive learning system. 1 Introduction Technical and theoretical advances allow achieving more and more powerful and efficient operations with the help of computers. However, this does not necessarily make it easier to work with the machine. Recent supervised learning work (Allen et al., 2007; Volkova et al., 2013) exploited the richness of human-computer interaction for improving the efficiency of a human performed task with the help of the computer. Contrary to most of what was proposed so far, our long term goal is to build an assistant system learning from interaction to construct a correct formal language (FL) command for a given natural language (NL) utterance, see Table 1. However, designing such a system requires data collection, and early attempts highlighted the importance of usability for the learning process: a system that is hard to use (eg. having very poor performance) 1We call operationa</context>
</contexts>
<marker>Volkova, Choudhury, Quirk, Dolan, Zettlemoyer, 2013</marker>
<rawString>Svitlana Volkova, Pallavi Choudhury, Chris Quirk, Bill Dolan, and Luke Zettlemoyer. 2013. Lightly Supervised Learning of Procedural Dialog System In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounded Language Learning from Video Described with Sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5676" citStr="Yu and Siskind (2013)" startWordPosition="947" endWordPosition="950">ighly abstract schemes. In the problematic of interactive continuous learning, Artzi and Zettlemoyer (2011) build by learning a semantic NL parser based on combinatory categorial grammars (CCG). Kushman and Barzilay (2013) also use CCG in order to generate regular expressions corresponding to their NL descriptions. This constructive approach by translation allows to generalize over learning examples, while the expressive power of regular expressions correspond to the type-3 grammars of the Chomsky hierarchy. This is not the case for the programming languages since they are at least of type-2. Yu and Siskind (2013) use hidden Markov models to learn a mapping between object tracks from a video sequence and predicates extracted from a NL description. The goal of their approach is different from ours but the underlying problem of finding a map between objects can be compared. The matched objects constitute here a FL expression instead of a video sequence track. 2.2 Machine Translation Machine translation usually refers to transforming a NL sentence from a source language to another sentence of the same significance in another natural language, called target language. This task is achieved by building an in</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu, and Jeffrey Mark Siskind. 2013. Grounded Language Learning from Video Described with Sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>