<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032117">
<title confidence="0.946622">
Multi-modal Visualization and Search for Text and Prosody Annotations
</title>
<author confidence="0.993544">
Markus G¨artner Katrin Schweitzer Kerstin Eckart Jonas Kuhn
</author>
<affiliation confidence="0.9961345">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.991558">
{markus.gaertner,kati,eckartkn,kuhn}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.997289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960428571429">
We present ICARUS for intonation, an in-
teractive tool to browse and search au-
tomatically derived descriptions of fun-
damental frequency contours. It offers
access to tonal features in combination
with other annotation layers like part-of-
speech, syntax or coreference and visual-
izes them in a highly customizable graphi-
cal interface with various playback func-
tions. The built-in search allows multi-
level queries, the construction of which
can be done graphically or textually, and
includes the ability to search F0 contours
based on various similarity measures.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.960339050847458">
In this paper we present ICARUS for intonation,
a new module for the query and visualization tool
ICARUS by G¨artner et al. (2013). 1
So far, ICARUS included modules for the han-
dling of dependency treebanks (G¨artner et al.,
2013) and coreference data (G¨artner et al., 2014),
thus supporting typical annotation layers from the
processing of written data. However, the graphi-
cal query builder and the intuitive example-based
search could prove just as expedient for other
types of data, such as speech corpora, transcribed
and annotated for sub word features. This also al-
lows combined research on speech and text data,
e.g. the analysis of different tonal realizations of a
certain syntactic structure.
ICARUS for intonation allows to import
syllable-based prosodic features into ICARUS,
which can then be visualized and queried either
1ICARUS for intonation is written in Java and is
therefore platform independent. It is open source (un-
der GNU GPL) and we provide both sources and binaries
for download on http://www.ims.uni-stuttgart.
de/data/icarus.html
individually or in a combined search with e.g. syn-
tactic features or coreference information. The
latter targets several user groups: speech data ex-
perts can adjust fine-grained settings on pitch ac-
cent shapes in their queries and can easily add con-
straints on part-of-speech or syntax information,
while an expert user of dependency treebanks can
get a simple visualization of the intonation contour
of a sentence.
Furthermore ICARUS focuses on automatic an-
notations to allow for search on large data sets.
Thus ICARUS for intonation’s main features for
prosodic search are based on PaIntE, a parametric
intonation model (M¨ohler, 1998; M¨ohler, 2001).
So far, most data in intonation research is man-
ually annotated, which is a very time consuming
task: the time for annotating speech data is many
times higher than the real time of the audio record-
ing. For example the Tones and Break Indices
(ToBI) system for American English (Beckman
and Hirschberg, 1999) takes experienced annota-
tors about 100-200 times the real time (Syrdal et
al., 2001). While manual annotations for pitch
accents and prosodic phrase boundaries can also
be imported, our main goal with this module is
to provide intonation researchers with a customiz-
able tool to conduct thorough studies on very large
sets of only automatically annotated speech data.
In Sections 2 and 3 we introduce the PaIntE
model and describe the current input format for
the data importer. Section 4 demonstrates several
visualization functionalities, and Section 5 dis-
cusses the search facilities, including dependency
and intonation as well as coreference and intona-
tion queries. After discussing some related work
in Section 6 we conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.997149" genericHeader="method">
2 The PaIntE Model
</sectionHeader>
<bodyText confidence="0.987480666666667">
The PaIntE model (M¨ohler, 1998; M¨ohler, 2001)
approximates a peak in the FO contour by em-
ploying a model function operating on a 3-syllable
</bodyText>
<page confidence="0.986733">
25
</page>
<note confidence="0.752908">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 25–30,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<figure confidence="0.523356">
time (syllable−normalized)
</figure>
<figureCaption confidence="0.999572">
Figure 1: The PaIntE model function and its pa-
rameters. Figure adapted from (M¨ohler, 2001).
</figureCaption>
<bodyText confidence="0.999297">
window. There are 6 free parameters in the func-
tion term which are set by the model so that the
actual Fo shape is fit best. They are linguistically
meaningful: parameter b locates the peak within
the 3-syllable window, parameter d encodes its ab-
solute height. The remaining parameters specify
the steepness and amplitude of the rise before, and
the fall after the peak (parameters a1 and a2 for
the steepness and c1/c2 for the amplitude).
Figure 1 illustrates the function. It displays the
syllable for which the parametrization is carried
out (σ∗) and its immediate neighbors. The x-axis
indicates time (normalized for syllable duration,
the current syllable spans from 0 to 1) and the y-
axis displays the fundamental frequency in Hertz.
The PaIntE model has been used for the model-
ing of different languages, e.g. Norwegian, Ital-
ian, German and English (Cosi et al., 2002; Kelly
and Schweitzer, in press; Schweitzer et al., 2015).
</bodyText>
<sectionHeader confidence="0.986541" genericHeader="method">
3 Data Representation
</sectionHeader>
<bodyText confidence="0.999337875">
ICARUS for intonation ships with reader imple-
mentations for two very different formats. One
is an extended version of the format used for the
2011 and 2012 CoNLL shared tasks (Pradhan et
al., 2011; Pradhan et al., 2012) with a number of
additional columns to accommodate features for
the syllable level. This format stores all annota-
tions corresponding to a word token in one line
and packs syllable features into a list separated
by pipe-characters (’|’). To address syllable cen-
tric data like the typical output of speech process-
ing systems, a second flexible tabular format was
specified where each line of text corresponds to a
single syllable and a global header describes the
content of all columns and how to read and map
them to the internal data model of ICARUS.
</bodyText>
<figureCaption confidence="0.973461333333333">
Figure 2: PaIntE Editor currently displaying 2
curves and their respective parameters. The lower
section shows saved and named prototypes.
</figureCaption>
<bodyText confidence="0.999453555555556">
To enable audio playback functionality
ICARUS for intonation requires access to the
appropriate sound files. In both formats described
above, special properties define the name of a
sound file to be used for playback. Timestamp
values on various levels (syllable, word, sentence
or document) point to the respective section in the
audio data, which currently is required to be in the
Waveform Audio File Format (*.wav files).
</bodyText>
<sectionHeader confidence="0.996297" genericHeader="method">
4 Visualization
</sectionHeader>
<bodyText confidence="0.999969555555555">
Since the ICARUS for intonation module is build
on the data model used for corpora with corefer-
ence annotations in ICARUS, existing visualiza-
tions for coreference data can be used. However,
they make no use of syllable level features and
do not provide playback functionality. Therefore
a couple of new visualizations have been imple-
mented, adding visual information about PaIntE
curves in several levels of granularity.
</bodyText>
<subsectionHeader confidence="0.998303">
4.1 PaIntE Editor
</subsectionHeader>
<bodyText confidence="0.999939428571429">
To get familiar with the visualization of PaIntE pa-
rameters the PaIntE Editor (Figure 2) offers users
with little or no knowledge about PaIntE a starting
point to directly see the impact of changes to cer-
tain parameters. In this editor the user can define
multiple PaIntE curves either from scratch or by
importing them from real examples in a corpus.
Changes to individual parameters can be applied
via sliders or input fields and are displayed in real-
time. Additionally a persistent storage of PaIntE
curves is provided where the user can save param-
eter sets that are of interest to him along with a
description and identifier, the latter of which can
be used when searching (see Section 5).
</bodyText>
<subsectionHeader confidence="0.989945">
4.2 Curve Preview
</subsectionHeader>
<bodyText confidence="0.913182">
For all visualizations dealing with PaIntE curves
ICARUS for intonation provides a compact “pre-
Hz
</bodyText>
<page confidence="0.972845">
26
</page>
<bodyText confidence="0.999970636363636">
view” on the sentence level (lower parts of Fig-
ures 3 and 4b). Instead of drawing the full curves
for all syllables, only syllables in which a peak
was found (based on the peak’s timing encoded in
the PaIntE parameter b) are displayed. The visual-
ization of the curve then only uses the amplitudes
of rise and fall and the absolute height of the peak
(c1, c2 and d). Since the user can freely customize
the filter window for the peak this curve preview
offers a fast way to spot interesting parts of the F0
contour when exploring data manually.
</bodyText>
<subsectionHeader confidence="0.999744">
4.3 Document Outline
</subsectionHeader>
<bodyText confidence="0.999995428571429">
Figure 3 shows parts of the main entry point for
manual exploration in ICARUS for intonation.
Having selected a section of the corpus the user
wishes to inspect (with sentences grouped into
documents in the left section of the figure) he then
gets a detailed outline of the contents of that doc-
ument using one of several available presentation
modes. The default visualization for data holding
PaIntE annotations arranges the document’s con-
tent one sentence per line, making use of the above
mentioned curve preview to provide the user with
a very compact overview of an entire document.
For each sentence a detail panel can be unfolded
which renders the complete PaIntE curves above
the preview area. Several aspects of the visualiza-
tion are highly customizable (like the number of
words to show detailed curves for) and the user
can select the content of the detail panel by mov-
ing a slider through the sentence.
An important feature of the Document Outline
is the fine-grained playback functionality. The
user is free to play a variety of sections of the
sound data linked to the document currently being
displayed. Speaker buttons at the left border play
predefined parts of the sound data like sentences or
the current content of a detail panel. By clicking
on individual word or syllable labels in the detail
panel the playback can be selected even finer.
</bodyText>
<subsectionHeader confidence="0.999">
4.4 Sentence Outline
</subsectionHeader>
<bodyText confidence="0.999964857142857">
When only single sentences are visualized,
ICARUS for intonation displays a more detailed
outline showing the PaIntE curves for all syllables
in the sentence grouped by the surrounding words.
In Figure 4b part of a sentence is visualized in this
way (the screenshot also contains visual highlight-
ing as its content is the result of a search).
In contrast to the more condensed document
outline, this visualization offers a great deal more
space for additional information on the syllable
level to be displayed. As for playback function-
ality it offers granularity similar to the document
outline, allowing the user to play the entire sen-
tence or restrict it to individual words or syllables.
</bodyText>
<subsectionHeader confidence="0.994093">
4.5 Label Patterns
</subsectionHeader>
<bodyText confidence="0.999990620689655">
Both formats currently read by ICARUS for in-
tonation can contain more information on the
syllable and word level than can be presented
to the user without overloading the visualiza-
tion. Therefore the two visualizations described
above make heavy use of so called label pat-
terns to produce the actual text displayed at var-
ious locations. A label pattern is a string de-
scribing a format according to which a certain
text should be created. Expressions of the form
“{&lt;level&gt;:&lt;property&gt;}” define where informa-
tion extracted from the visualized data should be
inserted. The &lt;level&gt; specifies the level of data
to query ({syl,word,sent,doc} for the syllable,
word, sentence and document levels). For example
the default pattern “{word:form}\n{word:pos}”,
used in the Document Outline (see Section 4.3) to
display the text for a sentence, extracts the surface
form and part-of-speech tag for a word and places
them below each other as shown in Figure 3. The
user can freely define the default patterns for a
number of locations as well as change the patterns
used for the active visualization on the fly. Besides
directly extracting data and displaying it as text,
patterns offer additional options that define how to
convert e.g. numerical values into strings or how
to post process or aggregate generated texts. How-
ever, going into details of the pattern engine is be-
yond the scope of this paper.
</bodyText>
<sectionHeader confidence="0.995031" genericHeader="method">
5 Search
</sectionHeader>
<bodyText confidence="0.997635076923077">
ICARUS for intonation augments both the coref-
erence and dependency search facilities already
available in ICARUS by adding access to vari-
ous syllable features and implementing multiple
specialized search constraints based on the PaIntE
model. For example the user can search for prede-
fined F0 contours (rise, fall, rise-fall or
unaccented) based on customizable criteria or
use one of several similarity measures available,
like Euclidean distance or cosine similarity.
Sets of PaIntE parameters can either be defined
explicitly by listing all values or by referencing a
previously saved prototype from the PaIntE Editor
</bodyText>
<page confidence="0.998503">
27
</page>
<figureCaption confidence="0.981456333333333">
Figure 3: Visualization of the first few sentences in a document with preview curves painted above the
raw text outline. The top sentence has its detail panel unfolded, showing PaIntE curves for all syllables
of a selected number of words.
</figureCaption>
<bodyText confidence="0.921611846153846">
by name (see Section 4.1). The ICARUS search
engine allows queries to be created either graphi-
cally (by creating nodes and attaching constraints
to them) or textually via a simple query language
(G¨artner et al., 2013).
The following two sections outline some exam-
ple use cases that combine prosodic features with
structural information on different layers for anal-
ysis and Section 5.3 shows some of the similar-
ity measures used for searching. Example data in
those sections is taken from the DIRNDL corpus
(Eckart et al., 2012) with coreference information
(Bj¨orkelund et al., 2014) and some added features.
</bodyText>
<subsectionHeader confidence="0.995753">
5.1 Syntax and Intonation
</subsectionHeader>
<bodyText confidence="0.999826142857143">
As part of a recent study (Riester and Pio-
ntek, in press) adjective-noun sequences from the
DIRNDL corpus have been analyzed based on
their tonal realization. Of interest in this study
concerning relative givenness (Wagner, 2006)
were those adjective-noun sequences where the
adjective was tonally more prominent than the ad-
jacent noun. An example of how to find them is
shown in Figure 4. The query (Figure 4a) will
match adjectives (ADJA) adjacent to a following
noun (NN) which must not have another dependent
that is either a modifying noun or name (NE). The
results are presented to the user using the detailed
Sentence Outline (Figure 4b) from Section 4.4.
</bodyText>
<subsectionHeader confidence="0.999467">
5.2 Coreference and Intonation
</subsectionHeader>
<bodyText confidence="0.999915266666667">
Besides finding exact matches in a data set the
search engine in ICARUS can be used to analyze
value distributions for an annotation. Using the
query in Figure 5a the search engine is asked to
look for mentions the size of up to 2 words that
are not the root of a coreference chain. The spe-
cial grouping operator &lt;*&gt; results in the creation
of a frequency list (Figure 5b) over the Boolean
tonal prominence property (which purely relies on
the peak excursion with a customizable threshold)
of the head word of each mention that was found
based on the above constraints. By clicking on one
of the entries in this list the user will then be pre-
sented with all the instances that contributed to the
respective frequency for further exploration.
</bodyText>
<subsectionHeader confidence="0.996139">
5.3 Similarity Search
</subsectionHeader>
<bodyText confidence="0.9757366">
The continuous nature of the PaIntE parameters
makes using absolute values to search for curve
forms very impractical. Therefore ICARUS for
intonation provides a collection of similarity mea-
sures and other constraints that can be used to find
syllables with PaIntE curves similar to a given pro-
totype. Most of them are customizable by the user
and investigation and refinement of the available
similarity measures is subject of ongoing work.
Figure 6 shows an example of using co-
sine similarity to find instances in the data
set that are similar to a defined prototype
curve. In this case the first syllable of the
accented word “Steinmeier” was found to be
of interest and saved in the PaIntE editor with
the identifier prototype stein. The query
[painteAngle$&amp;quot;$prototype stein&amp;quot;&lt;=&amp;quot;5.0&amp;quot;]
then looks for PaIntE curves which do not differ
from the prototype by more than 5 degrees.
When using PaIntE curves as part of a search
</bodyText>
<page confidence="0.993821">
28
</page>
<figure confidence="0.987754">
(b) result outline with highlighting
</figure>
<figureCaption confidence="0.988436">
Figure 4: Example search query combining syntax and intonation constraints and an excerpt of the
corresponding result outline.
</figureCaption>
<figure confidence="0.979798">
(a) graphical query
</figure>
<figureCaption confidence="0.988458">
Figure 5: Simple search combining coreference
and intonation features. It is meant to investigate
the distribution of “tonally prominent” mentions
that are given (already introduced) in a discourse.
Figure 6: Prototype of a PaIntE curve as found in
the data and an example result of a search using
cosine similarity.
</figureCaption>
<bodyText confidence="0.9996925">
constraint the corresponding result visualization
will render those curves when highlighting result
instances as can be seen on the first peak (dashed
blue curve) in Figures 6b. This provides the user
with accurate information on how “visually close”
a match is towards the used constraints.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999569875">
A number of well established tools exist for visual-
ization of text corpora annotated with dependency
or coreference, many of which have been dis-
cussed in other ICARUS related papers (G¨artner
et al., 2013; G¨artner et al., 2014). In terms of
search functionality those tools offer a broad range
of complexity, ranging from string-searching on
surface forms2 up to queries on multi-level anno-
</bodyText>
<footnote confidence="0.851875">
2http://code.google.com/p/whatswrong/
</footnote>
<bodyText confidence="0.999674392857143">
tations (Zeldes et al., 2009; Pajas and ˇStˇep´anek,
2009). However, they do not support a dedicated
search and visualization for prosodic syllable level
annotations. Tools like ELAN (Wittenburg et al.,
2006) provide an interface for adding (flat) anno-
tations to multi-modal corpora, but focus on audio
and video data. More importantly, ICARUS for
intonation is so far the first tool using the PaIntE
model for F0 contour visualizations, a task pre-
viously worked around via general curve plotting
tools like R3 and also is first to provide a collection
of search constraints dedicated to PaIntE curves.
Eckart et al. (2010) describe a database that
serves as a generic query tool for multiple anno-
tation layers. It allows to take annotations of tonal
features into account and has also been tested with
the DIRNDL corpus. However, this database has
been designed as an expert system, e.g. for inter-
nal use in projects that create annotations. It does
not provide any visualization or query functions
besides basic SQL queries and no sound playback.
The focus on preprocessed or completely anno-
tated data in ICARUS distinguishes it from typical
tools in the domain of Spoken Document Retrieval
(SDR) or Spoken Term Detection (STD). These
use automatic speech recognition and information
retrieval technologies in order to prepare and pro-
cess audio data (Garofolo et al., 2000).
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999936">
We presented ICARUS for intonation, a flexible
visualization and search tool for multi-modal (text
and speech) data. The tool augments existing vi-
sualization and search features of ICARUS to han-
dle prosodic annotations and introduces a collec-
</bodyText>
<footnote confidence="0.823505">
3http://www.r-project.org
</footnote>
<figure confidence="0.999888">
(a) (b)
(a) (b)
</figure>
<page confidence="0.996658">
29
</page>
<bodyText confidence="0.999978916666667">
tion of novel visualizations and search functional-
ities. In addition to the highly customizable visu-
alizations it allows for a very fine-grained play-
back of speech data for displayed sections of a
corpus directly from within the graphical user in-
terface. The built-in search engine lets the user
combine prosodic constraints with constraints of
other annotation layers like syntax or coreference,
thereby supporting complex search queries, and it
features aggregated result views. Being based on
the ICARUS platform’s plugin-engine, the module
can be extended to cover additional data formats.
</bodyText>
<sectionHeader confidence="0.99811" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9856924">
This work was funded by the German Federal
Ministry of Education and Research (BMBF) via
CLARIN-D, No. 01UG1120F and the German
Research Foundation (DFG) via the SFB 732,
project INF.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99980661627907">
Mary Beckman and Julia Hirschberg. 1999. The ToBI
Annotation Conventions. http://www.ling.
ohio-state.edu/˜tobi/ame_tobi/
annotation_conventions.html.
Anders Bj¨orkelund, Kerstin Eckart, Arndt Riester,
Nadja Schauffler, and Katrin Schweitzer. 2014. The
Extended DIRNDL Corpus as a Resource for Coref-
erence and Bridging Resolution. In LREC.
P. Cosi, C. Avesani, F. Tesser, R. Gretter, and F. Pi-
anesi. 2002. A modified “PaIntE” model for Ital-
ian TTS. In Speech Synthesis, 2002. Proceedings of
2002 IEEE Workshop on, pages 131 – 134.
Kerstin Eckart, Kurt Eberle, and Ulrich Heid. 2010.
An Infrastructure for More Reliable Corpus Analy-
sis. In LREC: Workshop on Web Services and Pro-
cessing Pipelines in HLT, pages 8–14, Valletta.
Kerstin Eckart, Arndt Riester, and Katrin Schweitzer.
2012. A Discourse Information Radio News
Database for Linguistic Analysis. In Christian
Chiarcos, Sebastian Nordhoff, and Sebastian Hell-
mann, editors, Linked Data in Linguistics, pages 65–
75. Springer, Heidelberg.
John S. Garofolo, Cedric G. P. Auzanne, and Ellen M.
Voorhees. 2000. The TREC Spoken Document Re-
trieval Track: A Success Story. In in Text Retrieval
Conference (TREC) 8, pages 16–19.
Markus G¨artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj¨orkelund, and Jonas Kuhn. 2013. ICARUS
– An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55–60, Sofia, Bulgaria.
Markus G¨artner, Anders Bj¨orkelund, Gregor Thiele,
Wolfgang Seeker, and Jonas Kuhn. 2014. Visualiza-
tion, Search, and Error Analysis for Coreference An-
notations. In ACL: System Demonstrations, pages
7–12, Baltimore, Maryland.
Niamh Kelly and Katrin Schweitzer. in press. Examin-
ing Lexical Tonal Contrast in Norwegian Using Into-
nation Modelling. In Proceedings of the 18th Inter-
national Congress of Phonetic Sciences, Glasgow,
UK.
Gregor M¨ohler. 1998. Describing intonation with a
parametric model. In ICSLP, volume 7, pages 2851–
2854.
Gregor M¨ohler. 2001. Improvements of the PaIntE
model for Fo parametrization. Technical report, In-
stitute of Natural Language Processing, University
of Stuttgart. Draft version.
Petr Pajas and Jan ˇStˇep´anek. 2009. System for
Querying Syntactically Annotated Corpora. InACL-
IJCNLP: Software Demonstrations, pages 33–36,
Suntec, Singapore.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In CoNLL:
Shared Task, pages 1–27, Portland, Oregon, USA.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In EMNLP-
CoNLL: Shared Task, pages 1–40, Jeju Island, Ko-
rea.
Arndt Riester and J¨orn Piontek. in press. Anarchy in
the NP. When new nouns get deaccented and given
nouns don’t. Lingua.
Katrin Schweitzer, Michael Walsh, Sasha Calhoun,
Hinrich Sch¨utze, Bernd M¨obius, Antje Schweitzer,
and Grzegorz Dogil. 2015. Exploring the relation-
ship between intonation and the lexicon: Evidence
for lexicalised storage of intonation. Speech Com-
munication, 66(0):65–81.
Ann K. Syrdal, Julia Hirschberg, Julie McGory, and
Mary Beckman. 2001. Automatic ToBI Predic-
tion and Alignment to Speed Manual Labeling of
Prosody. Speech Commun., 33(1-2):135–151.
Michael Wagner. 2006. Givenness and Locality.
In Masayuki Gibson and Jonathan Howell, editors,
Proceedings of SALT XVI, pages 295–312.
P. Wittenburg, H. Brugman, A. Russel, A. Klassmann,
and H. Sloetjes. 2006. ELAN: a Professional
Framework for Multimodality Research. In LREC.
Amir Zeldes, Julia Ritz, Anke L¨udeling, and Christian
Chiarcos. 2009. ANNIS: A Search Tool for Multi-
Layer Annotated Corpora. In Proceedings of Cor-
pus Linguistics.
</reference>
<page confidence="0.998809">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.993952">
<title confidence="0.999934">Multi-modal Visualization and Search for Text and Prosody Annotations</title>
<author confidence="0.99992">Markus G¨artner Katrin Schweitzer Kerstin Eckart Jonas</author>
<affiliation confidence="0.998669">Institute for Natural Language University of</affiliation>
<abstract confidence="0.9997742">We present ICARUS for intonation, an interactive tool to browse and search automatically derived descriptions of fundamental frequency contours. It offers access to tonal features in combination with other annotation layers like part-ofspeech, syntax or coreference and visualizes them in a highly customizable graphical interface with various playback functions. The built-in search allows multilevel queries, the construction of which can be done graphically or textually, and the ability to search based on various similarity measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mary Beckman</author>
<author>Julia Hirschberg</author>
</authors>
<title>The ToBI Annotation Conventions.</title>
<date>1999</date>
<note>http://www.ling. ohio-state.edu/˜tobi/ame_tobi/ annotation_conventions.html.</note>
<contexts>
<context position="2857" citStr="Beckman and Hirschberg, 1999" startWordPosition="430" endWordPosition="433">cy treebanks can get a simple visualization of the intonation contour of a sentence. Furthermore ICARUS focuses on automatic annotations to allow for search on large data sets. Thus ICARUS for intonation’s main features for prosodic search are based on PaIntE, a parametric intonation model (M¨ohler, 1998; M¨ohler, 2001). So far, most data in intonation research is manually annotated, which is a very time consuming task: the time for annotating speech data is many times higher than the real time of the audio recording. For example the Tones and Break Indices (ToBI) system for American English (Beckman and Hirschberg, 1999) takes experienced annotators about 100-200 times the real time (Syrdal et al., 2001). While manual annotations for pitch accents and prosodic phrase boundaries can also be imported, our main goal with this module is to provide intonation researchers with a customizable tool to conduct thorough studies on very large sets of only automatically annotated speech data. In Sections 2 and 3 we introduce the PaIntE model and describe the current input format for the data importer. Section 4 demonstrates several visualization functionalities, and Section 5 discusses the search facilities, including de</context>
</contexts>
<marker>Beckman, Hirschberg, 1999</marker>
<rawString>Mary Beckman and Julia Hirschberg. 1999. The ToBI Annotation Conventions. http://www.ling. ohio-state.edu/˜tobi/ame_tobi/ annotation_conventions.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Kerstin Eckart</author>
<author>Arndt Riester</author>
<author>Nadja Schauffler</author>
<author>Katrin Schweitzer</author>
</authors>
<title>The Extended DIRNDL Corpus as a Resource for Coreference and Bridging Resolution.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<marker>Bj¨orkelund, Eckart, Riester, Schauffler, Schweitzer, 2014</marker>
<rawString>Anders Bj¨orkelund, Kerstin Eckart, Arndt Riester, Nadja Schauffler, and Katrin Schweitzer. 2014. The Extended DIRNDL Corpus as a Resource for Coreference and Bridging Resolution. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cosi</author>
<author>C Avesani</author>
<author>F Tesser</author>
<author>R Gretter</author>
<author>F Pianesi</author>
</authors>
<title>A modified “PaIntE” model for Italian TTS. In Speech Synthesis,</title>
<date>2002</date>
<booktitle>Proceedings of 2002 IEEE Workshop on,</booktitle>
<pages>131--134</pages>
<contexts>
<context position="4881" citStr="Cosi et al., 2002" startWordPosition="757" endWordPosition="760">ght. The remaining parameters specify the steepness and amplitude of the rise before, and the fall after the peak (parameters a1 and a2 for the steepness and c1/c2 for the amplitude). Figure 1 illustrates the function. It displays the syllable for which the parametrization is carried out (σ∗) and its immediate neighbors. The x-axis indicates time (normalized for syllable duration, the current syllable spans from 0 to 1) and the yaxis displays the fundamental frequency in Hertz. The PaIntE model has been used for the modeling of different languages, e.g. Norwegian, Italian, German and English (Cosi et al., 2002; Kelly and Schweitzer, in press; Schweitzer et al., 2015). 3 Data Representation ICARUS for intonation ships with reader implementations for two very different formats. One is an extended version of the format used for the 2011 and 2012 CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) with a number of additional columns to accommodate features for the syllable level. This format stores all annotations corresponding to a word token in one line and packs syllable features into a list separated by pipe-characters (’|’). To address syllable centric data like the typical output of s</context>
</contexts>
<marker>Cosi, Avesani, Tesser, Gretter, Pianesi, 2002</marker>
<rawString>P. Cosi, C. Avesani, F. Tesser, R. Gretter, and F. Pianesi. 2002. A modified “PaIntE” model for Italian TTS. In Speech Synthesis, 2002. Proceedings of 2002 IEEE Workshop on, pages 131 – 134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Eckart</author>
<author>Kurt Eberle</author>
<author>Ulrich Heid</author>
</authors>
<title>An Infrastructure for More Reliable Corpus Analysis.</title>
<date>2010</date>
<booktitle>In LREC: Workshop on Web Services and Processing Pipelines in HLT,</booktitle>
<pages>8--14</pages>
<location>Valletta.</location>
<contexts>
<context position="17373" citStr="Eckart et al. (2010)" startWordPosition="2807" endWordPosition="2810">ions (Zeldes et al., 2009; Pajas and ˇStˇep´anek, 2009). However, they do not support a dedicated search and visualization for prosodic syllable level annotations. Tools like ELAN (Wittenburg et al., 2006) provide an interface for adding (flat) annotations to multi-modal corpora, but focus on audio and video data. More importantly, ICARUS for intonation is so far the first tool using the PaIntE model for F0 contour visualizations, a task previously worked around via general curve plotting tools like R3 and also is first to provide a collection of search constraints dedicated to PaIntE curves. Eckart et al. (2010) describe a database that serves as a generic query tool for multiple annotation layers. It allows to take annotations of tonal features into account and has also been tested with the DIRNDL corpus. However, this database has been designed as an expert system, e.g. for internal use in projects that create annotations. It does not provide any visualization or query functions besides basic SQL queries and no sound playback. The focus on preprocessed or completely annotated data in ICARUS distinguishes it from typical tools in the domain of Spoken Document Retrieval (SDR) or Spoken Term Detection</context>
</contexts>
<marker>Eckart, Eberle, Heid, 2010</marker>
<rawString>Kerstin Eckart, Kurt Eberle, and Ulrich Heid. 2010. An Infrastructure for More Reliable Corpus Analysis. In LREC: Workshop on Web Services and Processing Pipelines in HLT, pages 8–14, Valletta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Eckart</author>
<author>Arndt Riester</author>
<author>Katrin Schweitzer</author>
</authors>
<title>A Discourse Information Radio News Database for Linguistic Analysis.</title>
<date>2012</date>
<booktitle>Linked Data in Linguistics,</booktitle>
<pages>65--75</pages>
<editor>In Christian Chiarcos, Sebastian Nordhoff, and Sebastian Hellmann, editors,</editor>
<publisher>Springer,</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="13019" citStr="Eckart et al., 2012" startWordPosition="2101" endWordPosition="2104">tail panel unfolded, showing PaIntE curves for all syllables of a selected number of words. by name (see Section 4.1). The ICARUS search engine allows queries to be created either graphically (by creating nodes and attaching constraints to them) or textually via a simple query language (G¨artner et al., 2013). The following two sections outline some example use cases that combine prosodic features with structural information on different layers for analysis and Section 5.3 shows some of the similarity measures used for searching. Example data in those sections is taken from the DIRNDL corpus (Eckart et al., 2012) with coreference information (Bj¨orkelund et al., 2014) and some added features. 5.1 Syntax and Intonation As part of a recent study (Riester and Piontek, in press) adjective-noun sequences from the DIRNDL corpus have been analyzed based on their tonal realization. Of interest in this study concerning relative givenness (Wagner, 2006) were those adjective-noun sequences where the adjective was tonally more prominent than the adjacent noun. An example of how to find them is shown in Figure 4. The query (Figure 4a) will match adjectives (ADJA) adjacent to a following noun (NN) which must not ha</context>
</contexts>
<marker>Eckart, Riester, Schweitzer, 2012</marker>
<rawString>Kerstin Eckart, Arndt Riester, and Katrin Schweitzer. 2012. A Discourse Information Radio News Database for Linguistic Analysis. In Christian Chiarcos, Sebastian Nordhoff, and Sebastian Hellmann, editors, Linked Data in Linguistics, pages 65– 75. Springer, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Garofolo</author>
<author>Cedric G P Auzanne</author>
<author>Ellen M Voorhees</author>
</authors>
<title>The TREC Spoken Document Retrieval Track: A Success Story.</title>
<date>2000</date>
<booktitle>In in Text Retrieval Conference (TREC) 8,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="18125" citStr="Garofolo et al., 2000" startWordPosition="2929" endWordPosition="2932">atures into account and has also been tested with the DIRNDL corpus. However, this database has been designed as an expert system, e.g. for internal use in projects that create annotations. It does not provide any visualization or query functions besides basic SQL queries and no sound playback. The focus on preprocessed or completely annotated data in ICARUS distinguishes it from typical tools in the domain of Spoken Document Retrieval (SDR) or Spoken Term Detection (STD). These use automatic speech recognition and information retrieval technologies in order to prepare and process audio data (Garofolo et al., 2000). 7 Conclusion We presented ICARUS for intonation, a flexible visualization and search tool for multi-modal (text and speech) data. The tool augments existing visualization and search features of ICARUS to handle prosodic annotations and introduces a collec3http://www.r-project.org (a) (b) (a) (b) 29 tion of novel visualizations and search functionalities. In addition to the highly customizable visualizations it allows for a very fine-grained playback of speech data for displayed sections of a corpus directly from within the graphical user interface. The built-in search engine lets the user co</context>
</contexts>
<marker>Garofolo, Auzanne, Voorhees, 2000</marker>
<rawString>John S. Garofolo, Cedric G. P. Auzanne, and Ellen M. Voorhees. 2000. The TREC Spoken Document Retrieval Track: A Success Story. In in Text Retrieval Conference (TREC) 8, pages 16–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus G¨artner</author>
<author>Gregor Thiele</author>
<author>Wolfgang Seeker</author>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
</authors>
<title>ICARUS – An Extensible Graphical Search Tool for Dependency Treebanks.</title>
<date>2013</date>
<booktitle>In ACL: System Demonstrations,</booktitle>
<pages>55--60</pages>
<location>Sofia, Bulgaria.</location>
<marker>G¨artner, Thiele, Seeker, Bj¨orkelund, Kuhn, 2013</marker>
<rawString>Markus G¨artner, Gregor Thiele, Wolfgang Seeker, Anders Bj¨orkelund, and Jonas Kuhn. 2013. ICARUS – An Extensible Graphical Search Tool for Dependency Treebanks. In ACL: System Demonstrations, pages 55–60, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus G¨artner</author>
<author>Anders Bj¨orkelund</author>
<author>Gregor Thiele</author>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Visualization, Search, and Error Analysis for Coreference Annotations.</title>
<date>2014</date>
<booktitle>In ACL: System Demonstrations,</booktitle>
<pages>7--12</pages>
<location>Baltimore, Maryland.</location>
<marker>G¨artner, Bj¨orkelund, Thiele, Seeker, Kuhn, 2014</marker>
<rawString>Markus G¨artner, Anders Bj¨orkelund, Gregor Thiele, Wolfgang Seeker, and Jonas Kuhn. 2014. Visualization, Search, and Error Analysis for Coreference Annotations. In ACL: System Demonstrations, pages 7–12, Baltimore, Maryland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Niamh Kelly</author>
<author>Katrin Schweitzer</author>
</authors>
<title>in press. Examining Lexical Tonal Contrast in Norwegian Using Intonation Modelling.</title>
<booktitle>In Proceedings of the 18th International Congress of Phonetic Sciences,</booktitle>
<location>Glasgow, UK.</location>
<marker>Kelly, Schweitzer, </marker>
<rawString>Niamh Kelly and Katrin Schweitzer. in press. Examining Lexical Tonal Contrast in Norwegian Using Intonation Modelling. In Proceedings of the 18th International Congress of Phonetic Sciences, Glasgow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor M¨ohler</author>
</authors>
<title>Describing intonation with a parametric model.</title>
<date>1998</date>
<booktitle>In ICSLP,</booktitle>
<volume>7</volume>
<pages>2851--2854</pages>
<marker>M¨ohler, 1998</marker>
<rawString>Gregor M¨ohler. 1998. Describing intonation with a parametric model. In ICSLP, volume 7, pages 2851– 2854.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor M¨ohler</author>
</authors>
<title>Improvements of the PaIntE model for Fo parametrization.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Institute of Natural Language Processing, University of Stuttgart. Draft version.</institution>
<marker>M¨ohler, 2001</marker>
<rawString>Gregor M¨ohler. 2001. Improvements of the PaIntE model for Fo parametrization. Technical report, Institute of Natural Language Processing, University of Stuttgart. Draft version.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
</authors>
<title>System for Querying Syntactically Annotated Corpora. InACLIJCNLP: Software Demonstrations,</title>
<date>2009</date>
<pages>33--36</pages>
<location>Suntec, Singapore.</location>
<marker>Pajas, ˇStˇep´anek, 2009</marker>
<rawString>Petr Pajas and Jan ˇStˇep´anek. 2009. System for Querying Syntactically Annotated Corpora. InACLIJCNLP: Software Demonstrations, pages 33–36, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes. In CoNLL: Shared Task,</title>
<date>2011</date>
<pages>1--27</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="5159" citStr="Pradhan et al., 2011" startWordPosition="803" endWordPosition="806">s carried out (σ∗) and its immediate neighbors. The x-axis indicates time (normalized for syllable duration, the current syllable spans from 0 to 1) and the yaxis displays the fundamental frequency in Hertz. The PaIntE model has been used for the modeling of different languages, e.g. Norwegian, Italian, German and English (Cosi et al., 2002; Kelly and Schweitzer, in press; Schweitzer et al., 2015). 3 Data Representation ICARUS for intonation ships with reader implementations for two very different formats. One is an extended version of the format used for the 2011 and 2012 CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) with a number of additional columns to accommodate features for the syllable level. This format stores all annotations corresponding to a word token in one line and packs syllable features into a list separated by pipe-characters (’|’). To address syllable centric data like the typical output of speech processing systems, a second flexible tabular format was specified where each line of text corresponds to a single syllable and a global header describes the content of all columns and how to read and map them to the internal data model of ICARUS. Figure 2: PaIntE Editor </context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes. In CoNLL: Shared Task, pages 1–27, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In EMNLPCoNLL: Shared Task,</booktitle>
<pages>1--40</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5182" citStr="Pradhan et al., 2012" startWordPosition="807" endWordPosition="810"> its immediate neighbors. The x-axis indicates time (normalized for syllable duration, the current syllable spans from 0 to 1) and the yaxis displays the fundamental frequency in Hertz. The PaIntE model has been used for the modeling of different languages, e.g. Norwegian, Italian, German and English (Cosi et al., 2002; Kelly and Schweitzer, in press; Schweitzer et al., 2015). 3 Data Representation ICARUS for intonation ships with reader implementations for two very different formats. One is an extended version of the format used for the 2011 and 2012 CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) with a number of additional columns to accommodate features for the syllable level. This format stores all annotations corresponding to a word token in one line and packs syllable features into a list separated by pipe-characters (’|’). To address syllable centric data like the typical output of speech processing systems, a second flexible tabular format was specified where each line of text corresponds to a single syllable and a global header describes the content of all columns and how to read and map them to the internal data model of ICARUS. Figure 2: PaIntE Editor currently displaying 2 </context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes. In EMNLPCoNLL: Shared Task, pages 1–40, Jeju Island, Korea.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Arndt Riester</author>
<author>J¨orn Piontek</author>
</authors>
<title>in press. Anarchy in the NP. When new nouns get deaccented and given nouns don’t.</title>
<journal>Lingua.</journal>
<marker>Riester, Piontek, </marker>
<rawString>Arndt Riester and J¨orn Piontek. in press. Anarchy in the NP. When new nouns get deaccented and given nouns don’t. Lingua.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Schweitzer</author>
<author>Michael Walsh</author>
<author>Sasha Calhoun</author>
<author>Hinrich Sch¨utze</author>
<author>Bernd M¨obius</author>
<author>Antje Schweitzer</author>
<author>Grzegorz Dogil</author>
</authors>
<title>Exploring the relationship between intonation and the lexicon: Evidence for lexicalised storage of intonation.</title>
<date>2015</date>
<journal>Speech Communication,</journal>
<volume>66</volume>
<issue>0</issue>
<marker>Schweitzer, Walsh, Calhoun, Sch¨utze, M¨obius, Schweitzer, Dogil, 2015</marker>
<rawString>Katrin Schweitzer, Michael Walsh, Sasha Calhoun, Hinrich Sch¨utze, Bernd M¨obius, Antje Schweitzer, and Grzegorz Dogil. 2015. Exploring the relationship between intonation and the lexicon: Evidence for lexicalised storage of intonation. Speech Communication, 66(0):65–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann K Syrdal</author>
<author>Julia Hirschberg</author>
<author>Julie McGory</author>
<author>Mary Beckman</author>
</authors>
<title>Automatic ToBI Prediction and Alignment to Speed Manual Labeling of Prosody.</title>
<date>2001</date>
<journal>Speech Commun.,</journal>
<pages>33--1</pages>
<contexts>
<context position="2942" citStr="Syrdal et al., 2001" startWordPosition="444" endWordPosition="447">re ICARUS focuses on automatic annotations to allow for search on large data sets. Thus ICARUS for intonation’s main features for prosodic search are based on PaIntE, a parametric intonation model (M¨ohler, 1998; M¨ohler, 2001). So far, most data in intonation research is manually annotated, which is a very time consuming task: the time for annotating speech data is many times higher than the real time of the audio recording. For example the Tones and Break Indices (ToBI) system for American English (Beckman and Hirschberg, 1999) takes experienced annotators about 100-200 times the real time (Syrdal et al., 2001). While manual annotations for pitch accents and prosodic phrase boundaries can also be imported, our main goal with this module is to provide intonation researchers with a customizable tool to conduct thorough studies on very large sets of only automatically annotated speech data. In Sections 2 and 3 we introduce the PaIntE model and describe the current input format for the data importer. Section 4 demonstrates several visualization functionalities, and Section 5 discusses the search facilities, including dependency and intonation as well as coreference and intonation queries. After discussi</context>
</contexts>
<marker>Syrdal, Hirschberg, McGory, Beckman, 2001</marker>
<rawString>Ann K. Syrdal, Julia Hirschberg, Julie McGory, and Mary Beckman. 2001. Automatic ToBI Prediction and Alignment to Speed Manual Labeling of Prosody. Speech Commun., 33(1-2):135–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wagner</author>
</authors>
<title>Givenness and Locality.</title>
<date>2006</date>
<booktitle>In Masayuki Gibson and Jonathan Howell, editors, Proceedings of SALT XVI,</booktitle>
<pages>295--312</pages>
<contexts>
<context position="13356" citStr="Wagner, 2006" startWordPosition="2154" endWordPosition="2155">ne some example use cases that combine prosodic features with structural information on different layers for analysis and Section 5.3 shows some of the similarity measures used for searching. Example data in those sections is taken from the DIRNDL corpus (Eckart et al., 2012) with coreference information (Bj¨orkelund et al., 2014) and some added features. 5.1 Syntax and Intonation As part of a recent study (Riester and Piontek, in press) adjective-noun sequences from the DIRNDL corpus have been analyzed based on their tonal realization. Of interest in this study concerning relative givenness (Wagner, 2006) were those adjective-noun sequences where the adjective was tonally more prominent than the adjacent noun. An example of how to find them is shown in Figure 4. The query (Figure 4a) will match adjectives (ADJA) adjacent to a following noun (NN) which must not have another dependent that is either a modifying noun or name (NE). The results are presented to the user using the detailed Sentence Outline (Figure 4b) from Section 4.4. 5.2 Coreference and Intonation Besides finding exact matches in a data set the search engine in ICARUS can be used to analyze value distributions for an annotation. U</context>
</contexts>
<marker>Wagner, 2006</marker>
<rawString>Michael Wagner. 2006. Givenness and Locality. In Masayuki Gibson and Jonathan Howell, editors, Proceedings of SALT XVI, pages 295–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wittenburg</author>
<author>H Brugman</author>
<author>A Russel</author>
<author>A Klassmann</author>
<author>H Sloetjes</author>
</authors>
<title>ELAN: a Professional Framework for Multimodality Research.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="16958" citStr="Wittenburg et al., 2006" startWordPosition="2738" endWordPosition="2741">ell established tools exist for visualization of text corpora annotated with dependency or coreference, many of which have been discussed in other ICARUS related papers (G¨artner et al., 2013; G¨artner et al., 2014). In terms of search functionality those tools offer a broad range of complexity, ranging from string-searching on surface forms2 up to queries on multi-level anno2http://code.google.com/p/whatswrong/ tations (Zeldes et al., 2009; Pajas and ˇStˇep´anek, 2009). However, they do not support a dedicated search and visualization for prosodic syllable level annotations. Tools like ELAN (Wittenburg et al., 2006) provide an interface for adding (flat) annotations to multi-modal corpora, but focus on audio and video data. More importantly, ICARUS for intonation is so far the first tool using the PaIntE model for F0 contour visualizations, a task previously worked around via general curve plotting tools like R3 and also is first to provide a collection of search constraints dedicated to PaIntE curves. Eckart et al. (2010) describe a database that serves as a generic query tool for multiple annotation layers. It allows to take annotations of tonal features into account and has also been tested with the D</context>
</contexts>
<marker>Wittenburg, Brugman, Russel, Klassmann, Sloetjes, 2006</marker>
<rawString>P. Wittenburg, H. Brugman, A. Russel, A. Klassmann, and H. Sloetjes. 2006. ELAN: a Professional Framework for Multimodality Research. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Zeldes</author>
<author>Julia Ritz</author>
<author>Anke L¨udeling</author>
<author>Christian Chiarcos</author>
</authors>
<title>ANNIS: A Search Tool for MultiLayer Annotated Corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of Corpus Linguistics.</booktitle>
<marker>Zeldes, Ritz, L¨udeling, Chiarcos, 2009</marker>
<rawString>Amir Zeldes, Julia Ritz, Anke L¨udeling, and Christian Chiarcos. 2009. ANNIS: A Search Tool for MultiLayer Annotated Corpora. In Proceedings of Corpus Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>