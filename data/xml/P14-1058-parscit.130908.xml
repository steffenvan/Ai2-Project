<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.981719">
Learning to Predict Distributions of Words Across Domains
</title>
<author confidence="0.992828">
Danushka Bollegala
</author>
<affiliation confidence="0.998747">
Department of Computer Science
University of Liverpool
</affiliation>
<address confidence="0.393665666666667">
Liverpool,
L69 3BX, UK
danushka.bollegala@
</address>
<email confidence="0.573219">
liverpool.ac.uk
</email>
<author confidence="0.993441">
David Weir
</author>
<affiliation confidence="0.996752">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.684947666666667">
Falmer, Brighton,
BN1 9QJ, UK
d.j.weir@
</address>
<email confidence="0.684545">
sussex.ac.uk
</email>
<author confidence="0.975183">
John Carroll
</author>
<affiliation confidence="0.9965975">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.8484765">
Falmer, Brighton,
BN1 9QJ, UK
</address>
<email confidence="0.5443265">
j.a.carroll@
sussex.ac.uk
</email>
<sectionHeader confidence="0.990045" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872458333333">
Although the distributional hypothesis has
been applied successfully in many natural
language processing tasks, systems using
distributional information have been lim-
ited to a single domain because the dis-
tribution of a word can vary between do-
mains as the word’s predominant mean-
ing changes. However, if it were pos-
sible to predict how the distribution of
a word changes from one domain to an-
other, the predictions could be used to
adapt a system trained in one domain to
work in another. We propose an unsuper-
vised method to predict the distribution of
a word in one domain, given its distribu-
tion in another domain. We evaluate our
method on two tasks: cross-domain part-
of-speech tagging and cross-domain sen-
timent classification. In both tasks, our
method significantly outperforms compet-
itive baselines and returns results that are
statistically comparable to current state-
of-the-art methods, while requiring no
task-specific customisations.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943538461538">
The Distributional Hypothesis, summarised by the
memorable line of Firth (1957) – You shall know
a word by the company it keeps – has inspired a
diverse range of research in natural language pro-
cessing. In such work, a word is represented by
the distribution of other words that co-occur with
it. Distributional representations of words have
been successfully used in many language process-
ing tasks such as entity set expansion (Pantel et al.,
2009), part-of-speech (POS) tagging and chunk-
ing (Huang and Yates, 2009), ontology learning
(Curran, 2005), computing semantic textual sim-
ilarity (Besanc¸on et al., 1999), and lexical infer-
ence (Kotlerman et al., 2012).
However, the distribution of a word often varies
from one domain1 to another. For example, in
the domain of portable computer reviews the word
lightweight is often associated with positive sen-
timent bearing words such as sleek or compact,
whereas in the movie review domain the same
word is often associated with negative sentiment-
bearing words such as superficial or formulaic.
Consequently, the distributional representations of
the word lightweight will differ considerably be-
tween the two domains. In this paper, given the
distribution wS of a word w in the source domain
S, we propose an unsupervised method for pre-
dicting its distribution wT in a different target do-
main T .
The ability to predict how the distribution of a
word varies from one domain to another is vital
for numerous adaptation tasks. For example, un-
supervised cross-domain sentiment classification
(Blitzer et al., 2007; Aue and Gamon, 2005) in-
volves using sentiment-labeled user reviews from
the source domain, and unlabeled reviews from
both the source and the target domains to learn
a sentiment classifier for the target domain. Do-
main adaptation (DA) of sentiment classification
becomes extremely challenging when the distribu-
tions of words in the source and the target domains
are very different, because the features learnt from
the source domain labeled reviews might not ap-
pear in the target domain reviews that must be
classified. By predicting the distribution of a word
across different domains, we can find source do-
main features that are similar to the features in
target domain reviews, thereby reducing the mis-
match of features between the two domains.
We propose a two-step unsupervised approach
to predict the distribution of a word across do-
mains. First, we create two lower dimensional la-
</bodyText>
<footnote confidence="0.924291666666667">
1In this paper, we use the term domain to refer to a col-
lection of documents about a particular topic, for example
reviews of a particular kind of product.
</footnote>
<page confidence="0.951144">
613
</page>
<note confidence="0.8323445">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 613–623,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999687827586207">
tent feature spaces separately for the source and
the target domains using Singular Value Decom-
position (SVD). Second, we learn a mapping from
the source domain latent feature space to the tar-
get domain latent feature space using Partial Least
Square Regression (PLSR). The SVD smoothing
in the first step both reduces the data sparseness in
distributional representations of individual words,
as well as the dimensionality of the feature space,
thereby enabling us to efficiently and accurately
learn a prediction model using PLSR in the sec-
ond step. Our proposed cross-domain word dis-
tribution prediction method is unsupervised in the
sense that it does not require any labeled data in
either of the two steps.
Using two popular multi-domain datasets, we
evaluate the proposed method in two prediction
tasks: (a) predicting the POS of a word in a tar-
get domain, and (b) predicting the sentiment of a
review in a target domain. Without requiring any
task specific customisations, systems based on our
distribution prediction method significantly out-
perform competitive baselines in both tasks. Be-
cause our proposed distribution prediction method
is unsupervised and task independent, it is poten-
tially useful for a wide range of DA tasks such en-
tity extraction (Guo et al., 2009) or dependency
parsing (McClosky et al., 2010). Our contribu-
tions are summarised as follows:
</bodyText>
<listItem confidence="0.994427375">
• Given the distribution wS of a word w in a
source domain S, we propose a method for
learning its distribution wT in a target do-
main T .
• Using the learnt distribution prediction
model, we propose a method to learn a cross-
domain POS tagger.
• Using the learnt distribution prediction
</listItem>
<bodyText confidence="0.8106592">
model, we propose a method to learn a cross-
domain sentiment classifier.
To our knowledge, ours is the first successful at-
tempt to learn a model that predicts the distribu-
tion of a word across different domains.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999913732142857">
Learning semantic representations for words us-
ing documents from a single domain has received
much attention lately (Vincent et al., 2010; Socher
et al., 2013; Baroni and Lenci, 2010). As we have
already discussed, the semantics of a word varies
across different domains, and such variations are
not captured by models that only learn a single se-
mantic representation for a word using documents
from a single domain.
The POS of a word is influenced both by its
context (contextual bias), and the domain of the
document in which it appears (lexical bias). For
example, the word signal is predominately used
as a noun in MEDLINE, whereas it appears pre-
dominantly as an adjective in the Wall Street Jour-
nal (WSJ) (Blitzer et al., 2006). Consequently, a
tagger trained on WSJ would incorrectly tag sig-
nal in MEDLINE. Blitzer et al. (2006) append
the source domain labeled data with predicted piv-
ots (i.e. words that appear in both the source and
target domains) to adapt a POS tagger to a tar-
get domain. Choi and Palmer (2012) propose
a cross-domain POS tagging method by training
two separate models: a generalised model and a
domain-specific model. At tagging time, a sen-
tence is tagged by the model that is most similar
to that sentence. Huang and Yates (2009) train a
Conditional Random Field (CRF) tagger with fea-
tures retrieved from a smoothing model trained us-
ing both source and target domain unlabeled data.
Adding latent states to the smoothing model fur-
ther improves the POS tagging accuracy (Huang
and Yates, 2012). Schnabel and Sch¨utze (2013)
propose a training set filtering method where they
eliminate shorter words from the training data
based on the intuition that longer words are more
likely to be examples of productive linguistic pro-
cesses than shorter words.
The sentiment of a word can vary from one do-
main to another. In Structural Correspondence
Learning (SCL) (Blitzer et al., 2006; Blitzer et
al., 2007), a set of pivots are chosen using point-
wise mutual information. Linear predictors are
then learnt to predict the occurrence of those piv-
ots, and SVD is used to construct a lower dimen-
sional representation in which a binary classifier
is trained. Spectral Feature Alignment (SFA) (Pan
et al., 2010) also uses pivots to compute an align-
ment between domain specific and domain inde-
pendent features. Spectral clustering is performed
on a bipartite graph representing domain specific
and domain independent features to find a lower-
dimensional projection between the two sets of
features. The cross-domain sentiment-sensitive
thesaurus (SST) (Bollegala et al., 2011) groups
together words that express similar sentiments in
</bodyText>
<page confidence="0.997945">
614
</page>
<bodyText confidence="0.999893785714286">
different domains. The created thesaurus is used to
expand feature vectors during train and test stages
in a binary classifier. However, unlike our method,
SCL, SFA, or SST do not learn a prediction model
between word distributions across domains.
Prior knowledge of the sentiment of words, such
as sentiment lexicons, has been incorporated into
cross-domain sentiment classification. He et al.
(2011) propose a joint sentiment-topic model that
imposes a sentiment-prior depending on the oc-
currence of a word in a sentiment lexicon. Pono-
mareva and Thelwall (2012) represent source and
target domain reviews as nodes in a graph and ap-
ply a label propagation algorithm to predict the
sentiment labels for target domain reviews from
the sentiment labels in source domain reviews. A
sentiment lexicon is used to create features for a
document. Although incorporation of prior senti-
ment knowledge is a promising technique to im-
prove accuracy in cross-domain sentiment classi-
fication, it is complementary to our task of distri-
bution prediction across domains.
The unsupervised DA setting that we consider
does not assume the availability of labeled data for
the target domain. However, if a small amount of
labeled data is available for the target domain, it
can be used to further improve the performance of
DA tasks (Xiao et al., 2013; Daum´e III, 2007).
</bodyText>
<sectionHeader confidence="0.999288" genericHeader="method">
3 Distribution Prediction
</sectionHeader>
<subsectionHeader confidence="0.999612">
3.1 In-domain Feature Vector Construction
</subsectionHeader>
<bodyText confidence="0.9988377">
Before we tackle the problem of learning a model
to predict the distribution of a word across do-
mains, we must first compute the distribution of
a word from a single domain. For this purpose, we
represent a word w using unigrams and bigrams
that co-occur with w in a sentence as follows.
Given a document H, such as a user-review of
a product, we split H into sentences, and lemma-
tize each word in a sentence using the RASP sys-
tem (Briscoe et al., 2006). Using a standard stop
word list, we filter out frequent non-content un-
igrams and select the remainder as unigram fea-
tures to represent a sentence. Next, we generate
bigrams of word lemmas and remove any bigrams
that consists only of stop words. Bigram features
capture negations more accurately than unigrams,
and have been found to be useful for sentiment
classification tasks. Table 1 shows the unigram
and bigram features we extract for a sentence us-
ing this procedure. Using data from a single do-
</bodyText>
<table confidence="0.857471818181818">
sentence This is an interesting and well researched book
unigrams this, is, an, interesting, and, well, researched,
(surface) book
unigrams this, be, an, interest, and, well, research, book
(lemma)
unigrams interest, well, research, book
(features)
bigrams this+be, be+an, an+interest, interest+and,
(lemma) and+well, well+research, research+book
bigrams an+interest, interest+and, and+well,
(features) well+research, research+book
</table>
<tableCaption confidence="0.99988">
Table 1: Extracting unigram and bigram features.
</tableCaption>
<bodyText confidence="0.999713692307692">
main, we construct a feature co-occurrence ma-
trix A in which columns correspond to unigram
features and rows correspond to either unigram or
bigram features. The value of the element azo in
the co-occurrence matrix A is set to the number of
sentences in which the i-th and j-th features co-
occur.
Typically, the number of unique bigrams is
much larger than that of unigrams. Moreover, co-
occurrences of bigrams are rare compared to co-
occurrences of unigrams, and co-occurrences in-
volving a unigram and a bigram. Consequently,
in matrix A, we consider co-occurrences only be-
tween unigrams vs. unigrams, and bigrams vs.
unigrams. We consider each row in A as repre-
senting the distribution of a feature (i.e. unigrams
or bigrams) in a particular domain over the uni-
gram features extracted from that domain (repre-
sented by the columns of A). We apply Positive
Pointwise Mutual Information (PPMI) to the co-
occurrence matrix A. This is a variation of the
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990), in which all PMI values that are less
than zero are replaced with zero (Lin, 1998; Bul-
linaria and Levy, 2007). Let F be the matrix that
results when PPMI is applied to A. Matrix F has
the same number of rows, nr, and columns, nc, as
the raw co-occurrence matrix A.
Note that in addition to the above-mentioned
representation, there are many other ways to rep-
resent the distribution of a word in a particular do-
main (Turney and Pantel, 2010). For example,
one can limit the definition of co-occurrence to
words that are linked by some dependency relation
(Pado and Lapata, 2007), or extend the window
of co-occurrence to the entire document (Baroni
and Lenci, 2010). Since the method we propose
in Section 3.2 to predict the distribution of a word
across domains does not depend on the particular
</bodyText>
<page confidence="0.995219">
615
</page>
<bodyText confidence="0.999987">
feature representation method, any of these alter-
native methods could be used.
To reduce the dimensionality of the feature
space, and create dense representations for words,
we perform SVD on F. We use the left singu-
lar vectors corresponding to the k largest singular
values to compute a rank k approximation ˆF, of
F. We perform truncated SVD using SVDLIBC2.
Each row in Fˆ is considered as representing a word
in a lower k («nc) dimensional feature space cor-
responding to a particular domain. Distribution
prediction in this lower dimensional feature space
is preferrable to prediction over the original fea-
ture space because there are reductions in overfit-
ting, feature sparseness, and the learning time. We
created two matrices, ˆFS and ˆFT from the source
and target domains, respectively, using the above
mentioned procedure.
</bodyText>
<subsectionHeader confidence="0.998552">
3.2 Cross-Domain Feature Vector Prediction
</subsectionHeader>
<bodyText confidence="0.99419193939394">
We propose a method to learn a model that can
predict the distribution wT of a word w in the
target domain T, given its distribution wS in
the source domain S. We denote the set of
features that occur in both domains by W =
{w(1), ... , w(n)}. In the literature, such features
are often referred to as pivots, and they have been
shown to be useful for DA, allowing the weights
learnt to be transferred from one domain to an-
other. Various criteria have been proposed for se-
lecting a small set of pivots for DA, such as the
mutual information of a word with the two do-
mains (Blitzer et al., 2007). However, we do not
impose any further restrictions on the set of pivots
W other than that they occur in both domains.
For each word w(i) ∈ W, we denote the cor-
responding rows in ˆFS and ˆFT by column vec-
tors w(i)
S and w(i)T . Note that the dimensional-
ity of w(i)
S and w(i)T need not be equal, and we
may select different numbers of singular vectors
to approximate ˆFS and ˆFT . We model distribu-
tion prediction as a multivariate regression prob-
lem where, given a set {(w(i)
S , w(i)T )}ni=1 consist-
ing of pairs of feature vectors selected from each
domain for the pivots in W, we learn a mapping
from the inputs (w(i)
S ) to the outputs (w(i)T ).
We use Partial Least Squares Regression
(PLSR) (Wold, 1985) to learn a regression model
using pairs of vectors. PLSR has been applied in
</bodyText>
<footnote confidence="0.959539">
2http://tedlab.mit.edu/˜dr/SVDLIBC/
</footnote>
<construct confidence="0.925101666666667">
Algorithm 1 Learning a prediction model.
Input: X, Y, L.
Output: Prediction matrix M.
</construct>
<listItem confidence="0.9233072">
1: Randomly select γl from columns in Yl.
����
2: vl = XlTγl/ ����XlTγl
3: λl = Xlvl
����
4: ql = YlTλl/ ����YlTλl
5: γl = Ylql
6: If γl is unchanged go to Line 7; otherwise go to Line 2
����
7: cl = λlTγl/ ����λlTγl
8: pl = XlTλl/λlTλl
9: Xl+1 = Xl − λlplT and Yl+1 = Yl − clλlqlT.
10: Stop if l = L; otherwise l = l + 1 and return to Line 1.
11: Let C = diag(c1, ... , cL), and V = [v1 ... vL]
12: M = V(PTV)−1CQT
</listItem>
<subsectionHeader confidence="0.497422">
13: return M
</subsectionHeader>
<bodyText confidence="0.9970406">
Chemometrics (Geladi and Kowalski, 1986), pro-
ducing stable prediction models even when the
number of samples is considerably smaller than
the dimensionality of the feature space. In particu-
lar, PLSR fits a smaller number of latent variables
(10 −100 in practice) such that the correlation be-
tween the feature vectors for pivots in the two do-
mains are maximised in this latent space.
Let X and Y denote matrices formed by ar-
ranging respectively the vectors w(i)
</bodyText>
<subsectionHeader confidence="0.555352">
S s and w(i)T in
</subsectionHeader>
<bodyText confidence="0.9855255">
rows. PLSR decomposes X and Y into a series of
products between rank 1 matrices as follows:
</bodyText>
<equation confidence="0.997678">
L
X ≈ λlpl&gt; = AP&gt; (1)
l=1
L
Y ≈ γlql&gt; = FQ&gt;. (2)
l=1
</equation>
<bodyText confidence="0.999665733333333">
Here, λl, γl, pl, and ql are column vectors, and
the summation is taken over the rank 1 matrices
that result from the outer product of those vectors.
The matrices, A, F, P, and Q are constructed re-
spectively by arranging λl, γl, pl, and ql vectors
as columns.
Our method for learning a distribution predic-
tion model is shown in Algorithm 1. It is based on
the two block NIPALS routine (Wold, 1975; Rosi-
pal and Kramer, 2006) and iteratively discovers L
pairs of vectors (λl, γl) such that the covariances,
Cov(λl, γl), are maximised under the constraint
||pl ||= ||ql ||= 1. Finally, the prediction matrix,
M is computed using λl, γl, pl, ql. The predicted
distribution ˆwT of a word w in T is given by
</bodyText>
<equation confidence="0.981005">
ˆwT = MwS. (3)
</equation>
<page confidence="0.9716">
616
</page>
<bodyText confidence="0.9999562">
Our distribution prediction learning method is un-
supervised in the sense that it does not require
manually labeled data for a particular task from
any of the domains. This is an important point,
and means that the distribution prediction method
is independent of the task to which it may subse-
quently be applied. As we go on to show in Sec-
tion 6, this enables us to use the same distribution
prediction method for both POS tagging and sen-
timent classification.
</bodyText>
<sectionHeader confidence="0.996732" genericHeader="method">
4 Domain Adaptation
</sectionHeader>
<bodyText confidence="0.999964">
The main reason that a model trained only on the
source domain labeled data performs poorly in
the target domain is the feature mismatch – few
features in target domain test instances appear in
source domain training instances. To overcome
this problem, we use the proposed distribution pre-
diction method to find those related features in the
source domain that correspond to the features ap-
pearing in the target domain test instances.
We consider two DA tasks: (a) cross-domain
POS tagging (Section 4.1), and (b) cross-domain
sentiment classification (Section 4.2). Note that
our proposed distribution prediction method can
be applied to numerous other NLP tasks that in-
volve sequence labelling and document classifica-
tion.
</bodyText>
<subsectionHeader confidence="0.985067">
4.1 Cross-Domain POS Tagging
</subsectionHeader>
<bodyText confidence="0.972365913043479">
We represent each word using a set of features
such as capitalisation (whether the first letter of the
word is capitalised), numeric (whether the word
contains digits), prefixes up to four letters, and
suffixes up to four letters (Miller et al., 2011).
Next, for each word w in a source domain labeled
(i.e. manually POS tagged) sentence, we select its
neighbours u(i) in the source domain as additional
features. Specifically, we measure the similarity,
sim(u(i)
S , wS), between the source domain distri-
butions of u(i) and w, and select the top r simi-
lar neighbours u(i) for each word w as additional
features for w. We refer to such features as dis-
tributional features in this work. The value of a
neighbour u(i) selected as a distributional feature
is set to its similarity score sim(u(i)
S , wS). Next,
we train a CRF model using all features (i.e. cap-
italisation, numeric, prefixes, suffixes, and distri-
butional features) on source domain labeled sen-
tences.
We train a PLSR model, M, that predicts the
target domain distribution Mu(i)
S of a word u(i) in
the source domain labeled sentences, given its dis-
tribution, u(i)
S . At test time, for each word w that
appears in a target domain test sentence, we mea-
sure the similarity, sim(Mu(i)
S , wT ), and select
the most similar r words u(i) in the source domain
labeled sentences as the distributional features for
w, with their values set to sim(Mu(i)
S , wT ). Fi-
nally, the trained CRF model is applied to a target
domain test sentence.
Note that distributional features are always se-
lected from the source domain during both train
and test times, thereby increasing the number of
overlapping features between the trained model
and test sentences. To make the inference tractable
and efficient, we use a first-order Markov factori-
sation, in which we consider all pairwise combi-
nations between the features for the current word
and its immediate predecessor.
</bodyText>
<subsectionHeader confidence="0.960594">
4.2 Cross-Domain Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.998886029411765">
Unlike in POS tagging, where we must individ-
ually tag each word in a target domain test sen-
tence, in sentiment classification we must classify
the sentiment for the entire review. We modify the
DA method presented in Section 4.1 to satisfy this
requirement as follows.
Let us assume that we are given a set
{(x(i)
S , y(i))}ni=1 of n labeled reviews x(i)
S for the
source domain S. For simplicity, let us consider
binary sentiment classification where each review
x(i) is labeled either as positive (i.e. y(i) = 1) or
negative (i.e. y(i) = −1). Our cross-domain bi-
nary sentiment classification method can be easily
extended to the multi-class setting as well. First,
we lemmatise each word in a source domain la-
beled review x(i)
S , and extract both unigrams and
bigrams as features to represent x(i)
S by a binary-
valued feature vector. Next, we train a binary clas-
sification model, θ, using those feature vectors.
Any binary classification algorithm can be used
to learn θ. In our experiments, we used L2 reg-
ularised logistic regression.
Next, we train a PLSR model, M, as described
in Section 3.2 using unlabeled reviews in the
source and target domains. At test time, we rep-
resent a test target review H using a binary-valued
feature vector h of unigrams and bigrams of lem-
mas of the words in H, as we did for source do-
main labeled train reviews. Next, for each feature
w(j) extracted from H, we measure the similarity,
</bodyText>
<page confidence="0.896583">
617
</page>
<equation confidence="0.8034915">
sim(Mu(i)
S , w(j)
</equation>
<bodyText confidence="0.994438368421053">
T ), between the target domain dis-
tribution of w(j), and each feature (unigram or bi-
gram) u(i) in the source domain labeled reviews.
We score each source domain feature u(i) for its
relatedness to H using the formula:
where |H |denotes the total number of features ex-
tracted from the test review H. We select the top
scoring r features u(i) as distributional features for
H, and append those to h. The corresponding val-
ues of those distributional features are set to the
scores given by Equation 4. Finally, we classify
h using the trained binary classifier θ. Note that
given a test review, we find the distributional fea-
tures that are similar to all the words in the test re-
view from the source domain. In particular, we do
not find distributional features independently for
each word in the test review. This enables us to
find distributional features that are consistent with
all the features in a test review.
</bodyText>
<subsectionHeader confidence="0.999249">
4.3 Model Choices
</subsectionHeader>
<bodyText confidence="0.999978">
For both POS tagging and sentiment classifica-
tion, we experimented with several alternative
approaches for feature weighting, representation,
and similarity measures using development data,
which we randomly selected from the training in-
stances from the datasets described in Section 5.
For feature weighting for sentiment classifica-
tion, we considered using the number of occur-
rences of a feature in a review and tf-idf weight-
ing (Salton and Buckley, 1983). For representa-
tion, we considered distributional features u(i) in
descending order of their scores given by Equa-
tion 4, and then taking the inverse-rank as the val-
ues for the distributional features (Bollegala et al.,
2011). However, none of these alternatives re-
sulted in performance gains. With respect to simi-
larity measures, we experimented with cosine sim-
ilarity and the similarity measure proposed by Lin
(1998); cosine similarity performed consistently
well over all the experimental settings. The feature
representation was held fixed during these similar-
ity measure comparisons.
For POS tagging, we measured the effect of
varying r, the number of distributional features,
using a development dataset. We observed that
setting r larger than 10 did not result in signifi-
cant improvements in tagging accuracy, but only
increased the train time due to the larger feature
space. Consequently, we set r = 10 in POS tag-
ging. For sentiment analysis, we used all features
in the source domain labeled reviews as distri-
butional features, weighted by their scores given
by Equation 4, taking the inverse-rank. In both
tasks, we parallelised similarity computations us-
ing BLAS3 level-3 routines to speed up the com-
putations. The source code of our implementation
is publicly available4.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="method">
5 Datasets
</sectionHeader>
<bodyText confidence="0.9999835">
To evaluate DA for POS tagging, following Blitzer
et al. (2006), we use sections 2 − 21 from Wall
Street Journal (WSJ) as the source domain labeled
data. An additional 100, 000 WSJ sentences from
the 1988 release of the WSJ corpus are used as the
source domain unlabeled data. Following Schn-
abel and Sch¨utze (2013), we use the POS labeled
sentences in the SACNL dataset (Petrov and Mc-
Donald, 2012) for the five target domains: QA fo-
rums, Emails, Newsgroups, Reviews, and Blogs.
Each target domain contains around 1000 POS
labeled test sentences and around 100, 000 unla-
beled sentences.
To evaluate DA for sentiment classification,
we use the Amazon product reviews collected by
Blitzer et al. (2007) for four different product cat-
egories: books (B), DVDs (D), electronic items
(E), and kitchen appliances (K). There are 1000
positive and 1000 negative sentiment labeled re-
views for each domain. Moreover, each domain
has on average 17, 547 unlabeled reviews. We use
the standard split of 800 positive and 800 negative
labeled reviews from each domain as training data,
and the remainder for testing.
</bodyText>
<sectionHeader confidence="0.998353" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9999444">
For each domain D in the SANCL (POS tag-
ging) and Amazon review (sentiment classifica-
tion) datasets, we create a PPMI weighted co-
occurrence matrix FD. On average, FD created
for a target domain in the SANCL dataset con-
tains 104, 598 rows and 65, 528 columns, whereas
those numbers in the Amazon dataset are 27,397
and 35,200 respectively. In cross-domain senti-
ment classification, we measure the binary senti-
ment classification accuracy for the target domain
</bodyText>
<footnote confidence="0.947182333333333">
3http://www.openblas.net/
4http://www.csc.liv.ac.uk/˜danushka/
software.html
</footnote>
<equation confidence="0.985634666666667">
|H|
score(u(i),H) = |HI Lsim(MusZ),wT�) (4)
j=1
</equation>
<page confidence="0.989324">
618
</page>
<bodyText confidence="0.9998971875">
test reviews for each pair of domains (12 pairs in
total for 4 domains). On average, we have 40,176
pivots for a pair of domains in the Amazon dataset.
In cross-domain POS tagging, WSJ is always
the source domain, whereas the five domains in
SANCL dataset are considered as the target do-
mains. For this setting we have 9822 pivots on
average. The number of singular vectors k se-
lected in SVD, and the number of PLSR dimen-
sions L are set respectively to 1000 and 50 for the
remainder of the experiments described in the pa-
per. Later we study the effect of those two param-
eters on the performance of the proposed method.
The L-BFGS (Liu and Nocedal, 1989) method is
used to train the CRF and logistic regression mod-
els.
</bodyText>
<subsectionHeader confidence="0.986942">
6.1 POS Tagging Results
</subsectionHeader>
<bodyText confidence="0.997698515151515">
Table 2 shows the token-level POS tagging accu-
racy for unseen words (i.e. words that appear in the
target domain test sentences but not in the source
domain labeled train sentences). By limiting the
evaluation to unseen words instead of all words,
we can evaluate the gain in POS tagging accuracy
solely due to DA. The NA (no-adapt) baseline sim-
ulates the effect of not performing any DA. Specif-
ically, in POS tagging, a CRF trained on source
domain labeled sentences is applied to target do-
main test sentences, whereas in sentiment classi-
fication, a logistic regression classifier trained us-
ing source domain labeled reviews is applied to the
target domain test reviews. The Spred baseline di-
rectly uses the source domain distributions for the
words instead of projecting them to the target do-
main. This is equivalent to setting the prediction
matrix M to the unit matrix. The Tpred baseline
uses the target domain distribution wT for a word
w instead of MwS. If w does not appear in the
target domain, then wT is set to the zero vector.
The Spred and Tpred baselines simulate the two al-
ternatives of using source and target domain dis-
tributions instead of learning a PLSR model. The
DA method proposed in Section 4.1 is shown as
the Proposed method. Filter denotes the train-
ing set filtering method proposed by Schnabel and
Sch¨utze (2013) for the DA of POS taggers.
From Table 2, we see that the Proposed method
achieves the best performance in all five domains,
followed by the Tpred baseline. Recall that the
Tpred baseline cannot find source domain words
that do not appear in the target domain as distri-
</bodyText>
<table confidence="0.998649333333333">
Target NA Spred Tpred Filter Proposed
QA 67.34 68.18 68.75 57.08 69.28†
Emails 65.62 66.62 67.07 65.61 67.09
Newsgroups 75.71 75.09 75.57 70.37 75.85†
Reviews 56.36 54.60 56.68 47.91 56.93†
Blogs 76.64 54.78 76.90 74.56 76.97†
</table>
<tableCaption confidence="0.999104">
Table 2: POS tagging accuracies on SANCL.
</tableCaption>
<bodyText confidence="0.999938842105263">
butional features for the words in the target do-
main test reviews. Therefore, when the overlap be-
tween the vocabularies used in the source and the
target domains is small, Tpred cannot reduce the
mismatch between the feature spaces. Poor perfor-
mance of the Spred baseline shows that the distri-
butions of a word in the source and target domains
are different to the extent that the distributional
features found using source domain distributions
are inadequate. The two baselines Spred and Tpred
collectively motivate our proposal to learn a distri-
bution prediction model from the source domain
to the target. The improvements of Proposed over
the previously proposed Filter are statistically sig-
nificant in all domains except the Emails domain
(denoted by † in Table 2 according to the Bino-
mial exact test at 95% confidence). However, the
differences between the Tpred and Proposed meth-
ods are not statistically significant.
</bodyText>
<subsectionHeader confidence="0.99963">
6.2 Sentiment Classification Results
</subsectionHeader>
<bodyText confidence="0.999951318181818">
In Figure 1, we compare the Proposed cross-
domain sentiment classification method (Section
4.2) against several baselines and the current state-
of-the-art methods. The baselines NA, Spred, and
Tpred are defined similarly as in Section 6.1. SST
is the Sentiment Sensitive Thesaurus proposed by
Bollegala et al. (2011). SST creates a single distri-
bution for a word using both source and target do-
main reviews, instead of two separate distributions
as done by the Proposed method. SCL denotes
the Structural Correspondence Learning method
proposed by Blitzer et al. (2006). SFA denotes
the Spectral Feature Alignment method proposed
by Pan et al. (2010). SFA and SCL represent the
current state-of-the-art methods for cross-domain
sentiment classification. All methods are evalu-
ated under the same settings, including train/test
split, feature spaces, pivots, and classification al-
gorithms so that any differences in performance
can be directly attributable to their domain adapt-
ability. For each domain, the accuracy obtained
by a classifier trained using labeled data from that
</bodyText>
<page confidence="0.990944">
619
</page>
<figure confidence="0.997903685714286">
E−&gt;B D−&gt;B K−&gt;B
85
80
75
Accuracy
70
65
60
55
50
90
80
Accuracy
70
60
50
B−&gt;E D−&gt;E K−&gt;E
Accuracy
85
80
75
Accuracy
70
65
60
55
50
50
90
80
70
60
B−&gt;D E−&gt;D K−&gt;D
B−&gt;K E−&gt;K D−&gt;K
NA SFA SST SCL Spred Tpred Proposed
</figure>
<figureCaption confidence="0.999974">
Figure 1: Cross-Domain sentiment classification.
</figureCaption>
<bodyText confidence="0.999600454545454">
domain is indicated by a solid horizontal line in
each sub-figure. This upper baseline represents
the classification accuracy we could hope to obtain
if we were to have labeled data for the target do-
main. Clopper-Pearson 95% binomial confidence
intervals are superimposed on each vertical bar.
From Figure 1 we see that the Proposed method
reports the best results in 8 out of the 12 domain
pairs, whereas SCL, SFA, and Spred report the
best results in other cases. Except for the D-E set-
ting in which Proposed method significantly out-
performs both SFA and SCL, the performance of
the Proposed method is not statistically signifi-
cantly different to that of SFA or SCL.
The selection of pivots is vital to the perfor-
mance of SFA. However, unlike SFA, which re-
quires us to carefully select a small subset of pivots
(ca. less than 500) using some heuristic approach,
our Proposed method does not require any pivot
selection. Moreover, SFA projects source domain
reviews to a lower-dimensional latent space, in
which a binary sentiment classifier is subsequently
trained. At test time SFA projects a target review
into this lower-dimensional latent space and ap-
plies the trained classifier. In contrast, our Pro-
posed method predicts the distribution of a word
in the target domain, given its distribution in the
source domain, thereby explicitly translating the
source domain reviews to the target. This property
enables us to apply the proposed distribution pre-
diction method to tasks other than sentiment anal-
ysis such as POS tagging where we must identify
distributional features for individual words.
</bodyText>
<figure confidence="0.512718">
PLSR dimensions
</figure>
<figureCaption confidence="0.99976">
Figure 2: The effect of PLSR dimensions.
</figureCaption>
<bodyText confidence="0.999874944444445">
Unlike our distribution prediction method,
which is unsupervised, SST requires labeled data
for the source domain to learn a feature mapping
between a source and a target domain in the form
of a thesaurus. However, from Figure 1 we see
that in 10 out of the 12 domain-pairs the Proposed
method returns higher accuracies than SST.
To evaluate the overall effect of the number of
singular vectors k used in the SVD step, and the
number of PLSR components L used in Algorithm
1, we conduct two experiments. To evaluate the ef-
fect of the PLSR dimensions, we fixed k = 1000
and measured the cross-domain sentiment classi-
fication accuracy over a range of L values. As
shown in Figure 2, accuracy remains stable across
a wide range of PLSR dimensions. Because the
time complexity of Algorithm 1 increases linearly
with L, it is desirable that we select smaller L val-
</bodyText>
<figure confidence="0.990151133333333">
0.78
0.76
E−−&gt;B
D−−&gt;B
0.72
0.7
0.68
0.66
0.64
10 100 200 300 400 500 600 700 800
Accuracy
0.74
620
1000 1500 2000 2500 3000
SVD dimensions
</figure>
<figureCaption confidence="0.999908">
Figure 3: The effect of SVD dimensions.
</figureCaption>
<table confidence="0.999261333333333">
Measure Distributional features
sim(uS, wS) thin (0.1733), digestible (0.1728),
small+print (0.1722)
sim(uT , wT ) travel+companion (0.6018), snap-in
(0.6010), touchpad(0.6016)
sim(uS, wT ) segregation (0.1538), participation
(0.1512), depression+era (0.1508)
sim(MuS, wT ) small (0.2794), compact (0.2641),
sturdy (0.2561)
</table>
<tableCaption confidence="0.99663">
Table 3: Top 3 distributional features u ∈ S for
</tableCaption>
<bodyText confidence="0.989830367346939">
the word lightweight (w).
ues in practice.
To evaluate the effect of the SVD dimensions,
we fixed L = 100 and measured the cross-domain
sentiment classification accuracy for different k
values as shown in Figure 3. We see an overall
decrease in classification accuracy when k is in-
creased. Because the dimensionality of the source
and target domain feature spaces is equal to k, the
complexity of the least square regression problem
increases with k. Therefore, larger k values result
in overfitting to the train data and classification ac-
curacy is reduced on the target test data.
As an example of the distribution prediction
method, in Table 3 we show the top 3 similar
distributional features u in the books (source) do-
main, predicted for the electronics (target) domain
word w = lightweight, by different similarity
measures. Bigrams are indicted by a + sign and
the similarity scores of the distributional features
are shown within brackets.
Using the source domain distributions for both
u and w (i.e. sim(uS, wS)) produces distribu-
tional features that are specific to the books do-
main, or to the dominant adjectival sense of hav-
ing no importance or influence. On the other
hand, using target domain distributions for u and
w (i.e. sim(uT, wT )) returns distributional fea-
tures of the dominant nominal sense of lower in
weight frequently associated with electronic de-
vices. Simply using source domain distributions
uS (i.e. sim(uS, wT )) returns totally unrelated dis-
tributional features. This shows that word distribu-
tions in source and target domains are very differ-
ent and some adaptation is required prior to com-
puting distributional features.
Interestingly, we see that by using the dis-
tributions predicted by the proposed method
(i.e. sim(MuS, wT )) we overcome this problem
and find relevant distributional features from the
source domain. Although for illustrative purposes
we used the word lightweight, which occurs in
both the source and the target domains, our pro-
posed method does not require the source domain
distribution wS for a word w in a target domain
document. Therefore, it can find distributional fea-
tures even for words occurring only in the target
domain, thereby reducing the feature mismatch
between the two domains.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999867733333333">
We proposed a method to predict the distribution
of a word across domains. We first create a distri-
butional representation for a word using the data
from a single domain, and then learn a Partial
Least Square Regression (PLSR) model to pre-
dict the distribution of a word in a target domain
given its distribution in a source domain. We eval-
uated the proposed method in two domain adapta-
tion tasks: cross-domain POS tagging and cross-
domain sentiment classification. Our experiments
show that without requiring any task-specific cus-
tomisations to our distribution prediction method,
it outperforms competitive baselines and achieves
comparable results to the current state-of-the-art
domain adaptation methods.
</bodyText>
<sectionHeader confidence="0.997391" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967472333333333">
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case
study. Technical report, Microsoft Research.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673 – 721.
Romaric Besanc¸on, Martin Rajman, and Jean-C´edric
Chappelier. 1999. Textual similarities based on a
</reference>
<figure confidence="0.999235538461539">
Accuracy
0.76
0.74
0.72
0.68
0.66
0.64
0.62
0.58
0.7
0.6
E−−&gt;B
D−−&gt;B
</figure>
<page confidence="0.975262">
621
</page>
<reference confidence="0.999823695238095">
distributional approach. In Proc. of DEXA, pages
180 – 184.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120 –
128.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proc. of ACL, pages 440 – 447.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a senti-
ment sensitive thesaurus for cross-domain sentiment
classification. In Proc. of ACL/HLT, pages 132 –
141.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proc. of
COLING/ACL Interactive Presentation Sessions.
John A. Bullinaria and Jospeh P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510 – 526.
Jinho D. Choi and Martha Palmer. 2012. Fast and
robust part-of-speech tagging using dynamic model
selection. In Proc. of ACL Short Papers, volume 2,
pages 363 – 367.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22 – 29,
March.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 26 – 33.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proc. of ACL, pages 256 – 263.
John R. Firth. 1957. A synopsis of linguistic theory
1930-55. Studies in Linguistic Analysis, pages 1 –
32.
Paul Geladi and Bruce R. Kowalski. 1986. Partial
least-squares regression: a tutorial. Analytica Chim-
ica Acta, 185(0):1 – 17.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named en-
tity recognition. In Proc. of NAACL, pages 281 –
289.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proc. of
ACL/HLT, pages 123 – 131.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL-IJCNLP’09, pages 495
– 503.
Fei Huang and Alexander Yates. 2012. Biased repre-
sentation learning for domain adaptation. In Proc.
of EMNLP/CoNLL, pages 1313 – 1323.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2012. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359 – 389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL, pages 768 – 774.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503 – 528.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proc. of NAACL/HLT, pages 28 – 36.
John E. Miller, Manabu Torii, and K. Vijay-Shanker.
2011. Building domain-specific taggers without an-
notated (domain) data. In Proc. of EMNLP/CoNLL,
pages 1103 – 1111.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161 –
199.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proc. of WWW, pages 751 – 760.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proc. of EMNLP, pages 938 – 947.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes of
the 1st SANCL Workshop.
Natalia Ponomareva and Mike Thelwall. 2012. Do
neighbours help? an exploration of graph-based al-
gorithms for cross-domain sentiment classification.
In Proc. of EMNLP, pages 655 – 665.
Roman Rosipal and Nicole Kramer. 2006. Overview
and recent advances in partial least squares. In
C. Saunders et al., editor, SLSFS’05, volume 3940 of
LNCS, pages 34 – 51, Berlin Heidelberg. Springer-
Verlag.
G. Salton and C. Buckley. 1983. Introduction to
Modern Information Retreival. McGraw-Hill Book
Company.
Tobias Schnabel and Hinrich Sch¨utze. 2013. Towards
robust cross-domain domain adaptation for part-of-
speech tagging. In Proc. of IJCNLP, pages 198 –
206.
</reference>
<page confidence="0.976913">
622
</page>
<reference confidence="0.999622185185185">
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proc. of EMNLP, pages 1631 – 1642.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Aritificial Intelligence Research,
37:141 – 188.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antonie Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. Journal of Machine Learning
Research, 11:3371 – 3408.
Herman Wold. 1975. Path models with latent vari-
ables: the NIPALS approach. In H. M. Blalock
et al., editor, Quantitative socialogy: international
perspective on mathematical and statistical model-
ing, pages 307 – 357. Academic.
Herman Wold. 1985. Partial least squares. In Samel
Kotz and Norman L. Johnson, editors, Encyclopedia
of the Statistical Sciences, pages 581 – 591. Wiley.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In
Proc. of EMNLP, pages 152 – 162.
</reference>
<page confidence="0.99916">
623
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.236314">
<title confidence="0.99945">Learning to Predict Distributions of Words Across Domains</title>
<author confidence="0.93877">Danushka</author>
<affiliation confidence="0.999597">Department of Computer University of</affiliation>
<address confidence="0.457471">L69 3BX,</address>
<email confidence="0.994079">liverpool.ac.uk</email>
<author confidence="0.95801">David</author>
<affiliation confidence="0.950893">Department of University of Falmer,</affiliation>
<address confidence="0.9315">BN1 9QJ,</address>
<email confidence="0.994541">sussex.ac.uk</email>
<author confidence="0.892465">John</author>
<affiliation confidence="0.950723333333333">Department of University of Falmer,</affiliation>
<address confidence="0.931687">BN1 9QJ,</address>
<email confidence="0.998354">sussex.ac.uk</email>
<abstract confidence="0.99910348">Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word’s predominant meaning changes. However, if it were posto the distribution of a word changes from one domain to another, the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsupervised method to predict the distribution of a word in one domain, given its distribution in another domain. We evaluate our method on two tasks: cross-domain partof-speech tagging and cross-domain sentiment classification. In both tasks, our method significantly outperforms competitive baselines and returns results that are statistically comparable to current stateof-the-art methods, while requiring no task-specific customisations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: a case study.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="2969" citStr="Aue and Gamon, 2005" startWordPosition="456" endWordPosition="459"> associated with negative sentimentbearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably between the two domains. In this paper, given the distribution wS of a word w in the source domain S, we propose an unsupervised method for predicting its distribution wT in a different target domain T . The ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks. For example, unsupervised cross-domain sentiment classification (Blitzer et al., 2007; Aue and Gamon, 2005) involves using sentiment-labeled user reviews from the source domain, and unlabeled reviews from both the source and the target domains to learn a sentiment classifier for the target domain. Domain adaptation (DA) of sentiment classification becomes extremely challenging when the distributions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not appear in the target domain reviews that must be classified. By predicting the distribution of a word across different domains, we can find source domain feature</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case study. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>721</pages>
<contexts>
<context position="6260" citStr="Baroni and Lenci, 2010" startWordPosition="994" endWordPosition="997"> a method for learning its distribution wT in a target domain T . • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single semantic representation for a word using documents from a single domain. The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incor</context>
<context position="13254" citStr="Baroni and Lenci, 2010" startWordPosition="2142" endWordPosition="2145">n zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr, and columns, nc, as the raw co-occurrence matrix A. Note that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the window of co-occurrence to the entire document (Baroni and Lenci, 2010). Since the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular 615 feature representation method, any of these alternative methods could be used. To reduce the dimensionality of the feature space, and create dense representations for words, we perform SVD on F. We use the left singular vectors corresponding to the k largest singular values to compute a rank k approximation ˆF, of F. We perform truncated SVD using SVDLIBC2. Each row in Fˆ is considered as representing a word in a lower k («nc) dimensional feature space corresp</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673 – 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Romaric Besanc¸on</author>
<author>Martin Rajman</author>
<author>Jean-C´edric Chappelier</author>
</authors>
<title>Textual similarities based on a distributional approach.</title>
<date>1999</date>
<booktitle>In Proc. of DEXA,</booktitle>
<pages>180--184</pages>
<marker>Besanc¸on, Rajman, Chappelier, 1999</marker>
<rawString>Romaric Besanc¸on, Martin Rajman, and Jean-C´edric Chappelier. 1999. Textual similarities based on a distributional approach. In Proc. of DEXA, pages 180 – 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="6809" citStr="Blitzer et al., 2006" startWordPosition="1090" endWordPosition="1093">tely (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single semantic representation for a word using documents from a single domain. The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved fr</context>
<context position="24930" citStr="Blitzer et al. (2006)" startWordPosition="4177" endWordPosition="4180">larger than 10 did not result in significant improvements in tagging accuracy, but only increased the train time due to the larger feature space. Consequently, we set r = 10 in POS tagging. For sentiment analysis, we used all features in the source domain labeled reviews as distributional features, weighted by their scores given by Equation 4, taking the inverse-rank. In both tasks, we parallelised similarity computations using BLAS3 level-3 routines to speed up the computations. The source code of our implementation is publicly available4. 5 Datasets To evaluate DA for POS tagging, following Blitzer et al. (2006), we use sections 2 − 21 from Wall Street Journal (WSJ) as the source domain labeled data. An additional 100, 000 WSJ sentences from the 1988 release of the WSJ corpus are used as the source domain unlabeled data. Following Schnabel and Sch¨utze (2013), we use the POS labeled sentences in the SACNL dataset (Petrov and McDonald, 2012) for the five target domains: QA forums, Emails, Newsgroups, Reviews, and Blogs. Each target domain contains around 1000 POS labeled test sentences and around 100, 000 unlabeled sentences. To evaluate DA for sentiment classification, we use the Amazon product revie</context>
<context position="30731" citStr="Blitzer et al. (2006)" startWordPosition="5144" endWordPosition="5147">lly significant. 6.2 Sentiment Classification Results In Figure 1, we compare the Proposed crossdomain sentiment classification method (Section 4.2) against several baselines and the current stateof-the-art methods. The baselines NA, Spred, and Tpred are defined similarly as in Section 6.1. SST is the Sentiment Sensitive Thesaurus proposed by Bollegala et al. (2011). SST creates a single distribution for a word using both source and target domain reviews, instead of two separate distributions as done by the Proposed method. SCL denotes the Structural Correspondence Learning method proposed by Blitzer et al. (2006). SFA denotes the Spectral Feature Alignment method proposed by Pan et al. (2010). SFA and SCL represent the current state-of-the-art methods for cross-domain sentiment classification. All methods are evaluated under the same settings, including train/test split, feature spaces, pivots, and classification algorithms so that any differences in performance can be directly attributable to their domain adaptability. For each domain, the accuracy obtained by a classifier trained using labeled data from that 619 E−&gt;B D−&gt;B K−&gt;B 85 80 75 Accuracy 70 65 60 55 50 90 80 Accuracy 70 60 50 B−&gt;E D−&gt;E K−&gt;E A</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of EMNLP, pages 120 – 128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="2947" citStr="Blitzer et al., 2007" startWordPosition="452" endWordPosition="455">the same word is often associated with negative sentimentbearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably between the two domains. In this paper, given the distribution wS of a word w in the source domain S, we propose an unsupervised method for predicting its distribution wT in a different target domain T . The ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks. For example, unsupervised cross-domain sentiment classification (Blitzer et al., 2007; Aue and Gamon, 2005) involves using sentiment-labeled user reviews from the source domain, and unlabeled reviews from both the source and the target domains to learn a sentiment classifier for the target domain. Domain adaptation (DA) of sentiment classification becomes extremely challenging when the distributions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not appear in the target domain reviews that must be classified. By predicting the distribution of a word across different domains, we can find</context>
<context position="8004" citStr="Blitzer et al., 2007" startWordPosition="1291" endWordPosition="1294">with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words. The sentiment of a word can vary from one domain to another. In Structural Correspondence Learning (SCL) (Blitzer et al., 2006; Blitzer et al., 2007), a set of pivots are chosen using pointwise mutual information. Linear predictors are then learnt to predict the occurrence of those pivots, and SVD is used to construct a lower dimensional representation in which a binary classifier is trained. Spectral Feature Alignment (SFA) (Pan et al., 2010) also uses pivots to compute an alignment between domain specific and domain independent features. Spectral clustering is performed on a bipartite graph representing domain specific and domain independent features to find a lowerdimensional projection between the two sets of features. The cross-domain</context>
<context position="14859" citStr="Blitzer et al., 2007" startWordPosition="2421" endWordPosition="2424"> Feature Vector Prediction We propose a method to learn a model that can predict the distribution wT of a word w in the target domain T, given its distribution wS in the source domain S. We denote the set of features that occur in both domains by W = {w(1), ... , w(n)}. In the literature, such features are often referred to as pivots, and they have been shown to be useful for DA, allowing the weights learnt to be transferred from one domain to another. Various criteria have been proposed for selecting a small set of pivots for DA, such as the mutual information of a word with the two domains (Blitzer et al., 2007). However, we do not impose any further restrictions on the set of pivots W other than that they occur in both domains. For each word w(i) ∈ W, we denote the corresponding rows in ˆFS and ˆFT by column vectors w(i) S and w(i)T . Note that the dimensionality of w(i) S and w(i)T need not be equal, and we may select different numbers of singular vectors to approximate ˆFS and ˆFT . We model distribution prediction as a multivariate regression problem where, given a set {(w(i) S , w(i)T )}ni=1 consisting of pairs of feature vectors selected from each domain for the pivots in W, we learn a mapping </context>
<context position="25567" citStr="Blitzer et al. (2007)" startWordPosition="4285" endWordPosition="4288">s 2 − 21 from Wall Street Journal (WSJ) as the source domain labeled data. An additional 100, 000 WSJ sentences from the 1988 release of the WSJ corpus are used as the source domain unlabeled data. Following Schnabel and Sch¨utze (2013), we use the POS labeled sentences in the SACNL dataset (Petrov and McDonald, 2012) for the five target domains: QA forums, Emails, Newsgroups, Reviews, and Blogs. Each target domain contains around 1000 POS labeled test sentences and around 100, 000 unlabeled sentences. To evaluate DA for sentiment classification, we use the Amazon product reviews collected by Blitzer et al. (2007) for four different product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). There are 1000 positive and 1000 negative sentiment labeled reviews for each domain. Moreover, each domain has on average 17, 547 unlabeled reviews. We use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data, and the remainder for testing. 6 Experiments and Results For each domain D in the SANCL (POS tagging) and Amazon review (sentiment classification) datasets, we create a PPMI weighted cooccurrence matrix FD. On average, FD created </context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. of ACL, pages 440 – 447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proc. of ACL/HLT,</booktitle>
<pages>132--141</pages>
<contexts>
<context position="8665" citStr="Bollegala et al., 2011" startWordPosition="1393" endWordPosition="1396">ntwise mutual information. Linear predictors are then learnt to predict the occurrence of those pivots, and SVD is used to construct a lower dimensional representation in which a binary classifier is trained. Spectral Feature Alignment (SFA) (Pan et al., 2010) also uses pivots to compute an alignment between domain specific and domain independent features. Spectral clustering is performed on a bipartite graph representing domain specific and domain independent features to find a lowerdimensional projection between the two sets of features. The cross-domain sentiment-sensitive thesaurus (SST) (Bollegala et al., 2011) groups together words that express similar sentiments in 614 different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon. Ponom</context>
<context position="23795" citStr="Bollegala et al., 2011" startWordPosition="3998" endWordPosition="4001">imented with several alternative approaches for feature weighting, representation, and similarity measures using development data, which we randomly selected from the training instances from the datasets described in Section 5. For feature weighting for sentiment classification, we considered using the number of occurrences of a feature in a review and tf-idf weighting (Salton and Buckley, 1983). For representation, we considered distributional features u(i) in descending order of their scores given by Equation 4, and then taking the inverse-rank as the values for the distributional features (Bollegala et al., 2011). However, none of these alternatives resulted in performance gains. With respect to similarity measures, we experimented with cosine similarity and the similarity measure proposed by Lin (1998); cosine similarity performed consistently well over all the experimental settings. The feature representation was held fixed during these similarity measure comparisons. For POS tagging, we measured the effect of varying r, the number of distributional features, using a development dataset. We observed that setting r larger than 10 did not result in significant improvements in tagging accuracy, but onl</context>
<context position="30478" citStr="Bollegala et al. (2011)" startWordPosition="5103" endWordPosition="5106">usly proposed Filter are statistically significant in all domains except the Emails domain (denoted by † in Table 2 according to the Binomial exact test at 95% confidence). However, the differences between the Tpred and Proposed methods are not statistically significant. 6.2 Sentiment Classification Results In Figure 1, we compare the Proposed crossdomain sentiment classification method (Section 4.2) against several baselines and the current stateof-the-art methods. The baselines NA, Spred, and Tpred are defined similarly as in Section 6.1. SST is the Sentiment Sensitive Thesaurus proposed by Bollegala et al. (2011). SST creates a single distribution for a word using both source and target domain reviews, instead of two separate distributions as done by the Proposed method. SCL denotes the Structural Correspondence Learning method proposed by Blitzer et al. (2006). SFA denotes the Spectral Feature Alignment method proposed by Pan et al. (2010). SFA and SCL represent the current state-of-the-art methods for cross-domain sentiment classification. All methods are evaluated under the same settings, including train/test split, feature spaces, pivots, and classification algorithms so that any differences in pe</context>
</contexts>
<marker>Bollegala, Weir, Carroll, 2011</marker>
<rawString>Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification. In Proc. of ACL/HLT, pages 132 – 141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL Interactive Presentation Sessions.</booktitle>
<contexts>
<context position="10599" citStr="Briscoe et al., 2006" startWordPosition="1713" endWordPosition="1716">t can be used to further improve the performance of DA tasks (Xiao et al., 2013; Daum´e III, 2007). 3 Distribution Prediction 3.1 In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows. Given a document H, such as a user-review of a product, we split H into sentences, and lemmatize each word in a sentence using the RASP system (Briscoe et al., 2006). Using a standard stop word list, we filter out frequent non-content unigrams and select the remainder as unigram features to represent a sentence. Next, we generate bigrams of word lemmas and remove any bigrams that consists only of stop words. Bigram features capture negations more accurately than unigrams, and have been found to be useful for sentiment classification tasks. Table 1 shows the unigram and bigram features we extract for a sentence using this procedure. Using data from a single dosentence This is an interesting and well researched book unigrams this, is, an, interesting, and, </context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proc. of COLING/ACL Interactive Presentation Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Jospeh P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<pages>526</pages>
<contexts>
<context position="12699" citStr="Bullinaria and Levy, 2007" startWordPosition="2044" endWordPosition="2048">gram and a bigram. Consequently, in matrix A, we consider co-occurrences only between unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as representing the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr, and columns, nc, as the raw co-occurrence matrix A. Note that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the window of co-occurrence to the entire document (Baroni and Lenci, 2010). Since the method we propose in Section 3.2 </context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Jospeh P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39(3):510 – 526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Fast and robust part-of-speech tagging using dynamic model selection.</title>
<date>2012</date>
<booktitle>In Proc. of ACL Short Papers,</booktitle>
<volume>2</volume>
<pages>363--367</pages>
<contexts>
<context position="7099" citStr="Choi and Palmer (2012)" startWordPosition="1142" endWordPosition="1145">a single domain. The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter </context>
</contexts>
<marker>Choi, Palmer, 2012</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2012. Fast and robust part-of-speech tagging using dynamic model selection. In Proc. of ACL Short Papers, volume 2, pages 363 – 367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="12588" citStr="Church and Hanks, 1990" startWordPosition="2024" endWordPosition="2027"> cooccurrences of bigrams are rare compared to cooccurrences of unigrams, and co-occurrences involving a unigram and a bigram. Consequently, in matrix A, we consider co-occurrences only between unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as representing the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr, and columns, nc, as the raw co-occurrence matrix A. Note that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the windo</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22 – 29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>Supersense tagging of unknown nouns using semantic similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="1939" citStr="Curran, 2005" startWordPosition="292" endWordPosition="293">hile requiring no task-specific customisations. 1 Introduction The Distributional Hypothesis, summarised by the memorable line of Firth (1957) – You shall know a word by the company it keeps – has inspired a diverse range of research in natural language processing. In such work, a word is represented by the distribution of other words that co-occur with it. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al., 2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc¸on et al., 1999), and lexical inference (Kotlerman et al., 2012). However, the distribution of a word often varies from one domain1 to another. For example, in the domain of portable computer reviews the word lightweight is often associated with positive sentiment bearing words such as sleek or compact, whereas in the movie review domain the same word is often associated with negative sentimentbearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably between t</context>
</contexts>
<marker>Curran, 2005</marker>
<rawString>James Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 26 – 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proc. of ACL, pages 256 – 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-55. Studies in Linguistic Analysis,</title>
<date>1957</date>
<pages>1--32</pages>
<contexts>
<context position="1468" citStr="Firth (1957)" startWordPosition="214" endWordPosition="215"> to adapt a system trained in one domain to work in another. We propose an unsupervised method to predict the distribution of a word in one domain, given its distribution in another domain. We evaluate our method on two tasks: cross-domain partof-speech tagging and cross-domain sentiment classification. In both tasks, our method significantly outperforms competitive baselines and returns results that are statistically comparable to current stateof-the-art methods, while requiring no task-specific customisations. 1 Introduction The Distributional Hypothesis, summarised by the memorable line of Firth (1957) – You shall know a word by the company it keeps – has inspired a diverse range of research in natural language processing. In such work, a word is represented by the distribution of other words that co-occur with it. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al., 2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc¸on et al., 1999), and lexical inference (Kotlerman et al., 2012). However, the di</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A synopsis of linguistic theory 1930-55. Studies in Linguistic Analysis, pages 1 – 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Geladi</author>
<author>Bruce R Kowalski</author>
</authors>
<title>Partial least-squares regression: a tutorial.</title>
<date>1986</date>
<journal>Analytica Chimica Acta,</journal>
<volume>185</volume>
<issue>0</issue>
<contexts>
<context position="16238" citStr="Geladi and Kowalski, 1986" startWordPosition="2693" endWordPosition="2696"> vectors. PLSR has been applied in 2http://tedlab.mit.edu/˜dr/SVDLIBC/ Algorithm 1 Learning a prediction model. Input: X, Y, L. Output: Prediction matrix M. 1: Randomly select γl from columns in Yl. ���� 2: vl = XlTγl/ ����XlTγl 3: λl = Xlvl ���� 4: ql = YlTλl/ ����YlTλl 5: γl = Ylql 6: If γl is unchanged go to Line 7; otherwise go to Line 2 ���� 7: cl = λlTγl/ ����λlTγl 8: pl = XlTλl/λlTλl 9: Xl+1 = Xl − λlplT and Yl+1 = Yl − clλlqlT. 10: Stop if l = L; otherwise l = l + 1 and return to Line 1. 11: Let C = diag(c1, ... , cL), and V = [v1 ... vL] 12: M = V(PTV)−1CQT 13: return M Chemometrics (Geladi and Kowalski, 1986), producing stable prediction models even when the number of samples is considerably smaller than the dimensionality of the feature space. In particular, PLSR fits a smaller number of latent variables (10 −100 in practice) such that the correlation between the feature vectors for pivots in the two domains are maximised in this latent space. Let X and Y denote matrices formed by arranging respectively the vectors w(i) S s and w(i)T in rows. PLSR decomposes X and Y into a series of products between rank 1 matrices as follows: L X ≈ λlpl&gt; = AP&gt; (1) l=1 L Y ≈ γlql&gt; = FQ&gt;. (2) l=1 Here, λl, γl, pl,</context>
</contexts>
<marker>Geladi, Kowalski, 1986</marker>
<rawString>Paul Geladi and Bruce R. Kowalski. 1986. Partial least-squares regression: a tutorial. Analytica Chimica Acta, 185(0):1 – 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Huijia Zhu</author>
<author>Zhili Guo</author>
<author>Xiaoxun Zhang</author>
<author>Xian Wu</author>
<author>Zhong Su</author>
</authors>
<title>Domain adaptation with latent semantic association for named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>281--289</pages>
<contexts>
<context position="5472" citStr="Guo et al., 2009" startWordPosition="857" endWordPosition="860">uire any labeled data in either of the two steps. Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain. Without requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: • Given the distribution wS of a word w in a source domain S, we propose a method for learning its distribution wT in a target domain T . • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related </context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Wu, Su, 2009</marker>
<rawString>Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, Xian Wu, and Zhong Su. 2009. Domain adaptation with latent semantic association for named entity recognition. In Proc. of NAACL, pages 281 – 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Chenghua Lin</author>
<author>Harith Alani</author>
</authors>
<title>Automatically extracting polarity-bearing topics for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proc. of ACL/HLT,</booktitle>
<pages>123--131</pages>
<contexts>
<context position="9128" citStr="He et al. (2011)" startWordPosition="1463" endWordPosition="1466">res to find a lowerdimensional projection between the two sets of features. The cross-domain sentiment-sensitive thesaurus (SST) (Bollegala et al., 2011) groups together words that express similar sentiments in 614 different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon. Ponomareva and Thelwall (2012) represent source and target domain reviews as nodes in a graph and apply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to</context>
</contexts>
<marker>He, Lin, Alani, 2011</marker>
<rawString>Yulan He, Chenghua Lin, and Harith Alani. 2011. Automatically extracting polarity-bearing topics for cross-domain sentiment classification. In Proc. of ACL/HLT, pages 123 – 131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence-labeling.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP’09,</booktitle>
<pages>495--503</pages>
<contexts>
<context position="1905" citStr="Huang and Yates, 2009" startWordPosition="286" endWordPosition="289">rable to current stateof-the-art methods, while requiring no task-specific customisations. 1 Introduction The Distributional Hypothesis, summarised by the memorable line of Firth (1957) – You shall know a word by the company it keeps – has inspired a diverse range of research in natural language processing. In such work, a word is represented by the distribution of other words that co-occur with it. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al., 2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc¸on et al., 1999), and lexical inference (Kotlerman et al., 2012). However, the distribution of a word often varies from one domain1 to another. For example, in the domain of portable computer reviews the word lightweight is often associated with positive sentiment bearing words such as sleek or compact, whereas in the movie review domain the same word is often associated with negative sentimentbearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight </context>
<context position="7336" citStr="Huang and Yates (2009)" startWordPosition="1182" endWordPosition="1185">t appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words. The sentiment of a word can vary from one domain to another. In Structural Corres</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence-labeling. In ACL-IJCNLP’09, pages 495 – 503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Biased representation learning for domain adaptation.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP/CoNLL,</booktitle>
<pages>1313--1323</pages>
<contexts>
<context position="7599" citStr="Huang and Yates, 2012" startWordPosition="1225" endWordPosition="1228"> words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words. The sentiment of a word can vary from one domain to another. In Structural Correspondence Learning (SCL) (Blitzer et al., 2006; Blitzer et al., 2007), a set of pivots are chosen using pointwise mutual information. Linear predictors are then learnt to predict the occurrence of those pivots, and SVD is used to construct a lower dimensional repr</context>
</contexts>
<marker>Huang, Yates, 2012</marker>
<rawString>Fei Huang and Alexander Yates. 2012. Biased representation learning for domain adaptation. In Proc. of EMNLP/CoNLL, pages 1313 – 1323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2012</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<pages>389</pages>
<contexts>
<context position="2051" citStr="Kotlerman et al., 2012" startWordPosition="307" endWordPosition="310">ed by the memorable line of Firth (1957) – You shall know a word by the company it keeps – has inspired a diverse range of research in natural language processing. In such work, a word is represented by the distribution of other words that co-occur with it. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al., 2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc¸on et al., 1999), and lexical inference (Kotlerman et al., 2012). However, the distribution of a word often varies from one domain1 to another. For example, in the domain of portable computer reviews the word lightweight is often associated with positive sentiment bearing words such as sleek or compact, whereas in the movie review domain the same word is often associated with negative sentimentbearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably between the two domains. In this paper, given the distribution wS of a word w in the source domain S, we propose an unsup</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2012</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2012. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359 – 389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="12671" citStr="Lin, 1998" startWordPosition="2042" endWordPosition="2043">lving a unigram and a bigram. Consequently, in matrix A, we consider co-occurrences only between unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as representing the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr, and columns, nc, as the raw co-occurrence matrix A. Note that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the window of co-occurrence to the entire document (Baroni and Lenci, 2010). Since the metho</context>
<context position="23989" citStr="Lin (1998)" startWordPosition="4030" endWordPosition="4031">cribed in Section 5. For feature weighting for sentiment classification, we considered using the number of occurrences of a feature in a review and tf-idf weighting (Salton and Buckley, 1983). For representation, we considered distributional features u(i) in descending order of their scores given by Equation 4, and then taking the inverse-rank as the values for the distributional features (Bollegala et al., 2011). However, none of these alternatives resulted in performance gains. With respect to similarity measures, we experimented with cosine similarity and the similarity measure proposed by Lin (1998); cosine similarity performed consistently well over all the experimental settings. The feature representation was held fixed during these similarity measure comparisons. For POS tagging, we measured the effect of varying r, the number of distributional features, using a development dataset. We observed that setting r larger than 10 did not result in significant improvements in tagging accuracy, but only increased the train time due to the larger feature space. Consequently, we set r = 10 in POS tagging. For sentiment analysis, we used all features in the source domain labeled reviews as distr</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of ACL, pages 768 – 774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="27234" citStr="Liu and Nocedal, 1989" startWordPosition="4562" endWordPosition="4565">omains (12 pairs in total for 4 domains). On average, we have 40,176 pivots for a pair of domains in the Amazon dataset. In cross-domain POS tagging, WSJ is always the source domain, whereas the five domains in SANCL dataset are considered as the target domains. For this setting we have 9822 pivots on average. The number of singular vectors k selected in SVD, and the number of PLSR dimensions L are set respectively to 1000 and 50 for the remainder of the experiments described in the paper. Later we study the effect of those two parameters on the performance of the proposed method. The L-BFGS (Liu and Nocedal, 1989) method is used to train the CRF and logistic regression models. 6.1 POS Tagging Results Table 2 shows the token-level POS tagging accuracy for unseen words (i.e. words that appear in the target domain test sentences but not in the source domain labeled train sentences). By limiting the evaluation to unseen words instead of all words, we can evaluate the gain in POS tagging accuracy solely due to DA. The NA (no-adapt) baseline simulates the effect of not performing any DA. Specifically, in POS tagging, a CRF trained on source domain labeled sentences is applied to target domain test sentences,</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503 – 528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL/HLT,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="5518" citStr="McClosky et al., 2010" startWordPosition="864" endWordPosition="867">o steps. Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain. Without requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: • Given the distribution wS of a word w in a source domain S, we propose a method for learning its distribution wT in a target domain T . • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for wor</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In Proc. of NAACL/HLT, pages 28 – 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Miller</author>
<author>Manabu Torii</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Building domain-specific taggers without annotated (domain) data.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP/CoNLL,</booktitle>
<pages>1103--1111</pages>
<contexts>
<context position="19028" citStr="Miller et al., 2011" startWordPosition="3181" endWordPosition="3184">res appearing in the target domain test instances. We consider two DA tasks: (a) cross-domain POS tagging (Section 4.1), and (b) cross-domain sentiment classification (Section 4.2). Note that our proposed distribution prediction method can be applied to numerous other NLP tasks that involve sequence labelling and document classification. 4.1 Cross-Domain POS Tagging We represent each word using a set of features such as capitalisation (whether the first letter of the word is capitalised), numeric (whether the word contains digits), prefixes up to four letters, and suffixes up to four letters (Miller et al., 2011). Next, for each word w in a source domain labeled (i.e. manually POS tagged) sentence, we select its neighbours u(i) in the source domain as additional features. Specifically, we measure the similarity, sim(u(i) S , wS), between the source domain distributions of u(i) and w, and select the top r similar neighbours u(i) for each word w as additional features for w. We refer to such features as distributional features in this work. The value of a neighbour u(i) selected as a distributional feature is set to its similarity score sim(u(i) S , wS). Next, we train a CRF model using all features (i.</context>
</contexts>
<marker>Miller, Torii, Vijay-Shanker, 2011</marker>
<rawString>John E. Miller, Manabu Torii, and K. Vijay-Shanker. 2011. Building domain-specific taggers without annotated (domain) data. In Proc. of EMNLP/CoNLL, pages 1103 – 1111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="13167" citStr="Pado and Lapata, 2007" startWordPosition="2128" endWordPosition="2131"> Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr, and columns, nc, as the raw co-occurrence matrix A. Note that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the window of co-occurrence to the entire document (Baroni and Lenci, 2010). Since the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular 615 feature representation method, any of these alternative methods could be used. To reduce the dimensionality of the feature space, and create dense representations for words, we perform SVD on F. We use the left singular vectors corresponding to the k largest singular values to compute a rank k approximation ˆF, of F. We perform truncated SVD using SVDLIBC2. Each row in Fˆ is</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161 – 199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Cross-domain sentiment classification via spectral feature alignment.</title>
<date>2010</date>
<booktitle>In Proc. of WWW,</booktitle>
<pages>751--760</pages>
<contexts>
<context position="8302" citStr="Pan et al., 2010" startWordPosition="1341" endWordPosition="1344">inate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words. The sentiment of a word can vary from one domain to another. In Structural Correspondence Learning (SCL) (Blitzer et al., 2006; Blitzer et al., 2007), a set of pivots are chosen using pointwise mutual information. Linear predictors are then learnt to predict the occurrence of those pivots, and SVD is used to construct a lower dimensional representation in which a binary classifier is trained. Spectral Feature Alignment (SFA) (Pan et al., 2010) also uses pivots to compute an alignment between domain specific and domain independent features. Spectral clustering is performed on a bipartite graph representing domain specific and domain independent features to find a lowerdimensional projection between the two sets of features. The cross-domain sentiment-sensitive thesaurus (SST) (Bollegala et al., 2011) groups together words that express similar sentiments in 614 different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do</context>
<context position="30812" citStr="Pan et al. (2010)" startWordPosition="5157" endWordPosition="5160">posed crossdomain sentiment classification method (Section 4.2) against several baselines and the current stateof-the-art methods. The baselines NA, Spred, and Tpred are defined similarly as in Section 6.1. SST is the Sentiment Sensitive Thesaurus proposed by Bollegala et al. (2011). SST creates a single distribution for a word using both source and target domain reviews, instead of two separate distributions as done by the Proposed method. SCL denotes the Structural Correspondence Learning method proposed by Blitzer et al. (2006). SFA denotes the Spectral Feature Alignment method proposed by Pan et al. (2010). SFA and SCL represent the current state-of-the-art methods for cross-domain sentiment classification. All methods are evaluated under the same settings, including train/test split, feature spaces, pivots, and classification algorithms so that any differences in performance can be directly attributable to their domain adaptability. For each domain, the accuracy obtained by a classifier trained using labeled data from that 619 E−&gt;B D−&gt;B K−&gt;B 85 80 75 Accuracy 70 65 60 55 50 90 80 Accuracy 70 60 50 B−&gt;E D−&gt;E K−&gt;E Accuracy 85 80 75 Accuracy 70 65 60 55 50 50 90 80 70 60 B−&gt;D E−&gt;D K−&gt;D B−&gt;K E−&gt;K </context>
</contexts>
<marker>Pan, Ni, Sun, Yang, Chen, 2010</marker>
<rawString>Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain sentiment classification via spectral feature alignment. In Proc. of WWW, pages 751 – 760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>938--947</pages>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proc. of EMNLP, pages 938 – 947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Notes of the 1st SANCL Workshop.</booktitle>
<contexts>
<context position="25265" citStr="Petrov and McDonald, 2012" startWordPosition="4236" endWordPosition="4240"> by Equation 4, taking the inverse-rank. In both tasks, we parallelised similarity computations using BLAS3 level-3 routines to speed up the computations. The source code of our implementation is publicly available4. 5 Datasets To evaluate DA for POS tagging, following Blitzer et al. (2006), we use sections 2 − 21 from Wall Street Journal (WSJ) as the source domain labeled data. An additional 100, 000 WSJ sentences from the 1988 release of the WSJ corpus are used as the source domain unlabeled data. Following Schnabel and Sch¨utze (2013), we use the POS labeled sentences in the SACNL dataset (Petrov and McDonald, 2012) for the five target domains: QA forums, Emails, Newsgroups, Reviews, and Blogs. Each target domain contains around 1000 POS labeled test sentences and around 100, 000 unlabeled sentences. To evaluate DA for sentiment classification, we use the Amazon product reviews collected by Blitzer et al. (2007) for four different product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). There are 1000 positive and 1000 negative sentiment labeled reviews for each domain. Moreover, each domain has on average 17, 547 unlabeled reviews. We use the standard split of 800 posit</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the 1st SANCL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalia Ponomareva</author>
<author>Mike Thelwall</author>
</authors>
<title>Do neighbours help? an exploration of graph-based algorithms for cross-domain sentiment classification.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>655--665</pages>
<contexts>
<context position="9290" citStr="Ponomareva and Thelwall (2012)" startWordPosition="1488" endWordPosition="1492">2011) groups together words that express similar sentiments in 614 different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon. Ponomareva and Thelwall (2012) represent source and target domain reviews as nodes in a graph and apply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to our task of distribution prediction across domains. The unsupervised DA setting that we consider does not assume the availability of labeled data for the target </context>
</contexts>
<marker>Ponomareva, Thelwall, 2012</marker>
<rawString>Natalia Ponomareva and Mike Thelwall. 2012. Do neighbours help? an exploration of graph-based algorithms for cross-domain sentiment classification. In Proc. of EMNLP, pages 655 – 665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Rosipal</author>
<author>Nicole Kramer</author>
</authors>
<title>Overview and recent advances in partial least squares.</title>
<date>2006</date>
<volume>05</volume>
<pages>34--51</pages>
<editor>In C. Saunders et al., editor,</editor>
<publisher>SpringerVerlag.</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="17244" citStr="Rosipal and Kramer, 2006" startWordPosition="2881" endWordPosition="2885">arranging respectively the vectors w(i) S s and w(i)T in rows. PLSR decomposes X and Y into a series of products between rank 1 matrices as follows: L X ≈ λlpl&gt; = AP&gt; (1) l=1 L Y ≈ γlql&gt; = FQ&gt;. (2) l=1 Here, λl, γl, pl, and ql are column vectors, and the summation is taken over the rank 1 matrices that result from the outer product of those vectors. The matrices, A, F, P, and Q are constructed respectively by arranging λl, γl, pl, and ql vectors as columns. Our method for learning a distribution prediction model is shown in Algorithm 1. It is based on the two block NIPALS routine (Wold, 1975; Rosipal and Kramer, 2006) and iteratively discovers L pairs of vectors (λl, γl) such that the covariances, Cov(λl, γl), are maximised under the constraint ||pl ||= ||ql ||= 1. Finally, the prediction matrix, M is computed using λl, γl, pl, ql. The predicted distribution ˆwT of a word w in T is given by ˆwT = MwS. (3) 616 Our distribution prediction learning method is unsupervised in the sense that it does not require manually labeled data for a particular task from any of the domains. This is an important point, and means that the distribution prediction method is independent of the task to which it may subsequently b</context>
</contexts>
<marker>Rosipal, Kramer, 2006</marker>
<rawString>Roman Rosipal and Nicole Kramer. 2006. Overview and recent advances in partial least squares. In C. Saunders et al., editor, SLSFS’05, volume 3940 of LNCS, pages 34 – 51, Berlin Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Introduction to Modern Information Retreival.</title>
<date>1983</date>
<publisher>McGraw-Hill Book Company.</publisher>
<contexts>
<context position="23570" citStr="Salton and Buckley, 1983" startWordPosition="3962" endWordPosition="3965">ndently for each word in the test review. This enables us to find distributional features that are consistent with all the features in a test review. 4.3 Model Choices For both POS tagging and sentiment classification, we experimented with several alternative approaches for feature weighting, representation, and similarity measures using development data, which we randomly selected from the training instances from the datasets described in Section 5. For feature weighting for sentiment classification, we considered using the number of occurrences of a feature in a review and tf-idf weighting (Salton and Buckley, 1983). For representation, we considered distributional features u(i) in descending order of their scores given by Equation 4, and then taking the inverse-rank as the values for the distributional features (Bollegala et al., 2011). However, none of these alternatives resulted in performance gains. With respect to similarity measures, we experimented with cosine similarity and the similarity measure proposed by Lin (1998); cosine similarity performed consistently well over all the experimental settings. The feature representation was held fixed during these similarity measure comparisons. For POS ta</context>
</contexts>
<marker>Salton, Buckley, 1983</marker>
<rawString>G. Salton and C. Buckley. 1983. Introduction to Modern Information Retreival. McGraw-Hill Book Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Schnabel</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Towards robust cross-domain domain adaptation for part-ofspeech tagging.</title>
<date>2013</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>198--206</pages>
<marker>Schnabel, Sch¨utze, 2013</marker>
<rawString>Tobias Schnabel and Hinrich Sch¨utze. 2013. Towards robust cross-domain domain adaptation for part-ofspeech tagging. In Proc. of IJCNLP, pages 198 – 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="6235" citStr="Socher et al., 2013" startWordPosition="990" endWordPosition="993"> domain S, we propose a method for learning its distribution wT in a target domain T . • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single semantic representation for a word using documents from a single domain. The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger t</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP, pages 1631 – 1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Aritificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="13030" citStr="Turney and Pantel, 2010" startWordPosition="2106" endWordPosition="2109">umns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr, and columns, nc, as the raw co-occurrence matrix A. Note that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the window of co-occurrence to the entire document (Baroni and Lenci, 2010). Since the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular 615 feature representation method, any of these alternative methods could be used. To reduce the dimensionality of the feature space, and create dense representations for words, we perform SVD on F. We use the left singular vectors correspondin</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Aritificial Intelligence Research, 37:141 – 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Isabelle Lajoie</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antonie Manzagol</author>
</authors>
<title>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--3371</pages>
<contexts>
<context position="6214" citStr="Vincent et al., 2010" startWordPosition="986" endWordPosition="989">f a word w in a source domain S, we propose a method for learning its distribution wT in a target domain T . • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single semantic representation for a word using documents from a single domain. The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Con</context>
</contexts>
<marker>Vincent, Larochelle, Lajoie, Bengio, Manzagol, 2010</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antonie Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371 – 3408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herman Wold</author>
</authors>
<title>Path models with latent variables: the NIPALS approach.</title>
<date>1975</date>
<booktitle>Quantitative socialogy: international perspective on mathematical and statistical modeling,</booktitle>
<pages>307--357</pages>
<editor>In H. M. Blalock et al., editor,</editor>
<publisher>Academic.</publisher>
<contexts>
<context position="17217" citStr="Wold, 1975" startWordPosition="2879" endWordPosition="2880">s formed by arranging respectively the vectors w(i) S s and w(i)T in rows. PLSR decomposes X and Y into a series of products between rank 1 matrices as follows: L X ≈ λlpl&gt; = AP&gt; (1) l=1 L Y ≈ γlql&gt; = FQ&gt;. (2) l=1 Here, λl, γl, pl, and ql are column vectors, and the summation is taken over the rank 1 matrices that result from the outer product of those vectors. The matrices, A, F, P, and Q are constructed respectively by arranging λl, γl, pl, and ql vectors as columns. Our method for learning a distribution prediction model is shown in Algorithm 1. It is based on the two block NIPALS routine (Wold, 1975; Rosipal and Kramer, 2006) and iteratively discovers L pairs of vectors (λl, γl) such that the covariances, Cov(λl, γl), are maximised under the constraint ||pl ||= ||ql ||= 1. Finally, the prediction matrix, M is computed using λl, γl, pl, ql. The predicted distribution ˆwT of a word w in T is given by ˆwT = MwS. (3) 616 Our distribution prediction learning method is unsupervised in the sense that it does not require manually labeled data for a particular task from any of the domains. This is an important point, and means that the distribution prediction method is independent of the task to </context>
</contexts>
<marker>Wold, 1975</marker>
<rawString>Herman Wold. 1975. Path models with latent variables: the NIPALS approach. In H. M. Blalock et al., editor, Quantitative socialogy: international perspective on mathematical and statistical modeling, pages 307 – 357. Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herman Wold</author>
</authors>
<title>Partial least squares.</title>
<date>1985</date>
<booktitle>In Samel Kotz and</booktitle>
<pages>581--591</pages>
<editor>Norman L. Johnson, editors,</editor>
<publisher>Wiley.</publisher>
<contexts>
<context position="15569" citStr="Wold, 1985" startWordPosition="2559" endWordPosition="2560">cur in both domains. For each word w(i) ∈ W, we denote the corresponding rows in ˆFS and ˆFT by column vectors w(i) S and w(i)T . Note that the dimensionality of w(i) S and w(i)T need not be equal, and we may select different numbers of singular vectors to approximate ˆFS and ˆFT . We model distribution prediction as a multivariate regression problem where, given a set {(w(i) S , w(i)T )}ni=1 consisting of pairs of feature vectors selected from each domain for the pivots in W, we learn a mapping from the inputs (w(i) S ) to the outputs (w(i)T ). We use Partial Least Squares Regression (PLSR) (Wold, 1985) to learn a regression model using pairs of vectors. PLSR has been applied in 2http://tedlab.mit.edu/˜dr/SVDLIBC/ Algorithm 1 Learning a prediction model. Input: X, Y, L. Output: Prediction matrix M. 1: Randomly select γl from columns in Yl. ���� 2: vl = XlTγl/ ����XlTγl 3: λl = Xlvl ���� 4: ql = YlTλl/ ����YlTλl 5: γl = Ylql 6: If γl is unchanged go to Line 7; otherwise go to Line 2 ���� 7: cl = λlTγl/ ����λlTγl 8: pl = XlTλl/λlTλl 9: Xl+1 = Xl − λlplT and Yl+1 = Yl − clλlqlT. 10: Stop if l = L; otherwise l = l + 1 and return to Line 1. 11: Let C = diag(c1, ... , cL), and V = [v1 ... vL] 12: </context>
</contexts>
<marker>Wold, 1985</marker>
<rawString>Herman Wold. 1985. Partial least squares. In Samel Kotz and Norman L. Johnson, editors, Encyclopedia of the Statistical Sciences, pages 581 – 591. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Xiao</author>
<author>Feipeng Zhao</author>
<author>Yuhong Guo</author>
</authors>
<title>Learning latent word representations for domain adaptation using supervised word clustering.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>152--162</pages>
<contexts>
<context position="10057" citStr="Xiao et al., 2013" startWordPosition="1616" endWordPosition="1619">ain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to our task of distribution prediction across domains. The unsupervised DA setting that we consider does not assume the availability of labeled data for the target domain. However, if a small amount of labeled data is available for the target domain, it can be used to further improve the performance of DA tasks (Xiao et al., 2013; Daum´e III, 2007). 3 Distribution Prediction 3.1 In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows. Given a document H, such as a user-review of a product, we split H into sentences, and lemmatize each word in a sentence using the RASP system (Briscoe et al., 2006). Using a standard stop word list, we filter out frequent </context>
</contexts>
<marker>Xiao, Zhao, Guo, 2013</marker>
<rawString>Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013. Learning latent word representations for domain adaptation using supervised word clustering. In Proc. of EMNLP, pages 152 – 162.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>