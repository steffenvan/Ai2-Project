<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016611">
<title confidence="0.9944955">
A Comparison of Head Transducers and Transfer for a Limited
Domain Translation Application
</title>
<author confidence="0.618495">
Hiyan Alshawi and Adam L. Buchsbaum
</author>
<affiliation confidence="0.375161">
AT&amp;T Labs
</affiliation>
<address confidence="0.4733265">
180 Park Avenue
Florham Park. NJ 07932-0971. USA
</address>
<email confidence="0.740517">
{hiyan.alb}gresearch.att.com
</email>
<author confidence="0.99076">
Fei Xia
</author>
<affiliation confidence="0.89077225">
Department of Computer and
Information Science
University of Pennsylvania
Philadelphia, PA 19104. USA
</affiliation>
<email confidence="0.836118">
fxiaAcis.upenn.edu
</email>
<sectionHeader confidence="0.99209" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995369">
We compare the effectiveness of two related
</bodyText>
<listItem confidence="0.861788">
• machine translation models applied to the
</listItem>
<bodyText confidence="0.914337444444444">
same limited-domain task. One is a trans-
fer model with monolingual head automata
for analysis and generation; the ocher is a
direct transduction model based on bilin-
gual head transducers. We conclude that
the head transducer model is more effective
according to measures of accuracy, compu-
tational requirements, model size, and de-
velopment effort.
</bodyText>
<sectionHeader confidence="0.99726" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999830736842106">
In this paper we describe an experimental ma-
chine translation system based on head transducer
models and compare it to a related transfer sys-
tem, described in Alshawi 1996a, based on mono-
lingual head automata. Head transducer models
consist of collections of finite state machines that
are associated with pairs of lexical items in a bilin-
gual lexicon. The transfer system follows the fa-
miliar analysis-transfer-generation architecture (Is-
abelle and Macklovitch 1986). with mapping of
dependency representations (Hudson 1984) in the
transfer phase. In contrast, the head transducer
approach is more closely aligned with earlier di-
rect translation methods: no explicit representa-
tions of the source language (interlingua or other-
wise) are created in the process of deriving the target
string. Despite the simple direct architecture, the
head transducer model does embody modern prin-
ciples of lexicalized recursive grammars and statis-
tical language processing. The context for evaluat-
ing both the transducer and transfer models was the
development of experimental prototypes for speech-
to-speech translation.
In the case of text translation for publishing, it
is reasonable to adopt economic measures of the
effectiveness of translation systems. This involves
assessing the total cost Df employing a translation
system, including, for example. the cost of manual
post-editing. Post-editing is not an option in speech
translation systems for person-to-person communi-
cation, and real-time operation is important in this
context, so in comparing the two translation models
we looked at a variety of other measures, including
translation accuracy. speed, and system complexity.
Both models underlying the translation systems
can be characterized as statistical translation mod-
els, but unlike the models proposed by Brown et
al. (1990, 1993), these models have non-uniform lin-
guistically motivated structure, at present coded by
hand. In fact, the original motivation for the head
transducer models was that they are simpler and
more amenable to automatic model structure acqui-
sition, while the transfer component of the tradi-
tional system was designed with regard to allowing
maximum flexibility in mapping between source and
target representations to overcome translation diver-
gences (Lindop and Tsujii 1991: Dorr 1994). In prac-
tice, it turned out that adopting the simpler trans-
ducer models did not involve sacrificing accuracy. at
least for our limited domain application.
We first describe the transfer and head transducer
approaches in Sections 2 and :3 and the method used
to assign the numerical parameters of the models in
Section 4. In Section 5. we compare experimental
systems, based on the two approaches. for English-
to-Chinese translation of air travel enquiries, and we
conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.963556" genericHeader="method">
2 Monolingual Automata and
Transfer
</sectionHeader>
<bodyText confidence="0.999501">
In this section we review the approach based on
monolingual head automata together with transfer
mapping. Further details of this approach, includ-
ing the analysis, transfer, and generation algorithms
appear in Alshawi 1996a.
</bodyText>
<page confidence="0.994713">
360
</page>
<subsectionHeader confidence="0.98372">
2.1 Monolingual Relational Models
</subsectionHeader>
<bodyText confidence="0.999921710526316">
We can characterize the language models used for
analysis and generation in the transfer system as
quantitative generative models of ordered depen-
dency trees. In the dependency trees generated by
these models, each node is labeled with a word w
from the vocabulary V of the language in question:
the nodes (and their word labels) immediately dom-
inated by such a node are the dependents of w in
the dependency derivation. Dependency tree arcs
are labeled with symbols taken from a set R of de-
pendency re;utiorts. These monolingual models are
reversible, in the sense they can be used for analy-
sis or generation. The motivation for these models is
similar to that for Probabilistic Link Grammar (Laf-
ferty, Sleator, and Temperley 1992). one difference
being that the head automata derivations are always
trees.
The models are quantitative in that they assign a
real-number cost to derivations. Various cost func-
tions are possible, though in the experiments re-
ported in this paper, a discriminative cost function
is used, as discussed in Section 4. In the monolin-
gual models, derivation events are actions performed
by relational head acceptors, a particular type of fi-
nite state automata associated with each word in the
language.
A relational head acceptor writes (or accepts) a
pair of symbol sequences, a left sequence and a right
sequence. The symbols in these sequences are taken
from the set R of dependency relations. In a de-
pendency derivation, an acceptor is associated with
a node with word w, and the sequences written by
the acceptor correspond to the relation labels of the
arcs to the left and right of the node. In other words,
they are the dependency relations between w and the
dependents of w to its left and right. The possible
actions taken by a relational head acceptor in. in
state qi are:
</bodyText>
<listItem confidence="0.986365176470588">
• Left transition: write a symbol r onto the right
end of the left sequence and enter state (hi.&apos; .
• Right transition: write a symbol r onto the left
end of the right sequence and enter state qi4.1.
• Stop: stop in state q, at which point the se-
quences are considered complete.
Derivation of ordered dependency trees proceeds
recursively by generating the dependent relations for
a node according to the word and acceptor at that
node, and then generating the trees dominated by
these relation edges. This process involves the fol-
lowing actions in addition to the acceptor actions
above:
• Selection of a word and acceptor to start an
entire derivation.
• Selection of a dependent word and acceptor
given a head word and a dependency relation.
</listItem>
<subsectionHeader confidence="0.849526">
2.2 Transfer
</subsectionHeader>
<bodyText confidence="0.9997459">
Transfer in this model is a mapping between un-
ordered dependency trees. Surface ordering of de-
pendent phrases of either the source or target is not
taken into account in the transfer mapping. This or-
dering is completely defined by the source and target
monolingual models.
Our transfer model involves a bilingual lexicon
specifying paired source-target fragments of depen-
dency trees. A bilingual lexical entry (see Alshawi
1996a for more details) includes a mapping function
between the source and target nodes of the frag-
ments. Valid transfer mappings are defined in terms
of a tiling of the source dependency tree with source
fragments from bilingual lexicon entries so that the
partial mappings defined in entries are extended to
a mapping for the entire source tree. This tiling pro-
cess has the side effect of creating an unordered tar-
get dependency representation. The following non-
deterministic actions are involved in the tiling pro-
cess:
</bodyText>
<listItem confidence="0.9900402">
• Selection of a bilingual entry given a source lan-
guage word, w.
• Matching the nodes and arcs of the source frag-
ment of an entry against a local subgraph in-
cluding a node labeled by w.
</listItem>
<sectionHeader confidence="0.943127" genericHeader="method">
3 Bilingual Head Transduction
</sectionHeader>
<subsectionHeader confidence="0.995983">
3.1 Bilingual Head Transducers
</subsectionHeader>
<bodyText confidence="0.999989777777778">
A head transducer is a transduction version of the
finite state head acceptors employed in the transfer
model. Such a transducer .11 is associated with a
pair of words, a source word w and a target word
v. In fact. w is taken from the set VI consisting of
the source language vocabulary augmented by the
-empty word&amp;quot; e, and u is taken from Vi , the tar-
get language vocabulary augmented with c. A head
transducer reads from a pair of source sequences, a
left source sequence L1 and a right source sequence
RI; it writes to a pair of target sequences, a left
target sequence L2 and a right target sequence R2
(Figure 1).
Head transducers were introduced in Alshawi
1996b, where the symbols in the source and target
sequences are source and target words respectively.
In the experiment described in this paper the sym-
bols written are dependency relation symbols or the
</bodyText>
<page confidence="0.997211">
361
</page>
<figureCaption confidence="0.999534">
Figure 1: Head transducer M converts the sequences
</figureCaption>
<bodyText confidence="0.988063357142857">
of left and right relations H. ... r) and (r44 r731)
of w into left and right relations (r? ri) and
(r2 r2) of v.
3+1 • • p
empty symbol e. While it is possible to construct a
translator based on head transduction models with-
out relation symbols, using a version of head trans-
ducers with relation symbols allowed for a more di-
rect comparison between the transfer and transducer
systems, as discussed in Section 5
We can think of the transducer as simultaneously
deriving the source and target sequences through a
series of transitions followed by a stop action. From
a state qi these actions are as follows:
</bodyText>
<listItem confidence="0.987114">
• Left transition: write a symbol r1 onto the right
end of LI, write symbol r, to position a in the
target sequences, and enter state
• Right transition: write a symbol r1 onto the left
end of R1, write a symbol r2 to position a in
the target sequences, and enter state qi+1.
• Stop: stop in state qi, at which point the se-
quences L1, RI, L2 and R2 are considered com-
plete.
</listItem>
<bodyText confidence="0.99994975">
In simple head transducers, the target positions
a can be restricted in a similar way to the source
positions, i.e., the right end of L2 or the left end of
R2. The version used in the experiment allows ad-
ditional positions, including the left end of L, and
the right end R2. Allowing additional target posi-
tions increases the flexibility of transducers in the
translation application without an adverse effect on
computational complexity. On the other hand, we
restrict the source side positions as indicated above
to keep the transduction search similar in nature to
head-outward context free parsing.
</bodyText>
<subsectionHeader confidence="0.997358">
3.2 Recursive Head Transduction
</subsectionHeader>
<bodyText confidence="0.999804285714286">
We can apply a set of head transducers recursively
to derive a pair of source-target ordered dependency
trees. This is a recursive process in which the depen-
dency relations for corresponding nodes in the two
trees are derived by a head transducer. In addition
to the actions performed by the head transducers.
this derivation process involves the actions:
</bodyText>
<listItem confidence="0.976283571428571">
• Selection of a pair of words wo E V1 and vo E V2,
and a head transducer Mo to start the entire
derivation.
• Selection of a pair of dependent words w&apos; and
v&apos; and transducer M&apos; given head words w and v
and source and target dependency relations r1
and r2. (w, w&apos; E VI; v&apos; E 172.)
</listItem>
<bodyText confidence="0.99956225">
The recursion takes place by running a head trans-
ducer (M&apos; in the second action above) to derive local
dependency trees for corresponding pairs of depen-
dent words (w&apos;, v&amp;quot;).
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="method">
4 Event Cost Assignment
</sectionHeader>
<bodyText confidence="0.999993090909091">
The transfer and head transduction derivation mod-
els can be formulated as probabilistic generative
models; such formulations were given in Alshawi
1996a and 1996b respectively. Under such a for-
mulation, negated log probabilities can be used as
the costs for the actions listed in Sections 2 and 3.
However, experimentation reported in Alshawi and
Buchsbaum 1997 suggests that improved translation
accuracy can be achieved by adopting cost functions
other than log probability. This is true in particular
for a family of discriminative cost functions.
We define a cost function f as a real valued func-
tion taking two arguments, a event e and a context
c. The context c is an equivalence class of states un-
der which an action is taken, and the event e is an
equivalence class of actions possible from that set of
states. We write the value of the function as f(eic),
borrowing notation from the special case of condi-
tional probabilities. The pair (eic) is referred to as a
choice. The cost of a solution (i.e., a possible trans-
lation of an input string) is the sum of costs for all
choices in the derivation of that solution.
Discriminative cost functions, including likelihood
ratios (cf. Dunning 1993), make use of both positive
and negative instances of performing a task. Here
we take a positive instance to be the derivation of
a &amp;quot;correct&amp;quot; translation, and a negative instance the
derivation of an &amp;quot;incorrect&amp;quot; translation, where cor-
rectness is judged by a speaker of both languages.
Let n+ (e lc) be the count of taking choice (elc) in pos-
itive instances resulting from processing the source
sentences in a training corpus. Similarly, let n- (elc)
be the count of taking (elc) for negative instances.
</bodyText>
<equation confidence="0.8469966">
L,
r I
Ri
rni
L2
</equation>
<page confidence="0.990644">
362
</page>
<bodyText confidence="0.9610715">
The cost function used in the experiments is com-
puted as:
</bodyText>
<equation confidence="0.994168">
f(elc) = log(n+(eic) ± n(elc)) log(n+ (eic)).
</equation>
<bodyText confidence="0.999689">
(By comparison, the usual &amp;quot;logprob&amp;quot; cost function
using only positive instances would be log(n+(c)) —
log(n+(elc)).) For unseen choices, we replace the
context c and event e with larger equivalence classes.
</bodyText>
<sectionHeader confidence="0.996906" genericHeader="method">
5 Effectiveness Comparison
</sectionHeader>
<subsectionHeader confidence="0.989339">
5.1 English-Chinese ATIS Models
</subsectionHeader>
<bodyText confidence="0.999790119047619">
Both the transfer and transducer systems were
trained and evaluated on English-to-Mandarin Chi-
nese translation of transcribed utterances from the
ATIS corpus (Hirschman et al. 1993). By train-
ing here we simply mean assignment of the cost
functions for fixed model structures. These model
structures were coded by hand as monolingual head
acceptor and bilingual dependency lexicons for the
transfer system and a head transducer lexicon for
the transducer system.
Positive and negative counts for cost assignment
were collected from two sources for both systems and
an additional third source for the transfer system.
The first set of counts was derived by processing
traces using around 1200 sample utterances from
the ATIS corpus. This involved running the sys-
tems on the sample utterances, starting initially with
uniform costs, and presenting the resulting trans-
lations to a human judge for classification as cor-
rect or incorrect. The second source of counts was
hand-tagging around 800 utterance transcriptions
to identify correct and incorrect attachment points
for prepositional phrases, PP-attachment being im-
portant for English-Chinese translation (Chen and
Chen 1992). This attachment information was con-
verted to corresponding counts for head-dependent
choices involving prepositional phrase attachment.
The additional source of counts used in the trans-
fer system was an unsupervised training method
in which 13000 training utterances were translated
from English to Chinese, and then back again; the
derivations were classified as positive (otherwise neg-
ative) if the resulting back-translation was suffi-
ciently close to the original English, as described in
Alshawi and Buchsbaum 1997.
There was a strong systematic relationship be-
tween the structure of the models used in the two
systems in the following sense. The head transducers
were built by modifying the English head acceptors
defined for the transfer system. This involved the
addition of target relations, including some epsilon
relations, to automaton transitions. In some cases,
</bodyText>
<table confidence="0.999400428571429">
Transfer Head Transducer
Word error rate 16.2 11.7
(per cent)
Time 1.09 0.17
(seconds/sent.)
Space 1.67 0.14
(Mbytes/sent.)
</table>
<tableCaption confidence="0.999955">
Table 1: Accuracy, time, and space comparison
</tableCaption>
<bodyText confidence="0.999604">
the automata needed to be modified to include addi-
tional states, and also some transitions with epsilon
relations on the English (source) side. Typically,
such cases arise when an additional particle needs
to be generated on the target side, for example the
yes-no question particle in Chinese. The inclusion of
such particles often depended on additional distinc-
tions not present in the original English automata.
hence the requirement for additional states in the
bilingual transducer versions.
</bodyText>
<subsectionHeader confidence="0.991319">
5.2 Performance
</subsectionHeader>
<bodyText confidence="0.999950483870968">
To evaluate the relative performance of the two
translators, 200 utterances were chosen at random
from a previously unseen test sample of ATIS utter-
ances having no overlap with samples used in model
building and cost assignment. There was no restric-
tion on utterance length or ATIS &amp;quot;class&amp;quot; (dialogue or
one-off queries, etc.) in making this selection. These
English test utterances were processed by both sys-
tems, yielding lowest cost Chinese translations.
Three measures of performance—accuracy, com-
putation time, and memory usage—were compared,
with the results in Table 1, showing improvements
by the transducer system for all three measures. The
accuracy figures are given in terms of translation
word error rate, a measure we believe to be some-
what less subjective than sentence level measures of
grammaticality and meaning preservation. Trans-
lation word error rate is defined as the number of
words in the source which are judged to have been
mistranslated. For the purposes of this definition,
mistranslation of a source word includes choice of
the wrong target word (or words), the absence (or
incorrect addition) of a particle related to the word,
and the generation of a correct target word in the
wrong position.
The improvement in word error rates of the trans-
ducer system was achieved without the benefit of the
additional counts from unsupervised training, men-
tioned above, with 13,000 utterances. Earlier experi-
ments (Alshawi and Buschbaum 1997) show that the
unsupervised training does lead to an improvement
</bodyText>
<page confidence="0.998223">
363
</page>
<bodyText confidence="0.999994176470588">
in the performance of the transfer system. How-
ever, this improvement is relatively small: around
2% reduction in the number of utterances contain-
ing translation errors. (Word error rates for direct
comparison with the results above are not available.)
We also know that some additional improvement of
the transducer system can be achieved by increasing
the amount of training data: with a further 600 su-
pervised training samples (for a total of 1800), the
error rate for the transducer system falls to 11.0%.
The processing times reported above are averages
over the same 200 test utterances used in the accu-
racy evaluation. These timings are for an implemen-
tation of the search algorithms in Lisp on a Silicon
Graphics machine with a 150MHz R4400 processor.
The space figures give the average amount of mem-
ory allocated in processing each utterance.
</bodyText>
<subsectionHeader confidence="0.999872">
5.3 Model Size and Development Effort
</subsectionHeader>
<bodyText confidence="0.999951114285714">
The performance comparison above is, of course, not
the whole story, particularly since manual effort was
required to build the model structures before train-
ing for cost assignment. However, we believe the
conclusion for the improvement in performance of
the transducer system is valid because the amount
of effort in building and training the transfer models
exceeded that for the the transducer systems. After
construction of the English head acceptor models,
common to both systems, a rough estimate of the
effort required for completing the models for English
to Chinese translation is 12 person-months for the
transfer system and 3 person-months for the trans-
ducer system. With respect to training effort, as
noted, the amount of supervised training effort in
the main experiment was the same for both systems
(supervised discriminative training for 1200 utter-
ances plus tagging of prepositional attachments for
800 utterances), while the transfer system also ben-
efited from unsupervised training with 13000 utter-
ances.
In comparing models for language processing, or
indeed other tasks, it is reasonable to ask if per-
formance improvements by one model over another
were achieved through an increase in model complex-
ity. We looked at three measures of model complex-
ity for the two systems, with the results shown in
Table 2. The first was the number of lexical entries.
For the transfer model this includes both monolin-
gual entries and the bilingual entries required for the
English to Chinese direction; there are only bilin-
gual entries in the transducer model. Comparing the
structural complexity of the two models is somewhat
more difficult but we can make a graph-theoretic ab-
straction and count the number of edges in model
</bodyText>
<table confidence="0.9522155">
Transfer Head Transducer
Lexical entries 3,250 1,201
Edges 72,180 47,910
Choices 100,472 67,011
</table>
<tableCaption confidence="0.999068">
Table 2: Lexicon and model size comparison
</tableCaption>
<bodyText confidence="0.9990945">
components. Both systems include edges for au-
tomaton state transitions. The edge count for the
transfer system includes the number of dependency
graph edges in bilingual entries. Finally, we also
looked at the number of choices for which train-
ing counts were available, i.e., the number of model
numerical parameters for which direct evidence was
present in training data. As can be seen from Ta-
ble 2, the transducer system has a lower model com-
plexity according to all three measures.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999985583333333">
There are many aspects to the effectiveness of the
translation component of a speech translator, mak-
ing comparisons between systems difficult. There is
also an inherent difficulty in evaluating the transla-
tion task: a single source utterance has many valid
translations and the validity of translations is a mat-
ter of degree. Despite this, we believe that in the
comparison considered in this paper, it is reason-
able to make an overall assessment that the head
transducer system is more effective that the transfer-
based system. One justification for this conclusion
is that the systems were closely related, having iden-
tical sublanguage domain and test data, and using
similar automata for analysis in the transfer system
and transduction in the transducer system. Another
justification is that it was not necessary to make
difficult comparisons between different aspects of ef-
fectiveness: the transducer system performed better
with respect to all the measures we looked at for
accuracy, speed, memory, development effort and
model complexity. Looking forward, the relative
simplicity of head transducer models makes them
more promising for further automating the develop-
ment of translation applications.
</bodyText>
<sectionHeader confidence="0.963799" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99998425">
We are grateful to Jishen He for building the Chinese
model and bilingual lexicon of the earlier transfer
system that we used in this work for comparison
with the head transducer system.
</bodyText>
<page confidence="0.998153">
364
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916705882353">
Alshawi, H. and A.L. Buchsbaum. 1997. &amp;quot;State-
Transition Cost Functions and an Application to
Language Translation&amp;quot;. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and
Signal Processing, IEEE, Munich, Germany.
Alshawi, H. 1996a. &amp;quot;Head Automata and Bilin-
gual Tiling: Translation with Minimal Represen-
tations&amp;quot;. In Proceedings of the nth Annual Meet-
ing of the Association for Computational Linguis-
tics, Santa Cruz, California, 167-176.
Alshawi, H. 1996b. &amp;quot;Head Automata for Speech
Translation&amp;quot;. In Proceedings of the Interna-
tional Conference on Spoken Language Processing,
Philadelphia, Pennsylvania.
Brown, P., J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J. Lafferty, R. Mercer and P. Rossin.
1990. &amp;quot;A Statistical Approach to Machine Trans-
lation&amp;quot;. Computational Linguistics 16:79-85.
Brown, P.F., S.A. Della Pietra, V.J. Della Pietra,
and R.L. Mercer. 1993. &amp;quot;The Mathematics of
Statistical Machine Translation: Parameter Esti-
mation&amp;quot;. Computational Linguistics 19:263-312.
Chen, K.H. and H. H. Chen. 1992. &amp;quot;Attachment and
Transfer of Prepositional Phrases with Constraint
Propagation&amp;quot;. Computer Processing of Chinese
and Oriental Languages, Vol. 6, No, 2, 123-142.
Dorr, B.J. 1994. &amp;quot;Machine Translation Divergences:
A Formal Description and Proposed Solution&amp;quot;.
Computational Linguistics 20:597-634.
Dunning, T. 1993. &amp;quot;Accurate Methods for Statis-
tics of Surprise and Coincidence.&amp;quot; Computational
Linguistics 19:61-74.
Hudson, R.A. 1984. Word Grammar. Blackwell,
Oxford.
Hirschman, L., M. Bates, D. Dahl, W. Fisher, J.
Garofolo, D. Pallett, K. Hunicke-Smith, P. Price,
A. Rudnicky, and E. Tzoukermann. 1993. &amp;quot;Multi-
Site Data Collection and Evaluation in Spoken
Language Understanding&amp;quot;. In Proceedings of the
Human Language Technology Workshop, Morgan
Kaufmann, San Francisco, 19-24.
Isabelle, P. and E. Macklovitch. 1986. &amp;quot;Transfer
and MT Modularity&amp;quot;, In Eleventh International
Conference on Computational Linguistics, Bonn,
Germany, 115-117.
Jelinek, F., R.L. Mercer and S. Roukos. 1992.
&amp;quot;Principles of Lexical Language Modeling for
Speech Recognition&amp;quot;. In S. Furui and M.M.
Sondhi (eds.), Advances in Speech Signal Process-
ing, Marcel Dekker, New York.
Lafferty, J., D. Sleator and D. Temperley. 1992.
&amp;quot;Grammatical Trigrams: A Probabilistic Model of
Link Grammar&amp;quot;. In Proceedings of the 1992 AAAI
Fall Symposium on Probabilistic Approaches to
Natural Language, 89-97.
Kay, M. 1989. &amp;quot;Head Driven Parsing&amp;quot;. In Pro-
ceedings of the Workshop on Parsing Technolo-
gies, Pittsburgh, 1989.
Lindop, J. and J. Tsujii. 1991. &amp;quot;Complex Transfer
in MT: A Survey of Examples&amp;quot;. Technical Re-
port 91/5, Centre for Computational Linguistics,
UMIST, Manchester, UK.
Sata, G. and 0. Stock. 1989. &amp;quot;Head-Driven Bidirec-
tional Parsing&amp;quot;. In Proceedings of the Workshop
on Parsing Technologies, Pittsburgh.
Younger, D. 1967. Recognition and Parsing of
Context-Free Languages in Time n3. Information
and Control, 10, 189-208.
</reference>
<page confidence="0.999097">
365
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.821091">
<title confidence="0.999608">A Comparison of Head Transducers and Transfer for a Limited Domain Translation Application</title>
<author confidence="0.999952">Alshawi L Buchsbaum</author>
<affiliation confidence="0.998959">AT&amp;T Labs</affiliation>
<address confidence="0.99881">180 Park Avenue Florham Park. NJ 07932-0971. USA</address>
<email confidence="0.99963">{hiyan.alb}gresearch.att.com</email>
<author confidence="0.971024">Xia</author>
<affiliation confidence="0.999901333333333">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.991595">Philadelphia, PA 19104. USA</address>
<email confidence="0.999921">fxiaAcis.upenn.edu</email>
<abstract confidence="0.987151166666667">We compare the effectiveness of two related • machine translation models applied to the same limited-domain task. One is a transfer model with monolingual head automata for analysis and generation; the ocher is a direct transduction model based on bilingual head transducers. We conclude that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>A L Buchsbaum</author>
</authors>
<title>StateTransition Cost Functions and an Application to Language Translation&amp;quot;.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<publisher>IEEE,</publisher>
<location>Munich, Germany.</location>
<contexts>
<context position="11396" citStr="Alshawi and Buchsbaum 1997" startWordPosition="1879" endWordPosition="1882"> and target dependency relations r1 and r2. (w, w&apos; E VI; v&apos; E 172.) The recursion takes place by running a head transducer (M&apos; in the second action above) to derive local dependency trees for corresponding pairs of dependent words (w&apos;, v&amp;quot;). 4 Event Cost Assignment The transfer and head transduction derivation models can be formulated as probabilistic generative models; such formulations were given in Alshawi 1996a and 1996b respectively. Under such a formulation, negated log probabilities can be used as the costs for the actions listed in Sections 2 and 3. However, experimentation reported in Alshawi and Buchsbaum 1997 suggests that improved translation accuracy can be achieved by adopting cost functions other than log probability. This is true in particular for a family of discriminative cost functions. We define a cost function f as a real valued function taking two arguments, a event e and a context c. The context c is an equivalence class of states under which an action is taken, and the event e is an equivalence class of actions possible from that set of states. We write the value of the function as f(eic), borrowing notation from the special case of conditional probabilities. The pair (eic) is referre</context>
<context position="14825" citStr="Alshawi and Buchsbaum 1997" startWordPosition="2426" endWordPosition="2429"> prepositional phrases, PP-attachment being important for English-Chinese translation (Chen and Chen 1992). This attachment information was converted to corresponding counts for head-dependent choices involving prepositional phrase attachment. The additional source of counts used in the transfer system was an unsupervised training method in which 13000 training utterances were translated from English to Chinese, and then back again; the derivations were classified as positive (otherwise negative) if the resulting back-translation was sufficiently close to the original English, as described in Alshawi and Buchsbaum 1997. There was a strong systematic relationship between the structure of the models used in the two systems in the following sense. The head transducers were built by modifying the English head acceptors defined for the transfer system. This involved the addition of target relations, including some epsilon relations, to automaton transitions. In some cases, Transfer Head Transducer Word error rate 16.2 11.7 (per cent) Time 1.09 0.17 (seconds/sent.) Space 1.67 0.14 (Mbytes/sent.) Table 1: Accuracy, time, and space comparison the automata needed to be modified to include additional states, and also</context>
</contexts>
<marker>Alshawi, Buchsbaum, 1997</marker>
<rawString>Alshawi, H. and A.L. Buchsbaum. 1997. &amp;quot;StateTransition Cost Functions and an Application to Language Translation&amp;quot;. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, IEEE, Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head Automata and Bilingual Tiling: Translation with Minimal Representations&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proceedings of the nth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>167--176</pages>
<location>Santa Cruz, California,</location>
<contexts>
<context position="975" citStr="Alshawi 1996" startWordPosition="144" endWordPosition="145"> the effectiveness of two related • machine translation models applied to the same limited-domain task. One is a transfer model with monolingual head automata for analysis and generation; the ocher is a direct transduction model based on bilingual head transducers. We conclude that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort. 1 Introduction In this paper we describe an experimental machine translation system based on head transducer models and compare it to a related transfer system, described in Alshawi 1996a, based on monolingual head automata. Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon. The transfer system follows the familiar analysis-transfer-generation architecture (Isabelle and Macklovitch 1986). with mapping of dependency representations (Hudson 1984) in the transfer phase. In contrast, the head transducer approach is more closely aligned with earlier direct translation methods: no explicit representations of the source language (interlingua or otherwise) are created in the process of derivin</context>
<context position="3862" citStr="Alshawi 1996" startWordPosition="578" endWordPosition="579">ed domain application. We first describe the transfer and head transducer approaches in Sections 2 and :3 and the method used to assign the numerical parameters of the models in Section 4. In Section 5. we compare experimental systems, based on the two approaches. for Englishto-Chinese translation of air travel enquiries, and we conclude in Section 6. 2 Monolingual Automata and Transfer In this section we review the approach based on monolingual head automata together with transfer mapping. Further details of this approach, including the analysis, transfer, and generation algorithms appear in Alshawi 1996a. 360 2.1 Monolingual Relational Models We can characterize the language models used for analysis and generation in the transfer system as quantitative generative models of ordered dependency trees. In the dependency trees generated by these models, each node is labeled with a word w from the vocabulary V of the language in question: the nodes (and their word labels) immediately dominated by such a node are the dependents of w in the dependency derivation. Dependency tree arcs are labeled with symbols taken from a set R of dependency re;utiorts. These monolingual models are reversible, in the</context>
<context position="6897" citStr="Alshawi 1996" startWordPosition="1090" endWordPosition="1091"> actions above: • Selection of a word and acceptor to start an entire derivation. • Selection of a dependent word and acceptor given a head word and a dependency relation. 2.2 Transfer Transfer in this model is a mapping between unordered dependency trees. Surface ordering of dependent phrases of either the source or target is not taken into account in the transfer mapping. This ordering is completely defined by the source and target monolingual models. Our transfer model involves a bilingual lexicon specifying paired source-target fragments of dependency trees. A bilingual lexical entry (see Alshawi 1996a for more details) includes a mapping function between the source and target nodes of the fragments. Valid transfer mappings are defined in terms of a tiling of the source dependency tree with source fragments from bilingual lexicon entries so that the partial mappings defined in entries are extended to a mapping for the entire source tree. This tiling process has the side effect of creating an unordered target dependency representation. The following nondeterministic actions are involved in the tiling process: • Selection of a bilingual entry given a source language word, w. • Matching the n</context>
<context position="8330" citStr="Alshawi 1996" startWordPosition="1344" endWordPosition="1345">te state head acceptors employed in the transfer model. Such a transducer .11 is associated with a pair of words, a source word w and a target word v. In fact. w is taken from the set VI consisting of the source language vocabulary augmented by the -empty word&amp;quot; e, and u is taken from Vi , the target language vocabulary augmented with c. A head transducer reads from a pair of source sequences, a left source sequence L1 and a right source sequence RI; it writes to a pair of target sequences, a left target sequence L2 and a right target sequence R2 (Figure 1). Head transducers were introduced in Alshawi 1996b, where the symbols in the source and target sequences are source and target words respectively. In the experiment described in this paper the symbols written are dependency relation symbols or the 361 Figure 1: Head transducer M converts the sequences of left and right relations H. ... r) and (r44 r731) of w into left and right relations (r? ri) and (r2 r2) of v. 3+1 • • p empty symbol e. While it is possible to construct a translator based on head transduction models without relation symbols, using a version of head transducers with relation symbols allowed for a more direct comparison betw</context>
<context position="11186" citStr="Alshawi 1996" startWordPosition="1847" endWordPosition="1848">pair of words wo E V1 and vo E V2, and a head transducer Mo to start the entire derivation. • Selection of a pair of dependent words w&apos; and v&apos; and transducer M&apos; given head words w and v and source and target dependency relations r1 and r2. (w, w&apos; E VI; v&apos; E 172.) The recursion takes place by running a head transducer (M&apos; in the second action above) to derive local dependency trees for corresponding pairs of dependent words (w&apos;, v&amp;quot;). 4 Event Cost Assignment The transfer and head transduction derivation models can be formulated as probabilistic generative models; such formulations were given in Alshawi 1996a and 1996b respectively. Under such a formulation, negated log probabilities can be used as the costs for the actions listed in Sections 2 and 3. However, experimentation reported in Alshawi and Buchsbaum 1997 suggests that improved translation accuracy can be achieved by adopting cost functions other than log probability. This is true in particular for a family of discriminative cost functions. We define a cost function f as a real valued function taking two arguments, a event e and a context c. The context c is an equivalence class of states under which an action is taken, and the event e i</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>Alshawi, H. 1996a. &amp;quot;Head Automata and Bilingual Tiling: Translation with Minimal Representations&amp;quot;. In Proceedings of the nth Annual Meeting of the Association for Computational Linguistics, Santa Cruz, California, 167-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head Automata for Speech Translation&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="975" citStr="Alshawi 1996" startWordPosition="144" endWordPosition="145"> the effectiveness of two related • machine translation models applied to the same limited-domain task. One is a transfer model with monolingual head automata for analysis and generation; the ocher is a direct transduction model based on bilingual head transducers. We conclude that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort. 1 Introduction In this paper we describe an experimental machine translation system based on head transducer models and compare it to a related transfer system, described in Alshawi 1996a, based on monolingual head automata. Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon. The transfer system follows the familiar analysis-transfer-generation architecture (Isabelle and Macklovitch 1986). with mapping of dependency representations (Hudson 1984) in the transfer phase. In contrast, the head transducer approach is more closely aligned with earlier direct translation methods: no explicit representations of the source language (interlingua or otherwise) are created in the process of derivin</context>
<context position="3862" citStr="Alshawi 1996" startWordPosition="578" endWordPosition="579">ed domain application. We first describe the transfer and head transducer approaches in Sections 2 and :3 and the method used to assign the numerical parameters of the models in Section 4. In Section 5. we compare experimental systems, based on the two approaches. for Englishto-Chinese translation of air travel enquiries, and we conclude in Section 6. 2 Monolingual Automata and Transfer In this section we review the approach based on monolingual head automata together with transfer mapping. Further details of this approach, including the analysis, transfer, and generation algorithms appear in Alshawi 1996a. 360 2.1 Monolingual Relational Models We can characterize the language models used for analysis and generation in the transfer system as quantitative generative models of ordered dependency trees. In the dependency trees generated by these models, each node is labeled with a word w from the vocabulary V of the language in question: the nodes (and their word labels) immediately dominated by such a node are the dependents of w in the dependency derivation. Dependency tree arcs are labeled with symbols taken from a set R of dependency re;utiorts. These monolingual models are reversible, in the</context>
<context position="6897" citStr="Alshawi 1996" startWordPosition="1090" endWordPosition="1091"> actions above: • Selection of a word and acceptor to start an entire derivation. • Selection of a dependent word and acceptor given a head word and a dependency relation. 2.2 Transfer Transfer in this model is a mapping between unordered dependency trees. Surface ordering of dependent phrases of either the source or target is not taken into account in the transfer mapping. This ordering is completely defined by the source and target monolingual models. Our transfer model involves a bilingual lexicon specifying paired source-target fragments of dependency trees. A bilingual lexical entry (see Alshawi 1996a for more details) includes a mapping function between the source and target nodes of the fragments. Valid transfer mappings are defined in terms of a tiling of the source dependency tree with source fragments from bilingual lexicon entries so that the partial mappings defined in entries are extended to a mapping for the entire source tree. This tiling process has the side effect of creating an unordered target dependency representation. The following nondeterministic actions are involved in the tiling process: • Selection of a bilingual entry given a source language word, w. • Matching the n</context>
<context position="8330" citStr="Alshawi 1996" startWordPosition="1344" endWordPosition="1345">te state head acceptors employed in the transfer model. Such a transducer .11 is associated with a pair of words, a source word w and a target word v. In fact. w is taken from the set VI consisting of the source language vocabulary augmented by the -empty word&amp;quot; e, and u is taken from Vi , the target language vocabulary augmented with c. A head transducer reads from a pair of source sequences, a left source sequence L1 and a right source sequence RI; it writes to a pair of target sequences, a left target sequence L2 and a right target sequence R2 (Figure 1). Head transducers were introduced in Alshawi 1996b, where the symbols in the source and target sequences are source and target words respectively. In the experiment described in this paper the symbols written are dependency relation symbols or the 361 Figure 1: Head transducer M converts the sequences of left and right relations H. ... r) and (r44 r731) of w into left and right relations (r? ri) and (r2 r2) of v. 3+1 • • p empty symbol e. While it is possible to construct a translator based on head transduction models without relation symbols, using a version of head transducers with relation symbols allowed for a more direct comparison betw</context>
<context position="11186" citStr="Alshawi 1996" startWordPosition="1847" endWordPosition="1848">pair of words wo E V1 and vo E V2, and a head transducer Mo to start the entire derivation. • Selection of a pair of dependent words w&apos; and v&apos; and transducer M&apos; given head words w and v and source and target dependency relations r1 and r2. (w, w&apos; E VI; v&apos; E 172.) The recursion takes place by running a head transducer (M&apos; in the second action above) to derive local dependency trees for corresponding pairs of dependent words (w&apos;, v&amp;quot;). 4 Event Cost Assignment The transfer and head transduction derivation models can be formulated as probabilistic generative models; such formulations were given in Alshawi 1996a and 1996b respectively. Under such a formulation, negated log probabilities can be used as the costs for the actions listed in Sections 2 and 3. However, experimentation reported in Alshawi and Buchsbaum 1997 suggests that improved translation accuracy can be achieved by adopting cost functions other than log probability. This is true in particular for a family of discriminative cost functions. We define a cost function f as a real valued function taking two arguments, a event e and a context c. The context c is an equivalence class of states under which an action is taken, and the event e i</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>Alshawi, H. 1996b. &amp;quot;Head Automata for Speech Translation&amp;quot;. In Proceedings of the International Conference on Spoken Language Processing, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>R Mercer</author>
<author>P Rossin</author>
</authors>
<title>A Statistical Approach to Machine Translation&amp;quot;.</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<pages>16--79</pages>
<contexts>
<context position="2630" citStr="Brown et al. (1990" startWordPosition="385" endWordPosition="388">ectiveness of translation systems. This involves assessing the total cost Df employing a translation system, including, for example. the cost of manual post-editing. Post-editing is not an option in speech translation systems for person-to-person communication, and real-time operation is important in this context, so in comparing the two translation models we looked at a variety of other measures, including translation accuracy. speed, and system complexity. Both models underlying the translation systems can be characterized as statistical translation models, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. In fact, the original motivation for the head transducer models was that they are simpler and more amenable to automatic model structure acquisition, while the transfer component of the traditional system was designed with regard to allowing maximum flexibility in mapping between source and target representations to overcome translation divergences (Lindop and Tsujii 1991: Dorr 1994). In practice, it turned out that adopting the simpler transducer models did not involve sacrificing accuracy. at</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Rossin, 1990</marker>
<rawString>Brown, P., J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer and P. Rossin. 1990. &amp;quot;A Statistical Approach to Machine Translation&amp;quot;. Computational Linguistics 16:79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation&amp;quot;.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<pages>19--263</pages>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, P.F., S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. &amp;quot;The Mathematics of Statistical Machine Translation: Parameter Estimation&amp;quot;. Computational Linguistics 19:263-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K H Chen</author>
<author>H H Chen</author>
</authors>
<title>Attachment and Transfer of Prepositional Phrases with Constraint Propagation&amp;quot;.</title>
<date>1992</date>
<booktitle>Computer Processing of Chinese and Oriental Languages,</booktitle>
<volume>6</volume>
<pages>123--142</pages>
<contexts>
<context position="14305" citStr="Chen and Chen 1992" startWordPosition="2351" endWordPosition="2354">ems and an additional third source for the transfer system. The first set of counts was derived by processing traces using around 1200 sample utterances from the ATIS corpus. This involved running the systems on the sample utterances, starting initially with uniform costs, and presenting the resulting translations to a human judge for classification as correct or incorrect. The second source of counts was hand-tagging around 800 utterance transcriptions to identify correct and incorrect attachment points for prepositional phrases, PP-attachment being important for English-Chinese translation (Chen and Chen 1992). This attachment information was converted to corresponding counts for head-dependent choices involving prepositional phrase attachment. The additional source of counts used in the transfer system was an unsupervised training method in which 13000 training utterances were translated from English to Chinese, and then back again; the derivations were classified as positive (otherwise negative) if the resulting back-translation was sufficiently close to the original English, as described in Alshawi and Buchsbaum 1997. There was a strong systematic relationship between the structure of the models</context>
</contexts>
<marker>Chen, Chen, 1992</marker>
<rawString>Chen, K.H. and H. H. Chen. 1992. &amp;quot;Attachment and Transfer of Prepositional Phrases with Constraint Propagation&amp;quot;. Computer Processing of Chinese and Oriental Languages, Vol. 6, No, 2, 123-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Dorr</author>
</authors>
<title>Machine Translation Divergences: A Formal Description and Proposed Solution&amp;quot;.</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<pages>20--597</pages>
<contexts>
<context position="3117" citStr="Dorr 1994" startWordPosition="460" endWordPosition="461">lation systems can be characterized as statistical translation models, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. In fact, the original motivation for the head transducer models was that they are simpler and more amenable to automatic model structure acquisition, while the transfer component of the traditional system was designed with regard to allowing maximum flexibility in mapping between source and target representations to overcome translation divergences (Lindop and Tsujii 1991: Dorr 1994). In practice, it turned out that adopting the simpler transducer models did not involve sacrificing accuracy. at least for our limited domain application. We first describe the transfer and head transducer approaches in Sections 2 and :3 and the method used to assign the numerical parameters of the models in Section 4. In Section 5. we compare experimental systems, based on the two approaches. for Englishto-Chinese translation of air travel enquiries, and we conclude in Section 6. 2 Monolingual Automata and Transfer In this section we review the approach based on monolingual head automata tog</context>
</contexts>
<marker>Dorr, 1994</marker>
<rawString>Dorr, B.J. 1994. &amp;quot;Machine Translation Divergences: A Formal Description and Proposed Solution&amp;quot;. Computational Linguistics 20:597-634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate Methods for Statistics of Surprise and Coincidence.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<pages>19--61</pages>
<contexts>
<context position="12236" citStr="Dunning 1993" startWordPosition="2028" endWordPosition="2029"> valued function taking two arguments, a event e and a context c. The context c is an equivalence class of states under which an action is taken, and the event e is an equivalence class of actions possible from that set of states. We write the value of the function as f(eic), borrowing notation from the special case of conditional probabilities. The pair (eic) is referred to as a choice. The cost of a solution (i.e., a possible translation of an input string) is the sum of costs for all choices in the derivation of that solution. Discriminative cost functions, including likelihood ratios (cf. Dunning 1993), make use of both positive and negative instances of performing a task. Here we take a positive instance to be the derivation of a &amp;quot;correct&amp;quot; translation, and a negative instance the derivation of an &amp;quot;incorrect&amp;quot; translation, where correctness is judged by a speaker of both languages. Let n+ (e lc) be the count of taking choice (elc) in positive instances resulting from processing the source sentences in a training corpus. Similarly, let n- (elc) be the count of taking (elc) for negative instances. L, r I Ri rni L2 362 The cost function used in the experiments is computed as: f(elc) = log(n+(ei</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. 1993. &amp;quot;Accurate Methods for Statistics of Surprise and Coincidence.&amp;quot; Computational Linguistics 19:61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Hudson</author>
</authors>
<title>Word Grammar.</title>
<date>1984</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1329" citStr="Hudson 1984" startWordPosition="194" endWordPosition="195">curacy, computational requirements, model size, and development effort. 1 Introduction In this paper we describe an experimental machine translation system based on head transducer models and compare it to a related transfer system, described in Alshawi 1996a, based on monolingual head automata. Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon. The transfer system follows the familiar analysis-transfer-generation architecture (Isabelle and Macklovitch 1986). with mapping of dependency representations (Hudson 1984) in the transfer phase. In contrast, the head transducer approach is more closely aligned with earlier direct translation methods: no explicit representations of the source language (interlingua or otherwise) are created in the process of deriving the target string. Despite the simple direct architecture, the head transducer model does embody modern principles of lexicalized recursive grammars and statistical language processing. The context for evaluating both the transducer and transfer models was the development of experimental prototypes for speechto-speech translation. In the case of text</context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>Hudson, R.A. 1984. Word Grammar. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>M Bates</author>
<author>D Dahl</author>
<author>W Fisher</author>
<author>J Garofolo</author>
<author>D Pallett</author>
<author>K Hunicke-Smith</author>
<author>P Price</author>
<author>A Rudnicky</author>
<author>E Tzoukermann</author>
</authors>
<title>MultiSite Data Collection and Evaluation in Spoken Language Understanding&amp;quot;.</title>
<date>1993</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="13311" citStr="Hirschman et al. 1993" startWordPosition="2200" endWordPosition="2203">the count of taking (elc) for negative instances. L, r I Ri rni L2 362 The cost function used in the experiments is computed as: f(elc) = log(n+(eic) ± n(elc)) log(n+ (eic)). (By comparison, the usual &amp;quot;logprob&amp;quot; cost function using only positive instances would be log(n+(c)) — log(n+(elc)).) For unseen choices, we replace the context c and event e with larger equivalence classes. 5 Effectiveness Comparison 5.1 English-Chinese ATIS Models Both the transfer and transducer systems were trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus (Hirschman et al. 1993). By training here we simply mean assignment of the cost functions for fixed model structures. These model structures were coded by hand as monolingual head acceptor and bilingual dependency lexicons for the transfer system and a head transducer lexicon for the transducer system. Positive and negative counts for cost assignment were collected from two sources for both systems and an additional third source for the transfer system. The first set of counts was derived by processing traces using around 1200 sample utterances from the ATIS corpus. This involved running the systems on the sample ut</context>
</contexts>
<marker>Hirschman, Bates, Dahl, Fisher, Garofolo, Pallett, Hunicke-Smith, Price, Rudnicky, Tzoukermann, 1993</marker>
<rawString>Hirschman, L., M. Bates, D. Dahl, W. Fisher, J. Garofolo, D. Pallett, K. Hunicke-Smith, P. Price, A. Rudnicky, and E. Tzoukermann. 1993. &amp;quot;MultiSite Data Collection and Evaluation in Spoken Language Understanding&amp;quot;. In Proceedings of the Human Language Technology Workshop, Morgan Kaufmann, San Francisco, 19-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Isabelle</author>
<author>E Macklovitch</author>
</authors>
<title>Transfer and MT Modularity&amp;quot;,</title>
<date>1986</date>
<booktitle>In Eleventh International Conference on Computational Linguistics,</booktitle>
<pages>115--117</pages>
<location>Bonn, Germany,</location>
<contexts>
<context position="1271" citStr="Isabelle and Macklovitch 1986" startWordPosition="184" endWordPosition="188">that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort. 1 Introduction In this paper we describe an experimental machine translation system based on head transducer models and compare it to a related transfer system, described in Alshawi 1996a, based on monolingual head automata. Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon. The transfer system follows the familiar analysis-transfer-generation architecture (Isabelle and Macklovitch 1986). with mapping of dependency representations (Hudson 1984) in the transfer phase. In contrast, the head transducer approach is more closely aligned with earlier direct translation methods: no explicit representations of the source language (interlingua or otherwise) are created in the process of deriving the target string. Despite the simple direct architecture, the head transducer model does embody modern principles of lexicalized recursive grammars and statistical language processing. The context for evaluating both the transducer and transfer models was the development of experimental proto</context>
</contexts>
<marker>Isabelle, Macklovitch, 1986</marker>
<rawString>Isabelle, P. and E. Macklovitch. 1986. &amp;quot;Transfer and MT Modularity&amp;quot;, In Eleventh International Conference on Computational Linguistics, Bonn, Germany, 115-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
<author>S Roukos</author>
</authors>
<title>Principles of Lexical Language Modeling for Speech Recognition&amp;quot;.</title>
<date>1992</date>
<booktitle>Advances in Speech Signal Processing,</booktitle>
<editor>In S. Furui and M.M. Sondhi (eds.),</editor>
<publisher>Marcel Dekker,</publisher>
<location>New York.</location>
<marker>Jelinek, Mercer, Roukos, 1992</marker>
<rawString>Jelinek, F., R.L. Mercer and S. Roukos. 1992. &amp;quot;Principles of Lexical Language Modeling for Speech Recognition&amp;quot;. In S. Furui and M.M. Sondhi (eds.), Advances in Speech Signal Processing, Marcel Dekker, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>D Sleator</author>
<author>D Temperley</author>
</authors>
<title>Grammatical Trigrams: A Probabilistic Model of Link Grammar&amp;quot;.</title>
<date>1992</date>
<booktitle>In Proceedings of the 1992 AAAI Fall Symposium on Probabilistic Approaches to Natural Language,</booktitle>
<pages>89--97</pages>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>Lafferty, J., D. Sleator and D. Temperley. 1992. &amp;quot;Grammatical Trigrams: A Probabilistic Model of Link Grammar&amp;quot;. In Proceedings of the 1992 AAAI Fall Symposium on Probabilistic Approaches to Natural Language, 89-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Head Driven Parsing&amp;quot;.</title>
<date>1989</date>
<booktitle>In Proceedings of the Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh,</location>
<marker>Kay, 1989</marker>
<rawString>Kay, M. 1989. &amp;quot;Head Driven Parsing&amp;quot;. In Proceedings of the Workshop on Parsing Technologies, Pittsburgh, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lindop</author>
<author>J Tsujii</author>
</authors>
<title>Complex Transfer in MT: A Survey of Examples&amp;quot;.</title>
<date>1991</date>
<tech>Technical Report 91/5,</tech>
<institution>Centre for Computational Linguistics,</institution>
<location>UMIST, Manchester, UK.</location>
<contexts>
<context position="3105" citStr="Lindop and Tsujii 1991" startWordPosition="456" endWordPosition="459">els underlying the translation systems can be characterized as statistical translation models, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. In fact, the original motivation for the head transducer models was that they are simpler and more amenable to automatic model structure acquisition, while the transfer component of the traditional system was designed with regard to allowing maximum flexibility in mapping between source and target representations to overcome translation divergences (Lindop and Tsujii 1991: Dorr 1994). In practice, it turned out that adopting the simpler transducer models did not involve sacrificing accuracy. at least for our limited domain application. We first describe the transfer and head transducer approaches in Sections 2 and :3 and the method used to assign the numerical parameters of the models in Section 4. In Section 5. we compare experimental systems, based on the two approaches. for Englishto-Chinese translation of air travel enquiries, and we conclude in Section 6. 2 Monolingual Automata and Transfer In this section we review the approach based on monolingual head </context>
</contexts>
<marker>Lindop, Tsujii, 1991</marker>
<rawString>Lindop, J. and J. Tsujii. 1991. &amp;quot;Complex Transfer in MT: A Survey of Examples&amp;quot;. Technical Report 91/5, Centre for Computational Linguistics, UMIST, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sata</author>
</authors>
<title>Head-Driven Bidirectional Parsing&amp;quot;.</title>
<date>1989</date>
<booktitle>In Proceedings of the Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh.</location>
<marker>Sata, 1989</marker>
<rawString>Sata, G. and 0. Stock. 1989. &amp;quot;Head-Driven Bidirectional Parsing&amp;quot;. In Proceedings of the Workshop on Parsing Technologies, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Younger</author>
</authors>
<title>Recognition and Parsing of Context-Free Languages</title>
<date>1967</date>
<booktitle>in Time n3. Information and Control,</booktitle>
<volume>10</volume>
<pages>189--208</pages>
<marker>Younger, 1967</marker>
<rawString>Younger, D. 1967. Recognition and Parsing of Context-Free Languages in Time n3. Information and Control, 10, 189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>