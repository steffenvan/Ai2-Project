<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000449">
<title confidence="0.985212">
The Dynamics of Action Corrections in Situated Interaction
</title>
<author confidence="0.996768">
Antoine Raux
</author>
<affiliation confidence="0.672647">
Honda Research Institute USA
Mountain View, CA, USA.
araux@honda-ri.com
</affiliation>
<author confidence="0.961666">
Mikio Nakano
</author>
<affiliation confidence="0.831363">
Honda Research Institute Japan
Wako, Japan
</affiliation>
<email confidence="0.988012">
nakano@jp.honda-ri.com
</email>
<sectionHeader confidence="0.993656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953580645161">
In spoken communications, correction ut-
terances, which are utterances correct-
ing other participants utterances and be-
haviors, play crucial roles, and detecting
them is one of the key issues. Previ-
ously, much work has been done on au-
tomatic detection of correction utterances
in human-human and human-computer di-
alogs, but they mostly dealt with the cor-
rection of erroneous utterances. How-
ever, in many real situations, especially in
communications between humans and mo-
bile robots, the misunderstandings man-
ifest themselves not only through utter-
ances but also through physical actions
performed by the participants. In this pa-
per, we focus on action corrections and
propose a classification of such utterances
into Omission, Commission, and Degree
corrections. We present the results of our
analysis of correction utterances in dialogs
between two humans who were engaging
in a kind of on-line computer game, where
one participant plays the role of the re-
mote manager of a convenience store, and
the other plays the role of a robot store
clerk. We analyze the linguistic content,
prosody as well as the timing of correction
utterances and found that all features were
significantly correlated with action correc-
tions.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879125">
Recent progress in robot technology made it a re-
ality to have robots work in offices and homes, and
spoken dialog is considered to be one of the most
desired interface for such robots. Our goal is to
build a spoken dialog interface for robots that can
move around in an office or a house and execute
tasks according to humans’ requests.
Building such spoken dialog interface for robots
raises new problems different from those of tra-
ditional spoken/multimodal dialog systems. The
intentions behind human utterances may vary de-
pending on the situation where the robot is and the
situation changes continuously not only because
the robot moves but also because humans and ob-
jects move, and human requests change. In this
sense human-robot interaction is situated.
Of the many aspects of situated interaction, we
focus on the timing structure of interaction. Al-
though traditional spoken dialog systems deal with
some timing issues such as turn-taking and han-
dling barge-ins, timing structure in human-robot
interaction is far more complex because the robot
can execute physical actions and those actions can
occur in parallel with utterances.
In this work we are concerned specifically with
corrections in situated interaction. In joint physi-
cal tasks, human corrective behavior, which allows
to repair discrepancies in participants’ mutual un-
derstanding, is tightly tied to actions.
While past work on non-situated spoken dialog
systems has shown the necessity and feasibility of
detecting and handling corrections (Kitaoka et al.,
2003; Litman et al., 2006; Gieselman and Osten-
dorf, 2007; Cevik et al., 2008), most of these mod-
els assume that corrections target past utterances
and rely on a strict turn-based structure which is
frequently violated in situated interaction. When
dialog is interleaved with physical actions, the spe-
cific timing of an utterance relative to other utter-
ances and actions is more relevant than the turn
sequence.
In this paper, we propose a classification of er-
rors and corrections in physical tasks and analyze
the properties of different types of corrections in
the context of human-human task-oriented inter-
actions in a virtual environment. The next section
gives some characteristics of corrections in situ-
ated interaction. Section 3 describes our experi-
</bodyText>
<note confidence="0.6229675">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 165–174,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</note>
<page confidence="0.996487">
165
</page>
<figure confidence="0.977815692307692">
Alice : Put it right above (1)
the lamp stand
Bob : Here? (2)
Alice : A little bit more (3)
to the right.
(Bob Moves the frame left) (4)
Alice : No the right! (5)
(Bob Moves the frame right) (6)
Alice : More... (7)
Alright, that’s ... (8)
(A bee flies next to Bob) (9)
Alice : Watch out! That bee is
going to sting you! (10)
</figure>
<figureCaption confidence="0.999953">
Figure 1: Example dialog from a situated task.
</figureCaption>
<bodyText confidence="0.999903">
mental set up and data collection effort. Section 4
presents the results of our analysis of corrections
in terms of timing, prosodic, and lexical features.
These results are discussed in Section 5.
</bodyText>
<sectionHeader confidence="0.894459" genericHeader="method">
2 Corrections in Situated Tasks
</sectionHeader>
<subsectionHeader confidence="0.980517">
2.1 Situated Tasks
</subsectionHeader>
<bodyText confidence="0.99996390625">
We define a situated task as one for which two or
more agents interact in order to perform physical
actions in the (real or virtual) world. Physical ac-
tions involve moving from one place to another,
as well as manipulating objects. In many cases,
interaction happens simultaneously with physical
actions and can be affected by them, or by other
external events happening in the world. For exam-
ple, Figure 1 shows an extract of a (constructed)
dialog where one person (Alice) assists another
(Bob) while he hangs a picture frame on a wall.
This interaction presents some similarities and
differences with unimodal, non-situated dialogs.
In addition to standard back-and-forth turn-taking
as in turns 1-3, this example features utterances
by Alice which are not motivated by Bob’s ut-
terances, but rather by (her perception of) his ac-
tions (e.g. utterance 5 is a reaction to action 4),
as well as external events such as 9, which trig-
gered response 10 from Alice. Therefore the con-
tent of Alice’s utterances is dependent not only on
Bob’s, but also on events happening in the world.
Similarly, the timing of Alice’s utterances is not
only conditioned on Bob’s speech, prosody, etc,
but also on asynchronous world events.
Robots and other agents that interact with peo-
ple in real world situations need to be able to ac-
count for the impact of physical actions and world
events on dialog. In the next section and the rest
of this paper, we focus on correction utterances
and how situational context affects how and when
speakers produce them.
</bodyText>
<subsectionHeader confidence="0.999059">
2.2 Corrections
</subsectionHeader>
<bodyText confidence="0.9996334">
Generally speaking, a correction is an utterance
which aims at signaling or remedying a misunder-
standing between the participants of a conversa-
tion. In other word, corrections help (re)establish
common ground (Clark, 1996).
</bodyText>
<subsectionHeader confidence="0.643099">
2.2.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.999790710526316">
There are many dimensions along which correc-
tions can be analyzed and many researchers have
addressed this issue. Conversational Analysis
(CA) has, from its early days, 1 concerned it-
self with corrections (usually called repairs in CA
work) (Schegloff et al., 1977).
More recently, spoken dialog systems re-
searchers have investigated ways to automatically
recognize corrections. For instance, Litman et al.
(2006) exploited features to automatically detect
correction utterances. In addition, several attempts
have been made to exploit the similarity in speech
sounds and speech recognition results of a correc-
tion and the previous user utterance to detect cor-
rections (Kitaoka et al., 2003; Cevik et al., 2008).
Going beyond a binary correction/non-
correction classification scheme, Levow (1998)
distinguished corrections of misrecognition errors
from corrections of rejection errors and found
them to have different prosodic features. Rog-
driguez and Schlangen (2004) and Rieser and
Moore (2005) classify corrections according to
their form (e.g. Repetition, Paraphrase, Addition
of information...) and function. The latter aspect
is mostly characterized in terms of the source of
the problem that is being corrected using models
of communication such as that of Clark (1996).
In all of this very rich literature, corrections are
assumed to target utterances from another partic-
ipant (or even oneself, in the case of self-repair)
that conflict with the hearer’s expectations. While
some work on embodied conversational agents
(Cassell et al., 2001; Traum and Rickel, 2002)
does consider physical actions as possible cues
to errors and corrections, the actions are typically
communicative in nature (e.g. nods, glances, ges-
tures). Comparatively, there is extremely little
work on corrections that target task actions.
</bodyText>
<page confidence="0.997996">
166
</page>
<bodyText confidence="0.9997565">
A couple of exceptions are Traum et al. (1999),
who discuss the type of context representation
needed to handle action corrections, and Fu-
nakoshi and Tokunaga (2006), who present a
model for identifying repair targets in human-
robot command-and-control dialogs. While im-
portant, these papers focus on theoretical planning
aspects of corrections whereas this paper focuses
on an empirical analysis of human conversational
behavior.
</bodyText>
<subsectionHeader confidence="0.89497">
2.2.2 Action Corrections in Situated
Interaction
</subsectionHeader>
<bodyText confidence="0.9999365">
As seen above, the vast majority of prior work on
corrections concerned corrections of previous (er-
roneous) utterances (i.e. utterance corrections). In
contrast, in this paper, we focus exclusively on
corrections that target previous physical actions
(i.e. action corrections).
While some classification schemes of utterance
corrections are applicable to task corrections (e.g.
those based on the form of the correction itself),
we focus on differences that are specific to action
corrections.
Namely, we distinguish three types of action er-
rors and their related action corrections:
Commission errors occur when Bob performs an
action that conflicts with Alice’s expectation.
Action 4 of Figure 1 is a commission error,
which is corrected in turn 5.
Omission errors occur when Bob fails to react to
one of Alice’s utterances. A typical way for
Alice to correct an omission error is to repeat
the utterance to which Bob did not react.
Degree errors occur when Bob reacts with an ap-
propriate action to Alice’s utterance but fails
to completely fulfill Alice’s goal. This is il-
lustrated by Alice’s use of ”More” in turn 7
in response to Bob’s insufficient action 6.
Figure 3 illustrates the three error categories
based on extracts from the corpus.
In some ways, the dichotomy Commission er-
rors/Omission errors parallels that of Misrecogni-
tions/Rejections by Levow (1998). This type of
classification is also commonly used to analyze
human errors in human factors research (Wick-
ens et al., 1998). In addition to these two cat-
egories, we added the Degree category based on
our observation of the data we collected. This as-
pect is somewhat specific to certain kinds of phys-
ical actions (those that can be performed to differ-
ent degrees, as opposed to binary actions such as
“opening the door”). However, it seems general
enough to be applied to many collaborative tasks
relevant to robots such as guidance, tele-operation,
and joint construction.
For an automated agent, being able to classify
a user utterance into one of these four categories
(including non-action-correction utterances) could
be very useful to make fast, appropriate decisions
such as canceling the current action, or asking a
clarification question to the user. This is impor-
tant because in human-robot interaction, respon-
siveness to a correction can be critical in avoiding
physical accidents. For instance, if the robot de-
tects that the user issued a commission error cor-
rection, it can stop performing its current action
even before understanding the details of the cor-
rection.
In the rest of the paper, we analyze some lexi-
cal, prosodic and temporal characteristics of action
corrections in the context of human-human con-
versations in a virtual world.
</bodyText>
<sectionHeader confidence="0.995571" genericHeader="method">
3 The Konbini Domain and System
</sectionHeader>
<subsectionHeader confidence="0.9986715">
3.1 Simulated Environments for
Human-Robot Interaction Research
</subsectionHeader>
<bodyText confidence="0.999975954545454">
One obstacle to the empirical study of situated in-
teraction is that it requires a fully functional so-
phisticated robot to collect data and conduct ex-
periments. Most such complex robots are still
fragile and thus it is typically challenging to run
user studies with naive subjects without severely
limiting the tasks or the scope of the interaction.
Another issue which comes with real world inter-
action is that it is difficult for the experimenter to
control or monitor the events that affect the inter-
action. Most of the time, an expensive manual
annotation of events and actions is required (see
Okita et al. (2009) for an example of such an ex-
perimental setup).
To avoid these issues, robot simulators have
been used. Koulouri and Lauria (2009) devel-
oped a simulator to collect dialogs between hu-
man and simulated robot using a Wizard-of-Oz
method. The human can see a map of a town and
teaches the robot a route and the operator oper-
ates the robot but he/she can see only a small area
around the robot in the map. However, the dialog
</bodyText>
<page confidence="0.98803">
167
</page>
<bodyText confidence="0.999579090909091">
is keyboard-based, and the situation does not dy-
namically change in this setting, making this ap-
proach unsuitable to the study of timing aspects.
Byron and Fosler-Lussier (2006) describe a cor-
pus of spoken dialogs collected in a setting very
similar to the one we are using but, again, the en-
vironment appears to be static, thus limiting the
importance of the timing of actions and utterances.
In this section, we describe a realistic, PC-based
virtual world that we used to collect human-human
situated dialogs.
</bodyText>
<subsectionHeader confidence="0.998901">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999953222222222">
In our experiment, two human participants collab-
orate in order to perform certain tasks pertaining
to the management of a small convenience store in
a virtual world. The two participants sit in differ-
ent rooms, both facing a computer that presents
a view of the virtual store. One of the partici-
pants, the Operator (O) controls a (simulated) hu-
manoid robot whose role is to answer all customer
requests. The other participant plays the role of a
remote Manager (M) who sees the whole store but
can only interact with O through speech.
Figure 2 shows the Operator and Manager
views. M can see the whole store at any time, in-
cluding how many customers there are and where
they are. In addition, M knows when a particu-
lar customer has a request because the customer’s
character starts blinking (initially green, then yel-
low, then red, as time passes). M’s role is then to
guide O towards the customers needing attention.
On the other hand, O sees the world through the
”eyes” of the robot, whose vision is limited both
in terms of field of view (90 degrees) and depth
(degradation of vision with depth is produced by
adding a virtual ”fog” to the view). When ap-
proaching a customer who has a pending request,
O’s view display the customer’s request in the
form of a caption.1 O can act upon the virtual
world by clicking on certain object such as items
on the counter (to check them out), machines in
the store (to repair them when needed), and vari-
ous objects littering the floor (to clean them up).
Each action takes a certain amount of time to per-
form (between 3 and 45 seconds), indicated by a
progress bar that decreases as O keeps the pointer
on the target object and the left mouse button
down. Once the counter goes to zero the action is
</bodyText>
<footnote confidence="0.6166955">
1No actual speech interaction happens between the Oper-
ator and the simulated customers.
</footnote>
<bodyText confidence="0.999951428571429">
completed and the participants receive 50 (for par-
tially fulfilling a customer request) or 100 points
(for completely fulfilling a request).
When the session begins, customers start enter-
ing the store at random intervals, with a maximum
of 4 customers in the store at any time. Each cus-
tomer follows one of 14 predefined scenarios, each
involving between 1 and 5 requests. Scenarios
represent the customer’s moves in terms of fixed
way points. As a simplification, we did not imple-
ment any reactive path planning. Rather, the ex-
perimenter, sitting in a different room than either
subject has the ability to temporarily take control
of any customer to make them avoid obstacles.
</bodyText>
<subsectionHeader confidence="0.7968785">
3.3 System Implementation: the Siros
architecture
</subsectionHeader>
<bodyText confidence="0.999844433333333">
The experimental system described above was im-
plemented using Siros (SItuated RObot Simula-
tor) a new client/server architecture developed at
Honda Research Institute USA to conduct human-
robot interaction research in virtual worlds. Siros
is similar to the architectures used by certain on-
line video games. The server’s role is to manage
the virtual world and broadcast world updates to
all clients so that they can be rendered to the hu-
man participants. The server receives commands
from the Operator client (robot moves), runs the
simulated customers according to the scenarios,
and maintains the timer and the score. Anytime
the trajectory of an entity (robot, customer, object)
changes, the server broadcasts the related infor-
mation, including entity location, orientation, and
speed, to all clients.
Clients are in charge of rendering a given view
of the virtual world. Rendering itself is performed
by the open source Ogre 3D engine (Open Source
3D Graphics Engine, 2010). In addition, clients
handle all required user interaction such as robot
control and mouse-based object selection. All net-
work messages and user actions are logged into
a text file for further processing. Finally, clients
have the ability to log incoming audio to a wave
file, allowing synchronization between the audio
signal, the user actions, and virtual world events.
Spoken communication itself is handled by an ex-
ternal VoIP client.2
</bodyText>
<footnote confidence="0.997808">
2We used the open source Mumble/Murmur (Mum-
ble/Murmur Project, 2010) system.
</footnote>
<page confidence="0.987148">
168
</page>
<figure confidence="0.996639">
(a) Manager View (b) Robot View
</figure>
<figureCaption confidence="0.999827">
Figure 2: Screenshots of the Konbini data collection system.
</figureCaption>
<subsectionHeader confidence="0.8345615">
3.4 The Konbini Corpus
3.4.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.998795696969697">
Using the system described above, we collected
data from 18 participants. There were 15 male and
3 female participants. All were adults living in the
United States, fluent in English. All were regular
users of computers but their experience with on-
line games was diverse (from none at all to regu-
lar player). All were co-workers (associates or in-
terns) at Honda Research Institute USA, and thus
they knew each other fairly well.
Participants were randomly paired into teams.
After being given the chance to read a brief in-
troduction to the experiment’s design and goals,
the participants did two two-minute practice ses-
sions to familiarize themselves with the task and
control. To avoid providing too much informa-
tion about the layout of the store from the start,
the practice sessions used a different virtual world
than the experimental sessions. The participants
switched roles between the two practice sessions
to get a sense of what both roles entailed (Man-
ager and Operator). After these sessions, the
team did one 10-minute experimental session, then
switched roles once again and did another 10-
minute session. Because the layout of the store
was kept the same between the two experimental
sessions, the first session represents a condition in
which the Operator learns the store layout as they
are performing the tasks, whereas the second ses-
sion corresponds to a case where the Operator al-
ready has knowledge of the layout. Overall, 18 10-
minute sessions were collected, including audio
recordings as well as timestamped logs of world
updates and operator actions.
</bodyText>
<subsectionHeader confidence="0.730888">
3.4.2 Annotation
</subsectionHeader>
<bodyText confidence="0.999923391304348">
All recordings were orthographically transcribed
and checked. The first author then segmented the
transcripts into dialog acts (DAs). A DA label was
attached to each act, though this information is not
used in the present paper.
Subsequently, the first author annotated each
semantic unit with the action correction labels
described in section 2: Non-correction, Omis-
sion Correction, Commission Correction, Degree
Correction. This annotation was done using the
Anvil video annotation tool,3 which presented au-
dio recordings, transcripts, a timeline of operator
actions, as well as videos of the computer screens
of the participants. Only Manager DAs were anno-
tated for corrections. The second author also anno-
tated a subset of the data in the same way to evalu-
ate inter-annotator agreement. Cohen’s kappa be-
tween the two annotators was 0.67 for the 4-class
task, and 0.76 for the binary task of any-action-
correction vs non-action-correction, which is rea-
sonable, though not very high, indicating that cor-
rection annotation on this type of dialogs is a non-
trivial task, even for human annotators.
</bodyText>
<sectionHeader confidence="0.568242" genericHeader="method">
4 Analysis of Action Corrections
</sectionHeader>
<subsectionHeader confidence="0.951663">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999988714285714">
The total number of DAs in the corpus is 6170.
Of those, 826 are corrections and 5303 are non-
corrections. Overall, corrections account thus for
13.4% of the dialog acts. The split among the dif-
ferent correction classes is roughly equal as shown
in Table 5 given in appendix. We found however
significant differences across participants, in terms
</bodyText>
<footnote confidence="0.966062">
3http://www.anvil-software.de
</footnote>
<page confidence="0.99868">
169
</page>
<bodyText confidence="0.9999515">
of total number of DAs (form 162 to 516), propor-
tion of corrections among those DAs (from 6.8%
to 30.6%), as well as distribution among the three
types of action corrections.
In this section, we present the results of our sta-
tistical analysis of the correlation between a num-
ber of features and correction type. To evaluate
statistical significance, we performed a one-way
ANOVA using each feature as dependent variable
and the correction type as independent variable.
All features described here were found to signifi-
cantly correlate with correction type.
</bodyText>
<subsectionHeader confidence="0.9073305">
4.2 Features Affecting Corrections
4.2.1 Timing
</subsectionHeader>
<bodyText confidence="0.923424451612903">
For each manager DA, we computed the time since
the beginning/end of the previous manager and
operator DAs, as well as of operator’s actions
(walk/turn). To account for reaction time, and
based on our observations we ignored events hap-
pening less than 1 second before a DA.
Table 1 shows the mean durations between these
events and a Manager DA, depending on the act’s
correction class. All corrections happen closer to
Manager dialog acts than non-corrections, which
reflects the fact that corrections typically occur in
phases when the Manager gives instructions, as
well as the fact that the Manager often repeats
corrections. Commission and Degree corrections
are produced closer to Operator actions than either
non-corrections or Omission corrections. This re-
flects the fact that both Commission and Degree
corrections are a reaction to an event that occurred
(the Operator moved or stopped moving unex-
pectedly), whereas Omission corrections address
a lack of action from the Operator, and act there-
fore as a ”time-out” mechanism.
To better understand the relationship between
moves and the timing of corrections, we computed
the probability of a given DA to be an Omission,
Commission and Degree correction as a function
Time since last... NC O C D
Mgr. DA 3.4 s 2.4 s 2.8 s 2.6 s
Ope. DA 5.8 s 6.7 s 6.5 s 7.5 s
Ope. move start 3.8 s 3.1 s 2.3 s 2.5 s
Ope. move end 3.9 s 3.3 s 2.7 s 2.3 s
</bodyText>
<tableCaption confidence="0.938006666666667">
Table 1: Mean duration between dialog acts and
Operator movements and the beginning of differ-
ent corrections.
</tableCaption>
<table confidence="0.9999655">
Feature Non-Corr Om Com Deg
Perc. voiced 0.48 0.46 0.55 0.53
Min F0 -0.61 -0.41 -0.40 -0.56
Max F0 0.81 0.68 1.02 0.46
Mean F0 -0.03 0.12 0.28 -0.05
Min Power -1.35 -1.24 -1.18 -1.55
Max Power 0.85 0.89 1.14 0.62
Mean Power -0.03 0.09 0.24 -0.2
</table>
<tableCaption confidence="0.9843955">
Table 2: Mean Z-score ofprosodic features for dif-
ferent correction classes.
</tableCaption>
<bodyText confidence="0.9997831">
of the time elapsed since the Operator last started
to move. Figure 4 shows the results.
The probability of a DA being an Omission cor-
rection is relatively stable over time. This is con-
sistent with the fact that Omission corrections are
related to lack of action rather than to a specific
action to which the Manager reacts. On the other
hand, the probability of a Commission, and to
lesser extent, Degree correction sharply decreases
with time after an action.
</bodyText>
<subsectionHeader confidence="0.616036">
4.2.2 Prosody
</subsectionHeader>
<bodyText confidence="0.9999905">
We extracted F0 and intensity from all manager
audio files using the Praat software (Boersma and
Weenink, 2010). We then normalized pitch and in-
tensity for each speaker using a Z-transform in or-
der to account for individual differences in mean
and standard deviation. For each DA, we com-
puted the minimum, maximum, and mean pitch
and intensity, using values from voiced frames.
Table 2 shows the mean Z-score of the prosodic
features for the different correction classes. Com-
mission corrections feature higher pitch and inten-
sity than all other classes. This is due to the fact
that such corrections typically involve a higher
emotional level, when the Manager is surprised
or even frustrated by the behavior of the Operator.
In contrast, Degree corrections, which represent a
smaller mismatch between the Operator’s action
and the Manager’s expectations are more subdued,
with mean power and intensity values lower than
even those of non-corrections.
</bodyText>
<subsectionHeader confidence="0.94855">
4.2.3 Lexical Features
</subsectionHeader>
<bodyText confidence="0.9995105">
In order to identify potential lexical characteristics
of correction utterances, we created binary vari-
ables indicating that a specific word from the vo-
cabulary (804 distinct words in total) appears in
a given DA based on the manual transcripts. We
computed the mutual information of those binary
</bodyText>
<page confidence="0.991209">
170
</page>
<bodyText confidence="0.996383333333333">
variables with DA’s correction label.
Figure 3 shows the 10 words with highest
mutual information. Not surprisingly, negative
words (”NO”, ”DON’T”), continuation words
(”MORE”, ”KEEP”) are correlated with respec-
tively commission and degree corrections. On the
other hand, positive words (”OKAY”, ”YEAH”)
are strong indicators that a DA is not a correction.
Another lexical feature we computed was a flag
indicating that a certain Manager DA is an ex-
act repetition of the immediately preceding Man-
ager DA. The intuition behind this feature is that
corrections often involve repetitions (e.g. ”Turn
left [Operator turns right] Turn left!”). Overall,
10.6% of the DAs are repetitions. This num-
ber is only 6.4% on non-corrections but jumps to
45.6%, 22.5%, and 43.4% on, respectively, Omis-
sion, Commission, and Degree corrections. This
confirms that, as for utterance corrections, detect-
ing exact repetitions could prove useful for correc-
tion classification.
</bodyText>
<subsectionHeader confidence="0.833387">
4.2.4 ASR Features
</subsectionHeader>
<bodyText confidence="0.999992454545455">
Since our goal is to build artificial agents, we
investigated features related to automatic speech
recognition. We used the Nuance Speech Recog-
nition System v8.5. Using cross-validation, we
trained a statistical language model for each cor-
rection category on the transcripts of the training
portion of the data. We then ran the recognizer se-
quentially with all 4 language models, which gen-
erated a confidence score for each category.
Table 4 shows the mean confidence scores ob-
tained on DAs of each class using a language
model trained on specific classes. While the
matching LM gives the highest score for any given
class, some classes have consistently higher scores
than others. In particular, Commission corrections
receive low confidence scores, which might hurt
the effectiveness of these features. Indeed, lexi-
cal content alone might not be not enough to dis-
tinguish non-corrections and various categories of
corrections since the same expression (e.g. ”Turn
left”) can express a simple instruction, or any kind
of correction, depending on context.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999752">
The results provide support for the correction
classification scheme we proposed. Not only
do corrections differ in many respects from non-
correction utterances, but there are also signifi-
cant differences between Omission, Commission,
</bodyText>
<table confidence="0.996157285714286">
Corr. NC O C D
��������
LM
Non-Correction 32.3 28.5 25.0 29.5
Omission 24.0 30.0 23.3 27.2
Commission 26.6 29.8 25.7 27.9
Degree 24.2 28.7 23.9 32.6
</table>
<tableCaption confidence="0.992903">
Table 4: Mean ASR confidence score using class-
specific LMs.
</tableCaption>
<bodyText confidence="0.9985514">
and Degree corrections. Timing features seem
most useful to distinguish Commission and De-
gree corrections from Omission corrections and
non-corrections. Emphasized prosody (high pitch
and energy) is a particularly strong indicator of
Commission, as well as Omission corrections.
Lexical cues could be useful to all categories, pro-
vided the speech recognizer is accurate enough to
recognize them, which is particularly challenging
on this data given the very conversational nature
of the speech. Finally, ASR scores are also po-
tentially useful features, particularly for Omission
and Degree corrections.
One advantage of timing over all other features
discussed here is that timing information is avail-
able before the correction is actually uttered. This
means that such information could be used to al-
low fast reaction, or to prime the speech recog-
nizer based on the instantaneous probability of the
different classes of correction.
</bodyText>
<sectionHeader confidence="0.996154" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999918923076923">
In this paper, we analyzed correction utterances in
the context of situated spoken interaction within
a virtual world. We proposed a classification of
action correction utterances into Omission, Com-
mission, and Degree corrections. Our analysis
of human-human data collected using a PC-based
simulated environment shows that the three types
of corrections have unique characteristics in terms
of prosody, lexical features, as well as timing with
regards to physical actions. These results can
serve as the basis for further investigations into au-
tomatic detection and understanding of correction
utterances in situated interaction.
</bodyText>
<sectionHeader confidence="0.998256" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.88097">
Paul Boersma and David Weenink. 2010.
Praat: doing phonetics by computer,
http://www.fon.hum.uva.nl/praat.
</reference>
<page confidence="0.991337">
171
</page>
<table confidence="0.999782272727273">
Word (W) P(Non − Corr|W) P(Om|W) P(Com|W) P(Deg|W)
MORE 0.41 0.01 0.02 0.56
NO 0.55 0.04 0.33 0.07
RIGHT 0.67 0.15 0.04 0.14
TURN 0.69 0.17 0.06 0.08
LEFT 0.65 0.18 0.07 0.10
OKAY 0.99 0.00 0.00 0.00
YEAH 0.99 0.00 0.00 0.00
DON’T 0.59 0.01 0.33 0.07
WAY 0.49 0.05 0.42 0.04
KEEP 0.79 0.03 0.03 0.15
</table>
<tableCaption confidence="0.99979">
Table 3: Keywords with highest mutual information with correction category.
</tableCaption>
<reference confidence="0.974549232876712">
Donna K. Byron and Eric Fosler-Lussier. 2006.
The OSU Quake 2004 corpus of two-party situated
problem-solving dialogs. In Proc. 15th Language
Resource and Evaluation Conference (LREC’06).
Justine Cassell, Timothy Bickmore, Hannes Hgni
Vilhj´amsson, and Hao Yan. 2001. More Than Just a
Pretty Face: Conversational Protocols and the Affor-
dances of Embodiment. Knowledge-Based Systems,
14:55–64.
Mert Cevik, Fuliang Weng, and Chin hui Lee. 2008.
Detection of repetitions in spontaneous speech di-
alogue sessions. In Proc. Interspeech 2008, pages
471–474.
Herbert Clark. 1996. Using Language. Cambridge
University Press.
Kotaro Funakoshi and Takenobu Tokunaga. 2006.
Identifying repair targets in action control dialogue.
In Proc. EACL 2006, pages 177–184.
Petra Gieselman and Mari Ostendorf. 2007. Problem-
Sensitive Response Generation in Human-Robot Di-
alogs. In Proc. SIGDIAL 2002.
Norihide Kitaoka, Naoko Kakutani, and Seiichi Naka-
gawa. 2003. Detection and Recognition of Correc-
tion Utterance in Spontaneously Spoken Dialog. In
Proc. Eurospeech 2003, pages 625–628.
Theodora Koulouri and Stanislao Lauria. 2009. Ex-
ploring miscommunication and collaborative be-
haviour in human-robot interaction. In Proc. SIG-
DIAL 2009, pages 111–119.
Gina-Anne Levow. 1998. Characterizing and recog-
nizing spoken corrections in human-computer dia-
logue. In Proc. COLING-ACL ’98, pages 736–742.
Diane Litman, Julia Hirschberg, and Marc Swerts.
2006. Characterizing and predicting corrections in
spoken dialogue systems. Computational Linguis-
tics, 32(3):417–438.
The Mumble/Murmur Project. 2010.
http://mumble.sourceforge.net.
Sandra Y Okita, Victor Ng-Thow-Hing, and Ravi K
Sarvadevabhatla. 2009. Learning Together:
ASIMO Developing an Interactive Learning Partner-
ship with Children. In Proc. RO-MAN 2009.
OGRE Open Source 3D Graphics Engine. 2010.
http://www.ogre3d.org.
Verena Rieser and Johanna Moore. 2005. Implications
for generating clarification requests in task-oriented
dialogues. In Proc. 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL-05),
pages 239–246.
Kepa Josepa Rogdriguez and David Schlangen. 2004.
Form, intonation and function of clarification re-
quests in german task-oriented spoken dialogues. In
Proc. 8th Workshop on the Semantics and Pragmat-
ics of Dialogue (CATALOG’04).
Emanuel A. Schegloff, Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. Lan-
guage, 53(2):361–382.
David Traum and Jeff Rickel. 2002. Embodied
agents for multiparty dialogue in immersive virtual
worlds. In Proc. International Joint Conference on
Autonomous Agents and Multi-agent Systems (AA-
MAS 2002), pages 766–773.
David R. Traum, Carl F. Andersen, Waiyian Chong,
Darsana P. Josyula, Yoshi Okamoto, Khemdut Pu-
rang, Michael O’Donovan-Anderson, and Donald
Perlis. 1999. Representations of Dialogue State
for Domain and Task Independent Meta-Dialogue.
Electron. Trans. Artif. Intell., 3(D):125–152.
Christopher D. Wickens, Sallie E. Gordon, and Yili
Liu. 1998. An Introduction to Human Factors En-
gineering. Addison-Wesley Educational Publishers
Inc.
</reference>
<page confidence="0.969209">
172
</page>
<figure confidence="0.998652721311476">
manager
utterance
operator
utterance
walk
turn
operator
action
forward
to your
right
omission
correction
to your
right
turn to your
right
left
stop and ..
ah too late
3’13”3’14”3’15”3’16”
(a) Omission correction
walk
turn
keep on going straight and then uh
right
forward
no no
the other way
commission
correction
2’49”2’50”2’51” 2’52”
(b) Commission correction
manager
utterance
operator
utterance
operator
action
operator
utterance
operator
action
manager
utterance
walk
turn
turn slightly right
forward
right
degree
correction
more
right
more
right
more
right
left
0’11”0’12”0’13”0’14” 0’15”0’16”
(c) Degree correction
</figure>
<figureCaption confidence="0.9780625">
Figure 3: Example omission, commission, and degree errors and corrections. The corresponding videos
can be found at http://sites.google.com/site/antoineraux/konbini.
</figureCaption>
<page confidence="0.911906">
173
</page>
<figure confidence="0.999885727272727">
P(Om)
0
2 4 6 8
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
Time since last move (s)
(a) Omissions
P(COM)
0.06
0.05
0.04
0.03
0.02
0.01
0
P(Deg)
0.06
0.05
0.04
0.03
0.02
0.01
0
2 4 6 8
2 4 6 8
Time since last move (s) Time since last move (s)
(b) Commissions (c) Degree
</figure>
<figureCaption confidence="0.999989">
Figure 4: Evolution of the probability of occurrence of corrections over time after an Operator move.
</figureCaption>
<table confidence="0.9999072">
Participant Total Non-Corr Omission Commission Degree
Total 6170 5303 (86.6%) 298 (4.8%) 277 (4.5%) 251 (4.1%)
1 338 299 (88.5%) 19 (5.6%) 15 (4.4%) 5 (1.5%)
2 249 232 (93.1%) 10 (4.0%) 2 (0.8%) 0 (0.0%)
3 440 383 (87.0%) 25 (5.7%) 11 (2.5%) 9 (2.0%)
4 265 247 (93.2%) 4 (1.5%) 8 (3.0%) 0 (0.0%)
5 313 270 (86.3%) 15 (4.8%) 5 (1.6%) 17 (5.4%)
6 238 198 (83.2%) 22 (9.2%) 13 (5.5%) 5 (2.1%)
7 426 361 (84.7%) 30 (7.0%) 10 (2.3%) 23 (5.4%)
8 244 218 (89.3%) 3 (1.2%) 13 (5.3%) 9 (3.7%)
9 162 137 (84.6%) 4 (2.5%) 13 (8.0%) 8 (4.9%)
10 229 202 (88.2%) 6 (2.6%) 3 (1.3%) 12 (5.2%)
11 380 326 (85.8%) 16 (4.2%) 19 (5.0%) 19 (5.0%)
12 427 385 (90.2%) 16 (3.7%) 11 (2.6%) 15 (3.5%)
13 327 281 (85.9%) 5 (1.5%) 14 (4.3%) 27 (8.3%)
14 516 358 (69.4%) 38 (7.4%) 79 (15.3%) 39 (7.6%)
15 362 332 (91.7%) 13 (3.6%) 6 (1.7%) 11 (3.0%)
16 392 321 (81.9%) 34 (8.7%) 27 (6.9%) 10 (2.6%)
17 362 338 (85.4%) 19 (4.8%) 22 (5.6%) 17 (4.3%)
18 466 415 (89.1%) 19 (4.1%) 6 (1.3%) 25 (5.4%)
</table>
<tableCaption confidence="0.999156">
Table 5: Frequency of the different types of corrections per participant.
</tableCaption>
<page confidence="0.997004">
174
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.311462">
<title confidence="0.999502">The Dynamics of Action Corrections in Situated Interaction</title>
<author confidence="0.958781">Antoine</author>
<affiliation confidence="0.999606">Honda Research Institute</affiliation>
<address confidence="0.942846">Mountain View, CA,</address>
<email confidence="0.999389">araux@honda-ri.com</email>
<author confidence="0.625946">Mikio</author>
<affiliation confidence="0.7998015">Honda Research Institute Wako,</affiliation>
<email confidence="0.999264">nakano@jp.honda-ri.com</email>
<abstract confidence="0.99736503125">In spoken communications, correction utterances, which are utterances correcting other participants utterances and behaviors, play crucial roles, and detecting them is one of the key issues. Previously, much work has been done on automatic detection of correction utterances in human-human and human-computer dialogs, but they mostly dealt with the correction of erroneous utterances. However, in many real situations, especially in communications between humans and mobile robots, the misunderstandings manifest themselves not only through utterances but also through physical actions performed by the participants. In this paper, we focus on action corrections and propose a classification of such utterances into Omission, Commission, and Degree corrections. We present the results of our analysis of correction utterances in dialogs between two humans who were engaging in a kind of on-line computer game, where one participant plays the role of the remote manager of a convenience store, and the other plays the role of a robot store clerk. We analyze the linguistic content, prosody as well as the timing of correction utterances and found that all features were significantly correlated with action corrections.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
<author>David Weenink</author>
</authors>
<title>Praat: doing phonetics by computer,</title>
<date>2010</date>
<location>http://www.fon.hum.uva.nl/praat.</location>
<contexts>
<context position="23424" citStr="Boersma and Weenink, 2010" startWordPosition="3809" endWordPosition="3812">ures for different correction classes. of the time elapsed since the Operator last started to move. Figure 4 shows the results. The probability of a DA being an Omission correction is relatively stable over time. This is consistent with the fact that Omission corrections are related to lack of action rather than to a specific action to which the Manager reacts. On the other hand, the probability of a Commission, and to lesser extent, Degree correction sharply decreases with time after an action. 4.2.2 Prosody We extracted F0 and intensity from all manager audio files using the Praat software (Boersma and Weenink, 2010). We then normalized pitch and intensity for each speaker using a Z-transform in order to account for individual differences in mean and standard deviation. For each DA, we computed the minimum, maximum, and mean pitch and intensity, using values from voiced frames. Table 2 shows the mean Z-score of the prosodic features for the different correction classes. Commission corrections feature higher pitch and intensity than all other classes. This is due to the fact that such corrections typically involve a higher emotional level, when the Manager is surprised or even frustrated by the behavior of</context>
</contexts>
<marker>Boersma, Weenink, 2010</marker>
<rawString>Paul Boersma and David Weenink. 2010. Praat: doing phonetics by computer, http://www.fon.hum.uva.nl/praat.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>The OSU Quake</title>
<date>2006</date>
<booktitle>In Proc. 15th Language Resource and Evaluation Conference (LREC’06).</booktitle>
<contexts>
<context position="12734" citStr="Byron and Fosler-Lussier (2006)" startWordPosition="2031" endWordPosition="2034">ired (see Okita et al. (2009) for an example of such an experimental setup). To avoid these issues, robot simulators have been used. Koulouri and Lauria (2009) developed a simulator to collect dialogs between human and simulated robot using a Wizard-of-Oz method. The human can see a map of a town and teaches the robot a route and the operator operates the robot but he/she can see only a small area around the robot in the map. However, the dialog 167 is keyboard-based, and the situation does not dynamically change in this setting, making this approach unsuitable to the study of timing aspects. Byron and Fosler-Lussier (2006) describe a corpus of spoken dialogs collected in a setting very similar to the one we are using but, again, the environment appears to be static, thus limiting the importance of the timing of actions and utterances. In this section, we describe a realistic, PC-based virtual world that we used to collect human-human situated dialogs. 3.2 Experimental Setup In our experiment, two human participants collaborate in order to perform certain tasks pertaining to the management of a small convenience store in a virtual world. The two participants sit in different rooms, both facing a computer that pr</context>
</contexts>
<marker>Byron, Fosler-Lussier, 2006</marker>
<rawString>Donna K. Byron and Eric Fosler-Lussier. 2006. The OSU Quake 2004 corpus of two-party situated problem-solving dialogs. In Proc. 15th Language Resource and Evaluation Conference (LREC’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
<author>Timothy Bickmore</author>
<author>Hannes Hgni Vilhj´amsson</author>
<author>Hao Yan</author>
</authors>
<title>More Than Just a Pretty Face: Conversational Protocols and the Affordances of Embodiment. Knowledge-Based Systems,</title>
<date>2001</date>
<pages>14--55</pages>
<marker>Cassell, Bickmore, Vilhj´amsson, Yan, 2001</marker>
<rawString>Justine Cassell, Timothy Bickmore, Hannes Hgni Vilhj´amsson, and Hao Yan. 2001. More Than Just a Pretty Face: Conversational Protocols and the Affordances of Embodiment. Knowledge-Based Systems, 14:55–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mert Cevik</author>
<author>Fuliang Weng</author>
<author>Chin hui Lee</author>
</authors>
<title>Detection of repetitions in spontaneous speech dialogue sessions.</title>
<date>2008</date>
<booktitle>In Proc. Interspeech</booktitle>
<pages>471--474</pages>
<contexts>
<context position="3073" citStr="Cevik et al., 2008" startWordPosition="476" endWordPosition="479"> in human-robot interaction is far more complex because the robot can execute physical actions and those actions can occur in parallel with utterances. In this work we are concerned specifically with corrections in situated interaction. In joint physical tasks, human corrective behavior, which allows to repair discrepancies in participants’ mutual understanding, is tightly tied to actions. While past work on non-situated spoken dialog systems has shown the necessity and feasibility of detecting and handling corrections (Kitaoka et al., 2003; Litman et al., 2006; Gieselman and Ostendorf, 2007; Cevik et al., 2008), most of these models assume that corrections target past utterances and rely on a strict turn-based structure which is frequently violated in situated interaction. When dialog is interleaved with physical actions, the specific timing of an utterance relative to other utterances and actions is more relevant than the turn sequence. In this paper, we propose a classification of errors and corrections in physical tasks and analyze the properties of different types of corrections in the context of human-human task-oriented interactions in a virtual environment. The next section gives some charact</context>
<context position="7107" citStr="Cevik et al., 2008" startWordPosition="1137" endWordPosition="1140">addressed this issue. Conversational Analysis (CA) has, from its early days, 1 concerned itself with corrections (usually called repairs in CA work) (Schegloff et al., 1977). More recently, spoken dialog systems researchers have investigated ways to automatically recognize corrections. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech recognition results of a correction and the previous user utterance to detect corrections (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections of rejection errors and found them to have different prosodic features. Rogdriguez and Schlangen (2004) and Rieser and Moore (2005) classify corrections according to their form (e.g. Repetition, Paraphrase, Addition of information...) and function. The latter aspect is mostly characterized in terms of the source of the problem that is being corrected using models of communication such as that of Clark (1996). In all of this very rich literatur</context>
</contexts>
<marker>Cevik, Weng, Lee, 2008</marker>
<rawString>Mert Cevik, Fuliang Weng, and Chin hui Lee. 2008. Detection of repetitions in spontaneous speech dialogue sessions. In Proc. Interspeech 2008, pages 471–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6374" citStr="Clark, 1996" startWordPosition="1028" endWordPosition="1029">, prosody, etc, but also on asynchronous world events. Robots and other agents that interact with people in real world situations need to be able to account for the impact of physical actions and world events on dialog. In the next section and the rest of this paper, we focus on correction utterances and how situational context affects how and when speakers produce them. 2.2 Corrections Generally speaking, a correction is an utterance which aims at signaling or remedying a misunderstanding between the participants of a conversation. In other word, corrections help (re)establish common ground (Clark, 1996). 2.2.1 Previous Work There are many dimensions along which corrections can be analyzed and many researchers have addressed this issue. Conversational Analysis (CA) has, from its early days, 1 concerned itself with corrections (usually called repairs in CA work) (Schegloff et al., 1977). More recently, spoken dialog systems researchers have investigated ways to automatically recognize corrections. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech</context>
<context position="7671" citStr="Clark (1996)" startWordPosition="1219" endWordPosition="1220">tions (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections of rejection errors and found them to have different prosodic features. Rogdriguez and Schlangen (2004) and Rieser and Moore (2005) classify corrections according to their form (e.g. Repetition, Paraphrase, Addition of information...) and function. The latter aspect is mostly characterized in terms of the source of the problem that is being corrected using models of communication such as that of Clark (1996). In all of this very rich literature, corrections are assumed to target utterances from another participant (or even oneself, in the case of self-repair) that conflict with the hearer’s expectations. While some work on embodied conversational agents (Cassell et al., 2001; Traum and Rickel, 2002) does consider physical actions as possible cues to errors and corrections, the actions are typically communicative in nature (e.g. nods, glances, gestures). Comparatively, there is extremely little work on corrections that target task actions. 166 A couple of exceptions are Traum et al. (1999), who di</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Funakoshi</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>Identifying repair targets in action control dialogue.</title>
<date>2006</date>
<booktitle>In Proc. EACL</booktitle>
<pages>177--184</pages>
<contexts>
<context position="8382" citStr="Funakoshi and Tokunaga (2006)" startWordPosition="1325" endWordPosition="1329">s from another participant (or even oneself, in the case of self-repair) that conflict with the hearer’s expectations. While some work on embodied conversational agents (Cassell et al., 2001; Traum and Rickel, 2002) does consider physical actions as possible cues to errors and corrections, the actions are typically communicative in nature (e.g. nods, glances, gestures). Comparatively, there is extremely little work on corrections that target task actions. 166 A couple of exceptions are Traum et al. (1999), who discuss the type of context representation needed to handle action corrections, and Funakoshi and Tokunaga (2006), who present a model for identifying repair targets in humanrobot command-and-control dialogs. While important, these papers focus on theoretical planning aspects of corrections whereas this paper focuses on an empirical analysis of human conversational behavior. 2.2.2 Action Corrections in Situated Interaction As seen above, the vast majority of prior work on corrections concerned corrections of previous (erroneous) utterances (i.e. utterance corrections). In contrast, in this paper, we focus exclusively on corrections that target previous physical actions (i.e. action corrections). While so</context>
</contexts>
<marker>Funakoshi, Tokunaga, 2006</marker>
<rawString>Kotaro Funakoshi and Takenobu Tokunaga. 2006. Identifying repair targets in action control dialogue. In Proc. EACL 2006, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petra Gieselman</author>
<author>Mari Ostendorf</author>
</authors>
<title>ProblemSensitive Response Generation in Human-Robot Dialogs.</title>
<date>2007</date>
<booktitle>In Proc. SIGDIAL</booktitle>
<contexts>
<context position="3052" citStr="Gieselman and Ostendorf, 2007" startWordPosition="471" endWordPosition="475">ing barge-ins, timing structure in human-robot interaction is far more complex because the robot can execute physical actions and those actions can occur in parallel with utterances. In this work we are concerned specifically with corrections in situated interaction. In joint physical tasks, human corrective behavior, which allows to repair discrepancies in participants’ mutual understanding, is tightly tied to actions. While past work on non-situated spoken dialog systems has shown the necessity and feasibility of detecting and handling corrections (Kitaoka et al., 2003; Litman et al., 2006; Gieselman and Ostendorf, 2007; Cevik et al., 2008), most of these models assume that corrections target past utterances and rely on a strict turn-based structure which is frequently violated in situated interaction. When dialog is interleaved with physical actions, the specific timing of an utterance relative to other utterances and actions is more relevant than the turn sequence. In this paper, we propose a classification of errors and corrections in physical tasks and analyze the properties of different types of corrections in the context of human-human task-oriented interactions in a virtual environment. The next secti</context>
</contexts>
<marker>Gieselman, Ostendorf, 2007</marker>
<rawString>Petra Gieselman and Mari Ostendorf. 2007. ProblemSensitive Response Generation in Human-Robot Dialogs. In Proc. SIGDIAL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norihide Kitaoka</author>
<author>Naoko Kakutani</author>
<author>Seiichi Nakagawa</author>
</authors>
<title>Detection and Recognition of Correction Utterance in Spontaneously Spoken Dialog.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech</booktitle>
<pages>625--628</pages>
<contexts>
<context position="3000" citStr="Kitaoka et al., 2003" startWordPosition="463" endWordPosition="466">timing issues such as turn-taking and handling barge-ins, timing structure in human-robot interaction is far more complex because the robot can execute physical actions and those actions can occur in parallel with utterances. In this work we are concerned specifically with corrections in situated interaction. In joint physical tasks, human corrective behavior, which allows to repair discrepancies in participants’ mutual understanding, is tightly tied to actions. While past work on non-situated spoken dialog systems has shown the necessity and feasibility of detecting and handling corrections (Kitaoka et al., 2003; Litman et al., 2006; Gieselman and Ostendorf, 2007; Cevik et al., 2008), most of these models assume that corrections target past utterances and rely on a strict turn-based structure which is frequently violated in situated interaction. When dialog is interleaved with physical actions, the specific timing of an utterance relative to other utterances and actions is more relevant than the turn sequence. In this paper, we propose a classification of errors and corrections in physical tasks and analyze the properties of different types of corrections in the context of human-human task-oriented i</context>
<context position="7086" citStr="Kitaoka et al., 2003" startWordPosition="1133" endWordPosition="1136">many researchers have addressed this issue. Conversational Analysis (CA) has, from its early days, 1 concerned itself with corrections (usually called repairs in CA work) (Schegloff et al., 1977). More recently, spoken dialog systems researchers have investigated ways to automatically recognize corrections. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech recognition results of a correction and the previous user utterance to detect corrections (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections of rejection errors and found them to have different prosodic features. Rogdriguez and Schlangen (2004) and Rieser and Moore (2005) classify corrections according to their form (e.g. Repetition, Paraphrase, Addition of information...) and function. The latter aspect is mostly characterized in terms of the source of the problem that is being corrected using models of communication such as that of Clark (1996). In all of thi</context>
</contexts>
<marker>Kitaoka, Kakutani, Nakagawa, 2003</marker>
<rawString>Norihide Kitaoka, Naoko Kakutani, and Seiichi Nakagawa. 2003. Detection and Recognition of Correction Utterance in Spontaneously Spoken Dialog. In Proc. Eurospeech 2003, pages 625–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodora Koulouri</author>
<author>Stanislao Lauria</author>
</authors>
<title>Exploring miscommunication and collaborative behaviour in human-robot interaction.</title>
<date>2009</date>
<booktitle>In Proc. SIGDIAL</booktitle>
<pages>111--119</pages>
<contexts>
<context position="12262" citStr="Koulouri and Lauria (2009)" startWordPosition="1946" endWordPosition="1949"> collect data and conduct experiments. Most such complex robots are still fragile and thus it is typically challenging to run user studies with naive subjects without severely limiting the tasks or the scope of the interaction. Another issue which comes with real world interaction is that it is difficult for the experimenter to control or monitor the events that affect the interaction. Most of the time, an expensive manual annotation of events and actions is required (see Okita et al. (2009) for an example of such an experimental setup). To avoid these issues, robot simulators have been used. Koulouri and Lauria (2009) developed a simulator to collect dialogs between human and simulated robot using a Wizard-of-Oz method. The human can see a map of a town and teaches the robot a route and the operator operates the robot but he/she can see only a small area around the robot in the map. However, the dialog 167 is keyboard-based, and the situation does not dynamically change in this setting, making this approach unsuitable to the study of timing aspects. Byron and Fosler-Lussier (2006) describe a corpus of spoken dialogs collected in a setting very similar to the one we are using but, again, the environment app</context>
</contexts>
<marker>Koulouri, Lauria, 2009</marker>
<rawString>Theodora Koulouri and Stanislao Lauria. 2009. Exploring miscommunication and collaborative behaviour in human-robot interaction. In Proc. SIGDIAL 2009, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
</authors>
<title>Characterizing and recognizing spoken corrections in human-computer dialogue.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL ’98,</booktitle>
<pages>736--742</pages>
<contexts>
<context position="7191" citStr="Levow (1998)" startWordPosition="1149" endWordPosition="1150">self with corrections (usually called repairs in CA work) (Schegloff et al., 1977). More recently, spoken dialog systems researchers have investigated ways to automatically recognize corrections. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech recognition results of a correction and the previous user utterance to detect corrections (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections of rejection errors and found them to have different prosodic features. Rogdriguez and Schlangen (2004) and Rieser and Moore (2005) classify corrections according to their form (e.g. Repetition, Paraphrase, Addition of information...) and function. The latter aspect is mostly characterized in terms of the source of the problem that is being corrected using models of communication such as that of Clark (1996). In all of this very rich literature, corrections are assumed to target utterances from another participant (or even on</context>
<context position="10065" citStr="Levow (1998)" startWordPosition="1588" endWordPosition="1589">on errors occur when Bob fails to react to one of Alice’s utterances. A typical way for Alice to correct an omission error is to repeat the utterance to which Bob did not react. Degree errors occur when Bob reacts with an appropriate action to Alice’s utterance but fails to completely fulfill Alice’s goal. This is illustrated by Alice’s use of ”More” in turn 7 in response to Bob’s insufficient action 6. Figure 3 illustrates the three error categories based on extracts from the corpus. In some ways, the dichotomy Commission errors/Omission errors parallels that of Misrecognitions/Rejections by Levow (1998). This type of classification is also commonly used to analyze human errors in human factors research (Wickens et al., 1998). In addition to these two categories, we added the Degree category based on our observation of the data we collected. This aspect is somewhat specific to certain kinds of physical actions (those that can be performed to different degrees, as opposed to binary actions such as “opening the door”). However, it seems general enough to be applied to many collaborative tasks relevant to robots such as guidance, tele-operation, and joint construction. For an automated agent, be</context>
</contexts>
<marker>Levow, 1998</marker>
<rawString>Gina-Anne Levow. 1998. Characterizing and recognizing spoken corrections in human-computer dialogue. In Proc. COLING-ACL ’98, pages 736–742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Julia Hirschberg</author>
<author>Marc Swerts</author>
</authors>
<title>Characterizing and predicting corrections in spoken dialogue systems.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="3021" citStr="Litman et al., 2006" startWordPosition="467" endWordPosition="470">turn-taking and handling barge-ins, timing structure in human-robot interaction is far more complex because the robot can execute physical actions and those actions can occur in parallel with utterances. In this work we are concerned specifically with corrections in situated interaction. In joint physical tasks, human corrective behavior, which allows to repair discrepancies in participants’ mutual understanding, is tightly tied to actions. While past work on non-situated spoken dialog systems has shown the necessity and feasibility of detecting and handling corrections (Kitaoka et al., 2003; Litman et al., 2006; Gieselman and Ostendorf, 2007; Cevik et al., 2008), most of these models assume that corrections target past utterances and rely on a strict turn-based structure which is frequently violated in situated interaction. When dialog is interleaved with physical actions, the specific timing of an utterance relative to other utterances and actions is more relevant than the turn sequence. In this paper, we propose a classification of errors and corrections in physical tasks and analyze the properties of different types of corrections in the context of human-human task-oriented interactions in a virt</context>
<context position="6809" citStr="Litman et al. (2006)" startWordPosition="1091" endWordPosition="1094">n utterance which aims at signaling or remedying a misunderstanding between the participants of a conversation. In other word, corrections help (re)establish common ground (Clark, 1996). 2.2.1 Previous Work There are many dimensions along which corrections can be analyzed and many researchers have addressed this issue. Conversational Analysis (CA) has, from its early days, 1 concerned itself with corrections (usually called repairs in CA work) (Schegloff et al., 1977). More recently, spoken dialog systems researchers have investigated ways to automatically recognize corrections. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech recognition results of a correction and the previous user utterance to detect corrections (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections of rejection errors and found them to have different prosodic features. Rogdriguez and Schlangen (2004) and Rieser and Moore (2005) classify correcti</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2006</marker>
<rawString>Diane Litman, Julia Hirschberg, and Marc Swerts. 2006. Characterizing and predicting corrections in spoken dialogue systems. Computational Linguistics, 32(3):417–438.</rawString>
</citation>
<citation valid="true">
<title>The Mumble/Murmur Project.</title>
<date>2010</date>
<note>http://mumble.sourceforge.net.</note>
<marker>2010</marker>
<rawString>The Mumble/Murmur Project. 2010. http://mumble.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Y Okita</author>
<author>Victor Ng-Thow-Hing</author>
<author>Ravi K Sarvadevabhatla</author>
</authors>
<title>Learning Together: ASIMO Developing an Interactive Learning Partnership with Children. In</title>
<date>2009</date>
<booktitle>Proc. RO-MAN</booktitle>
<contexts>
<context position="12132" citStr="Okita et al. (2009)" startWordPosition="1924" endWordPosition="1927">h One obstacle to the empirical study of situated interaction is that it requires a fully functional sophisticated robot to collect data and conduct experiments. Most such complex robots are still fragile and thus it is typically challenging to run user studies with naive subjects without severely limiting the tasks or the scope of the interaction. Another issue which comes with real world interaction is that it is difficult for the experimenter to control or monitor the events that affect the interaction. Most of the time, an expensive manual annotation of events and actions is required (see Okita et al. (2009) for an example of such an experimental setup). To avoid these issues, robot simulators have been used. Koulouri and Lauria (2009) developed a simulator to collect dialogs between human and simulated robot using a Wizard-of-Oz method. The human can see a map of a town and teaches the robot a route and the operator operates the robot but he/she can see only a small area around the robot in the map. However, the dialog 167 is keyboard-based, and the situation does not dynamically change in this setting, making this approach unsuitable to the study of timing aspects. Byron and Fosler-Lussier (200</context>
</contexts>
<marker>Okita, Ng-Thow-Hing, Sarvadevabhatla, 2009</marker>
<rawString>Sandra Y Okita, Victor Ng-Thow-Hing, and Ravi K Sarvadevabhatla. 2009. Learning Together: ASIMO Developing an Interactive Learning Partnership with Children. In Proc. RO-MAN 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>OGRE Open</author>
</authors>
<title>Source 3D Graphics Engine.</title>
<date>2010</date>
<note>http://www.ogre3d.org.</note>
<marker>Open, 2010</marker>
<rawString>OGRE Open Source 3D Graphics Engine. 2010. http://www.ogre3d.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Johanna Moore</author>
</authors>
<title>Implications for generating clarification requests in task-oriented dialogues.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>239--246</pages>
<contexts>
<context position="7391" citStr="Rieser and Moore (2005)" startWordPosition="1175" endWordPosition="1178">tions. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech recognition results of a correction and the previous user utterance to detect corrections (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections of rejection errors and found them to have different prosodic features. Rogdriguez and Schlangen (2004) and Rieser and Moore (2005) classify corrections according to their form (e.g. Repetition, Paraphrase, Addition of information...) and function. The latter aspect is mostly characterized in terms of the source of the problem that is being corrected using models of communication such as that of Clark (1996). In all of this very rich literature, corrections are assumed to target utterances from another participant (or even oneself, in the case of self-repair) that conflict with the hearer’s expectations. While some work on embodied conversational agents (Cassell et al., 2001; Traum and Rickel, 2002) does consider physical</context>
</contexts>
<marker>Rieser, Moore, 2005</marker>
<rawString>Verena Rieser and Johanna Moore. 2005. Implications for generating clarification requests in task-oriented dialogues. In Proc. 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kepa Josepa Rogdriguez</author>
<author>David Schlangen</author>
</authors>
<title>Form, intonation and function of clarification requests in german task-oriented spoken dialogues.</title>
<date>2004</date>
<booktitle>In Proc. 8th Workshop on the Semantics and Pragmatics of Dialogue (CATALOG’04).</booktitle>
<contexts>
<context position="7363" citStr="Rogdriguez and Schlangen (2004)" startWordPosition="1169" endWordPosition="1173">ys to automatically recognize corrections. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech recognition results of a correction and the previous user utterance to detect corrections (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections of rejection errors and found them to have different prosodic features. Rogdriguez and Schlangen (2004) and Rieser and Moore (2005) classify corrections according to their form (e.g. Repetition, Paraphrase, Addition of information...) and function. The latter aspect is mostly characterized in terms of the source of the problem that is being corrected using models of communication such as that of Clark (1996). In all of this very rich literature, corrections are assumed to target utterances from another participant (or even oneself, in the case of self-repair) that conflict with the hearer’s expectations. While some work on embodied conversational agents (Cassell et al., 2001; Traum and Rickel, </context>
</contexts>
<marker>Rogdriguez, Schlangen, 2004</marker>
<rawString>Kepa Josepa Rogdriguez and David Schlangen. 2004. Form, intonation and function of clarification requests in german task-oriented spoken dialogues. In Proc. 8th Workshop on the Semantics and Pragmatics of Dialogue (CATALOG’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
<author>Harvey Sacks</author>
</authors>
<title>The preference for self-correction in the organization of repair in conversation.</title>
<date>1977</date>
<journal>Language,</journal>
<volume>53</volume>
<issue>2</issue>
<contexts>
<context position="6661" citStr="Schegloff et al., 1977" startWordPosition="1071" endWordPosition="1074"> on correction utterances and how situational context affects how and when speakers produce them. 2.2 Corrections Generally speaking, a correction is an utterance which aims at signaling or remedying a misunderstanding between the participants of a conversation. In other word, corrections help (re)establish common ground (Clark, 1996). 2.2.1 Previous Work There are many dimensions along which corrections can be analyzed and many researchers have addressed this issue. Conversational Analysis (CA) has, from its early days, 1 concerned itself with corrections (usually called repairs in CA work) (Schegloff et al., 1977). More recently, spoken dialog systems researchers have investigated ways to automatically recognize corrections. For instance, Litman et al. (2006) exploited features to automatically detect correction utterances. In addition, several attempts have been made to exploit the similarity in speech sounds and speech recognition results of a correction and the previous user utterance to detect corrections (Kitaoka et al., 2003; Cevik et al., 2008). Going beyond a binary correction/noncorrection classification scheme, Levow (1998) distinguished corrections of misrecognition errors from corrections o</context>
</contexts>
<marker>Schegloff, Jefferson, Sacks, 1977</marker>
<rawString>Emanuel A. Schegloff, Gail Jefferson, and Harvey Sacks. 1977. The preference for self-correction in the organization of repair in conversation. Language, 53(2):361–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Jeff Rickel</author>
</authors>
<title>Embodied agents for multiparty dialogue in immersive virtual worlds.</title>
<date>2002</date>
<booktitle>In Proc. International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS</booktitle>
<pages>766--773</pages>
<contexts>
<context position="7968" citStr="Traum and Rickel, 2002" startWordPosition="1263" endWordPosition="1266">d Schlangen (2004) and Rieser and Moore (2005) classify corrections according to their form (e.g. Repetition, Paraphrase, Addition of information...) and function. The latter aspect is mostly characterized in terms of the source of the problem that is being corrected using models of communication such as that of Clark (1996). In all of this very rich literature, corrections are assumed to target utterances from another participant (or even oneself, in the case of self-repair) that conflict with the hearer’s expectations. While some work on embodied conversational agents (Cassell et al., 2001; Traum and Rickel, 2002) does consider physical actions as possible cues to errors and corrections, the actions are typically communicative in nature (e.g. nods, glances, gestures). Comparatively, there is extremely little work on corrections that target task actions. 166 A couple of exceptions are Traum et al. (1999), who discuss the type of context representation needed to handle action corrections, and Funakoshi and Tokunaga (2006), who present a model for identifying repair targets in humanrobot command-and-control dialogs. While important, these papers focus on theoretical planning aspects of corrections whereas</context>
</contexts>
<marker>Traum, Rickel, 2002</marker>
<rawString>David Traum and Jeff Rickel. 2002. Embodied agents for multiparty dialogue in immersive virtual worlds. In Proc. International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS 2002), pages 766–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Carl F Andersen</author>
<author>Waiyian Chong</author>
<author>Darsana P Josyula</author>
<author>Yoshi Okamoto</author>
</authors>
<title>Khemdut Purang, Michael O’Donovan-Anderson, and Donald Perlis.</title>
<date>1999</date>
<booktitle>Representations of Dialogue State for Domain and Task Independent Meta-Dialogue. Electron. Trans. Artif. Intell.,</booktitle>
<pages>3--125</pages>
<contexts>
<context position="8263" citStr="Traum et al. (1999)" startWordPosition="1308" endWordPosition="1311">uch as that of Clark (1996). In all of this very rich literature, corrections are assumed to target utterances from another participant (or even oneself, in the case of self-repair) that conflict with the hearer’s expectations. While some work on embodied conversational agents (Cassell et al., 2001; Traum and Rickel, 2002) does consider physical actions as possible cues to errors and corrections, the actions are typically communicative in nature (e.g. nods, glances, gestures). Comparatively, there is extremely little work on corrections that target task actions. 166 A couple of exceptions are Traum et al. (1999), who discuss the type of context representation needed to handle action corrections, and Funakoshi and Tokunaga (2006), who present a model for identifying repair targets in humanrobot command-and-control dialogs. While important, these papers focus on theoretical planning aspects of corrections whereas this paper focuses on an empirical analysis of human conversational behavior. 2.2.2 Action Corrections in Situated Interaction As seen above, the vast majority of prior work on corrections concerned corrections of previous (erroneous) utterances (i.e. utterance corrections). In contrast, in th</context>
</contexts>
<marker>Traum, Andersen, Chong, Josyula, Okamoto, 1999</marker>
<rawString>David R. Traum, Carl F. Andersen, Waiyian Chong, Darsana P. Josyula, Yoshi Okamoto, Khemdut Purang, Michael O’Donovan-Anderson, and Donald Perlis. 1999. Representations of Dialogue State for Domain and Task Independent Meta-Dialogue. Electron. Trans. Artif. Intell., 3(D):125–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Wickens</author>
<author>Sallie E Gordon</author>
<author>Yili Liu</author>
</authors>
<title>An Introduction to Human Factors Engineering.</title>
<date>1998</date>
<publisher>Addison-Wesley Educational Publishers Inc.</publisher>
<contexts>
<context position="10189" citStr="Wickens et al., 1998" startWordPosition="1606" endWordPosition="1610">n error is to repeat the utterance to which Bob did not react. Degree errors occur when Bob reacts with an appropriate action to Alice’s utterance but fails to completely fulfill Alice’s goal. This is illustrated by Alice’s use of ”More” in turn 7 in response to Bob’s insufficient action 6. Figure 3 illustrates the three error categories based on extracts from the corpus. In some ways, the dichotomy Commission errors/Omission errors parallels that of Misrecognitions/Rejections by Levow (1998). This type of classification is also commonly used to analyze human errors in human factors research (Wickens et al., 1998). In addition to these two categories, we added the Degree category based on our observation of the data we collected. This aspect is somewhat specific to certain kinds of physical actions (those that can be performed to different degrees, as opposed to binary actions such as “opening the door”). However, it seems general enough to be applied to many collaborative tasks relevant to robots such as guidance, tele-operation, and joint construction. For an automated agent, being able to classify a user utterance into one of these four categories (including non-action-correction utterances) could b</context>
</contexts>
<marker>Wickens, Gordon, Liu, 1998</marker>
<rawString>Christopher D. Wickens, Sallie E. Gordon, and Yili Liu. 1998. An Introduction to Human Factors Engineering. Addison-Wesley Educational Publishers Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>