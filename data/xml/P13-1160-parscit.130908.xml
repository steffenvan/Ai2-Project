<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011308">
<title confidence="0.999551">
A Bayesian Model for Joint Unsupervised Induction
of Sentiment, Aspect and Discourse Representations
</title>
<author confidence="0.991906">
Angeliki Lazaridou
</author>
<affiliation confidence="0.997143">
University of Trento
</affiliation>
<email confidence="0.970517">
angeliki.lazaridou@unitn.it
</email>
<author confidence="0.991884">
Ivan Titov
</author>
<affiliation confidence="0.992973">
Saarland University
</affiliation>
<email confidence="0.963746">
titov@mmci.uni-saarland.de
</email>
<author confidence="0.993829">
Caroline Sporleder
</author>
<affiliation confidence="0.987659">
Trier University
</affiliation>
<email confidence="0.996792">
csporled@coli.uni-sb.de
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874428571429">
We propose a joint model for unsuper-
vised induction of sentiment, aspect and
discourse information and show that by in-
corporating a notion of latent discourse re-
lations in the model, we improve the pre-
diction accuracy for aspect and sentiment
polarity on the sub-sentential level. We
deviate from the traditional view of dis-
course, as we induce types of discourse re-
lations and associated discourse cues rel-
evant to the considered opinion analysis
task; consequently, the induced discourse
relations play the role of opinion and as-
pect shifters. The quantitative analysis that
we conducted indicated that the integra-
tion of a discourse model increased the
prediction accuracy results with respect to
the discourse-agnostic approach and the
qualitative analysis suggests that the in-
duced representations encode a meaning-
ful discourse structure.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987770897959184">
With the rapid growth of the Web, it is becoming
increasingly difficult to discern useful from irrel-
evant information, particularly in user-generated
content, such as product reviews. To make it easier
for the reader to separate the wheat from the chaff,
it is necessary to structure the available informa-
tion. In the review domain, this is done in aspect-
based sentiment analysis which aims at identify-
ing text fragments in which opinions are expressed
about ratable aspects of products, such as ‘room
quality’ or ‘service quality’. Such fine-grained
analysis can serve as the first step in aspect-based
sentiment summarization (Hu and Liu, 2004), a
task with many practical applications.
Aspect-based summarization is an active re-
search area for which various techniques have
been developed, both statistical (Mei et al., 2007;
Titov and McDonald, 2008b) and not (Hu and Liu,
2004), and relying on different types of supervi-
sion sources, such as sentiment-annotated texts or
polarity lexica (Turney and Littman, 2002). Most
methods rely on local information (bag-of-words,
short ngrams or elementary syntactic fragments)
and do not attempt to account for more complex
interactions. However, these local lexical repre-
sentations by themselves are often not sufficient to
infer a sentiment or aspect for a fragment of text.
For instance, in the following example taken from
a TripAdvisor1 review:
Example 1. The room was nice but let’s not talk
about the view.
it is difficult to deduce on the basis of local lexical
features alone that the opinion about the view is
negative. The clause let’s not talk about the view
could by itself be neutral or even positive given the
right context (e.g., I’ve never seen such a fancy ho-
tel room, my living room doesn’t look that cool...
and let’s not talk about the view). However, the
contrast relation signaled by the connective but
makes it clear that the second clause has a nega-
tive polarity. The same observations can be made
about transitions between aspects: changes in as-
pect are often clearly marked by discourse connec-
tives. Importantly, some of these cues are not dis-
course connectives in the strict linguistic sense and
are specific to the review domain (e.g., the phrase
I would also in a review indicates that the topic
is likely to be changed). In order to accurately
predict sentiment and topic,2 a model needs to ac-
</bodyText>
<footnote confidence="0.843965">
1http://www.tripadvisor.com/
1630 2In what follows, we use the terms aspect and topic, inter-
</footnote>
<note confidence="0.9719005">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1630–1639,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999758957142858">
count for these discourse phenomena and cannot
rely solely on local lexical information.
These issues have not gone unnoticed to the re-
search community. Consequently, there has re-
cently been an increased interest in models that
leverage content and discourse structure in senti-
ment analysis tasks. However, discourse-level in-
formation is typically incorporated in a pipeline
architecture, either in the form of sentiment po-
larity shifters (Polanyi and Zaenen, 2006; Naka-
gawa et al., 2010) that operate on the lexical level
or by using discourse relations (Taboada et al.,
2008; Zhou et al., 2011) that comply with dis-
course theories like Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988). Such ap-
proaches have a number of disadvantages. First,
they require additional resources, such as lists of
polarity shifters or discourse connectives which
signal specific relations. These resources are avail-
able only for a handful of languages. Second, re-
lying on a generic discourse analysis step that is
carried out before sentiment analysis may intro-
duce additional noise and lead to error propaga-
tion. Furthermore, these techniques will not nec-
essarily be able to induce discourse relations in-
formative for the sentiment analysis domain (Voll
and Taboada, 2007).
An alternative approach is to define a task-
specific scheme of discourse relations (Somasun-
daran et al., 2009). This previous work showed
that task-specific discourse relations are helpful in
predicting sentiment, however, in doing so they re-
lied on gold-standard discourse annotation at test
time rather than predicting it automatically or in-
ducing it jointly with sentiment polarity.
We take a different approach and induce dis-
course and sentiment information jointly in an un-
supervised (or weakly supervised) manner. This
has the advantage of not having to pre-specify a
mapping from discourse cues to discourse rela-
tions; our model induces this automatically, which
makes it portable to new domains and languages.
Joint induction of discourse and sentiment struc-
ture also has the added benefit that the model is
able to learn exactly those aspects of discourse
structure that are relevant for sentiment analysis.
We start with a relatively standard joint model
of sentiment and topic, which can be regarded as a
cross-breed between the JST model (Lin and He,
2009) and the ASUM model (Jo and Oh, 2011),
changeably as well as sentiment levels and opinion polarity.
both state-of-the-art techniques. This model is
weakly supervised, as it relies solely on document-
level (i.e. not aspect-specific) opinion polarity la-
bels to induce topics and sentiment on the sub-
sentential level. In order to test our hypothesis
that discourse information is beneficial, we add
a discourse modeling component. Note that in
modeling discourse we do not exploit any kind
of supervision. We demonstrate that the resulting
model outperforms the baseline on a product re-
view dataset (see Section 5).
To the best of our knowledge, unsupervised
joint induction of discourse structure, sentiment
and topic information has not been considered
before, particularly not in the context of the
aspect-based sentiment analysis task. Importantly,
our method for discourse modeling is a general
method which can be integrated in virtually any
LDA-style model of aspect and sentiment.
</bodyText>
<sectionHeader confidence="0.951309" genericHeader="method">
2 Modeling Discourse Structure
</sectionHeader>
<bodyText confidence="0.999815384615385">
Discourse cues typically do not directly indicate
sentiment polarity (or aspect). However, they can
indicate how polarity (or aspect) changes as the
text unfolds. As we have seen in the examples
above, changes in polarity can happen on a sub-
sentential level, i.e., between adjacent clauses or,
from a discourse-theoretic point of view, between
adjacent elementary discourse units (EDUs). To
model these changes we need a strong linguistic
signal, for example, in the form of discourse con-
nectives or other discourse cues. We hypothesize
that these are more likely to occur at the beginning
of an EDU than in the middle or at the end. This is
certainly true for most of the traditional discourse
relation cues (particularly connectives).
Changes in polarity or aspect are often cor-
related with specific discourse relations, such as
‘contrast’. However, not all relations are rele-
vant and there is no one-to-one correspondence
between relations and sentiment changes.3 Fur-
thermore, if a discourse relation signals a change,
it is typically ambiguous whether this change oc-
curs with the polarity (example 1) or the aspect
(the room was nice but the breakfast was even bet-
ter) or both (the room was nice but the breakfast
was awful). Therefore, we do not explicitly model
</bodyText>
<footnote confidence="0.9935236">
3The ‘explanation’ relation, for example, can occur with
a polarity change (We were upgraded to a really nice room
because the hotel made a terrible blunder with our booking)
but does not have to (The room was really nice because the
hotel was newly renovated).
</footnote>
<page confidence="0.950909">
1631
</page>
<table confidence="0.99521975">
Name Description
AltSame different polarity, same aspect
SameAlt same polarity, different aspect
AltAlt different polarity and aspect
</table>
<tableCaption confidence="0.998271">
Table 1: Discourse relations
</tableCaption>
<bodyText confidence="0.999943466666667">
generic discourse relations; instead, inspired by
the work of Somasundaran et al. (2008), we define
three very general relations which encode how po-
larity and aspect change (Table 1). Note that we
do not have a discourse relation SameSame since
we do not expect to have strong linguistic evidence
which states that an EDU contains the same senti-
ment information as the previous one.4 However,
we assume that the sentiment and topic flow is
fairly smooth in general. In other words, for two
adjacent EDUs not connected by any of the above
three relations, the prior probability of staying at
the same topic and sentiment level is higher than
picking a new topic and sentiment level (i.e. we
use “sticky states” (Fox et al., 2008)).
</bodyText>
<sectionHeader confidence="0.993082" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999966">
In this section we describe our Bayesian model,
first the discourse-agnostic model and then an ex-
tension needed to encode discourse information.
The formal generative story is presented in Fig-
ure 1: the red fragments correspond to the dis-
course modeling component. In order to obtain the
generative story for the discourse-agnostic model,
they simply need to be ignored.
</bodyText>
<subsectionHeader confidence="0.937489">
3.1 Discourse-agnostic model
</subsectionHeader>
<bodyText confidence="0.99999275">
In our approach we make an assumption that all
the words in an EDU correspond to the same topic
and sentiment level. We also assume that an over-
all sentiment of the document is defined, this is
the only supervision we use in inducing the model.
Unlike some of the previous work (e.g., (Titov and
McDonald, 2008a)), we do not constrain aspect-
specific sentiment to be the same across the docu-
ment. We describe our discourse-agnostic model
by first describing the set of corpus-level and
document-level parameters, and then explain how
the content of each document is generated.
Drawing model parameters On the corpus
level, for every topic z ∈ {1, ... , K} and ev-
ery sentiment polarity level y ∈ {−1,0,+1},
we start by drawing a unigram language model
</bodyText>
<footnote confidence="0.97905">
4The typical connective in this situation would be and
which is highly ambiguous and can signal several traditional
discourse relations.
</footnote>
<bodyText confidence="0.999965784313726">
from a Dirichlet prior. For example, the language
model of the aspect service may indicate that the
word friendly is used to express a positive opinion,
whereas the word rude expresses a negative one.
Similarly, for every topic z and every over-
all sentiment polarity ˆy, we draw a distribution
ψ9,z over opinion polarity in this topic z. Intu-
itively, one would expect the sentiment of an as-
pect to more often agree with the overall sentiment
yˆ than not. This intuition is encoded in an asym-
metric Dirichlet prior Dir(-yy) for ψ9,z : -yy =
(γy,1, . . . ,γy,M), γy,y = β + τδy,g, where δy,y is
the Kronecker symbol, β and τ are nonnegative
scalar parameters. Using these “heavy-diagonal”
priors is crucial, as this is the way to ensure that
the overall sentiment level is tied to the aspect-
specific sentiment level. Otherwise, sentiment lev-
els will be specific to individual aspects (e.g., the
”+1” sentiment for one topic may correspond to
a ”-1” sentiment for another one). Without this
property we would not be able to encode soft con-
straints imposed by the discourse relations.
Drawing documents On the document level, as
in the standard LDA model, we choose the distri-
bution over topics for the document from a sym-
metric Dirichlet prior parametrized by α, which is
used to control sparsity of topic assignments. Fur-
thermore, we draw the global sentiment ˆyd from a
uniform distribution.
The generation of a document is done on the
EDU-by-EDU basis. In this work, we assume
that EDU segmentation is provided by the prepro-
cessing step. First, we generate the aspect zd,s
for EDU s according to the distribution of top-
ics θd. Then, we choose a sentiment level yd,s
for the considered EDU from the categorical dis-
tribution ψ0d,zd,s, conditioned on the aspect zd,s,
as well as on the global sentiment of the document
ˆyd. Finally, we generate the bag of words for the
EDU by drawing the words from the aspect- and
sentiment-specific language model.
This model can be seen as a variant of a state-of-
the-art model for jointly inducing sentiment and
aspect at the sentence level (Jo and Oh, 2011), or,
more precisely, as its combination with the JST
model (Lin and He, 2009), adapted to the specifics
of our setting. Both these models have been shown
to perform well on sentiment and topic prediction
tasks, outperforming earlier models, such as the
TSM model (Mei et al., 2007). Consequently, it
can be considered as a strong baseline.
</bodyText>
<page confidence="0.944133">
1632
</page>
<subsectionHeader confidence="0.915221">
3.2 Discourse-informed model
</subsectionHeader>
<bodyText confidence="0.9999354">
In order to integrate discourse information into the
discourse-agnostic model, we need to define a set
of extra parameters and random variables.
Drawing model parameters First, at the corpus
level, we draw a distribution co over four discourse
relations: three relations as defined in Table 1 and
an additional dummy relation 4 to indicate that
there is no relation between two adjacent EDUs
(NoRelation). This distribution is drawn from an
asymmetric Dirichlet prior parametrized by a vec-
tor of hyperparameters ν. These parameters en-
code the intuition that most pairs of EDUs do not
exhibit a discourse relation relevant for the task
(i.e. favor NoRelation), that is v4 has a distinct
and larger value than other parameters v¯4.
Every discourse relation c (including
NoRelation which is treated here as Same-
Same) is associated with two groups of transition
distributions, one governing transitions of sen-
timent (˜ c) and another one controlling topic
transitions ( ˜Bc). The parameter˜ c,y3, defines a
distribution over sentiment polarity for the EDU
s + 1 given the sentiment for the sth EDU ys and
the discourse relation c. This distribution encodes
our beliefs about sentiment transitions between
EDUs s and s + 1 related through c. For example,
the distribution ˜ SameAlt,+1 would assign higher
probability mass to the positive sentiment polarity
(+1) than to the other 2 sentiment levels (0,
-1). Similarly, the parameter ˜Bc,z3, defines a
distribution over K aspects.
These two families of transition distributions
are each defined in the following way. For the dis-
tribution ˜B, for relations that favor changing the
aspect (SameAlt and AltAlt), the probability of the
preferred (K-1) transitions is proportional to !θ
and for the remaining transitions it is proportional
to 1. On the other hand, for the relations that fa-
vor keeping the same aspect (NoRelation and Alt-
Same), the probability of the preferred transition is
proportional to !&apos;θ, whereas the probability of the
(K-1) remaining transitions is again proportional
to 1. For the sentiment transitions, the distribution
˜ c,y3 is defined in the analogous way (but depends
on !ψ and !&apos;ψ). These scalars are hand-coded and
define soft constraints that discourse relations im-
pose on the local flow of sentiment and aspects.
The parameter ˜~c is a language model over dis-
course cues ˜w, which are not restricted to uni-
grams but can generate phrases of arbitrary (and
variable) size. For this reason, we draw them
from a Dirichlet process (DP) (i.e. one for each
discourse relation, except for NoRelation). The
base measure G0 provides the probability of an n-
word sequence calculated with the bigram prob-
ability model estimated from the corpus.5 This
model component bears strong similarities to the
Bayesian model of word segmentation (Goldwa-
ter et al., 2009), though we use the DP process
to generate only the prefix of the EDU, whereas
the rest of the EDU is generated from the bag-of-
words model.
Drawing documents As pointed out above, the
content generation is broken into two steps, where
first we draw the discourse cue ˜wd,s from ˜~c and
then we generate the remaining words.
The second difference at the data generation
step (Figure 1) is in the way the aspect and sen-
timent labels are drawn. As the discourse rela-
tion between the EDUs has already been chosen,
we have some expectations about the values of the
sentiment and aspect of the following EDU, which
are encoded by the distributions ˜ and ˜B. These
are only soft constraints that have to be taken into
consideration along with the information provided
by the aspect-sentiment model. This coupling of
information naturally translates into the product-
of-experts (PoE) (Hinton, 1999) approach, where
two sources of information jointly contribute to
the final result. The PoE model seems to be more
appropriate here than a mixture model, as we do
not want the discourse transition to overpower the
sentiment-topic model. In the PoE model, in or-
der for an outcome to be chosen, it needs to have
a non-negligible probability under both models.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999408727272727">
Since exact inference of our model is intractable,
we use collapsed Gibbs sampling. The variables
that need to be inferred are the topic assignments
z, the sentiment assignments y, the discourse re-
lations c and the discourse cue w˜ (or, more pre-
cisely, its length) and are all sampled jointly (for
each EDU) since we expect them to be highly de-
pendent. All other variables (i.e. unknown dis-
tributions) could be marginalized out to obtain a
collapsed Gibbs sampler (Griffiths and Steyvers,
2004).
</bodyText>
<footnote confidence="0.993723">
5This measure is improper but it serves the purpose of
favoring long cues, the behavior arguably desirable for our
application.
</footnote>
<page confidence="0.677348">
1633
</page>
<table confidence="0.950587869565217">
Global parameters:
ϕ˜ ∼ Dir(ν) [distrib of disc rel]
for each discourse relation c = 1, .., 4:
˜φc ∼ DP(η, Go) [distrib of disc rel specific disc cues]
˜θc,k -fixed [distrib of rel specific aspect transitions]
˜φc,y - fixed [distrib of rel specific sent transitions]
for each aspect k = 1, 2...K:
for each sentiment y = −1, 0, +1:
φk,y ∼ Dir(λk) [unigram language models]
for each global sentiment yˆ = −1, 0, +1:
ψˆy,k ∼ Dir(γ) [sent distrib given overall sentiment]
Data Generation:
for each document d:
ˆyd ∼ Unif(−1, 0, +1) [global sentiment]
Bd ∼ Dir(α) [distr over aspects]
for every EDU s:
cd,s ∼ ϕ˜ [draw disc relation]
if cd,s =6 NoRelation
˜wd,s ∼ ˜φcd,e [draw disc cue]
zd,s ∼ Bd ∗ ˜θcd,e, zd,e−1 [draw aspect]
yd,s ∼ ψˆyd,zd,s∗ ˜ψcd,e,yd,e−1 [draw sentiment level]
for each word after disc cue:
wd,s ∼ φzd ,s,yd,s [draw words]
</table>
<figureCaption confidence="0.993464">
Figure 1: The generative story for the joint model.
</figureCaption>
<bodyText confidence="0.992215076923077">
The components responsible for modeling dis-
course information are emphasized in red: when
dropped, one is left with the discourse-agnostic
model.
Unfortunately, the use of the PoE model pre-
vents us from marginalizing the parameters ex-
actly. Instead, as in Naseem et al. (2009), we re-
sort to an approximation. We assume that zd,s and
yd,s are drawn twice; once from the document spe-
cific distribution and once from the discourse tran-
sition distributions. Under this simplification, we
can easily derive the conditional probabilities for
the collapsed Gibbs sampling.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999008846153846">
To the best of our knowledge, this is the first work
that aims at evaluating directly the joint informa-
tion of the sentiment and aspect assignment at the
sub-sentential level of full reviews; most existing
studies either focus on indirect evaluation of the
produced models (e.g., classifying the overall sen-
timent of sentences (Titov and McDonald, 2008a;
Brody and Elhadad, 2010) or even reviews (Naka-
gawa et al., 2010; Jo and Oh, 2011)) or evaluated
solely at the sentential or even document level.
Consequently, in order to evaluate our methods,
we created a new dataset which will be publicly
released.
</bodyText>
<table confidence="0.999642666666666">
Aspects Frequency
service 246
value 55
location 121
rooms 316
sleep quality 56
cleanliness 59
amenities 180
food 81
recommendation 121
rest 306
Total 1541
</table>
<tableCaption confidence="0.997012">
Table 2: Distribution of aspects in the data.
</tableCaption>
<bodyText confidence="0.998938257142857">
Dataset and Annotation The dataset we created
consists of 13559 hotel reviews from TripAdvi-
sor.com.6 Since our modeling is performed on the
EDU level, all sentences where segmented using
the SLSEG software package.7 As a result, our
dataset consists of 322,935 EDUs.
For creating the gold standard, 9 annotators an-
notated a random subset of our dataset (65 re-
views, 1541 EDUs). The annotators were pre-
sented with the whole review partitioned in EDUs
and were asked to annotate every EDU with the
aspect and sentiment (i.e. +1, 0 or −1) it ex-
presses. Table 2 presents the distribution of as-
pects in the dataset. The distribution of the sen-
timents is uniform. The label rest captures cases
where EDUs do not refer to any aspect or to a very
rare aspect. The inter-annotator agreement (IAA),
as measured in terms of Cohen’s kappa score, was
66% for the aspect labeling, 70% for the sentiment
annotation and 61% for the joint task of sentiment
and aspect annotation. Though these scores may
not seem very high, they are similar to the ones re-
ported in related sentiment annotation efforts (see
e.g., Ganu et al. (2009)).
Experimental setup In order to quantitatively
evaluate the model predictions, we run two sets of
experiments. In the first, we treat the task as an un-
supervised classification problem and evaluate the
output of the models directly against the gold stan-
dard annotation. This is a very challenging set-up,
as the model has no prior information about the
aspects defined (Table 2). In the second set of
experiments, we show that aspects and sentiments
induced by our model can be used to construct in-
formative features for supervised classification. In
</bodyText>
<footnote confidence="0.9989186">
6Downloadable from http://clic.cimec.
unitn.it/˜angeliki.lazaridou/datasets/
ACL2013Sentiment.tar.gz
7www.sfu.ca/˜mtaboada/research/SLSeg.
html
</footnote>
<page confidence="0.959396">
1634
</page>
<table confidence="0.9987785">
Model Precision Recall F1
Random 3.9 3.8 3.8
SentAsp 15.0 10.2 9.2
Discourse 16.5 13.8 10.8
</table>
<tableCaption confidence="0.9620545">
Table 3: Results in terms of macro-averaged pre-
cision, recall and F1.
</tableCaption>
<table confidence="0.993467666666667">
Model Unmarked Marked
SentAsp 9.2 5.4
Discourse 9.3 11.5
</table>
<tableCaption confidence="0.9143115">
Table 4: Separate evaluation (F1) of the “marked”
and the “unmarked” EDUs.
</tableCaption>
<table confidence="0.614326">
Content Aspect Polarity
1 but certainly off its greatness value neg
</table>
<listItem confidence="0.608554428571429">
2 and while small they are nice rooms pos
3 but it is not free for all guests amenities neg
4 and the water was brown clean neg
5 and no tea making facilities rooms neg
6 when i checked out service pos
7 and if you do not service neg
8 when we got home clean neu
</listItem>
<tableCaption confidence="0.883311">
Table 5: Examples of EDUs where local informa-
tion is not sufficiently informative.
</tableCaption>
<bodyText confidence="0.991750076923077">
all the cases, we compare the discourse-agnostic
and the discourse-informed models.
In order to induce the model, we let the sampler
run for 2000 iterations. We use the last sample to
define the labeling. The number of topics K was
set to 10 in order to match the number of aspects
defined in our annotation scheme (see Table 2).
The hyperpriors were chosen in a qualitative ex-
periment over a subset of our dataset by manually
inspecting the produced languages models. The
resulting values are: α = 10−3, 0 = 5 * 10−4,
T = 5 * 10−4, 77 = 10−3, v4 = 103, v¯4 = 10−4,
wo = 85 and w&apos; o = wψ = w&apos; ψ = 5.
</bodyText>
<subsectionHeader confidence="0.988683">
5.1 Direct clustering evaluation
</subsectionHeader>
<bodyText confidence="0.999980476923077">
Our labels encoding aspect and sentiment level can
be regarded as clusters. Consequently we can ap-
ply techniques developed in the context of cluster-
ing evaluation. We use a version of the standard
metrics considered for the word sense induction
task (Agirre and Soroa, 2007) where a clustering
is converted to a classification problem. This is
achieved by splitting the gold standard into two
subsets; the training portion is used to choose one-
to-one correspondence from the gold classes to the
induced clusters and then the chosen mapping is
applied to the testing portion. We perform 10-fold
cross validation and report precision, recall and F1
score. Our dataset is very skewed and the majority
class (rest) is arguably the least important, so we
use macro-averaging over labels and then average
those across folds to arrive to the reported num-
bers. We compare the discourse-informed model
(Discourse) against two baselines; the discourse-
agnostic SentAsp model and Random which as-
signs a random label to an EDU while respecting
the distribution of labels in the training set.
Table 3 presents the first analysis conducted on
the full set of EDUs. We observe that by incor-
porating latent discourse relation we improve per-
formance over the discourse-agnostic model Sen-
tAsp (statistically significant according to paired t-
test with p &lt; 0.01). Note that fairly low scores in
this evaluation setting are expected for any unsu-
pervised model of sentiment and topics, as models
are unsupervised both in the aspect-specific senti-
ment and in topic labels and the total number of
labels is 28 (all aspects can be associated with the
3 sentiment levels except for rest which can only
be used with neutral (0) sentiment). Consequently,
induced topics, though informative (as we confirm
in Section 5.3), may not correspond to the topics
defined in the gold standard. For example, one
well-known property of LDA-style topic models
is their tendency to induce topics which account
for similar fraction of words in the dataset (Jagar-
lamudi et al., 2012), thus, over-splitting ‘heavy’
topics (e.g. rooms in our case). The same, though
to lesser degree, is true for sentiment levels where
the border between neutral and positive (or nega-
tive) is also vaguely defined.
To gain insight into our model, we conducted
an experiment similar to the one presented in So-
masundaran et al. (2009). We divide the dataset in
two subsets; one containing all EDUs starting with
a discourse cue (“marked”) and one containing the
remaining EDUs (“unmarked”). We hypothesize
that the effect of the discourse-aware model should
be stronger on the first subset, since the presence
of the connective indicates the possibility of a dis-
course relation with the previous EDU. The set of
discourse connectives is taken from the Penn Dis-
course Treebank (Prasad et al., 2008), thus creat-
ing a list of 240 potential connectives.
Table 5 presents a subset of “marked” EDUs for
which trying to assign the sentiment and aspect
out of context (i.e. without the previous EDU) is
a difficult task. In examples 1-3 there is no ex-
plicit mention of the aspect. However, there is
an anaphoric expression (marked in bold) which
</bodyText>
<page confidence="0.960981">
1635
</page>
<bodyText confidence="0.999969172413793">
refers to a mention of the aspect in some previous
EDU. On the other hand, in examples 4 and 5 there
is an ambiguity in the choice of aspect; in example
5, tea making facilities can refer to a breakfast at
the hotel (label food) or to facilities in the room
(label rooms). Finally, examples 6-8 are too short
and not informative at all which indicates that the
segmentation tool does not always predict a de-
sirable segmentation. Consequently, automatic in-
duction of segmentation may be a better option.
Table 4 presents quantitative results of this anal-
ysis. Although the performance over the “un-
marked” example is the same for the two mod-
els, this is not the case for the “marked” instances
where the discourse-informed model leverages the
discourse signal and achieves better performance.
This behavior agrees with our initial hypothesis,
and suggests that our discourse representation,
though application-specific, relies in part on the
information encoded in linguistically-defined dis-
course cues. We will confirm this intuition in the
qualitative evaluation section. The increase for the
“marked” EDUs does not translate into greater dif-
ferences for the overall scores (Table 3) as marked
relations are considerably less frequent than un-
marked ones in our gold standard (i.e. 35% of the
EDUs are “marked”). Nevertheless, this clearly
suggests that the discourse-informed model is in
fact capable of exploiting discourse signal.
</bodyText>
<subsectionHeader confidence="0.999956">
5.2 Qualitative analysis
</subsectionHeader>
<bodyText confidence="0.995837257142857">
To investigate the quality of the induced discourse
structure, we present the most frequent discourse
cues extracted for every discourse relation. Ta-
ble 6 presents a selection of cues that best explain
the discourse relation they have been associated
with. A general observation is that among the cues
there are not only “traditional” discourse connec-
tives like even though, although, and, but also cues
that are discriminative for the specific application.
In relation SameAlt we can mostly observe
phrases that tend to introduce a new aspect, since
an explicit mention of it is provided (e.g the loca-
tion is, the room was) and more specific phrases
like in addition are used to introduce a new aspect
with the same sentiment. However, these cues re-
veal important information about the aspect of the
EDU, and since they are associated with the lan-
guage model ˜φ, they are not visible anymore to
the language model of aspects φ.
Cues for the relation AltSame also include
Discourse Discourse Cues
relation
SameAlt the location is , the room was, the hotel
has, and the room, and the bed, breakfast
was, the staff were, in addition, good luck
AltSame but, and, it was, and it was, and they, al-
though, and it, but it, but it was, however,
which was, this is, this was, they were,
the only thing, even though, unfortunately,
needless to say, fortunately
AltAlt the room was, the staff were, the only, the
hotel is, but the, however, also, or, overall
i, unfortunately, we will definitely, on the
plus, the only downside, even though, and
even though, i would definately
</bodyText>
<tableCaption confidence="0.881239">
Table 6: Induced cues from the discourse relations
</tableCaption>
<bodyText confidence="0.998752388888889">
phrases that contain some anaphoric expressions,
which might refer to previous mentions of an as-
pect in the discourse (i.e. previous EDU). We ex-
pect that since there is an anaphoric expression,
explicit lexical features for the aspect will be miss-
ing, making thus the decision concerning aspect
assignment ambiguous for any discourse-agnostic
model. Interestingly, we found the expressions un-
fortunately, fortunately, the only thing in the same
relation, since all indicate a change in sentiment.
Finally, AltAlt can be viewed as a mixture of the
other two relations. Furthermore, for this relation
we can find expressions that tend to be used at the
end of a review, since at this point we normally
change the aspect and often even sentiment. Some
examples of these cases are overall, we will defi-
nitely and even the misspelled version of the latter
i would definately.
</bodyText>
<subsectionHeader confidence="0.980528">
5.3 Features in supervised learning
</subsectionHeader>
<bodyText confidence="0.9999558125">
As an additional experiment to demonstrate infor-
mative of the output of the two models, we de-
sign a supervised learning task of predicting sen-
timent and topic of EDUs. In this setting, the
feature vector of every EDU consists of its bag-
of-word-representation to which we add two extra
features; the models’ predictions of topic and sen-
timent. We train a support vector machine with a
polynomial kernel using the default parameters of
Weka8 and perform 10-fold cross-validation.
Table 7 presents results of this analysis in terms
of accuracy for four classification tasks, i.e. pre-
dicting both sentiment and topic, only sentiment
and only topic for all EDUs, as well as predict-
ing sentiment and topic for the “marked” dataset.
First, we observe that incorporation of the topic-
</bodyText>
<footnote confidence="0.698186">
1636 8http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<table confidence="0.999512">
Features aspect+sentiment aspect sentiment Marked only
(28 classes) (10 classes) (3 classes) sentiment+aspect (28 classes)
only unigrams 36.3 49.8 57.1 26.2
unigrams + SentAsp 38.0 50.4 59.3 27.8
unigrams + Discourse 39.1 52.4 59.4 29.1
</table>
<tableCaption confidence="0.999466">
Table 7: Supervised learning at the EDU level (accuracy)
</tableCaption>
<bodyText confidence="0.999984727272727">
model features on a unigram-only model results
in an improvement in classification performance
across all tasks (predicting sentiment, predicting
aspects, or both); as a matter of fact, our accu-
racy results for predicting sentiment are compa-
rable to the sentence-level results presented by
T¨ackstr¨om and McDonald (2011). We have to
stress that accuracies for the joint task (i.e. pre-
dicting both sentiment and topic) are expected to
be lower since it can also be seen as the product
of the two other tasks (i.e. predicting only senti-
ment and only topic). We also observe that the fea-
tures induced from the Discourse model result in
higher accuracy than the ones from the discourse-
agnostic model SentAsp both in the complete set
of EDUs and the “marked” subset, results that are
in line with the ones presented in Table 4. Fi-
nally, the fact that the results for the complete set
of EDUs are higher than the ones for the “marked”
dataset clearly suggests that the latter constitute a
hard case for sentiment analysis, in which exploit-
ing discourse signal proves to be beneficial.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999989634615385">
Recently, there has been significant interest in
leveraging content structure for a number of NLP
tasks (Webber et al., 2011). Sentiment analysis
has not been an exception to this and discourse has
been used in order to enforce constraints on the
assignment of polarity labels at several granular-
ity levels, ranging from the lexical level (Polanyi
and Zaenen, 2006) to the review level (Taboada
et al., 2011). One way to deal with this prob-
lem is to model the interactions by using a pre-
compiled set of polarity shifters (Nakagawa et al.,
2010; Polanyi and Zaenen, 2006; Sadamitsu et al.,
2008). Socher et al. (2011) defined a recurrent
neural network model, which, in essence, learns
those polarity shifters relying on sentence-level
sentiment labels. Though successful, this model is
unlikely to capture intra-sentence non-local phe-
nomena such as effect of discourse connectives,
unless it is provided with syntactic information
as an input. This may be problematic for the
noisy sentiment-analysis domain and especially
for poor-resource languages. Similar to our work,
others have focused on modeling interactions be-
tween phrases and sentences. However, this has
been achieved by either using a subset of relations
that can be found in discourse theories (Zhou et
al., 2011; Asher et al., 2008; Snyder and Barzi-
lay, 2007) or by using directly (Taboada et al.,
2008) the output of discourse parsers (Soricut and
Marcu, 2003). Discourse cues as predictive fea-
tures of topic boundaries have also been consid-
ered in Eisenstein and Barzilay (2008). This work
was extended by Trivedi and Eisenstein (2013),
where discourse connectors are used as features
for modeling subjectivity transitions.
Another related line of research was presented
in Somasundaran et al. (2009) where a domain-
specific discourse scheme is considered. Simi-
larly to our set-up, discourse relations enforce con-
straints on sentiment polarity of associated sen-
timent expressions. Somasundaran et al. (2009)
show that gold-standard discourse information en-
coded in this way provides a useful signal for pre-
diction of sentiment, but they leave automatic dis-
course relation prediction for future work. They
use an integer linear programming framework to
enforce agreement between classifiers and soft
constraints provided by discourse annotations.
This contrasts with our work; we do not rely on
expert discourse annotation, but rather induce both
discourse relations and cues jointly with aspect
and sentiment.
</bodyText>
<sectionHeader confidence="0.997258" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999701">
In this work, we showed that by jointly induc-
ing discourse information in the form of discourse
cues, we can achieve better predictions for aspect-
specific sentiment polarity. Our contribution con-
sists in proposing a general way of how discourse
information can be integrated in any LDA-style
discourse-agnostic model of aspect and sentiment.
In the future, we aim at modeling more flexible
sets of discourse relations and automatically in-
ducing discourse segmentation relevant to the task.
</bodyText>
<page confidence="0.985949">
1637
</page>
<sectionHeader confidence="0.989791" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883920634921">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Se-
mEval, pages 7–12.
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. Proceedings of Coling, pages 5–
8.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804–812.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334–343.
Emily B Fox, Erik B Sudderth, Michael I Jordan, and
Alan S Willsky. 2008. An HDP-HMM for systems
with state persistence. In Proceedings of ICML.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of
WebDB.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235.
Geoffrey E Hinton. 1999. Products of experts. In Pro-
ceedings of ICANN, volume 1, pages 1–6.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD, pages 168–177.
Jagadeesh Jagarlamudi, Hal Daum´e III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. Proceedings of EACL, pages 204–
213.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM, pages 815–824.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceeding of CIKM, pages 375–384.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW, pages 171–180.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Proceed-
ings of NAACL, pages 786–794.
Tahira Naseem, Benjamin Snyder, Jacob Eisen-
stein, and Regina Barzilay. 2009. Multilin-
gual part-of-speech tagging: Two unsupervised ap-
proaches. Journal of Artificial Intelligence Re-
search, 36(1):341–385.
Livia Polanyi and Annie Zaenen. 2006. Contextual
valence shifters. Computing attitude and affect in
text: Theory and applications, pages 1–10.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Kugatsu Sadamitsu, Satoshi Sekine, and Mikio Ya-
mamoto. 2008. Sentiment analysis based on proba-
bilistic models using inter-sentence information. In
Proceedings of ACL, pages 2892–2896.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In
Proceedings of HLT-NAACL, pages 300–307.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151–161.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of Coling, pages 801–808.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP, pages 170–179.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of NAACL, pages 149–156.
Maite Taboada, Kimberly Voll, and Julian Brooke.
2008. Extracting sentiment as a function of dis-
course structure and topicality. Simon Fraser Uni-
versity, Tech. Rep, 20.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267–307.
Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of ACL, pages
569–574.
Ivan Titov and Ryan McDonald. 2008a. A joint model
of text and aspect ratings for sentiment summariza-
1638 tion. In Proceedings ofACL, pages 308–316.
Ivan Titov and Ryan McDonald. 2008b. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of WWW, pages 112–120.
Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse
connectors for latent subjectivity in sentiment anal-
ysis. In In Proceedings of NAACL.
Peter D Turney and Michael L Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus.
Kimberly Voll and Maite Taboada. 2007. Not all
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. In Pro-
ceedings of Australian Conf. on AI.
Bonnie Webber, Markus Egg, and Valia Kordoni.
2011. Discourse structure and language technology.
Natural Language Engineering, 1(1):1–54.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,
and Kam-Fai Wong. 2011. Unsupervised discovery
of discourse relations for eliminating intra-sentence
polarity ambiguities. In Proceedings EMNLP, pages
162–171.
</reference>
<page confidence="0.995369">
1639
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239593">
<title confidence="0.997195">A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</title>
<author confidence="0.977136">Angeliki</author>
<affiliation confidence="0.996213">University of</affiliation>
<email confidence="0.845189">angeliki.lazaridou@unitn.it</email>
<author confidence="0.983952">Ivan Titov</author>
<affiliation confidence="0.517523">Saarland</affiliation>
<email confidence="0.978701">titov@mmci.uni-saarland.de</email>
<author confidence="0.615802">Caroline</author>
<affiliation confidence="0.999599">Trier University</affiliation>
<email confidence="0.977317">csporled@coli.uni-sb.de</email>
<abstract confidence="0.998349227272727">We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the SemEval,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="23764" citStr="Agirre and Soroa, 2007" startWordPosition="3899" endWordPosition="3902">fined in our annotation scheme (see Table 2). The hyperpriors were chosen in a qualitative experiment over a subset of our dataset by manually inspecting the produced languages models. The resulting values are: α = 10−3, 0 = 5 * 10−4, T = 5 * 10−4, 77 = 10−3, v4 = 103, v¯4 = 10−4, wo = 85 and w&apos; o = wψ = w&apos; ψ = 5. 5.1 Direct clustering evaluation Our labels encoding aspect and sentiment level can be regarded as clusters. Consequently we can apply techniques developed in the context of clustering evaluation. We use a version of the standard metrics considered for the word sense induction task (Agirre and Soroa, 2007) where a clustering is converted to a classification problem. This is achieved by splitting the gold standard into two subsets; the training portion is used to choose oneto-one correspondence from the gold classes to the induced clusters and then the chosen mapping is applied to the testing portion. We perform 10-fold cross validation and report precision, recall and F1 score. Our dataset is very skewed and the majority class (rest) is arguably the least important, so we use macro-averaging over labels and then average those across folds to arrive to the reported numbers. We compare the discou</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the SemEval, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Farah Benamara</author>
<author>Yvette Yannick Mathieu</author>
</authors>
<title>Distilling opinion in discourse: A preliminary study.</title>
<date>2008</date>
<booktitle>Proceedings of Coling,</booktitle>
<pages>5--8</pages>
<contexts>
<context position="34155" citStr="Asher et al., 2008" startWordPosition="5603" endWordPosition="5606">earns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on senti</context>
</contexts>
<marker>Asher, Benamara, Mathieu, 2008</marker>
<rawString>Nicholas Asher, Farah Benamara, and Yvette Yannick Mathieu. 2008. Distilling opinion in discourse: A preliminary study. Proceedings of Coling, pages 5– 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>804--812</pages>
<contexts>
<context position="19905" citStr="Brody and Elhadad, 2010" startWordPosition="3236" endWordPosition="3239">,s are drawn twice; once from the document specific distribution and once from the discourse transition distributions. Under this simplification, we can easily derive the conditional probabilities for the collapsed Gibbs sampling. 5 Experiments To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e.g., classifying the overall sentiment of sentences (Titov and McDonald, 2008a; Brody and Elhadad, 2010) or even reviews (Nakagawa et al., 2010; Jo and Oh, 2011)) or evaluated solely at the sentential or even document level. Consequently, in order to evaluate our methods, we created a new dataset which will be publicly released. Aspects Frequency service 246 value 55 location 121 rooms 316 sleep quality 56 cleanliness 59 amenities 180 food 81 recommendation 121 rest 306 Total 1541 Table 2: Distribution of aspects in the data. Dataset and Annotation The dataset we created consists of 13559 hotel reviews from TripAdvisor.com.6 Since our modeling is performed on the EDU level, all sentences where s</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Samuel Brody and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews. In Proceedings of NAACL, pages 804–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>334--343</pages>
<contexts>
<context position="34404" citStr="Eisenstein and Barzilay (2008)" startWordPosition="5644" endWordPosition="5647">ic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation predict</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of EMNLP, pages 334–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily B Fox</author>
<author>Erik B Sudderth</author>
<author>Michael I Jordan</author>
<author>Alan S Willsky</author>
</authors>
<title>An HDP-HMM for systems with state persistence.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="9581" citStr="Fox et al., 2008" startWordPosition="1509" endWordPosition="1512">general relations which encode how polarity and aspect change (Table 1). Note that we do not have a discourse relation SameSame since we do not expect to have strong linguistic evidence which states that an EDU contains the same sentiment information as the previous one.4 However, we assume that the sentiment and topic flow is fairly smooth in general. In other words, for two adjacent EDUs not connected by any of the above three relations, the prior probability of staying at the same topic and sentiment level is higher than picking a new topic and sentiment level (i.e. we use “sticky states” (Fox et al., 2008)). 3 Model In this section we describe our Bayesian model, first the discourse-agnostic model and then an extension needed to encode discourse information. The formal generative story is presented in Figure 1: the red fragments correspond to the discourse modeling component. In order to obtain the generative story for the discourse-agnostic model, they simply need to be ignored. 3.1 Discourse-agnostic model In our approach we make an assumption that all the words in an EDU correspond to the same topic and sentiment level. We also assume that an overall sentiment of the document is defined, thi</context>
</contexts>
<marker>Fox, Sudderth, Jordan, Willsky, 2008</marker>
<rawString>Emily B Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. 2008. An HDP-HMM for systems with state persistence. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gayatree Ganu</author>
<author>Noemie Elhadad</author>
<author>Amelie Marian</author>
</authors>
<title>Beyond the stars: Improving rating predictions using review text content.</title>
<date>2009</date>
<booktitle>In Proceedings of WebDB.</booktitle>
<contexts>
<context position="21447" citStr="Ganu et al. (2009)" startWordPosition="3501" endWordPosition="3504">aspect and sentiment (i.e. +1, 0 or −1) it expresses. Table 2 presents the distribution of aspects in the dataset. The distribution of the sentiments is uniform. The label rest captures cases where EDUs do not refer to any aspect or to a very rare aspect. The inter-annotator agreement (IAA), as measured in terms of Cohen’s kappa score, was 66% for the aspect labeling, 70% for the sentiment annotation and 61% for the joint task of sentiment and aspect annotation. Though these scores may not seem very high, they are similar to the ones reported in related sentiment annotation efforts (see e.g., Ganu et al. (2009)). Experimental setup In order to quantitatively evaluate the model predictions, we run two sets of experiments. In the first, we treat the task as an unsupervised classification problem and evaluate the output of the models directly against the gold standard annotation. This is a very challenging set-up, as the model has no prior information about the aspects defined (Table 2). In the second set of experiments, we show that aspects and sentiments induced by our model can be used to construct informative features for supervised classification. In 6Downloadable from http://clic.cimec. unitn.it/</context>
</contexts>
<marker>Ganu, Elhadad, Marian, 2009</marker>
<rawString>Gayatree Ganu, Noemie Elhadad, and Amelie Marian. 2009. Beyond the stars: Improving rating predictions using review text content. In Proceedings of WebDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="16167" citStr="Goldwater et al., 2009" startWordPosition="2606" endWordPosition="2610"> constraints that discourse relations impose on the local flow of sentiment and aspects. The parameter ˜~c is a language model over discourse cues ˜w, which are not restricted to unigrams but can generate phrases of arbitrary (and variable) size. For this reason, we draw them from a Dirichlet process (DP) (i.e. one for each discourse relation, except for NoRelation). The base measure G0 provides the probability of an nword sequence calculated with the bigram probability model estimated from the corpus.5 This model component bears strong similarities to the Bayesian model of word segmentation (Goldwater et al., 2009), though we use the DP process to generate only the prefix of the EDU, whereas the rest of the EDU is generated from the bag-ofwords model. Drawing documents As pointed out above, the content generation is broken into two steps, where first we draw the discourse cue ˜wd,s from ˜~c and then we generate the remaining words. The second difference at the data generation step (Figure 1) is in the way the aspect and sentiment labels are drawn. As the discourse relation between the EDUs has already been chosen, we have some expectations about the values of the sentiment and aspect of the following ED</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="17920" citStr="Griffiths and Steyvers, 2004" startWordPosition="2904" endWordPosition="2907">. In the PoE model, in order for an outcome to be chosen, it needs to have a non-negligible probability under both models. 4 Inference Since exact inference of our model is intractable, we use collapsed Gibbs sampling. The variables that need to be inferred are the topic assignments z, the sentiment assignments y, the discourse relations c and the discourse cue w˜ (or, more precisely, its length) and are all sampled jointly (for each EDU) since we expect them to be highly dependent. All other variables (i.e. unknown distributions) could be marginalized out to obtain a collapsed Gibbs sampler (Griffiths and Steyvers, 2004). 5This measure is improper but it serves the purpose of favoring long cues, the behavior arguably desirable for our application. 1633 Global parameters: ϕ˜ ∼ Dir(ν) [distrib of disc rel] for each discourse relation c = 1, .., 4: ˜φc ∼ DP(η, Go) [distrib of disc rel specific disc cues] ˜θc,k -fixed [distrib of rel specific aspect transitions] ˜φc,y - fixed [distrib of rel specific sent transitions] for each aspect k = 1, 2...K: for each sentiment y = −1, 0, +1: φk,y ∼ Dir(λk) [unigram language models] for each global sentiment yˆ = −1, 0, +1: ψˆy,k ∼ Dir(γ) [sent distrib given overall sentimen</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Products of experts.</title>
<date>1999</date>
<booktitle>In Proceedings of ICANN,</booktitle>
<volume>1</volume>
<pages>1--6</pages>
<contexts>
<context position="17056" citStr="Hinton, 1999" startWordPosition="2760" endWordPosition="2761">m ˜~c and then we generate the remaining words. The second difference at the data generation step (Figure 1) is in the way the aspect and sentiment labels are drawn. As the discourse relation between the EDUs has already been chosen, we have some expectations about the values of the sentiment and aspect of the following EDU, which are encoded by the distributions ˜ and ˜B. These are only soft constraints that have to be taken into consideration along with the information provided by the aspect-sentiment model. This coupling of information naturally translates into the productof-experts (PoE) (Hinton, 1999) approach, where two sources of information jointly contribute to the final result. The PoE model seems to be more appropriate here than a mixture model, as we do not want the discourse transition to overpower the sentiment-topic model. In the PoE model, in order for an outcome to be chosen, it needs to have a non-negligible probability under both models. 4 Inference Since exact inference of our model is intractable, we use collapsed Gibbs sampling. The variables that need to be inferred are the topic assignments z, the sentiment assignments y, the discourse relations c and the discourse cue w</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>Geoffrey E Hinton. 1999. Products of experts. In Proceedings of ICANN, volume 1, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of ACM SIGKDD,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="1801" citStr="Hu and Liu, 2004" startWordPosition="262" endWordPosition="265">Web, it is becoming increasingly difficult to discern useful from irrelevant information, particularly in user-generated content, such as product reviews. To make it easier for the reader to separate the wheat from the chaff, it is necessary to structure the available information. In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’. Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (Hu and Liu, 2004), a task with many practical applications. Aspect-based summarization is an active research area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of ACM SIGKDD, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Raghavendra Udupa</author>
</authors>
<title>Incorporating lexical priors into topic models.</title>
<date>2012</date>
<booktitle>Proceedings of EACL,</booktitle>
<pages>204--213</pages>
<marker>Jagarlamudi, Daum´e, Udupa, 2012</marker>
<rawString>Jagadeesh Jagarlamudi, Hal Daum´e III, and Raghavendra Udupa. 2012. Incorporating lexical priors into topic models. Proceedings of EACL, pages 204– 213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>815--824</pages>
<contexts>
<context position="6176" citStr="Jo and Oh, 2011" startWordPosition="962" endWordPosition="965">(or weakly supervised) manner. This has the advantage of not having to pre-specify a mapping from discourse cues to discourse relations; our model induces this automatically, which makes it portable to new domains and languages. Joint induction of discourse and sentiment structure also has the added benefit that the model is able to learn exactly those aspects of discourse structure that are relevant for sentiment analysis. We start with a relatively standard joint model of sentiment and topic, which can be regarded as a cross-breed between the JST model (Lin and He, 2009) and the ASUM model (Jo and Oh, 2011), changeably as well as sentiment levels and opinion polarity. both state-of-the-art techniques. This model is weakly supervised, as it relies solely on documentlevel (i.e. not aspect-specific) opinion polarity labels to induce topics and sentiment on the subsentential level. In order to test our hypothesis that discourse information is beneficial, we add a discourse modeling component. Note that in modeling discourse we do not exploit any kind of supervision. We demonstrate that the resulting model outperforms the baseline on a product review dataset (see Section 5). To the best of our knowle</context>
<context position="12971" citStr="Jo and Oh, 2011" startWordPosition="2089" endWordPosition="2092"> that EDU segmentation is provided by the preprocessing step. First, we generate the aspect zd,s for EDU s according to the distribution of topics θd. Then, we choose a sentiment level yd,s for the considered EDU from the categorical distribution ψ0d,zd,s, conditioned on the aspect zd,s, as well as on the global sentiment of the document ˆyd. Finally, we generate the bag of words for the EDU by drawing the words from the aspect- and sentiment-specific language model. This model can be seen as a variant of a state-ofthe-art model for jointly inducing sentiment and aspect at the sentence level (Jo and Oh, 2011), or, more precisely, as its combination with the JST model (Lin and He, 2009), adapted to the specifics of our setting. Both these models have been shown to perform well on sentiment and topic prediction tasks, outperforming earlier models, such as the TSM model (Mei et al., 2007). Consequently, it can be considered as a strong baseline. 1632 3.2 Discourse-informed model In order to integrate discourse information into the discourse-agnostic model, we need to define a set of extra parameters and random variables. Drawing model parameters First, at the corpus level, we draw a distribution co o</context>
<context position="19962" citStr="Jo and Oh, 2011" startWordPosition="3248" endWordPosition="3251">and once from the discourse transition distributions. Under this simplification, we can easily derive the conditional probabilities for the collapsed Gibbs sampling. 5 Experiments To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e.g., classifying the overall sentiment of sentences (Titov and McDonald, 2008a; Brody and Elhadad, 2010) or even reviews (Nakagawa et al., 2010; Jo and Oh, 2011)) or evaluated solely at the sentential or even document level. Consequently, in order to evaluate our methods, we created a new dataset which will be publicly released. Aspects Frequency service 246 value 55 location 121 rooms 316 sleep quality 56 cleanliness 59 amenities 180 food 81 recommendation 121 rest 306 Total 1541 Table 2: Distribution of aspects in the data. Dataset and Annotation The dataset we created consists of 13559 hotel reviews from TripAdvisor.com.6 Since our modeling is performed on the EDU level, all sentences where segmented using the SLSEG software package.7 As a result, </context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of WSDM, pages 815–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceeding of CIKM,</booktitle>
<pages>375--384</pages>
<contexts>
<context position="6139" citStr="Lin and He, 2009" startWordPosition="954" endWordPosition="957">nformation jointly in an unsupervised (or weakly supervised) manner. This has the advantage of not having to pre-specify a mapping from discourse cues to discourse relations; our model induces this automatically, which makes it portable to new domains and languages. Joint induction of discourse and sentiment structure also has the added benefit that the model is able to learn exactly those aspects of discourse structure that are relevant for sentiment analysis. We start with a relatively standard joint model of sentiment and topic, which can be regarded as a cross-breed between the JST model (Lin and He, 2009) and the ASUM model (Jo and Oh, 2011), changeably as well as sentiment levels and opinion polarity. both state-of-the-art techniques. This model is weakly supervised, as it relies solely on documentlevel (i.e. not aspect-specific) opinion polarity labels to induce topics and sentiment on the subsentential level. In order to test our hypothesis that discourse information is beneficial, we add a discourse modeling component. Note that in modeling discourse we do not exploit any kind of supervision. We demonstrate that the resulting model outperforms the baseline on a product review dataset (see </context>
<context position="13049" citStr="Lin and He, 2009" startWordPosition="2103" endWordPosition="2106">ate the aspect zd,s for EDU s according to the distribution of topics θd. Then, we choose a sentiment level yd,s for the considered EDU from the categorical distribution ψ0d,zd,s, conditioned on the aspect zd,s, as well as on the global sentiment of the document ˆyd. Finally, we generate the bag of words for the EDU by drawing the words from the aspect- and sentiment-specific language model. This model can be seen as a variant of a state-ofthe-art model for jointly inducing sentiment and aspect at the sentence level (Jo and Oh, 2011), or, more precisely, as its combination with the JST model (Lin and He, 2009), adapted to the specifics of our setting. Both these models have been shown to perform well on sentiment and topic prediction tasks, outperforming earlier models, such as the TSM model (Mei et al., 2007). Consequently, it can be considered as a strong baseline. 1632 3.2 Discourse-informed model In order to integrate discourse information into the discourse-agnostic model, we need to define a set of extra parameters and random variables. Drawing model parameters First, at the corpus level, we draw a distribution co over four discourse relations: three relations as defined in Table 1 and an add</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceeding of CIKM, pages 375–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="4502" citStr="Mann and Thompson, 1988" startWordPosition="696" endWordPosition="699">al information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations informative for the sentiment analysis domain (Voll and Taboada, 2007). An alternative approach is to </context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="1982" citStr="Mei et al., 2007" startWordPosition="289" endWordPosition="292">eader to separate the wheat from the chaff, it is necessary to structure the available information. In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’. Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (Hu and Liu, 2004), a task with many practical applications. Aspect-based summarization is an active research area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect for a fragment of text. For instance, in the following example taken from a TripAdvisor1 review: Example 1. The room was nice but</context>
<context position="13253" citStr="Mei et al., 2007" startWordPosition="2137" endWordPosition="2140">t zd,s, as well as on the global sentiment of the document ˆyd. Finally, we generate the bag of words for the EDU by drawing the words from the aspect- and sentiment-specific language model. This model can be seen as a variant of a state-ofthe-art model for jointly inducing sentiment and aspect at the sentence level (Jo and Oh, 2011), or, more precisely, as its combination with the JST model (Lin and He, 2009), adapted to the specifics of our setting. Both these models have been shown to perform well on sentiment and topic prediction tasks, outperforming earlier models, such as the TSM model (Mei et al., 2007). Consequently, it can be considered as a strong baseline. 1632 3.2 Discourse-informed model In order to integrate discourse information into the discourse-agnostic model, we need to define a set of extra parameters and random variables. Drawing model parameters First, at the corpus level, we draw a distribution co over four discourse relations: three relations as defined in Table 1 and an additional dummy relation 4 to indicate that there is no relation between two adjacent EDUs (NoRelation). This distribution is drawn from an asymmetric Dirichlet prior parametrized by a vector of hyperparame</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of WWW, pages 171–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using crfs with hidden variables.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>786--794</pages>
<contexts>
<context position="4293" citStr="Nakagawa et al., 2010" startWordPosition="661" endWordPosition="665">on for Computational Linguistics, pages 1630–1639, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to e</context>
<context position="19944" citStr="Nakagawa et al., 2010" startWordPosition="3243" endWordPosition="3247"> specific distribution and once from the discourse transition distributions. Under this simplification, we can easily derive the conditional probabilities for the collapsed Gibbs sampling. 5 Experiments To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e.g., classifying the overall sentiment of sentences (Titov and McDonald, 2008a; Brody and Elhadad, 2010) or even reviews (Nakagawa et al., 2010; Jo and Oh, 2011)) or evaluated solely at the sentential or even document level. Consequently, in order to evaluate our methods, we created a new dataset which will be publicly released. Aspects Frequency service 246 value 55 location 121 rooms 316 sleep quality 56 cleanliness 59 amenities 180 food 81 recommendation 121 rest 306 Total 1541 Table 2: Distribution of aspects in the data. Dataset and Annotation The dataset we created consists of 13559 hotel reviews from TripAdvisor.com.6 Since our modeling is performed on the EDU level, all sentences where segmented using the SLSEG software packa</context>
<context position="33401" citStr="Nakagawa et al., 2010" startWordPosition="5489" endWordPosition="5492"> in which exploiting discourse signal proves to be beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sente</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using crfs with hidden variables. In Proceedings of NAACL, pages 786–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Benjamin Snyder</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="19222" citStr="Naseem et al. (2009)" startWordPosition="3128" endWordPosition="3131">Bd ∼ Dir(α) [distr over aspects] for every EDU s: cd,s ∼ ϕ˜ [draw disc relation] if cd,s =6 NoRelation ˜wd,s ∼ ˜φcd,e [draw disc cue] zd,s ∼ Bd ∗ ˜θcd,e, zd,e−1 [draw aspect] yd,s ∼ ψˆyd,zd,s∗ ˜ψcd,e,yd,e−1 [draw sentiment level] for each word after disc cue: wd,s ∼ φzd ,s,yd,s [draw words] Figure 1: The generative story for the joint model. The components responsible for modeling discourse information are emphasized in red: when dropped, one is left with the discourse-agnostic model. Unfortunately, the use of the PoE model prevents us from marginalizing the parameters exactly. Instead, as in Naseem et al. (2009), we resort to an approximation. We assume that zd,s and yd,s are drawn twice; once from the document specific distribution and once from the discourse transition distributions. Under this simplification, we can easily derive the conditional probabilities for the collapsed Gibbs sampling. 5 Experiments To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e.g., classifying the </context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal of Artificial Intelligence Research, 36(1):341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual valence shifters. Computing attitude and affect in text: Theory and applications,</title>
<date>2006</date>
<pages>1--10</pages>
<contexts>
<context position="4269" citStr="Polanyi and Zaenen, 2006" startWordPosition="657" endWordPosition="660">l Meeting of the Association for Computational Linguistics, pages 1630–1639, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additi</context>
<context position="33223" citStr="Polanyi and Zaenen, 2006" startWordPosition="5455" endWordPosition="5458">e fact that the results for the complete set of EDUs are higher than the ones for the “marked” dataset clearly suggests that the latter constitute a hard case for sentiment analysis, in which exploiting discourse signal proves to be beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problemat</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>Livia Polanyi and Annie Zaenen. 2006. Contextual valence shifters. Computing attitude and affect in text: Theory and applications, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="26322" citStr="Prasad et al., 2008" startWordPosition="4318" endWordPosition="4321"> (or negative) is also vaguely defined. To gain insight into our model, we conducted an experiment similar to the one presented in Somasundaran et al. (2009). We divide the dataset in two subsets; one containing all EDUs starting with a discourse cue (“marked”) and one containing the remaining EDUs (“unmarked”). We hypothesize that the effect of the discourse-aware model should be stronger on the first subset, since the presence of the connective indicates the possibility of a discourse relation with the previous EDU. The set of discourse connectives is taken from the Penn Discourse Treebank (Prasad et al., 2008), thus creating a list of 240 potential connectives. Table 5 presents a subset of “marked” EDUs for which trying to assign the sentiment and aspect out of context (i.e. without the previous EDU) is a difficult task. In examples 1-3 there is no explicit mention of the aspect. However, there is an anaphoric expression (marked in bold) which 1635 refers to a mention of the aspect in some previous EDU. On the other hand, in examples 4 and 5 there is an ambiguity in the choice of aspect; in example 5, tea making facilities can refer to a breakfast at the hotel (label food) or to facilities in the r</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kugatsu Sadamitsu</author>
<author>Satoshi Sekine</author>
<author>Mikio Yamamoto</author>
</authors>
<title>Sentiment analysis based on probabilistic models using inter-sentence information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>2892--2896</pages>
<contexts>
<context position="33452" citStr="Sadamitsu et al., 2008" startWordPosition="5497" endWordPosition="5500">e beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either usi</context>
</contexts>
<marker>Sadamitsu, Sekine, Yamamoto, 2008</marker>
<rawString>Kugatsu Sadamitsu, Satoshi Sekine, and Mikio Yamamoto. 2008. Sentiment analysis based on probabilistic models using inter-sentence information. In Proceedings of ACL, pages 2892–2896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Multiple aspect ranking using the good grief algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>300--307</pages>
<contexts>
<context position="34183" citStr="Snyder and Barzilay, 2007" startWordPosition="5607" endWordPosition="5611"> shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated </context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Multiple aspect ranking using the good grief algorithm. In Proceedings of HLT-NAACL, pages 300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="33474" citStr="Socher et al. (2011)" startWordPosition="5501" endWordPosition="5504">ork Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relatio</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of EMNLP, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
<author>Josef Ruppenhofer</author>
</authors>
<title>Discourse level opinion interpretation.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="8941" citStr="Somasundaran et al. (2008)" startWordPosition="1397" endWordPosition="1400">even better) or both (the room was nice but the breakfast was awful). Therefore, we do not explicitly model 3The ‘explanation’ relation, for example, can occur with a polarity change (We were upgraded to a really nice room because the hotel made a terrible blunder with our booking) but does not have to (The room was really nice because the hotel was newly renovated). 1631 Name Description AltSame different polarity, same aspect SameAlt same polarity, different aspect AltAlt different polarity and aspect Table 1: Discourse relations generic discourse relations; instead, inspired by the work of Somasundaran et al. (2008), we define three very general relations which encode how polarity and aspect change (Table 1). Note that we do not have a discourse relation SameSame since we do not expect to have strong linguistic evidence which states that an EDU contains the same sentiment information as the previous one.4 However, we assume that the sentiment and topic flow is fairly smooth in general. In other words, for two adjacent EDUs not connected by any of the above three relations, the prior probability of staying at the same topic and sentiment level is higher than picking a new topic and sentiment level (i.e. w</context>
</contexts>
<marker>Somasundaran, Wiebe, Ruppenhofer, 2008</marker>
<rawString>Swapna Somasundaran, Janyce Wiebe, and Josef Ruppenhofer. 2008. Discourse level opinion interpretation. In Proceedings of Coling, pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>170--179</pages>
<contexts>
<context position="5181" citStr="Somasundaran et al., 2009" startWordPosition="801" endWordPosition="805">st, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations informative for the sentiment analysis domain (Voll and Taboada, 2007). An alternative approach is to define a taskspecific scheme of discourse relations (Somasundaran et al., 2009). This previous work showed that task-specific discourse relations are helpful in predicting sentiment, however, in doing so they relied on gold-standard discourse annotation at test time rather than predicting it automatically or inducing it jointly with sentiment polarity. We take a different approach and induce discourse and sentiment information jointly in an unsupervised (or weakly supervised) manner. This has the advantage of not having to pre-specify a mapping from discourse cues to discourse relations; our model induces this automatically, which makes it portable to new domains and lan</context>
<context position="25859" citStr="Somasundaran et al. (2009)" startWordPosition="4242" endWordPosition="4246">hough informative (as we confirm in Section 5.3), may not correspond to the topics defined in the gold standard. For example, one well-known property of LDA-style topic models is their tendency to induce topics which account for similar fraction of words in the dataset (Jagarlamudi et al., 2012), thus, over-splitting ‘heavy’ topics (e.g. rooms in our case). The same, though to lesser degree, is true for sentiment levels where the border between neutral and positive (or negative) is also vaguely defined. To gain insight into our model, we conducted an experiment similar to the one presented in Somasundaran et al. (2009). We divide the dataset in two subsets; one containing all EDUs starting with a discourse cue (“marked”) and one containing the remaining EDUs (“unmarked”). We hypothesize that the effect of the discourse-aware model should be stronger on the first subset, since the presence of the connective indicates the possibility of a discourse relation with the previous EDU. The set of discourse connectives is taken from the Penn Discourse Treebank (Prasad et al., 2008), thus creating a list of 240 potential connectives. Table 5 presents a subset of “marked” EDUs for which trying to assign the sentiment </context>
<context position="34626" citStr="Somasundaran et al. (2009)" startWordPosition="5676" endWordPosition="5679">ences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation prediction for future work. They use an integer linear programming framework to enforce agreement between classifiers and soft constraints provided by discourse annotations. This contrasts with our work; we do not rely on expert </context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of EMNLP, pages 170–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>149--156</pages>
<contexts>
<context position="34285" citStr="Soricut and Marcu, 2003" startWordPosition="5625" endWordPosition="5628">re intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encode</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of NAACL, pages 149–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Kimberly Voll</author>
<author>Julian Brooke</author>
</authors>
<title>Extracting sentiment as a function of discourse structure and topicality.</title>
<date>2008</date>
<tech>Tech. Rep, 20.</tech>
<institution>Simon Fraser University,</institution>
<contexts>
<context position="4381" citStr="Taboada et al., 2008" startWordPosition="677" endWordPosition="680">3 Association for Computational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce d</context>
<context position="34227" citStr="Taboada et al., 2008" startWordPosition="5616" endWordPosition="5619">els. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (</context>
</contexts>
<marker>Taboada, Voll, Brooke, 2008</marker>
<rawString>Maite Taboada, Kimberly Voll, and Julian Brooke. 2008. Extracting sentiment as a function of discourse structure and topicality. Simon Fraser University, Tech. Rep, 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="33266" citStr="Taboada et al., 2011" startWordPosition="5463" endWordPosition="5466"> EDUs are higher than the ones for the “marked” dataset clearly suggests that the latter constitute a hard case for sentiment analysis, in which exploiting discourse signal proves to be beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain </context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Semisupervised latent variable models for sentence-level sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>569--574</pages>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semisupervised latent variable models for sentence-level sentiment analysis. In Proceedings of ACL, pages 569–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summariza1638 tion.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>308--316</pages>
<contexts>
<context position="2008" citStr="Titov and McDonald, 2008" startWordPosition="293" endWordPosition="296">the wheat from the chaff, it is necessary to structure the available information. In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’. Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (Hu and Liu, 2004), a task with many practical applications. Aspect-based summarization is an active research area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect for a fragment of text. For instance, in the following example taken from a TripAdvisor1 review: Example 1. The room was nice but let’s not talk about the </context>
<context position="10302" citStr="Titov and McDonald, 2008" startWordPosition="1630" endWordPosition="1633"> then an extension needed to encode discourse information. The formal generative story is presented in Figure 1: the red fragments correspond to the discourse modeling component. In order to obtain the generative story for the discourse-agnostic model, they simply need to be ignored. 3.1 Discourse-agnostic model In our approach we make an assumption that all the words in an EDU correspond to the same topic and sentiment level. We also assume that an overall sentiment of the document is defined, this is the only supervision we use in inducing the model. Unlike some of the previous work (e.g., (Titov and McDonald, 2008a)), we do not constrain aspectspecific sentiment to be the same across the document. We describe our discourse-agnostic model by first describing the set of corpus-level and document-level parameters, and then explain how the content of each document is generated. Drawing model parameters On the corpus level, for every topic z ∈ {1, ... , K} and every sentiment polarity level y ∈ {−1,0,+1}, we start by drawing a unigram language model 4The typical connective in this situation would be and which is highly ambiguous and can signal several traditional discourse relations. from a Dirichlet prior.</context>
<context position="19878" citStr="Titov and McDonald, 2008" startWordPosition="3232" endWordPosition="3235"> We assume that zd,s and yd,s are drawn twice; once from the document specific distribution and once from the discourse transition distributions. Under this simplification, we can easily derive the conditional probabilities for the collapsed Gibbs sampling. 5 Experiments To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e.g., classifying the overall sentiment of sentences (Titov and McDonald, 2008a; Brody and Elhadad, 2010) or even reviews (Nakagawa et al., 2010; Jo and Oh, 2011)) or evaluated solely at the sentential or even document level. Consequently, in order to evaluate our methods, we created a new dataset which will be publicly released. Aspects Frequency service 246 value 55 location 121 rooms 316 sleep quality 56 cleanliness 59 amenities 180 food 81 recommendation 121 rest 306 Total 1541 Table 2: Distribution of aspects in the data. Dataset and Annotation The dataset we created consists of 13559 hotel reviews from TripAdvisor.com.6 Since our modeling is performed on the EDU l</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008a. A joint model of text and aspect ratings for sentiment summariza1638 tion. In Proceedings ofACL, pages 308–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>112--120</pages>
<contexts>
<context position="2008" citStr="Titov and McDonald, 2008" startWordPosition="293" endWordPosition="296">the wheat from the chaff, it is necessary to structure the available information. In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’. Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (Hu and Liu, 2004), a task with many practical applications. Aspect-based summarization is an active research area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect for a fragment of text. For instance, in the following example taken from a TripAdvisor1 review: Example 1. The room was nice but let’s not talk about the </context>
<context position="10302" citStr="Titov and McDonald, 2008" startWordPosition="1630" endWordPosition="1633"> then an extension needed to encode discourse information. The formal generative story is presented in Figure 1: the red fragments correspond to the discourse modeling component. In order to obtain the generative story for the discourse-agnostic model, they simply need to be ignored. 3.1 Discourse-agnostic model In our approach we make an assumption that all the words in an EDU correspond to the same topic and sentiment level. We also assume that an overall sentiment of the document is defined, this is the only supervision we use in inducing the model. Unlike some of the previous work (e.g., (Titov and McDonald, 2008a)), we do not constrain aspectspecific sentiment to be the same across the document. We describe our discourse-agnostic model by first describing the set of corpus-level and document-level parameters, and then explain how the content of each document is generated. Drawing model parameters On the corpus level, for every topic z ∈ {1, ... , K} and every sentiment polarity level y ∈ {−1,0,+1}, we start by drawing a unigram language model 4The typical connective in this situation would be and which is highly ambiguous and can signal several traditional discourse relations. from a Dirichlet prior.</context>
<context position="19878" citStr="Titov and McDonald, 2008" startWordPosition="3232" endWordPosition="3235"> We assume that zd,s and yd,s are drawn twice; once from the document specific distribution and once from the discourse transition distributions. Under this simplification, we can easily derive the conditional probabilities for the collapsed Gibbs sampling. 5 Experiments To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e.g., classifying the overall sentiment of sentences (Titov and McDonald, 2008a; Brody and Elhadad, 2010) or even reviews (Nakagawa et al., 2010; Jo and Oh, 2011)) or evaluated solely at the sentential or even document level. Consequently, in order to evaluate our methods, we created a new dataset which will be publicly released. Aspects Frequency service 246 value 55 location 121 rooms 316 sleep quality 56 cleanliness 59 amenities 180 food 81 recommendation 121 rest 306 Total 1541 Table 2: Distribution of aspects in the data. Dataset and Annotation The dataset we created consists of 13559 hotel reviews from TripAdvisor.com.6 Since our modeling is performed on the EDU l</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008b. Modeling online reviews with multi-grain topic models. In Proceedings of WWW, pages 112–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakshit Trivedi</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Discourse connectors for latent subjectivity in sentiment analysis.</title>
<date>2013</date>
<booktitle>In In Proceedings of NAACL.</booktitle>
<contexts>
<context position="34461" citStr="Trivedi and Eisenstein (2013)" startWordPosition="5653" endWordPosition="5656">e noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation prediction for future work. They use an integer linear programmi</context>
</contexts>
<marker>Trivedi, Eisenstein, 2013</marker>
<rawString>Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse connectors for latent subjectivity in sentiment analysis. In In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Unsupervised learning of semantic orientation from a hundred-billion-word corpus.</title>
<date>2002</date>
<contexts>
<context position="2173" citStr="Turney and Littman, 2002" startWordPosition="319" endWordPosition="322"> identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’. Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (Hu and Liu, 2004), a task with many practical applications. Aspect-based summarization is an active research area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect for a fragment of text. For instance, in the following example taken from a TripAdvisor1 review: Example 1. The room was nice but let’s not talk about the view. it is difficult to deduce on the basis of local lexical features alone that the opinion about the view is negative. The clause let’s not talk about the view co</context>
</contexts>
<marker>Turney, Littman, 2002</marker>
<rawString>Peter D Turney and Michael L Littman. 2002. Unsupervised learning of semantic orientation from a hundred-billion-word corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimberly Voll</author>
<author>Maite Taboada</author>
</authors>
<title>Not all words are created equal: Extracting semantic orientation as a function of adjective relevance.</title>
<date>2007</date>
<booktitle>In Proceedings of Australian Conf. on AI.</booktitle>
<contexts>
<context position="5070" citStr="Voll and Taboada, 2007" startWordPosition="784" endWordPosition="787">orical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations informative for the sentiment analysis domain (Voll and Taboada, 2007). An alternative approach is to define a taskspecific scheme of discourse relations (Somasundaran et al., 2009). This previous work showed that task-specific discourse relations are helpful in predicting sentiment, however, in doing so they relied on gold-standard discourse annotation at test time rather than predicting it automatically or inducing it jointly with sentiment polarity. We take a different approach and induce discourse and sentiment information jointly in an unsupervised (or weakly supervised) manner. This has the advantage of not having to pre-specify a mapping from discourse cu</context>
</contexts>
<marker>Voll, Taboada, 2007</marker>
<rawString>Kimberly Voll and Maite Taboada. 2007. Not all words are created equal: Extracting semantic orientation as a function of adjective relevance. In Proceedings of Australian Conf. on AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Markus Egg</author>
<author>Valia Kordoni</author>
</authors>
<title>Discourse structure and language technology.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="32983" citStr="Webber et al., 2011" startWordPosition="5416" endWordPosition="5419">rom the Discourse model result in higher accuracy than the ones from the discourseagnostic model SentAsp both in the complete set of EDUs and the “marked” subset, results that are in line with the ones presented in Table 4. Finally, the fact that the results for the complete set of EDUs are higher than the ones for the “marked” dataset clearly suggests that the latter constitute a hard case for sentiment analysis, in which exploiting discourse signal proves to be beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sente</context>
</contexts>
<marker>Webber, Egg, Kordoni, 2011</marker>
<rawString>Bonnie Webber, Markus Egg, and Valia Kordoni. 2011. Discourse structure and language technology. Natural Language Engineering, 1(1):1–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanjun Zhou</author>
<author>Binyang Li</author>
<author>Wei Gao</author>
<author>Zhongyu Wei</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities.</title>
<date>2011</date>
<booktitle>In Proceedings EMNLP,</booktitle>
<pages>162--171</pages>
<contexts>
<context position="4401" citStr="Zhou et al., 2011" startWordPosition="681" endWordPosition="684">utational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations i</context>
<context position="34135" citStr="Zhou et al., 2011" startWordPosition="5599" endWordPosition="5602">hich, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce </context>
</contexts>
<marker>Zhou, Li, Gao, Wei, Wong, 2011</marker>
<rawString>Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, and Kam-Fai Wong. 2011. Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities. In Proceedings EMNLP, pages 162–171.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>