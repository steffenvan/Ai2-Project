<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.69244125">
FACTORING RECURSION AND DEPENDENCIES: AN ASPECT OF TREE ADJOINING GRAMMARS (TAG) AND
A COMPARISON OF SOME FORMAL PROPERTIES OF TAGS, GPSGS, PLGS, AND LPGS *
Aravind K. Joshi
Department of Computer and Information Science
</note>
<author confidence="0.659195">
R. 268 Moore School
</author>
<affiliation confidence="0.681965">
University of Pennsylvania
</affiliation>
<address confidence="0.208003">
Philadelphia, PA 19104
</address>
<sectionHeader confidence="0.861583" genericHeader="method">
I. .INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999807034482759">
During the last few years there is vigorous
activity in constructing highly constrained
grammatical systems by eliminating the
transformational component either totally or
partially. There is increasing recognition of
the fact that the entire range of dependencies
that transformational grammars in their various
incarnations have tried to account for can be
satisfactorily captured by classes of rules that
are non-transformational and at the same time
highly constrianed in terms of the classes of
grammars and languages that they define.
Two types of dependencies are especially
important: subcategorization and filler-gap
dependencies. Moreover,these dependencies can
be unbounded. One of the motivations for
transformations was to account for unbounded
dependencies. The so-called
non-transformational grammars account for the
unbounded dependencies in different ways. In a
tree-adjoining grammar (TAG), which has been
introduced earlier in (Joshi,1982),
unhoundedness is achieved by factoring the
dependencies and recursion in a novel and, we
believe, in a linguistically interesting manner.
All dependencies are defined on a finite set of
basic structures (trees) which are bounded.
Unhoundedness is then a corollary of a
particular composition operation called
adjoining. There are thus no unbounded
dependencies in a sense.
In this paper, we will first briefly
describe TAG&apos;s, which have the following
Important properties: (1) we can represent the
usual transformational relations more or less
directly in TAG&apos;s, (2) the power of TAG&apos;s is
only slightly more than that of context-free
grammars (CFG&apos;s) in what appears to be just the
right way, and (3) TAG&apos;s are powerful enough to
characterize dependencies (e.g.,
subcategorization, as in verb subcategorization,
and filler-gap dependencies, as in the case of
moved constitutents in wh-questions) which might
*GPSG: Generalized phrase structure grammar,
PLG: Phrase linking grammar, and LFG: Lexical
functional grammar.
This work is partially supported by the NSF
Grant MCS 81-07290.
be at unbounded distance and nested or crossed.
We will then compare some of the formal
properties of TAG&apos;s, GPSG&apos;s,PLG&apos;s, and LFG&apos;s, in
particular, concerning (1) the types of
languages, reflecting different patterns of
dependencies that can or cannot be generated by
the different types of grammars, (2) the degree
of free word ordering permitted by different
grammars, and (3) parsing complexity of the
different grammars.
</bodyText>
<sectionHeader confidence="0.657733" genericHeader="method">
2.TREE ADJOINING GRAMMAR(TAG)
</sectionHeader>
<bodyText confidence="0.999486133333333">
A tree adjoining grammar (TAG), C = (I,A)
consists of two finite sets of elementary trees.
The trees in I will be called the initial trees
and the trees in A, the auxiliary trees. A tree
g is an initial tree if the root node of
is labeled S and the frontier nodes are all
terminal symbols (the interior nodes are all
non-terminals). A tree is an auxiliary tree
if the root node of la is labeled by a
non-terminal, say, X, and the frontier nodes are
all terminals except one which is also labeled
X, the same label as that of the root. The node
labeled by X on the frontier will be called the
foot node of /11 . The internal nodes are
non-terminals.
</bodyText>
<sectionHeader confidence="0.425104" genericHeader="method">
= S
</sectionHeader>
<bodyText confidence="0.966784631578947">
terinsisaiz term,i1.413
As defined above, the initial trees and the
auxiliary trees are not constrained in any
manner other than as indicated above. The idea,
however, is that both the initial and the
auxiliary trees will be minimal in some sense.
An initial tree will correspond to a minimal
sentential tree (i.e., for example, without
recursing on any non-terminal) and an auxiliary
tree, with the root node and the foot node
labeled X, will correspond to a minimal
structure that must be brought into the
derivation, if one recurses on X.
* I wish to thank Bob Berwick, Tim Finin, Jean
Gallier, Gerald Gazdar, Ron Kaplan, Tony Kroch,
Bill Marsh, Mitch Marcus, Ellen Prince, Geoff
Pullum, R. Shyamasundar, Bonnie Webber, Scott
Weinstein, and Takashi Yokomori for their
valuable comments
</bodyText>
<page confidence="0.998851">
7
</page>
<bodyText confidence="0.9953226">
We will now define a composition operation
called adjoining (or adjunction) which composes
an auxiliary tree /3 with a tree X . Let y
be tree with a node labeled X and let /3 be an
auxiliary tree with the root labeled X also.
Note that ft must have,by definition, a node
(and only one)labeled X on the frontier.
Adjoining can now be defined as follows. If /3
is adjoining to 71 at the node n then the
resulting tree e is as shown in Fig.l.
</bodyText>
<construct confidence="0.645499777777778">
FIG, 1.
The tree t dominated by X in X is
excised, A3 is inserted at the node n in
and the tree t is attached to the foot node
(labeled X) of A , i.e., A is inserted or
&apos;adjoined&apos; to the node n in y pushing t
downwards. Note that adjoining is not a
substitution operation in the usual sense.
Example 2.1: Let G = (I,A) be a TAG where
</construct>
<figure confidence="0.961028">
a-r
b
■
b
</figure>
<equation confidence="0.2284012">
df) 0)
PI= PZ Nt= /1
o‘I /Is
0..
Is o.. 1b
</equation>
<bodyText confidence="0.466662666666667">
The root node and the foot node of each
auxiliary tree is circled for convenience. Let
us look at some derivations in G.
A3 will be adjoined to X&apos;0 at the
indicated node in 4 . The resulting tree
is then Xi
</bodyText>
<figure confidence="0.969086375">
S — 1-
Xes (:411, r•-• 3 /
b .5 0.
CIL ba. -r
T
&amp;quot;a.-•-4-z_. las
b &apos;
b
</figure>
<subsectionHeader confidence="0.302639">
We can continue the derivation by
</subsectionHeader>
<bodyText confidence="0.5254245">
adjoining, say Att, at S as indicated in .
The resulting tree 1&amp;quot;2 is then
</bodyText>
<equation confidence="0.447066636363636">
.Y2 =
./
-r 12
■0.
61- -r Is •
1114
%, •
o
— I
r
GL b
</equation>
<bodyText confidence="0.989073642857143">
Note that 1.0 is an initial tree, a
sentential tree. The derived trees •11 and Y2
are also sentential trees.
We will now define
T(G): The set of all trees derived in G
starting from the initial trees in I. This set
will be called the tree setof G.
L(G): The set of all terminal strings of
the trees in T(G). This set will be called the
string language(or language) of G.
The relationship between TAG&apos;s CFG&apos;s and
the corresponding string languages can be
summarized as follows (Joshi, Levy, and
Takahashi, 1975).
</bodyText>
<construct confidence="0.715639">
Theorem 2.1: For every CFG, G&apos;, there is
an equivalent TAG, G, both weakly and strongly.
Theorem 2.2: For every TAG, G, one of the
</construct>
<bodyText confidence="0.879341166666667">
following statements holds:
(a)there is a cfg, G&apos;, that is both weakly
and strongly equivalent to G,
(b)there is a cfg,G&apos;, that is weakly
equivalent to G but not strongly equivalent to
G, or
</bodyText>
<listItem confidence="0.649355">
(3) there is no cfg, G&apos;, that is weakly
equivalent to G.
</listItem>
<page confidence="0.996041">
8
</page>
<bodyText confidence="0.80039275">
Parts (a) and (c) appear in (Joshi, Levy,
and Takahashi, 1975). Part (b) is implicit in
that paper, but it is important to state it
explicitly as we have done here. For the TAG,
G, in Example 2.1, it can be shown that there is
a CFG, G&apos;, such that G&apos; is both weakly and
strongly equivalent to G. Examples 2.2 and 2.3
below illustrate parts (b) and (c) respectively.
</bodyText>
<figure confidence="0.9661731875">
Example 2.2: Let G ■ (I,A) be a TAG where
D s 7r
A
) y
-r
Some derivations in G.
12: tat
S tts
5
13&amp;quot; /
&lt;Pt
fiz sai L.
-r
b 41..1 incif oalMa 1.&amp;quot; 1(2.
a
.5
</figure>
<subsectionHeader confidence="0.411833">
Clearly,
</subsectionHeader>
<bodyText confidence="0.982058166666667">
Clearly, L(G)-L■ { b&amp;quot;/ n Of, which
is a cfl. Thus there must exist a CFG, G&apos;,
which is at least weakly equivalent to G. It
can be shown however that there is no CFG, G&apos;,
which is strongly equivalent to G,i.e.,
T(G)=T(G&apos;). This follows from the fact that
T(G), the tree set of G, is
&apos;non-recognizable&apos;,i.e., there is no finite
state bottom to top automaton that can recognize
precisely T(G). Thus a TAG may generate a cfl,
yet assign structural descriptions to the
strings that cannot be assigned by any CFG.
</bodyText>
<figure confidence="0.803113125">
Example 2.3: Let G = (I,A) be a TAG where
2 • ec = s
&apos; 1
a
A a : S
/1
O
5 c
</figure>
<bodyText confidence="0.983133681818182">
It can be shown that L(G) ■ LI ■ { w e cm/
n 01, w is a string of a&apos;s and b&apos;s such that
(I) the number of a&apos;s = the number of b&apos;s and
. (2) for any initial substring of w, the number
of a&apos;s at the number of b&apos;s./
LI can be characterized as follows. We
start with the language L ■ { (ba)me c&amp;quot;/ n3 0
/. LI is then obtained by taking strings in L
and moving (dislocating) some a&apos;s to the left.
It can be shown that LI is a strictly
context-sensitive language (csl), thus there can
be no CFG that is weakly equivalent to G.
TAG&apos;s have more power than CFG&apos;s, however,
the extra power is quite limited. The language
LI has equal number of a&apos;s ,b&apos;s nad c&apos;s;
however, the a&apos;s and b&apos;s are mixed in a certain
way. The Language L2 ={a&amp;quot;lare cn/ n 01 is
similar to LI, except that all a&apos;s come before
all b&apos;s. TAG&apos;s are not powerful to generate L2.
The so-called copy lnguage L3 = {w e a /w e{a,b}P
also cannot be generated by a TAG.
The fact that TAG&apos;s cannot generate L2 and
L3 is important, because it shows that TAG&apos;s are
only slightly more powerful than CFG&apos;s. The way
TAG&apos;s acquire this power is linguistically
significant. With some modifications of TAG&apos;s
or rather the operation of adjoining, which is
linguistically motivated, it is possible to
generate L2 and L3, but only in some special
ways. (This modification consists of allowing
for the possibility for checking left-right tree
context(in terms of A proper analysis) as well
as top-bottom tree context (in terms of
domination) around the node at which adjunction
is made. This is the notion of local
constraints in (Joshi and Levy,1961)1. Thus L2
and 1.3 in some ways characterize *the limiting
cases of context-sensitivity that can be
achieved by TAG&apos;s and TAG&apos;s with local
constraints.
In (Joshi,Levy, and Takahashi,1975) it is
also shown that
CFL&apos;s C TAL&apos;s C IL&apos;s C CSL&apos;s.
where IL&apos;s denotes indexed languages.
</bodyText>
<equation confidence="0.979161714285714">
Sro -
Yo 0‹
e. I&amp;quot;-T._
t i, ,&apos;
e.
otajoinea t4i+e
imitis..tesi e. S We
</equation>
<page confidence="0.841346">
9
</page>
<bodyText confidence="0.994502230769231">
3. We will now consider TAGS with links.
The elementary trees (initial and auxiliary
trees) are the appropriate domains 63r
characterizing certain dependencies. The domain
of the dependency is defined by the elementary
tree itself. However, the dependency can be
characterized explicitly by introducing a
special relationship between certain specified
pairs of nodes of an elementary tree. This
relationship is pictorially exhibited by an arc
(a dotted line) from one node to the other. For
example, in the tree below, the nodes labeled 13
and Q are linked,
</bodyText>
<figure confidence="0.864025764705882">
A
/
c
/N
c a:F
/
f
In the APPENDIX we have given some examples
to show how certain sentences could be deirved
In a TAG.
Example 2.4: Let G e (I,A) be a TAG with
links where
: (41 -5
As let 2 $
cpt T
I
I 5
</figure>
<subsectionHeader confidence="0.361703">
Some derivations in G:
</subsectionHeader>
<bodyText confidence="0.999361432432432">
We will require the following conditions to
hold for a link in an elementary tree. If a
node nl is linked to a node n2 then (1) n2
c—commands n1 and (2) nl dominates a null string
(or a terminal symbol in the non—linguistic
formal grammar examples).
The notion of a link introduced here is
closely related to that of Peters and Ritchie
(1982).
A TAG with links is a TAG where some of the
elementary trees may have links as defined
above. Henceforth, we may often refer to a TAG
with links as just a TAG. Links are defined on
the elementary trees. However, the important
idea is that the composition operation of
adioining will preserve the links. Links
defined on the elementary trees may become
stretched as the derivation proceeds.
In a TAG the dependencies are defined on
the elementary trees(which are bounded) and
these dependencies are then preserved by the
adjoining(recursive) operation. This is how
recursion and dependencies are factored in a
TAG. This is in contrast to transformational
grammars (TG) where recursion is defined in the
base and the transformations essentially carry
out the checking of the dependencies. The PLG&apos;s
and LFG&apos;s share this aspect of TG,i.e.,
recursion builds up a set of structures, some of
which are filtered out by transformations in a
TG, by the constraints on linking in a PLG, and
by the constraints introduced via functional
structures in LFG. In a GPSG on the other hand,
recursion and the checking of the dependencies
go hand in hand in a sense. In a TAG,
dependencies are defined initially on bounded
structures and recursion simply preserves them.
</bodyText>
<figure confidence="0.834731692307692">
ove044 sv■ot
crested
e 6
dependencl&apos;es)
/
T
5
0. os. e
( s teal ettnol enact)
crq v.
Ara..
$
%.../
</figure>
<page confidence="0.953272">
10
</page>
<bodyText confidence="0.999333042857143">
At and/2 each have one link. )(land
show how the linking is preserved in
adjoining. In f3 one of the links is
stretched. It should be clear now, how, in
general, the links will be preserved during the
derivation. We note in this example that in XL
the dependencies between the a&apos;s and the b&apos;s as
reflected in the terminal string are properly
nested, while in 33 two of them are properly
nested, and the third one is cross-serial and it
is crossed with respect to the nested ones. The
two elementary trees 01 and /112 have only one
link each. The nestings and crossings in Yx
and Y3 are the result of adjoining. There are
two points to note here: (1) TAG&apos;s with links
can characterize certain cross-serial
dependencies as well as, of course, nested
dependencies. (2) The cross-serial dependencies
as well as the nested dependencies arise as a
result of adjoining. But this is not the only
way they can arise. It is possible to have two
links in an elementary tree which represent
crossed or nested dependencies, which will then
be preserved during the derivation.
It is clear from Example 2.4 that the
string language of TAG with links is not
affected by the links. Thus if G is a TAG with
links. Then L(G).L(G&apos;) where G&apos; is a TAG which
is obtained from G by removing all the links in
the elementary trees of G. The links do not
affect the weak generative capacity. However,
they make certain aspects of the structural
description explicit, which is implicit in the
TAG without the links.
TAG&apos;s (or TAL&apos;s) also have the following
three important properties:
(1) Limited cross-serial dependencies:
Although TAG&apos;s permit cross-serial dependencies,
these are restricted. The restriction is that
if there are two sets of crossing dependencies,
then they must be either disjoint or one of them
must be properly nested inside the other.
Hence, languages such as the double copy
language, L4 . (wewew/wE fa,bel or L5
(anb1%edne4/ n 11 cannot be generated by
TAG&apos;s. For details, see (Joshi,1983).
(2)Constant growth property: In a TAG,G,at
each step of the derivation, we have a
sentential tree with the terminal string which
Is a string in L(G). As we adjoin an auxiliary
tree, we augment the length of the terminal
string by the length of the terminal string of
A (not counting the single non-terminal symbol
in the frontier of A ).Thus for any string, w,
of L(G), we have
where wtis the terminal string of some
Initial tree and wol :„S is m, the terminal
string of the 1-th auxiliary tree, assuming
there are m auxiliary trees. Thus w is a linear
combination of the length of the terminal string
of some initial tree and the lengths of the
terminal strings of the auxiliary trees. ThP
constant growth property severely restricts the
class of languages generated by TAG&apos;s.
Hence,lfnguages such as L6 = { as&amp;quot; / n 3. 1} or
L8 ={an In 1} cannot be generated by TAG&apos;s.
(3)Polynomial parsing:TAL&apos;s can be parsed
in time 0(n4 )(Joshi and Yokomori, 1983).
Whether or not an 0(n3 ) algorithm exists for
TAL&apos;s is not known at present.
</bodyText>
<listItem confidence="0.543406">
3. A COMPARISION OF GPSG&apos;s,TAG&apos;s,PFG&apos;s,and
LFG&apos;s WITH RESPECT TO SOME OF THEIR FORMAL
PROPERTIES
</listItem>
<bodyText confidence="0.978621566666667">
TABLE 1 lists (i) a set of languages
reflecting different patterns of dependencies
that can or cannot be generated by the different
types of grammars, and (10 the three properties
just mentioned above.
As regards the degree of free word order
permitted by each grammar, the languages
1,2,3,4,5, and 6 in TABLE 1 give some 1th:a of
the degree of freedom. The language in 3 in
TABLE 1 is the extreme case where the a&apos;s,
b&apos;s,and c&apos;s can he any order, as long as the
number of a&apos;s =the number of b&apos;s=the number of
c&apos;s. GPSG;and TAG&apos;s cannot generate this
language (although for TAG&apos;s a proof is not in
hand yet). LFG&apos;s can generate this language.
In a TAG for each elementary tree, we can
add more elementary trees, systematically
generated from the given tree to provide
additional freedom of word order (in a somewhat
similar fashion as in (Pullum,1982)). Since the
adjoining operation in a TAG gives some
additional power to a TAG beyond that of a CFG,
this device of augmenting the set of elementary
trees should give more freedom, for example, by
allowing some limited scrambling of an item
outside of the constituent it belongs Co. Even
then a TAG does not seem to be capable of
generating the language in 3 in TABLE 1. Thus
there is extra freedom but it is quite limited.
I Wk. I -t attWetI 4- (.42.IW11 + • • • +
</bodyText>
<page confidence="0.997748">
11
</page>
<tableCaption confidence="0.727305">
TABLE 1
</tableCaption>
<table confidence="0.978974875">
GPSG TAG PLG LFG
(and CFG) (with or
without local
constraints)
no yes yes yes
no yes yes yes
no no(?) yes yes
no no yes yes
no yes no(?) yes(?)
no yes yes(?) yes(?)
no yes no yes
no yes no yes
no no no yes
no yes yes(?) yes
no no ? yes
no no no(?) ?
no yes ? yes(?)
no no no(?) yes
no no no(?) yes
no yes ? no(?)
yes yes yes(?) no
yes yes ? no(?)
Notation: ?: answer unknown to the author. yes(?): conjectured yes
no(?): conjectured no.
</table>
<figure confidence="0.572092342857143">
1. Language obtained by
starting with
Lv{ (ba )11 c&amp;quot; in a 1} and
then dislocating some a&apos;s
to the left.
2. Same as 1 above except
that the dislocated a&apos;s are
to the left of all b&apos;s.
3. Lv(w / w is string of
equal number of a&apos;s,b&apos;s and
c&apos;s but mixed in any order}
4. Lv{x 2/y/ nal, x,y are
strings of a&apos;s and b&apos;s such that
the number of a&apos;s in x and y v
the number of b&apos;s in x and yv n}
5. Same as above except that the
length of x v length of y.
6. L■fw cIA / nil, w is string of
a&apos;s and b&apos;s and the number of a&apos;s
in w v the number of b&apos;s in w v n}
7. Lv{av% b&amp;quot; el /n al}
8. Lv(an tr CI d /n1}
9. L-ia bC&amp;quot; d&amp;quot; /tia 11
10. Lv {w w/ w is string
of a&apos;s and b&apos;s}(copy language)
11. Lviw w w/ w is string of
a&apos;s and b&apos;s1(double copy language)
12. 1.■(a&amp;quot; cl&amp;quot; b&amp;quot; di&amp;quot; /m11,n)}1}
13. Lvfa&amp;quot; til c/) /n al, p f n1
14. Lv(al&amp;quot; inv11
15. Lvfanz /n ) 11
16. Limited cross-serial
dependencies.
17. Constant growth property
18. Polynomial parsing
</figure>
<page confidence="0.968529">
12
</page>
<sectionHeader confidence="0.940474" genericHeader="method">
REFERENCES
</sectionHeader>
<reference confidence="0.93132796875">
(1] Gazdar,G.,&amp;quot;Phrase structure grammars&amp;quot;
in The Nature of Syntactic Representations(eds.
P. Jacobson and G.K. Pullum),D. Reidel,
Dordrecht, (to appear).
[2] Joshi, A.K. and Levy, L.S.,&amp;quot;Phrase
structure trees bear more fruit than you would
have thought&amp;quot;, AJCL, 1982.
(3] Joshi, A.K., Levy, L.S., and Takahashi,
M.,&amp;quot;Tree adjunct grammars&amp;quot;, Journal of the
Computer and System Sciences,I975.
[4] Joshi, A.K.,&amp;quot;How much
context—sensitivity is required to provide
adequate structural descriptions ?&amp;quot;, in Natural
language processing: Psycholinguistic,
Theoretical, and Computational Perseptives,
(eds. Dowty, D., Karttunen, L., and Zwicky,
A.), Cambridge University Press, (to appear).
[51 Joshi, A.K. and Yokomori, T.,&amp;quot;Parsing
of tree adjoining grammars&amp;quot;, Tech. Rep.
Department of Computer and Information Science,
University of Pennsylvania,I983.
[6] Joshi, A.K. and Kroch, T., &amp;quot;Linguistic
significance of TAG&apos;s&amp;quot; (tentative title),
forthcoming.
(8) Peters, S. and Ritchie, R.W., &amp;quot;Phrase
linking grammars&amp;quot;,Tech. Rep. University of
Texas at Austin, Department of Linguistics,
1982.
[9] Pullum, G.K.,&amp;quot;Free word order and
phrase structure rules&amp;quot;, in Proceeding of NELS
I2(eds. Pustejovsky, J. and Sells, P.),
Amherst, MA, 1982.
</reference>
<sectionHeader confidence="0.526293" genericHeader="method">
APPENDIX
</sectionHeader>
<bodyText confidence="0.99975302">
We will give here some examples to show how
certain sentences could be derived in a TAG.
For further details about this TAG and its
linguistic relevance, see (Joshi,1983 and Joshi
and Kroch, forthcoming). Only the releva”,
trees of the TAG, G,=(I,A) are shown below. The
following points are worth noting: (I)tn a TAG
the derivation starts with an initial tree. The
appropriate lexical insertions are made for the
Initial tree and the corresponding constraints
as specified by the lexicon can be checked
(e.g., agreement and subcategorization). Then
as the derivation proceeds, as each auxiliary
tree is brought into the derivation, the
appropriate lexical items are inserted and the
constraints checked. Thus in a TAG, lexical
insertion goes hand in hand with the derivation.
(2) Each one of the two finite sets, I and A can
be quite large, but these sets need not be
explicitely listed. The trees in I roughly
correspond to all the &apos;minimal&apos; sentences
corresponding to different subcategorization
frames together with the &apos;transforms&apos; of these
sentences. We could , of course, provide rules
for obtaining the trees in I from a given subset
of I. These rules achieve the effect of
conventional transformational rules, however,
these rules can be formulated not as the usual
transformational rules but directly as tree
rewriting rules, since both the domains and the
co—domains of the rules are finite.
Introduction of links can be considered as a
part of this rewriting. In any case, these
rules will be abbreviatory in the sense that
they will generate only finite sets of trees.
Their adoption will be only a matter of
convenience and does not affect the TAG in any
essential manner. The set of auxiliary trees is
also finite. Again these trees could themselves
be &apos;derived&apos; from the corresponding trees in I
by introducing appropriate tree rewriting rules.
Again these rules will be abbreviatory only as
discussed above. It is in this sense that the
trees in I and A capture the usual
transformational relations more or less
directly.
Some derivations:
(One girl who met Bill is a senior.
We start with the initial tree 0(/ with the
appropriate lexical insertions.
</bodyText>
<figure confidence="0.8131873">
5
t.JP VP
/ /
v
J&gt;ET ii
, I /
I is ) rf N
11.e. 54.1
04 Stneer
&amp;quot;TI•e
</figure>
<subsectionHeader confidence="0.203102">
Stri 0 v&amp;quot;
</subsectionHeader>
<bodyText confidence="0.8335358">
[7] Kaplan R. and Bresnan J.W., &amp;quot;Lexical
functional grammar—a formal system for
grammatical representation&amp;quot;, in The Mental
Representation of Grammatical Relatic7WW(eT.
. Bresnan, J.), MIT Press, 1983.
</bodyText>
<page confidence="0.990317">
13
</page>
<figure confidence="0.986687123287671">
(3)Who did John persuade Bill to invite ?
Adjoining pl (with the appropriate lexical
insertions) to c{1 at the indicated node in ,
we obtain h.
Bill is a seNar
(2)John persuaded Bill to invite Mary.
■
irwit•
Mow.,
9120 tv■ 1,04%te. rtthrz
Adjoining Ac to yi at the indicated node
in yd., we obtain Yz .
v417)
Fe P
I IN
PRo Tu \If
/*\
V
.,, I -: 1
•N inviblI:e&apos;
&apos;`....- - ..
1.46 Pe o to iv■Vite
Adjoining ge to Yj at the indicated node
In yt , we obtain
P4 0
V...\
7/0 N 9 &apos;If
/
NP 0
446. t
PtYjimmie
aid 34 WA lotrs‘...ole
Y3.
I \ . i \ f /&amp;quot;••• gip_
I- i r4 v N P S
I 1 I / -.-----■ &apos;&apos;
t
ww;
■ . So ■ 1.
, •pc --,..---,......
-S._. _ _ _ _ • , d
_...-.....
. , % 1 To IN,_
s
S. P■tr.mula I ss pRo v ,1,1 p
—
--s - _
PIP NJ e
S. — ,
Ni
ivonio ;
•••• •
%Ji&amp;quot;) 70^% parly,44 BM +0 i&apos;AV% t 2.
%As met
•
yi 0(25 z
5 — —
,-
I I• / I&apos;s&amp;quot;&apos; 3 --)
..N v &apos;JP &apos;
I• */ --... _
I I
CVO I
I&apos; / tj :We V i I)\
1
I
-is VIP
\
ja(V.Itoodoot Pit°
• v 14r
itwitt
Mary
w&amp;quot; 1,tr•ouuttsi. gilt Ivuht tiorz
</figure>
<page confidence="0.993397">
14
</page>
<bodyText confidence="0.9582015">
Note the link in Yy is &apos;preserved&apos; in &apos;6
it is &apos;stretched&apos; resulting in the so—called
unbounded dependency.
(4)John tried to please Mary.
</bodyText>
<figure confidence="0.964286125">
e425.
NP VP
/
two To P
/
v NP
0/eare P4,
146% Y*4
</figure>
<bodyText confidence="0.960523666666667">
fRo to 1,It owe r4or.1
Adjoining $7 to j at the indicated node
in Yi we obtain Ys.
On the other hand
(5)John seems to like Mary. could be
derived as follows. We will start with .L4-.
</bodyText>
<figure confidence="0.978913176470588">
ol214;
/
INP NIP
(`■
)4 -r VP
MI)&amp;quot;, NP
I
rj
Mari
&apos;Zak.&amp;quot; to like Mo,rj
Adjoining Auto VI at the indicated node
in Yj , we obtain 4. .
..1(2
•%,&amp;quot; *vied. 5 N v—fa&apos; P
/*&amp;quot;..
NI I v
`G. — &apos;
/37 ,
: v
-reern.1
V Nr
.p4
70&amp;quot;4Z f.4P r
\
like P141,&amp;quot;_,
Yoldsmz
IA.% ec4 PRO to Ivieooto. Mary
/ `112 &apos;
I I
NJ S 1
- /..%•••.
-ro
flto /N.
N? 11.12M1 tu
</figure>
<page confidence="0.77776">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000463">
<title confidence="0.99895">FACTORING RECURSION AND DEPENDENCIES: AN ASPECT OF TREE ADJOINING GRAMMARS (TAG) AND A COMPARISON OF SOME FORMAL PROPERTIES OF TAGS, GPSGS, PLGS, AND LPGS *</title>
<author confidence="0.999991">Aravind K Joshi</author>
<affiliation confidence="0.99949">Department of Computer and Information Science</affiliation>
<address confidence="0.494379">R. 268 Moore School</address>
<affiliation confidence="0.999729">University of Pennsylvania</affiliation>
<address confidence="0.998647">Philadelphia, PA 19104</address>
<abstract confidence="0.997695652173913">During the last few years there is vigorous activity in constructing highly constrained grammatical systems by eliminating the transformational component either totally or partially. There is increasing recognition of the fact that the entire range of dependencies that transformational grammars in their various incarnations have tried to account for can be satisfactorily captured by classes of rules that non-transformationaland at the same time highly constrianed in terms of the classes of grammars and languages that they define. Two types of dependencies are especially important: subcategorization and filler-gap dependencies. Moreover,these dependencies can be unbounded. One of the motivations for transformations was to account for unbounded dependencies. The so-called non-transformational grammars account for the unbounded dependencies in different ways. In a tree-adjoining grammar (TAG), which has been introduced earlier in (Joshi,1982), unhoundedness is achieved by factoring the dependencies and recursion in a novel and, we believe, in a linguistically interesting manner. All dependencies are defined on a finite set of basic structures (trees) which are bounded. Unhoundedness is then a corollary of a particular composition operation called adjoining.There are thus no unbounded dependencies in a sense. In this paper, we will first briefly describe TAG&apos;s, which have the following properties: can represent the usual transformational relations more or less directly in TAG&apos;s, (2) the power of TAG&apos;s is only slightly more than that of context-free in appears to be just the right way, and (3) TAG&apos;s are powerful enough to characterize dependencies (e.g., subcategorization, as in verb subcategorization, and filler-gap dependencies, as in the case of moved constitutents in wh-questions) which might phrase structure grammar, PLG: Phrase linking grammar, and LFG: Lexical functional grammar.</abstract>
<note confidence="0.9796095">This work is partially supported by the NSF Grant MCS 81-07290.</note>
<abstract confidence="0.986536564102564">be at unbounded distance and nested or crossed. We will then compare some of the formal properties of TAG&apos;s, GPSG&apos;s,PLG&apos;s, and LFG&apos;s, in particular, concerning (1) the types of languages, reflecting different patterns of dependencies that can or cannot be generated by the different types of grammars, (2) the degree of free word ordering permitted by different and (3) parsing complexity the different grammars. GRAMMAR(TAG) A tree adjoining grammar (TAG), C = (I,A) of two finite sets of elementarytrees. trees in I will be called the trees the trees in A, the auxiliarytrees. A tree is an initialtree if the root node of is labeled S and the frontier nodes are all terminal symbols (the interior nodes are all non-terminals). A tree is an auxiliary tree the root node of labeled by a non-terminal, say, X, and the frontier nodes are all terminals except one which is also labeled X, the same label as that of the root. The node by X on the frontier will be the node of The nodes are non-terminals. terinsisaiz term,i1.413 defined above, the trees and the auxiliary trees are not constrained in any other than as indicated The however, is that both the initial and the trees will be minimalin some sense. An initial tree will correspond to a minimal sentential tree (i.e., for example, without recursing on any non-terminal) and an auxiliary tree, with the root node and the foot node labeled X, will correspond to a minimal structure that must be brought into the derivation, if one recurses on X.</abstract>
<author confidence="0.83057925">I thank Bob Berwick</author>
<author confidence="0.83057925">Tim Finin</author>
<author confidence="0.83057925">Gerald Gazdar Gallier</author>
<author confidence="0.83057925">Ron Kaplan</author>
<author confidence="0.83057925">Tony Kroch</author>
<author confidence="0.83057925">Bill Marsh</author>
<author confidence="0.83057925">Mitch Marcus</author>
<author confidence="0.83057925">Ellen Prince</author>
<author confidence="0.83057925">Geoff Pullum</author>
<author confidence="0.83057925">R Shyamasundar</author>
<author confidence="0.83057925">Bonnie Webber</author>
<author confidence="0.83057925">Scott</author>
<abstract confidence="0.997478492113564">Weinstein, and Takashi Yokomori for their valuable comments 7 We will now define a composition operation adjoining(or adjunction) which composes auxiliary tree a tree X . Let tree with a node labeled X and let an auxiliary tree with the root labeled X also. that ftmust have,by definition, a node (and only one)labeled X on the frontier. Adjoining can now be defined as follows. If /3 is adjoining to 71 at the node n then the resulting tree e is as shown in Fig.l. FIG, 1. The tree t dominated by X in X is excised, A3 is inserted at the node n in and the tree t is attached to the foot node X) of i.e., inserted or to the node n in t downwards. Note that adjoining is not a substitution operation in the usual sense. Example 2.1: Let G = (I,A) be a TAG where a-r b ■ b /1 0.. 1b The root node and the foot node of each auxiliary tree is circled for convenience. Let us look at some derivations in G. will be adjoined to X&apos;0 at the indicated node in 4 . The resulting tree then r•-• 3 / T b &apos; b We can continue the derivation by say S as indicated in . resulting tree is then = ./ ■0. %, • o — I r that is an initial tree, a tree. The derived trees and are also sentential trees. We will now define T(G): The set of all trees derived in G starting from the initial trees in I. This set will be called the tree setof G. L(G): The set of all terminal strings of the trees in T(G). This set will be called the language(or language)of G. The relationship between TAG&apos;s CFG&apos;s and the corresponding string languages can be summarized as follows (Joshi, Levy, and Takahashi, 1975). Theorem 2.1: For every CFG, G&apos;, there is an equivalent TAG, G, both weakly and strongly. Theorem 2.2: For every TAG, G, one of the following statements holds: (a)there is a cfg, G&apos;, that is both weakly and strongly equivalent to G, (b)there is a cfg,G&apos;, that is weakly equivalent to G but not strongly equivalent to G, or (3) there is no cfg, G&apos;, that is weakly equivalent to G. 8 Parts (a) and (c) appear in (Joshi, Levy, and Takahashi, 1975). Part (b) is implicit in that paper, but it is important to state it explicitly as we have done here. For the TAG, G, in Example 2.1, it can be shown that there is a CFG, G&apos;, such that G&apos; is both weakly and strongly equivalent to G. Examples 2.2 and 2.3 below illustrate parts (b) and (c) respectively. Example 2.2: Let G ■ (I,A) be a TAG where 7r A ) y Some derivations in G. 5 / &lt;Pt fiz L. -r 41..1 oalMa a .5 Clearly, Clearly, L(G)-L■ { n Of, which is a cfl. Thus there must exist a CFG, G&apos;, which is at least weakly equivalent to G. It can be shown however that there is no CFG, G&apos;, which is strongly equivalent to G,i.e., T(G)=T(G&apos;). This follows from the fact that T(G), the tree set of G, is there is no finite state bottom to top automaton that can recognize Thus a TAG may generate a cfl, yet assign structural descriptions to the strings that cannot be assigned by any CFG. Let G = (I,A) be a TAG where • ec s &apos; 1 a S /1 O 5 c can be shown that L(G) ■ LI ■ { e 01, w is a string of and b&apos;s such that the number a&apos;s = the number of (2) for any initial of w, the number a&apos;s at the number b&apos;s./ be characterized as follows. We with the language L ■ { n3 0 /. LI is then obtained by taking strings in L and moving (dislocating) some a&apos;s to the left. It can be shown that LI is a strictly context-sensitive language (csl), thus there can be no CFG that is weakly equivalent to G. TAG&apos;s have more power than CFG&apos;s, however, the extra power is quite limited. The language LI has equal number of a&apos;s ,b&apos;s nad c&apos;s; however, the a&apos;s and b&apos;s are mixed in a certain way. The Language L2 ={a&amp;quot;lare cn/ n 01 is similar to LI, except that all a&apos;s come before all b&apos;s. TAG&apos;s are not powerful to generate L2. The so-called copy lnguage L3 = {w e a /w e{a,b}P also cannot be generated by a TAG. The fact that TAG&apos;s cannot generate L2 and L3 is important, because it shows that TAG&apos;s are only slightly more powerful than CFG&apos;s. The way TAG&apos;s acquire this power is linguistically significant. With some modifications of TAG&apos;s or rather the operation of adjoining, which is linguistically motivated, it is possible to generate L2 and L3, but only in some special ways. (This modification consists of allowing for the possibility for checking left-right tree terms of analysis) as well as top-bottom tree context (in terms of domination) around the node at which adjunction made. This is the notion of constraintsin (Joshi and Levy,1961)1. Thus L2 and 1.3 in some ways characterize *the limiting cases of context-sensitivity that can be achieved by TAG&apos;s and TAG&apos;s with local constraints. In (Joshi,Levy, and Takahashi,1975) it is also shown that C CSL&apos;s. where IL&apos;s denotes indexed languages. Yo 0‹ i, e. e. 9 will now consider TAGS with links. The elementary trees (initial and auxiliary trees) are the appropriate domains 63r characterizing certain dependencies. The domain of the dependency is defined by the elementary tree itself. However, the dependency can be characterized explicitly by introducing a special relationship between certain specified pairs of nodes of an elementary tree. This relationship is pictorially exhibited by an arc (a dotted line) from one node to the other. For example, in the tree below, the nodes labeled 13 and Q are linked, A / c /N c a:F / f In the APPENDIX we have given some examples to show how certain sentences could be deirved In a TAG. Example 2.4: Let G e (I,A) be a TAG with links where -5 2 T I 5 derivations in We will require the following conditions to hold for a link in an elementary tree. If a node nl is linked to a node n2 then (1) n2 c—commands n1 and (2) nl dominates a null string (or a terminal symbol in the non—linguistic formal grammar examples). The notion of a link introduced here is closely related to that of Peters and Ritchie (1982). A TAG with links is a TAG where some of the elementary trees may have links as defined above. Henceforth, we may often refer to a TAG with links as just a TAG. Links are defined on the elementary trees. However, the important idea is that the composition operation of will preservethe links. Links defined on the elementary trees may become stretchedas the derivation proceeds. In a TAG the dependencies are defined on the elementary trees(which are bounded) and these dependencies are then preserved by the adjoining(recursive) operation. This is how recursion and dependencies are factored in a TAG. This is in contrast to transformational grammars (TG) where recursion is defined in the base and the transformations essentially carry out the checking of the dependencies. The PLG&apos;s and LFG&apos;s share this aspect of TG,i.e., recursion builds up a set of structures, some of which are filtered out by transformations in a TG, by the constraints on linking in a PLG, and by the constraints introduced via functional structures in LFG. In a GPSG on the other hand, recursion and the checking of the dependencies go hand in hand in a sense. In a TAG, dependencies are defined initially on bounded structures and recursion simply preserves them. sv■ot crested dependencl&apos;es) / T 5 os. s enact) crq v. Ara.. $ %.../ 10 and/2 each have one link. show how the linking is preserved in adjoining. In f3 one of the links is stretched. It should be clear now, how, in general, the links will be preserved during the We note in this example that in the dependencies between the a&apos;s and the b&apos;s as reflected in the terminal string are properly nested, while in 33 two of them are properly nested, and the third one is cross-serial and it is crossed with respect to the nested ones. The elementary trees and have only one each. The nestings and crossings in and Y3 are the result of adjoining. There are points to note here: with links can characterize certain cross-serial dependencies as well as, of course, nested cross-serial dependencies as well as the nested dependencies arise as a result of adjoining. But this is not the only way they can arise. It is possible to have two links in an elementary tree which represent crossed or nested dependencies, which will then be preserved during the derivation. It is clear from Example 2.4 that the string language of TAG with links is not affected by the links. Thus if G is a TAG with links. Then L(G).L(G&apos;) where G&apos; is a TAG which is obtained from G by removing all the links in the elementary trees of G. The links do not affect the weak generative capacity. However, they make certain aspects of the structural description explicit, which is implicit in the TAG without the links. TAG&apos;s (or TAL&apos;s) also have the following three important properties: cross-serial dependencies: Although TAG&apos;s permit cross-serial dependencies, these are restricted. The restriction is that if there are two sets of crossing dependencies, then they must be either disjoint or one of them must be properly nested inside the other. Hence, languages such as the double copy L4 . or L5 n 11 cannot be generated by TAG&apos;s. For details, see (Joshi,1983). growth property:In a TAG,G,at each step of the derivation, we have a sentential tree with the terminal string which Is a string in L(G). As we adjoin an auxiliary tree, we augment the length of the terminal string by the length of the terminal string of counting the single non-terminal symbol the frontier of for any string, w, of L(G), we have the terminal string of some Initial tree and wol :„S is m, the terminal string of the 1-th auxiliary tree, assuming there are m auxiliary trees. Thus w is a linear combination of the length of the terminal string of some initial tree and the lengths of the terminal strings of the auxiliary trees. ThP constant growth property severely restricts the class of languages generated by TAG&apos;s. such as L6 = { / n 3.1} or L8 ={an In 1} cannot be generated by TAG&apos;s. parsing:TAL&apos;scan be parsed time )(Joshi and Yokomori, 1983). or not ) for TAL&apos;s is not known at present.</abstract>
<title confidence="0.735924">A COMPARISIONOF WITH RESPECTTO SOME OF THEIR PROPERTIES</title>
<abstract confidence="0.951287072072072">TABLE 1 lists (i) a set of languages reflecting different patterns of dependencies that can or cannot be generated by the different of grammars, and three properties just mentioned above. As regards the degree of free word order permitted by each grammar, the languages 1,2,3,4,5, and 6 in TABLE 1 give some 1th:a of degree of freedom. The language in 3 is the case the a&apos;s, c&apos;s can he any as long as the number of a&apos;s =the number of b&apos;s=the number of c&apos;s. GPSG;and TAG&apos;s cannot generate this language (although for TAG&apos;s a proof is not in hand yet). LFG&apos;s can generate this language. In a TAG for each elementary tree, we can add more elementary trees, systematically generated from the given tree to provide additional freedom of word order (in a somewhat similar fashion as in (Pullum,1982)). Since the adjoining operation in a TAG gives some additional power to a TAG beyond that of a CFG, this device of augmenting the set of elementary trees should give more freedom, for example, by allowing some limited scrambling of an item outside of the constituent it belongs Co. Even a TAG does not seem to of generating the language in 3 in TABLE 1. Thus there is extra freedom but it is quite limited. I 4- + • • • + 11 TABLE 1 GPSG TAG PLG LFG (and CFG) (with or without local constraints) no yes yes yes no yes yes yes no no(?) yes yes no no yes yes no yes no(?) yes(?) no yes yes(?) yes(?) no yes no yes no yes no yes no no no yes no yes yes(?) yes no no ? yes no no no(?) ? no yes ? yes(?) no no no(?) yes no no no(?) yes no yes ? no(?) yes yes yes(?) no yes yes ? no(?) Notation: ?: answer unknown to the author. yes(?): conjectured yes no(?): conjectured no. 1. Language obtained by starting with (ba c&amp;quot; in a 1}and then dislocating some a&apos;s to the left. 2. Same as 1 above except that the dislocated a&apos;s are to the left of all b&apos;s. 3. Lv(w / w is string of equal number of a&apos;s,b&apos;s and c&apos;s but mixed in any order} Lv{x nal, x,y are strings of a&apos;s and b&apos;s such that the number of a&apos;s in x and y v the number of b&apos;s in x and yv n} 5. Same as above except that the length of x v length of y. L■fw / nil, w is string of a&apos;s and b&apos;s and the number of a&apos;s in w v the number of b&apos;s in w v n} 7. b&amp;quot; /n al} 8. tr d 9. L-ia bC&amp;quot; d&amp;quot; /tia 11 10. Lv {w w/ w is string of a&apos;s and b&apos;s}(copy language) 11. Lviw w w/ w is string of a&apos;s and b&apos;s1(double copy language) 1.■(a&amp;quot; b&amp;quot; Lvfa&amp;quot; /n al, p f n1 14. inv11 15. /n ) 11 16. Limited cross-serial dependencies. 17. Constant growth property 18. Polynomial parsing 12 REFERENCES (1] Gazdar,G.,&amp;quot;Phrase structure grammars&amp;quot; The Natureof Representations(eds. P. Jacobson and G.K. Pullum),D. Reidel, Dordrecht, (to appear). [2] Joshi, A.K. and Levy, L.S.,&amp;quot;Phrase structure trees bear more fruit than you would have thought&amp;quot;, AJCL, 1982. (3] Joshi, A.K., Levy, L.S., and Takahashi, adjunct grammars&amp;quot;, Journalof the Computerand Sciences,I975. [4] Joshi, A.K.,&amp;quot;How much context—sensitivity is required to provide structural descriptions ?&amp;quot;, in language processing: Psycholinguistic, Theoretical,and Perseptives, (eds. Dowty, D., Karttunen, L., and Zwicky, A.), Cambridge University Press, (to appear). [51 Joshi, A.K. and Yokomori, T.,&amp;quot;Parsing adjoining grammars&amp;quot;, Tech. Rep.</abstract>
<affiliation confidence="0.992016">Department of Computer and Information Science, University of Pennsylvania,I983.</affiliation>
<address confidence="0.595991">[6] Joshi, A.K. and Kroch, T., &amp;quot;Linguistic</address>
<abstract confidence="0.7970595">significance of TAG&apos;s&amp;quot; (tentative title), forthcoming.</abstract>
<note confidence="0.587905">(8) Peters, S. and Ritchie, R.W., &amp;quot;Phrase linking grammars&amp;quot;,Tech. Rep. University of Texas at Austin, Department of Linguistics, 1982. [9] Pullum, G.K.,&amp;quot;Free word order and structure rules&amp;quot;, in Proceedingof NELS I2(eds. Pustejovsky, J. and Sells, P.),</note>
<address confidence="0.509307">Amherst, MA, 1982.</address>
<abstract confidence="0.9871240625">APPENDIX We will give here some examples to show how certain sentences could be derived in a TAG. For further details about this TAG and its linguistic relevance, see (Joshi,1983 and Joshi Kroch, forthcoming). Only the of the TAG, are shown below. The following points are worth noting: (I)tn a TAG the derivation starts with an initial tree. The appropriate lexical insertions are made for the Initial tree and the corresponding constraints as specified by the lexicon can be checked (e.g., agreement and subcategorization). Then as the derivation proceeds, as each auxiliary tree is brought into the derivation, the appropriate lexical items are inserted and the constraints checked. Thus in a TAG, lexical insertion goes hand in hand with the derivation. (2) Each one of the two finite sets, I and A can be quite large, but these sets need not be explicitely listed. The trees in I roughly correspond to all the &apos;minimal&apos; sentences corresponding to different subcategorization frames together with the &apos;transforms&apos; of these sentences. We could , of course, provide rules for obtaining the trees in I from a given subset of I. These rules achieve the effect of conventional transformational rules, however, these rules can be formulated not as the usual transformational rules but directly as tree rewriting rules, since both the domains and the co—domains of the rules are finite.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>(1] Gazdar,G.,&amp;quot;Phrase structure grammars&amp;quot; in The Nature of Syntactic Representations(eds.</title>
<editor>P. Jacobson and G.K. Pullum),D. Reidel,</editor>
<location>Dordrecht,</location>
<note>(to appear).</note>
<marker></marker>
<rawString>(1] Gazdar,G.,&amp;quot;Phrase structure grammars&amp;quot; in The Nature of Syntactic Representations(eds. P. Jacobson and G.K. Pullum),D. Reidel, Dordrecht, (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Levy</author>
</authors>
<title>L.S.,&amp;quot;Phrase structure trees bear more fruit than you would have thought&amp;quot;,</title>
<date>1982</date>
<location>AJCL,</location>
<marker>Joshi, Levy, 1982</marker>
<rawString>[2] Joshi, A.K. and Levy, L.S.,&amp;quot;Phrase structure trees bear more fruit than you would have thought&amp;quot;, AJCL, 1982.</rawString>
</citation>
<citation valid="false">
<title>M.,&amp;quot;Tree adjunct grammars&amp;quot;,</title>
<journal>Journal of the Computer and System Sciences,I975.</journal>
<marker></marker>
<rawString>(3] Joshi, A.K., Levy, L.S., and Takahashi, M.,&amp;quot;Tree adjunct grammars&amp;quot;, Journal of the Computer and System Sciences,I975.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joshi</author>
</authors>
<title>A.K.,&amp;quot;How much context—sensitivity is required to provide adequate structural descriptions ?&amp;quot;,</title>
<booktitle>in Natural language processing: Psycholinguistic, Theoretical, and Computational Perseptives,</booktitle>
<editor>(eds. Dowty, D., Karttunen, L., and Zwicky, A.),</editor>
<publisher>Cambridge University Press,</publisher>
<note>(to appear).</note>
<marker>Joshi, </marker>
<rawString>[4] Joshi, A.K.,&amp;quot;How much context—sensitivity is required to provide adequate structural descriptions ?&amp;quot;, in Natural language processing: Psycholinguistic, Theoretical, and Computational Perseptives, (eds. Dowty, D., Karttunen, L., and Zwicky, A.), Cambridge University Press, (to appear).</rawString>
</citation>
<citation valid="false">
<title>T.,&amp;quot;Parsing of tree adjoining grammars&amp;quot;,</title>
<tech>Tech. Rep.</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania,I983.</institution>
<marker></marker>
<rawString>[51 Joshi, A.K. and Yokomori, T.,&amp;quot;Parsing of tree adjoining grammars&amp;quot;, Tech. Rep. Department of Computer and Information Science, University of Pennsylvania,I983.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A K Joshi</author>
<author>T Kroch</author>
</authors>
<title>Linguistic significance of TAG&apos;s&amp;quot; (tentative title), forthcoming.</title>
<marker>Joshi, Kroch, </marker>
<rawString>[6] Joshi, A.K. and Kroch, T., &amp;quot;Linguistic significance of TAG&apos;s&amp;quot; (tentative title), forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Peters</author>
<author>R W Ritchie</author>
</authors>
<title>Phrase linking grammars&amp;quot;,Tech.</title>
<date>1982</date>
<institution>Rep. University of Texas at Austin, Department of Linguistics,</institution>
<contexts>
<context position="10591" citStr="Peters and Ritchie (1982)" startWordPosition="1902" endWordPosition="1905">the tree below, the nodes labeled 13 and Q are linked, A / c /N c a:F / f In the APPENDIX we have given some examples to show how certain sentences could be deirved In a TAG. Example 2.4: Let G e (I,A) be a TAG with links where : (41 -5 As let 2 $ cpt T I I 5 Some derivations in G: We will require the following conditions to hold for a link in an elementary tree. If a node nl is linked to a node n2 then (1) n2 c—commands n1 and (2) nl dominates a null string (or a terminal symbol in the non—linguistic formal grammar examples). The notion of a link introduced here is closely related to that of Peters and Ritchie (1982). A TAG with links is a TAG where some of the elementary trees may have links as defined above. Henceforth, we may often refer to a TAG with links as just a TAG. Links are defined on the elementary trees. However, the important idea is that the composition operation of adioining will preserve the links. Links defined on the elementary trees may become stretched as the derivation proceeds. In a TAG the dependencies are defined on the elementary trees(which are bounded) and these dependencies are then preserved by the adjoining(recursive) operation. This is how recursion and dependencies are fac</context>
</contexts>
<marker>Peters, Ritchie, 1982</marker>
<rawString>(8) Peters, S. and Ritchie, R.W., &amp;quot;Phrase linking grammars&amp;quot;,Tech. Rep. University of Texas at Austin, Department of Linguistics, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pullum</author>
</authors>
<title>G.K.,&amp;quot;Free word order and phrase structure rules&amp;quot;,</title>
<date>1982</date>
<booktitle>in Proceeding of NELS I2(eds.</booktitle>
<editor>Pustejovsky, J. and Sells, P.),</editor>
<location>Amherst, MA,</location>
<marker>Pullum, 1982</marker>
<rawString>[9] Pullum, G.K.,&amp;quot;Free word order and phrase structure rules&amp;quot;, in Proceeding of NELS I2(eds. Pustejovsky, J. and Sells, P.), Amherst, MA, 1982.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>