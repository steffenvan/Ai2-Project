<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005326">
<title confidence="0.988174">
KyotoEBMT: An Example-Based Dependency-to-Dependency
Translation Framework
</title>
<author confidence="0.996827">
John Richardson† Fabien Cromières‡ Toshiaki Nakazawa‡ Sadao Kurohashi††Graduate School of Informatics, Kyoto University, Kyoto 606-8501
</author>
<affiliation confidence="0.995544">
‡Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012
</affiliation>
<email confidence="0.960945">
john@nlp.ist.i.kyoto-u.ac.jp,{fabien, nakazawa}@pa.jst.jp,
kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.979734" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998081875">
This paper introduces the Ky-
otoEBMT Example-Based Machine
Translation framework. Our system
uses a tree-to-tree approach, employing
syntactic dependency analysis for
both source and target languages
in an attempt to preserve non-local
structure. The effectiveness of our
system is maximized with online ex-
ample matching and a flexible decoder.
Evaluation demonstrates BLEU scores
competitive with state-of-the-art SMT
systems such as Moses. The current
implementation is intended to be
released as open-source in the near
future.
</bodyText>
<sectionHeader confidence="0.9963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995325">
Corpus-based approaches have become a ma-
jor focus of Machine Translation research.
We present here a fully-fledged Example-
Based Machine Translation (EBMT) plat-
form making use of both source-language
and target-language dependency structure.
This paradigm has been explored compar-
atively less, as studies on Syntactic-based
SMT/EBMT tend to focus on constituent
trees rather than dependency trees, and
on tree-to-string rather than tree-to-tree ap-
proaches. Furthermore, we employ separate
dependency parsers for each language rather
than projecting the dependencies from one lan-
guage to another, as in (Quirk et. al, 2005).
The dependency structure information is
used end-to-end: for improving the quality
of the alignment of the translation examples,
for constraining the translation rule extraction
and for guiding the decoding. We believe that
dependency structure, which considers more
than just local context, is important in order
to generate fluent and accurate translations
of complex sentences across distant language
pairs.
Our experiments focus on technical do-
main translation for Japanese-Chinese and
Japanese-English, however our implementa-
tion is applicable to any domain and language
pair for which there exist translation examples
and dependency parsers.
A further unique characteristic of our sys-
tem is that, again contrary to the majority of
similar systems, it does not rely on precompu-
tation of translation rules. Instead it matches
each input sentence to the full database of
translation examples before extracting trans-
lation rules online. This has the merit of max-
imizing the information available when creat-
ing and combining translation rules, while re-
taining the ability to produce excellent trans-
lations for input sentences similar to an exist-
ing translation example.
The system is mostly developed in C++ and
incorporates a web-based translation interface
for ease of use. The web interface (see Fig-
ure 1) also displays information useful for error
analysis such as the list of translation exam-
ples used. Experiments are facilitated through
the inclusion of a curses-based graphical in-
terface for performing tuning and evaluation.
The decoder supports multiple threads.
We are currently making preparations for
the project to be released with an open-
source license. The code will be available at
http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.
</bodyText>
<sectionHeader confidence="0.922139" genericHeader="introduction">
2 System Overview
</sectionHeader>
<bodyText confidence="0.89682825">
Figure 2 shows the basic structure of the pro-
posed translation pipeline.
The training process begins with parsing
and aligning parallel sentences from the train-
</bodyText>
<page confidence="0.990456">
79
</page>
<affiliation confidence="0.437331">
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 79–84,
Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<figureCaption confidence="0.998421">
Figure 1: A screenshot of the web interface
</figureCaption>
<bodyText confidence="0.990819407407407">
showing a Japanese-English translation. The
interface provides the source and target side
dependency tree, as well as the list of exam-
ples used with their alignments. The web in-
terface facilitates easy and intuitive error anal-
ysis, and can be used as a tool for computer-
aided translation.
ing corpus. Alignment uses a Bayesian sub-
tree alignment model based on dependency
trees. This contains a tree-based reorder-
ing model and can capture non-local reorder-
ings, which sequential word-based models of-
ten cannot handle effectively. The alignments
are then used to build an example database
(‘translation memory’) containing ‘examples’
or ‘treelets’ that form the hypotheses to be
combined during decoding.
Translation is performed by first parsing
an input sentence then searching for treelets
matching entries in the example database.
The retrieved treelets are combined by a de-
coder that optimizes a log linear model score.
The example retrieval and decoding steps are
explained in more detail in sections 3 and 4
respectively. The choice of features and the
tuning of the log linear model is described in
section 5.
</bodyText>
<figureCaption confidence="0.55063325">
Figure 3 shows the process of combining ex-
amples matching the input tree to create an
output sentence.
Figure 2: Translation pipeline. An example
</figureCaption>
<bodyText confidence="0.9949712">
database is first trained from a parallel cor-
pus. Translation is performed by the decoder,
which combines initial hypotheses generated
by the example retrieval module. Weights can
be improved with batch tuning.
</bodyText>
<sectionHeader confidence="0.779299" genericHeader="method">
3 Example retrieval and translation
hypothesis construction
</sectionHeader>
<bodyText confidence="0.999946290322581">
An important characteristic of our system is
that we do not extract and store translation
rules in advance: the alignment of translation
examples is performed offline. However, for a
given input sentence i, the steps for finding
examples partially matching i and extracting
their translation hypotheses is an online pro-
cess. This approach could be considered to be
more faithful to the original EBMT approach
advocated by Nagao (1984). It has already
been proposed for phrase-based (Callison-
Burch et al., 2005), hierarchical (Lopez, 2007),
and syntax-based (Cromières and Kurohashi,
2011) systems. It does not however, seem to
be very commonly integrated in syntax-based
MT.
This approach has several benefits. The first
is that we are not required to impose a limit
on the size of translation hypotheses. Systems
extracting rules in advance typically restrict
the size and number of extracted rules for fear
of becoming unmanageable. In particular, if
an input sentence is the same or very similar
to one of our translation examples, we will be
able to retrieve a perfect translation. A second
advantage is that we can make use of the full
context of the example to assign features and
scores to each translation hypothesis.
The main drawback of our approach is that
it can be computationally more expensive to
retrieve arbitrarily large matchings in the ex-
</bodyText>
<page confidence="0.992782">
80
</page>
<figureCaption confidence="0.983504">
Figure 3: The process of translation. The source sentence is parsed and matching subtrees from
</figureCaption>
<bodyText confidence="0.744150833333333">
the example database are retrieved. From the examples, we extract translation hypotheses than
can contain optional target words and several position for each non-terminals. For example the
translation hypothesis containing “textbook” has three possible position for the non-terminal X3
(as a left-child before “a”, as a left-child after “a” or as a right-child). The translation hypotheses
are then combined during decoding. Choice of optional words and final Non-Terminal positions
is also done during decoding.
</bodyText>
<figure confidence="0.999539135135135">
*+254 ,-.274
/294 02:4
#$%&amp;2;4
&gt;)?@# A*$$
!284
&apos;()234
2GBBA4
2F84
2F54
2F74
2F54 * 1 2F74
!
#$%&amp;
,-.
/ 0
*+
23
C%!.?,$ D!#!&amp;!+$
&apos;()
2F84 !&amp;quot; 2F84 2F84
E$ &amp;&apos;&apos;(
2F54 2F74
?!?$*+
&apos;)
#*!)+,!-&apos;)
.!/01)$
#$%#&amp;&apos;&apos;(
*$!&lt;
=0$
!
=0$
*$!&lt;
! &apos;)
#*!)+,!-&apos;)
.!/01)$
#$%#&amp;&apos;&apos;(
B@#?@# A*$$
</figure>
<figureCaption confidence="0.60081625">
ample database online than it is to match pre-
computed rules. We use the techniques de-
scribed in (Cromières and Kurohashi, 2011)
to perform this step as efficiently as possible.
</figureCaption>
<bodyText confidence="0.999831">
Once we have found an example translation
(s, t) for which s partially matches i, we pro-
ceed to extract a translation hypothesis from
it. A translation hypothesis is defined as a
generic translation rule for a part p of the in-
put sentence that is represented as a target-
language treelet, with non-terminals repre-
senting the insertion positions for the transla-
tions of other parts of the sentence. A trans-
lation hypothesis is created from a translation
example as follows:
</bodyText>
<listItem confidence="0.5399222">
1. We project the part of s that is matched
into the target side t using the alignment
of s and t. This is trivial if each word of
s and t is aligned, but this is not typi-
cally the case. Therefore our translation
</listItem>
<bodyText confidence="0.998337142857143">
hypotheses will often have some target
words/nodes marked as optionals: this
means that we will decide if they should
be added to the final translation only at
the moment of combination.
2. We insert the non-terminals as child
nodes of the projected subtree. This is
simple if i, s and t have the same struc-
ture and are perfectly aligned, but again
this is not typically the case. A conse-
quence is that we will sometimes have sev-
eral possible insertion positions for each
non-terminal. The choice of insertion po-
sition is again made during combination.
</bodyText>
<sectionHeader confidence="0.998755" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.99953925">
After having extracted translation hypotheses
for as many parts of the input tree as possible,
we need to decide how to select and combine
them. Our approach here is similar to what
</bodyText>
<page confidence="0.997143">
81
</page>
<figureCaption confidence="0.977232">
Figure 4: A translation hypothesis endoded
</figureCaption>
<bodyText confidence="0.994105880952381">
as a lattice. This representation allows us to
handle efficiently the ambiguities of our trans-
lation rules. Note that each path in this lat-
tice corresponds to different choices of inser-
tion position for X2, morphological forms of
“be”, and the optional insertion of “at”.
has been proposed for Corpus-Based Machine
Translation. We first choose a number of fea-
tures and create a linear model scoring each
possible combination of hypotheses (see Sec-
tion 5). We then attempt to find the combi-
nation that maximizes this model score.
The combination of rules is constrained by
the structure of the input dependency tree. If
we only consider local features1, then a simple
bottom-up dynamic programming approach
can efficiently find the optimal combination
with linear O(|H|) complexity2. However,
non-local features (such as language models)
will force us to prune the search space. This
pruning is done efficiently through a varia-
tion of cube-pruning (Chiang, 2007). We
use KenLM3 (Heafield, 2011) for computing
the target language model score. Decoding
is made more efficient by using some of the
more advanced features of KenLM such as
state-reduction ((Li and Khudanpur, 2008),
(Heafield et al., 2011)) and rest-cost estima-
tions(Heafield et al., 2012).
Compared with the original cube-pruning
algorithm, our decoder is designed to handle
an arbitrary number of non-terminals. In ad-
dition, as we have seen in Section 3, the trans-
lation hypotheses we initially extract from ex-
amples are ambiguous in term of which target
word is going to be used and which will be the
final position of each non-terminal. In order to
handle such ambiguities, we use a lattice-based
internal representation that can encode them
efficiently (see Figure 4). This lattice represen-
tation also allows the decoder to make choices
between various morphological variations of a
</bodyText>
<footnote confidence="0.9995475">
1The score of a combination will be the sum of the
local scores of each translation hypothesis.
29-1 = set of translation hypotheses
3http://kheafield.com/code/kenlm/
</footnote>
<note confidence="0.625118">
word (e.g. be/is/are).
</note>
<sectionHeader confidence="0.854083" genericHeader="method">
5 Features and Tuning
</sectionHeader>
<bodyText confidence="0.999831666666667">
During decoding we use a linear model to score
each possible combination of hypotheses. This
linear model is based on a linear combination
of both local features (local to each translation
hypothesis) and non-local features (such as a
5-gram language model score of the final trans-
lation). The decoder considers in total a com-
bination of 34 features, a selection of which are
given below.
</bodyText>
<listItem confidence="0.99982825">
• Example penalty and example size
• Translation probability
• Language model score
• Optional words added/removed
</listItem>
<bodyText confidence="0.999924285714286">
The optimal weights for each feature are
estimated using the Pairwise Ranking Op-
timization (PRO) algorithm (Hopkins and
May, 2011) and parameter optimization with
MegaM4. We use the implementation of PRO
that is provided with the Moses SMT system
and the default settings of MegaM.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99358515">
In order to evaluate our system, we conducted
translation experiments on four language
pairs: Japanese-English (JA–EN), English-
Japanese (EN–JA), Japanese-Chinese (JA–
ZH) and Chinese-Japanese (ZH–JA).
For Japanese-English, we evaluated on the
NTCIR-10 PatentMT task data (patents)
(Goto et al., 2013) and compared our system
with the official baseline scores. For Japanese-
Chinese, we used parallel scientific paper ex-
cerpts from the ASPEC5 corpus and com-
pared against the same baseline system as for
Japanese-English. The corpora contain 3M
parallel sentences for Japanese-English and
670K for Japanese-Chinese.
The two baseline systems are based on the
open-source GIZA++/Moses pipeline. The
baseline labeled “Moses” uses the classic
phrase-based engine, while “Moses-Hiero” uses
the Hierarchical Phrase-Based decoder. These
</bodyText>
<footnote confidence="0.9999625">
4http://www.umiacs.umd.edu/~hal/megam/
5http://orchid.kuee.kyoto-u.ac.jp/ASPEC/
</footnote>
<page confidence="0.995116">
82
</page>
<table confidence="0.997337">
System JA–EN EN–JA JA–ZH ZH–JA
Moses 28.86 33.61 32.90 42.79
Moses-Hiero 28.56 32.98 — —
Proposed 29.00 32.15 32.99 37.64
</table>
<tableCaption confidence="0.976247">
Table 1: Scores
</tableCaption>
<table confidence="0.999805153846154">
System BLEU Translation
Moses 31.09 Further, the expansion stroke, the sectional area of the inner tube 12,
and the oil is supplied to the lower oil chamber S2 from the oil reservoir
chamber R × stroke.
Moses- 21.49 Also, the expansion stroke, the cross-sectional area of the inner tube
Hiero 12 × stroke of oil supplied from the oil reservoir chamber R lower oil
chamber S2.
Proposed 44.99 Further in this expansion stroke, the oil at an amount obtained by mul-
tiplying cross sectional area of the inner tube 12 from the oil reservoir
chamber R is resupplied to the lower oil chamber S2.
Reference 100.00 In this expansion stroke, oil in an amount obtained by multiplying the
cross sectional area of the inner tube 12 by the stroke is resupplied from
the upper oil reservoir chamber R to the lower oil chamber S2.
</table>
<tableCaption confidence="0.999649">
Table 2: Example of JA–EN translation with better translation quality than baselines.
</tableCaption>
<bodyText confidence="0.9992895">
correspond to the highest performing official
baselines for the NTCIR-10 PatentMT task.
As it appeared Moses was giving similar
and slightly higher BLEU scores than Moses-
Hiero for Japanese-English, we restricted eval-
uation to the standard settings for Moses for
our Japanese-Chinese experiments.
The following dependency parsers were
used. The scores in parentheses are the ap-
proximate parsing accuracies (micro-average),
which were evaluated by hand on a random
subset of sentences from the test data. The
parsers were trained on domains different to
those used in the experiments.
</bodyText>
<listItem confidence="0.992713833333334">
• English: NLParser6 (92%) (Charniak and
Johnson, 2005)
• Japanese: KNP (96%) (Kawahara and
Kurohashi, 2006)
• Chinese: SKP (88%) (Shen et al., 2012)
6.1 Results
</listItem>
<bodyText confidence="0.999588833333333">
The results shown are for evaluation on the
test set after tuning. Tuning was conducted
over 50 iterations on the development set using
an n-best list of length 500.
Table 2 shows an example sentence showing
significant improvement over the baseline. In
</bodyText>
<footnote confidence="0.9823975">
6Converted to dependency parses with in-house
tool.
</footnote>
<bodyText confidence="0.999957">
particular, non-local structure has been pre-
served by the proposed system, such as the
modification of ‘oil’ by the ‘in an amount... by
the stroke’ phrase. Another example is the in-
correct location of ‘× stroke’ in the Moses out-
put. The proposed system produces a much
more fluent output than the hierarchical-based
baseline Moses-Hiero.
The proposed system also outperforms the
baseline for JA–ZH, however falls short for
ZH–JA. We believe this is due to the low qual-
ity of parsing for Chinese input.
The decoder requires on average 0.94 sec-
onds per sentence when loading from precom-
piled hypothesis files. As a comparison, Moses
(default settings) takes 1.78 seconds per sen-
tence, loading from a binarized and filtered
phrase table.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999717">
This paper introduces an example-based
translation system exploiting both source and
target dependency analysis and online exam-
ple retrieving, allowing the availability of full
translation examples at translation time.
We believe that the use of dependency pars-
ing is important for accurate translation across
distant language pairs, especially in settings
such as ours with many long sentences. We
have designed a complete translation frame-
</bodyText>
<page confidence="0.996042">
83
</page>
<bodyText confidence="0.999976">
work around this idea, using dependency-
parsed trees at each step from alignment to
example retrieval to example combination.
The current performance (BLEU) of our
system is similar to (or even slightly bet-
ter than) state-of-the-art open-source SMT
systems. As we have been able to obtain
steady performance improvements during de-
velopment, we are hopeful that this trend will
continue and we will shortly obtain even bet-
ter results. Future plans include enriching
the feature set, adding a tree-based language
model and considering forest input for multi-
ple parses to provide robustness against pars-
ing errors. When the code base is sufficiently
stable, we intend to release the entire system
as open-source, in the hope of providing a
more syntactically-focused alternative to ex-
isting open-source SMT engines.
</bodyText>
<sectionHeader confidence="0.99139" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999784">
This work was partially supported by the
Japan Science and Technology Agency. The
first author is supported by a Japanese Gov-
ernment (MEXT) research scholarship. We
would like to thank the anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.997337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991871875">
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Dis-
criminative Reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, ACL 2005.
Fabien Cromières and Sadao Kurohashi. 2011. Ef-
ficient retrieval of tree translation examples for
syntax-based machine translation. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita
and Benjamin Tsou. 2013. Overview of
the Patent Machine Translation Task at the
NTCIR-10 Workshop. In Proceedings of the 10th
NTCIR Workshop Meeting on Evaluation of In-
formation Access Technologies (NTCIR-10).
Mark Hopkins and Jonathan May. 2011. Tuning
as Ranking. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing.
Daisuke Kawahara and Sadao Kurohashi. 2006.
A Fully-Lexicalized Probabilistic Model for
Japanese Syntactic and Case Structure Anal-
ysis. In Proceedings of the Human Language
Technology Conference of the NAACL.
Makoto Nagao. 1984. A framework of a mechan-
ical translation between Japanese and English
by analogy principle. In A. Elithorn and R.
Banerji. Artificial and Human Intelligence.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by bilingual generation and mono-
lingual derivation. In Proceedings of COLING
2012.
Mo Shen, Daisuke Kawahara and Sadao Kuro-
hashi. 2012. A Reranking Approach for De-
pendency Parsing with Variable-sized Subtree
Features. In Proceedings of 26th Pacific Asia
Conference on Language Information and Com-
puting.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. Scaling phrase-based statistical ma-
chine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 255–262. Association for Compu-
tational Linguistics, 2005.
David Chiang. 2007. Hierarchical phrase-based
translation. In Computational Linguistics.
Kenneth Heafield. 2011. KenLM: faster and
smaller language model queries. In Proceedings
of the EMNLP 2011 Sixth Workshop on Statis-
tical Machine Translation, 2011.
Kenneth Heafield, Hieu Hoang, Philipp Koehn,
Tetsuo Kiso, and Marcello Federico. 2011.
Left language model state for syntactic ma-
chine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Trans-
lation, 2011.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-
efficient storage. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Natu-
ral Language Learning, 2012.
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine transla-
tion with equivalent language model state main-
tenance. In Proceedings of the Second Workshop
on Syntax and Structure in Statistical Transla-
tion. Association for Computational Linguistics,
2008.
Adam Lopez. 2007. Hierarchical phrase-based
translation with suffix arrays. In EMNLP-
CoNLL 2007.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency Treelet Translation: Syn-
tactically Informed Phrasal SMT. In Proceed-
ings of the 43rd Annual Meeting on Association
for Computational Linguistics. Association for
Computational Linguistics, 2005.
</reference>
<page confidence="0.999245">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.405782">
<title confidence="0.999048">KyotoEBMT: An Example-Based</title>
<author confidence="0.608268">Translation Framework</author>
<affiliation confidence="0.961911">School of Informatics, Kyoto University, Kyoto</affiliation>
<address confidence="0.753184">Science and Technology Agency, Kawaguchi-shi, Saitama</address>
<email confidence="0.989689">kuro@i.kyoto-u.ac.jp</email>
<abstract confidence="0.991892352941177">This paper introduces the KyotoEBMT Example-Based Machine Translation framework. Our system a employing syntactic dependency analysis for both source and target languages in an attempt to preserve non-local structure. The effectiveness of our system is maximized with online example matching and a flexible decoder. Evaluation demonstrates BLEU scores competitive with state-of-the-art SMT systems such as Moses. The current implementation is intended to be released as open-source in the near future.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<contexts>
<context position="14592" citStr="Charniak and Johnson, 2005" startWordPosition="2262" endWordPosition="2265">to the highest performing official baselines for the NTCIR-10 PatentMT task. As it appeared Moses was giving similar and slightly higher BLEU scores than MosesHiero for Japanese-English, we restricted evaluation to the standard settings for Moses for our Japanese-Chinese experiments. The following dependency parsers were used. The scores in parentheses are the approximate parsing accuracies (micro-average), which were evaluated by hand on a random subset of sentences from the test data. The parsers were trained on domains different to those used in the experiments. • English: NLParser6 (92%) (Charniak and Johnson, 2005) • Japanese: KNP (96%) (Kawahara and Kurohashi, 2006) • Chinese: SKP (88%) (Shen et al., 2012) 6.1 Results The results shown are for evaluation on the test set after tuning. Tuning was conducted over 50 iterations on the development set using an n-best list of length 500. Table 2 shows an example sentence showing significant improvement over the baseline. In 6Converted to dependency parses with in-house tool. particular, non-local structure has been preserved by the proposed system, such as the modification of ‘oil’ by the ‘in an amount... by the stroke’ phrase. Another example is the incorrec</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabien Cromières</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Efficient retrieval of tree translation examples for syntax-based machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5840" citStr="Cromières and Kurohashi, 2011" startWordPosition="850" endWordPosition="853">l and translation hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed offline. However, for a given input sentence i, the steps for finding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several benefits. The first is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and s</context>
<context position="7619" citStr="Cromières and Kurohashi, 2011" startWordPosition="1144" endWordPosition="1147">l X3 (as a left-child before “a”, as a left-child after “a” or as a right-child). The translation hypotheses are then combined during decoding. Choice of optional words and final Non-Terminal positions is also done during decoding. *+254 ,-.274 /294 02:4 #$%&amp;2;4 &gt;)?@# A*$$ !284 &apos;()234 2GBBA4 2F84 2F54 2F74 2F54 * 1 2F74 ! #$%&amp; ,-. / 0 *+ 23 C%!.?,$ D!#!&amp;!+$ &apos;() 2F84 !&amp;quot; 2F84 2F84 E$ &amp;&apos;&apos;( 2F54 2F74 ?!?$*+ &apos;) #*!)+,!-&apos;) .!/01)$ #$%#&amp;&apos;&apos;( *$!&lt; =0$ ! =0$ *$!&lt; ! &apos;) #*!)+,!-&apos;) .!/01)$ #$%#&amp;&apos;&apos;( B@#?@# A*$$ ample database online than it is to match precomputed rules. We use the techniques described in (Cromières and Kurohashi, 2011) to perform this step as efficiently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is defined as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: 1. We project the part of s that is matched into the target side t using the a</context>
</contexts>
<marker>Cromières, Kurohashi, 2011</marker>
<rawString>Fabien Cromières and Sadao Kurohashi. 2011. Efficient retrieval of tree translation examples for syntax-based machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Ka Po Chow</author>
<author>Bin Lu</author>
<author>Eiichiro Sumita</author>
<author>Benjamin Tsou</author>
</authors>
<title>Overview of the Patent Machine Translation Task at the NTCIR-10 Workshop.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies (NTCIR-10).</booktitle>
<contexts>
<context position="12307" citStr="Goto et al., 2013" startWordPosition="1910" endWordPosition="1913">l words added/removed The optimal weights for each feature are estimated using the Pairwise Ranking Optimization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM4. We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM. 6 Experiments In order to evaluate our system, we conducted translation experiments on four language pairs: Japanese-English (JA–EN), EnglishJapanese (EN–JA), Japanese-Chinese (JA– ZH) and Chinese-Japanese (ZH–JA). For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the official baseline scores. For JapaneseChinese, we used parallel scientific paper excerpts from the ASPEC5 corpus and compared against the same baseline system as for Japanese-English. The corpora contain 3M parallel sentences for Japanese-English and 670K for Japanese-Chinese. The two baseline systems are based on the open-source GIZA++/Moses pipeline. The baseline labeled “Moses” uses the classic phrase-based engine, while “Moses-Hiero” uses the Hierarchical Phrase-Based decoder. These 4http://www.umiacs.umd.edu/~hal/megam/ 5http://orchid.kuee.kyoto-u.ac.jp/A</context>
</contexts>
<marker>Goto, Chow, Lu, Sumita, Tsou, 2013</marker>
<rawString>Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita and Benjamin Tsou. 2013. Overview of the Patent Machine Translation Task at the NTCIR-10 Workshop. In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies (NTCIR-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as Ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11841" citStr="Hopkins and May, 2011" startWordPosition="1843" endWordPosition="1846">ing we use a linear model to score each possible combination of hypotheses. This linear model is based on a linear combination of both local features (local to each translation hypothesis) and non-local features (such as a 5-gram language model score of the final translation). The decoder considers in total a combination of 34 features, a selection of which are given below. • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the Pairwise Ranking Optimization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM4. We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM. 6 Experiments In order to evaluate our system, we conducted translation experiments on four language pairs: Japanese-English (JA–EN), EnglishJapanese (EN–JA), Japanese-Chinese (JA– ZH) and Chinese-Japanese (ZH–JA). For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the official baseline scores. For JapaneseChinese, we used parallel scientific paper excerpts from the A</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as Ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A Fully-Lexicalized Probabilistic Model for Japanese Syntactic and Case Structure Analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL.</booktitle>
<contexts>
<context position="14645" citStr="Kawahara and Kurohashi, 2006" startWordPosition="2270" endWordPosition="2273">e NTCIR-10 PatentMT task. As it appeared Moses was giving similar and slightly higher BLEU scores than MosesHiero for Japanese-English, we restricted evaluation to the standard settings for Moses for our Japanese-Chinese experiments. The following dependency parsers were used. The scores in parentheses are the approximate parsing accuracies (micro-average), which were evaluated by hand on a random subset of sentences from the test data. The parsers were trained on domains different to those used in the experiments. • English: NLParser6 (92%) (Charniak and Johnson, 2005) • Japanese: KNP (96%) (Kawahara and Kurohashi, 2006) • Chinese: SKP (88%) (Shen et al., 2012) 6.1 Results The results shown are for evaluation on the test set after tuning. Tuning was conducted over 50 iterations on the development set using an n-best list of length 500. Table 2 shows an example sentence showing significant improvement over the baseline. In 6Converted to dependency parses with in-house tool. particular, non-local structure has been preserved by the proposed system, such as the modification of ‘oil’ by the ‘in an amount... by the stroke’ phrase. Another example is the incorrect location of ‘× stroke’ in the Moses output. The pro</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. A Fully-Lexicalized Probabilistic Model for Japanese Syntactic and Case Structure Analysis. In Proceedings of the Human Language Technology Conference of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japanese and English by analogy principle.</title>
<date>1984</date>
<booktitle>In A. Elithorn</booktitle>
<contexts>
<context position="5686" citStr="Nagao (1984)" startWordPosition="831" endWordPosition="832">ich combines initial hypotheses generated by the example retrieval module. Weights can be improved with batch tuning. 3 Example retrieval and translation hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed offline. However, for a given input sentence i, the steps for finding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several benefits. The first is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples,</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Makoto Nagao. 1984. A framework of a mechanical translation between Japanese and English by analogy principle. In A. Elithorn and R. Banerji. Artificial and Human Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Nakazawa</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Alignment by bilingual generation and monolingual derivation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<marker>Nakazawa, Kurohashi, 2012</marker>
<rawString>Toshiaki Nakazawa and Sadao Kurohashi. 2012. Alignment by bilingual generation and monolingual derivation. In Proceedings of COLING 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Shen</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A Reranking Approach for Dependency Parsing with Variable-sized Subtree Features.</title>
<date>2012</date>
<booktitle>In Proceedings of 26th Pacific Asia Conference on Language Information and Computing.</booktitle>
<contexts>
<context position="14686" citStr="Shen et al., 2012" startWordPosition="2278" endWordPosition="2281">iving similar and slightly higher BLEU scores than MosesHiero for Japanese-English, we restricted evaluation to the standard settings for Moses for our Japanese-Chinese experiments. The following dependency parsers were used. The scores in parentheses are the approximate parsing accuracies (micro-average), which were evaluated by hand on a random subset of sentences from the test data. The parsers were trained on domains different to those used in the experiments. • English: NLParser6 (92%) (Charniak and Johnson, 2005) • Japanese: KNP (96%) (Kawahara and Kurohashi, 2006) • Chinese: SKP (88%) (Shen et al., 2012) 6.1 Results The results shown are for evaluation on the test set after tuning. Tuning was conducted over 50 iterations on the development set using an n-best list of length 500. Table 2 shows an example sentence showing significant improvement over the baseline. In 6Converted to dependency parses with in-house tool. particular, non-local structure has been preserved by the proposed system, such as the modification of ‘oil’ by the ‘in an amount... by the stroke’ phrase. Another example is the incorrect location of ‘× stroke’ in the Moses output. The proposed system produces a much more fluent </context>
</contexts>
<marker>Shen, Kawahara, Kurohashi, 2012</marker>
<rawString>Mo Shen, Daisuke Kawahara and Sadao Kurohashi. 2012. A Reranking Approach for Dependency Parsing with Variable-sized Subtree Features. In Proceedings of 26th Pacific Asia Conference on Language Information and Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>255--262</pages>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 255–262. Association for Computational Linguistics, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="10107" citStr="Chiang, 2007" startWordPosition="1570" endWordPosition="1571">mber of features and create a linear model scoring each possible combination of hypotheses (see Section 5). We then attempt to find the combination that maximizes this model score. The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features1, then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2. However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<contexts>
<context position="10139" citStr="Heafield, 2011" startWordPosition="1575" endWordPosition="1576">linear model scoring each possible combination of hypotheses (see Section 5). We then attempt to find the combination that maximizes this model score. The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features1, then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2. However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the final position of each non-t</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Tetsuo Kiso</author>
<author>Marcello Federico</author>
</authors>
<title>Left language model state for syntactic machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<contexts>
<context position="10346" citStr="Heafield et al., 2011" startWordPosition="1606" endWordPosition="1609">ructure of the input dependency tree. If we only consider local features1, then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2. However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the final position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them efficiently (see Figure 4). This lattice representation also allows the decoder to make choic</context>
</contexts>
<marker>Heafield, Hoang, Koehn, Kiso, Federico, 2011</marker>
<rawString>Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo Kiso, and Marcello Federico. 2011. Left language model state for syntactic machine translation. In Proceedings of the International Workshop on Spoken Language Translation, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Language model rest costs and spaceefficient storage.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<contexts>
<context position="10396" citStr="Heafield et al., 2012" startWordPosition="1612" endWordPosition="1616">onsider local features1, then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2. However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the final position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them efficiently (see Figure 4). This lattice representation also allows the decoder to make choices between various morphological variations of a 1</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2012</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012. Language model rest costs and spaceefficient storage. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="10321" citStr="Li and Khudanpur, 2008" startWordPosition="1602" endWordPosition="1605">s is constrained by the structure of the input dependency tree. If we only consider local features1, then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2. However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the final position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them efficiently (see Figure 4). This lattice representation also allows </context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation. Association for Computational Linguistics, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL</booktitle>
<contexts>
<context position="5790" citStr="Lopez, 2007" startWordPosition="846" endWordPosition="847">batch tuning. 3 Example retrieval and translation hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed offline. However, for a given input sentence i, the steps for finding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several benefits. The first is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the fu</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In EMNLPCoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>