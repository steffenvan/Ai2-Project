<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004145">
<title confidence="0.984385">
Phrase-Based and Deep Syntactic
English-to-Czech Statistical Machine Translation �
</title>
<author confidence="0.949933">
Ondˇrej Bojar and Jan Hajiˇc
</author>
<affiliation confidence="0.945612">
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.8074305">
´UFAL MFF UK, Malostransk´e n´amˇesti25
CZ-11800 Praha, Czech Republic
</address>
<email confidence="0.999544">
{bojar,hajic}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999674">
This paper describes our two contributions to
WMT08 shared task: factored phrase-based
model using Moses and a probabilistic tree-
transfer model at a deep syntactic layer.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999852772727273">
Czech is a Slavic language with very rich morphol-
ogy and relatively free word order. The Czech
morphological system (Hajiˇc, 2004) defines 4,000
tags in theory and 2,000 were actually seen in a
big tagged corpus while the English Penn Treebank
tagset contains just about 50 tags. In our parallel
corpus (see below), the English vocabulary size is
148k distinct word forms but more than twice as big
in Czech, 343k distinct word forms.
When translating to Czech from an analytic lan-
guage such as English, target word forms have to
be chosen correctly to produce a grammatical sen-
tence and preserve the expressed relations between
elements in the sentence, e.g. verbs and their modi-
fiers.
This year, we have taken two radically different
approaches to English-to-Czech MT. Section 2 de-
scribes our setup of the phrase-based system Moses
(Koehn et al., 2007) and Section 3 focuses on a sys-
tem with probabilistic tree transfer employed at a
deep syntactic layer and the new challenges this ap-
proach brings.
</bodyText>
<footnote confidence="0.936457">
*The work on this project was supported by the grants FP6-
IST-5-034291-STP (EuroMatrix), MSM0021620838, MˇSMT
ˇCR LC536, and GA405/06/0589.
</footnote>
<sectionHeader confidence="0.960165" genericHeader="method">
2 Factored Phrase-Based MT to Czech
</sectionHeader>
<bodyText confidence="0.99994396">
Bojar (2007) describes various experiments with
factored translation to Czech aimed at improving
target-side morphology. We use essentially the same
setup with some cleanup and significantly larger
target-side training data:
Parallel data from CzEng 0.7 (Bojar et al., 2008),
with original sentence-level alignment and tokeniza-
tion. The parallel corpus was taken as a monolithic
text source disregarding differences between CzEng
data sources. We use only 1-1 aligned sentences.
Word alignment using GIZA++ toolkit (Och and
Ney, 2000), the default configuration as available in
training scripts for Moses. We based the word align-
ment on Czech and English lemmas (base forms
of words) as provided by the combination of tag-
gers and lemmatizers by Hajiˇc (2004) for Czech and
Brants (2000) followed by Minnen et al. (2001) for
English. We symmetrized the two GIZA++ runs us-
ing grow-diag-final heuristic.
Truecasing. We attempted to preserve meaning-
bearing case distinctions. The Czech lemmatizer
produces case-sensitive lemmas and thus makes it
easy to cast the capitalization of the lemma back on
the word form.1 For English we approximate the
same effect by a two-step procedure.2
</bodyText>
<footnote confidence="0.958131375">
1We change the capitalization of the form to match the
lemma in cases where the lemma is lowercase, capitalized (uc-
first) or all-caps. For mixed-case lemmas, we keep the form
intact.
2We first collect a lexicon of the most typical “shapes” for
each word form (ignoring title-like sentences with most words
capitalized and the first word in a sentence). Capitalized and
all-caps words in title-like sentences are then changed to their
</footnote>
<page confidence="0.951692">
143
</page>
<note confidence="0.441574">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 143–146,
</note>
<page confidence="0.557338">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figure confidence="0.9740972">
Sb uvedla ,ˇze Pred
Pred
=
NP said VP
VP
</figure>
<figureCaption confidence="0.999456">
Figure 1: Sample treelet pair, a-layer.
</figureCaption>
<bodyText confidence="0.999431470588236">
of syntactic analysis, both formally captured as la-
belled ordered dependency trees: the ANALYTICAL
(a-, surface syntax) representation bears a 1-1 corre-
spondence between tokens in the sentence and nodes
in the tree; the TECTOGRAMMATICAL (t-, deep syn-
tax) representation contains nodes only for autose-
mantic words and adds nodes for elements not ex-
pressed on the surface but required by the grammar
(e.g. dropped pronouns).
We use the following tools to automatically anno-
tate plaintext up to the t-layer: (1) TextSeg (ˇCeˇska,
2006) for tokenization, (2) tagging and lemmatiza-
tion see above, (3) parsing to a-layer: Collins (1996)
followed by head-selection rules for English, Mc-
Donald and others (2005) for Czech, (4) parsing to t-
layer: ˇZabokrtsk´y (2008) for English, Klimeˇs (2006)
for Czech.
</bodyText>
<subsectionHeader confidence="0.996316">
3.2 Probabilistic Tree Transfer
</subsectionHeader>
<bodyText confidence="0.998758833333333">
The transfer step is based on Synchronous Tree Sub-
stitution Grammars (STSG), see Bojar and ˇCmejrek
(2007) for a detailed explanation. The essence is a
log-linear model to search for the most likely syn-
chronous derivation S of the source T1 and target T2
dependency trees:
</bodyText>
<equation confidence="0.996679166666667">
Amhm(S)) (1)
S = argmax
6 s.t. source is Tl
M
(exp
m=1
</equation>
<bodyText confidence="0.917231">
Decoding steps. We use a simple two-step sce-
nario similar to class-based models (Brown and oth-
ers, 1992): (1) the source English word forms are
translated to Czech word forms and (2) full Czech
morphological tags are generated from the Czech
forms.
Language models. We use the following 6 inde-
pendently weighted language models for the target
(Czech) side:
</bodyText>
<listItem confidence="0.9986931">
• 3-grams of word forms based on all CzEng 0.7
data, 15M tokens,
• 3-grams of word forms in Project Syndicate
section of CzEng (in-domain for WMT07 and
WMT08 NC-test set), 1.8M tokens,
• 4-grams of word forms based on Czech Na-
tional Corpus (Kocek et al., 2000), version
SYN2006, 365M tokens,
• three models of 7-grams of morphological tags
from the same sources.
</listItem>
<bodyText confidence="0.994459909090909">
Lexicalized reordering using the mono-
tone/swap/discontinuous bidirectional model based
on both source and target word forms.
MERT. We use the minimum-error rate training
procedure by Och (2003) as implemented in the
Moses toolkit to set the weights of the various trans-
lation and language models, optimizing for BLEU.
Final detokenization is a simple rule-based pro-
cedure based on Czech typographical conventions.
Finally, we capitalize the beginnings of sentences.
See BLEU scores in Table 2 below.
</bodyText>
<sectionHeader confidence="0.938407" genericHeader="method">
3 MT with a Deep Syntactic Transfer
</sectionHeader>
<subsectionHeader confidence="0.996893">
3.1 Theoretical Background
</subsectionHeader>
<bodyText confidence="0.99971775">
Czech has a well-established theory of linguistic
analysis called Functional Generative Description
(Sgall et al., 1986) supported by a big treebanking
enterprise (Hajiˇc and others, 2006) and on-going
adaptations for other languages including English
(Cinkov´a and others, 2004). There are two layers
typical shape. In other sentences we change the case only if a
typically lowercase word is capitalized (e.g. at the beginning
of the sentence) or if a typically capitalized word is all-caps.
Unknown words in title-like sentences are lowercased and left
intact in other sentences.
The key feature function hm in STSG represents
the probability of attaching pairs of dependency
treelets ti1:2 such as in Figure 1 into aligned pairs of
frontiers ( ) in another treelet pair t1:2 given fron-
tier state labels (e.g. Pred- VP in Figure 1):
</bodyText>
<equation confidence="0.980100333333333">
k
hSTSG(S) = log ri Xti1:2  |frontier states) (2)
i=0
</equation>
<bodyText confidence="0.99912725">
Other features include e.g. number of internal
nodes (drawn as in Figure 1) produced, number
of treelets produced, and more importantly the tra-
ditional n-gram language model if the target (a-)tree
</bodyText>
<page confidence="0.983315">
144
</page>
<table confidence="0.772557666666667">
LM Type BLEU
n-gram 10.9±0.6
n-gram 8.8±0.6
none 8.7±0.6
none 6.6±0.5
n-gram 6.3±0.6
binode 5.6±0.5
none 5.3±0.5
binode 3.0±0.3
binode 2.6±0.3
none 1.6±0.3
none 0.7±0.2
Tree-based Transfer
epcp
eaca
epcp
eaca
etca
etct factored, preserving structure
etct factored, preserving structure
eact, target side atomic
etct, atomic, all attributes
etct, atomic, all attributes
etct, atomic, just t-lemmas
Phrase-based (Moses) as reported by Bojar (2007)
Vanilla n-gram 12.9±0.6
Factored to improve target morphology n-gram 14.2±0.7
</table>
<bodyText confidence="0.999206">
is linearized right away or a binode model promot-
ing likely combinations of the governor g(e) and the
child c(e) of an edge e E T2:
</bodyText>
<equation confidence="0.995213">
�hbinode(�) = log p(c(e)  |g(e)) (3)
eET2
</equation>
<bodyText confidence="0.818586">
The probabilistic dictionary of aligned treelet
pairs is extracted from node-aligned (GIZA++ on
linearized trees) parallel automatic treebank as in
Moses’ training: all treelet pairs compatible with the
node alignment.
</bodyText>
<subsectionHeader confidence="0.673468">
3.2.1 Factored Treelet Translation
</subsectionHeader>
<bodyText confidence="0.999993214285714">
Labels of nodes at the t-layer are not atomic but
consist of more than 20 attributes representing var-
ious linguistic features.3 We can consider the at-
tributes as individual factors (Koehn and Hoang,
2007). This allows us to condition the translation
choice on a subset of source factors only. In order to
generate a value for each target-side factor, we use
a sequence of mapping steps similar to Koehn and
Hoang (2007). For technical reasons, our current
implementation allows to generate factored target-
side only when translating a single node to a single
node, i.e. preserving the tree structure.
In our experiments we used 8 source (English) t-
node attributes and 14 target (Czech) attributes.
</bodyText>
<subsectionHeader confidence="0.998716">
3.3 Recent Experimental Results
</subsectionHeader>
<bodyText confidence="0.999570615384615">
Table 1 shows BLEU scores for various configura-
tions of our decoder. The abbreviations indicate be-
tween which layers the tree transfer was employed
(e.g. “eact” means English a-layer to Czech t-layer).
The “p” layer is an approximation of phrase-based
MT: the surface “syntactic” analysis is just a left-to-
right linear tree.4 For setups ending in t-layer, we
use a deterministic generation the of Czech sentence
by Pt´aˇcek and ˇZabokrtsk´y (2006).
For WMT08 shared task, Table 2, we used a vari-
ant of the “etct factored” setup with the annotation
pipeline as incorporated in TectoMT (ˇZabokrtsk´y,
2008) environment and using TectoMT internal
</bodyText>
<footnote confidence="0.994283333333333">
3Treated as atomic, t-node labels have higher entropy
(11.54) than lowercase plaintext (10.74). The t-layer by itself
does not bring any reduction in vocabulary. The idea is that the
attributes should be more or less independent and should map
easier across languages.
4Unlike Moses, “epcp” does not permit phrase reordering.
</footnote>
<tableCaption confidence="0.892275">
Table 1: English-to-Czech BLEU scores for syntax-based
MT on WMT07 DevTest.
</tableCaption>
<table confidence="0.9874802">
WMT07 WMT08
DevTest NC Test News Test
Moses 14.9±0.9 16.4±0.6 12.3±0.6
Moses, CzEng data only 13.9±0.9 15.2±0.6 10.0±0.5
etct, TectoMT annotation 4.7±0.5 4.9±0.3 3.3±0.3
</table>
<tableCaption confidence="0.999399">
Table 2: WMT08 shared task BLEU scores.
</tableCaption>
<bodyText confidence="0.8737085">
rules for t-layer parsing and generation instead of
Klimeˇs (2006) and (Pt´aˇcek and ˇZabokrtsk´y, 2006).
</bodyText>
<subsectionHeader confidence="0.6335">
3.3.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999511428571429">
Our syntax-based approach does not reach scores
of phrase-based MT due to the following reasons:
Cumulation of errors at every step of analysis.
Data loss due to incompatible parses and node
alignment. Unlike e.g. Quirk et al. (2005) or Huang
et al. (2006) who parse only one side and project the
structure, we parse both languages independently.
Natural divergence and random errors in either of
the parses and/or the alignment prevent us from ex-
tracting many treelet pairs.
Combinatorial explosion in target node at-
tributes. Currently, treelet options are fully built in
advance. Uncertainty in the many t-node attributes
leads to too many insignificant variations while e.g.
different lexical choices are pushed off the stack.
While vital for final sentence generation (see Ta-
ble 1), fine-grained t-node attributes should be pro-
duced only once all key structural, lexical and form
decisions have been made. The same sort of explo-
sion makes complicated factored setups not yet fea-
sible in Moses, either.
</bodyText>
<page confidence="0.997689">
145
</page>
<bodyText confidence="0.998712125">
Lack of n-gram LM in the (deterministic) gen-
eration procedures from a t-tree. While we support
final LM-based rescoring, there is too little variance
in n-best lists due to the explosion mentioned above.
Too many model parameters given our stack
limit. We use identical MERT implementation to
optimize Ams but in the large space of hypotheses,
MERT does not converge.
</bodyText>
<sectionHeader confidence="0.560181" genericHeader="method">
3.3.2 Related Research
</sectionHeader>
<bodyText confidence="0.999562714285714">
Our approach should not be confused with the
TectoMT submission by Zdenˇek ˇZabokrtsk´y with a
deterministic transfer: heuristics fully exploiting the
similarity of English and Czech t-layers.
Ding and Palmer (2005) improve over word-based
MT baseline with a formalism very similar to STSG.
Though not explicitly stated, they seem not to en-
code frontiers in the treelets and allow for adjunction
(adding siblings), like Quirk et al. (2005), which sig-
nificantly reduces data sparseness.
Riezler and III (2006) report an improvement in
MT grammaticality on a very restricted test set:
short sentences parsable by an LFG grammar with-
out back-off rules.
</bodyText>
<sectionHeader confidence="0.999532" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999981666666667">
We have presented our best-performing factored
phrase-based English-to-Czech translation and a
highly experimental complex system with tree-
based transfer at a deep syntactic layer. We have
discussed some of the reasons why the phrase-based
MT currently performs much better.
</bodyText>
<sectionHeader confidence="0.998956" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999420161764706">
Ondˇrej Bojar and Martin ˇCmejrek. 2007. Mathematical
Model of Tree Transformations. Project EuroMatrix -
Deliverable 3.2, ´UFAL, Charles University, Prague.
Ondˇrej Bojar, Zdenˇek ˇZabokrtsk´y, Pavel ˇCeˇska, Peter
Beˇna, and Miroslav Janiˇcek. 2008. CzEng 0.7: Paral-
lel Corpus with Community-Supplied Translations. In
Proc. ofLREC2008. ELRA.
Ondˇrej Bojar. 2007. English-to-Czech Factored Machine
Translation. In Proc. of ACL Workshop on Statistical
Machine Translation, pages 232–239, Prague.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proc. ofANLP-NAACL.
Peter F. Brown et al. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467–479.
Pavel ˇCeˇska. 2006. Segmentace textu. Bachelor’s The-
sis, MFF, Charles University in Prague.
Silvie Cinkov´a et al. 2004. Annotation of English on the
tectogrammatical level. Technical Report TR-2006-
35, ´UFAL/CKL, Prague, Czech Republic.
Michael Collins. 1996. A New Statistical Parser Based
on Bigram Lexical Dependencies. In Proc. ofACL.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency In-
sertion Grammars. In Proc. ofACL.
Jan Hajiˇc. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stviKarolinum, Prague.
Jan Hajiˇc et al. 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical Syntax-Directed Translation with Extended
Domain of Locality. In Proc. ofAMTA, Boston, MA.
V´aclav Klimeˇs. 2006. Analytical and Tectogrammatical
Analysis of a Natural Language. Ph.D. thesis, ´UFAL,
MFF UK, Prague, Czech Republic.
Jan Kocek, Marie Kopˇrivov´a, and Karel Kuˇcera, edi-
tors. 2000. ˇCesk´y n´arodnikorpus - ´uvod a pˇriruˇcka
uˇzivatele. FF UK - U´ˇCNK, Praha.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. ofEMNLP.
Philipp Koehn, Hieu Hoang, et al. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation. In
Proc. ofACL Demo and Poster Sessions.
Ryan McDonald et al. 2005. Non-Projective Depen-
dency Parsing using Spanning Tree Algorithms. In
Proc. ofHLT/EMNLP 2005.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207–223.
Franz Josef Och and Hermann Ney. 2000. A Comparison
of Alignment Models for Statistical Machine Transla-
tion. In Proc. of COLING, pages 1086–1090.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. ofACL.
Jan Pt´aˇcek and Zdenˇek ˇZabokrtsk´y. 2006. Synthesis
of Czech Sentences from Tectogrammatical Trees. In
Proc. of TSD, pages 221–228.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proc. ofACL, pages 271–279.
Stefan Riezler and John T. Maxwell III. 2006. Grammat-
ical Machine Translation. In Proc. ofHLT/NAACL.
Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia, Prague.
Zdenˇek ˇZabokrtsk´y. 2008. Tecto MT. Technical report,
´UFAL/CKL, Prague, Czech Republic. In prep.
</reference>
<page confidence="0.998821">
146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.391214">
<title confidence="0.9981695">Phrase-Based and Deep Statistical Machine Translation</title>
<author confidence="0.783293">Bojar Hajiˇc</author>
<affiliation confidence="0.643495">Institute of Formal and Applied</affiliation>
<address confidence="0.5064945">UFAL MFF UK, Malostransk´e n´amˇesti25 CZ-11800 Praha, Czech</address>
<abstract confidence="0.996691">This paper describes our two contributions to WMT08 shared task: factored phrase-based model using Moses and a probabilistic treetransfer model at a deep syntactic layer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Martin ˇCmejrek</author>
</authors>
<date>2007</date>
<booktitle>Mathematical Model of Tree Transformations. Project EuroMatrix -Deliverable 3.2, ´UFAL,</booktitle>
<institution>Charles University,</institution>
<location>Prague.</location>
<marker>Bojar, ˇCmejrek, 2007</marker>
<rawString>Ondˇrej Bojar and Martin ˇCmejrek. 2007. Mathematical Model of Tree Transformations. Project EuroMatrix -Deliverable 3.2, ´UFAL, Charles University, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Pavel ˇCeˇska</author>
<author>Peter Beˇna</author>
<author>Miroslav Janiˇcek</author>
</authors>
<title>CzEng 0.7: Parallel Corpus with Community-Supplied Translations.</title>
<date>2008</date>
<booktitle>In Proc. ofLREC2008. ELRA.</booktitle>
<marker>Bojar, ˇZabokrtsk´y, ˇCeˇska, Beˇna, Janiˇcek, 2008</marker>
<rawString>Ondˇrej Bojar, Zdenˇek ˇZabokrtsk´y, Pavel ˇCeˇska, Peter Beˇna, and Miroslav Janiˇcek. 2008. CzEng 0.7: Parallel Corpus with Community-Supplied Translations. In Proc. ofLREC2008. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
</authors>
<title>English-to-Czech Factored Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>232--239</pages>
<location>Prague.</location>
<contexts>
<context position="1644" citStr="Bojar (2007)" startWordPosition="253" endWordPosition="254">ence and preserve the expressed relations between elements in the sentence, e.g. verbs and their modifiers. This year, we have taken two radically different approaches to English-to-Czech MT. Section 2 describes our setup of the phrase-based system Moses (Koehn et al., 2007) and Section 3 focuses on a system with probabilistic tree transfer employed at a deep syntactic layer and the new challenges this approach brings. *The work on this project was supported by the grants FP6- IST-5-034291-STP (EuroMatrix), MSM0021620838, MˇSMT ˇCR LC536, and GA405/06/0589. 2 Factored Phrase-Based MT to Czech Bojar (2007) describes various experiments with factored translation to Czech aimed at improving target-side morphology. We use essentially the same setup with some cleanup and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We bas</context>
<context position="7471" citStr="Bojar (2007)" startWordPosition="1174" endWordPosition="1175"> of internal nodes (drawn as in Figure 1) produced, number of treelets produced, and more importantly the traditional n-gram language model if the target (a-)tree 144 LM Type BLEU n-gram 10.9±0.6 n-gram 8.8±0.6 none 8.7±0.6 none 6.6±0.5 n-gram 6.3±0.6 binode 5.6±0.5 none 5.3±0.5 binode 3.0±0.3 binode 2.6±0.3 none 1.6±0.3 none 0.7±0.2 Tree-based Transfer epcp eaca epcp eaca etca etct factored, preserving structure etct factored, preserving structure eact, target side atomic etct, atomic, all attributes etct, atomic, all attributes etct, atomic, just t-lemmas Phrase-based (Moses) as reported by Bojar (2007) Vanilla n-gram 12.9±0.6 Factored to improve target morphology n-gram 14.2±0.7 is linearized right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e E T2: �hbinode(�) = log p(c(e) |g(e)) (3) eET2 The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses’ training: all treelet pairs compatible with the node alignment. 3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes represent</context>
</contexts>
<marker>Bojar, 2007</marker>
<rawString>Ondˇrej Bojar. 2007. English-to-Czech Factored Machine Translation. In Proc. of ACL Workshop on Statistical Machine Translation, pages 232–239, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - A Statistical Part-ofSpeech Tagger. In</title>
<date>2000</date>
<booktitle>Proc. ofANLP-NAACL.</booktitle>
<contexts>
<context position="2418" citStr="Brants (2000)" startWordPosition="371" endWordPosition="372">and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approximate the same effect by a two-step procedure.2 1We change the capitalization of the form to match the lemma in cases where the lemma is lowercase, capitalized (ucfirst) or all-caps. For mixed-case lemmas, we keep the form intact. 2We first collect a lex</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - A Statistical Part-ofSpeech Tagger. In Proc. ofANLP-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, 1992</marker>
<rawString>Peter F. Brown et al. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel ˇCeˇska</author>
</authors>
<title>Segmentace textu. Bachelor’s Thesis,</title>
<date>2006</date>
<institution>MFF, Charles University in Prague.</institution>
<marker>ˇCeˇska, 2006</marker>
<rawString>Pavel ˇCeˇska. 2006. Segmentace textu. Bachelor’s Thesis, MFF, Charles University in Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
</authors>
<title>Annotation of English on the tectogrammatical level.</title>
<date>2004</date>
<tech>Technical Report TR-2006-35,</tech>
<location>UFAL/CKL, Prague, Czech Republic.</location>
<marker>Cinkov´a, 2004</marker>
<rawString>Silvie Cinkov´a et al. 2004. Annotation of English on the tectogrammatical level. Technical Report TR-2006-35, ´UFAL/CKL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies. In</title>
<date>1996</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="4127" citStr="Collins (1996)" startWordPosition="640" endWordPosition="641">lysis, both formally captured as labelled ordered dependency trees: the ANALYTICAL (a-, surface syntax) representation bears a 1-1 correspondence between tokens in the sentence and nodes in the tree; the TECTOGRAMMATICAL (t-, deep syntax) representation contains nodes only for autosemantic words and adds nodes for elements not expressed on the surface but required by the grammar (e.g. dropped pronouns). We use the following tools to automatically annotate plaintext up to the t-layer: (1) TextSeg (ˇCeˇska, 2006) for tokenization, (2) tagging and lemmatization see above, (3) parsing to a-layer: Collins (1996) followed by head-selection rules for English, McDonald and others (2005) for Czech, (4) parsing to tlayer: ˇZabokrtsk´y (2008) for English, Klimeˇs (2006) for Czech. 3.2 Probabilistic Tree Transfer The transfer step is based on Synchronous Tree Substitution Grammars (STSG), see Bojar and ˇCmejrek (2007) for a detailed explanation. The essence is a log-linear model to search for the most likely synchronous derivation S of the source T1 and target T2 dependency trees: Amhm(S)) (1) S = argmax 6 s.t. source is Tl M (exp m=1 Decoding steps. We use a simple two-step scenario similar to class-based </context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars. In</title>
<date>2005</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="11701" citStr="Ding and Palmer (2005)" startWordPosition="1835" endWordPosition="1838">es, either. 145 Lack of n-gram LM in the (deterministic) generation procedures from a t-tree. While we support final LM-based rescoring, there is too little variance in n-best lists due to the explosion mentioned above. Too many model parameters given our stack limit. We use identical MERT implementation to optimize Ams but in the large space of hypotheses, MERT does not converge. 3.3.2 Related Research Our approach should not be confused with the TectoMT submission by Zdenˇek ˇZabokrtsk´y with a deterministic transfer: heuristics fully exploiting the similarity of English and Czech t-layers. Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG. Though not explicitly stated, they seem not to encode frontiers in the treelets and allow for adjunction (adding siblings), like Quirk et al. (2005), which significantly reduces data sparseness. Riezler and III (2006) report an improvement in MT grammaticality on a very restricted test set: short sentences parsable by an LFG grammar without back-off rules. 4 Conclusion We have presented our best-performing factored phrase-based English-to-Czech translation and a highly experimental complex system with treebased transfe</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Disambiguation of Rich Inflection (Computational Morphology of Czech).</title>
<date>2004</date>
<location>NakladatelstviKarolinum, Prague.</location>
<marker>Hajiˇc, 2004</marker>
<rawString>Jan Hajiˇc. 2004. Disambiguation of Rich Inflection (Computational Morphology of Czech). NakladatelstviKarolinum, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. LDC2006T01, ISBN:</booktitle>
<pages>1--58563</pages>
<marker>Hajiˇc, 2006</marker>
<rawString>Jan Hajiˇc et al. 2006. Prague Dependency Treebank 2.0. LDC2006T01, ISBN: 1-58563-370-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical Syntax-Directed Translation with Extended Domain of Locality.</title>
<date>2006</date>
<booktitle>In Proc. ofAMTA,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="10341" citStr="Huang et al. (2006)" startWordPosition="1622" endWordPosition="1625">ax-based MT on WMT07 DevTest. WMT07 WMT08 DevTest NC Test News Test Moses 14.9±0.9 16.4±0.6 12.3±0.6 Moses, CzEng data only 13.9±0.9 15.2±0.6 10.0±0.5 etct, TectoMT annotation 4.7±0.5 4.9±0.3 3.3±0.3 Table 2: WMT08 shared task BLEU scores. rules for t-layer parsing and generation instead of Klimeˇs (2006) and (Pt´aˇcek and ˇZabokrtsk´y, 2006). 3.3.1 Discussion Our syntax-based approach does not reach scores of phrase-based MT due to the following reasons: Cumulation of errors at every step of analysis. Data loss due to incompatible parses and node alignment. Unlike e.g. Quirk et al. (2005) or Huang et al. (2006) who parse only one side and project the structure, we parse both languages independently. Natural divergence and random errors in either of the parses and/or the alignment prevent us from extracting many treelet pairs. Combinatorial explosion in target node attributes. Currently, treelet options are fully built in advance. Uncertainty in the many t-node attributes leads to too many insignificant variations while e.g. different lexical choices are pushed off the stack. While vital for final sentence generation (see Table 1), fine-grained t-node attributes should be produced only once all key s</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical Syntax-Directed Translation with Extended Domain of Locality. In Proc. ofAMTA, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´aclav Klimeˇs</author>
</authors>
<title>Analytical and Tectogrammatical Analysis of a Natural Language.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<location>UFAL, MFF UK, Prague, Czech Republic.</location>
<marker>Klimeˇs, 2006</marker>
<rawString>V´aclav Klimeˇs. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, ´UFAL, MFF UK, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<title>Cesk´y n´arodnikorpus - ´uvod a pˇriruˇcka uˇzivatele. FF UK - U´ˇCNK,</title>
<date>2000</date>
<editor>Jan Kocek, Marie Kopˇrivov´a, and Karel Kuˇcera, editors.</editor>
<location>Praha.</location>
<contexts>
<context position="2418" citStr="(2000)" startWordPosition="372" endWordPosition="372">nificantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approximate the same effect by a two-step procedure.2 1We change the capitalization of the form to match the lemma in cases where the lemma is lowercase, capitalized (ucfirst) or all-caps. For mixed-case lemmas, we keep the form intact. 2We first collect a lex</context>
</contexts>
<marker>2000</marker>
<rawString>Jan Kocek, Marie Kopˇrivov´a, and Karel Kuˇcera, editors. 2000. ˇCesk´y n´arodnikorpus - ´uvod a pˇriruˇcka uˇzivatele. FF UK - U´ˇCNK, Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models. In</title>
<date>2007</date>
<booktitle>Proc. ofEMNLP.</booktitle>
<contexts>
<context position="8181" citStr="Koehn and Hoang, 2007" startWordPosition="1283" endWordPosition="1286">ized right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e E T2: �hbinode(�) = log p(c(e) |g(e)) (3) eET2 The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses’ training: all treelet pairs compatible with the node alignment. 3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.3 We can consider the attributes as individual factors (Koehn and Hoang, 2007). This allows us to condition the translation choice on a subset of source factors only. In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang (2007). For technical reasons, our current implementation allows to generate factored targetside only when translating a single node to a single node, i.e. preserving the tree structure. In our experiments we used 8 source (English) tnode attributes and 14 target (Czech) attributes. 3.3 Recent Experimental Results Table 1 shows BLEU scores for various configurations of our decoder. The ab</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. ofACL Demo</booktitle>
<contexts>
<context position="8181" citStr="Koehn and Hoang, 2007" startWordPosition="1283" endWordPosition="1286">ized right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e E T2: �hbinode(�) = log p(c(e) |g(e)) (3) eET2 The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses’ training: all treelet pairs compatible with the node alignment. 3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.3 We can consider the attributes as individual factors (Koehn and Hoang, 2007). This allows us to condition the translation choice on a subset of source factors only. In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang (2007). For technical reasons, our current implementation allows to generate factored targetside only when translating a single node to a single node, i.e. preserving the tree structure. In our experiments we used 8 source (English) tnode attributes and 14 target (Czech) attributes. 3.3 Recent Experimental Results Table 1 shows BLEU scores for various configurations of our decoder. The ab</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, et al. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. ofACL Demo and Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Non-Projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT/EMNLP</booktitle>
<marker>McDonald, 2005</marker>
<rawString>Ryan McDonald et al. 2005. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proc. ofHLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="2451" citStr="Minnen et al. (2001)" startWordPosition="375" endWordPosition="378">arget-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approximate the same effect by a two-step procedure.2 1We change the capitalization of the form to match the lemma in cases where the lemma is lowercase, capitalized (ucfirst) or all-caps. For mixed-case lemmas, we keep the form intact. 2We first collect a lexicon of the most typical “shapes”</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Comparison of Alignment Models for Statistical Machine Translation.</title>
<date>2000</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="2166" citStr="Och and Ney, 2000" startWordPosition="326" endWordPosition="329">0021620838, MˇSMT ˇCR LC536, and GA405/06/0589. 2 Factored Phrase-Based MT to Czech Bojar (2007) describes various experiments with factored translation to Czech aimed at improving target-side morphology. We use essentially the same setup with some cleanup and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approxim</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. A Comparison of Alignment Models for Statistical Machine Translation. In Proc. of COLING, pages 1086–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation. In</title>
<date>2003</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="5567" citStr="Och (2003)" startWordPosition="879" endWordPosition="880">weighted language models for the target (Czech) side: • 3-grams of word forms based on all CzEng 0.7 data, 15M tokens, • 3-grams of word forms in Project Syndicate section of CzEng (in-domain for WMT07 and WMT08 NC-test set), 1.8M tokens, • 4-grams of word forms based on Czech National Corpus (Kocek et al., 2000), version SYN2006, 365M tokens, • three models of 7-grams of morphological tags from the same sources. Lexicalized reordering using the monotone/swap/discontinuous bidirectional model based on both source and target word forms. MERT. We use the minimum-error rate training procedure by Och (2003) as implemented in the Moses toolkit to set the weights of the various translation and language models, optimizing for BLEU. Final detokenization is a simple rule-based procedure based on Czech typographical conventions. Finally, we capitalize the beginnings of sentences. See BLEU scores in Table 2 below. 3 MT with a Deep Syntactic Transfer 3.1 Theoretical Background Czech has a well-established theory of linguistic analysis called Functional Generative Description (Sgall et al., 1986) supported by a big treebanking enterprise (Hajiˇc and others, 2006) and on-going adaptations for other langua</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Pt´aˇcek</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>Synthesis of Czech Sentences from Tectogrammatical Trees.</title>
<date>2006</date>
<booktitle>In Proc. of TSD,</booktitle>
<pages>221--228</pages>
<marker>Pt´aˇcek, ˇZabokrtsk´y, 2006</marker>
<rawString>Jan Pt´aˇcek and Zdenˇek ˇZabokrtsk´y. 2006. Synthesis of Czech Sentences from Tectogrammatical Trees. In Proc. of TSD, pages 221–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="10318" citStr="Quirk et al. (2005)" startWordPosition="1617" endWordPosition="1620">ch BLEU scores for syntax-based MT on WMT07 DevTest. WMT07 WMT08 DevTest NC Test News Test Moses 14.9±0.9 16.4±0.6 12.3±0.6 Moses, CzEng data only 13.9±0.9 15.2±0.6 10.0±0.5 etct, TectoMT annotation 4.7±0.5 4.9±0.3 3.3±0.3 Table 2: WMT08 shared task BLEU scores. rules for t-layer parsing and generation instead of Klimeˇs (2006) and (Pt´aˇcek and ˇZabokrtsk´y, 2006). 3.3.1 Discussion Our syntax-based approach does not reach scores of phrase-based MT due to the following reasons: Cumulation of errors at every step of analysis. Data loss due to incompatible parses and node alignment. Unlike e.g. Quirk et al. (2005) or Huang et al. (2006) who parse only one side and project the structure, we parse both languages independently. Natural divergence and random errors in either of the parses and/or the alignment prevent us from extracting many treelet pairs. Combinatorial explosion in target node attributes. Currently, treelet options are fully built in advance. Uncertainty in the many t-node attributes leads to too many insignificant variations while e.g. different lexical choices are pushed off the stack. While vital for final sentence generation (see Table 1), fine-grained t-node attributes should be produ</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proc. ofACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>Grammatical Machine Translation. In</title>
<date>2006</date>
<booktitle>Proc. ofHLT/NAACL.</booktitle>
<marker>Riezler, Maxwell, 2006</marker>
<rawString>Stefan Riezler and John T. Maxwell III. 2006. Grammatical Machine Translation. In Proc. ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence and Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<location>Academia, Prague.</location>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence and Its Semantic and Pragmatic Aspects. Academia, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>Tecto MT.</title>
<date>2008</date>
<tech>Technical report, ´UFAL/CKL,</tech>
<location>Prague, Czech Republic.</location>
<note>In prep.</note>
<marker>ˇZabokrtsk´y, 2008</marker>
<rawString>Zdenˇek ˇZabokrtsk´y. 2008. Tecto MT. Technical report, ´UFAL/CKL, Prague, Czech Republic. In prep.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>