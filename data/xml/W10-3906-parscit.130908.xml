<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001121">
<title confidence="0.9974245">
Even Unassociated Features Can Improve
Lexical Distributional Similarity
</title>
<author confidence="0.990044">
Kazuhide Yamamoto and Takeshi Asakura
</author>
<affiliation confidence="0.9997895">
Department of Electrical Engineering
Nagaoka University of Technology
</affiliation>
<email confidence="0.992173">
{yamamoto, asakura}@jnlp.org
</email>
<sectionHeader confidence="0.997197" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974230769231">
This paper presents a new computation
of lexical distributional similarity, which
is a corpus-based method for computing
similarity of any two words. Although
the conventional method focuses on em-
phasizing features with which a given
word is associated, we propose that even
unassociated features of two input words
can further improve the performance in
total. We also report in addition that
more than 90% of the features has no
contribution and thus could be reduced
in future.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993770909091">
Similarity calculation is one of essential tasks in
natural language processing (1990; 1992; 1994;
1997; 1998; 1999; 2005). We look for a seman-
tically similar word to do corpus-driven summa-
rization, machine translation, language genera-
tion, recognition of textual entailment and other
tasks. In task of language modeling and disam-
biguation we also need to semantically general-
ize words or cluster words into some groups. As
the amount of text increases more and more in
the contemporary world, the importance of sim-
ilarity calculation also increases concurrently.
Similarity is computed by roughly two ap-
proaches: based on thesaurus and based on cor-
pus. The former idea uses thesaurus, such as
WordNet, that is a knowledge resource of hi-
erarchical word classification. The latter idea,
that is the target of our work, originates from
Harris’s distributional hypothesis more than four
decades ago (1968), stating that semantically
similar words tend to appear in similar contexts.
In many cases a context of a word is represented
as a feature vector, where each feature is another
expression that co-occurs with the given word in
the context.
Over a long period of its history, in partic-
ular in recent years, several works have been
done on distributional similarity calculation. Al-
though the conventional works have attained the
fine performance, we attempt to further improve
the quality of this measure. Our motivation of
this work simply comes from our observation
and analysis of the output by conventional meth-
ods; Japanese, our target language here, is writ-
ten in a mixture of four scripts: Chinese char-
acters, Latin alphabet, and two Japanese-origin
characters. In this writing environment some
words which have same meaning and same pro-
nunciation are written in two (or more) different
scripts. This is interesting in terms of similarity
calculation since these two words are completely
same in semantics so the similarity should be
ideally 1.0. However, the reality is, as far as we
have explored, that the score is far from 1.0 in
many same word pairs. This fact implies that the
conventional calculation methods are far enough
to the goal and are expected to improve further.
The basic framework for computing distribu-
tional similarity is same; for each of two input
words a context (i.e., surrounding words) is ex-
tracted from a corpus, a vector is made in which
an element of the vector is a value or a weight,
and two vectors are compared with a formula to
compute similarity. Among these processes we
have focused on features, that are elements of
</bodyText>
<page confidence="0.993155">
32
</page>
<note confidence="0.737643">
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39,
</note>
<bodyText confidence="0.968375761904762">
Beijing, August 2010
the vector, some of which, we think, adversely
affect the performance. That is, traditional ap-
proaches such as Lin (1998) basically use all of
observed words as context, that causes noise in
feature vector comparison. One may agree that
the number of the characteristic words to deter-
mine the meaning of a word is some, not all, of
words around the target word. Thus our goal is
to detect and reduce such noisy features.
Zhitomirsky-Geffet and Dagan (2009) have
same motivation with us and introduced a boot-
strapping strategy that changes the original fea-
tures weights. The general idea here is to pro-
mote the weights of features that are common
for associated words, since these features are
likely to be most characteristic for determining
the word’s meaning. In this paper, we propose
instead a method to using features that are both
unassociated to the two input words, in addition
to use of features that are associated to the input.
</bodyText>
<sectionHeader confidence="0.960843" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999674">
The lexical distributional similarity of the input
two words is computed by comparing two vec-
tors that express the context of the word. In this
section we first explain the feature vector, and
how we define initial weight for each feature of
the vector. We then introduce in Subsection 2.3
the way to compute similarity by two vectors.
After that, we emphasize some of the features by
their association to the word, that is explained in
Subsection 2.4. We finally present in Subsection
2.5 feature reduction which is our core contribu-
tion of this work. Although our target language
is Japanese, we use English examples in order to
provide better understanding to the readers.
</bodyText>
<subsectionHeader confidence="0.9946">
2.1 Feature Vector
</subsectionHeader>
<bodyText confidence="0.999978086956522">
We first explain how to construct our feature vec-
tor from a text corpus.
A word is represented by a feature vector,
where features are collection of syntactically de-
pendent words co-occurred in a given corpus.
Thus, we first collect syntactically dependent
words for each word. This is defined, as in
Lin (1998), as a triple (w, r, w′), where w and
w′ are words and r is a syntactic role. As for
definition of word, we use not only words given
by a morphological analyzer but also compound
words. Nine case particles are used as syntactic
roles, that roughly express subject, object, modi-
fier, and so on, since they are easy to be obtained
from text with no need of semantic analysis. In
order to reduce noise we delete triples that ap-
pears only once in the corpus.
We then construct a feature vector out of col-
lection of the triples. A feature of a word is an
another word syntactically dependent with a cer-
tain role. In other words, given a triple (w, r, w′),
a feature of w corresponds to a dependent word
with a role (r, w′).
</bodyText>
<subsectionHeader confidence="0.997851">
2.2 (Initial) Filtering of Features
</subsectionHeader>
<bodyText confidence="0.999947666666667">
There are several weighting functions to deter-
mine a value for each feature element. As far
as we have investigated the literature the most
widely used feature weighting function is point-
wise mutual information (MI), that is defined as
follows:
</bodyText>
<equation confidence="0.99688">
freq(w, r, w′)5
MI(w, r, w′) = log2 freq(w)freq(r, w′) (1)
</equation>
<bodyText confidence="0.9999388125">
where freq(r, w′) is the frequency of the co-
occurrence word w′ with role r, freq(w)
is the independent frequency of a word w,
freq(w, r, w′) is the frequency of the triples
(w, r, w′), and 5 is the number of all triples.
In this paper we do not discuss what is the
best weighting functions, since this is out of tar-
get. We use mutual information here because it
is most widely used, i.e., in order to compare per-
formance with others we want to adopt the stan-
dard approach.
As other works do, we filter out features that
have a value lower than a minimal weight thresh-
olds α. The thresholds are determined according
to our preliminary experiment, that is explained
later.
</bodyText>
<subsectionHeader confidence="0.99536">
2.3 Vector Similarity
</subsectionHeader>
<bodyText confidence="0.9998735">
Similarity measures of the two vectors are com-
puted by various measures. Shibata and Kuro-
hashi (2009) have compared several similarity
measures including Cosine (Ruge, 1992), (Lin,
</bodyText>
<page confidence="0.96003">
33
</page>
<equation confidence="0.838985833333333">
✓ ✏
(input word) w: boy
(feature) v: guardOBJ
(synonyms of w, shown with its similarity to w) Syn(w) _
{ child(0.135), girl(0.271), pupil(0.143), woman(0.142), young people(0.147) }
✓ ✏
(feature vectors V ):
V(boy) _ { parentsMOD, runawaySUBJ, reclaimOBJ, fatherMOD, guardOBJ , · · · }
V(child) _ { guardOBJ, lookOBJ, bringOBJ, give birthOBJ, careOBJ , · · · }
V(girl) _ { parentsMOD, guardOBJ, fatherMOD, testifySUBJ, lookOBJ, · · · }
V(pupil) _ { targetOBJ, guardOBJ, careOBJ, aimOBJ, increaseSUBJ, · · · }
V(woman) _ { nameMOD, give birthOBJ, groupMOD, together+with, parentsMOD, · · · }
V(young people) _ { harmfulTO, globalMOD, reclaimOBJ, wrongdoingMOD , · · · }
✒ ✑
(words that has feature v) Asc(v) _ {boy, child, girl, pupil, · · ·}
weight(w, v) = weight (boy, guardOBJ) = Ewf∈Asc(v)∩Syn(w) sim(w, wf)
= 0.135 + 0.271 + 0.143 = 0.549
✒ ✑
</equation>
<figureCaption confidence="0.999682">
Figure 1: Example of feature weighting for word boy.
</figureCaption>
<bodyText confidence="0.9935478">
1998), (Lin, 2002), Simpson, Simpson-Jaccard,
and conclude that Simpson-Jaccard index attains
best performance of all. Simpson-Jaccard index
is an arithmetic mean of Simpson index and Jac-
card index, defined in the following equation:
</bodyText>
<equation confidence="0.999633">
simJ(w1,w2) = |V1n V2 |(3)
|V1 � V2|
simS(w1, w2) = |V1 nV2|
min(|V1|, |V2|) (4)
</equation>
<bodyText confidence="0.998937727272727">
where V1 and V2 is set of features for w1 and
w2, respectively, and |A |is the number of set A.
It is interesting to note that both Simpson and
Jaccard compute similarity according to degree
of overlaps of the two input sets, that is, the re-
ported best measure computes similarity by ig-
noring the weight of the features. In this paper
we adopt Simpson-Jaccard index, sim, which
indicates that the weight of features that is ex-
plained below is only used for feature reduction,
not for similarity calculation.
</bodyText>
<subsectionHeader confidence="0.994595">
2.4 Feature Weighting by Association
</subsectionHeader>
<bodyText confidence="0.9999599375">
We then compute weights of the features of the
word w according to the degree of semantic as-
sociation to w. The weight is biased because all
of the features, i.e., the surrounding words, are
not equally characteristic to the input word. The
core idea for feature weighting is that a feature
v in w is more weighted when more synonyms
(words of high similarity) of w also have v.
Figure 1 illustrates this process by examples.
Now we calculate a feature guardOBJ for a word
boy, we first collect synonyms of w, denoted by
Syn(w), from a thesaurus. We then compute
similarities between w and each word in Syn(w)
by Equation 2. The weight is the sum of the sim-
ilarities of words in Syn(w) that have feature v,
defined in Equation 5.
</bodyText>
<equation confidence="0.972942">
weight(w, v) = � sim(w, wf)
wf∈Asc(v)∩Syn(w)
(5)
1
sim(w1, w2) = 2(simJ(w1, w2)+simS(w1, w2))
(2)
</equation>
<page confidence="0.985116">
34
</page>
<figureCaption confidence="0.943107">
Figure 2: An illustration of similarity calculation of Zhitomirsky-Geffet and Dagan (2009) (a) and
</figureCaption>
<bodyText confidence="0.807223">
the proposed method (b1 and b2) in feature space. In order to measure the distance of the two words
(shown in black dots) they use only associated words, while we additionally use unassociated words
in which the distances to the words are similar.
</bodyText>
<subsectionHeader confidence="0.946569">
2.5 Feature Reduction
</subsectionHeader>
<bodyText confidence="0.9937344">
We finally reduce features according to the dif-
ference of weights of each feature in words we
compare. In computing similarity of two words,
w1 and w2, a feature v satisfying Equation 6 is
reduced.
</bodyText>
<equation confidence="0.916576">
abs(weight(w1, v) − weight(w2, v)) &gt; β (6)
</equation>
<bodyText confidence="0.999964894736842">
where abs() is a function of absolute value, and
β is a threshold for feature reduction.
Figure 2 illustrates our idea and compares
the similar approach proposed by Zhitomirsky-
Geffet and Dagan (2009). Roughly speaking,
Zhitomirsky-Geffet and Dagan (2009) compute
similarity of two words, shown as black dots
in (a), mainly according to associated features
(dark-colored circle), or features that has high
weights in Equation 5. And the associated fea-
tures are determined word by word indepen-
dently.
In contrast, the proposed method relatively re-
duces features, depending on location of input
two words. At (b1) in the figure, not only asso-
ciated (high-colored area) but unassociated fea-
tures (light-colored area) are used for similar-
ity computation in our method. As Equation 6
shows, regardless of how much a feature is as-
sociated to the word, the feature is not reduced
when it has similar weight to both w1 and w2,
located at the middle area of the two words in
the figure.
This idea seems to work more effectively,
compared with Zhitomirsky-Geffet and Da-
gan (2009), in case that input two words are not
so similar, that is shown at (b2) of the figure.
As they define associated features independently,
it is likely that the overlapped area is little or
none between the two words. In contrast, our
method uses features at the middle area of two
input words, where there is always certain fea-
tures provided for similarity computation, shown
in case (b2). Simplified explanation is that our
similarity is computed as the ratio of the associ-
ated area to the unassociated area in the figure.
We will verify later if the method works better in
low similarity calculation.
</bodyText>
<subsectionHeader confidence="0.981988">
2.6 Final Similarity
</subsectionHeader>
<bodyText confidence="0.999631333333333">
The final similarity of two words are calculated
by two shrunk vectors (or feature sets) and Equa-
tion 2, that gives a value between 0 and 1.
</bodyText>
<page confidence="0.998621">
35
</page>
<sectionHeader confidence="0.999681" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999184">
3.1 Evaluation Method
</subsectionHeader>
<bodyText confidence="0.955153466666667">
In general it is difficult to answer how similar
two given words are. Human have no way to
judge correctness if computed similarity of two
words is, for instance, 0.7. However, given two
word pairs, such as (w, w1) and (w, w2), we may
answer which of two words, w1 or w2, is more
similar to w than the other one. That is, degree
of similarity is defined relatively hence accuracy
of similarity measures is evaluated by way of rel-
ative comparisons.
In this paper we employ an automatic eval-
uation method in order to reduce time, human
labor, and individual variations. We first col-
lect four levels of similar word pairs from a the-
saurus1. Thesaurus is a resource of hierarchi-
cal words classification, hence we can collect
several levels of similar word pairs according
to the depth of common parent nodes that two
words have. Accordingly, we constructed four
levels of similarity pairs, Level 0, 1, 2, and 3,
where the number increases as the similarity in-
creases. Each level includes 800 word pairs that
are randomly selected. The following examples
are pairs with word Asia in each Level.
✓ ✏
Example: Four similarity levels for pair of
Asia.
Level 3(high): Asia vs. Europe
Level 2: Asia vs. Brazil
Level 1: Asia vs. my country
Level 0(low): Asia vs. system
✒ ✑
We then combine word pairs of adjacent sim-
ilarity Levels, such as Level 0 and 1, that is a
test set to see low-level similarity discrimination
power. The performance is calculated in terms
of how clearly the measure distinguishes the dif-
ferent levels. In a similar fashion, Level 1 and 2,
as well as 2 and 3, are combined and tested for
middle-level and high-level similarity discrimi-
nation, respectively. The number of pairs in each
1In this experiment we use Bunrui Goi Hyo also for
evaluation. Therefore, this experimental setting is a kind
of closed test. However, we see that the advantage to use
the same thesaurus in the evaluation seems to be small.
</bodyText>
<figureCaption confidence="0.991807">
Figure 3: Relation between threshold α and per-
formance in F-measures for Level 3+2 test set.
</figureCaption>
<bodyText confidence="0.952629">
test set is 1,600 as two Levels are combined.
</bodyText>
<subsectionHeader confidence="0.990546">
3.2 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999947954545455">
The corpus we use in this experiment is all the
articles in The Nihon Keizai Shimbun Database,
a Japanese business newspaper corpus cover-
ing the years 1990 through 2004. As morpho-
logical analyzer we use Chasen 2.3.3 with IPA
morpheme dictionary. The number of collected
triples is 2,584,905, that excludes deleted ones
due to one time appearance and words including
some symbols.
In Subsection 2.4 we use Bunrui Goi Hyo, a
Japanese thesaurus for synonym collection. The
potential target words are all content words, ex-
cept words that have less than twenty features.
The number of words after exclusion is 75,530.
Moreover, words that have four or less words in
the same category in the thesaurus are regarded
as out of target in this paper, due to limitation
of Syn(w) in Subsection 2.4. Also, in order
to avoid word sense ambiguity, words that have
more than two meanings, i.e., those classified in
more than two categories in the thesaurus, also
remain to be solved.
</bodyText>
<subsectionHeader confidence="0.940376">
3.3 Threshold for Initial Filtering
</subsectionHeader>
<bodyText confidence="0.999588">
Figure 3 shows relation between threshold α and
the performance of similarity distinction that is
drawn in F-measures, for Level 3+2 test set. As
can be seen, the plots seem to be concave down
</bodyText>
<page confidence="0.998102">
36
</page>
<figureCaption confidence="0.99543825">
Figure 4: Threshold vs. accuracy in Level 3+2
set.
Figure 5: Threshold vs. accuracy in Level 2+1
set.
</figureCaption>
<bodyText confidence="0.969553625">
and there is a clear peak when α is between 2
and 3.
In the following experiments we set α the
value where the best performance is given for
each test set. We have observed similar phenom-
ena in other test sets. The thresholds we use is
2.1 for Level 3+2, 2.4 for Level 2+1, and 2.4 for
Level 1+0.
</bodyText>
<subsectionHeader confidence="0.841905">
3.4 Threshold for Weighting Function
</subsectionHeader>
<bodyText confidence="0.995381">
Figure 4, 5, and 6 show relation between thresh-
old β and performance in Level 3+2, 2+1, 1+0
test set, respectively. The threshold at the point
where highest performance is obtained greatly
depends on Levels: 0.3 in Level 3+2, 0.5 in Level
2+1, and 0.9 in Level 1+0. Comparison of these
three figures indicates that similarity distinction
</bodyText>
<figureCaption confidence="0.97062">
Figure 6: Threshold vs. accuracy in Level 1+0
set.
</figureCaption>
<tableCaption confidence="0.94956">
Table 1: Performance comparison of three meth-
ods in each task (in F-measures).
</tableCaption>
<table confidence="0.9980845">
Level S&amp;K ZG&amp;D proposed
Lvl.3+Lvl.2 0.702 0.791 0.797
Lvl.2+Lvl.1 0.747 0.771 0.773
Lvl.1+Lvl.0 0.838 0.789 0.840
</table>
<bodyText confidence="0.9979674">
power in higher similarity region requires lower
threshold, i.e., fewer features. In contrast, con-
ducting fine distinction in lower similarity level
requires higher threshold, i.e., a lot of features
most of which may be unassociated ones.
</bodyText>
<subsectionHeader confidence="0.902697">
3.5 Performance
</subsectionHeader>
<bodyText confidence="0.999535428571429">
Table 1 shows performance of the pro-
posed method, compared with Shibata
and Kurohashi (2009) (S&amp;K in the table)
and Zhitomirsky-Geffet and Dagan (2009)
(ZG&amp;D)2. The method of Shibata and Kuro-
hashi (2009) here is the best one among those
compared. It uses only initial filtering described
in Subsection 2.2. The method of Zhitomirsky-
Geffet and Dagan (2009) in addition emphasize
associated features as explained in Subsection
2.4. All of the results in the table are the best
ones among several threshold settings.
The result shows that the accuracy is 0.797
(+0.006) in Level 3+2, 0.773 (+0.002) in Level
</bodyText>
<footnote confidence="0.879946666666667">
2The implementations of providing associated words
and the bootstrapping are slightly different to Zhitomirsky-
Geffet and Dagan (2009).
</footnote>
<page confidence="0.999414">
37
</page>
<bodyText confidence="0.999574909090909">
2+1, and 0.840 (+0.001) in Level 1+0, where the
degree of improvement here are those compared
with best ones except our proposed method. This
confirms that our method attains equivalent or
better performance in all of low, middle, and
high similarity levels.
We also see in the table that S&amp;K and ZG&amp;D
show different behavior according to the Level.
However, it is important to note here that our
proposed method performs equivalent or outper-
forms both methods in all Levels.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="method">
4 Discussions
</sectionHeader>
<subsectionHeader confidence="0.999144">
4.1 Behavior at Each Similarity Level
</subsectionHeader>
<bodyText confidence="0.999995277777778">
As we have discussed in Subsection 2.5, our
method is expected to perform better than
Zhitomirsky-Geffet and Dagan (2009) in distinc-
tion in lower similarity area. Roughly speak-
ing, we interpret the results as follows. Shi-
bata and Kurohashi (2009) always has many fea-
tures that degrades the performance in higher
similarity level, since the ratio of noisy fea-
tures may throw into confusion. Zhitomirsky-
Geffet and Dagan (2009) reduces such noise
that gives better performance in higher similarity
level and is stable in all levels. And our proposed
method maintains performance of Zhitomirsky-
Geffet and Dagan (2009) in higher level while
improves performance that is close to Shibata
and Kurohashi (2009) in lower level, utilizing
fewer features. We think our method can include
advantages over the two methods.
</bodyText>
<subsectionHeader confidence="0.821461">
4.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999999375">
We overview the result and see that the major er-
rors are NOT due to lack of features. Table 2
illustrates the statistics of words with a few fea-
tures (less than 50 or 20). This table clearly tells
us that, in the low similarity level (Level 1+0) in
particular, there are few pairs in which the word
has less than 50 or 20, that is, these pairs are con-
sidered that the features are erroneously reduced.
</bodyText>
<subsectionHeader confidence="0.887697">
4.3 Estimation of Potential Feature
Reduction
</subsectionHeader>
<bodyText confidence="0.984823714285714">
It is interesting to note that we may reduce 81%
of features in Level 3+2 test set while keeping
Table 2: Relation of errors and words with a few
features. In the table, (h) and (l) shows pairs that
are judged higher (lower) by the system. Column
of &lt; 50 (&lt; 20) means number of pairs each of
which has less than 50 (20) features.
</bodyText>
<table confidence="0.996118">
Level #errs &lt; 50 fea. &lt; 20 fea.
Lvl.3+2 (h) 125 76 (61%) 32 (26%)
Lvl.3+2 (l) 220 150 (68%) 60 (27%)
Lvl.2+1 (h) 137 75 (55%) 32 (23%)
Lvl.2+1 (l) 253 135 (53%) 52 (21%)
Lvl.1+0 (h) 149 23 (15%) 4 ( 3%)
Lvl.1+0 (l) 100 17 (17%) 3 ( 3%)
</table>
<bodyText confidence="0.994710368421053">
the performance, if we can reduce them prop-
erly. In a same way, 87% of features in Level
2+1 set, and 52% of features in Level 1+0 set,
may also be reduced. These numbers are given
at the situation in which F-measure attains best
performance. Here, it is not to say that we are
sure to reduce them in future, but to estimate how
many features are really effective to distinguish
the similarity.
Here we have more look at the statistics. The
number of initial features on average is 609 in
Level 3+2 test set. If we decrease threshold by
0.1, we can reduce 98% of features at the thresh-
old of 0.8, where the performance remains best
(0.791). This is a surprising fact for us since
only 12 (&apos;= 609×(1−0.98)) features really con-
tribute the performance. Therefore, we estimate
that there is a lot to be reduced further in order
to purify the features.
</bodyText>
<sectionHeader confidence="0.997704" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999886">
This paper illustrates improvement of lexical
distributional similarity by not only associated
features but also utilizing unassociated features.
The core idea is simple, and is reasonable when
we look at machine learning; in many cases we
use training instances of not only something pos-
itive but something negative to make the distinc-
tion of the two sides clearer. Similarly, in our
task we use features of not only associated but
unassociated to make computation of similarity
(or distance in semantic space) clearer. We as-
</bodyText>
<page confidence="0.996994">
38
</page>
<bodyText confidence="0.999802333333333">
sert in this work that a feature that has similar
weight to two given words also plays important
role, regardless of how much it is associated to
the given words.
Among several future works we need to fur-
ther explore reduction of features. It is reported
by some literature such as Hagiwara et al. (2006)
that we can reduce so many features while pre-
serving the same accuracy in distributional sim-
ilarity calculation. This implies that, some of
them are still harmful and are expected to be re-
duced further.
</bodyText>
<subsectionHeader confidence="0.8284">
List of Tools and Resources
</subsectionHeader>
<reference confidence="0.99872375">
1. Chasen, a morphological analyzer,
Ver.2.3.3. Matsumoto Lab., Nara Institute
of Science and Technology. http://chasen-
legacy.sourceforge.jp/
2. IPADIC, a dictionary for morphologi-
cal analyzer. Ver.2.7.0. Information-
Technology Promotion Agency, Japan.
http://sourceforge.jp/projects/ipadic/
3. Bunrui Goihyo, a word list by semantic
principles, revised and enlarged edi-
tion. The National Institute for Japanese
Language. http://www.kokken.go.jp
/en/publications/bunrui goihyo/
4. Nihon Keizai Shimbun Newspaper Corpus,
years 1990-2004, Nihon Keizai Shimbun,
Inc.
</reference>
<sectionHeader confidence="0.975573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925019607843">
Dagan, Ido, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based Models of Co-occurrence Proba-
bilities. Machine Learning, 34(1-3):43–69.
Grefenstette, Gregory. 1994. Exploration in Auto-
matic Thesaurus Discovery. Kluwer Academic
Publishers. Norwell, MA.
Hagiwara, Masato, Yasuhiro Ogawa, Katsuhiko
Toyama. 2006. Selection of Effective Contextual
Information for Automatic Synonym Acquisition.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pp.353–360.
Harris, Zelig S. 1968. Mathematical Structures of
Language. Wiley, New Jersey.
Hindle, Donald. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings
of the 28th Annual Meeting of the Association for
Computational Linguistics, pp.268–275.
Lee, Lillian. 1997. Similarity-Based Approaches to
Natural Language Processing. Ph.D. thesis, Har-
vard University, Cambridge, MA.
Lee, Lillian. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pp. 25–32, College Park, MD.
Lin, Dekang. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Confer-
ence on Computational Linguistics, pp.768–774.
Montreal.
Lin, Dekang and and Patrick Pantel. 2002. Con-
cept Discovery from Text. In Proceedings of 19th
International Conference on Computational Lin-
guistics, pp.577–583. Taipei.
Ruge, Gerda. 1992. Experiments of Linguistically-
based Term Associations. Information Processing
&amp; Management, 28(3):317–332.
Shibata, Tomohide and Sadao Kurohashi. 2009. Dis-
tributional similarity calculation using very large
scale Web corpus. In Proceedings ofAnnual Meet-
ing ofAssociation for Natural Language Process-
ing. pp. 705–708.
Weeds, Julie and David Weir. 2005. Co-occurrence
retrieval: A Flexible Framework for Lexical Dis-
tributional Similarity. Computational Linguistics.
31(4):439–476.
Zhitomirsky-Geffet, Maayan and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Qual-
ity. Computational Linguistics, 35(3):435–461.
</reference>
<page confidence="0.999533">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.826167">
<title confidence="0.885451">Even Unassociated Features Can Lexical Distributional Similarity</title>
<author confidence="0.986551">Yamamoto</author>
<affiliation confidence="0.997307">Department of Electrical Nagaoka University of</affiliation>
<abstract confidence="0.998270285714286">This paper presents a new computation of lexical distributional similarity, which is a corpus-based method for computing similarity of any two words. Although the conventional method focuses on emphasizing features with which a given word is associated, we propose that even unassociated features of two input words can further improve the performance in total. We also report in addition that more than 90% of the features has no contribution and thus could be reduced in future.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Chasen</author>
</authors>
<title>a morphological analyzer,</title>
<institution>Lab., Nara Institute of Science and Technology.</institution>
<location>Ver.2.3.3. Matsumoto</location>
<note>http://chasenlegacy.sourceforge.jp/</note>
<marker>Chasen, </marker>
<rawString>1. Chasen, a morphological analyzer, Ver.2.3.3. Matsumoto Lab., Nara Institute of Science and Technology. http://chasenlegacy.sourceforge.jp/</rawString>
</citation>
<citation valid="true">
<authors>
<author>IPADIC</author>
</authors>
<title>a dictionary for morphological analyzer.</title>
<date></date>
<booktitle>Ver.2.7.0. InformationTechnology Promotion Agency,</booktitle>
<note>http://sourceforge.jp/projects/ipadic/</note>
<marker>IPADIC, </marker>
<rawString>2. IPADIC, a dictionary for morphological analyzer. Ver.2.7.0. InformationTechnology Promotion Agency, Japan. http://sourceforge.jp/projects/ipadic/</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bunrui</author>
</authors>
<title>Goihyo, a word list by semantic principles, revised and enlarged edition. The National Institute for Japanese Language. http://www.kokken.go.jp /en/publications/bunrui goihyo/</title>
<marker>Bunrui, </marker>
<rawString>3. Bunrui Goihyo, a word list by semantic principles, revised and enlarged edition. The National Institute for Japanese Language. http://www.kokken.go.jp /en/publications/bunrui goihyo/</rawString>
</citation>
<citation valid="false">
<title>Nihon Keizai Shimbun Newspaper Corpus, years 1990-2004, Nihon Keizai Shimbun,</title>
<publisher>Inc.</publisher>
<marker></marker>
<rawString>4. Nihon Keizai Shimbun Newspaper Corpus, years 1990-2004, Nihon Keizai Shimbun, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based Models of Co-occurrence Probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Dagan, Ido, Lillian Lee, and Fernando Pereira. 1999. Similarity-based Models of Co-occurrence Probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Exploration in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Norwell, MA.</location>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, Gregory. 1994. Exploration in Automatic Thesaurus Discovery. Kluwer Academic Publishers. Norwell, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Yasuhiro Ogawa</author>
<author>Katsuhiko Toyama</author>
</authors>
<title>Selection of Effective Contextual Information for Automatic Synonym Acquisition.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>353--360</pages>
<marker>Hagiwara, Ogawa, Toyama, 2006</marker>
<rawString>Hagiwara, Masato, Yasuhiro Ogawa, Katsuhiko Toyama. 2006. Selection of Effective Contextual Information for Automatic Synonym Acquisition. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp.353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New Jersey.</location>
<marker>Harris, 1968</marker>
<rawString>Harris, Zelig S. 1968. Mathematical Structures of Language. Wiley, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<marker>Hindle, 1990</marker>
<rawString>Hindle, Donald. 1990. Noun Classification from Predicate-Argument Structures. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pp.268–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Similarity-Based Approaches to Natural Language Processing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, MA.</location>
<marker>Lee, 1997</marker>
<rawString>Lee, Lillian. 1997. Similarity-Based Approaches to Natural Language Processing. Ph.D. thesis, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<location>College Park, MD.</location>
<marker>Lee, 1999</marker>
<rawString>Lee, Lillian. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pp. 25–32, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pp.768–774.</booktitle>
<location>Montreal.</location>
<contexts>
<context position="3535" citStr="Lin (1998)" startWordPosition="563" endWordPosition="564"> computing distributional similarity is same; for each of two input words a context (i.e., surrounding words) is extracted from a corpus, a vector is made in which an element of the vector is a value or a weight, and two vectors are compared with a formula to compute similarity. Among these processes we have focused on features, that are elements of 32 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39, Beijing, August 2010 the vector, some of which, we think, adversely affect the performance. That is, traditional approaches such as Lin (1998) basically use all of observed words as context, that causes noise in feature vector comparison. One may agree that the number of the characteristic words to determine the meaning of a word is some, not all, of words around the target word. Thus our goal is to detect and reduce such noisy features. Zhitomirsky-Geffet and Dagan (2009) have same motivation with us and introduced a bootstrapping strategy that changes the original features weights. The general idea here is to promote the weights of features that are common for associated words, since these features are likely to be most characteri</context>
<context position="5368" citStr="Lin (1998)" startWordPosition="875" endWordPosition="876">iation to the word, that is explained in Subsection 2.4. We finally present in Subsection 2.5 feature reduction which is our core contribution of this work. Although our target language is Japanese, we use English examples in order to provide better understanding to the readers. 2.1 Feature Vector We first explain how to construct our feature vector from a text corpus. A word is represented by a feature vector, where features are collection of syntactically dependent words co-occurred in a given corpus. Thus, we first collect syntactically dependent words for each word. This is defined, as in Lin (1998), as a triple (w, r, w′), where w and w′ are words and r is a syntactic role. As for definition of word, we use not only words given by a morphological analyzer but also compound words. Nine case particles are used as syntactic roles, that roughly express subject, object, modifier, and so on, since they are easy to be obtained from text with no need of semantic analysis. In order to reduce noise we delete triples that appears only once in the corpus. We then construct a feature vector out of collection of the triples. A feature of a word is an another word syntactically dependent with a certai</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pp.768–774. Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Concept Discovery from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of 19th International Conference on Computational Linguistics, pp.577–583.</booktitle>
<location>Taipei.</location>
<marker>Lin, Pantel, 2002</marker>
<rawString>Lin, Dekang and and Patrick Pantel. 2002. Concept Discovery from Text. In Proceedings of 19th International Conference on Computational Linguistics, pp.577–583. Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerda Ruge</author>
</authors>
<title>Experiments of Linguisticallybased Term Associations.</title>
<date>1992</date>
<journal>Information Processing &amp; Management,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="7293" citStr="Ruge, 1992" startWordPosition="1223" endWordPosition="1224">at is the best weighting functions, since this is out of target. We use mutual information here because it is most widely used, i.e., in order to compare performance with others we want to adopt the standard approach. As other works do, we filter out features that have a value lower than a minimal weight thresholds α. The thresholds are determined according to our preliminary experiment, that is explained later. 2.3 Vector Similarity Similarity measures of the two vectors are computed by various measures. Shibata and Kurohashi (2009) have compared several similarity measures including Cosine (Ruge, 1992), (Lin, 33 ✓ ✏ (input word) w: boy (feature) v: guardOBJ (synonyms of w, shown with its similarity to w) Syn(w) _ { child(0.135), girl(0.271), pupil(0.143), woman(0.142), young people(0.147) } ✓ ✏ (feature vectors V ): V(boy) _ { parentsMOD, runawaySUBJ, reclaimOBJ, fatherMOD, guardOBJ , · · · } V(child) _ { guardOBJ, lookOBJ, bringOBJ, give birthOBJ, careOBJ , · · · } V(girl) _ { parentsMOD, guardOBJ, fatherMOD, testifySUBJ, lookOBJ, · · · } V(pupil) _ { targetOBJ, guardOBJ, careOBJ, aimOBJ, increaseSUBJ, · · · } V(woman) _ { nameMOD, give birthOBJ, groupMOD, together+with, parentsMOD, · · · </context>
</contexts>
<marker>Ruge, 1992</marker>
<rawString>Ruge, Gerda. 1992. Experiments of Linguisticallybased Term Associations. Information Processing &amp; Management, 28(3):317–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomohide Shibata</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Distributional similarity calculation using very large scale Web corpus.</title>
<date>2009</date>
<booktitle>In Proceedings ofAnnual Meeting ofAssociation for Natural Language Processing.</booktitle>
<pages>705--708</pages>
<contexts>
<context position="7221" citStr="Shibata and Kurohashi (2009)" startWordPosition="1211" endWordPosition="1215">riples (w, r, w′), and 5 is the number of all triples. In this paper we do not discuss what is the best weighting functions, since this is out of target. We use mutual information here because it is most widely used, i.e., in order to compare performance with others we want to adopt the standard approach. As other works do, we filter out features that have a value lower than a minimal weight thresholds α. The thresholds are determined according to our preliminary experiment, that is explained later. 2.3 Vector Similarity Similarity measures of the two vectors are computed by various measures. Shibata and Kurohashi (2009) have compared several similarity measures including Cosine (Ruge, 1992), (Lin, 33 ✓ ✏ (input word) w: boy (feature) v: guardOBJ (synonyms of w, shown with its similarity to w) Syn(w) _ { child(0.135), girl(0.271), pupil(0.143), woman(0.142), young people(0.147) } ✓ ✏ (feature vectors V ): V(boy) _ { parentsMOD, runawaySUBJ, reclaimOBJ, fatherMOD, guardOBJ , · · · } V(child) _ { guardOBJ, lookOBJ, bringOBJ, give birthOBJ, careOBJ , · · · } V(girl) _ { parentsMOD, guardOBJ, fatherMOD, testifySUBJ, lookOBJ, · · · } V(pupil) _ { targetOBJ, guardOBJ, careOBJ, aimOBJ, increaseSUBJ, · · · } V(woman)</context>
<context position="17028" citStr="Shibata and Kurohashi (2009)" startWordPosition="2895" endWordPosition="2898">dicates that similarity distinction Figure 6: Threshold vs. accuracy in Level 1+0 set. Table 1: Performance comparison of three methods in each task (in F-measures). Level S&amp;K ZG&amp;D proposed Lvl.3+Lvl.2 0.702 0.791 0.797 Lvl.2+Lvl.1 0.747 0.771 0.773 Lvl.1+Lvl.0 0.838 0.789 0.840 power in higher similarity region requires lower threshold, i.e., fewer features. In contrast, conducting fine distinction in lower similarity level requires higher threshold, i.e., a lot of features most of which may be unassociated ones. 3.5 Performance Table 1 shows performance of the proposed method, compared with Shibata and Kurohashi (2009) (S&amp;K in the table) and Zhitomirsky-Geffet and Dagan (2009) (ZG&amp;D)2. The method of Shibata and Kurohashi (2009) here is the best one among those compared. It uses only initial filtering described in Subsection 2.2. The method of ZhitomirskyGeffet and Dagan (2009) in addition emphasize associated features as explained in Subsection 2.4. All of the results in the table are the best ones among several threshold settings. The result shows that the accuracy is 0.797 (+0.006) in Level 3+2, 0.773 (+0.002) in Level 2The implementations of providing associated words and the bootstrapping are slightly d</context>
<context position="18453" citStr="Shibata and Kurohashi (2009)" startWordPosition="3125" endWordPosition="3129">irms that our method attains equivalent or better performance in all of low, middle, and high similarity levels. We also see in the table that S&amp;K and ZG&amp;D show different behavior according to the Level. However, it is important to note here that our proposed method performs equivalent or outperforms both methods in all Levels. 4 Discussions 4.1 Behavior at Each Similarity Level As we have discussed in Subsection 2.5, our method is expected to perform better than Zhitomirsky-Geffet and Dagan (2009) in distinction in lower similarity area. Roughly speaking, we interpret the results as follows. Shibata and Kurohashi (2009) always has many features that degrades the performance in higher similarity level, since the ratio of noisy features may throw into confusion. ZhitomirskyGeffet and Dagan (2009) reduces such noise that gives better performance in higher similarity level and is stable in all levels. And our proposed method maintains performance of ZhitomirskyGeffet and Dagan (2009) in higher level while improves performance that is close to Shibata and Kurohashi (2009) in lower level, utilizing fewer features. We think our method can include advantages over the two methods. 4.2 Error Analysis We overview the r</context>
</contexts>
<marker>Shibata, Kurohashi, 2009</marker>
<rawString>Shibata, Tomohide and Sadao Kurohashi. 2009. Distributional similarity calculation using very large scale Web corpus. In Proceedings ofAnnual Meeting ofAssociation for Natural Language Processing. pp. 705–708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>Co-occurrence retrieval: A Flexible Framework for Lexical Distributional Similarity.</title>
<date>2005</date>
<journal>Computational Linguistics.</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Weeds, Weir, 2005</marker>
<rawString>Weeds, Julie and David Weir. 2005. Co-occurrence retrieval: A Flexible Framework for Lexical Distributional Similarity. Computational Linguistics. 31(4):439–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Bootstrapping Distributional Feature Vector Quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="3870" citStr="Zhitomirsky-Geffet and Dagan (2009)" startWordPosition="619" endWordPosition="622">focused on features, that are elements of 32 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39, Beijing, August 2010 the vector, some of which, we think, adversely affect the performance. That is, traditional approaches such as Lin (1998) basically use all of observed words as context, that causes noise in feature vector comparison. One may agree that the number of the characteristic words to determine the meaning of a word is some, not all, of words around the target word. Thus our goal is to detect and reduce such noisy features. Zhitomirsky-Geffet and Dagan (2009) have same motivation with us and introduced a bootstrapping strategy that changes the original features weights. The general idea here is to promote the weights of features that are common for associated words, since these features are likely to be most characteristic for determining the word’s meaning. In this paper, we propose instead a method to using features that are both unassociated to the two input words, in addition to use of features that are associated to the input. 2 Method The lexical distributional similarity of the input two words is computed by comparing two vectors that expre</context>
<context position="9985" citStr="Zhitomirsky-Geffet and Dagan (2009)" startWordPosition="1686" endWordPosition="1689">ture v in w is more weighted when more synonyms (words of high similarity) of w also have v. Figure 1 illustrates this process by examples. Now we calculate a feature guardOBJ for a word boy, we first collect synonyms of w, denoted by Syn(w), from a thesaurus. We then compute similarities between w and each word in Syn(w) by Equation 2. The weight is the sum of the similarities of words in Syn(w) that have feature v, defined in Equation 5. weight(w, v) = � sim(w, wf) wf∈Asc(v)∩Syn(w) (5) 1 sim(w1, w2) = 2(simJ(w1, w2)+simS(w1, w2)) (2) 34 Figure 2: An illustration of similarity calculation of Zhitomirsky-Geffet and Dagan (2009) (a) and the proposed method (b1 and b2) in feature space. In order to measure the distance of the two words (shown in black dots) they use only associated words, while we additionally use unassociated words in which the distances to the words are similar. 2.5 Feature Reduction We finally reduce features according to the difference of weights of each feature in words we compare. In computing similarity of two words, w1 and w2, a feature v satisfying Equation 6 is reduced. abs(weight(w1, v) − weight(w2, v)) &gt; β (6) where abs() is a function of absolute value, and β is a threshold for feature re</context>
<context position="11573" citStr="Zhitomirsky-Geffet and Dagan (2009)" startWordPosition="1953" endWordPosition="1957">he associated features are determined word by word independently. In contrast, the proposed method relatively reduces features, depending on location of input two words. At (b1) in the figure, not only associated (high-colored area) but unassociated features (light-colored area) are used for similarity computation in our method. As Equation 6 shows, regardless of how much a feature is associated to the word, the feature is not reduced when it has similar weight to both w1 and w2, located at the middle area of the two words in the figure. This idea seems to work more effectively, compared with Zhitomirsky-Geffet and Dagan (2009), in case that input two words are not so similar, that is shown at (b2) of the figure. As they define associated features independently, it is likely that the overlapped area is little or none between the two words. In contrast, our method uses features at the middle area of two input words, where there is always certain features provided for similarity computation, shown in case (b2). Simplified explanation is that our similarity is computed as the ratio of the associated area to the unassociated area in the figure. We will verify later if the method works better in low similarity calculatio</context>
<context position="17087" citStr="Zhitomirsky-Geffet and Dagan (2009)" startWordPosition="2904" endWordPosition="2907">old vs. accuracy in Level 1+0 set. Table 1: Performance comparison of three methods in each task (in F-measures). Level S&amp;K ZG&amp;D proposed Lvl.3+Lvl.2 0.702 0.791 0.797 Lvl.2+Lvl.1 0.747 0.771 0.773 Lvl.1+Lvl.0 0.838 0.789 0.840 power in higher similarity region requires lower threshold, i.e., fewer features. In contrast, conducting fine distinction in lower similarity level requires higher threshold, i.e., a lot of features most of which may be unassociated ones. 3.5 Performance Table 1 shows performance of the proposed method, compared with Shibata and Kurohashi (2009) (S&amp;K in the table) and Zhitomirsky-Geffet and Dagan (2009) (ZG&amp;D)2. The method of Shibata and Kurohashi (2009) here is the best one among those compared. It uses only initial filtering described in Subsection 2.2. The method of ZhitomirskyGeffet and Dagan (2009) in addition emphasize associated features as explained in Subsection 2.4. All of the results in the table are the best ones among several threshold settings. The result shows that the accuracy is 0.797 (+0.006) in Level 3+2, 0.773 (+0.002) in Level 2The implementations of providing associated words and the bootstrapping are slightly different to ZhitomirskyGeffet and Dagan (2009). 37 2+1, and</context>
<context position="18328" citStr="Zhitomirsky-Geffet and Dagan (2009)" startWordPosition="3105" endWordPosition="3108"> (+0.001) in Level 1+0, where the degree of improvement here are those compared with best ones except our proposed method. This confirms that our method attains equivalent or better performance in all of low, middle, and high similarity levels. We also see in the table that S&amp;K and ZG&amp;D show different behavior according to the Level. However, it is important to note here that our proposed method performs equivalent or outperforms both methods in all Levels. 4 Discussions 4.1 Behavior at Each Similarity Level As we have discussed in Subsection 2.5, our method is expected to perform better than Zhitomirsky-Geffet and Dagan (2009) in distinction in lower similarity area. Roughly speaking, we interpret the results as follows. Shibata and Kurohashi (2009) always has many features that degrades the performance in higher similarity level, since the ratio of noisy features may throw into confusion. ZhitomirskyGeffet and Dagan (2009) reduces such noise that gives better performance in higher similarity level and is stable in all levels. And our proposed method maintains performance of ZhitomirskyGeffet and Dagan (2009) in higher level while improves performance that is close to Shibata and Kurohashi (2009) in lower level, ut</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>Zhitomirsky-Geffet, Maayan and Ido Dagan. 2009. Bootstrapping Distributional Feature Vector Quality. Computational Linguistics, 35(3):435–461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>