<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.075097">
<title confidence="0.993014">
Statistical Models for Topic Segmentation
</title>
<author confidence="0.962076">
Jeffrey C. Reynari
</author>
<affiliation confidence="0.931847">
Microsoft Corporation
</affiliation>
<address confidence="0.874198">
One Microsoft Way
Redmond, WA 98052 USA
</address>
<email confidence="0.970396">
jreynar@microsoft.com
</email>
<sectionHeader confidence="0.973271" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999635">
Most documents are about more than one
subject, but many NLP and IR techniques
implicitly assume documents have just one
topic. We describe new clues that mark shifts
to new topics, novel algorithms for
identifying topic boundaries and the uses of
such boundaries once identified. We report
topic segmentation performance on several
corpora as well as improvement on an IR task
that benefits from good segmentation.
</bodyText>
<sectionHeader confidence="0.812955" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999897111111111">
Dividing documents into topically-coherent
sections has many uses, but the primary
motivation for this work comes from information
retrieval (IR). Documents in many collections
vary widely in length and while the shortest may
address one topic, modest length and long
documents are likely to address multiple topics or
be comprised of sections that address various
aspects of the primary topic. Despite this fact,
most IR systems treat documents as indivisible
units and index them in their entirety.
This is problematic for two reasons. First, most
relevance metrics are based on word frequency,
which can be viewed as a function of the topic
being discussed (Church and Gale, 1995). (For
example, the word header is rare in general
English, but it enjoys higher frequency in
documents about soccer.) In general, word
frequency is a good indicator of whether a
document is relevant to a query, but consider a
long document containing only one section
relevant to a query. If a keyword is used only in
the pertinent section, its overall frequency in the
document will be low and, as a result, the
document as a whole may be judged irrelevant
despite the relevance of one section.
The second reason it would be beneficial to index
sections of documents is that, once a search engine
has identified a relevant document, users would
benefit from direct access to the relevant sections.
This problem is compounded when searching
multimedia documents. If a user wants to find a
particular news item in a database of radio or
television news programs, they may not have the
patience to suffer through a 30 minute broadcast to
find the one minute clip that interests them.
Dividing documents into sections based on topic
addresses both of these problems. IR engines can
index the resulting sections just like documents
and subsequently users can peruse those sections
their search engine deems relevant. In the next
section we will discuss the nature of our approach,
then briefly describe previous work, discuss
various indicators of topic shifts, outline novel
algorithms based on them and present our results.
</bodyText>
<sectionHeader confidence="0.821744" genericHeader="method">
1 Our Approach
</sectionHeader>
<bodyText confidence="0.984967947368421">
We treat the process of creating documents as an
instance of the noisy channel model. In this
idealization, prior to writing, the author has in
mind a collection of disjoint topics that she intends
to address. During the writing process, due to the
goals of writing smooth prose and knitting her
document into a coherent whole, she blurs the
boundaries between these topics. Thus, we assume
there is a correct segmentation that has been
hidden from our view. Our goal, therefore, is to
model the clues about the original segmentation
that were not obliterated while writing.
We view segmentation as a labeling task. Given
the text of a document and a collection of putative
topic boundary locations—which could
correspond to sentence boundaries, paragraph
boundaries, pauses between utterances, changes in
speaker or some arbitrary list of choice points—
This work was conducted as part of my Ph.D. thesis work at the University of Pennsylvania.
</bodyText>
<page confidence="0.977873">
357
</page>
<bodyText confidence="0.9997158">
we label each of them as either the location of a
topic boundary or not. We perform this labeling
using statistical algorithms that combine diverse
sources of evidence to determine the likelihood of
a topic boundary.
</bodyText>
<sectionHeader confidence="0.936229" genericHeader="method">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999471848484848">
Much research has been devoted to the task of
structuring text—that is dividing texts into units
based on information within the text. This work
falls roughly into two categories. Topic
segmentation focuses on identifying topically-
coherent blocks of text several sentences through
several paragraphs in length (e.g. see Hearst,
1994). The prime motivation for identifying such
units is to improve performance on language-
processing or IR tasks. Discourse segmentation,
on the other hand, is often finer-grained, and
focuses on identifying relations between
utterances (e.g. Grosz and Sidner, 1986 or
Hirschberg and Grosz, 1992).
Many topic segmentations algorithms have been
proposed in the literature. There is not enough
space to review them all here, so we will focus on
describing a representative sample that covers
most of the features used to predict the location
of boundaries. See (Reynar, 1998) for a more
thorough review.
Youmans devised a technique called the
Vocabulary Management Profile based on the
location of first uses of word types. He posited
that large clusters of first uses frequently
followed topic boundaries since new topics
generally introduce new vocabulary items
(Youmans, 1991).
Morris and Hirst developed an algorithm (Morris
and Hirst, 1991) based on lexical cohesion
relations (Halliday and Hasan, 1976). They used
Roget&apos;s 1977 Thesaurus to identify synonyms
and other cohesion relations.
Kozima defined a measure called the Lexical
Cohesion Profile (LCP) based on spreading
activation within a semantic network derived
from a machine-readable dictionary. He
identified topic boundaries where the LCP score
was low (Kozima, 1993).
Hearst developed a technique called TextTiling
that automatically divides expository texts into
multi-paragraph segments using the vector space
model from IR (Hearst, 1994). Topic boundaries
were positioned where the similarity between the
block of text before and after the boundary was
low.
In previous work (Reynar, 1994), we described a
method of finding topic boundaries using an
optimisation algorithm based on word repetition
that was inspired by a visualization technique
known as dotplotting (Helfman, 1994).
Ponte and Croft predict topic boundaries using a
model of likely topic length and a query expansion
technique called Local Content Analysis that maps
sets of words into a space of concepts (Ponte and
Croft, 1997).
Richmond, Smith and Amitay designed an
algorithm for topic segmentation that weighted
words based on their frequency within a document
and subsequently used these weights in a formula
based on the distance between repetitions of word
types (Richmond et al., 1997).
Beeferman, Berger and Lafferty used the relative
performance of two statistical language models
and cue words to identify topic boundaries
(Beeferman et al., 1997).
</bodyText>
<sectionHeader confidence="0.869882" genericHeader="method">
3 New Clues for Topic Segmentation
</sectionHeader>
<bodyText confidence="0.999969636363636">
Prior work on topic segmentation has exploited
many different hints about where topic boundaries
lie. The algorithms we present use many cues from
the literature as well as novel ones. Our approach
is statistical in nature and weights evidence based
on its utility in segmenting a training corpus. As a
result, we do not use clues to form hard and fast
rules. Instead, they all contribute evidence used to
either increase or decrease the likelihood of
proposing a topic boundary between two regions
of text.
</bodyText>
<subsectionHeader confidence="0.999954">
3.1 Domain-specific Cue Phrases
</subsectionHeader>
<bodyText confidence="0.997630666666667">
Many discourse segmentation techniques (e.g.
Hirschberg and Litman, 1993) as well as some
topic segmentation algorithms rely on cue words
and phrases (e.g. Beeferman et al., 1997), but the
types of cue words used vary greatly. Those we
employ are highly domain specific. Taking an
</bodyText>
<page confidence="0.997634">
358
</page>
<bodyText confidence="0.9998164">
example from the broadcast news domain where
we will demonstrate the effectiveness of our
algorithms, the phrase joining us is a good
indicator that a topic shift has just occurred
because news anchors frequently say things such
as joining us to discuss the crisis in Kosovo is
Congressman... when beginning new stories.
Consequently, our algorithms use the presence of
phrases such as this one to boost the probability
of a topic boundary having occurred.
</bodyText>
<table confidence="0.962939428571429">
joining us
good evening
brought to you by
this just in
welcome back
&lt;person name&gt; &lt;station&gt;
this is &lt;person name&gt;
</table>
<tableCaption confidence="0.994377">
Table 1: A sampling of domain-specific cue phrases
we employ.
</tableCaption>
<bodyText confidence="0.999909512820513">
Some cue phrases are more complicated and
contain word sequences of particular types. Not
surprisingly, the phrase this is is common in
broadcast news. When it is followed by a
person&apos;s name, however, it serves as a good clue
that a topic is about to end. This is &lt;person
name&gt; is almost always said when a reporter is
signing off after finishing an on-location report.
Generally such signoffs are followed by the start
of new news stories. A sampling of the cue
phrases we use is found in Table 1. Since our
training corpus was relatively small we identified
these by hand, but on a different corpus we
induced them automatically (Reynar, 1998). The
results we present later in the paper rely solely on
manually identified cues phrases.
Identifying complex cue phrases involves pattern
matching and determining whether particular
word sequences belong to various classes. To
address this, we built a named entity recognition
system in the spirit of those used for the Message
Understanding Conference evaluations (e.g. Bikel
et al., 1997). Our named entity recognizer used a
maximum entropy model built with Adwait
Ratnaparkhi&apos;s tools (Ratnaparkhi, 1996) to label
word sequences as either person, place, company
or none of the above based on local cues
including the surrounding words and whether
honorifics (e.g. Mrs. or Gen.) or corporate
designators (e.g. Corp. or Inc.) were present. Our
algorithm&apos;s labelling accuracy of 96.0% by token
was sufficient for our purposes, but performance is
not directly comparable to the MUC competitors&apos;.
Though we trained from the same data, we
preprocessed the data to remove punctuation and
capitalization so the model could be applied to
broadcast news data that lacked these helpful
clues. We separately identified television network
acronyms using simple regular expressions.
</bodyText>
<subsectionHeader confidence="0.999941">
3.2 Word Bigram Frequency
</subsectionHeader>
<bodyText confidence="0.999990333333333">
Many topic segmentation algorithms in the
literature use word frequency (e.g. Hearst, 1994;
Reynar, 1994; Beeferman et al., 1997). An
obvious extension to using word frequency is to
use the frequency of multi-word phrases. Such
phrases are useful because they approximate word
sense disambiguation techniques. Algorithms that
rely exclusively on word frequency might be
fooled into suggesting that two stretches of text
containing the word plant were part of the same
story simply because of the rarity of plant and the
low odds that two adjacent stories contained it due
to chance. However, if plant in one section
participated in bigrams such as wild plant, native
plant and woody plant but in the other section was
only in the bigrams chemical plant, manufacturing
plant and processing plant, the lack of overlap
between sets of bigrams could be used to decrease
the probability that the two sections of text were in
the same story. We limited the bigrams we used to
those containing two content words.
</bodyText>
<subsectionHeader confidence="0.999984">
3.3 Repetition of Named Entities
</subsectionHeader>
<bodyText confidence="0.99999575">
The named entities we identified for use in cue
phrases are also good indicators of whether two
sections are likely to be in the same story or not.
Companies, people and places figure prominently
in many documents, particularly those in the
domain of broadcast news. The odds that different
stories discuss the same entities are generally low.
There are obviously exceptions—the President of
the U.S. may figure in many stories in a single
broadcast—but nonetheless the presence of the
same entities in two blocks of text suggest that
they are likely to be part of the same story.
</bodyText>
<subsectionHeader confidence="0.998287">
3.4 Pronoun Usage
</subsectionHeader>
<bodyText confidence="0.99988175">
In her dissertation, Levy described a study of the
impact of the type of referring expressions used,
the location of first mentions of people and the
gestures speakers make upon the cohesiveness of
</bodyText>
<page confidence="0.994951">
359
</page>
<bodyText confidence="0.99977535">
discourse (Levy, 1984). She found a strong
correlation between the types of referring
expressions people used, in particular how
explicit
they were, and the degree of cohesiveness with
the preceding context. Less cohesive utterances
generally contained more explicit referring
expressions, such as definite noun phrases or
phrases consisting of a possessive followed by a
noun, while more cohesive utterances more
frequently contained zeroes and pronouns.
We will use the converse of Levy&apos;s observation
about pronouns to gauge the likelihood of a topic
shift. Since Levy generally found pronouns in
utterances that exhibited a high degree of
cohesion with the prior context, we assume that
the presence of a pronoun among the first words
immediately following a putative topic boundary
provides some evidence that no topic boundary
actually exists there.
</bodyText>
<sectionHeader confidence="0.983343" genericHeader="method">
4 Our Algorithms
</sectionHeader>
<bodyText confidence="0.99993725">
We designed two algorithms for topic
segmentation. The first is based solely on word
frequency and the second combines the results of
the first with other sources of evidence. Both of
these algorithms are applied to text following
some preprocessing including tokenization,
conversion to lowercase and the application of a
lemmatizer (Karp et al., 1992).
</bodyText>
<subsectionHeader confidence="0.999888">
4.1 Word Frequency Algorithm
</subsectionHeader>
<bodyText confidence="0.999953764705882">
Our word frequency algorithm uses Katz&apos;s G
model (Katz, 1996). The G model stipulates that
words occur in documents either topically or non-
topically. The model defines topical words as
those that occur more than 1 time, while non-
topical words occur only once. Counterexamples
of these uses of topical and nontopical, of course,
abound.
We use the G model, shown below, to determine
the probability that a particular word, w, occurred
k times in a document. We trained the model
from a corpus of 78 million words of Wall Street
Journal text and smoothed the parameters using
Dan Melamed&apos;s implementation of Good-Turing
smoothing (Gale and Sampson, 1995) and
additional ad hoc smoothing to account for
unknown words.
</bodyText>
<equation confidence="0.996092">
Pr(k, w) = (1 – )(5k,0 + a (1 – ,v)b k,i
n,
(aw 1 )k-2 )(1 8k.0 Ok.1)
B-1 B,-1
</equation>
<bodyText confidence="0.999687512195122">
ass, is the probability that a document contains at
least 1 occurrence of word w.
y v, is the probability that w is used topically in a
document given that it occurs at all.
B,,, is the average number of occurrences in
documents with more than 1 occurrence of w.
g is a function with value 1 if x = y and 0
otherwise.
The simplest way to view the G model is to
decompose it into 3 separate terms that are
summed. The first term is the probablility of zero
occurrences of a word, the second is the
probability of one occurrence and the third is the
probability of any number of occurrences greater
than one.
To detect topic boundaries, we used the model to
answer this simple question. Is it more or less
likely that the words following a putative topic
boundary were generated independently of those
before it?
Given a potential topic boundary, we call the text
before the boundary region 1 and the text after it
region 2. For the sake of our algorithm, the size of
these regions was fixed at 230 words—the average
size of a topic segment in our training corpus, 30
files from the HUB-4 Broadcast News Corpus
annotated with topic boundaries by the LDC
(HUB-4, 1996). Since the G model, unlike
language models used for speech recognition,
computes the probability of a bag of words rather
than a word sequence, we can use it to compute
the probability of some text given knowledge of
what words have occurred before that text. We
computed two probabilities with the model. is
the probability that region 1 and region 2 discuss
the same subject matter and hence that there is no
topic boundary between them. P„,, is the
probability that they discuss different subjects and
are separated by a topic boundary. therefore,
is the probability of seeing the words in region 2
given the context, called C, of region 1. P is the
</bodyText>
<page confidence="0.987361">
360
</page>
<bodyText confidence="0.977764619047619">
probability of seeing the words in region 2
independent of the words in region 1. Formulae
for Pmte and P are shown below. Boundaries
were placed where P was greater than /3 by a
certain threshold. The threshold was used to
trade precision for recall and vice versa when
identifying topic boundaries. The most natural
threshold is a very small nonzero value, which is
equivalent to placing a boundary wherever P is
greater than
Pone = Pr(k, w I C) Po.,0 = Pr(k, w)
Computing 13,, is straightforward, but Ponerequires
computing conditional probabilities of the
number of occurrences of each word in region 2
given the number in region 1. The formulae for
the conditional probabilities are shown in Table
2. We do not have space to derive these formulae
here, but they can be found in (Reynar, 1998). M
is a normalizing term required to make the
conditional probabilities sum to 1. In the table,
x+ means x occurrences or more.
</bodyText>
<table confidence="0.9883401875">
Occurrences Occurrences Conditional probability
in region 1 in region 2
0 0 I - a
0 1 a( 1-y)
0 2+ a - y 1 k_,
-
(1 )
B-1 B-1
1 o 1-y
1 1+ r I k-1
-
(I )
B-1 B-1
2+ 0+ 1 1 lc-,
(1 )
M(B-1) B-1
</table>
<tableCaption confidence="0.998078">
Table 2: Conditional probabilities used to compute
</tableCaption>
<subsectionHeader confidence="0.996809">
4.2 A Maximum Entropy Model
</subsectionHeader>
<bodyText confidence="0.9998635">
Our second algorithm is a maximum entropy
model that uses these features:
</bodyText>
<listItem confidence="0.991017411764706">
• Did our word frequency algorithm
suggest a topic boundary?
• Which domain cues (such as Joining us
or This is &lt;person&gt;) were present?
• How many content word bigrams were
common to both regions adjoining the
putative topic boundary?
• How many named entities were common
to both regions?
• How many content words in both regions
were synonyms according to WordNet
(Miller et al., 1990)?
• What percentage of content words in the
region after the putative boundary were
first uses?
• Were pronouns used in the first five words
after the putative topic boundary?
</listItem>
<bodyText confidence="0.9981015">
We trained this model from 30 files of HUB-4
data that was disjoint from our test data.
</bodyText>
<sectionHeader confidence="0.994325" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999833666666667">
We will present results for broadcast news data
and for identifying chapter boundaries labelled by
authors.
</bodyText>
<subsectionHeader confidence="0.997573">
5.1 HUB-4 Corpus Performance
</subsectionHeader>
<bodyText confidence="0.999926047619048">
Table 3 shows the results of segmenting the test
portion of the HUB-4 corpus, which consisted of
transcribed broadcasts divided into segments by
the LDC. We measured performance by
comparing our segmentation to the gold standard
annotation produced by the LDC.
The row labelled Random guess shows the
performance of a baseline algorithm that randomly
guessed boundary locations with probability equal
to the fraction of possible boundary sites that were
boundaries in the gold standard. The row
TextTiling shows the performance of the publicly
available version of that algorithm (Hearst, 1994).
Optimization is the algorithm we proposed in
(Reynar, 1994). Word frequency and Max. Ent.
Model are the algorithms we described above. Our
word frequency algorithm does better than chance,
TextTiling and our previous work and our
maximum entropy model does better still. See
(Reynar, 1998) for graphs showing the effects of
trading precision for recall with these models.
</bodyText>
<table confidence="0.998576833333333">
Algorithm Precision Recall
Random guess 0.16 0.16
TextTiling 0.21 0.41
Optimization 0.36 0.20
Word Frequency 0.55 0.52
Max. Ent. Model 0.59 0.60
</table>
<tableCaption confidence="0.999935">
Table 3: Performance on the HUB-4 English corpus.
</tableCaption>
<page confidence="0.998609">
361
</page>
<bodyText confidence="0.999981944444445">
We also tested our models on speech-recognized
broadcasts from the 1997 TREC spoken
document retrieval corpus. We did not have
sufficient data to train the maximum entropy
model, but our word frequency algorithm
achieved precision of 0.36 and recall of 0.52,
considerably better than the baseline of 0.19
precision and recall. Using manually produced
transcripts of the same data naturally yielded
better performance—precision was 0.50 and
recall 0.58.
Our performance on broadcast data was
surprisingly good considering we trained the
word frequency model from newswire data.
Given a large corpus of broadcast data, we expect
our algorithms would perform even better.
We were curious, however, how much of the
performance was attributable to having numerous
parameters (3 per word) in the G model and how
much comes from the nature of the model. To
address this, we discarded the a, y and B
parameters particular to each word and instead
used the same parameter values for each word—
namely, those assigned to unknown words
through our smoothing process. This reduced the
number of parameters from 3 per word to only 3
parameters total. Performance of this hobbled
version of our word frequency algorithm was so
good on the HUB-4 English corpus—achieving
precision of 0.42 and recall of 0.50—that we
tested it on Spanish broadcast news data from the
HUB-4 corpus. Even for that corpus we found
much better than baseline performance. Baseline
for Spanish was precision and recall of 0.28, yet
our 3-parameter word frequency model achieved
0.50 precision and recall of 0.62. To reiterate, we
used our word frequency model with a total of 3
parameters trained from English newswire text to
segment Spanish broadcast news data
We believe that the G model, which captures the
notion of burstiness very well, is a good model
for segmentation. However, the more important
lesson from this work is that the concept of
burstiness alone can be used to segment texts.
Segmentation performance is better when models
have accurate measures of the likelihood of 0, 1
and 2 or more occurrences of a word. However,
the mere fact that content words are bursty and
are relatively unlikely to appear in neighboring
regions of a document unless those two regions are
about the same topic is sufficient to segment many
texts. This explains our ability to segment Spanish
broadcast news using a 3 parameter model trained
from English newswire data.
</bodyText>
<subsectionHeader confidence="0.999959">
5.2 Recovering Authorial Structure
</subsectionHeader>
<bodyText confidence="0.997743119047619">
Authors endow some types of documents with
structure as they write. They may divide
documents into chapters, chapters into sections,
sections into subsections and so forth. We
exploited these structures to evaluate topic
segmentation techniques by comparing
algorithmic determinations of structure to the
author&apos;s original divisions. This method of
evaluation is especially useful because numerous
documents are now available in electronic form.
We tested our word frequency algorithm on four
randomly selected texts from Project Gutenberg.
The four texts were Thomas Paine&apos;s pamphlet
Common Sense which was published in 1791, the
first &apos;volume of Decline and Fall of the Roman
Empire by Edward Gibbon, G.K. Chesterton&apos;s
book Orthodoxy and Herman Melville&apos;s classic
Moby Dick. We permitted the algorithm to guess
boundaries only between paragraphs, which were
marked by blank lines in each document.
To assess performance, we set the number of
boundaries to be guessed to the number the
authors themselves had identified. As a result, this
evaluation focuses solely on the algorithm&apos;s ability
to rank candidate boundaries and not on its
adeptness at determining how many boundaries to
select. To evaluate performance, we computed the
accuracy of the algorithm&apos;s guesses compared to
the chapter boundaries the authors identified. The
documents we used for this evaluation may have
contained legitimate topic boundaries which did
not correspond to chapter boundaries, but we
scored guesses at those boundaries incorrect.
Table 4 presents results for the four works. Our
algorithm performed better than randomly
assigning boundaries for each of the documents
except the pamphlet Common Sense. Performance
on the other three works was significantly better
than chance and ranged from an improvement of a
factor of three in accuracy over the baseline to a
factor of nearly 9 for the lengthy Decline and Fall
of the Roman Empire.
</bodyText>
<page confidence="0.994533">
362
</page>
<table confidence="0.992293">
Conclusion
Work # of Word Random
Boundaries Frequency
Common 7 0.00 0.36
Sense
Decline 53 0.21 0.0024
and Fall
Moby 132 0.55 0.173
Dick
Orthodoxy 8 0.25 0.033
Combined 200 0.43 0.059
</table>
<tableCaption confidence="0.998209">
Table 4: Accuracy of the Word Frequency
algorithm on identifying chapter boundaries.
</tableCaption>
<subsectionHeader confidence="0.973834">
5.3 IR Task Performance
</subsectionHeader>
<bodyText confidence="0.999612695652174">
The data from the HUB-4 corpus was also used
for the TREC Spoken document retrieval task.
We tested the utility of our segmentations by
comparing IR performance when we indexed
documents, the segments annotated by the LDC
and the segments identified by our algorithms.
We modified SMART (Buckley, 1985) to
perform better normalization for variations in
document length (Singhal et al., 1996) prior to
conducting our IR experiments.
This IR task is atypical in that there is only 1
relevant document in the collection for each
query. Consequently, performance is measured
by determining the average rank determined by
the IR system for the document relevant to each
query. Perfect performance would be an average
rank of 1, hence lower average ranks are better.
Table 5 presents our results. Note that indexing
the segments identified by our algorithms was
better than indexing entire documents and that
our best algorithm even outperformed indexing
the gold standard annotation produced by the
LDC.
</bodyText>
<table confidence="0.997217">
Method Average Rank
Documents 9.52
Annotator segments 8.42
Word frequency model 9.48
Max. Ent. Model 7.54
</table>
<tableCaption confidence="0.9832665">
Table 5: Performance on an IR task. Lower
numbers are better.
</tableCaption>
<bodyText confidence="0.99998088">
We described two new algorithms for topic
segmentation. The first, based solely on word
frequency, performs better than previous
algorithms on• broadcast news data. It performs
well on speech recognized English despite
recognition errors. Most surprisingly, a version of
our first model that requires little training data
could segment Spanish broadcast news documents
as well—even with parameters estimated from
English documents. Our second technique, a
statistical model that combined numerous clues
about segmentation, performs better than the first,
but requires segmented training data.
We showed an improvement on a simple JR task
to demonstrate the potential of topic segmentation
algorithms for improving IR. Other potential uses
of these algorithms include better language
modeling by building topic-based language
models, improving NLP algorithms (e.g.
coreference resolution), summarization, hypertext
linking (Salton and Buckley, 1992), automated
essay grading (Burstein et al., 1997) and topic
detection and tracking (TDT program committee,
1998). Some of these are discussed in (Reynar,
1998), and others will be addressed in future work.
</bodyText>
<sectionHeader confidence="0.976032" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.994798714285714">
My thanks to the anonymous reviewers and the
members of my thesis committee, Mitch Marcus,
Aravind Joshi, Mark Liberman, Julia Hirschberg and
Lyle Ungar for useful feedback. Thanks also to Dan
Melamed for use of his smoothing tools and to Adwait
Ratnaparkhi for use of his maximum entropy modelling
software.
</bodyText>
<sectionHeader confidence="0.990174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998273142857143">
Beefennan, D., Berger, A., and Lafferty, J. (1997). Text
segmentation using exponential models. In
Proceedings of the Second Conference on
Empirical Methods in Natural Language
Processing, pages 35-46, Providence, Rhode
Island.
Bikel, D.M., Miller, S., Schwartz, R., and Weischedel,
R. (1997). Nymble: a high-performance learning
name-finder. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 194-201, Washington, D.C.
Buckley, C. (1985). Implementation of the SMART
information retrieval system. Technical Report
Technical Report 85-686, Cornell University.
</reference>
<page confidence="0.995999">
363
</page>
<reference confidence="0.998097428571429">
Burstein, J., Wolff, S., Lu, C., and Kaplan, R.(1997).
An automatic scoring system for advanced
placement biology essays. In Proceedings of the
Fifth Conference on Applied Natural Language
Processing, pages 174-181, Washington, D.C.
Church, K.W. and Gale, W.A. (1995). Inverse
document frequency (IDF): A measure of
deviations from Poisson. In Yarowsky, D. and
Church, K., editors, Proceedings of the Third
Workshop on Very Large Corpora, pages 121-
130. Association for Computational Linguistics.
Gale, W. and Sampson, G. (1995). Good-Turing
smoothing without tears. Journal of Quantitative
Linguistics, 2.
Grosz, B. J. and Sidner, C.L. (1986). Attention,
Intentions and the Structure of Discourse.
Computational Linguistics, 12 (3): 175-204.
Halliday, M. and Hasan, R. (1976). Cohesion in
English. Longman Group, New York.
Hearst, M.A. (1994). Multi-paragraph segmentation of
expository text. In Proceedings of the 32&amp;quot; Annual
Meeting of the Association for Computational
Linguistics, pages 9-16, Las Cruces, New Mexico.
Helfman, J.I. (1994). Similarity patterns in language.
In IEEE Symposium on Visual Languages.
Hirschberg, J. and Grosz, B. (1992). Intonational
features of local and global discourse. In
Proceedings of the Workshop on Spoken
Language Systems, pages 441-446. DARPA.
Hirschberg, J. and Litman, D. (1993). Empirical
studies on the disambiguation of cue phrases.
Computational Linguistics, 19(3):501-530.
HUB-4 Program Committee (1996). The 1996 HUB-4
annotation specification for evaluation of speech
recognition on broadcast news, version 3.5.
Karp, D., Schabes, Y:, Zaidel, M. and Egedi, D.
(1992). A Freely Available Wide Coverage
Morphological Analyzer for English. Proceedings
of the 15&amp;quot; International Conference on
Computational Linguistics. Nantes, France.
Katz, S.M. (1996). Distribution of content words and
phrases in text and language modeling. Natural
Language Engineering, 2(1):15-59.
Kozima, H. (1993). Text segmentation based on
similarity between words. In Proceedings of the
31&apos; Annual Meeting of the Association for
Computational Linguistics, Student Session,
pages 286-288.
Levy, E.T. (1984). Communicating Thematic
Structure in Narrative Discourse: The Use of
Referring Terms and Gestures. Ph.D. thesis,
University of Chicago.
Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D.,
and Miller, K. (1990). Five papers on WordNet.
Technical report, Cognitive Science Laboratory,
Princeton University.
Morris, J. and Hirst, G. (1991). Lexical cohesion
computed by thesaural relations as an indicator of
the structure of text. Computational Linguistics,
17(1):21-42.
Ponte, J.M. and Croft, W.B. (1997). Text segmentation
by topic. In European Conference on Digital
Libraries, pages 113-125, Pisa, Italy.
Ratnaparkhi, A: (1996). A maximum entropy model for
part-of-speech tagging. In Proceedings of the First
Conference on Empirical Methods in Natural
Language Processing, pages 133-142, University
of Pennsylvania.
Reynar, J.C. (1994). An automatic method of finding
topic boundaries. In Proceedings of the 32&amp;quot; Annual
Meeting of the Association for Computational
Linguistics, Student Session, pages 331-333, Las
Cruces, New Mexico.
Reynar, J.C. (1998). Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of
Pennsylvania, Department of Computer Science.
Richmond, K., Smith, A., and Amitay, E. (1997).
Detecting subject boundaries within text: A
language independent statistical approach. In
Exploratory Methods in Natural Language
Processing, pages 47-54, Providence, Rhode
Island.
Salton, G. and Buckley, C. (1992). Automatic text
structuring experiments. In Jacobs, P.S., editor,
Text-Based Intelligent Systems: Current Research
and Practice in Information Extraction and
Retrieval, pages 199-210. Lawrence Erlbaum
Associates, Hillsdale, New Jersey.
Singhal, A., Buckley, C., and Mitra, M. (1996). Pivoted
document length normalization. In Proceedings of
the ACM-SIGIR Conference on Research and
Development in Information Retrieval, pages 21-
29, Zurich, Switzerland. ACM.
TDT Program Committee (1998). Topic Detection and
Tracking Phase 2 Evaluation Plan, version 2.1.
Youmans, G. (1991). A new tool for discourse analysis:
The vocabulary management profile. Language,
67(4):763-789.
</reference>
<page confidence="0.998927">
364
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976474">
<title confidence="0.99973">Statistical Models for Topic Segmentation</title>
<author confidence="0.999974">Jeffrey C Reynari</author>
<affiliation confidence="0.999983">Microsoft Corporation</affiliation>
<address confidence="0.996861">One Microsoft Way Redmond, WA 98052 USA</address>
<email confidence="0.999898">jreynar@microsoft.com</email>
<abstract confidence="0.998399363636364">Most documents are about more than one subject, but many NLP and IR techniques implicitly assume documents have just one topic. We describe new clues that mark shifts to new topics, novel algorithms for identifying topic boundaries and the uses of such boundaries once identified. We report topic segmentation performance on several corpora as well as improvement on an IR task that benefits from good segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beefennan</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Text segmentation using exponential models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>35--46</pages>
<location>Providence, Rhode Island.</location>
<marker>Beefennan, Berger, Lafferty, 1997</marker>
<rawString>Beefennan, D., Berger, A., and Lafferty, J. (1997). Text segmentation using exponential models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 35-46, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a high-performance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<location>Washington, D.C.</location>
<contexts>
<context position="9245" citStr="Bikel et al., 1997" startWordPosition="1465" endWordPosition="1468">of new news stories. A sampling of the cue phrases we use is found in Table 1. Since our training corpus was relatively small we identified these by hand, but on a different corpus we induced them automatically (Reynar, 1998). The results we present later in the paper rely solely on manually identified cues phrases. Identifying complex cue phrases involves pattern matching and determining whether particular word sequences belong to various classes. To address this, we built a named entity recognition system in the spirit of those used for the Message Understanding Conference evaluations (e.g. Bikel et al., 1997). Our named entity recognizer used a maximum entropy model built with Adwait Ratnaparkhi&apos;s tools (Ratnaparkhi, 1996) to label word sequences as either person, place, company or none of the above based on local cues including the surrounding words and whether honorifics (e.g. Mrs. or Gen.) or corporate designators (e.g. Corp. or Inc.) were present. Our algorithm&apos;s labelling accuracy of 96.0% by token was sufficient for our purposes, but performance is not directly comparable to the MUC competitors&apos;. Though we trained from the same data, we preprocessed the data to remove punctuation and capital</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Bikel, D.M., Miller, S., Schwartz, R., and Weischedel, R. (1997). Nymble: a high-performance learning name-finder. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 194-201, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
</authors>
<title>Implementation of the SMART information retrieval system.</title>
<date>1985</date>
<tech>Technical Report Technical Report 85-686,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="23990" citStr="Buckley, 1985" startWordPosition="3903" endWordPosition="3904">e Roman Empire. 362 Conclusion Work # of Word Random Boundaries Frequency Common 7 0.00 0.36 Sense Decline 53 0.21 0.0024 and Fall Moby 132 0.55 0.173 Dick Orthodoxy 8 0.25 0.033 Combined 200 0.43 0.059 Table 4: Accuracy of the Word Frequency algorithm on identifying chapter boundaries. 5.3 IR Task Performance The data from the HUB-4 corpus was also used for the TREC Spoken document retrieval task. We tested the utility of our segmentations by comparing IR performance when we indexed documents, the segments annotated by the LDC and the segments identified by our algorithms. We modified SMART (Buckley, 1985) to perform better normalization for variations in document length (Singhal et al., 1996) prior to conducting our IR experiments. This IR task is atypical in that there is only 1 relevant document in the collection for each query. Consequently, performance is measured by determining the average rank determined by the IR system for the document relevant to each query. Perfect performance would be an average rank of 1, hence lower average ranks are better. Table 5 presents our results. Note that indexing the segments identified by our algorithms was better than indexing entire documents and that</context>
</contexts>
<marker>Buckley, 1985</marker>
<rawString>Buckley, C. (1985). Implementation of the SMART information retrieval system. Technical Report Technical Report 85-686, Cornell University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Burstein</author>
<author>S Wolff</author>
<author>C Lu</author>
<author>R Kaplan</author>
</authors>
<title>An automatic scoring system for advanced placement biology essays.</title>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>174--181</pages>
<location>Washington, D.C.</location>
<marker>Burstein, Wolff, Lu, Kaplan, </marker>
<rawString>Burstein, J., Wolff, S., Lu, C., and Kaplan, R.(1997). An automatic scoring system for advanced placement biology essays. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 174-181, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Inverse document frequency (IDF): A measure of deviations from Poisson.</title>
<date>1995</date>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>121--130</pages>
<editor>In Yarowsky, D. and Church, K., editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<contexts>
<context position="1266" citStr="Church and Gale, 1995" startWordPosition="190" endWordPosition="193">uses, but the primary motivation for this work comes from information retrieval (IR). Documents in many collections vary widely in length and while the shortest may address one topic, modest length and long documents are likely to address multiple topics or be comprised of sections that address various aspects of the primary topic. Despite this fact, most IR systems treat documents as indivisible units and index them in their entirety. This is problematic for two reasons. First, most relevance metrics are based on word frequency, which can be viewed as a function of the topic being discussed (Church and Gale, 1995). (For example, the word header is rare in general English, but it enjoys higher frequency in documents about soccer.) In general, word frequency is a good indicator of whether a document is relevant to a query, but consider a long document containing only one section relevant to a query. If a keyword is used only in the pertinent section, its overall frequency in the document will be low and, as a result, the document as a whole may be judged irrelevant despite the relevance of one section. The second reason it would be beneficial to index sections of documents is that, once a search engine h</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>Church, K.W. and Gale, W.A. (1995). Inverse document frequency (IDF): A measure of deviations from Poisson. In Yarowsky, D. and Church, K., editors, Proceedings of the Third Workshop on Very Large Corpora, pages 121-130. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>G Sampson</author>
</authors>
<title>Good-Turing smoothing without tears.</title>
<date>1995</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>2</volume>
<contexts>
<context position="13800" citStr="Gale and Sampson, 1995" startWordPosition="2191" endWordPosition="2194">uses Katz&apos;s G model (Katz, 1996). The G model stipulates that words occur in documents either topically or nontopically. The model defines topical words as those that occur more than 1 time, while nontopical words occur only once. Counterexamples of these uses of topical and nontopical, of course, abound. We use the G model, shown below, to determine the probability that a particular word, w, occurred k times in a document. We trained the model from a corpus of 78 million words of Wall Street Journal text and smoothed the parameters using Dan Melamed&apos;s implementation of Good-Turing smoothing (Gale and Sampson, 1995) and additional ad hoc smoothing to account for unknown words. Pr(k, w) = (1 – )(5k,0 + a (1 – ,v)b k,i n, (aw 1 )k-2 )(1 8k.0 Ok.1) B-1 B,-1 ass, is the probability that a document contains at least 1 occurrence of word w. y v, is the probability that w is used topically in a document given that it occurs at all. B,,, is the average number of occurrences in documents with more than 1 occurrence of w. g is a function with value 1 if x = y and 0 otherwise. The simplest way to view the G model is to decompose it into 3 separate terms that are summed. The first term is the probablility of zero oc</context>
</contexts>
<marker>Gale, Sampson, 1995</marker>
<rawString>Gale, W. and Sampson, G. (1995). Good-Turing smoothing without tears. Journal of Quantitative Linguistics, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<date>1986</date>
<journal>Attention, Intentions and the Structure of Discourse. Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="4482" citStr="Grosz and Sidner, 1986" startWordPosition="710" endWordPosition="713"> boundary. 2 Previous Work Much research has been devoted to the task of structuring text—that is dividing texts into units based on information within the text. This work falls roughly into two categories. Topic segmentation focuses on identifying topicallycoherent blocks of text several sentences through several paragraphs in length (e.g. see Hearst, 1994). The prime motivation for identifying such units is to improve performance on languageprocessing or IR tasks. Discourse segmentation, on the other hand, is often finer-grained, and focuses on identifying relations between utterances (e.g. Grosz and Sidner, 1986 or Hirschberg and Grosz, 1992). Many topic segmentations algorithms have been proposed in the literature. There is not enough space to review them all here, so we will focus on describing a representative sample that covers most of the features used to predict the location of boundaries. See (Reynar, 1998) for a more thorough review. Youmans devised a technique called the Vocabulary Management Profile based on the location of first uses of word types. He posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (Yo</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. J. and Sidner, C.L. (1986). Attention, Intentions and the Structure of Discourse. Computational Linguistics, 12 (3): 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group,</publisher>
<location>New York.</location>
<contexts>
<context position="5223" citStr="Halliday and Hasan, 1976" startWordPosition="825" endWordPosition="828">not enough space to review them all here, so we will focus on describing a representative sample that covers most of the features used to predict the location of boundaries. See (Reynar, 1998) for a more thorough review. Youmans devised a technique called the Vocabulary Management Profile based on the location of first uses of word types. He posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (Youmans, 1991). Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). They used Roget&apos;s 1977 Thesaurus to identify synonyms and other cohesion relations. Kozima defined a measure called the Lexical Cohesion Profile (LCP) based on spreading activation within a semantic network derived from a machine-readable dictionary. He identified topic boundaries where the LCP score was low (Kozima, 1993). Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space model from IR (Hearst, 1994). Topic boundaries were positioned where the similarity between the block of text before and after t</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, M. and Hasan, R. (1976). Cohesion in English. Longman Group, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32&amp;quot; Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="4220" citStr="Hearst, 1994" startWordPosition="674" endWordPosition="675">is work at the University of Pennsylvania. 357 we label each of them as either the location of a topic boundary or not. We perform this labeling using statistical algorithms that combine diverse sources of evidence to determine the likelihood of a topic boundary. 2 Previous Work Much research has been devoted to the task of structuring text—that is dividing texts into units based on information within the text. This work falls roughly into two categories. Topic segmentation focuses on identifying topicallycoherent blocks of text several sentences through several paragraphs in length (e.g. see Hearst, 1994). The prime motivation for identifying such units is to improve performance on languageprocessing or IR tasks. Discourse segmentation, on the other hand, is often finer-grained, and focuses on identifying relations between utterances (e.g. Grosz and Sidner, 1986 or Hirschberg and Grosz, 1992). Many topic segmentations algorithms have been proposed in the literature. There is not enough space to review them all here, so we will focus on describing a representative sample that covers most of the features used to predict the location of boundaries. See (Reynar, 1998) for a more thorough review. Y</context>
<context position="5723" citStr="Hearst, 1994" startWordPosition="897" endWordPosition="898">st developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). They used Roget&apos;s 1977 Thesaurus to identify synonyms and other cohesion relations. Kozima defined a measure called the Lexical Cohesion Profile (LCP) based on spreading activation within a semantic network derived from a machine-readable dictionary. He identified topic boundaries where the LCP score was low (Kozima, 1993). Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space model from IR (Hearst, 1994). Topic boundaries were positioned where the similarity between the block of text before and after the boundary was low. In previous work (Reynar, 1994), we described a method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by a visualization technique known as dotplotting (Helfman, 1994). Ponte and Croft predict topic boundaries using a model of likely topic length and a query expansion technique called Local Content Analysis that maps sets of words into a space of concepts (Ponte and Croft, 1997). Richmond, Smith and Amitay designed an a</context>
<context position="10142" citStr="Hearst, 1994" startWordPosition="1602" endWordPosition="1603">en.) or corporate designators (e.g. Corp. or Inc.) were present. Our algorithm&apos;s labelling accuracy of 96.0% by token was sufficient for our purposes, but performance is not directly comparable to the MUC competitors&apos;. Though we trained from the same data, we preprocessed the data to remove punctuation and capitalization so the model could be applied to broadcast news data that lacked these helpful clues. We separately identified television network acronyms using simple regular expressions. 3.2 Word Bigram Frequency Many topic segmentation algorithms in the literature use word frequency (e.g. Hearst, 1994; Reynar, 1994; Beeferman et al., 1997). An obvious extension to using word frequency is to use the frequency of multi-word phrases. Such phrases are useful because they approximate word sense disambiguation techniques. Algorithms that rely exclusively on word frequency might be fooled into suggesting that two stretches of text containing the word plant were part of the same story simply because of the rarity of plant and the low odds that two adjacent stories contained it due to chance. However, if plant in one section participated in bigrams such as wild plant, native plant and woody plant b</context>
<context position="18432" citStr="Hearst, 1994" startWordPosition="3020" endWordPosition="3021">Corpus Performance Table 3 shows the results of segmenting the test portion of the HUB-4 corpus, which consisted of transcribed broadcasts divided into segments by the LDC. We measured performance by comparing our segmentation to the gold standard annotation produced by the LDC. The row labelled Random guess shows the performance of a baseline algorithm that randomly guessed boundary locations with probability equal to the fraction of possible boundary sites that were boundaries in the gold standard. The row TextTiling shows the performance of the publicly available version of that algorithm (Hearst, 1994). Optimization is the algorithm we proposed in (Reynar, 1994). Word frequency and Max. Ent. Model are the algorithms we described above. Our word frequency algorithm does better than chance, TextTiling and our previous work and our maximum entropy model does better still. See (Reynar, 1998) for graphs showing the effects of trading precision for recall with these models. Algorithm Precision Recall Random guess 0.16 0.16 TextTiling 0.21 0.41 Optimization 0.36 0.20 Word Frequency 0.55 0.52 Max. Ent. Model 0.59 0.60 Table 3: Performance on the HUB-4 English corpus. 361 We also tested our models o</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Hearst, M.A. (1994). Multi-paragraph segmentation of expository text. In Proceedings of the 32&amp;quot; Annual Meeting of the Association for Computational Linguistics, pages 9-16, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J I Helfman</author>
</authors>
<title>Similarity patterns in language.</title>
<date>1994</date>
<booktitle>In IEEE Symposium on Visual Languages.</booktitle>
<contexts>
<context position="6067" citStr="Helfman, 1994" startWordPosition="949" endWordPosition="950">e-readable dictionary. He identified topic boundaries where the LCP score was low (Kozima, 1993). Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space model from IR (Hearst, 1994). Topic boundaries were positioned where the similarity between the block of text before and after the boundary was low. In previous work (Reynar, 1994), we described a method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by a visualization technique known as dotplotting (Helfman, 1994). Ponte and Croft predict topic boundaries using a model of likely topic length and a query expansion technique called Local Content Analysis that maps sets of words into a space of concepts (Ponte and Croft, 1997). Richmond, Smith and Amitay designed an algorithm for topic segmentation that weighted words based on their frequency within a document and subsequently used these weights in a formula based on the distance between repetitions of word types (Richmond et al., 1997). Beeferman, Berger and Lafferty used the relative performance of two statistical language models and cue words to identi</context>
</contexts>
<marker>Helfman, 1994</marker>
<rawString>Helfman, J.I. (1994). Similarity patterns in language. In IEEE Symposium on Visual Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>B Grosz</author>
</authors>
<title>Intonational features of local and global discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the Workshop on Spoken Language Systems,</booktitle>
<pages>441--446</pages>
<publisher>DARPA.</publisher>
<contexts>
<context position="4513" citStr="Hirschberg and Grosz, 1992" startWordPosition="715" endWordPosition="718"> Much research has been devoted to the task of structuring text—that is dividing texts into units based on information within the text. This work falls roughly into two categories. Topic segmentation focuses on identifying topicallycoherent blocks of text several sentences through several paragraphs in length (e.g. see Hearst, 1994). The prime motivation for identifying such units is to improve performance on languageprocessing or IR tasks. Discourse segmentation, on the other hand, is often finer-grained, and focuses on identifying relations between utterances (e.g. Grosz and Sidner, 1986 or Hirschberg and Grosz, 1992). Many topic segmentations algorithms have been proposed in the literature. There is not enough space to review them all here, so we will focus on describing a representative sample that covers most of the features used to predict the location of boundaries. See (Reynar, 1998) for a more thorough review. Youmans devised a technique called the Vocabulary Management Profile based on the location of first uses of word types. He posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (Youmans, 1991). Morris and Hirst </context>
</contexts>
<marker>Hirschberg, Grosz, 1992</marker>
<rawString>Hirschberg, J. and Grosz, B. (1992). Intonational features of local and global discourse. In Proceedings of the Workshop on Spoken Language Systems, pages 441-446. DARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--3</pages>
<contexts>
<context position="7361" citStr="Hirschberg and Litman, 1993" startWordPosition="1152" endWordPosition="1155">c Segmentation Prior work on topic segmentation has exploited many different hints about where topic boundaries lie. The algorithms we present use many cues from the literature as well as novel ones. Our approach is statistical in nature and weights evidence based on its utility in segmenting a training corpus. As a result, we do not use clues to form hard and fast rules. Instead, they all contribute evidence used to either increase or decrease the likelihood of proposing a topic boundary between two regions of text. 3.1 Domain-specific Cue Phrases Many discourse segmentation techniques (e.g. Hirschberg and Litman, 1993) as well as some topic segmentation algorithms rely on cue words and phrases (e.g. Beeferman et al., 1997), but the types of cue words used vary greatly. Those we employ are highly domain specific. Taking an 358 example from the broadcast news domain where we will demonstrate the effectiveness of our algorithms, the phrase joining us is a good indicator that a topic shift has just occurred because news anchors frequently say things such as joining us to discuss the crisis in Kosovo is Congressman... when beginning new stories. Consequently, our algorithms use the presence of phrases such as th</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Hirschberg, J. and Litman, D. (1993). Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501-530.</rawString>
</citation>
<citation valid="true">
<title>The</title>
<date>1996</date>
<booktitle>HUB-4 Program Committee</booktitle>
<marker>1996</marker>
<rawString>HUB-4 Program Committee (1996). The 1996 HUB-4 annotation specification for evaluation of speech recognition on broadcast news, version 3.5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zaidel</author>
<author>D Egedi</author>
</authors>
<title>A Freely Available Wide Coverage Morphological Analyzer for English.</title>
<date>1992</date>
<booktitle>Proceedings of the 15&amp;quot; International Conference on Computational Linguistics.</booktitle>
<location>Nantes, France.</location>
<marker>Zaidel, Egedi, 1992</marker>
<rawString>Karp, D., Schabes, Y:, Zaidel, M. and Egedi, D. (1992). A Freely Available Wide Coverage Morphological Analyzer for English. Proceedings of the 15&amp;quot; International Conference on Computational Linguistics. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Distribution of content words and phrases in text and language modeling.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<pages>2--1</pages>
<contexts>
<context position="13209" citStr="Katz, 1996" startWordPosition="2095" endWordPosition="2096">a pronoun among the first words immediately following a putative topic boundary provides some evidence that no topic boundary actually exists there. 4 Our Algorithms We designed two algorithms for topic segmentation. The first is based solely on word frequency and the second combines the results of the first with other sources of evidence. Both of these algorithms are applied to text following some preprocessing including tokenization, conversion to lowercase and the application of a lemmatizer (Karp et al., 1992). 4.1 Word Frequency Algorithm Our word frequency algorithm uses Katz&apos;s G model (Katz, 1996). The G model stipulates that words occur in documents either topically or nontopically. The model defines topical words as those that occur more than 1 time, while nontopical words occur only once. Counterexamples of these uses of topical and nontopical, of course, abound. We use the G model, shown below, to determine the probability that a particular word, w, occurred k times in a document. We trained the model from a corpus of 78 million words of Wall Street Journal text and smoothed the parameters using Dan Melamed&apos;s implementation of Good-Turing smoothing (Gale and Sampson, 1995) and addi</context>
</contexts>
<marker>Katz, 1996</marker>
<rawString>Katz, S.M. (1996). Distribution of content words and phrases in text and language modeling. Natural Language Engineering, 2(1):15-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31&apos; Annual Meeting of the Association for Computational Linguistics, Student Session,</booktitle>
<pages>286--288</pages>
<contexts>
<context position="5549" citStr="Kozima, 1993" startWordPosition="874" endWordPosition="875">s. He posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (Youmans, 1991). Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). They used Roget&apos;s 1977 Thesaurus to identify synonyms and other cohesion relations. Kozima defined a measure called the Lexical Cohesion Profile (LCP) based on spreading activation within a semantic network derived from a machine-readable dictionary. He identified topic boundaries where the LCP score was low (Kozima, 1993). Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space model from IR (Hearst, 1994). Topic boundaries were positioned where the similarity between the block of text before and after the boundary was low. In previous work (Reynar, 1994), we described a method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by a visualization technique known as dotplotting (Helfman, 1994). Ponte and Croft predict topic boundaries using a model of likely topic length an</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Kozima, H. (1993). Text segmentation based on similarity between words. In Proceedings of the 31&apos; Annual Meeting of the Association for Computational Linguistics, Student Session, pages 286-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Levy</author>
</authors>
<title>Communicating Thematic Structure in Narrative Discourse: The Use of Referring Terms and Gestures.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Chicago.</institution>
<contexts>
<context position="11914" citStr="Levy, 1984" startWordPosition="1897" endWordPosition="1898">s, particularly those in the domain of broadcast news. The odds that different stories discuss the same entities are generally low. There are obviously exceptions—the President of the U.S. may figure in many stories in a single broadcast—but nonetheless the presence of the same entities in two blocks of text suggest that they are likely to be part of the same story. 3.4 Pronoun Usage In her dissertation, Levy described a study of the impact of the type of referring expressions used, the location of first mentions of people and the gestures speakers make upon the cohesiveness of 359 discourse (Levy, 1984). She found a strong correlation between the types of referring expressions people used, in particular how explicit they were, and the degree of cohesiveness with the preceding context. Less cohesive utterances generally contained more explicit referring expressions, such as definite noun phrases or phrases consisting of a possessive followed by a noun, while more cohesive utterances more frequently contained zeroes and pronouns. We will use the converse of Levy&apos;s observation about pronouns to gauge the likelihood of a topic shift. Since Levy generally found pronouns in utterances that exhibit</context>
</contexts>
<marker>Levy, 1984</marker>
<rawString>Levy, E.T. (1984). Communicating Thematic Structure in Narrative Discourse: The Use of Referring Terms and Gestures. Ph.D. thesis, University of Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on WordNet.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>Cognitive Science Laboratory, Princeton University.</institution>
<contexts>
<context position="17424" citStr="Miller et al., 1990" startWordPosition="2858" endWordPosition="2861">(1 ) B-1 B-1 1 o 1-y 1 1+ r I k-1 - (I ) B-1 B-1 2+ 0+ 1 1 lc-, (1 ) M(B-1) B-1 Table 2: Conditional probabilities used to compute 4.2 A Maximum Entropy Model Our second algorithm is a maximum entropy model that uses these features: • Did our word frequency algorithm suggest a topic boundary? • Which domain cues (such as Joining us or This is &lt;person&gt;) were present? • How many content word bigrams were common to both regions adjoining the putative topic boundary? • How many named entities were common to both regions? • How many content words in both regions were synonyms according to WordNet (Miller et al., 1990)? • What percentage of content words in the region after the putative boundary were first uses? • Were pronouns used in the first five words after the putative topic boundary? We trained this model from 30 files of HUB-4 data that was disjoint from our test data. 5 Evaluation We will present results for broadcast news data and for identifying chapter boundaries labelled by authors. 5.1 HUB-4 Corpus Performance Table 3 shows the results of segmenting the test portion of the HUB-4 corpus, which consisted of transcribed broadcasts divided into segments by the LDC. We measured performance by compa</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K. (1990). Five papers on WordNet. Technical report, Cognitive Science Laboratory, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--1</pages>
<contexts>
<context position="5160" citStr="Morris and Hirst, 1991" startWordPosition="816" endWordPosition="819">ns algorithms have been proposed in the literature. There is not enough space to review them all here, so we will focus on describing a representative sample that covers most of the features used to predict the location of boundaries. See (Reynar, 1998) for a more thorough review. Youmans devised a technique called the Vocabulary Management Profile based on the location of first uses of word types. He posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (Youmans, 1991). Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). They used Roget&apos;s 1977 Thesaurus to identify synonyms and other cohesion relations. Kozima defined a measure called the Lexical Cohesion Profile (LCP) based on spreading activation within a semantic network derived from a machine-readable dictionary. He identified topic boundaries where the LCP score was low (Kozima, 1993). Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space model from IR (Hearst, 1994). Topic boundaries were positioned wh</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, J. and Hirst, G. (1991). Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>Text segmentation by topic.</title>
<date>1997</date>
<booktitle>In European Conference on Digital Libraries,</booktitle>
<pages>113--125</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="6281" citStr="Ponte and Croft, 1997" startWordPosition="983" endWordPosition="986">aph segments using the vector space model from IR (Hearst, 1994). Topic boundaries were positioned where the similarity between the block of text before and after the boundary was low. In previous work (Reynar, 1994), we described a method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by a visualization technique known as dotplotting (Helfman, 1994). Ponte and Croft predict topic boundaries using a model of likely topic length and a query expansion technique called Local Content Analysis that maps sets of words into a space of concepts (Ponte and Croft, 1997). Richmond, Smith and Amitay designed an algorithm for topic segmentation that weighted words based on their frequency within a document and subsequently used these weights in a formula based on the distance between repetitions of word types (Richmond et al., 1997). Beeferman, Berger and Lafferty used the relative performance of two statistical language models and cue words to identify topic boundaries (Beeferman et al., 1997). 3 New Clues for Topic Segmentation Prior work on topic segmentation has exploited many different hints about where topic boundaries lie. The algorithms we present use m</context>
</contexts>
<marker>Ponte, Croft, 1997</marker>
<rawString>Ponte, J.M. and Croft, W.B. (1997). Text segmentation by topic. In European Conference on Digital Libraries, pages 113-125, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the First Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="9361" citStr="Ratnaparkhi, 1996" startWordPosition="1483" endWordPosition="1484">ly small we identified these by hand, but on a different corpus we induced them automatically (Reynar, 1998). The results we present later in the paper rely solely on manually identified cues phrases. Identifying complex cue phrases involves pattern matching and determining whether particular word sequences belong to various classes. To address this, we built a named entity recognition system in the spirit of those used for the Message Understanding Conference evaluations (e.g. Bikel et al., 1997). Our named entity recognizer used a maximum entropy model built with Adwait Ratnaparkhi&apos;s tools (Ratnaparkhi, 1996) to label word sequences as either person, place, company or none of the above based on local cues including the surrounding words and whether honorifics (e.g. Mrs. or Gen.) or corporate designators (e.g. Corp. or Inc.) were present. Our algorithm&apos;s labelling accuracy of 96.0% by token was sufficient for our purposes, but performance is not directly comparable to the MUC competitors&apos;. Though we trained from the same data, we preprocessed the data to remove punctuation and capitalization so the model could be applied to broadcast news data that lacked these helpful clues. We separately identifi</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A: (1996). A maximum entropy model for part-of-speech tagging. In Proceedings of the First Conference on Empirical Methods in Natural Language Processing, pages 133-142, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32&amp;quot; Annual Meeting of the Association for Computational Linguistics, Student Session,</booktitle>
<pages>331--333</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="5875" citStr="Reynar, 1994" startWordPosition="921" endWordPosition="922">dentify synonyms and other cohesion relations. Kozima defined a measure called the Lexical Cohesion Profile (LCP) based on spreading activation within a semantic network derived from a machine-readable dictionary. He identified topic boundaries where the LCP score was low (Kozima, 1993). Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space model from IR (Hearst, 1994). Topic boundaries were positioned where the similarity between the block of text before and after the boundary was low. In previous work (Reynar, 1994), we described a method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by a visualization technique known as dotplotting (Helfman, 1994). Ponte and Croft predict topic boundaries using a model of likely topic length and a query expansion technique called Local Content Analysis that maps sets of words into a space of concepts (Ponte and Croft, 1997). Richmond, Smith and Amitay designed an algorithm for topic segmentation that weighted words based on their frequency within a document and subsequently used these weights in a formula based on</context>
<context position="10156" citStr="Reynar, 1994" startWordPosition="1604" endWordPosition="1605">ate designators (e.g. Corp. or Inc.) were present. Our algorithm&apos;s labelling accuracy of 96.0% by token was sufficient for our purposes, but performance is not directly comparable to the MUC competitors&apos;. Though we trained from the same data, we preprocessed the data to remove punctuation and capitalization so the model could be applied to broadcast news data that lacked these helpful clues. We separately identified television network acronyms using simple regular expressions. 3.2 Word Bigram Frequency Many topic segmentation algorithms in the literature use word frequency (e.g. Hearst, 1994; Reynar, 1994; Beeferman et al., 1997). An obvious extension to using word frequency is to use the frequency of multi-word phrases. Such phrases are useful because they approximate word sense disambiguation techniques. Algorithms that rely exclusively on word frequency might be fooled into suggesting that two stretches of text containing the word plant were part of the same story simply because of the rarity of plant and the low odds that two adjacent stories contained it due to chance. However, if plant in one section participated in bigrams such as wild plant, native plant and woody plant but in the othe</context>
<context position="18493" citStr="Reynar, 1994" startWordPosition="3029" endWordPosition="3030">e test portion of the HUB-4 corpus, which consisted of transcribed broadcasts divided into segments by the LDC. We measured performance by comparing our segmentation to the gold standard annotation produced by the LDC. The row labelled Random guess shows the performance of a baseline algorithm that randomly guessed boundary locations with probability equal to the fraction of possible boundary sites that were boundaries in the gold standard. The row TextTiling shows the performance of the publicly available version of that algorithm (Hearst, 1994). Optimization is the algorithm we proposed in (Reynar, 1994). Word frequency and Max. Ent. Model are the algorithms we described above. Our word frequency algorithm does better than chance, TextTiling and our previous work and our maximum entropy model does better still. See (Reynar, 1998) for graphs showing the effects of trading precision for recall with these models. Algorithm Precision Recall Random guess 0.16 0.16 TextTiling 0.21 0.41 Optimization 0.36 0.20 Word Frequency 0.55 0.52 Max. Ent. Model 0.59 0.60 Table 3: Performance on the HUB-4 English corpus. 361 We also tested our models on speech-recognized broadcasts from the 1997 TREC spoken docu</context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>Reynar, J.C. (1994). An automatic method of finding topic boundaries. In Proceedings of the 32&amp;quot; Annual Meeting of the Association for Computational Linguistics, Student Session, pages 331-333, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
</authors>
<title>Topic Segmentation: Algorithms and Applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, Department of Computer Science.</institution>
<contexts>
<context position="4790" citStr="Reynar, 1998" startWordPosition="762" endWordPosition="763">aragraphs in length (e.g. see Hearst, 1994). The prime motivation for identifying such units is to improve performance on languageprocessing or IR tasks. Discourse segmentation, on the other hand, is often finer-grained, and focuses on identifying relations between utterances (e.g. Grosz and Sidner, 1986 or Hirschberg and Grosz, 1992). Many topic segmentations algorithms have been proposed in the literature. There is not enough space to review them all here, so we will focus on describing a representative sample that covers most of the features used to predict the location of boundaries. See (Reynar, 1998) for a more thorough review. Youmans devised a technique called the Vocabulary Management Profile based on the location of first uses of word types. He posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (Youmans, 1991). Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). They used Roget&apos;s 1977 Thesaurus to identify synonyms and other cohesion relations. Kozima defined a measure called the Lexical Cohesion Profile (LCP) based on sprea</context>
<context position="8851" citStr="Reynar, 1998" startWordPosition="1408" endWordPosition="1409">cated and contain word sequences of particular types. Not surprisingly, the phrase this is is common in broadcast news. When it is followed by a person&apos;s name, however, it serves as a good clue that a topic is about to end. This is &lt;person name&gt; is almost always said when a reporter is signing off after finishing an on-location report. Generally such signoffs are followed by the start of new news stories. A sampling of the cue phrases we use is found in Table 1. Since our training corpus was relatively small we identified these by hand, but on a different corpus we induced them automatically (Reynar, 1998). The results we present later in the paper rely solely on manually identified cues phrases. Identifying complex cue phrases involves pattern matching and determining whether particular word sequences belong to various classes. To address this, we built a named entity recognition system in the spirit of those used for the Message Understanding Conference evaluations (e.g. Bikel et al., 1997). Our named entity recognizer used a maximum entropy model built with Adwait Ratnaparkhi&apos;s tools (Ratnaparkhi, 1996) to label word sequences as either person, place, company or none of the above based on lo</context>
<context position="16562" citStr="Reynar, 1998" startWordPosition="2693" endWordPosition="2694">in threshold. The threshold was used to trade precision for recall and vice versa when identifying topic boundaries. The most natural threshold is a very small nonzero value, which is equivalent to placing a boundary wherever P is greater than Pone = Pr(k, w I C) Po.,0 = Pr(k, w) Computing 13,, is straightforward, but Ponerequires computing conditional probabilities of the number of occurrences of each word in region 2 given the number in region 1. The formulae for the conditional probabilities are shown in Table 2. We do not have space to derive these formulae here, but they can be found in (Reynar, 1998). M is a normalizing term required to make the conditional probabilities sum to 1. In the table, x+ means x occurrences or more. Occurrences Occurrences Conditional probability in region 1 in region 2 0 0 I - a 0 1 a( 1-y) 0 2+ a - y 1 k_, - (1 ) B-1 B-1 1 o 1-y 1 1+ r I k-1 - (I ) B-1 B-1 2+ 0+ 1 1 lc-, (1 ) M(B-1) B-1 Table 2: Conditional probabilities used to compute 4.2 A Maximum Entropy Model Our second algorithm is a maximum entropy model that uses these features: • Did our word frequency algorithm suggest a topic boundary? • Which domain cues (such as Joining us or This is &lt;person&gt;) wer</context>
<context position="18723" citStr="Reynar, 1998" startWordPosition="3065" endWordPosition="3066">elled Random guess shows the performance of a baseline algorithm that randomly guessed boundary locations with probability equal to the fraction of possible boundary sites that were boundaries in the gold standard. The row TextTiling shows the performance of the publicly available version of that algorithm (Hearst, 1994). Optimization is the algorithm we proposed in (Reynar, 1994). Word frequency and Max. Ent. Model are the algorithms we described above. Our word frequency algorithm does better than chance, TextTiling and our previous work and our maximum entropy model does better still. See (Reynar, 1998) for graphs showing the effects of trading precision for recall with these models. Algorithm Precision Recall Random guess 0.16 0.16 TextTiling 0.21 0.41 Optimization 0.36 0.20 Word Frequency 0.55 0.52 Max. Ent. Model 0.59 0.60 Table 3: Performance on the HUB-4 English corpus. 361 We also tested our models on speech-recognized broadcasts from the 1997 TREC spoken document retrieval corpus. We did not have sufficient data to train the maximum entropy model, but our word frequency algorithm achieved precision of 0.36 and recall of 0.52, considerably better than the baseline of 0.19 precision and</context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Reynar, J.C. (1998). Topic Segmentation: Algorithms and Applications. Ph.D. thesis, University of Pennsylvania, Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Richmond</author>
<author>A Smith</author>
<author>E Amitay</author>
</authors>
<title>Detecting subject boundaries within text: A language independent statistical approach.</title>
<date>1997</date>
<booktitle>In Exploratory Methods in Natural Language Processing,</booktitle>
<pages>47--54</pages>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="6546" citStr="Richmond et al., 1997" startWordPosition="1024" endWordPosition="1027">es using an optimisation algorithm based on word repetition that was inspired by a visualization technique known as dotplotting (Helfman, 1994). Ponte and Croft predict topic boundaries using a model of likely topic length and a query expansion technique called Local Content Analysis that maps sets of words into a space of concepts (Ponte and Croft, 1997). Richmond, Smith and Amitay designed an algorithm for topic segmentation that weighted words based on their frequency within a document and subsequently used these weights in a formula based on the distance between repetitions of word types (Richmond et al., 1997). Beeferman, Berger and Lafferty used the relative performance of two statistical language models and cue words to identify topic boundaries (Beeferman et al., 1997). 3 New Clues for Topic Segmentation Prior work on topic segmentation has exploited many different hints about where topic boundaries lie. The algorithms we present use many cues from the literature as well as novel ones. Our approach is statistical in nature and weights evidence based on its utility in segmenting a training corpus. As a result, we do not use clues to form hard and fast rules. Instead, they all contribute evidence </context>
</contexts>
<marker>Richmond, Smith, Amitay, 1997</marker>
<rawString>Richmond, K., Smith, A., and Amitay, E. (1997). Detecting subject boundaries within text: A language independent statistical approach. In Exploratory Methods in Natural Language Processing, pages 47-54, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring experiments.</title>
<date>1992</date>
<booktitle>Text-Based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval,</booktitle>
<pages>199--210</pages>
<editor>In Jacobs, P.S., editor,</editor>
<location>Hillsdale, New Jersey.</location>
<marker>Salton, Buckley, 1992</marker>
<rawString>Salton, G. and Buckley, C. (1992). Automatic text structuring experiments. In Jacobs, P.S., editor, Text-Based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval, pages 199-210. Lawrence Erlbaum Associates, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singhal</author>
<author>C Buckley</author>
<author>M Mitra</author>
</authors>
<title>Pivoted document length normalization.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>21--29</pages>
<publisher>ACM.</publisher>
<location>Zurich, Switzerland.</location>
<contexts>
<context position="24079" citStr="Singhal et al., 1996" startWordPosition="3914" endWordPosition="3917"> 0.00 0.36 Sense Decline 53 0.21 0.0024 and Fall Moby 132 0.55 0.173 Dick Orthodoxy 8 0.25 0.033 Combined 200 0.43 0.059 Table 4: Accuracy of the Word Frequency algorithm on identifying chapter boundaries. 5.3 IR Task Performance The data from the HUB-4 corpus was also used for the TREC Spoken document retrieval task. We tested the utility of our segmentations by comparing IR performance when we indexed documents, the segments annotated by the LDC and the segments identified by our algorithms. We modified SMART (Buckley, 1985) to perform better normalization for variations in document length (Singhal et al., 1996) prior to conducting our IR experiments. This IR task is atypical in that there is only 1 relevant document in the collection for each query. Consequently, performance is measured by determining the average rank determined by the IR system for the document relevant to each query. Perfect performance would be an average rank of 1, hence lower average ranks are better. Table 5 presents our results. Note that indexing the segments identified by our algorithms was better than indexing entire documents and that our best algorithm even outperformed indexing the gold standard annotation produced by t</context>
</contexts>
<marker>Singhal, Buckley, Mitra, 1996</marker>
<rawString>Singhal, A., Buckley, C., and Mitra, M. (1996). Pivoted document length normalization. In Proceedings of the ACM-SIGIR Conference on Research and Development in Information Retrieval, pages 21-29, Zurich, Switzerland. ACM.</rawString>
</citation>
<citation valid="true">
<title>Topic Detection and Tracking Phase 2 Evaluation Plan, version 2.1.</title>
<date>1998</date>
<booktitle>TDT Program Committee</booktitle>
<marker>1998</marker>
<rawString>TDT Program Committee (1998). Topic Detection and Tracking Phase 2 Evaluation Plan, version 2.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Youmans</author>
</authors>
<title>A new tool for discourse analysis: The vocabulary management profile.</title>
<date>1991</date>
<journal>Language,</journal>
<pages>67--4</pages>
<contexts>
<context position="5094" citStr="Youmans, 1991" startWordPosition="808" endWordPosition="809">86 or Hirschberg and Grosz, 1992). Many topic segmentations algorithms have been proposed in the literature. There is not enough space to review them all here, so we will focus on describing a representative sample that covers most of the features used to predict the location of boundaries. See (Reynar, 1998) for a more thorough review. Youmans devised a technique called the Vocabulary Management Profile based on the location of first uses of word types. He posited that large clusters of first uses frequently followed topic boundaries since new topics generally introduce new vocabulary items (Youmans, 1991). Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). They used Roget&apos;s 1977 Thesaurus to identify synonyms and other cohesion relations. Kozima defined a measure called the Lexical Cohesion Profile (LCP) based on spreading activation within a semantic network derived from a machine-readable dictionary. He identified topic boundaries where the LCP score was low (Kozima, 1993). Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space</context>
</contexts>
<marker>Youmans, 1991</marker>
<rawString>Youmans, G. (1991). A new tool for discourse analysis: The vocabulary management profile. Language, 67(4):763-789.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>