<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001054">
<title confidence="0.992611">
In-tool Learning for Selective Manual Annotation in Large Corpora
</title>
<author confidence="0.984045">
Erik-Lˆan Do Dinh†, Richard Eckart de Castilho†, Iryna Gurevych†‡†Ubiquitous Knowledge Processing Lab (UKP-TUDA)
</author>
<affiliation confidence="0.958918333333333">
Department of Computer Science, Technische Universit¨at Darmstadt
‡Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.948352">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.992391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934181818182">
We present a novel approach to the selec-
tive annotation of large corpora through
the use of machine learning. Linguis-
tic search engines used to locate potential
instances of an infrequent phenomenon
do not support ranking the search re-
sults. This favors the use of high-precision
queries that return only a few results over
broader queries that have a higher recall.
Our approach introduces a classifier used
to rank the search results and thus help-
ing the annotator focus on those results
with the highest potential of being an in-
stance of the phenomenon in question,
even in low-precision queries. The clas-
sifier is trained in an in-tool fashion, ex-
cept for preprocessing relying only on the
manual annotations done by the users in
the querying tool itself. To implement
this approach, we build upon CSniper1, a
web-based multi-user search and annota-
tion tool.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996540090909091">
With the rapidly growing body of digitally avail-
able language data, it becomes possible to investi-
gate phenomena of the language system that man-
ifest themselves infrequently in corpus data, e.g.
non-canonical constructions. To pinpoint occur-
rences of such phenomena and to annotate them
requires a new kind of annotation tool, since man-
ual, sequential annotation is not feasible anymore
for large amounts of texts.
An annotation-by-query approach to identify
such phenomena in large corpora is implemented
</bodyText>
<footnote confidence="0.941201">
1https://dkpro.github.io/dkpro-csniper
</footnote>
<bodyText confidence="0.999886">
in the recently published open-source tool CSniper
(Eckart de Castilho et al., 2012).
To enable a selective manual annotation pro-
cess, a linguistic search engine is used, allowing
the creation of queries which single out potential
instances of the phenomenon in question. Those
potential instances are then displayed to the user,
who annotates each one as being an instance of
the phenomenon or not. This process of search-
ing and annotating can be performed by multiple
users concurrently; the annotations are stored for
each user separately. In a subsequent evaluation
step, a user can review the annotations of all users,
e.g. to discard a query if it yields unsatisfying re-
sults. Finally, the annotations of multiple users can
be merged into a gold standard.
</bodyText>
<figure confidence="0.344376">
Aggregate
</figure>
<figureCaption confidence="0.9628485">
Figure 1: Annotation-by-query workflow extended
with a ranking step.
</figureCaption>
<bodyText confidence="0.9993828">
This approach relieves the annotator from hav-
ing to read through the corpus from the beginning
to the end to look for instances of a phenomenon.
However, the search may yield many results that
may superficially appear to be an instance of the
desired phenomenon, but due to ambiguities or
due to a broadly defined query only a small sub-
set may be actual instances. This still leaves the
annotator with the tedious task of clicking through
the search results to mark the true instances.
</bodyText>
<figure confidence="0.976282875">
refine
query
Annotate
Evaluate
Query
review
assessments
Rank
</figure>
<page confidence="0.986634">
13
</page>
<note confidence="0.6903135">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 13–18,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<bodyText confidence="0.999874642857143">
To reduce the time and effort required, we
present an extension of the annotation-by-query
approach (Figure 1) that introduces a ranking of
the query results (Section 2) by means of machine
learning; we order the results by confidence of
the used classifier. To obtain a model for the
classifier, we employ an in-tool learning approach,
where we learn from the annotations that are made
by users in the tool itself. This makes our ranking
approach useful for highly specific tasks, since no
pre-trained models are needed.
Finally we demonstrate the viability of our con-
cept by the example task of finding non-canonical
constructions in Section 3.
</bodyText>
<sectionHeader confidence="0.894606" genericHeader="method">
2 Ranking linguistic query results
</sectionHeader>
<bodyText confidence="0.999963586206897">
Our approach employs machine learning to facili-
tate — but not to completely replace — the man-
ual annotation of query results. A query expresses
the intention of the user to find a specific linguis-
tic phenomenon (information need). An infor-
mation retrieval search engine would provide the
user with a list of results that are ranked accord-
ing to their relevance, fulfilling the information
need. However, linguistic search engines such as
CQP (Evert and Hardie, 2011) — which is used
by CSniper — are basically pattern-matching en-
gines, operating on different lexical or morpho-
syntactic features like part-of-speech (POS) tags
and lemmata and do not have a concept of rele-
vance. Thus, if the query provided by the user
overgeneralizes, relevant results are hidden among
many irrelevant ones, ultimately failing to satisfy
the user’s information need.
To tackle this problem, we use the annotations
already created by users on the search results to
train a classifier. Unannotated query results are
then fed to the classifier whose output values are
then used as relevance ratings by which the results
are ranked. The classifier producing the ranking
can be invoked by the user at any time; it can be
configured in certain characteristics, e.g. the an-
notations of which users should be used as train-
ing data, or how many of the selected users have
to agree on an annotation for it to be included.
</bodyText>
<subsectionHeader confidence="0.7955875">
2.1 Workflow and ranking process in
CSniper
</subsectionHeader>
<bodyText confidence="0.99978605882353">
Currently, we train the classifier on features de-
rived from the constituency parse tree, which
makes it useful for tasks such as locating sen-
tences containing infrequent ambiguous grammat-
ical constructions (cf. Section 3). Since parsing
the query results is too time-intensive to be done
during runtime, we parsed the corpora in advance
and stored the parse trees in a database. To train
the classifier, we employed SVM-light-tk (Mos-
chitti, 2006; Joachims, 1999), a support vector ma-
chine implementation which uses a tree kernel to
integrate all sub-trees of the parse tree as features.
Consider the following typical scenario incor-
porating the ranking: A user constructs a query
based on various features, such as POS tags or
lemmata, which are used to search for matching
sentences, e.g.
</bodyText>
<equation confidence="0.5961705">
“It” [lemma=“be”] [pos=“AT0”]?
[pos=“NN.*”]2
</equation>
<bodyText confidence="0.999482695652174">
The result is a list of sentences presented in a
keywords-in-context (KWIC) view, along with an
annotation field (Figure 2).
Then the user starts to annotate these sentences
as Correct or Wrong, depending whether they
truly represent instances of the phenomenon in
question. Clicking on the Rank results button
(Figure 2) invokes our ranking process: The SVM-
light-tk classifier is trained using the parse trees of
the sentences which the user previously annotated.
The resulting model is then used to classify the re-
maining sentences in the query results. We rank
the sentences according to the output value of the
decision function of the classifier (which we in-
terpret as a relevance/confidence rating) and tran-
siently label a sentence as either (Correct) (output
value &gt; 0) or (Wrong) (output value ≤ 0). The re-
sults in the KWIC view are then reordered accord-
ing to the rank, showing the highest-ranking result
first. Repeatedly annotating those highest-ranking
results and re-ranking allows for quickly annotat-
ing instances of the phenomenon, while also im-
proving the classifier accuracy at the same time.
</bodyText>
<subsectionHeader confidence="0.99261">
2.2 Find mode
</subsectionHeader>
<bodyText confidence="0.9945203">
After annotating instances based on simple queries
and ML-supported ranked queries, we considered
the natural next step to be searching automatically
for the phenomenon in question utilizing machine
learning, using arbitrary sentences from the cor-
pus as input for the classifier instead of only the
results returned by a query. Such an automatic
search could address two concerns: 1) it removes
2It-cleft example query: “It”, followed by a form of “to
be”, an optional determiner and a common noun.
</bodyText>
<page confidence="0.99759">
14
</page>
<figureCaption confidence="0.992255">
Figure 2: A screenshot showing the results table after the ranking process, with sentences sorted by
</figureCaption>
<bodyText confidence="0.967570076923077">
confidence of the classifier (Score). The results are shown in a keywords-in-context (KWIC) view, sepa-
rating left context, query match and right context (within a range of one sentence). Clicking on (Correct)
changes the label to Correct.
the need for the user to design new queries, al-
lowing users less experienced in the query lan-
guage to annotate more effectively side-by-side
with advanced users; 2) it could optimally gener-
alize over all the queries that users have already
made and potentially locate instances that had not
been found by individual high-precision queries.
To support this, we implemented the Find
mode, to locate instances of a phenomenon while
abstracting from the queries. In this mode, the
SVM is first trained from all previously (manu-
ally) labeled instances for a given phenomenon,
without taking the queries into account that were
used to find those instances. Then the corpus is
partitioned into smaller parts containing a prede-
fined amount of sentences (we used 500). One
of these partitions is chosen at random, and the
sentences therein are ranked using the SVM. This
step is repeated, until a previously defined num-
ber of sentences have been classified as Correct.
Those sentences are then shown to the user, who
now can either confirm a sentence as containing
the phenomenon or label it Wrong otherwise.
</bodyText>
<sectionHeader confidence="0.786294" genericHeader="method">
2.3 Related work
</sectionHeader>
<bodyText confidence="0.999987301886793">
Existing annotation tools include automation
functionality for annotation tasks, ranging from
rule-based tagging to more complex, machine-
learning-based approaches.
Such functionalities can be found in the anno-
tation software WordFreak (Morton and LaCivita,
2003), where a plug-in architecture allows for a
variety of different taggers and classifiers to be in-
tegrated, for example part-of-speech taggers or co-
reference resolution engines. Those require pre-
trained models, which limits the applicability of
the automation capabilities of WordFreak to tasks
for which such models are actually available. In
addition to assigning annotations a single label,
WordFreak allows plugins to rank labels for each
annotation based on the confidence of the used
classifier. Note that this is different to our rank-
ing approach, where we instead perform a ranking
of the search results which shall be annotated.
Another tool incorporating machine learning is
WebAnno (Yimam et al., 2014), which imple-
ments features such as custom labels and anno-
tation types. In addition, WebAnno supports au-
tomatic annotation similar to our approach, also
employing machine learning to build models from
the data annotated by users. Those models are then
used to annotate the remainder of the documents.
To accomplish this, WebAnno uses a split-pane
view, showing automatic suggestions in one pane
and manually entered annotations in another. The
user can accept a suggested annotation, which is
transferred to the manual pane. Lacking the search
capability, WebAnno lists automatic annotations
in the running corpus text, which makes it unsuited
for selective annotation in large corpora. The ap-
proach that we implemented on top of CSniper in-
stead ranks the search results for a given query by
confidence of the classifier.
Yet another form of in-tool learning is active
learning, as is implemented, e.g., in Dualist (Set-
tles, 2011). In an active learning scenario the sys-
tem aims to efficiently train an accurate classifier
(i.e. with as little training data as possible) and
thus repeatedly asks the user to annotate instances
from which it can learn the most. Such an ap-
proach can work well for reducing the amount of
training data needed to produce a model which
achieves high accuracy, as has been — amongst
others — shown by Hachey et al. (2005). How-
ever, they also learn in their experiments that those
highly informative instances are often harder to
annotate and increase required time and effort of
annotators. Our approach is different from active
</bodyText>
<page confidence="0.995148">
15
</page>
<bodyText confidence="0.9999856">
learning as our goal is not to improve the training
efficiency of the classifier but rather to allow the
user to interactively find and label as many true
instances of a phenomenon as possible in a large
corpus. Thus, the items presented to the user are
not determined by the expected information gain
for the classifier but rather by the confidence of the
classifier, presenting the user with those instances
first which are most likely to be occurrences of the
phenomenon in question.
</bodyText>
<sectionHeader confidence="0.9848405" genericHeader="method">
3 Case study: Finding non-canonical
constructions
</sectionHeader>
<bodyText confidence="0.999960628571429">
We demonstrate our novel approach on the task of
locating non-canonical constructions (NCC) and
conduct an intrinsic evaluation of the accuracy
of the system augmented with machine learning
output on the data annotated by expert linguists.
The linguists annotated sentences for occurrences
of certain NCC subtypes: information-packaging
constructions (Huddleston and Pullum, 2002, pp.
1365ff.), which present information in a differ-
ent way from their canonical counterparts without
changing truth conditions; specifically It-clefts (“It
was Peter who made lunch.”), NP-preposing (“A
treasure, he was searching.”), and PP-inversion
(“To his left lay the forest.”) clauses.
For our experiments, we used the British Na-
tional Corpus (2007), comprising 100 million
words in multiple domains3. Constituency pars-
ing was conducted using the factored variant of
the Stanford Parser (Klein and Manning, 2003),
incorporated into a UIMA pipeline using DKPro
Core (Eckart de Castilho and Gurevych, 2014).
As a baseline we use queries representing the
experts’ intuition about the realization of the
NCCs in terms of POS tags and lemmata. We
show that our system improves the precision of the
query results even with little training data. Also
we present run times for our ranking system un-
der real-world conditions for different training set
sizes. Further, we compare Krippendorff’s α co-
efficient as an inter-annotator agreement measure
among only annotators to the α which treats our
system as one additional annotator.
We conducted the experiments based on the
manually assigned labels of up to five annota-
tors. If a sentence has been annotated by multiple
</bodyText>
<footnote confidence="0.761162">
3CSniper and the used SVM implementation are language
independent, which allowed us to also run additional prelim-
inary tests using German data.
</footnote>
<bodyText confidence="0.996595344827586">
users, we use the label that has been assigned by
the majority; in case of a tie, we ignore the sen-
tence. These so created gold standard annotations
were used in an iterative cross-validation setting:
for each query and the corresponding annotated
sentences we ran nine cross-validation configura-
tions, ranging from a 10/90 split between training
and testing data to a 90/10 split, to investigate the
reliability of the classifier as well as its ability to
achieve usable results with little training data.
For It-clefts, we observe that elaborate queries
already have a high precision, on which the SVM
improves only marginally. The query
“It” /VCC[] [pos=“NP0”]+ /RC[]4 (it17)
already yields a precision of 0.9598, which does
not increase using our method (using 10% as train-
ing data, comparing the precision for the remain-
ing 90%). However, while broader queries yield
lower precision, the gain by using the SVM be-
comes significant (Table 1), as exemplified by the
precision improvement from 0.4919 to 0.7782 for
the following It-cleft query, even at a 10/90 split.
“It” /VCC[] /NP[] /RC[]5 (it2)
For other inspected types of NCC, even elaborate
queries yield a low baseline precision, which our
approach can improve significantly. This effect
can be observed for example in the following NP-
preposing query, where precision can be improved
from 0.3946 to 0.5871.
</bodyText>
<equation confidence="0.9866945">
[pos=”N.*”]{1,2} [pos=”PNP” &amp; word!=”I”]
[pos=”V.*”]6 (np55)
</equation>
<bodyText confidence="0.999978307692308">
We conducted a cursory, “real-world” test re-
garding the speed of the ranking system.7 Training
the SVM on differently sized subsets of the 449
sentences returned by a test query, we measured
the time from clicking the Rank results button until
the process was complete and the GUI had updated
to reorder the sentences (i.e. including database
queries, training, classifying, GUI update). The
process times averaged over five “runs” for each
training set size (20%, 50% and 90%) amount to 5
seconds, 7 seconds, and 14 seconds respectively.
This leaves us with the preliminary impression
that our system is fast enough for small to medium
</bodyText>
<footnote confidence="0.921735285714286">
4“It”, verb clause, one or more proper nouns, relative
clause. VCC, NC, and RC are macros we defined in CQP,
see Table 2.
5“It”, verb clause, noun phrase, relative clause.
6One to two nouns, personal pronoun other than “I”, verb.
7System configuration: Intel i5 2,4 GHz, 2GB RAM, SSD
3GB/s, Linux in a VM
</footnote>
<page confidence="0.992524">
16
</page>
<table confidence="0.9985692">
it2 it17 it33 np34 np55 np76 pp42 pp99 pp103
Baseline 0.4919 0.9598 0.7076 0.4715 0.3946 0.4985 0.7893 0.4349 0.2365
SVM, 10/90 0.7782 0.9598 0.7572 0.5744 0.5871 0.5274 0.8152 0.8357 0.8469
SVM, 50/50 0.8517 0.9608 0.8954 0.6410 0.6872 0.6193 0.8657 0.8769 0.8720
SVM, 90/10 0.8634 0.9646 0.9261 0.6822 0.7723 0.6806 0.8865 0.8820 0.8796
</table>
<tableCaption confidence="0.9750715">
Table 1: Precision for various NCC queries (Baseline) and for using the SVM with 10%, 50% and 90%
training data.
</tableCaption>
<bodyText confidence="0.9983205">
sized training sets; as the last result suggests, for
larger sets it would be desirable for our system to
be faster overall. One way to achieve this is to
pre-compute the feature vectors used in the train-
ing phase once — this could be done at the same
time with the parsing of the sentences, i.e. at the
setup time of the system.
Krippendorff’s α, an inter-annotator agreement
(IAA) measure which usually assumes values be-
tween 0 (no reliable agreement) and 1 (perfect
agreement), amounts to 0.8207 averaged over all
manually created It-cleft annotations. If we inter-
pret the SVM as an additional annotator (αsvm),
the IAA drops to 0.5903. At first glance this
seems quite low, however upon closer inspection
this can be explained by an overfitting of the clas-
sifier. This effect occurs for the already precise
baseline queries, where in some cases less than
5% of the query results were labeled as Wrong.
The same holds for NP-preposing (α: 0.6574,
αsvm: 0.3835) and PP-inversion (α: 0.9410,
αsvm: 0.6964). We interpret this as the classifier
being successful in helping the annotators after a
brief training phase identifying additional occur-
rences of particular variants of a phenomenon as
covered by the queries, but not easily generalizing
to variants substantially different from those cov-
ered by the queries.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99997045">
With automatic ranking we introduced an exten-
sion to the annotation-by-query workflow which
facilitates manual, selective annotation of large
corpora. We explained the benefits of in-tool
learning to this task and our extension of an open-
source tool to incorporate this functionality. Fi-
nally, we showed the applicability of the concept
and its implementation to the task of finding non-
canonical constructions.
For future work, we plan to speed up the learn-
ing process (e.g. by saving feature vectors instead
of re-calculating them), and also add the ability
for users to configure the features used to train the
classifier, e.g. incorporating lemmata or named
entities instead of only using the parse tree. In-
tegrating such configuration options in an easily
understandable and user-friendly fashion may not
be trivial but can help to generalize the approach
to support additional kinds of sentence level anno-
tation.
</bodyText>
<sectionHeader confidence="0.996932" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998591142857143">
We would like to thank Pia Gerhard, Sabine
Bartsch, Gert Webelhuth, and Janina Rado for an-
notating and testing. Furthermore we would like
to thank Janina Rado for creating the CQP macros
used in the tests.
This work has been supported by the Ger-
man Federal Ministry of Education and Re-
search (BMBF) under the promotional reference
01UG1416B (CEDIFOR), by the German Insti-
tute for Educational Research (DIPF) as part of
the graduate program “Knowledge Discovery in
Scientific Literature” (KDSL), and by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
</bodyText>
<sectionHeader confidence="0.972063" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.891464666666667">
Richard Eckart de Castilho and Iryna Gurevych. 2014.
A broad-coverage collection of portable NLP com-
ponents for building shareable analysis pipelines.
</bodyText>
<reference confidence="0.95208575">
In Proceedings of the Workshop on OIAF4HLT at
COLING 2014, pages 1–11.
Richard Eckart de Castilho, Iryna Gurevych, and
Sabine Bartsch. 2012. CSniper: Annotation-
by-query for Non-canonical Constructions in Large
Corpora. In Proceedings of ACL 2012, System
Demonstrations, pages 85–90, Stroudsburg, PA,
USA. ACL.
Stefan Evert and Andrew Hardie. 2011. Twenty-first
century corpus workbench: Updating a query archi-
tecture for the new millennium. In Proceedings of
CL2011, Birmingham, UK.
</reference>
<page confidence="0.998092">
17
</page>
<bodyText confidence="0.568448">
Shortcut Expansion
</bodyText>
<equation confidence="0.843811857142857">
VCC ([pos=”VBB”  |pos=”VBD”  |pos=”VBZ”]* [lemma=”be”]) |
([pos=”V.*”]* [pos=”VBG”  |pos=”VBI”  |pos=”VBN”]* [lemma=”be”])
NP [pos=”AT0”]? []? [pos=”AJ.*”]* [pos=”N.*”]
RC ([pos=”DTQ”  |pos=”PNQ”  |pos=”CJT”] /VCF[] []?) |
([pos=”CJT”]? /NP[] /VCF[] []?) |
([pos=”PR.*”]* [pos=”.Q”] /NP[] /VCF[] []?)
VCF [pos=”V.?B”  |pos=”V.?D”  |pos=”V.?Z”  |pos=”VM0”] [pos=”V.*”]*
</equation>
<tableCaption confidence="0.5102195">
Table 2: CQP macro expansions for self-defined macros. BNC uses the CLAWS5 tagset for POS tags
(http://www.natcorp.ox.ac.uk/docs/c5spec.html).
</tableCaption>
<reference confidence="0.999520384615385">
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the Effects of Selective Sampling on
the Annotation Task. In Proceedings of CoNLL
2005, pages 144–151, Stroudsburg, PA, USA. ACL.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Sch¨olkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in Kernel Methods,
pages 169–184. MIT Press, Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003, pages 423–430, Stroudsburg, PA, USA. ACL.
Thomas Morton and Jeremy LaCivita. 2003.
WordFreak: An open tool for linguistic annota-
tion. In Proceedings of NAACL HLT 2003, NAACL-
Demonstrations, pages 17–18, Stroudsburg, PA,
USA. ACL.
Alessandro Moschitti. 2006. Making Tree Kernels
Practical for Natural Language Learning. In Pro-
ceedings of EACL 2006, pages 113–120, Trento,
Italy.
Burr Settles. 2011. Closing the Loop: Fast, Inter-
active Semi-supervised Annotation with Queries on
Features and Instances. In Proceedings of EMNLP
2011, pages 1467–1478, Stroudsburg, PA, USA.
ACL.
The British National Corpus, version 3 (BNC XML
Edition). 2007. Distributed by Oxford University
Computing Services on behalf of the BNC Consor-
tium. URL: http://www.natcorp.ox.ac.uk/.
Seid Muhie Yimam, Richard Eckart de Castilho, Iryna
Gurevych, and Chris Biemann. 2014. Automatic
Annotation Suggestions and Custom Annotation
Layers in WebAnno. In Proceedings of ACL 2014,
System Demonstrations, pages 91–96, Stroudsburg,
PA, USA. ACL.
</reference>
<page confidence="0.999293">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929332">
<title confidence="0.998758">In-tool Learning for Selective Manual Annotation in Large Corpora</title>
<author confidence="0.996097">Do Richard Eckart de_Iryna Knowledge Processing Lab</author>
<affiliation confidence="0.998371">Department of Computer Science, Technische Universit¨at Knowledge Processing Lab German Institute for Educational Research and Educational</affiliation>
<web confidence="0.978993">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.998184434782609">We present a novel approach to the selective annotation of large corpora through the use of machine learning. Linguistic search engines used to locate potential instances of an infrequent phenomenon do not support ranking the search results. This favors the use of high-precision queries that return only a few results over broader queries that have a higher recall. Our approach introduces a classifier used to rank the search results and thus helping the annotator focus on those results with the highest potential of being an instance of the phenomenon in question, even in low-precision queries. The clasis trained in an except for preprocessing relying only on the manual annotations done by the users in the querying tool itself. To implement approach, we build upon a web-based multi-user search and annotation tool.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>In Proceedings of the Workshop on OIAF4HLT at COLING 2014,</booktitle>
<pages>1--11</pages>
<marker></marker>
<rawString>In Proceedings of the Workshop on OIAF4HLT at COLING 2014, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
<author>Sabine Bartsch</author>
</authors>
<title>CSniper: Annotationby-query for Non-canonical Constructions in Large Corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012, System Demonstrations,</booktitle>
<pages>85--90</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<marker>de Castilho, Gurevych, Bartsch, 2012</marker>
<rawString>Richard Eckart de Castilho, Iryna Gurevych, and Sabine Bartsch. 2012. CSniper: Annotationby-query for Non-canonical Constructions in Large Corpora. In Proceedings of ACL 2012, System Demonstrations, pages 85–90, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
<author>Andrew Hardie</author>
</authors>
<title>Twenty-first century corpus workbench: Updating a query architecture for the new millennium.</title>
<date>2011</date>
<booktitle>In Proceedings of CL2011,</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="4487" citStr="Evert and Hardie, 2011" startWordPosition="697" endWordPosition="700">ally we demonstrate the viability of our concept by the example task of finding non-canonical constructions in Section 3. 2 Ranking linguistic query results Our approach employs machine learning to facilitate — but not to completely replace — the manual annotation of query results. A query expresses the intention of the user to find a specific linguistic phenomenon (information need). An information retrieval search engine would provide the user with a list of results that are ranked according to their relevance, fulfilling the information need. However, linguistic search engines such as CQP (Evert and Hardie, 2011) — which is used by CSniper — are basically pattern-matching engines, operating on different lexical or morphosyntactic features like part-of-speech (POS) tags and lemmata and do not have a concept of relevance. Thus, if the query provided by the user overgeneralizes, relevant results are hidden among many irrelevant ones, ultimately failing to satisfy the user’s information need. To tackle this problem, we use the annotations already created by users on the search results to train a classifier. Unannotated query results are then fed to the classifier whose output values are then used as relev</context>
</contexts>
<marker>Evert, Hardie, 2011</marker>
<rawString>Stefan Evert and Andrew Hardie. 2011. Twenty-first century corpus workbench: Updating a query architecture for the new millennium. In Proceedings of CL2011, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Beatrice Alex</author>
<author>Markus Becker</author>
</authors>
<title>Investigating the Effects of Selective Sampling on the Annotation Task.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>144--151</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11701" citStr="Hachey et al. (2005)" startWordPosition="1859" endWordPosition="1862"> CSniper instead ranks the search results for a given query by confidence of the classifier. Yet another form of in-tool learning is active learning, as is implemented, e.g., in Dualist (Settles, 2011). In an active learning scenario the system aims to efficiently train an accurate classifier (i.e. with as little training data as possible) and thus repeatedly asks the user to annotate instances from which it can learn the most. Such an approach can work well for reducing the amount of training data needed to produce a model which achieves high accuracy, as has been — amongst others — shown by Hachey et al. (2005). However, they also learn in their experiments that those highly informative instances are often harder to annotate and increase required time and effort of annotators. Our approach is different from active 15 learning as our goal is not to improve the training efficiency of the classifier but rather to allow the user to interactively find and label as many true instances of a phenomenon as possible in a large corpus. Thus, the items presented to the user are not determined by the expected information gain for the classifier but rather by the confidence of the classifier, presenting the user </context>
</contexts>
<marker>Hachey, Alex, Becker, 2005</marker>
<rawString>Ben Hachey, Beatrice Alex, and Markus Becker. 2005. Investigating the Effects of Selective Sampling on the Annotation Task. In Proceedings of CoNLL 2005, pages 144–151, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Huddleston</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12828" citStr="Huddleston and Pullum, 2002" startWordPosition="2033" endWordPosition="2036">rmation gain for the classifier but rather by the confidence of the classifier, presenting the user with those instances first which are most likely to be occurrences of the phenomenon in question. 3 Case study: Finding non-canonical constructions We demonstrate our novel approach on the task of locating non-canonical constructions (NCC) and conduct an intrinsic evaluation of the accuracy of the system augmented with machine learning output on the data annotated by expert linguists. The linguists annotated sentences for occurrences of certain NCC subtypes: information-packaging constructions (Huddleston and Pullum, 2002, pp. 1365ff.), which present information in a different way from their canonical counterparts without changing truth conditions; specifically It-clefts (“It was Peter who made lunch.”), NP-preposing (“A treasure, he was searching.”), and PP-inversion (“To his left lay the forest.”) clauses. For our experiments, we used the British National Corpus (2007), comprising 100 million words in multiple domains3. Constituency parsing was conducted using the factored variant of the Stanford Parser (Klein and Manning, 2003), incorporated into a UIMA pipeline using DKPro Core (Eckart de Castilho and Gure</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>Rodney D. Huddleston and Geoffrey K. Pullum. 2002. The Cambridge Grammar of the English Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods,</booktitle>
<pages>169--184</pages>
<editor>In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="5922" citStr="Joachims, 1999" startWordPosition="936" endWordPosition="937">ed as training data, or how many of the selected users have to agree on an annotation for it to be included. 2.1 Workflow and ranking process in CSniper Currently, we train the classifier on features derived from the constituency parse tree, which makes it useful for tasks such as locating sentences containing infrequent ambiguous grammatical constructions (cf. Section 3). Since parsing the query results is too time-intensive to be done during runtime, we parsed the corpora in advance and stored the parse trees in a database. To train the classifier, we employed SVM-light-tk (Moschitti, 2006; Joachims, 1999), a support vector machine implementation which uses a tree kernel to integrate all sub-trees of the parse tree as features. Consider the following typical scenario incorporating the ranking: A user constructs a query based on various features, such as POS tags or lemmata, which are used to search for matching sentences, e.g. “It” [lemma=“be”] [pos=“AT0”]? [pos=“NN.*”]2 The result is a list of sentences presented in a keywords-in-context (KWIC) view, along with an annotation field (Figure 2). Then the user starts to annotate these sentences as Correct or Wrong, depending whether they truly rep</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<pages>423--430</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13347" citStr="Klein and Manning, 2003" startWordPosition="2109" endWordPosition="2112">for occurrences of certain NCC subtypes: information-packaging constructions (Huddleston and Pullum, 2002, pp. 1365ff.), which present information in a different way from their canonical counterparts without changing truth conditions; specifically It-clefts (“It was Peter who made lunch.”), NP-preposing (“A treasure, he was searching.”), and PP-inversion (“To his left lay the forest.”) clauses. For our experiments, we used the British National Corpus (2007), comprising 100 million words in multiple domains3. Constituency parsing was conducted using the factored variant of the Stanford Parser (Klein and Manning, 2003), incorporated into a UIMA pipeline using DKPro Core (Eckart de Castilho and Gurevych, 2014). As a baseline we use queries representing the experts’ intuition about the realization of the NCCs in terms of POS tags and lemmata. We show that our system improves the precision of the query results even with little training data. Also we present run times for our ranking system under real-world conditions for different training set sizes. Further, we compare Krippendorff’s α coefficient as an inter-annotator agreement measure among only annotators to the α which treats our system as one additional </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of ACL 2003, pages 423–430, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
<author>Jeremy LaCivita</author>
</authors>
<title>WordFreak: An open tool for linguistic annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL HLT 2003, NAACLDemonstrations,</booktitle>
<pages>17--18</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9640" citStr="Morton and LaCivita, 2003" startWordPosition="1526" endWordPosition="1529"> 500). One of these partitions is chosen at random, and the sentences therein are ranked using the SVM. This step is repeated, until a previously defined number of sentences have been classified as Correct. Those sentences are then shown to the user, who now can either confirm a sentence as containing the phenomenon or label it Wrong otherwise. 2.3 Related work Existing annotation tools include automation functionality for annotation tasks, ranging from rule-based tagging to more complex, machinelearning-based approaches. Such functionalities can be found in the annotation software WordFreak (Morton and LaCivita, 2003), where a plug-in architecture allows for a variety of different taggers and classifiers to be integrated, for example part-of-speech taggers or coreference resolution engines. Those require pretrained models, which limits the applicability of the automation capabilities of WordFreak to tasks for which such models are actually available. In addition to assigning annotations a single label, WordFreak allows plugins to rank labels for each annotation based on the confidence of the used classifier. Note that this is different to our ranking approach, where we instead perform a ranking of the sear</context>
</contexts>
<marker>Morton, LaCivita, 2003</marker>
<rawString>Thomas Morton and Jeremy LaCivita. 2003. WordFreak: An open tool for linguistic annotation. In Proceedings of NAACL HLT 2003, NAACLDemonstrations, pages 17–18, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making Tree Kernels Practical for Natural Language Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL 2006,</booktitle>
<pages>113--120</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="5905" citStr="Moschitti, 2006" startWordPosition="933" endWordPosition="935">sers should be used as training data, or how many of the selected users have to agree on an annotation for it to be included. 2.1 Workflow and ranking process in CSniper Currently, we train the classifier on features derived from the constituency parse tree, which makes it useful for tasks such as locating sentences containing infrequent ambiguous grammatical constructions (cf. Section 3). Since parsing the query results is too time-intensive to be done during runtime, we parsed the corpora in advance and stored the parse trees in a database. To train the classifier, we employed SVM-light-tk (Moschitti, 2006; Joachims, 1999), a support vector machine implementation which uses a tree kernel to integrate all sub-trees of the parse tree as features. Consider the following typical scenario incorporating the ranking: A user constructs a query based on various features, such as POS tags or lemmata, which are used to search for matching sentences, e.g. “It” [lemma=“be”] [pos=“AT0”]? [pos=“NN.*”]2 The result is a list of sentences presented in a keywords-in-context (KWIC) view, along with an annotation field (Figure 2). Then the user starts to annotate these sentences as Correct or Wrong, depending wheth</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making Tree Kernels Practical for Natural Language Learning. In Proceedings of EACL 2006, pages 113–120, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Closing the Loop: Fast, Interactive Semi-supervised Annotation with Queries on Features and Instances.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>1467--1478</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11282" citStr="Settles, 2011" startWordPosition="1785" endWordPosition="1787">his, WebAnno uses a split-pane view, showing automatic suggestions in one pane and manually entered annotations in another. The user can accept a suggested annotation, which is transferred to the manual pane. Lacking the search capability, WebAnno lists automatic annotations in the running corpus text, which makes it unsuited for selective annotation in large corpora. The approach that we implemented on top of CSniper instead ranks the search results for a given query by confidence of the classifier. Yet another form of in-tool learning is active learning, as is implemented, e.g., in Dualist (Settles, 2011). In an active learning scenario the system aims to efficiently train an accurate classifier (i.e. with as little training data as possible) and thus repeatedly asks the user to annotate instances from which it can learn the most. Such an approach can work well for reducing the amount of training data needed to produce a model which achieves high accuracy, as has been — amongst others — shown by Hachey et al. (2005). However, they also learn in their experiments that those highly informative instances are often harder to annotate and increase required time and effort of annotators. Our approac</context>
</contexts>
<marker>Settles, 2011</marker>
<rawString>Burr Settles. 2011. Closing the Loop: Fast, Interactive Semi-supervised Annotation with Queries on Features and Instances. In Proceedings of EMNLP 2011, pages 1467–1478, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<title>The British National Corpus, version 3 (BNC XML Edition).</title>
<date>2007</date>
<booktitle>Distributed by Oxford University Computing Services on behalf of the BNC Consortium. URL: http://www.natcorp.ox.ac.uk/.</booktitle>
<contexts>
<context position="13184" citStr="(2007)" startWordPosition="2088" endWordPosition="2088">f the accuracy of the system augmented with machine learning output on the data annotated by expert linguists. The linguists annotated sentences for occurrences of certain NCC subtypes: information-packaging constructions (Huddleston and Pullum, 2002, pp. 1365ff.), which present information in a different way from their canonical counterparts without changing truth conditions; specifically It-clefts (“It was Peter who made lunch.”), NP-preposing (“A treasure, he was searching.”), and PP-inversion (“To his left lay the forest.”) clauses. For our experiments, we used the British National Corpus (2007), comprising 100 million words in multiple domains3. Constituency parsing was conducted using the factored variant of the Stanford Parser (Klein and Manning, 2003), incorporated into a UIMA pipeline using DKPro Core (Eckart de Castilho and Gurevych, 2014). As a baseline we use queries representing the experts’ intuition about the realization of the NCCs in terms of POS tags and lemmata. We show that our system improves the precision of the query results even with little training data. Also we present run times for our ranking system under real-world conditions for different training set sizes.</context>
</contexts>
<marker>2007</marker>
<rawString>The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. URL: http://www.natcorp.ox.ac.uk/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seid Muhie Yimam</author>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
<author>Chris Biemann</author>
</authors>
<title>Automatic Annotation Suggestions and Custom Annotation Layers in WebAnno.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014, System Demonstrations,</booktitle>
<pages>91--96</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<marker>Yimam, de Castilho, Gurevych, Biemann, 2014</marker>
<rawString>Seid Muhie Yimam, Richard Eckart de Castilho, Iryna Gurevych, and Chris Biemann. 2014. Automatic Annotation Suggestions and Custom Annotation Layers in WebAnno. In Proceedings of ACL 2014, System Demonstrations, pages 91–96, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>