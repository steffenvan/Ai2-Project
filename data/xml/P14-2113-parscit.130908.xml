<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.998802">
A Piece of My Mind: A Sentiment Analysis Approach
for Online Dispute Detection
</title>
<author confidence="0.999256">
Lu Wang
</author>
<affiliation confidence="0.9962725">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.794296">
Ithaca, NY 14853
</address>
<email confidence="0.999203">
luwang@cs.cornell.edu
</email>
<sectionHeader confidence="0.993905" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998735714285714">
We investigate the novel task of online dis-
pute detection and propose a sentiment analy-
sis solution to the problem: we aim to identify
the sequence of sentence-level sentiments ex-
pressed during a discussion and to use them
as features in a classifier that predicts the
DISPUTE/NON-DISPUTE label for the dis-
cussion as a whole. We evaluate dispute de-
tection approaches on a newly created corpus
of Wikipedia Talk page disputes and find that
classifiers that rely on our sentiment tagging
features outperform those that do not. The best
model achieves a very promising F1 score of
0.78 and an accuracy of 0.80.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972681818182">
As the web has grown in popularity and scope, so
has the promise of collaborative information en-
vironments for the joint creation and exchange of
knowledge (Jones and Rafaeli, 2000; Sack, 2005).
Wikipedia, a wiki-based online encyclopedia, is
arguably the best example: its distributed edit-
ing environment allows readers to collaborate as
content editors and has facilitated the production
of over four billion articles1 of surprisingly high
quality (Giles, 2005) in English alone since its de-
but in 2001.
Existing studies of collaborative knowledge
systems have shown, however, that the quality of
the generated content (e.g. an encyclopedia arti-
cle) is highly correlated with the effectiveness of
the online collaboration (Kittur and Kraut, 2008;
Kraut and Resnick, 2012); fruitful collaboration,
in turn, inevitably requires dealing with the dis-
putes and conflicts that arise (Kittur et al., 2007).
Unfortunately, human monitoring of the often
massive social media and collaboration sites to de-
tect, much less mediate, disputes is not feasible.
</bodyText>
<footnote confidence="0.988747">
1http://en.wikipedia.org
</footnote>
<author confidence="0.891457">
Claire Cardie
</author>
<affiliation confidence="0.9929785">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.599109">
Ithaca, NY 14853
</address>
<email confidence="0.991492">
cardie@cs.cornell.edu
</email>
<bodyText confidence="0.999976902439024">
In this work, we investigate the heretofore novel
task of dispute detection in online discussions.
Previous work in this general area has analyzed
dispute-laden content to discover features corre-
lated with conflicts and disputes (Kittur et al.,
2007). Research focused primarily on cues de-
rived from the edit history of the jointly created
content (e.g. the number of revisions, their tem-
poral density (Kittur et al., 2007; Yasseri et al.,
2012)) and relied on small numbers of manually
selected discussions known to involve disputes. In
contrast, we investigate methods for the automatic
detection, i.e. prediction, of discussions involving
disputes. We are also interested in understanding
whether, and which, linguistic features of the dis-
cussion are important for dispute detection.
Drawing inspiration from studies of human me-
diation of online conflicts (e.g. Billings and Watts
(2010), Kittur et al. (2007), Kraut and Resnick
(2012)), we hypothesize that effective methods
for dispute detection should take into account the
sentiment and opinions expressed by participants
in the collaborative endeavor. As a result, we
propose a sentiment analysis approach for online
dispute detection that identifies the sequence of
sentence-level sentiments (i.e. very negative, neg-
ative, neutral, positive, very positive) expressed
during the discussion and uses them as features
in a classifier that predicts the DISPUTE/NON-
DISPUTE label for the discussion as a whole. Con-
sider, for example, the snippet in Figure 1 from the
Wikipedia Talk page for the article on Philadel-
phia; it discusses the choice of a picture for the
article’s “infobox”. The sequence of almost exclu-
sively negative statements provides evidence of a
dispute in this portion of the discussion.
Unfortunately, sentence-level sentiment tagging
for this domain is challenging in its own right
due to the less formal, often ungrammatical, lan-
guage and the dynamic nature of online conver-
sations. “Really, grow up” (segment 3) should
</bodyText>
<page confidence="0.987814">
693
</page>
<note confidence="0.5759815">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 693–699,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<tableCaption confidence="0.742428666666667">
1-Emy111: I think everyone is forgetting that my previous image was the
lead image for well over a year! ...
&gt; Massimo: I’m sorry to say so, but it is grossly over processed...
2-Emy111: i’m glad you paid more money for a camera than I did. con-
grats... i appreciate your constructive criticism. thank you.
&gt; Massimo: I just want to have the best picture as a lead for the article ...
3-Emy111: Wow, I am really enjoying this photography debate... [so don’t
make assumptions you know nothing about.]NN [Really, grow up.]N [If you
all want to complain about Photoshop editing, lets all go buy medium for-
mat film cameras, shoot film, and scan it, so no manipulation is possible.]O
[Sound good?]NN
&gt; Massimo:... I do feel it is a pity, that you turned out to be a sore loser...
</tableCaption>
<figureCaption confidence="0.99138625">
Figure 1: From the Wikipedia Talk page for the article
“Philadelphia”. Omitted sentences are indicated by ellipsis.
Names of editors are in bold. The start of each set of related
turns is numbered; “&gt;” is an indicator for the reply structure.
</figureCaption>
<bodyText confidence="0.999763058823529">
presumably be tagged as a negative sentence as
should the sarcastic sentences “Sounds good?” (in
the same turn) and “congrats” and “thank you”
(in segment 2). We expect that these, and other,
examples will be difficult for the sentence-level
classifier unless the discourse context of each sen-
tence is considered. Previous research on senti-
ment prediction for online discussions, however,
focuses on turn-level predictions (Hahn et al.,
2006; Yin et al., 2012).2 As the first work that
predicts sentence-level sentiment for online dis-
cussions, we investigate isotonic Conditional Ran-
dom Fields (CRFs) (Mao and Lebanon, 2007) for
the sentiment-tagging task as they preserve the ad-
vantages of the popular CRF-based sequential tag-
ging models (Lafferty et al., 2001) while provid-
ing an efficient mechanism for encoding domain
knowledge — in our case, a sentiment lexicon —
through isotonic constraints on model parameters.
We evaluate our dispute detection approach us-
ing a newly created corpus of discussions from
Wikipedia Talk pages (3609 disputes, 3609 non-
disputes).3 We find that classifiers that employ the
learned sentiment features outperform others that
do not. The best model achieves a very promis-
ing F1 score of 0.78 and an accuracy of 0.80 on
the Wikipedia dispute corpus. To the best of our
knowledge, this represents the first computational
approach to automatically identify online disputes
on a dataset of scale.
Additional Related Work. Sentiment analysis
has been utilized as a key enabling technique in
a number of conversation-based applications. Pre-
vious work mainly studies the attitudes in spoken
</bodyText>
<footnote confidence="0.982459125">
2A notable exception is Hassan et al. (2010), which identi-
fies sentences containing “attitudes” (e.g. opinions), but does
not distinguish them w.r.t. sentiment. Context information is
also not considered.
3The talk page associated with each article records con-
versations among editors about the article content and allows
editors to discuss the writing process, e.g. planning and orga-
nizing the content.
</footnote>
<bodyText confidence="0.9932676">
meetings (Galley et al., 2004; Hahn et al., 2006) or
broadcast conversations (Wang et al., 2011) using
variants of Conditional Random Fields (Lafferty et
al., 2001) and predicts sentiment at the turn-level,
while our predictions are made for each sentence.
</bodyText>
<sectionHeader confidence="0.943007" genericHeader="method">
2 Data Construction: A Dispute Corpus
</sectionHeader>
<bodyText confidence="0.996871314285714">
We construct the first dispute detection corpus to
date; it consists of dispute and non-dispute discus-
sions from Wikipedia Talk pages.
Step 1: Get Talk Pages of Disputed Articles.
Wikipedia articles are edited by different editors.
If an article is observed to have disputes on its
talk page, editors can assign dispute tags to the
article to flag it for attention. In this research, we
are interested in talk pages whose corresponding
articles are labeled with the following tags:
DISPUTED, TOTALLYDISPUTED, DISPUTED-
SECTION, TOTALLYDISPUTED-SECTION, POV.
The tags indicate that an article is disputed, or the
neutrality of the article is disputed (POV).
We use the 2013-03-04 Wikipedia data dump,
and extract talk pages for articles that are labeled
with dispute tags by checking the revision history.
This results in 19,071 talk pages.
Step 2: Get Discussions with Disputes. Dis-
pute tags can also be added to talk pages them-
selves. Therefore, in addition to the tags men-
tioned above, we also consider the “Request for
Comment” (RFC) tag on talk pages. According to
Wikipedia4, RFC is used to request outside opin-
ions concerning the disputes.
3609 discussions are collected with dispute
tags found in the revision history. We further
classify dispute discussions into three subcate-
gories: CONTROVERSY, REQUEST FOR COM-
MENT (RFC), and RESOLVED based on the tags
found in discussions (see Table 1). The numbers
of discussions for the three types are 42, 3484, and
105, respectively. Note that dispute tags only ap-
pear in a small number of articles and talk pages.
There may exist other discussions with disputes.
</bodyText>
<table confidence="0.9914374">
Dispute Subcategory Wikipedia Tags on Talk pages
Controversy CONTROVERSIAL, TOTALLYDISPUTED,
DISPUTED, CALM TALK, POV
Request for Comment RFC
Resolved Any tag from above+ RESOLVED
</table>
<tableCaption confidence="0.942231">
Table 1: Subcategory for disputes with corresponding tags.
Note that each discussion in the RESOLVED class has more
than one tag.
</tableCaption>
<bodyText confidence="0.865269">
Step 3: Get Discussions without Disputes. Like-
wise, we collect non-dispute discussions from
</bodyText>
<footnote confidence="0.9436155">
4http://en.wikipedia.org/wiki/Wikipedia:
Requests_for_comment
</footnote>
<page confidence="0.997379">
694
</page>
<bodyText confidence="0.999754">
pages that are never tagged with disputes. We con-
sider non-dispute discussions with at least 3 dis-
tinct speakers and 10 turns. 3609 discussions are
randomly selected with this criterion. The average
turn numbers for dispute and non-dispute discus-
sions are 45.03 and 22.95, respectively.
</bodyText>
<subsectionHeader confidence="0.660015">
Lexical Features
</subsectionHeader>
<table confidence="0.755104333333333">
- unigram/bigram
- number of words all uppercased
- number of words
Discourse Features
- initial uni-/bi-/tri-gram
- repeated punctuations
- hedging phrases collected from
Farkas et al. (2010)
- number of negators
Syntactic/Semantic Features
- unigram with POS tag
- dependency relation
</table>
<subsectionHeader confidence="0.393328">
Conversation Features
</subsectionHeader>
<bodyText confidence="0.586966666666667">
- quote overlap with target
- TFIDF similarity with target
(remove quote first)
</bodyText>
<subsectionHeader confidence="0.743547">
Sentiment Features
</subsectionHeader>
<bodyText confidence="0.614685">
- connective + sentiment words
- sentiment dependency relation
- sentiment words
</bodyText>
<sectionHeader confidence="0.887766" genericHeader="method">
3 Sentence-level Sentiment Prediction
</sectionHeader>
<bodyText confidence="0.994228488372093">
This section describes our sentence-level senti-
ment tagger, from which we construct features for
dispute detection (Section 4).
Consider a discussion comprised of sequential
turns; each turn consists of a sequence of sen-
tences. Our model takes as input the sentences
x = {x1, · · · , xn} from a single turn, and out-
puts the corresponding sequence of sentiment la-
bels y = {y1, · · · , yn}, where yi E O, O =
{NN, N, O, P, PP}. The labels in O represent
very negative (NN), negative (N), neutral (O), pos-
itive (P), and very positive (PP), respectively.
Given that traditional Conditional Random
Fields (CRFs) (Lafferty et al., 2001) ignore the or-
dinal relations among sentiment labels, we choose
isotonic CRFs (Mao and Lebanon, 2007) for
sentence-level sentiment analysis as they can en-
force monotonicity constraints on the parameters
consistent with the ordinal structure and domain
knowledge (e.g. word-level sentiment conveyed
via a lexicon). Concretely, we take a lexicon M =
MpUMn, where Mp and Mn are two sets of fea-
tures (usually words) identified as strongly associ-
ated with positive and negative sentiment. Assume
encodes the weight between label Q and
feature w, for each feature w E Mp; then the iso-
tonic CRF enforces Q &lt; Q0 ==&gt;. µhσ,wi &lt; µhσ,,wi.
For example, when “totally agree” is observed in
training, parameter µhPP,totally agreei is likely to
increase. Similar constraints are defined on Mn.
Our lexicon is built by combining MPQA (Wil-
son et al., 2005), General Inquirer (Stone et al.,
1966), and SentiWordNet (Esuli and Sebastiani,
2006) lexicons. Words with contradictory senti-
ments are removed. We use the features in Table 2
for sentiment prediction.
Syntactic/Semantic Features. We have two ver-
sions of dependency relation features, the origi-
nal form and a form that generalizes a word to its
POS tag, e.g. “nsubj(wrong, you)” is generalized
to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”.
Discourse Features. We extract the initial uni-
gram, bigram, and trigram of each utterance as dis-
</bodyText>
<tableCaption confidence="0.997996666666667">
Table 2: Features used in sentence-level sentiment predic-
tion. Numerical features are first normalized by standardiza-
tion, then binned into 5 categories.
</tableCaption>
<bodyText confidence="0.9990995">
course features (Hirschberg and Litman, 1993).
Sentiment Features. We gather connectives from
the Penn Discourse TreeBank (Rashmi Prasad and
Webber, 2008) and combine them with any senti-
ment word that precedes or follows it as new fea-
tures. Sentiment dependency relations are the de-
pendency relations that include a sentiment word.
We replace those words with their polarity equiv-
alents. For example, relation “nsubj(wrong, you)”
becomes “nsubj(SentiWordneg, you)”.
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="method">
4 Online Dispute Detection
</sectionHeader>
<subsectionHeader confidence="0.998645">
4.1 Training A Sentiment Classifier
</subsectionHeader>
<bodyText confidence="0.999590103448276">
Dataset. We train the sentiment classifier using
the Authority and Alignment in Wikipedia Discus-
sions (AAWD) corpus (Bender et al., 2011) on a 5-
point scale (i.e. NN, N, O, P, PP). AAWD consists
of 221 English Wikipedia discussions with posi-
tive and negative alignment annotations. Annota-
tors either label each sentence as positive, negative
or neutral, or label the full turn. For instances that
have only a turn-level label, we assume all sen-
tences have the same label as the turn. We further
transform the labels into the five sentiment labels.
Sentences annotated as being a positive alignment
by at least two annotators are treated as very posi-
tive (PP). If a sentence is only selected as positive
by one annotator or obtains the label via turn-level
annotation, it is positive (P). Very negative (NN)
and negative (N) are collected in the same way.
All others are neutral (O). Among all 16,501 sen-
tences in AAWD, 1,930 and 1,102 are labeled as
NN and N. 532 and 99 of them are PP and P. The
other 12,648 are considered neutral.
Evaluation. To evaluate the performance of the
sentiment tagger, we compare to two baselines.
(1) Baseline (Polarity): a sentence is predicted as
positive if it has more positive words than nega-
tive words, or negative if more negative words are
observed. Otherwise, it is neutral. (2) Baseline
(Distance) is extended from (Hassan et al., 2010).
Each sentiment word is associated with the closest
</bodyText>
<page confidence="0.991865">
695
</page>
<table confidence="0.999929285714286">
Pos Neg Neutral
Baseline (Polarity) 22.53 38.61 66.45
Baseline (Distance) 33.75 55.79 88.97
SVM (3-way) 44.62 52.56 80.84
CRF (3-way) 56.28 56.37 89.41
CRF (5-way) 58.39 56.30 90.10
isotonic CRF 68.18 62.53 88.87
</table>
<tableCaption confidence="0.997067">
Table 3: F1 scores for positive and negative alignment on
</tableCaption>
<bodyText confidence="0.977765074074074">
Wikipedia Talk pages (AAWD) using 5-fold cross-validation.
In each column, bold entries (if any) are statistically signif-
icantly higher than all the rest. We also compare with an
SVM and linear CRF trained with three classes (3-way). Our
model based on the isotonic CRF produces significantly bet-
ter results than all the other systems.
second person pronoun, and a surface distance is
computed. An SVM classifier (Joachims, 1999) is
trained using features of the sentiment words and
minimum/maximum/average of the distances.
We also compare with two state-of-the-art
methods that are used in sentiment prediction for
conversations: (1) an SVM (RBF kernel) that is
employed for identifying sentiment-bearing sen-
tences (Hassan et al., 2010), and (dis)agreement
detection (Yin et al., 2012) in online debates; (2)
a Linear CRF for (dis)agreement identification in
broadcast conversations (Wang et al., 2011).
We evaluate the systems using standard F1 on
classes of positive, negative, and neutral, where
samples predicted as PP and P are positive align-
ment, and samples tagged as NN and N are neg-
ative alignment. Table 3 describes the main re-
sults on the AAWD dataset: our isotonic CRF
based system significantly outperforms the alter-
natives for positive and negative alignment detec-
tion (paired-t test, p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.980875">
4.2 Dispute Detection
</subsectionHeader>
<bodyText confidence="0.999865823529412">
We model dispute detection as a standard bi-
nary classification task, and investigate four major
types of features as described below.
Lexical Features. We first collect unigram and
bigram features for each discussion.
Topic Features. Articles on specific topics, such
as politics or religions, tend to arouse more dis-
putes. We thus extract the category informa-
tion of the corresponding article for each talk page.
We further utilize unigrams and bigrams of
the category as topic features.
Discussion Features. This type of feature aims
to capture the structure of the discussion. Intu-
itively, the more turns or the more participants
a discussion has, the more likely there is a
dispute. Meanwhile, participants tend to produce
longer utterances when they make arguments.
We choose number of turns, number
of participants, average number of
words in each turn as features. In addi-
tion, the frequency of revisions made during the
discussion has been shown to be good indicator
for controversial articles (Vuong et al., 2008), that
are presumably prone to have disputes. Therefore,
we encode the number of revisions that
happened during the discussion as a feature.
Sentiment Features. This set of features en-
code the sentiment distribution and transition in
the discussion. We train our sentiment tagging
model on the full AAWD dataset, and run it on
the Wikipedia dispute corpus.
Given that consistent negative senti-
ment flow usually indicates an ongoing
dispute, we first extract features from
sentiment distribution in the form
of number/probability of sentiment
per type. We also estimate the sentiment
transition probability P(5t → 5t+1) from
our predictions, where 5t and 5t+1 are sentiment
labels for the current sentence and the next. We
then have features as number/portion of
sentiment transitions per type.
Features described above mostly depict the
global sentiment flow in the discussions. We fur-
ther construct a local version of them, since sen-
timent distribution may change as discussion pro-
ceeds. For example, less positive sentiment can be
observed as dispute being escalated. We thus split
each discussion into three equal length stages, and
create sentiment distribution and transition fea-
tures for each stage.
</bodyText>
<table confidence="0.999904333333333">
Prec Rec F1 Acc
Baseline (Random) 50.00 50.00 50.00 50.00
Baseline (All dispute) 50.00 100.00 66.67 50.00
Logistic Regression 74.76 72.29 73.50 73.94
SVMLinear 69.81 71.90 70.84 70.41
SVMRBF 77.38 79.14 78.25 80.00
</table>
<tableCaption confidence="0.6915196">
Table 4: Dispute detection results on Wikipedia Talk pages.
The numbers are multiplied by 100. The items in bold are sta-
tistically significantly higher than others in the same column
(paired-t test, p &lt; 0.05). SVM with the RBF kernel achieves
the best performance in precision, F1, and accuracy.
</tableCaption>
<sectionHeader confidence="0.761622" genericHeader="evaluation">
Results and Error Analysis. We experiment with
</sectionHeader>
<bodyText confidence="0.998214888888889">
logistic regression, SVM with linear and RBF ker-
nels, which are effective methods in multiple text
categorization tasks (Joachims, 1999; Zhang and
J. Oles, 2001). We normalize the features by stan-
dardization and conduct a 5-fold cross-validation.
Two baselines are listed: (1) labels are randomly
assigned; (2) all discussions have disputes.
Main results for different classifiers are dis-
played in Table 4. All learning based methods
</bodyText>
<page confidence="0.992378">
696
</page>
<figure confidence="0.9888844">
Sentiment
Sentiment
1
0
1
</figure>
<page confidence="0.502525">
2
</page>
<table confidence="0.696006809523809">
Sentiment Flow in Discussion with Unresolved Dispute Sample sentences (sentiment in parentheses)
A: no, I sincerely plead with you... (N) If not, you are just wasting my
time. (NN)
B: I believe Sweet’s proposal... is quite silly. (NN)
C: Tell you what. (NN) If you can get two other editors to agree... I will
shut up and sit down. (NN)
D: But some idiot forging your signature claimed that doing so would
violate. (NN)... Please go have some morning coffee. (O)
E: And I don’t like coffee. (NN) Good luck to you. (NN)
F: Was that all? (NN)... I think that you are in error... (N)
Sentiment Flow in Discussion with Resolved Dispute CA: So far so confusing.
2 F B:... I can not see a rationale for the landrace having its own article...
(N) With Turkish Van being a miserable stub, there’s no such rationale for
forking off a new article... (NN)...
C: I’ve also copied your post immediately above to that article’s talk page
since it is a great “nutshell” summary. (PP)
D: Err.. how can the opposite be true... (N)
E: Thanks for this, though I have to say some of the facts floating around
this discussion are wrong. (P)
A F: Great. (PP) Let’s make sure the article is clear on this. (O)
T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T14
</table>
<figureCaption confidence="0.999719142857143">
Figure 2: Sentiment flow for a discussion with unresolved dispute about the definition of “white people” (top) and a dis-
cussion with resolved dispute on merging articles about van cat (bottom). The labels {NN, N, O, P, PP} are mapped to
{−2, −1, 0, 1, 2} in sequence. Sentiment values are convolved by Gaussian smoothing kernel, and cubic-spline interpolation is
then conducted. Different speakers are represented by curves of different colors. Dashed vertical lines delimit turns. Represen-
tative sentences are labeled with letters and their sentiment labels are shown on the right. For unresolved dispute (top), we see
that negative sentiment exists throughout the discussion. Whereas, for the resolved dispute (bottom), less negative sentiment is
observed at the end of the discussion; participants also show appreciation after the problem is solved (e.g. E and F in the plot).
</figureCaption>
<figure confidence="0.966349181818182">
2
1
0
1
2
A B
C n EF
T1 T2 T3T4T5 T6 T7 T8 T9 T10 T11 T12 T13T14 T15 T16 T17 T18
E
B
D
</figure>
<table confidence="0.998138888888889">
Prec Rec F1 Acc
Lexical (Lex) 75.86 34.66 47.58 61.82
Topic (Top) 68.44 71.46 69.92 69.26
Discussion (Dis) 69.73 76.14 72.79 71.54
Sentiment (Sentig+l) 72.54 69.52 71.00 71.60
Top + Dis 68.49 71.79 70.10 69.38
Top + Dis + Sentig 77.39 78.36 77.87 77.74
Top + Dis + Sentig+l 77.38 79.14 78.25 80.00
Lex + Top + Dis + Sentig+l 78.38 75.12 76.71 77.20
</table>
<tableCaption confidence="0.650798714285714">
Table 5: Dispute detection results with different feature
sets by SVM with RBF kernel. The numbers are multi-
plied by 100. Sentig represents global sentiment features, and
Sentig+l includes both global and local features. The number
in bold is statistically significantly higher than other numbers
in the same column (paired-t test, p &lt; 0.05), and the italic
entry has the highest absolute value.
</tableCaption>
<bodyText confidence="0.999909823529412">
outperform the two baselines, and among them,
SVM with the RBF kernel achieves the best F1
score and accuracy (0.78 and 0.80). Experimental
results with various combinations of features sets
are displayed in Table 5. As it can be seen, senti-
ment features obtains the best accuracy among the
four types of features. A combination of topic, dis-
cussion, and sentiment features achieves the best
performance on recall, F1, and accuracy. Specif-
ically, the accuracy is significantly higher than all
the other systems (paired-t test, p &lt; 0.05).
After a closer look at the results, we find two
main reasons for incorrect predictions. Firstly,
sentiment prediction errors get propagated into
dispute detection. Due to the limitation of ex-
isting general-purpose lexicons, some opinionated
dialog-specific terms are hard to catch. For exam-
ple, “I told you over and over again...” strongly
suggests a negative sentiment, but no single word
shows negative connotation. Constructing a lexi-
con tuned for conversational text may improve the
performance. Secondly, some dispute discussions
are harder to detect than the others due to differ-
ent dialog structures. For instance, the recalls for
dispute discussions of “controversy”, “RFC”, and
“resolved” are 0.78, 0.79, and 0.86 respectively.
We intend to design models that are able to cap-
ture dialog structures in the future work.
Sentiment Flow Visualization. We visualize the
sentiment flow of two disputed discussions in Fig-
ure 2. The plots reveal persistent negative sen-
timent in unresolved disputes (top). For the re-
solved dispute (bottom), participants show grati-
tude when the problem is settled.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999888476190477">
We present a sentiment analysis-based approach
to online dispute detection. We create a large-
scale dispute corpus from Wikipedia Talk pages to
study the problem. A sentiment prediction model
based on isotonic CRFs is proposed to output sen-
timent labels at the sentence-level. Experiments
on our dispute corpus also demonstrate that clas-
sifiers trained with sentiment tagging features out-
perform others that do not.
Acknowledgments We heartily thank the Cornell
NLP Group, the reviewers, and Yiye Ruan for
helpful comments. We also thank Emily Ben-
der and Mari Ostendorf for providing the AAWD
dataset. This work was supported in part by NSF
grants IIS-0968450 and IIS-1314778, and DARPA
DEFT Grant FA8750-13-2-0015. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or
the U.S. Government.
</bodyText>
<page confidence="0.997206">
697
</page>
<sectionHeader confidence="0.990097" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999602747747747">
Emily M. Bender, Jonathan T. Morgan, Meghan Ox-
ley, Mark Zachry, Brian Hutchinson, Alex Marin,
Bin Zhang, and Mari Ostendorf. 2011. Anno-
tating social acts: Authority claims and alignment
moves in wikipedia talk pages. In Proceedings of
the Workshop on Languages in Social Media, LSM
’11, pages 48–57, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Matt Billings and Leon Adam Watts. 2010. Under-
standing dispute resolution online: using text to re-
flect personal and substantive issues in conflict. In
Elizabeth D. Mynatt, Don Schoner, Geraldine Fitz-
patrick, Scott E. Hudson, W. Keith Edwards, and
Tom Rodden, editors, CHI, pages 1447–1456. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06), pages 417–422.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The conll-2010
shared task: Learning to detect hedges and their
scope in natural language text. In Proceedings of
the Fourteenth Conference on Computational Natu-
ral Language Learning —Shared Task, CoNLL ’10:
Shared Task, pages 1–12, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
use of Bayesian networks to model pragmatic de-
pendencies. In ACL ’04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, pages 669+, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438(7070):900–901.
Sangyun Hahn, Richard Ladner, and Mari Ostendorf.
2006. Agreement/disagreement classification: Ex-
ploiting unlabeled data using contrast classifiers.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 53–56, New York City, USA,
June. Association for Computational Linguistics.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What’s with the attitude?: Identify-
ing sentences with attitude in online discussions. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 1245–1255, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
put. Linguist., 19(3):501–530, September.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making Large-scale Support Vector
Machine Learning Practical, pages 169–184. MIT
Press, Cambridge, MA, USA.
Quentin Jones and Sheizaf Rafaeli. 2000. Time to
split, virtually: discourse architecture and commu-
nity building create vibrant virtual publics. Elec-
tronic Markets, 10:214–223.
Aniket Kittur and Robert E. Kraut. 2008. Harness-
ing the wisdom of crowds in wikipedia: Quality
through coordination. In Proceedings of the 2008
ACM Conference on Computer Supported Coopera-
tive Work, CSCW ’08, pages 37–46, New York, NY,
USA. ACM.
Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, and
Ed H. Chi. 2007. He says, she says: Conflict and
coordination in wikipedia. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, CHI ’07, pages 453–462, New York,
NY, USA. ACM.
Robert E. Kraut and Paul Resnick. 2012. Building suc-
cessful online communities: Evidence-based social
design. MIT Press, Cambridge, MA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional
random fields and local sentiment flow. In Advances
in Neural Information Processing Systems.
Alan Lee Eleni Miltsakaki Livio Robaldo Ar-
avind Joshi Rashmi Prasad, Nikhil Dinesh and Bon-
nie Webber. 2008. The penn discourse tree-
bank 2.0. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’08), Marrakech, Morocco, may. Eu-
ropean Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Warren Sack. 2005. Digital formations: It and new
architectures in the global realm. chapter Discourse
architecture and very large-scale conversation, pages
242–282. Princeton University Press, Princeton, NJ
USA.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press, Cambridge, MA.
Ba-Quy Vuong, Ee-Peng Lim, Aixin Sun, Minh-Tam
Le, Hady Wirawan Lauw, and Kuiyu Chang. 2008.
On ranking controversies in wikipedia: Models and
evaluation. In Proceedings of the 2008 Interna-
tional Conference on Web Search and Data Mining,
WSDM ’08, pages 171–182, New York, NY, USA.
ACM.
</reference>
<page confidence="0.980638">
698
</page>
<reference confidence="0.999329428571429">
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen
Richey, and Geoffrey Raymond. 2011. Detection of
agreement and disagreement in broadcast conversa-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ’11, pages 374–378, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Taha Yasseri, R´obert Sumi, Andr´as Rung, Andr´as Kor-
nai, and J´anos Kert´esz. 2012. Dynamics of conflicts
in wikipedia. CoRR, abs/1202.3643.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and
disagreement classification in online debates. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ’12, pages 61–69, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Tong Zhang and Frank J. Oles. 2001. Text categoriza-
tion based on regularized linear classification meth-
ods. Inf. Retr., 4(1):5–31, April.
</reference>
<page confidence="0.998848">
699
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741113">
<title confidence="0.9993145">A Piece of My Mind: A Sentiment Analysis for Online Dispute Detection</title>
<author confidence="0.996238">Lu</author>
<affiliation confidence="0.9630815">Department of Computer Cornell</affiliation>
<address confidence="0.875827">Ithaca, NY</address>
<email confidence="0.999676">luwang@cs.cornell.edu</email>
<abstract confidence="0.993835466666667">investigate the novel task of disdetection propose a sentiment analysis solution to the problem: we aim to identify the sequence of sentence-level sentiments expressed during a discussion and to use them as features in a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole. We evaluate dispute detection approaches on a newly created corpus of Wikipedia Talk page disputes and find that classifiers that rely on our sentiment tagging features outperform those that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Jonathan T Morgan</author>
<author>Meghan Oxley</author>
<author>Mark Zachry</author>
<author>Brian Hutchinson</author>
<author>Alex Marin</author>
<author>Bin Zhang</author>
<author>Mari Ostendorf</author>
</authors>
<title>Annotating social acts: Authority claims and alignment moves in wikipedia talk pages.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13262" citStr="Bender et al., 2011" startWordPosition="2070" endWordPosition="2073"> Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. We replace those words with their polarity equivalents. For example, relation “nsubj(wrong, you)” becomes “nsubj(SentiWordneg, you)”. 4 Online Dispute Detection 4.1 Training A Sentiment Classifier Dataset. We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011) on a 5- point scale (i.e. NN, N, O, P, PP). AAWD consists of 221 English Wikipedia discussions with positive and negative alignment annotations. Annotators either label each sentence as positive, negative or neutral, or label the full turn. For instances that have only a turn-level label, we assume all sentences have the same label as the turn. We further transform the labels into the five sentiment labels. Sentences annotated as being a positive alignment by at least two annotators are treated as very positive (PP). If a sentence is only selected as positive by one annotator or obtains the l</context>
</contexts>
<marker>Bender, Morgan, Oxley, Zachry, Hutchinson, Marin, Zhang, Ostendorf, 2011</marker>
<rawString>Emily M. Bender, Jonathan T. Morgan, Meghan Oxley, Mark Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and Mari Ostendorf. 2011. Annotating social acts: Authority claims and alignment moves in wikipedia talk pages. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 48–57, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Billings</author>
<author>Leon Adam Watts</author>
</authors>
<title>Understanding dispute resolution online: using text to reflect personal and substantive issues in conflict.</title>
<date>2010</date>
<pages>1447--1456</pages>
<editor>In Elizabeth D. Mynatt, Don Schoner, Geraldine Fitzpatrick, Scott E. Hudson, W. Keith Edwards, and Tom Rodden, editors, CHI,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="2874" citStr="Billings and Watts (2010)" startWordPosition="434" endWordPosition="437">sed primarily on cues derived from the edit history of the jointly created content (e.g. the number of revisions, their temporal density (Kittur et al., 2007; Yasseri et al., 2012)) and relied on small numbers of manually selected discussions known to involve disputes. In contrast, we investigate methods for the automatic detection, i.e. prediction, of discussions involving disputes. We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of online conflicts (e.g. Billings and Watts (2010), Kittur et al. (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. As a result, we propose a sentiment analysis approach for online dispute detection that identifies the sequence of sentence-level sentiments (i.e. very negative, negative, neutral, positive, very positive) expressed during the discussion and uses them as features in a classifier that predicts the DISPUTE/NONDISPUTE label for the discussion as a whole. Consider, for example, the</context>
</contexts>
<marker>Billings, Watts, 2010</marker>
<rawString>Matt Billings and Leon Adam Watts. 2010. Understanding dispute resolution online: using text to reflect personal and substantive issues in conflict. In Elizabeth D. Mynatt, Don Schoner, Geraldine Fitzpatrick, Scott E. Hudson, W. Keith Edwards, and Tom Rodden, editors, CHI, pages 1447–1456. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06),</booktitle>
<pages>417--422</pages>
<contexts>
<context position="11991" citStr="Esuli and Sebastiani, 2006" startWordPosition="1879" endWordPosition="1882"> via a lexicon). Concretely, we take a lexicon M = MpUMn, where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume encodes the weight between label Q and feature w, for each feature w E Mp; then the isotonic CRF enforces Q &lt; Q0 ==&gt;. µhσ,wi &lt; µhσ,,wi. For example, when “totally agree” is observed in training, parameter µhPP,totally agreei is likely to increase. Similar constraints are defined on Mn. Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as disTable 2: Features used in sentence-level sentiment prediction. Numerical features are first normalized by standardization, then binned into 5 categor</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06), pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The conll-2010 shared task: Learning to detect hedges and their scope in natural language text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning —Shared Task, CoNLL ’10: Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The conll-2010 shared task: Learning to detect hedges and their scope in natural language text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning —Shared Task, CoNLL ’10: Shared Task, pages 1–12, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Julia Hirschberg</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>669</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7261" citStr="Galley et al., 2004" startWordPosition="1130" endWordPosition="1133">ional Related Work. Sentiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken 2A notable exception is Hassan et al. (2010), which identifies sentences containing “attitudes” (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered. 3The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content. meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the ar</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Michel Galley, Kathleen McKeown, Julia Hirschberg, and Elizabeth Shriberg. 2004. Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 669+, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Giles</author>
</authors>
<title>Internet encyclopaedias go head to head.</title>
<date>2005</date>
<journal>Nature,</journal>
<volume>438</volume>
<issue>7070</issue>
<contexts>
<context position="1271" citStr="Giles, 2005" startWordPosition="199" endWordPosition="200">ent tagging features outperform those that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80. 1 Introduction As the web has grown in popularity and scope, so has the promise of collaborative information environments for the joint creation and exchange of knowledge (Jones and Rafaeli, 2000; Sack, 2005). Wikipedia, a wiki-based online encyclopedia, is arguably the best example: its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001. Existing studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g. an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise (Kittur et al., 2007). Unfortunately, human monitoring of the often massive social media and collaboration sites to detect, much less mediate, disputes is not feasible. 1http://en.wiki</context>
</contexts>
<marker>Giles, 2005</marker>
<rawString>Jim Giles. 2005. Internet encyclopaedias go head to head. Nature, 438(7070):900–901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangyun Hahn</author>
<author>Richard Ladner</author>
<author>Mari Ostendorf</author>
</authors>
<title>Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>53--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="5652" citStr="Hahn et al., 2006" startWordPosition="881" endWordPosition="884">a”. Omitted sentences are indicated by ellipsis. Names of editors are in bold. The start of each set of related turns is numbered; “&gt;” is an indicator for the reply structure. presumably be tagged as a negative sentence as should the sarcastic sentences “Sounds good?” (in the same turn) and “congrats” and “thank you” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 dis</context>
<context position="7281" citStr="Hahn et al., 2006" startWordPosition="1134" endWordPosition="1137">entiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken 2A notable exception is Hassan et al. (2010), which identifies sentences containing “attitudes” (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered. 3The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content. meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the article to flag it for</context>
</contexts>
<marker>Hahn, Ladner, Ostendorf, 2006</marker>
<rawString>Sangyun Hahn, Richard Ladner, and Mari Ostendorf. 2006. Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 53–56, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
</authors>
<title>What’s with the attitude?: Identifying sentences with attitude in online discussions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1245--1255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6872" citStr="Hassan et al. (2010)" startWordPosition="1073" endWordPosition="1076">putes, 3609 nondisputes).3 We find that classifiers that employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80 on the Wikipedia dispute corpus. To the best of our knowledge, this represents the first computational approach to automatically identify online disputes on a dataset of scale. Additional Related Work. Sentiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken 2A notable exception is Hassan et al. (2010), which identifies sentences containing “attitudes” (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered. 3The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content. meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made fo</context>
<context position="14503" citStr="Hassan et al., 2010" startWordPosition="2285" endWordPosition="2288">nnotation, it is positive (P). Very negative (NN) and negative (N) are collected in the same way. All others are neutral (O). Among all 16,501 sentences in AAWD, 1,930 and 1,102 are labeled as NN and N. 532 and 99 of them are PP and P. The other 12,648 are considered neutral. Evaluation. To evaluate the performance of the sentiment tagger, we compare to two baselines. (1) Baseline (Polarity): a sentence is predicted as positive if it has more positive words than negative words, or negative if more negative words are observed. Otherwise, it is neutral. (2) Baseline (Distance) is extended from (Hassan et al., 2010). Each sentiment word is associated with the closest 695 Pos Neg Neutral Baseline (Polarity) 22.53 38.61 66.45 Baseline (Distance) 33.75 55.79 88.97 SVM (3-way) 44.62 52.56 80.84 CRF (3-way) 56.28 56.37 89.41 CRF (5-way) 58.39 56.30 90.10 isotonic CRF 68.18 62.53 88.87 Table 3: F1 scores for positive and negative alignment on Wikipedia Talk pages (AAWD) using 5-fold cross-validation. In each column, bold entries (if any) are statistically significantly higher than all the rest. We also compare with an SVM and linear CRF trained with three classes (3-way). Our model based on the isotonic CRF pr</context>
</contexts>
<marker>Hassan, Qazvinian, Radev, 2010</marker>
<rawString>Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev. 2010. What’s with the attitude?: Identifying sentences with attitude in online discussions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1245–1255, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="12641" citStr="Hirschberg and Litman, 1993" startWordPosition="1978" endWordPosition="1981">contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as disTable 2: Features used in sentence-level sentiment prediction. Numerical features are first normalized by standardization, then binned into 5 categories. course features (Hirschberg and Litman, 1993). Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. We replace those words with their polarity equivalents. For example, relation “nsubj(wrong, you)” becomes “nsubj(SentiWordneg, you)”. 4 Online Dispute Detection 4.1 Training A Sentiment Classifier Dataset. We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus </context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Comput. Linguist., 19(3):501–530, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Advances in kernel methods. chapter Making Large-scale Support Vector Machine Learning Practical,</title>
<date>1999</date>
<pages>169--184</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="15260" citStr="Joachims, 1999" startWordPosition="2407" endWordPosition="2408"> 88.97 SVM (3-way) 44.62 52.56 80.84 CRF (3-way) 56.28 56.37 89.41 CRF (5-way) 58.39 56.30 90.10 isotonic CRF 68.18 62.53 88.87 Table 3: F1 scores for positive and negative alignment on Wikipedia Talk pages (AAWD) using 5-fold cross-validation. In each column, bold entries (if any) are statistically significantly higher than all the rest. We also compare with an SVM and linear CRF trained with three classes (3-way). Our model based on the isotonic CRF produces significantly better results than all the other systems. second person pronoun, and a surface distance is computed. An SVM classifier (Joachims, 1999) is trained using features of the sentiment words and minimum/maximum/average of the distances. We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates; (2) a Linear CRF for (dis)agreement identification in broadcast conversations (Wang et al., 2011). We evaluate the systems using standard F1 on classes of positive, negative, and neutral, where samples predicted as PP and </context>
<context position="19086" citStr="Joachims, 1999" startWordPosition="3007" endWordPosition="3008">) 50.00 100.00 66.67 50.00 Logistic Regression 74.76 72.29 73.50 73.94 SVMLinear 69.81 71.90 70.84 70.41 SVMRBF 77.38 79.14 78.25 80.00 Table 4: Dispute detection results on Wikipedia Talk pages. The numbers are multiplied by 100. The items in bold are statistically significantly higher than others in the same column (paired-t test, p &lt; 0.05). SVM with the RBF kernel achieves the best performance in precision, F1, and accuracy. Results and Error Analysis. We experiment with logistic regression, SVM with linear and RBF kernels, which are effective methods in multiple text categorization tasks (Joachims, 1999; Zhang and J. Oles, 2001). We normalize the features by standardization and conduct a 5-fold cross-validation. Two baselines are listed: (1) labels are randomly assigned; (2) all discussions have disputes. Main results for different classifiers are displayed in Table 4. All learning based methods 696 Sentiment Sentiment 1 0 1 2 Sentiment Flow in Discussion with Unresolved Dispute Sample sentences (sentiment in parentheses) A: no, I sincerely plead with you... (N) If not, you are just wasting my time. (NN) B: I believe Sweet’s proposal... is quite silly. (NN) C: Tell you what. (NN) If you can </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Advances in kernel methods. chapter Making Large-scale Support Vector Machine Learning Practical, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quentin Jones</author>
<author>Sheizaf Rafaeli</author>
</authors>
<title>Time to split, virtually: discourse architecture and community building create vibrant virtual publics. Electronic Markets,</title>
<date>2000</date>
<pages>10--214</pages>
<contexts>
<context position="988" citStr="Jones and Rafaeli, 2000" startWordPosition="157" endWordPosition="160">ressed during a discussion and to use them as features in a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole. We evaluate dispute detection approaches on a newly created corpus of Wikipedia Talk page disputes and find that classifiers that rely on our sentiment tagging features outperform those that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80. 1 Introduction As the web has grown in popularity and scope, so has the promise of collaborative information environments for the joint creation and exchange of knowledge (Jones and Rafaeli, 2000; Sack, 2005). Wikipedia, a wiki-based online encyclopedia, is arguably the best example: its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001. Existing studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g. an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruit</context>
</contexts>
<marker>Jones, Rafaeli, 2000</marker>
<rawString>Quentin Jones and Sheizaf Rafaeli. 2000. Time to split, virtually: discourse architecture and community building create vibrant virtual publics. Electronic Markets, 10:214–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Robert E Kraut</author>
</authors>
<title>Harnessing the wisdom of crowds in wikipedia: Quality through coordination.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work, CSCW ’08,</booktitle>
<pages>37--46</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1555" citStr="Kittur and Kraut, 2008" startWordPosition="241" endWordPosition="244">tion and exchange of knowledge (Jones and Rafaeli, 2000; Sack, 2005). Wikipedia, a wiki-based online encyclopedia, is arguably the best example: its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001. Existing studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g. an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise (Kittur et al., 2007). Unfortunately, human monitoring of the often massive social media and collaboration sites to detect, much less mediate, disputes is not feasible. 1http://en.wikipedia.org Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu In this work, we investigate the heretofore novel task of dispute detection in online discussions. Previous work in this general area has analyzed dispute-laden content to</context>
</contexts>
<marker>Kittur, Kraut, 2008</marker>
<rawString>Aniket Kittur and Robert E. Kraut. 2008. Harnessing the wisdom of crowds in wikipedia: Quality through coordination. In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work, CSCW ’08, pages 37–46, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Bongwon Suh</author>
<author>Bryan A Pendleton</author>
<author>Ed H Chi</author>
</authors>
<title>He says, she says: Conflict and coordination in wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’07,</booktitle>
<pages>453--462</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1708" citStr="Kittur et al., 2007" startWordPosition="264" endWordPosition="267">buted editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001. Existing studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g. an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise (Kittur et al., 2007). Unfortunately, human monitoring of the often massive social media and collaboration sites to detect, much less mediate, disputes is not feasible. 1http://en.wikipedia.org Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu In this work, we investigate the heretofore novel task of dispute detection in online discussions. Previous work in this general area has analyzed dispute-laden content to discover features correlated with conflicts and disputes (Kittur et al., 2007). Research focused primarily on cues derived from the edit history of the </context>
</contexts>
<marker>Kittur, Suh, Pendleton, Chi, 2007</marker>
<rawString>Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, and Ed H. Chi. 2007. He says, she says: Conflict and coordination in wikipedia. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’07, pages 453–462, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Kraut</author>
<author>Paul Resnick</author>
</authors>
<title>Building successful online communities: Evidence-based social design.</title>
<date>2012</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1581" citStr="Kraut and Resnick, 2012" startWordPosition="245" endWordPosition="248">wledge (Jones and Rafaeli, 2000; Sack, 2005). Wikipedia, a wiki-based online encyclopedia, is arguably the best example: its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001. Existing studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g. an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise (Kittur et al., 2007). Unfortunately, human monitoring of the often massive social media and collaboration sites to detect, much less mediate, disputes is not feasible. 1http://en.wikipedia.org Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu In this work, we investigate the heretofore novel task of dispute detection in online discussions. Previous work in this general area has analyzed dispute-laden content to discover features correla</context>
<context position="2922" citStr="Kraut and Resnick (2012)" startWordPosition="442" endWordPosition="445">ry of the jointly created content (e.g. the number of revisions, their temporal density (Kittur et al., 2007; Yasseri et al., 2012)) and relied on small numbers of manually selected discussions known to involve disputes. In contrast, we investigate methods for the automatic detection, i.e. prediction, of discussions involving disputes. We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of online conflicts (e.g. Billings and Watts (2010), Kittur et al. (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. As a result, we propose a sentiment analysis approach for online dispute detection that identifies the sequence of sentence-level sentiments (i.e. very negative, negative, neutral, positive, very positive) expressed during the discussion and uses them as features in a classifier that predicts the DISPUTE/NONDISPUTE label for the discussion as a whole. Consider, for example, the snippet in Figure 1 from the Wikipedia Talk pag</context>
</contexts>
<marker>Kraut, Resnick, 2012</marker>
<rawString>Robert E. Kraut and Paul Resnick. 2012. Building successful online communities: Evidence-based social design. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5973" citStr="Lafferty et al., 2001" startWordPosition="931" endWordPosition="934">u” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 disputes, 3609 nondisputes).3 We find that classifiers that employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80 on the Wikipedia dispute corpus. To the best of our knowledge, this represents the first computational approach t</context>
<context position="7396" citStr="Lafferty et al., 2001" startWordPosition="1151" endWordPosition="1154"> Previous work mainly studies the attitudes in spoken 2A notable exception is Hassan et al. (2010), which identifies sentences containing “attitudes” (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered. 3The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content. meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the article to flag it for attention. In this research, we are interested in talk pages whose corresponding articles are labeled with the fol</context>
<context position="11067" citStr="Lafferty et al., 2001" startWordPosition="1729" endWordPosition="1732">tion describes our sentence-level sentiment tagger, from which we construct features for dispute detection (Section 4). Consider a discussion comprised of sequential turns; each turn consists of a sequence of sentences. Our model takes as input the sentences x = {x1, · · · , xn} from a single turn, and outputs the corresponding sequence of sentiment labels y = {y1, · · · , yn}, where yi E O, O = {NN, N, O, P, PP}. The labels in O represent very negative (NN), negative (N), neutral (O), positive (P), and very positive (PP), respectively. Given that traditional Conditional Random Fields (CRFs) (Lafferty et al., 2001) ignore the ordinal relations among sentiment labels, we choose isotonic CRFs (Mao and Lebanon, 2007) for sentence-level sentiment analysis as they can enforce monotonicity constraints on the parameters consistent with the ordinal structure and domain knowledge (e.g. word-level sentiment conveyed via a lexicon). Concretely, we take a lexicon M = MpUMn, where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume encodes the weight between label Q and feature w, for each feature w E Mp; then the isotonic CRF enforces Q &lt;</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Mao</author>
<author>Guy Lebanon</author>
</authors>
<title>Isotonic conditional random fields and local sentiment flow.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5835" citStr="Mao and Lebanon, 2007" startWordPosition="909" endWordPosition="912">esumably be tagged as a negative sentence as should the sarcastic sentences “Sounds good?” (in the same turn) and “congrats” and “thank you” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 disputes, 3609 nondisputes).3 We find that classifiers that employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78</context>
<context position="11168" citStr="Mao and Lebanon, 2007" startWordPosition="1745" endWordPosition="1748">ction (Section 4). Consider a discussion comprised of sequential turns; each turn consists of a sequence of sentences. Our model takes as input the sentences x = {x1, · · · , xn} from a single turn, and outputs the corresponding sequence of sentiment labels y = {y1, · · · , yn}, where yi E O, O = {NN, N, O, P, PP}. The labels in O represent very negative (NN), negative (N), neutral (O), positive (P), and very positive (PP), respectively. Given that traditional Conditional Random Fields (CRFs) (Lafferty et al., 2001) ignore the ordinal relations among sentiment labels, we choose isotonic CRFs (Mao and Lebanon, 2007) for sentence-level sentiment analysis as they can enforce monotonicity constraints on the parameters consistent with the ordinal structure and domain knowledge (e.g. word-level sentiment conveyed via a lexicon). Concretely, we take a lexicon M = MpUMn, where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume encodes the weight between label Q and feature w, for each feature w E Mp; then the isotonic CRF enforces Q &lt; Q0 ==&gt;. µhσ,wi &lt; µhσ,,wi. For example, when “totally agree” is observed in training, parameter µhPP,</context>
</contexts>
<marker>Mao, Lebanon, 2007</marker>
<rawString>Yi Mao and Guy Lebanon. 2007. Isotonic conditional random fields and local sentiment flow. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Lee</author>
</authors>
<title>Eleni Miltsakaki Livio Robaldo Aravind Joshi Rashmi Prasad, Nikhil Dinesh</title>
<date>2008</date>
<journal>European Language Resources Association</journal>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<marker>Lee, 2008</marker>
<rawString>Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi Rashmi Prasad, Nikhil Dinesh and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren Sack</author>
</authors>
<title>Digital formations: It and new architectures in the global realm. chapter Discourse architecture and very large-scale conversation,</title>
<date>2005</date>
<pages>242--282</pages>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ USA.</location>
<contexts>
<context position="1001" citStr="Sack, 2005" startWordPosition="161" endWordPosition="162">n and to use them as features in a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole. We evaluate dispute detection approaches on a newly created corpus of Wikipedia Talk page disputes and find that classifiers that rely on our sentiment tagging features outperform those that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80. 1 Introduction As the web has grown in popularity and scope, so has the promise of collaborative information environments for the joint creation and exchange of knowledge (Jones and Rafaeli, 2000; Sack, 2005). Wikipedia, a wiki-based online encyclopedia, is arguably the best example: its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001. Existing studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g. an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruitful collabora</context>
</contexts>
<marker>Sack, 2005</marker>
<rawString>Warren Sack. 2005. Digital formations: It and new architectures in the global realm. chapter Discourse architecture and very large-scale conversation, pages 242–282. Princeton University Press, Princeton, NJ USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11944" citStr="Stone et al., 1966" startWordPosition="1873" endWordPosition="1876">dge (e.g. word-level sentiment conveyed via a lexicon). Concretely, we take a lexicon M = MpUMn, where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume encodes the weight between label Q and feature w, for each feature w E Mp; then the isotonic CRF enforces Q &lt; Q0 ==&gt;. µhσ,wi &lt; µhσ,,wi. For example, when “totally agree” is observed in training, parameter µhPP,totally agreei is likely to increase. Similar constraints are defined on Mn. Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as disTable 2: Features used in sentence-level sentiment prediction. Numerical features are first normalized</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ba-Quy Vuong</author>
<author>Ee-Peng Lim</author>
<author>Aixin Sun</author>
<author>Minh-Tam Le</author>
<author>Hady Wirawan Lauw</author>
<author>Kuiyu Chang</author>
</authors>
<title>On ranking controversies in wikipedia: Models and evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08,</booktitle>
<pages>171--182</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="17187" citStr="Vuong et al., 2008" startWordPosition="2709" endWordPosition="2712">talk page. We further utilize unigrams and bigrams of the category as topic features. Discussion Features. This type of feature aims to capture the structure of the discussion. Intuitively, the more turns or the more participants a discussion has, the more likely there is a dispute. Meanwhile, participants tend to produce longer utterances when they make arguments. We choose number of turns, number of participants, average number of words in each turn as features. In addition, the frequency of revisions made during the discussion has been shown to be good indicator for controversial articles (Vuong et al., 2008), that are presumably prone to have disputes. Therefore, we encode the number of revisions that happened during the discussion as a feature. Sentiment Features. This set of features encode the sentiment distribution and transition in the discussion. We train our sentiment tagging model on the full AAWD dataset, and run it on the Wikipedia dispute corpus. Given that consistent negative sentiment flow usually indicates an ongoing dispute, we first extract features from sentiment distribution in the form of number/probability of sentiment per type. We also estimate the sentiment transition probab</context>
</contexts>
<marker>Vuong, Lim, Sun, Le, Lauw, Chang, 2008</marker>
<rawString>Ba-Quy Vuong, Ee-Peng Lim, Aixin Sun, Minh-Tam Le, Hady Wirawan Lauw, and Kuiyu Chang. 2008. On ranking controversies in wikipedia: Models and evaluation. In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08, pages 171–182, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Sibel Yaman</author>
<author>Kristin Precoda</author>
<author>Colleen Richey</author>
<author>Geoffrey Raymond</author>
</authors>
<title>Detection of agreement and disagreement in broadcast conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>374--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7328" citStr="Wang et al., 2011" startWordPosition="1141" endWordPosition="1144">abling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken 2A notable exception is Hassan et al. (2010), which identifies sentences containing “attitudes” (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered. 3The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content. meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the article to flag it for attention. In this research, we are interested</context>
<context position="15735" citStr="Wang et al., 2011" startWordPosition="2474" endWordPosition="2477">cantly better results than all the other systems. second person pronoun, and a surface distance is computed. An SVM classifier (Joachims, 1999) is trained using features of the sentiment words and minimum/maximum/average of the distances. We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates; (2) a Linear CRF for (dis)agreement identification in broadcast conversations (Wang et al., 2011). We evaluate the systems using standard F1 on classes of positive, negative, and neutral, where samples predicted as PP and P are positive alignment, and samples tagged as NN and N are negative alignment. Table 3 describes the main results on the AAWD dataset: our isotonic CRF based system significantly outperforms the alternatives for positive and negative alignment detection (paired-t test, p &lt; 0.05). 4.2 Dispute Detection We model dispute detection as a standard binary classification task, and investigate four major types of features as described below. Lexical Features. We first collect u</context>
</contexts>
<marker>Wang, Yaman, Precoda, Richey, Raymond, 2011</marker>
<rawString>Wen Wang, Sibel Yaman, Kristin Precoda, Colleen Richey, and Geoffrey Raymond. 2011. Detection of agreement and disagreement in broadcast conversations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 374–378, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11905" citStr="Wilson et al., 2005" startWordPosition="1866" endWordPosition="1870"> the ordinal structure and domain knowledge (e.g. word-level sentiment conveyed via a lexicon). Concretely, we take a lexicon M = MpUMn, where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume encodes the weight between label Q and feature w, for each feature w E Mp; then the isotonic CRF enforces Q &lt; Q0 ==&gt;. µhσ,wi &lt; µhσ,,wi. For example, when “totally agree” is observed in training, parameter µhPP,totally agreei is likely to increase. Similar constraints are defined on Mn. Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as disTable 2: Features used in sentence-level sentiment prediction. </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taha Yasseri</author>
<author>R´obert Sumi</author>
<author>Andr´as Rung</author>
<author>Andr´as Kornai</author>
<author>J´anos Kert´esz</author>
</authors>
<title>Dynamics of conflicts in wikipedia.</title>
<date>2012</date>
<journal>CoRR,</journal>
<pages>1202--3643</pages>
<marker>Yasseri, Sumi, Rung, Kornai, Kert´esz, 2012</marker>
<rawString>Taha Yasseri, R´obert Sumi, Andr´as Rung, Andr´as Kornai, and J´anos Kert´esz. 2012. Dynamics of conflicts in wikipedia. CoRR, abs/1202.3643.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Yin</author>
<author>Paul Thomas</author>
<author>Nalin Narang</author>
<author>Cecile Paris</author>
</authors>
<title>Unifying local and global agreement and disagreement classification in online debates.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12,</booktitle>
<pages>61--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5671" citStr="Yin et al., 2012" startWordPosition="885" endWordPosition="888">es are indicated by ellipsis. Names of editors are in bold. The start of each set of related turns is numbered; “&gt;” is an indicator for the reply structure. presumably be tagged as a negative sentence as should the sarcastic sentences “Sounds good?” (in the same turn) and “congrats” and “thank you” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 disputes, 3609 nondisp</context>
<context position="15618" citStr="Yin et al., 2012" startWordPosition="2457" endWordPosition="2460"> with an SVM and linear CRF trained with three classes (3-way). Our model based on the isotonic CRF produces significantly better results than all the other systems. second person pronoun, and a surface distance is computed. An SVM classifier (Joachims, 1999) is trained using features of the sentiment words and minimum/maximum/average of the distances. We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates; (2) a Linear CRF for (dis)agreement identification in broadcast conversations (Wang et al., 2011). We evaluate the systems using standard F1 on classes of positive, negative, and neutral, where samples predicted as PP and P are positive alignment, and samples tagged as NN and N are negative alignment. Table 3 describes the main results on the AAWD dataset: our isotonic CRF based system significantly outperforms the alternatives for positive and negative alignment detection (paired-t test, p &lt; 0.05). 4.2 Dispute Detection We model dispute detection as a standard binary class</context>
</contexts>
<marker>Yin, Thomas, Narang, Paris, 2012</marker>
<rawString>Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris. 2012. Unifying local and global agreement and disagreement classification in online debates. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12, pages 61–69, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Frank J Oles</author>
</authors>
<title>Text categorization based on regularized linear classification methods.</title>
<date>2001</date>
<journal>Inf. Retr.,</journal>
<volume>4</volume>
<issue>1</issue>
<marker>Zhang, Oles, 2001</marker>
<rawString>Tong Zhang and Frank J. Oles. 2001. Text categorization based on regularized linear classification methods. Inf. Retr., 4(1):5–31, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>