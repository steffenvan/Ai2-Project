<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014347">
<title confidence="0.982252">
Mining of Parsed Data to Derive Deverbal Argument Structure
</title>
<author confidence="0.887023">
Olga Gurevich Scott A. Waterman
</author>
<affiliation confidence="0.834447">
Microsoft / Powerset
</affiliation>
<address confidence="0.970654">
475 Brannan Street, Ste. 330
San Francisco, CA 94107
</address>
<email confidence="0.999758">
{olya.gurevich,scott.waterman}@microsoft.com
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933545454545">
The availability of large parsed corpora
and improved computing resources now
make it possible to extract vast amounts
of lexical data. We describe the pro-
cess of extracting structured data and sev-
eral methods of deriving argument struc-
ture mappings for deverbal nouns that sig-
nificantly improves upon non-lexicalized
rule-based methods. For a typical model,
the F-measure of performance improves
from a baseline of about 0.72 to 0.81.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961214285714">
There is a long-standing division in natural lan-
guage processing between symbolic, rule-based
approaches and data-driven, statistical ones. Rule-
based, human-curated approaches are thought to
be more accurate for linguistic constructions ex-
plicitly covered by the rules. However, such
approaches often have trouble scaling up to a
wider range of phenomena or different genres of
text. There have been repeated moves towards hy-
bridized approaches, in which rules created with
human linguistic intuitions are supplemented by
automatically derived corpus data (cf. (Klavans
and Resnik, 1996)).
Unstructured corpus data for English can eas-
ily be found on the Internet. Large corpora of
text annotated with part of speech information are
also available (such as the British National Cor-
pus). However, it is much harder to find widely
available, large corpora annotated for syntactic or
semantic structure. The Penn Treebank (Marcus
et al., 1993) has until recently been the only such
corpus, covering 4.5M words in a single genre of
financial reporting. At the same time, the accuracy
and speed of syntactic parsers has been improving
greatly, so that in recent years it has become possi-
ble to automatically create parsed corpora of rea-
sonable quality, using much larger amounts of text
with greater genre variation. For many NLP tasks,
having more training data greatly improves the
quality of the resulting models (Banko and Brill,
2001), even if the training data are not perfect.
We have access to the entire English-language
text of Wikipedia (about 2M pages) that was
parsed using the XLE parser (Riezler et al., 2002),
as well as an architecture for distributed data-
mining within this corpus, called Oceanography
(Waterman, 2009). Using the parsed corpus, we
extract a large volume of dependency relations and
derive lexical models that significantly improve
a rule-based system for determining the underly-
ing argument structure of deverbal noun construc-
tions.
</bodyText>
<sectionHeader confidence="0.995881" genericHeader="introduction">
2 Deverbal Argument Mapping
</sectionHeader>
<bodyText confidence="0.999772875">
Deverbal nouns, or nominalizations, are nouns
that designate some aspect of the event referred
to by the verb from which they are morphologi-
cally derived (Quirk et al., 1985). For example,
the noun destruction refers to the action described
by the verb destroy, and destroyer may refer to the
agent of that event. Deverbal nouns are very com-
mon in English texts: by one count, about half of
all sentences in written text contain at least one
deverbal noun (Gurevich et al., 2008). Thus, a
computational system that aims to match multi-
ple ways of expressing the same underlying events
(such as question answering or search) must be
able to deal with deverbal nouns.
To interpret deverbal constructions, one must be
able to map nominal and prepositional modifiers to
the various roles in the verbal frame. For intran-
sitive verbs, almost any argument of the deverbal
noun is mapped to the verb’s subject, e.g. abun-
dance of food gives rise to subj(abound, food).
If the underlying verb is transitive, and the de-
verbal noun has two arguments, the mappings
are also fairly straightforward. For example, the
phrase Carthage’s defeat by Rome gives rise to
</bodyText>
<page confidence="0.992381">
19
</page>
<note confidence="0.9989885">
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 19–27,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999840902439025">
the arguments subj(defeat, Rome) and obj(defeat,
Carthage), based on knowledge that a “by” argu-
ment usually maps to the subject, and the posses-
sive in the presence of a “by” argument usually
maps to the object (Nunes, 1993).
However, in many cases a deverbal noun has
only one argument, even though the underlying
verb may be transitive. In such cases, our system
has to decide whether to map the lone argument
of the deverbal onto the subject or object of the
verb. This mapping is in many cases obvious to a
human: e.g., the king’s abdication corresponds to
subj(abdicate, king), whereas the room’s adorn-
ment corresponds to obj(adorn, room). In some
cases, the mapping is truly ambiguous, e.g., They
enjoyed the support of the Queen vs. They jumped
to the support of the Queen. Yet in other cases, the
lone argument of the deverbal noun is neither the
subject nor the object of the underlying verb, but it
may correspond to a different (e.g. prepositional)
argument of the verb, as in the travels of 1996 (cor-
responding to someone traveled in 1996). Finally,
in some cases the deverbal noun is being used in
a truly nominal sense, without an underlying map-
ping to a verb, as in Bill Gates’ foundation, and
the possessive is not a verbal argument.
The predictive models in this paper focus on this
case of single arguments of deverbal nouns with
transitive underlying verbs. To constrain the scope
of the task, we focus on possessive arguments, like
the room’s adornment, and ‘of’ arguments, like
the support of the Queen. Our goal is to improve
the accuracy of verbal roles assigned in such cases
by creating lexically-specific preferences for indi-
vidual deverbal noun / verb pairs. Some of our
experiments also take into account some lexical
properties of the deverbal noun’s arguments. The
lexical preferences are derived by comparing ar-
gument preferences of verbs with those of related
deverbal nouns, derived from a large parsed cor-
pus using Oceanography.
</bodyText>
<subsectionHeader confidence="0.996885">
2.1 Current Deverbal Mapping System
</subsectionHeader>
<bodyText confidence="0.999939545454545">
We have a list of approximately 4000 deverbal
noun / verb pairs, constructed from a combina-
tion of WordNet’s derivational links (Fellbaum,
1998), NomLex (Macleod et al., 1998), NomL-
exPlus (Meyers et al., 2004b) and some indepen-
dent curation. In the current system implementa-
tion, we attempt to map deverbal nouns onto cor-
responding verbs using a small set of heuristics
described in (Gurevich et al., 2008). We distin-
guish between event nouns like destruction, agen-
tive nouns like destroyer, and patient-like nouns
like employee.
If a deverbal noun maps onto a transitive verb
and has only one argument, the heuristics are as
follows. Arguments of agentive nouns become ob-
jects while the nouns themselves become subjects,
so the ship’s destroyer maps to subj(destroy, de-
stroyer); obj(destroy, ship). Arguments of patient-
like nouns become subjects while the nouns them-
selves become objects, so the company’s employee
becomes subj(employ, company); obj(employ, em-
ployee).
The difficult case of event nouns is currently
handled through default mappings: possessive ar-
guments become subjects (e.g., his confession H
subj(confess, he)), and ‘of’ arguments become ob-
jects (e.g., confession of sin H obj(confess, sin)).
However, as we have seen from examples above,
these defaults are not always correct. The correct
mapping depends on the lexical nature of the de-
verbal noun and its corresponding verb, and pos-
sibly on properties of the possessive or ‘of’ argu-
ment as well.
</bodyText>
<subsectionHeader confidence="0.999528">
2.2 System Background
</subsectionHeader>
<bodyText confidence="0.999969136363636">
The deverbal argument mapping occurs in the con-
text of a larger semantic search application, where
the goal is to match alternate forms expressing
similar concepts. We are currently processing
the entire text of the English-language Wikipedia,
consisting of about 2M unique pages.
Parsing in this system is done using the XLE
parser (Kaplan and Maxwell, 1995) and a broad-
coverage grammar of English (Riezler et al., 2002;
Crouch et al., 2009), which produces constituent
structures and functional structures in accordance
with the theory of Lexical-Functional Grammar
(Dalrymple, 2001).
Parsing is followed by a semantic processing
phase, producing a more abstract argument struc-
ture. Semantic representations are created using
the Transfer system of successive rewrite rules
(Crouch and King, 2006). Numerous construc-
tions are normalized and rewritten (e.g., passives,
relative clauses, etc.) to maximize matching be-
tween alternate surface forms. This is the step in
which deverbal argument mapping occurs.
</bodyText>
<page confidence="0.987604">
20
</page>
<subsectionHeader confidence="0.997395">
2.3 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.9968015">
To evaluate the performance of the current and
experimental argument mappings, we extracted a
random set of 1000 sentences from the parsed
Wikipedia corpus in which a deverbal noun had
a single possessive argument. Each sentence was
manually annotated with the verb role mapping
between the deverbal and the possessive argu-
ments. One of six labels were assigned:
</bodyText>
<listItem confidence="0.987807333333333">
• Subject, e.g. John’s attention
• Object, e.g. arrangement offlowers
• Other: there is an underlying verb, but the
relationship between the verb and the argu-
ment is neither subject nor object; these rela-
tions often appear as prepositional arguments
in the verbal form, e.g. Declaration of Delhi
• Noun modifier: the argument modifies the
nominal sense of the deverbal noun, rather
than the underlying verb, although there is
still an underlying event, as in director of 25
years
• Not deverbal: the deverbal noun is not used
to designate an event in this context, e.g. the
rest of them
• Error: the parser incorrectly identified the ar-
gument as modifying the deverbal, or as be-
ing the only argument of the deverbal
</listItem>
<bodyText confidence="0.998772555555556">
Similarly, we extracted a sample of 750 sentences
in which a deverbal noun had a single ‘of’ argu-
ment, and annotated those manually.
The distribution of annotations is summarized
in Table 1. For possessive arguments, the preva-
lent role was subject, and for ‘of’ arguments it was
object.
The defaults will correctly assign the majority
of arguments roles.
</bodyText>
<table confidence="0.877637666666667">
Possessive ‘Of’
total 1000 750
unique deverbals 423 338
subj 511 (51%) 158 (21%)
obj 335 (34%) 411 (55%)
other 28 (3%) 50 (7%)
noun mod 23 (2%) 18 (2%)
not deverbal 21 (2%) 40 (5%)
error 82 (8%) 73 (10%)
</table>
<tableCaption confidence="0.991167">
Table 1: Evaluation Role Judgements, with de-
faults in bold
</tableCaption>
<subsectionHeader confidence="0.998414">
2.4 Lexicalizing Role Mappings
</subsectionHeader>
<bodyText confidence="0.999988909090909">
Our basic premise is that knowledge about role-
mapping behavior of particular verbs will inform
the role-mapping behavior of their corresponding
deverbal nouns. For example, if a particular argu-
ment of a given verb surfaces as the verb’s sub-
ject more often than as object, we might also pre-
fer the subject role when the same argument oc-
curs as a modifier of the corresponding deverbal
noun. However, as nominal modification con-
structions impose their own role-mapping pref-
erences (e.g., possessives are more likely to be
subjects than objects), we expect different dis-
tributions of arguments to appear in the various
deverbal modification patterns. Making use of
this intuition requires collecting sufficient infor-
mation about corresponding arguments of verbs
and deverbal nouns. This is available, given a
large parsed corpus, a reasonably accurate and fast
parser, and enough computing capacity. The re-
mainder of the paper details our data extraction,
model-building methods, and the results of some
experiments.
</bodyText>
<sectionHeader confidence="0.991498" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.999886172413793">
Oceanography is a pattern extraction and statistics
language for analyzing structural relationships in
corpora parsed using XLE (Waterman, 2009). It
simplifies the task of programming for NL analy-
sis over large corpora, and the sorting, counting,
and distributional analysis that often characterizes
statistical NLP. This corpus processing language is
accompanied by a distributed runtime, which uses
cluster computing to match patterns and collect
statistics simultaneously across many machines.
This is implemented in a specialized distributed
framework for parsing and text analysis built on
top of Hadoop (D. Cutting et al., ). Oceanography
programs compile down to distributed programs
which run in this cluster environment, allowing the
NL researcher to state declaratively the data gath-
ering and analysis tasks.
A typical program consists of two declarative
parts, a pattern matching specification, and a set
of statistics declarations. The pattern matching
section is written using Transfer, a specialized lan-
guage for identifying subgraphs in the dependency
structures used in XLE (Crouch and King, 2006).
Transfer rules use a declarative syntax for spec-
ifying elements and their relations; in this way,
it is much like a very specialized awk or grep
for matching within parse trees and dependency
graphs.
Statistics over these matched structures are
</bodyText>
<page confidence="0.994688">
21
</page>
<bodyText confidence="0.9999786">
also stated declaratively. The researcher states
which sub-elements or tuples are to be counted,
and the resulting compiled program will output
counts. Conditional distributions and comparisons
between distributions are available as well.
</bodyText>
<subsectionHeader confidence="0.999707">
3.1 Training Data
</subsectionHeader>
<bodyText confidence="0.91554736">
Using Oceanography, we extracted two sets of re-
lations from the parsed Wikipedia corpus, Full-
Wiki, with approximately 2 million documents. A
smaller 10,000-document subset, the 10K set, was
used in initial experiments. Some comparative re-
sults are shown to indicate effects of corpus size
on results. Summary corpus statistics are shown
in table 2. The two sets were:
1. All verb-argument pairs, using verb and ar-
gument lemmas. We recorded the verb, the
argument, the kind of relation between them
(e.g., subject, object, etc.), and part of speech
of the argument, distinguishing also among
pronouns, names, and common nouns. For
each combination, we record its frequency of
occurrence.
2. All deverbal-argument pairs, using deverbal
noun and argument lemmas. We recorded the
deverbal noun, the argument, the kind of rela-
tion (e.g., possessive, ‘of’, prenominal modi-
fier, etc.) and part of speech of the argument.
We record the frequency of occurrence for
each combination.
Some summary statistics about the extracted
data are in Table 2.
</bodyText>
<table confidence="0.997364444444444">
FullWiki training data
Documents 2 million
Sentences 121,428,873
Deverbal nouns with arguments 4,596
Unique verbs with deverbals 3,280
Verbs with arguments 7,682
Deverbal - role - argument sets 21,924,405
Deverbal - argument pairs 12,773,621
Deverbals with any poss argument 3,802
Possessive deverbal - argument pairs 611,192
Most frequent: poss(work, he) 75,343
Deverbals with any ‘of’ argument 4,075
‘Of’ deverbal- argument pairs 2,108,082
Most frequent: of(end, season) 15,282
Verb - role - argument sets 72,150,246
Verb - argument pairs 40,895,810
Overlapping pairs 5,069,479
Deverbals with overlapping arguments 3,211
</table>
<tableCaption confidence="0.998004">
Table 2: Training Data
</tableCaption>
<sectionHeader confidence="0.977799" genericHeader="method">
4 Assigning Roles
</sectionHeader>
<bodyText confidence="0.999956642857143">
The present method is based on projecting argu-
ment type preferences from the verbal usage to the
deverbal. The intuition is that if an argument X is
preferred as the subject (object) of verb V, then it
will also be preferred in the semantic frame of an
occurrence (N, X) with the corresponding dever-
bal noun N.
We model these preferences directly using the
relative frequency of subject and object occur-
rences of each possible argument with each verb.
Even with an extremely large corpus, it is unlikely
that one will find direct evidence for all such com-
binations, and one will need to generalize the pre-
diction.
</bodyText>
<sectionHeader confidence="0.698656" genericHeader="method">
4.1 Deverbal-only Model
</sectionHeader>
<bodyText confidence="0.999868538461539">
The first model, all-arg, specializes only for the
deverbal, and generalizes over all arguments, re-
lying on the overall preference of subject v. ob-
ject for the set of arguments that appear with both
verb and deverbal forms. Take as an example
deverbal nouns with possessive arguments (e.g.,
the city’s destruction). Given the phrase (X’s N),
where N is a deverbal noun related to verb V,
Fd(N, V, X) is a function that assigns one of the
roles subj, obj, unknown to the pair (V, X). In
this deverbal only model, the function depends on
N and V only, and not on the argument X. Fd for
a any pair (N, V) is calculated as follows:
</bodyText>
<listItem confidence="0.73447155">
1. Find all arguments X that occur in the con-
struction “N’s X” as well as either subj(V, X)
or obj(V, X). X, N, and V have all been lem-
matized. For example, poss(city, destruction)
occurs 10 times in the corpus; subj(destroy,
city) occurs 3 times, and obj(destroy, city) oc-
curs 12 times. This approach conflates in-
stances of the city’s destruction, the cities’
destruction, the city’s destructions, etc.
2. For each argument X, calculate the ratio be-
tween the number of occurrences of subj(V,
X) and obj(V, X). If the argument occurs
as subject more than 1.5 times as often as
the object, increment the count of subject-
preferring arguments of N by 1. If the ar-
gument occurs as object more than 1.5 times
as often as subject (as would be the case with
(destroy, city)), increment the count of object-
preferring arguments. If the ratio in frequen-
cies of occurrence is less than the cutoff ratio
</listItem>
<page confidence="0.9858">
22
</page>
<bodyText confidence="0.97453678125">
of 1.5, neither count is incremented. In ad-
dition to the number of arguments with each
preference, we keep track of the total num-
ber of instances for each argument prefer-
ence, summed up over all individual argu-
ments with that preference.
3. Compare the number of subject-preferring ar-
guments of N with the number of object-
preferring arguments. If one is greater than
the other by more than 1.5 times, state that the
deverbal noun N has a preference for map-
ping its possessive arguments to the appro-
priate verbal role. We ignore cases where the
total number of occurrences of the winning
arguments is too small to be informative (in
the current model, we require it to be greater
than 1).
If there is insufficient evidence for a deverbal
N, we fall back to the default preference across all
deverbals. Subject and object co-occurrences with
the verb forms are always counted, regardless of
other arguments the verb may have in each sen-
tence, on the intuition that the semantic role pref-
erence of the argument is relatively unaffected and
that this will map to the deverbal construction even
when the possessive is the only argument. Sum-
mary preferences for all-args are shown in Ta-
ble 3.
The same algorithm was applied to detect ar-
gument preferences for deverbals with ‘of’ argu-
ments (such as destruction of the city). Summary
preferences are shown in Table 4.
</bodyText>
<subsectionHeader confidence="0.864328">
4.2 Deverbal + Argument Animacy Model
</subsectionHeader>
<bodyText confidence="0.9999658125">
The second model tries to capture the intuition that
animate arguments often behave differently than
inanimate ones: in particular, animate arguments
are more often agents, encoded syntactically as
subjects.
We calculated argument preferences separately
for two classes of arguments: (1) animate pro-
nouns such as he, she, I; and (2) nouns that were
not identified as names by our name tagger. We
assumed that arguments in the first group were
animate, whereas arguments in the second group
were not. In these experiments, we did not try to
classify named entities as animate or inanimate,
resulting in less training data for both classes of
arguments. This strategy also incorrectly classifies
common nouns that refer to people (e.g., occupa-
tion names such as teacher).
The results of running both models on the 10K
and FullWiki training sets are in Table 3 for pos-
sessive arguments and Table 4 for ‘of’ arguments.
For possessives, animate arguments preferred
subject role mappings much more than the average
across all arguments. Inanimate arguments also on
the whole preferred subject mappings, but much
less strongly.
For ‘of’ arguments, in most cases there were
more object-preferring verbs, except for verbs
with animate arguments, which overwhelmingly
preferred subjects. We might therefore expect
there to be a difference in performance between
the model that treats all arguments equally and the
model that takes argument animacy into account.
</bodyText>
<table confidence="0.997368666666667">
Model: all-arg
10K FullWiki
Subj-preferring 391 (65%) 1786 (67%)
Obj-preferring 207 (35%) 884 (33%)
Total 598 (100%) 2670 (100%)
Model: animacy
Subj-pref animate 370 (78%) 1941 (79%)
Obj-pref animate 106 (22%) 511 (21%)
Total animate 476 (100%) 2452 (100%)
Subj-pref inanimate 45 (47%) 990 (57%)
Obj-pref inanimate 51 (53%) 748 (43%)
Total inanimate 96 (100%) 1738 (100%)
</table>
<tableCaption confidence="0.987483">
Table 3: Possessive argument preferences
</tableCaption>
<table confidence="0.999572">
Model: all-arg
10K FullWiki
Subj-preferring 143 (30%) 839 (29%)
Obj-preferring 328 (70%) 2036 (71%)
Total 471 (100%) 2875 (100%)
Model: animacy
Subj-pref animate 70 (83%) 1196 (74%)
Obj-pref animate 14 (17%) 423 (26%)
Total animate 84 (100 %) 1619 (100%)
Subj-pref inanimate 83 (23%) 699 (25%)
Obj-pref inanimate 272 (77%) 2068 (75%)
Total inanimate 355 (100%) 2767 (100%)
</table>
<tableCaption confidence="0.999565">
Table 4: ‘Of’ argument preferences
</tableCaption>
<sectionHeader confidence="0.998079" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999938">
The base system against which we compare these
models uses the output of the parser, identifies de-
verbal nouns and their arguments, and applies the
heuristics described in Section 2.1 to obtain verb
roles. Recall that possessive arguments of transi-
tive deverbals map to the subject role, and ‘of’ ar-
guments map to object. Also recall that these rules
apply only to eventive deverbals; mapping rules
for known agentive and patient-like deverbals re-
main as before.
</bodyText>
<page confidence="0.99571">
23
</page>
<bodyText confidence="0.999993944444445">
In the evaluation, the experimental models take
precedence: if the model predicts an outcome, it
is used. The default system behavior is used as a
fallback when the model does not have sufficient
evidence to make a prediction. This stacking of
models allows the use of corpus evidence when
available, and generalized defaults otherwise.
For the animacy model, we used our full sys-
tem to detect whether the argument of a deverbal
was animate (more precisely, human). In addi-
tion to the animate pronouns used to generate the
model, we also considered person names, as well
as common nouns that had the hypernym ‘person’
in WordNet. If the argument was animate and the
model had a prediction, that was used. If no pre-
diction was available for animate arguments, then
the inanimate prediction was used. Failing that,
the prediction falls back to the general defaults.
</bodyText>
<subsectionHeader confidence="0.981762">
5.1 Possessive Arguments of Deverbal Nouns
</subsectionHeader>
<bodyText confidence="0.9991008">
Model predictions were compared against the
hand-annotated evaluation set described in Sec-
tion 2.3. For each sentence in the evaluation set,
we used the models to make a two-way prediction
with respect to the default mapping: is the posses-
sive argument of the deverbal noun an underlying
subject or not. We ignored test sentences marked
as having erroneous parses, leaving 918 (of 1000
annotated). Since we were evaluating the accuracy
of the ‘subject’ label, all non-subject roles (object,
“other”, “not a deverbal”, and “nominal modifier”)
were in the same class. The baseline for compari-
son is the default ‘subject’ role.
The possible outcomes for each sentence
were:
</bodyText>
<listItem confidence="0.999405">
• True Positive: Expected role and role pro-
duced by the system are “subject”
• True Negative: Expected role is not subject,
and the model did not produce the label sub-
ject. Expected role and produced role may
differ (e.g. expected role may be “other”, and
the model may produce “object”, but since
neither one is “subject”, this counts as correct
• False Positive: Expected role is not subject,
but the model produced subject
• False Negative: Expected role is subject, but
the model produced some other role
</listItem>
<bodyText confidence="0.999984181818182">
As a quick evaluation, we compared baseline
and model-predicted results directly in the surface
string of the sentences, without reparsing the sen-
tences or using the semantic rewrite rules. The
advantage of this evaluation is that it is very fast
to run and is easily reproducible outside of our
specialized environment. This evaluation differed
from the full-pipeline evaluation in two ways: (1)
it did not distinguish event deverbals from agen-
tive and patient-like deverbals, thus possibly in-
troducing errors, and (2) it did not look up all ar-
gument lemmas to find out their animacy. This
baseline had precision of 0.56; recall of 1.0, and
an F-measure of 0.72.
The complete evaluation uses our full NL
pipeline, reparsing the sentences and applying all
of our deverbal mapping rules as described above.
The baseline for this evaluation had a precision of
0.65, recall of 0.94, and F-measure of 0.77. The
differences in the two baselines are mostly due to
the full-pipeline evaluation having different map-
ping rules for agentive and patient-like deverbals.
</bodyText>
<sectionHeader confidence="0.756901" genericHeader="method">
5.1.1 Results
</sectionHeader>
<bodyText confidence="0.999732642857143">
Results of applying the models are summarized
in Table 5, for all models, trained with both the
smaller and the larger data sets, and measured with
and without using the full pipeline.
All models performed better than the baseline.
The all-arg model did about the same as the an-
imacy model with both training sets. We suggest
some reasons for this in the next section.
It is unambiguously clear that adding lexical
knowledge to the rule system, even when this
knowledge is derived from a relatively small train-
ing set, significantly improves performance, and
also that more training data leads to greater im-
provements.
</bodyText>
<table confidence="0.999519307692308">
Model Training Precision Recall F-measure
Surface String Measure
Baseline - 0.56 1.00 0.72
all-arg 10K 0.64 0.92 0.76
animacy 10K 0.62 0.93 0.75
all-arg FullWiki 0.68 0.95 0.81
animacy FullWiki 0.70 0.92 0.79
Full NL pipeline
Baseline - 0.65 0.94 0.77
all-arg 10K 0.75 0.88 0.81
animacy 10K 0.73 0.90 0.80
all-arg FullWiki 0.78 0.90 0.84
animacy FullWiki 0.81 0.88 0.84
</table>
<tableCaption confidence="0.9855095">
Table 5: Performance on deverbal nouns with one
possessive argument
</tableCaption>
<subsubsectionHeader confidence="0.818597">
5.1.2 Error Analysis and Discussion
</subsubsectionHeader>
<bodyText confidence="0.993798">
We looked at the errors produced by the best-
performing model, all-arg trained on the FullWiki
</bodyText>
<page confidence="0.997017">
24
</page>
<bodyText confidence="0.999984163934426">
set. There were 49 false negatives (i.e. cases
where the human judge decided that the underly-
ing relationship between the deverbal and its ar-
gument is ‘subject’, but our system produced a
different relation or no relation at all), covering
39 unique deverbal nouns. Of these, 20 deverbal
nouns were predicted by the model to prefer ob-
jects (e.g., Hopkins’ accusation), and 19 did not
get assigned either subject or object due to other
errors (including a few mislabeled evaluation sen-
tences).
Some of the false negatives involved deverbal
nouns that refer to reciprocal predicates such as
his marriage, or causative ones such as Berlin’s
unity, which could map to subject or objects. Our
current system does not allow us to express such
ambiguity, but it is a possible future improvement.
Looking at the false negatives produced by the
all-arg model, 3 deverbal nouns received more ac-
curate predictions with the animacy model (e.g.,
his sight; Peter Kay’s statement). Intuitively, the
animacy model should in general make more in-
formed decisions about the argument mappings
because it takes properties of individual arguments
into account. However, as we have seen, it does
not in fact outperform the model that treats all ar-
guments the same way.
We believe this is due to the fact that the an-
imacy model was trained on less data than the
all-arg model, because we only considered ani-
mate pronouns and common nouns when gener-
ating argument-mapping predictions. Excluding
all named entities and non-animate pronouns most
likely had an effect on the number of deverbals for
which the model was able to make accurate pre-
dictions. In the next iteration, we would like to
use all available arguments, relying on the named
entity type and information available in WordNet
for common nouns to distinguish between animate
and inanimate arguments.
The all-arg model evaluation resulted in 131
false positives (cases where the model predicted
the relation to be ‘subject’, but the human judge
thought it was something else). Of these, 105 were
marked by the human judge as having objects, 8 as
having a verbal relation other than subject or ob-
ject, 9 as having nominal modifiers, 9 has having
no deverbal.
Altogether, false positives covered 85 unique
verbs. Of these, 48 had been explicitly predicted
by our model to prefer subjects, and the rest had
no explicit prediction, thus defaulting to having a
subject. 3 of these deverbals would have been cor-
rectly identified as having objects by the animacy
model (e.g., his composition; her representation).
Although it is hard to predict the outcome of a
statistical model, we feel that more reliable infor-
mation about the animacy of arguments at training
time would improve the performance of the ani-
macy model, potentially making it better than the
all-arg model.
</bodyText>
<subsectionHeader confidence="0.999576">
5.2 ‘Of’ Arguments of Deverbal Nouns
</subsectionHeader>
<bodyText confidence="0.999944857142857">
The evaluation procedure for ‘of’ arguments was
the same as for possessive arguments, except that
the default argument mapping was ‘object’, and
the evaluated decision was whether a particular
role was object or non-object. Ignoring sentence
with erroneous parses, we had 677 evaluation ex-
amples.
</bodyText>
<sectionHeader confidence="0.943482" genericHeader="evaluation">
5.2.1 Results
</sectionHeader>
<bodyText confidence="0.998770923076923">
Results for all models are summarized in Table 6.
All models outperformed the baseline on all train-
ing sets and on both the surface or full-pipeline
measures.
As with possessive arguments, the all-arg and
animacy models performed about the same, with
both the FullWiki and 10K training sets.
The 10K-trained animacy model did not do
as poorly as might have been expected given its
low prediction rate for deverbals with animate ar-
guments in our evaluation set. The better-than-
expected performance may be explained by low
incidence of animate arguments in this set.
</bodyText>
<table confidence="0.999332615384616">
Model Training Precision Recall F-measure
Surface String Measure
Baseline - 0.60 1.00 0.75
all-arg 10K 0.68 0.97 0.80
animacy 10K 0.66 0.94 0.78
all-arg FullWiki 0.71 0.97 0.82
animacy FullWiki 0.70 0.91 0.79
Full NL pipeline
Baseline - 0.61 0.89 0.73
all-arg 10K 0.71 0.86 0.78
animacy 10K 0.70 0.85 0.77
all-arg FullWiki 0.78 0.87 0.82
animacy FullWiki 0.80 0.85 0.82
</table>
<tableCaption confidence="0.973842">
Table 6: Performance on deverbal nouns with one
‘of’ argument
</tableCaption>
<subsubsectionHeader confidence="0.67413">
5.2.2 Error Analysis and Discussion
</subsubsectionHeader>
<bodyText confidence="0.9973575">
We looked at the errors produced by the best-
performing model, all-arg trained on the FullWiki
</bodyText>
<page confidence="0.992855">
25
</page>
<bodyText confidence="0.99987425">
set. There were 53 false negatives (cases where
the human judged marked the relation as ‘object’
but the system marked it as something else), cov-
ering 42 unique deverbal nouns. Of these 7 were
(incorrectly) predicted by the model to prefer sub-
jects (e.g., operation of a railway engine), and the
rest were misidentified due to other errors.
There were 101 false positives (cases where the
system marked the role as object, but the human
judge disagreed). Of these, the human judged
marked 54 as subject, 21 as other verbal role, 13
as nominal modifier, and 13 as non-deverbal.
Of the 72 unique deverbals in the false-positive
set, our model incorrectly predicted that 38 should
prefer objects (such as Adoration of the Magi; un-
der the direction of Bishop Smith)). For 30 de-
verbals, the model made no prediction, and the
default mapping to object turned out to be incor-
rect. It is unclear to what extent better information
about animacy would have helped.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999966821428571">
One of the earliest computational attempts to de-
rive argument structures for deverbal nouns is
(Hull and Gomez, 1996), with hand-crafted map-
ping rules for a small set of individual nouns, ex-
emplifying a highly precise but not easily scalable
method.
In recent years, NomBank (Meyers et al.,
2004a) has provided a set of about 200,000 manu-
ally annotated instances of nominalizations with
arguments, giving rise to supervised machine-
learned approaches such as (Pradhan et al., 2004)
and (Liu and Ng, 2007), which perform fairly well
in the overall task of classifying deverbal argu-
ments. However, no evaluation results are pro-
vided for specific, problematic classes of nominal
arguments such as possessives; it is likely that the
amount of annotations in NomBank is insufficient
to reliably map such cases onto verbal arguments.
(Pad´o et al., 2008) describe an unsupervised
approach that, like ours, uses verbal argument
patterns to deduce deverbal patterns, though the
resulting labels are semantic roles used in SLR
tasks (cf. (Gildea and Jurafsky, 2000)) rather
than syntactic roles. A combination of our much
larger training set and the sophisticated probabilis-
tic methods used by Pad´o et al. would most likely
improve performance for both syntactic and se-
mantic roles labelling tasks.
</bodyText>
<sectionHeader confidence="0.995847" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999992714285714">
We have demonstrated that large amounts of lexi-
cal data derived from an unsupervised parsed cor-
pus improve role assignment for deverbal nouns.
The improvements are significant even with a rel-
atively small training set, relying on parses that
have not been hand-corrected, using a very sim-
ple prediction model. Larger amounts of extracted
data improve performance even more.
There is clearly still headroom for improve-
ment in this method. In a pilot study, we used
argument preferences for individual deverbal-
argument pairs, falling back to deverbal-only gen-
eralizations when more specific patterns were not
available. This model had slightly higher preci-
sion and slightly lower recall than the deverbal-
only model, suggesting that a more sophisticcated
probabilistic prediction model may be needed.
In addition, performance should improve if we
allow non-binary decisions: in addition to map-
ping deverbal arguments to subject or object of the
underlying verb, we could allow mappings such
as “unknown” or “ambiguous”. The same training
sets can be used to produce a model that makes a
3- or 4-way split. In the possessive and ‘of’ sets,
the “unknown / ambiguous” class would cover be-
tween 15% and 20% of all the data. This third
possibility becomes even more important for other
deverbal arguments. For example, if the deverbal
noun has a prenominal modifier (as in city destruc-
tion), in a third of the cases the underlying relation
is neither the subject nor the object (Lapata, 2002).
And, of course, the methodology of extracting
lexical preferences based on large parsed corpora
can be applied to many other NL tasks not related
to deverbal nouns.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99982475">
We gratefully acknowledge the helpful advice and
comments of our colleagues Tracy Holloway King
and Dick Crouch, as well as the three anonymous
reviewers.
</bodyText>
<page confidence="0.993459">
26
</page>
<sectionHeader confidence="0.998343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999771818181818">
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In ACL, pages 26–33.
Richard S. Crouch and Tracy Holloway King. 2006.
Semantics via f-structure rewriting. In Proceed-
ings of the Lexical Functional Grammar Conference
2006.
Dick Crouch, Mary Dalrymple, Ron Kaplan,
Tracy Holloway King, John Maxwell, and Paula
Newman. 2009. XLE Documentation. Available
On-line.
D. Cutting et al. Apache Hadoop Project.
http://hadoop.apache.org/.
Mary Dalrymple. 2001. Lexical Functional Grammar.
Academic Press. Syntax and Semantics, volume 34.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 512–520, Hong Kong,
October. Association for Computational Linguistics.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2008. Deverbal nouns in
knowledge representation. Journal of Logic and
Computation, 18:385–404.
Richard D. Hull and Fernando Gomez. 1996. Seman-
tic interpretation of nominalizations. In AAAI/IAAI,
Vol. 2, pages 1062–1068.
Ronald Kaplan and John T. Maxwell. 1995. A method
for disjunctive constraint satisfaction. In Formal Is-
sues in Lexical-Functional Grammar. CSLI Press.
Judith Klavans and Philip Resnik, editors. 1996. The
Balancing Act. Combining Symbolic and Statistical
Approaches to Language. The MIT Press.
Maria Lapata. 2002. The disambiguation of nomi-
nalizations. Computational Linguistics, 28(3):357–
388.
Chang Liu and Hwee Tou Ng. 2007. Learning pre-
dictive structures for semantic role labeling of nom-
bank. In ACL.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of
EURALEX’98.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004a.
The nombank project: An interim report. In
A. Meyers, editor, HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, pages 24–31,
Boston, Massachusetts, USA, May 2 - May 7. As-
sociation for Computational Linguistics.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004b. The cross-breeding of
dictionaries. In Proceedings of LREC-2004.
Mary Nunes. 1993. Argument linking in english de-
rived nominals. In Robert Van Valin, editor, Ad-
vances in Role and Reference Grammar, pages 375–
432. John Benjamins.
Sebastian Pad´o, Marco Pennacchiotti, and Caroline
Sporleder. 2008. Semantic role assignment for
event nominalisations by leveraging verbal data. In
Proceedings of CoLing08.
Sameer Pradhan, Honglin Sun, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2004. Parsing argu-
ments of nominalizations in english and chinese. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 141–
144, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman.
Stefan Riezler, Tracy Holloway King, Ronald Kaplan,
John T. Maxwell II, Richard Crouch, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and discrimi-
native estimation techniques. In Proceedings of the
ACL’02.
Scott A. Waterman. 2009. Distributed parse mining.
In Software engineering, testing, and quality assur-
ance for natural language processing (SETQA-NLP
2009).
</reference>
<page confidence="0.998795">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.758089">
<title confidence="0.999616">Mining of Parsed Data to Derive Deverbal Argument Structure</title>
<author confidence="0.999982">Olga Gurevich Scott A Waterman</author>
<affiliation confidence="0.990052">Microsoft /</affiliation>
<address confidence="0.9762785">475 Brannan Street, Ste. San Francisco, CA</address>
<abstract confidence="0.982581583333333">The availability of large parsed corpora and improved computing resources now make it possible to extract vast amounts of lexical data. We describe the process of extracting structured data and several methods of deriving argument structure mappings for deverbal nouns that significantly improves upon non-lexicalized rule-based methods. For a typical model, the F-measure of performance improves from a baseline of about 0.72 to 0.81.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In ACL,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="2104" citStr="Banko and Brill, 2001" startWordPosition="318" endWordPosition="321">r to find widely available, large corpora annotated for syntactic or semantic structure. The Penn Treebank (Marcus et al., 1993) has until recently been the only such corpus, covering 4.5M words in a single genre of financial reporting. At the same time, the accuracy and speed of syntactic parsers has been improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation. For many NLP tasks, having more training data greatly improves the quality of the resulting models (Banko and Brill, 2001), even if the training data are not perfect. We have access to the entire English-language text of Wikipedia (about 2M pages) that was parsed using the XLE parser (Riezler et al., 2002), as well as an architecture for distributed datamining within this corpus, called Oceanography (Waterman, 2009). Using the parsed corpus, we extract a large volume of dependency relations and derive lexical models that significantly improve a rule-based system for determining the underlying argument structure of deverbal noun constructions. 2 Deverbal Argument Mapping Deverbal nouns, or nominalizations, are nou</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In ACL, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Crouch</author>
<author>Tracy Holloway King</author>
</authors>
<title>Semantics via f-structure rewriting.</title>
<date>2006</date>
<booktitle>In Proceedings of the Lexical Functional Grammar Conference</booktitle>
<contexts>
<context position="8259" citStr="Crouch and King, 2006" startWordPosition="1315" endWordPosition="1318">ntly processing the entire text of the English-language Wikipedia, consisting of about 2M unique pages. Parsing in this system is done using the XLE parser (Kaplan and Maxwell, 1995) and a broadcoverage grammar of English (Riezler et al., 2002; Crouch et al., 2009), which produces constituent structures and functional structures in accordance with the theory of Lexical-Functional Grammar (Dalrymple, 2001). Parsing is followed by a semantic processing phase, producing a more abstract argument structure. Semantic representations are created using the Transfer system of successive rewrite rules (Crouch and King, 2006). Numerous constructions are normalized and rewritten (e.g., passives, relative clauses, etc.) to maximize matching between alternate surface forms. This is the step in which deverbal argument mapping occurs. 20 2.3 Evaluation Data To evaluate the performance of the current and experimental argument mappings, we extracted a random set of 1000 sentences from the parsed Wikipedia corpus in which a deverbal noun had a single possessive argument. Each sentence was manually annotated with the verb role mapping between the deverbal and the possessive arguments. One of six labels were assigned: • Sub</context>
<context position="12364" citStr="Crouch and King, 2006" startWordPosition="1966" endWordPosition="1969">ines. This is implemented in a specialized distributed framework for parsing and text analysis built on top of Hadoop (D. Cutting et al., ). Oceanography programs compile down to distributed programs which run in this cluster environment, allowing the NL researcher to state declaratively the data gathering and analysis tasks. A typical program consists of two declarative parts, a pattern matching specification, and a set of statistics declarations. The pattern matching section is written using Transfer, a specialized language for identifying subgraphs in the dependency structures used in XLE (Crouch and King, 2006). Transfer rules use a declarative syntax for specifying elements and their relations; in this way, it is much like a very specialized awk or grep for matching within parse trees and dependency graphs. Statistics over these matched structures are 21 also stated declaratively. The researcher states which sub-elements or tuples are to be counted, and the resulting compiled program will output counts. Conditional distributions and comparisons between distributions are available as well. 3.1 Training Data Using Oceanography, we extracted two sets of relations from the parsed Wikipedia corpus, Full</context>
</contexts>
<marker>Crouch, King, 2006</marker>
<rawString>Richard S. Crouch and Tracy Holloway King. 2006. Semantics via f-structure rewriting. In Proceedings of the Lexical Functional Grammar Conference 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ron Kaplan</author>
<author>Tracy Holloway King</author>
<author>John Maxwell</author>
<author>Paula Newman</author>
</authors>
<title>XLE Documentation. Available On-line.</title>
<date>2009</date>
<contexts>
<context position="7902" citStr="Crouch et al., 2009" startWordPosition="1267" endWordPosition="1270">pping depends on the lexical nature of the deverbal noun and its corresponding verb, and possibly on properties of the possessive or ‘of’ argument as well. 2.2 System Background The deverbal argument mapping occurs in the context of a larger semantic search application, where the goal is to match alternate forms expressing similar concepts. We are currently processing the entire text of the English-language Wikipedia, consisting of about 2M unique pages. Parsing in this system is done using the XLE parser (Kaplan and Maxwell, 1995) and a broadcoverage grammar of English (Riezler et al., 2002; Crouch et al., 2009), which produces constituent structures and functional structures in accordance with the theory of Lexical-Functional Grammar (Dalrymple, 2001). Parsing is followed by a semantic processing phase, producing a more abstract argument structure. Semantic representations are created using the Transfer system of successive rewrite rules (Crouch and King, 2006). Numerous constructions are normalized and rewritten (e.g., passives, relative clauses, etc.) to maximize matching between alternate surface forms. This is the step in which deverbal argument mapping occurs. 20 2.3 Evaluation Data To evaluate</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2009</marker>
<rawString>Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy Holloway King, John Maxwell, and Paula Newman. 2009. XLE Documentation. Available On-line.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Cutting</author>
</authors>
<title>Apache Hadoop Project.</title>
<note>http://hadoop.apache.org/.</note>
<marker>Cutting, </marker>
<rawString>D. Cutting et al. Apache Hadoop Project. http://hadoop.apache.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
</authors>
<title>Lexical Functional Grammar. Academic Press. Syntax and Semantics,</title>
<date>2001</date>
<volume>34</volume>
<contexts>
<context position="8045" citStr="Dalrymple, 2001" startWordPosition="1286" endWordPosition="1287">as well. 2.2 System Background The deverbal argument mapping occurs in the context of a larger semantic search application, where the goal is to match alternate forms expressing similar concepts. We are currently processing the entire text of the English-language Wikipedia, consisting of about 2M unique pages. Parsing in this system is done using the XLE parser (Kaplan and Maxwell, 1995) and a broadcoverage grammar of English (Riezler et al., 2002; Crouch et al., 2009), which produces constituent structures and functional structures in accordance with the theory of Lexical-Functional Grammar (Dalrymple, 2001). Parsing is followed by a semantic processing phase, producing a more abstract argument structure. Semantic representations are created using the Transfer system of successive rewrite rules (Crouch and King, 2006). Numerous constructions are normalized and rewritten (e.g., passives, relative clauses, etc.) to maximize matching between alternate surface forms. This is the step in which deverbal argument mapping occurs. 20 2.3 Evaluation Data To evaluate the performance of the current and experimental argument mappings, we extracted a random set of 1000 sentences from the parsed Wikipedia corpu</context>
</contexts>
<marker>Dalrymple, 2001</marker>
<rawString>Mary Dalrymple. 2001. Lexical Functional Grammar. Academic Press. Syntax and Semantics, volume 34.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>512--520</pages>
<institution>Hong Kong, October. Association for Computational Linguistics.</institution>
<contexts>
<context position="31543" citStr="Gildea and Jurafsky, 2000" startWordPosition="5115" endWordPosition="5118">nelearned approaches such as (Pradhan et al., 2004) and (Liu and Ng, 2007), which perform fairly well in the overall task of classifying deverbal arguments. However, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likely that the amount of annotations in NomBank is insufficient to reliably map such cases onto verbal arguments. (Pad´o et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles. A combination of our much larger training set and the sophisticated probabilistic methods used by Pad´o et al. would most likely improve performance for both syntactic and semantic roles labelling tasks. 7 Conclusions and Future Work We have demonstrated that large amounts of lexical data derived from an unsupervised parsed corpus improve role assignment for deverbal nouns. The improvements are significant even with a relatively small training set, relying on parses that have not been hand-corrected, using a very simple prediction model. Larger amounts of extract</context>
</contexts>
<marker>Gildea, Jurafsky, 2000</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 512–520, Hong Kong, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Gurevich</author>
<author>Richard Crouch</author>
<author>Tracy Holloway King</author>
<author>Valeria de Paiva</author>
</authors>
<title>Deverbal nouns in knowledge representation.</title>
<date>2008</date>
<journal>Journal of Logic and Computation,</journal>
<pages>18--385</pages>
<marker>Gurevich, Crouch, King, de Paiva, 2008</marker>
<rawString>Olga Gurevich, Richard Crouch, Tracy Holloway King, and Valeria de Paiva. 2008. Deverbal nouns in knowledge representation. Journal of Logic and Computation, 18:385–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard D Hull</author>
<author>Fernando Gomez</author>
</authors>
<title>Semantic interpretation of nominalizations.</title>
<date>1996</date>
<booktitle>In AAAI/IAAI,</booktitle>
<volume>2</volume>
<pages>1062--1068</pages>
<contexts>
<context position="30605" citStr="Hull and Gomez, 1996" startWordPosition="4967" endWordPosition="4970">he human judged marked 54 as subject, 21 as other verbal role, 13 as nominal modifier, and 13 as non-deverbal. Of the 72 unique deverbals in the false-positive set, our model incorrectly predicted that 38 should prefer objects (such as Adoration of the Magi; under the direction of Bishop Smith)). For 30 deverbals, the model made no prediction, and the default mapping to object turned out to be incorrect. It is unclear to what extent better information about animacy would have helped. 6 Related Work One of the earliest computational attempts to derive argument structures for deverbal nouns is (Hull and Gomez, 1996), with hand-crafted mapping rules for a small set of individual nouns, exemplifying a highly precise but not easily scalable method. In recent years, NomBank (Meyers et al., 2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machinelearned approaches such as (Pradhan et al., 2004) and (Liu and Ng, 2007), which perform fairly well in the overall task of classifying deverbal arguments. However, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likel</context>
</contexts>
<marker>Hull, Gomez, 1996</marker>
<rawString>Richard D. Hull and Fernando Gomez. 1996. Semantic interpretation of nominalizations. In AAAI/IAAI, Vol. 2, pages 1062–1068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>John T Maxwell</author>
</authors>
<title>A method for disjunctive constraint satisfaction. In Formal Issues in Lexical-Functional Grammar.</title>
<date>1995</date>
<publisher>CSLI Press.</publisher>
<contexts>
<context position="7819" citStr="Kaplan and Maxwell, 1995" startWordPosition="1252" endWordPosition="1255"> we have seen from examples above, these defaults are not always correct. The correct mapping depends on the lexical nature of the deverbal noun and its corresponding verb, and possibly on properties of the possessive or ‘of’ argument as well. 2.2 System Background The deverbal argument mapping occurs in the context of a larger semantic search application, where the goal is to match alternate forms expressing similar concepts. We are currently processing the entire text of the English-language Wikipedia, consisting of about 2M unique pages. Parsing in this system is done using the XLE parser (Kaplan and Maxwell, 1995) and a broadcoverage grammar of English (Riezler et al., 2002; Crouch et al., 2009), which produces constituent structures and functional structures in accordance with the theory of Lexical-Functional Grammar (Dalrymple, 2001). Parsing is followed by a semantic processing phase, producing a more abstract argument structure. Semantic representations are created using the Transfer system of successive rewrite rules (Crouch and King, 2006). Numerous constructions are normalized and rewritten (e.g., passives, relative clauses, etc.) to maximize matching between alternate surface forms. This is the</context>
</contexts>
<marker>Kaplan, Maxwell, 1995</marker>
<rawString>Ronald Kaplan and John T. Maxwell. 1995. A method for disjunctive constraint satisfaction. In Formal Issues in Lexical-Functional Grammar. CSLI Press.</rawString>
</citation>
<citation valid="true">
<title>The Balancing Act. Combining Symbolic and Statistical Approaches to Language.</title>
<date>1996</date>
<editor>Judith Klavans and Philip Resnik, editors.</editor>
<publisher>The MIT Press.</publisher>
<marker>1996</marker>
<rawString>Judith Klavans and Philip Resnik, editors. 1996. The Balancing Act. Combining Symbolic and Statistical Approaches to Language. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The disambiguation of nominalizations.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>388</pages>
<marker>Lapata, 2002</marker>
<rawString>Maria Lapata. 2002. The disambiguation of nominalizations. Computational Linguistics, 28(3):357– 388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Learning predictive structures for semantic role labeling of nombank.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30991" citStr="Liu and Ng, 2007" startWordPosition="5030" endWordPosition="5033"> to be incorrect. It is unclear to what extent better information about animacy would have helped. 6 Related Work One of the earliest computational attempts to derive argument structures for deverbal nouns is (Hull and Gomez, 1996), with hand-crafted mapping rules for a small set of individual nouns, exemplifying a highly precise but not easily scalable method. In recent years, NomBank (Meyers et al., 2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machinelearned approaches such as (Pradhan et al., 2004) and (Liu and Ng, 2007), which perform fairly well in the overall task of classifying deverbal arguments. However, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likely that the amount of annotations in NomBank is insufficient to reliably map such cases onto verbal arguments. (Pad´o et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles. A combination of </context>
</contexts>
<marker>Liu, Ng, 2007</marker>
<rawString>Chang Liu and Hwee Tou Ng. 2007. Learning predictive structures for semantic role labeling of nombank. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>NOMLEX: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of EURALEX’98.</booktitle>
<contexts>
<context position="6145" citStr="Macleod et al., 1998" startWordPosition="987" endWordPosition="990">ccuracy of verbal roles assigned in such cases by creating lexically-specific preferences for individual deverbal noun / verb pairs. Some of our experiments also take into account some lexical properties of the deverbal noun’s arguments. The lexical preferences are derived by comparing argument preferences of verbs with those of related deverbal nouns, derived from a large parsed corpus using Oceanography. 2.1 Current Deverbal Mapping System We have a list of approximately 4000 deverbal noun / verb pairs, constructed from a combination of WordNet’s derivational links (Fellbaum, 1998), NomLex (Macleod et al., 1998), NomLexPlus (Meyers et al., 2004b) and some independent curation. In the current system implementation, we attempt to map deverbal nouns onto corresponding verbs using a small set of heuristics described in (Gurevich et al., 2008). We distinguish between event nouns like destruction, agentive nouns like destroyer, and patient-like nouns like employee. If a deverbal noun maps onto a transitive verb and has only one argument, the heuristics are as follows. Arguments of agentive nouns become objects while the nouns themselves become subjects, so the ship’s destroyer maps to subj(destroy, destroy</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. NOMLEX: A lexicon of nominalizations. In Proceedings of EURALEX’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1610" citStr="Marcus et al., 1993" startWordPosition="236" endWordPosition="239">ng up to a wider range of phenomena or different genres of text. There have been repeated moves towards hybridized approaches, in which rules created with human linguistic intuitions are supplemented by automatically derived corpus data (cf. (Klavans and Resnik, 1996)). Unstructured corpus data for English can easily be found on the Internet. Large corpora of text annotated with part of speech information are also available (such as the British National Corpus). However, it is much harder to find widely available, large corpora annotated for syntactic or semantic structure. The Penn Treebank (Marcus et al., 1993) has until recently been the only such corpus, covering 4.5M words in a single genre of financial reporting. At the same time, the accuracy and speed of syntactic parsers has been improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation. For many NLP tasks, having more training data greatly improves the quality of the resulting models (Banko and Brill, 2001), even if the training data are not perfect. We have access to the entire English-language text of Wikiped</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<volume>2</volume>
<pages>24--31</pages>
<editor>In A. Meyers, editor,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="6178" citStr="Meyers et al., 2004" startWordPosition="993" endWordPosition="996"> such cases by creating lexically-specific preferences for individual deverbal noun / verb pairs. Some of our experiments also take into account some lexical properties of the deverbal noun’s arguments. The lexical preferences are derived by comparing argument preferences of verbs with those of related deverbal nouns, derived from a large parsed corpus using Oceanography. 2.1 Current Deverbal Mapping System We have a list of approximately 4000 deverbal noun / verb pairs, constructed from a combination of WordNet’s derivational links (Fellbaum, 1998), NomLex (Macleod et al., 1998), NomLexPlus (Meyers et al., 2004b) and some independent curation. In the current system implementation, we attempt to map deverbal nouns onto corresponding verbs using a small set of heuristics described in (Gurevich et al., 2008). We distinguish between event nouns like destruction, agentive nouns like destroyer, and patient-like nouns like employee. If a deverbal noun maps onto a transitive verb and has only one argument, the heuristics are as follows. Arguments of agentive nouns become objects while the nouns themselves become subjects, so the ship’s destroyer maps to subj(destroy, destroyer); obj(destroy, ship). Argument</context>
<context position="30783" citStr="Meyers et al., 2004" startWordPosition="4997" endWordPosition="5000">rectly predicted that 38 should prefer objects (such as Adoration of the Magi; under the direction of Bishop Smith)). For 30 deverbals, the model made no prediction, and the default mapping to object turned out to be incorrect. It is unclear to what extent better information about animacy would have helped. 6 Related Work One of the earliest computational attempts to derive argument structures for deverbal nouns is (Hull and Gomez, 1996), with hand-crafted mapping rules for a small set of individual nouns, exemplifying a highly precise but not easily scalable method. In recent years, NomBank (Meyers et al., 2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machinelearned approaches such as (Pradhan et al., 2004) and (Liu and Ng, 2007), which perform fairly well in the overall task of classifying deverbal arguments. However, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likely that the amount of annotations in NomBank is insufficient to reliably map such cases onto verbal arguments. (Pad´o et al., 2008) describe an unsupervised approach that, like ou</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004a. The nombank project: An interim report. In A. Meyers, editor, HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The cross-breeding of dictionaries.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-2004.</booktitle>
<contexts>
<context position="6178" citStr="Meyers et al., 2004" startWordPosition="993" endWordPosition="996"> such cases by creating lexically-specific preferences for individual deverbal noun / verb pairs. Some of our experiments also take into account some lexical properties of the deverbal noun’s arguments. The lexical preferences are derived by comparing argument preferences of verbs with those of related deverbal nouns, derived from a large parsed corpus using Oceanography. 2.1 Current Deverbal Mapping System We have a list of approximately 4000 deverbal noun / verb pairs, constructed from a combination of WordNet’s derivational links (Fellbaum, 1998), NomLex (Macleod et al., 1998), NomLexPlus (Meyers et al., 2004b) and some independent curation. In the current system implementation, we attempt to map deverbal nouns onto corresponding verbs using a small set of heuristics described in (Gurevich et al., 2008). We distinguish between event nouns like destruction, agentive nouns like destroyer, and patient-like nouns like employee. If a deverbal noun maps onto a transitive verb and has only one argument, the heuristics are as follows. Arguments of agentive nouns become objects while the nouns themselves become subjects, so the ship’s destroyer maps to subj(destroy, destroyer); obj(destroy, ship). Argument</context>
<context position="30783" citStr="Meyers et al., 2004" startWordPosition="4997" endWordPosition="5000">rectly predicted that 38 should prefer objects (such as Adoration of the Magi; under the direction of Bishop Smith)). For 30 deverbals, the model made no prediction, and the default mapping to object turned out to be incorrect. It is unclear to what extent better information about animacy would have helped. 6 Related Work One of the earliest computational attempts to derive argument structures for deverbal nouns is (Hull and Gomez, 1996), with hand-crafted mapping rules for a small set of individual nouns, exemplifying a highly precise but not easily scalable method. In recent years, NomBank (Meyers et al., 2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machinelearned approaches such as (Pradhan et al., 2004) and (Liu and Ng, 2007), which perform fairly well in the overall task of classifying deverbal arguments. However, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likely that the amount of annotations in NomBank is insufficient to reliably map such cases onto verbal arguments. (Pad´o et al., 2008) describe an unsupervised approach that, like ou</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004b. The cross-breeding of dictionaries. In Proceedings of LREC-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Nunes</author>
</authors>
<title>Argument linking in english derived nominals.</title>
<date>1993</date>
<booktitle>Advances in Role and Reference Grammar,</booktitle>
<pages>375--432</pages>
<editor>In Robert Van Valin, editor,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="4195" citStr="Nunes, 1993" startWordPosition="662" endWordPosition="663">od gives rise to subj(abound, food). If the underlying verb is transitive, and the deverbal noun has two arguments, the mappings are also fairly straightforward. For example, the phrase Carthage’s defeat by Rome gives rise to 19 Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 19–27, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP the arguments subj(defeat, Rome) and obj(defeat, Carthage), based on knowledge that a “by” argument usually maps to the subject, and the possessive in the presence of a “by” argument usually maps to the object (Nunes, 1993). However, in many cases a deverbal noun has only one argument, even though the underlying verb may be transitive. In such cases, our system has to decide whether to map the lone argument of the deverbal onto the subject or object of the verb. This mapping is in many cases obvious to a human: e.g., the king’s abdication corresponds to subj(abdicate, king), whereas the room’s adornment corresponds to obj(adorn, room). In some cases, the mapping is truly ambiguous, e.g., They enjoyed the support of the Queen vs. They jumped to the support of the Queen. Yet in other cases, the lone argument of th</context>
</contexts>
<marker>Nunes, 1993</marker>
<rawString>Mary Nunes. 1993. Argument linking in english derived nominals. In Robert Van Valin, editor, Advances in Role and Reference Grammar, pages 375– 432. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Caroline Sporleder</author>
</authors>
<title>Semantic role assignment for event nominalisations by leveraging verbal data.</title>
<date>2008</date>
<booktitle>In Proceedings of CoLing08.</booktitle>
<marker>Pad´o, Pennacchiotti, Sporleder, 2008</marker>
<rawString>Sebastian Pad´o, Marco Pennacchiotti, and Caroline Sporleder. 2008. Semantic role assignment for event nominalisations by leveraging verbal data. In Proceedings of CoLing08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Honglin Sun</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Parsing arguments of nominalizations in english and chinese.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers,</booktitle>
<volume>2</volume>
<pages>141--144</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="30968" citStr="Pradhan et al., 2004" startWordPosition="5025" endWordPosition="5028">apping to object turned out to be incorrect. It is unclear to what extent better information about animacy would have helped. 6 Related Work One of the earliest computational attempts to derive argument structures for deverbal nouns is (Hull and Gomez, 1996), with hand-crafted mapping rules for a small set of individual nouns, exemplifying a highly precise but not easily scalable method. In recent years, NomBank (Meyers et al., 2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machinelearned approaches such as (Pradhan et al., 2004) and (Liu and Ng, 2007), which perform fairly well in the overall task of classifying deverbal arguments. However, no evaluation results are provided for specific, problematic classes of nominal arguments such as possessives; it is likely that the amount of annotations in NomBank is insufficient to reliably map such cases onto verbal arguments. (Pad´o et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic r</context>
</contexts>
<marker>Pradhan, Sun, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Honglin Sun, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2004. Parsing arguments of nominalizations in english and chinese. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 141– 144, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<contexts>
<context position="2835" citStr="Quirk et al., 1985" startWordPosition="433" endWordPosition="436">out 2M pages) that was parsed using the XLE parser (Riezler et al., 2002), as well as an architecture for distributed datamining within this corpus, called Oceanography (Waterman, 2009). Using the parsed corpus, we extract a large volume of dependency relations and derive lexical models that significantly improve a rule-based system for determining the underlying argument structure of deverbal noun constructions. 2 Deverbal Argument Mapping Deverbal nouns, or nominalizations, are nouns that designate some aspect of the event referred to by the verb from which they are morphologically derived (Quirk et al., 1985). For example, the noun destruction refers to the action described by the verb destroy, and destroyer may refer to the agent of that event. Deverbal nouns are very common in English texts: by one count, about half of all sentences in written text contain at least one deverbal noun (Gurevich et al., 2008). Thus, a computational system that aims to match multiple ways of expressing the same underlying events (such as question answering or search) must be able to deal with deverbal nouns. To interpret deverbal constructions, one must be able to map nominal and prepositional modifiers to the vario</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy Holloway King</author>
<author>Ronald Kaplan</author>
<author>John T Maxwell Richard Crouch</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL’02.</booktitle>
<contexts>
<context position="2289" citStr="Riezler et al., 2002" startWordPosition="350" endWordPosition="353">5M words in a single genre of financial reporting. At the same time, the accuracy and speed of syntactic parsers has been improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation. For many NLP tasks, having more training data greatly improves the quality of the resulting models (Banko and Brill, 2001), even if the training data are not perfect. We have access to the entire English-language text of Wikipedia (about 2M pages) that was parsed using the XLE parser (Riezler et al., 2002), as well as an architecture for distributed datamining within this corpus, called Oceanography (Waterman, 2009). Using the parsed corpus, we extract a large volume of dependency relations and derive lexical models that significantly improve a rule-based system for determining the underlying argument structure of deverbal noun constructions. 2 Deverbal Argument Mapping Deverbal nouns, or nominalizations, are nouns that designate some aspect of the event referred to by the verb from which they are morphologically derived (Quirk et al., 1985). For example, the noun destruction refers to the acti</context>
<context position="7880" citStr="Riezler et al., 2002" startWordPosition="1263" endWordPosition="1266">orrect. The correct mapping depends on the lexical nature of the deverbal noun and its corresponding verb, and possibly on properties of the possessive or ‘of’ argument as well. 2.2 System Background The deverbal argument mapping occurs in the context of a larger semantic search application, where the goal is to match alternate forms expressing similar concepts. We are currently processing the entire text of the English-language Wikipedia, consisting of about 2M unique pages. Parsing in this system is done using the XLE parser (Kaplan and Maxwell, 1995) and a broadcoverage grammar of English (Riezler et al., 2002; Crouch et al., 2009), which produces constituent structures and functional structures in accordance with the theory of Lexical-Functional Grammar (Dalrymple, 2001). Parsing is followed by a semantic processing phase, producing a more abstract argument structure. Semantic representations are created using the Transfer system of successive rewrite rules (Crouch and King, 2006). Numerous constructions are normalized and rewritten (e.g., passives, relative clauses, etc.) to maximize matching between alternate surface forms. This is the step in which deverbal argument mapping occurs. 20 2.3 Evalu</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy Holloway King, Ronald Kaplan, John T. Maxwell II, Richard Crouch, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of the ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A Waterman</author>
</authors>
<title>Distributed parse mining. In Software engineering, testing, and quality assurance for natural language processing (SETQA-NLP</title>
<date>2009</date>
<contexts>
<context position="2401" citStr="Waterman, 2009" startWordPosition="368" endWordPosition="369">een improving greatly, so that in recent years it has become possible to automatically create parsed corpora of reasonable quality, using much larger amounts of text with greater genre variation. For many NLP tasks, having more training data greatly improves the quality of the resulting models (Banko and Brill, 2001), even if the training data are not perfect. We have access to the entire English-language text of Wikipedia (about 2M pages) that was parsed using the XLE parser (Riezler et al., 2002), as well as an architecture for distributed datamining within this corpus, called Oceanography (Waterman, 2009). Using the parsed corpus, we extract a large volume of dependency relations and derive lexical models that significantly improve a rule-based system for determining the underlying argument structure of deverbal noun constructions. 2 Deverbal Argument Mapping Deverbal nouns, or nominalizations, are nouns that designate some aspect of the event referred to by the verb from which they are morphologically derived (Quirk et al., 1985). For example, the noun destruction refers to the action described by the verb destroy, and destroyer may refer to the agent of that event. Deverbal nouns are very co</context>
<context position="11395" citStr="Waterman, 2009" startWordPosition="1825" endWordPosition="1826">nt distributions of arguments to appear in the various deverbal modification patterns. Making use of this intuition requires collecting sufficient information about corresponding arguments of verbs and deverbal nouns. This is available, given a large parsed corpus, a reasonably accurate and fast parser, and enough computing capacity. The remainder of the paper details our data extraction, model-building methods, and the results of some experiments. 3 Data Collection Oceanography is a pattern extraction and statistics language for analyzing structural relationships in corpora parsed using XLE (Waterman, 2009). It simplifies the task of programming for NL analysis over large corpora, and the sorting, counting, and distributional analysis that often characterizes statistical NLP. This corpus processing language is accompanied by a distributed runtime, which uses cluster computing to match patterns and collect statistics simultaneously across many machines. This is implemented in a specialized distributed framework for parsing and text analysis built on top of Hadoop (D. Cutting et al., ). Oceanography programs compile down to distributed programs which run in this cluster environment, allowing the N</context>
</contexts>
<marker>Waterman, 2009</marker>
<rawString>Scott A. Waterman. 2009. Distributed parse mining. In Software engineering, testing, and quality assurance for natural language processing (SETQA-NLP 2009).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>