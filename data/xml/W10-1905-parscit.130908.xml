<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001066">
<title confidence="0.996926">
A Comparative Study of Syntactic Parsers for Event Extraction
</title>
<author confidence="0.934784">
Makoto Miwa&apos; Sampo Pyysalo&apos; Tadayoshi Hara&apos; Jun’ichi Tsujii&apos;,2,3
</author>
<affiliation confidence="0.96689825">
&apos;Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
</affiliation>
<email confidence="0.999154">
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994948">
The extraction of bio-molecular events
from text is an important task for a number
of domain applications such as pathway
construction. Several syntactic parsers
have been used in Biomedical Natural
Language Processing (BioNLP) applica-
tions, and the BioNLP 2009 Shared Task
results suggest that incorporation of syn-
tactic analysis is important to achieving
state-of-the-art performance. Direct com-
parison of parsers is complicated by to dif-
ferences in the such as the division be-
tween phrase structure- and dependency-
based analyses and the variety of output
formats, structures and representations ap-
plied. In this paper, we present a task-
oriented comparison of five parsers, mea-
suring their contribution to bio-molecular
event extraction using a state-of-the-art
event extraction system. The results show
that the parsers with domain models using
dependency formats provide very similar
performance, and that an ensemble of dif-
ferent parsers in different formats can im-
prove the event extraction system.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993217948718">
Bio-molecular events are useful for modeling and
understanding biological systems, and their au-
tomatic extraction from text is one of the key
tasks in Biomedical Natural Language Process-
ing (BioNLP). In the BioNLP 2009 Shared Task
on event extraction, participants constructed event
extraction systems using a variety of different
parsers, and the results indicated that the use of
a parser was correlated with high ranking in the
task (Kim et al., 2009). By contrast, the results
did not indicate a clear preference for a particular
parser, and there has so far been no direct compar-
ison of different parsers for event extraction.
While the outputs of parsers applying the same
out format can be compared using a gold standard
corpus, it is difficult to perform meaningful com-
parison of parsers applying different frameworks.
Additionally, it is still an open question to what ex-
tent high performance on a gold standard treebank
correlates with usefulness at practical tasks. Task-
based comparisons of parsers provide not only a
way to asses parsers across frameworks but also a
necessary measure of their practical applicability.
In this paper, five different parsers are com-
pared on the bio-molecular event extraction task
defined in the BioNLP 2009 Shared Task using a
state-of-the-art event extraction system. The data
sets share abstracts with GENIA treebank, and the
treebank is used as an evaluation standard. The
outputs of the parsers are converted into two de-
pendency formats with the help of existing conver-
sion methods, and the outputs are compared in the
two dependency formats. The evaluation results
show that different syntactic parsers with domain
models in the same dependency format achieve
closely similar performance, and that an ensemble
of different syntactic parsers in different formats
can improve the performance of an event extrac-
tion system.
</bodyText>
<sectionHeader confidence="0.973552" genericHeader="method">
2 Bio-molecular Event Extraction with
Several Syntactic Parsers
</sectionHeader>
<bodyText confidence="0.9997576">
This paper focuses on the comparison of several
syntactic parsers on a bio-molecular event extrac-
tion task with a state-of-the-art event extraction
system. This section explains the details of the
comparison. Section 2.1 presents the event ex-
</bodyText>
<page confidence="0.990856">
37
</page>
<note confidence="0.9792675">
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 37–45,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999329285714286">
traction task setting, following that of the BioNLP
2009 Shared Task. Section 2.2 then summa-
rizes the five syntactic parsers and three formats
adopted for the comparison. Section 2.3 described
how the state-of-the-art event extraction system of
Miwa et al. (2010) is modified and used for the
comparison.
</bodyText>
<subsectionHeader confidence="0.979619">
2.1 Bio-molecular Event Extraction
</subsectionHeader>
<bodyText confidence="0.999464444444445">
The bio-molecular event extraction task consid-
ered in this study is that defined in the BioNLP
2009 Shared Task (Kim et al., 2009)1. The shared
task provided common and consistent task defi-
nitions, data sets for training and evaluation, and
evaluation criteria. The shared task consists of
three subtasks: core event extraction (Task 1),
augmenting events with secondary arguments
(Task 2), and the recognition of speculation and
negation of the events (Task 3) (Kim et al., 2009).
In this paper we consider Task 1 and Task 2. The
shared task defined nine event types, which can be
divided into five simple events (Gene expression,
Transcription, Protein catabolism, Phosphoryla-
tion, and Localization) that take one core argu-
ment, a multi-participant binding event (Bind-
ing), and three regulation events (Regulation, Pos-
itive regulation, and Negative regulation) that can
take other events as arguments.
In the two tasks considered, events are repre-
sented with a textual trigger, type, and arguments,
where the trigger is a span of text that states the
event in text. In Task 1 the event arguments that
need to be extracted are restricted to the core ar-
guments Theme and Cause, and secondary argu-
ments (locations and sites) need to be attached in
Task 2.
</bodyText>
<subsectionHeader confidence="0.999517">
2.2 Parsers and Formats
</subsectionHeader>
<bodyText confidence="0.994851571428571">
Five parsers and three formats are adopted for
the evaluation. The parsers are GDep (Sagae and
Tsujii, 2007)2, the Bikel parser (Bikel) (Bikel,
2004)3, the Charniak-Johnson reranking parser,
using David McClosky’s self-trained biomedi-
cal parsing model (MC) (McClosky, 2009)4, the
C&amp;C CCG parser, adapted to biomedical text
</bodyText>
<footnote confidence="0.993164875">
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
2http://www.cs.cmu.edu/-sagae/parser/
gdep/
3http://www.cis.upenn.edu/-dbikel/
software.html
4http://www.cs.brown.edu/-dmcc/
biomedical.html
</footnote>
<figureCaption confidence="0.9415357">
Figure 1: Stanford basic dependency tree
Figure 2: CoNLL-X dependency tree
Figure 3: Predicate Argument Structure
Figure 4: Format conversion dependencies in five
parsers. Formats adopted for the evaluation is
shown in solid boxes. SD: Stanford Dependency
format, CCG: Combinatory Categorial Grammar
output format, PTB: Penn Treebank format, and
PAS: Predicate Argument Structure in Enju for-
mat.
</figureCaption>
<bodyText confidence="0.9975306">
(C&amp;C) (Rimell and Clark, 2009)5, and the Enju
parser with the GENIA model (Miyao et al.,
2009)6. The formats are Stanford Dependencies
(SD) (Figure 1), the CoNLL-X dependency for-
mat (Figure 2) and the predicate-argument struc-
ture (PAS) format used by Enju (Figure 3). With
the exception of Enju, the analyses of these parsers
were provided by the BioNLP 2009 Shared Task
organizers. Analysis of system features in the task
found that the use of parser output with one of
</bodyText>
<footnote confidence="0.99916225">
5http://svn.ask.it.usyd.edu.au/trac/
candc/
6http://www-tsujii.is.s.u-tokyo.ac.jp/
enju/
</footnote>
<page confidence="0.999519">
38
</page>
<bodyText confidence="0.999977666666667">
the formats considered here correlated with high
rank at the task (Kim et al., 2009). A number of
these parsers have also been shown to be effective
for protein-protein interactions extraction (Miyao
et al., 2009).
The five parsers operate in a number of different
frameworks, reflected in their analyses. GDep is a
native dependency parser that produces CoNLL-
X-format dependency trees. MC and Bikel are
phrase-structure parsers, and they produce Penn
Treebank (PTB) format analyses. C&amp;C is a deep
parser based on Combinatory Categorial Gram-
mar (CCG), and its native output is in a CCG-
specific format. The output of C&amp;C is converted
into SD by a rule-based conversion script (Rimell
and Clark, 2009). Enju is deep parser based on
Head-driven Phrase Structure Grammar (HPSG)
and produces a format containing predicate argu-
ment structures (PAS) along with a phrase struc-
ture tree in Enju format.
To study the contribution of the formats in
which the five parsers output their analyses to task
performance, we apply a number of conversions
between the outputs, shown in Figure 4. The Enju
PAS output is converted into Penn Treebank for-
mat using the method introduced by (Miyao et al.,
2009). SD is generated from PTB by the Stan-
ford tools (de Marneffe et al., 2006)7, and CoNLL-
X dependencies are generated from PTB by us-
ing Treebank Converter (Johansson and Nugues,
2007)8. We note that all of these conversions can
introduce some errors in the conversion process.
With the exception of Bikel, all the applied
parsers have models specifically adapted for
biomedical text. Further, all of the biomedical do-
main models have been created with reference and
for many parsers with direct training on the data
of (a subset of) the GENIA treebank (Tateisi et
al., 2005). The results of parsing with these mod-
els as provided for the BioNLP Shared Task are
used in this comparison. However, we note that
the shared task data, drawn from the GENIA event
corpus (Kim et al., 2008), contains abstracts that
are also in the GENIA treebank. This implies that
the parsers are likely to perform better on the texts
used in the shared task than on other biomedical
domain text, and similarly that systems building
on their output are expected to achieve best per-
</bodyText>
<footnote confidence="0.99377075">
7http://www-nlp.stanford.edu/software/
lex-parser.shtml
8http://nlp.cs.lth.se/software/
treebank converter/
</footnote>
<bodyText confidence="0.9997252">
formance on this data. However, it does not in-
validate comparison within the dataset. We fur-
ther note that the models do not incorporate any
knowledge of the event annotations of the shared
task.
</bodyText>
<subsectionHeader confidence="0.999654">
2.3 Event Extraction System
</subsectionHeader>
<bodyText confidence="0.999995863636364">
The system by Miwa et al. (2010) is adopted for
the evaluation. The system was originally devel-
oped for finding core events (Task 1 in the BioNLP
2009 Shared Task) using Enju and GDep with the
native output of these parsers. The system con-
sists of three supervised classification-based mod-
ules: a trigger detector, an event edge detector,
and a complex event detector. The trigger detec-
tor classifies each word into the appropriate event
types, the event edge detector classifies each edge
between an event and a protein into an argument
type, and the complex event detector classifies
event candidates constructed by all edge combina-
tions, deciding between event and non-event. The
system uses one-vs-all support vector machines
(SVMs) for the classifications.
The system operates on one sentence at a time,
building features for classification based on the
syntactic analyses for the sentence provided by
the two parsers as well as the sequence of the
words in the sentence, including the target candi-
date. The features include the constituents/words
around entities (triggers and proteins), the depen-
dencies, and the shortest paths among the enti-
ties. The feature generation is format-independent
regarding the shared properties of different for-
mats, but makes use also of format-specific infor-
mation when available for extracting features, in-
cluding the dependency tags, word-related infor-
mation (e.g. a lexical entry in Enju format), and
the constituents and their head information.
The previously introduced base system is here
improved with two modifications. One modifica-
tion is removing two classes of features from the
original features (for details of the original feature
representation, we refer to (Miwa et al., 2010));
specifically the features representing governor-
dependent relationships from the target word, and
the features representing each event edges in the
complex event detector are removed. The other
modification is to use head words in a trigger ex-
pression as a gold trigger word. This modification
is inspired by the part-of-speech (POS) based se-
lection proposed by Kilicoglu and Bergler (2009).
</bodyText>
<page confidence="0.996577">
39
</page>
<bodyText confidence="0.999974052631579">
The system uses a head word “in” as a trigger
word in a trigger expression “in the presence of”
instead of using all the words of the expression.
In cases where there is no head word information
in a parser output, head words are selected heuris-
tically: if a word does not modify another word
in the trigger expression, the word is selected as a
head word.
The system is also modified to find secondary
arguments (Task 2 in the BioNLP 2009 Shared
Task). The second arguments are treated as ad-
ditional arguments in Task 1: the trigger detec-
tor finds secondary argument candidates, the event
edge detector finds secondary argument edge can-
didates, and the complex event detector finds
events including secondary arguments. The fea-
tures are extracted using the same feature extrac-
tion method as for regulation events taking pro-
teins as arguments.
</bodyText>
<sectionHeader confidence="0.992939" genericHeader="method">
3 Evaluation Setting
</sectionHeader>
<bodyText confidence="0.99951724">
Event extraction performance is evaluated using
the evaluation script provided by the BioNLP’09
shared task organizers9 for the development data
set, and the online evaluation system of the tasks0
for the test data set. Results are reported under
the official evaluation criterion of the task, i.e. the
“Approximate Span Matching/Approximate Re-
cursive Matching” criterion. Task 1 and Task 2
are solved at once for the evaluation.
As discussed in Section 2.2, the texts of the GE-
NIA treebank are shared with the shared task data
sets, which allows the gold annotations of the tree-
bank to be used for reference. The GENIA tree-
bank is converted into the Enju format with Enju.
When the trees in the treebank cannot be converted
into the Enju format, parse results are used in-
stead. The GENIA treebank is also converted into
PTB formatss. The treebank is then converted into
the dependency formats with the conversions de-
scribed in Section 2.2. While based on manually
annotated gold data, the converted treebanks are
not always correct due to conversion errors.
The event extraction system described in Sec-
tion 2.3 is used with the default settings shown in
(Miwa et al., 2010). The positive and negative ex-
</bodyText>
<footnote confidence="0.996861333333333">
9http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/downloads.shtml
10http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/eval-test.shtml
11http://categorizer.tmit.bme.hu/
~illes/genia ptb/
</footnote>
<table confidence="0.999397666666667">
BD CD CDP CTD
Task 1 55.60 54.35 54.59 54.42
Task 2 53.94 52.65 52.88 52.76
</table>
<tableCaption confidence="0.999571">
Table 1: Comparison of the F-score results with
</tableCaption>
<bodyText confidence="0.951064142857143">
different Stanford dependency variants on the de-
velopment data set with the MC parser. Results for
basic dependencies (BD), collapsed dependencies
(CD), collapsed dependencies with propagation of
conjunct dependencies (CDP), and collapsed tree
dependencies (CTD) are shown. The best score in
each task is shown in bold.
</bodyText>
<figureCaption confidence="0.8983475">
Figure 5: Stanford collapsed dependencies with
propagation of conjunct dependencies
</figureCaption>
<bodyText confidence="0.970843545454546">
amples are balanced by placing more weight on
the positive examples. The examples predicted
with confidence greater than 0.5, as well as the
examples with the most confident labels, are ex-
tracted. The C-values of SVMs are set to 1.0.
Some of the parse results do not include word
base forms or part-of-speech (POS) tags, which
are required by the event extraction system. To
apply these parsers, the GENIA Tagger (Tsuruoka
et al., 2005) output is adopted to add this informa-
tion to the results.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9994326">
Results of event extraction with the setting in Sec-
tion 2.3 will be presented in this section. Sec-
tion 4.1 considers the effect of different variants
of the Stanford Dependency representation. Sec-
tion 4.2 presents the results of experiments with
different parsers, and Section 4.3 shows the per-
formance with ensembles of multiple parsers. Fi-
nally, the performance of the event extraction sys-
tem is discussed in context of other proposed
methods for the task in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.998646">
4.1 Stanford Dependency Setting
</subsectionHeader>
<bodyText confidence="0.9998446">
Stanford dependencies have four different vari-
ants: basic dependencies (BD), collapsed depen-
dencies (CD), collapsed dependencies with prop-
agation of conjunct dependencies (CDP), and col-
lapsed tree dependencies (CTD) (de Marneffe and
</bodyText>
<page confidence="0.99023">
40
</page>
<table confidence="0.9996726">
BD CD CDP CTD
Task 1 54.22 54.37 53.88 53.84
(-1.38) (+0.02) (-0.71) (-0.58)
Task 2 52.73 52.80 52.31 52.35
(-1.21) (+0.15) (-0.57) (-0.41)
</table>
<tableCaption confidence="0.790367333333333">
Table 2: Comparison of the F-score results with
different Stanford dependency variants without
dependency types.
</tableCaption>
<bodyText confidence="0.998613">
Manning, 2008). Except for BD, these variants do
not necessarily connect all the words in the sen-
tence, and CD and CDP do not necessarily form
a tree structure. Figure 5 shows an example of
CDP converted from the tree in Figure 1. To se-
lect a suitable alternative for the comparative ex-
periments, we first compared these variants as a
preliminary experiment. Table 1 shows the com-
parison results with the MC parser. Dependencies
are generalized by removing expressions after “ ”
of the dependencies (e.g. “ with” in prep with) for
better performance. We find that basic dependen-
cies give the best performance to event extraction,
with little difference between the other variants.
This result is surprising, as variants other than ba-
sic have features such as the resolution of con-
junctions that are specifically designed for prac-
tical applications. However, basic dependenden-
cies were found to consistently provide best per-
formance also for the other parsers12.
The SD variants differ from each other in two
key aspects: the dependency structure and the de-
pendency types. To gain insight into why the
basic dependencies should provide better perfor-
mance than other variants, we performed an ex-
periment attempting to isolate these factors by re-
peating the evaluation while eliminating the de-
pendency types. The results of this evaluation are
shown in Table 2. The results indicate that the
contribution of the dependency types to extraction
performance differs between the variants: the ex-
pected performance drop is most notable for the
basic dependencies, and for the collapsed depen-
dencies there is even a minute increase in per-
formance, making results for collapsed dependen-
cies best of the untyped results (by a very narrow
margin). While this result doesn’t unambiguously
point to a specific explanation for why basic de-
pendencies provide best performance when types
</bodyText>
<footnote confidence="0.6325775">
12Collapsed tree dependencies are not evaluated on the
C&amp;C parser since the conversion is not provided.
</footnote>
<bodyText confidence="0.9996255">
are not removed, possible explanations include er-
rors in typing or sparseness issues causing prob-
lems in generalization for the types of non-basic
dependencies. While achieving a clear resolution
of the results of the comparison between SD vari-
ants requires more analysis, from a performance
optimization perspective the results present an un-
complicated choice. Thus, in the following eval-
uation, the basic dependencies are adopted for all
SD results.
</bodyText>
<subsectionHeader confidence="0.984789">
4.2 Parser Comparison
</subsectionHeader>
<bodyText confidence="0.999978692307693">
Results with different parsers and different for-
mats on the development data set are summarized
in Table 3. Baseline results are produced by re-
moving dependency (or PAS) information from
the parse results. The baseline results differ be-
tween the represetations as the word base forms
and POS tags produced by the GENIA tagger for
use with the Stanford dependency and CoNLL-
X formats are different from those for Enju, and
because head word information in Enju format is
used. The evaluation finds best results for both
tasks with Enju, using its native output format.
However, as discussed in Section 2.3, the treat-
ment of the Enju format and the other two formats
are slightly different, this result does not necessar-
ily indicate that the Enju format is the best alter-
native for event extraction.
Unsurprisingly, we find that the Bikel parser,
the only one in the comparison lacking a model
adapted to the biomedical domain, performs worse
than the other parsers. For SD, we find best results
for C&amp;C, which is notable as the parser output is
processed into SD by a custom conversion, while
MC output uses the de facto conversion of the
Stanford tools. Similarly, MC produces the best
result for the CoNLL-X format, which is the na-
tive output format of GDep. Enju and GDep pro-
duces comparable results to the best formats for
both tasks. Overall, we find that event extraction
results for the parsers applying GENIA treebank
models are largely comparable for the dependency
formats (SD and CoNLL-X).
The results with the data derived from the GE-
NIA treebank can be considered as upper bounds
for the parsers and formats at the task, although
conversion errors are expected to lower these
bounds to some extent. Even though trained on
the treebank, using the parsers does not provide
performance as high as that for using the GE-
</bodyText>
<page confidence="0.99921">
41
</page>
<table confidence="0.999080777777778">
Task 1 Task 2
SD CoNLL PAS SD CoNLL PAS
Baseline 51.05 - 50.42 49.17 - 48.88
GDep - 55.70 - - 54.37 -
Bikel 53.29 53.22 - 51.40 51.27 -
MC 55.60 56.01 - 53.94 54.51 -
C&amp;C 56.09 - - 54.27 - -
Enju 55.48 55.74 56.57 54.06 54.37 55.31
GENIA 56.34 56.09 57.94 55.04 54.57 56.40
</table>
<tableCaption confidence="0.99479">
Table 3: Comparison of F-score results with five parsers in three different formats on the development
</tableCaption>
<bodyText confidence="0.7535592">
data set. SD: Stanford basic Dependency format, CoNLL: CoNLL-X format, and PAS: Predicate Argu-
ment Structure in Enju format. Results without dependency (or PAS) information are shown as baselines.
The results with the GENIA treebank (converted into PTB format and Enju format) are shown for com-
parison (GENIA). The best score in each task is shown in bold, and the best score in each task and format
is underlined.
</bodyText>
<table confidence="0.998485111111111">
Task 1 Task 2
C&amp;C MC Enju C&amp;C MC Enju
SD CoNLL CoNLL SD CoNLL CoNLL
MC 57.44 - - 55.75 - -
CoNLL (+1.35) - - (+1.24) - -
Enju 56.47 56.24 - 54.85 54.70 -
CoNLL (+0.38) (+0.23) - (+0.48) (+0.19) -
Enju 57.20 57.78 56.59 55.75 56.39 55.12
PAS (+0.63) (+1.21) (+0.02) (+0.44) (+1.08) (-0.19)
</table>
<tableCaption confidence="0.996237">
Table 4: Comparison of the F-score results with parser ensembles on the development data set. C&amp;C
</tableCaption>
<bodyText confidence="0.923566944444444">
with Stanford basic Dependency format, MC with CoNLL-X format, Enju with CoNLL-X format, and
Enju with Predicate Argument Structure in Enju format are used for the parser ensemble. The changes
from single-parser results are shown in parentheses. The best score in each task is shown in bold.
NIA treebank, but in many cases results with the
parsers are only slightly worse than results with
the treebank. The results suggest that there is rela-
tive little remaining benefit to be gained for event
extraction from improving parser performance.
This supports the claim that most of the errors in
event extraction are not caused by the parse er-
rors in (Miwa et al., 2010). Experiments using the
CoNLL-X format produce slightly worse results
than for SD with the gold treebank data, which is
at variance with the indication from parser-based
results with MC and Enju. Thus, the results do not
provide any systematic indication suggesting that
one dependency format would be superior to the
other in use for event extraction.
</bodyText>
<subsectionHeader confidence="0.994055">
4.3 Event Extraction with Parser Ensemble
</subsectionHeader>
<bodyText confidence="0.99993744">
The four parser outputs were selected for the eval-
uation of a parser ensemble: C&amp;C with Stan-
ford basic Dependency format, MC with CoNLL-
X format, Enju with CoNLL-X format, and Enju
with Predicate Argument Structure in Enju format.
Table 4 summarizes the parser ensemble results.
We find that all ensembles of different parsers in
different formats produce better results than those
for single parser outputs (Table 3); by contrast, the
results indicate that ensembles of the same formats
(MC + Enju in CoNLL-X format) or parsers (Enju
in CoNLL-X and Enju formats) produce relatively
small improvements, may in some cases even re-
duce performance. The results thus indicate that
while a parser ensemble can be effective but that it
is important to apply different parsers in different
formats.
Table 5 shows detailed results with three parsers
with three different formats. The ensembles sys-
tematically improve F-scores in regulation and the
overall performance (“All”), but the ensembles
can degrade the performance for simple and bind-
ing events. Different parser outputs are shown
to have their strengths and weaknesses in differ-
ent event groups. The use of Enju, for exam-
</bodyText>
<page confidence="0.997947">
42
</page>
<table confidence="0.966070714285714">
Simple Binding Regulation All
Task 1
BL-E 75.85 / 71.09 / 73.39 40.32 / 38.17 / 39.22 30.65 / 48.16 / 37.46 46.12 / 55.60 / 50.42
BL-G 76.03 / 73.48 / 74.73 40.32 / 38.17 / 39.22 33.50 / 45.95 / 38.75 47.74 / 54.86 / 51.05
C 78.89 / 78.43 / 78.66 48.79 / 43.37 / 45.92 37.17 / 54.07 / 44.06 51.82 / 61.12 / 56.09
M 79.79 / 77.12 / 78.43 43.95 / 41.13 / 42.50 39.41 / 52.94 / 45.18 52.66 / 59.82 / 56.01
E 79.79 / 76.07 / 77.88 45.16 / 43.75 / 44.44 40.12 / 53.68 / 45.92 53.21 / 60.38 / 56.57
C+M 80.50 / 79.05 / 79.77 48.39 / 42.25 / 45.11 41.85 / 53.17 / 46.84 54.84 / 60.31 / 57.44
C+E 79.79 / 76.46 / 78.09 47.98 / 45.59 / 46.76 41.04 / 53.66 / 46.51 54.11 / 60.66 / 57.20
E+M 80.50 / 77.15 / 78.79 44.35 / 42.97 / 43.65 42.26 / 55.63 / 48.03 54.50 / 61.49 / 57.78
C+E+M 80.14 / 77.07 / 78.58 51.61 / 42.95 / 46.89 42.46 / 54.30 / 47.66 55.51 / 60.27 / 57.79
Task 2
BL-E 74.60 / 69.10 / 71.75 36.55 / 34.73 / 35.62 29.89 / 47.20 / 36.60 44.74 / 53.86 / 48.88
BL-G 74.42 / 71.31 / 72.83 36.55 / 33.33 / 34.87 32.52 / 44.83 / 37.70 46.13 / 52.64 / 49.17
C 77.64 / 76.77 / 77.20 43.78 / 38.79 / 41.13 36.17 / 52.89 / 42.96 50.14 / 59.14 / 54.27
M 78.71 / 75.95 / 77.31 39.36 / 36.57 / 37.91 38.70 / 52.12 / 44.42 51.25 / 58.21 / 54.51
E 79.07 / 75.26 / 77.12 41.37 / 40.08 / 40.71 39.31 / 52.86 / 45.09 51.98 / 59.10 / 55.31
C+M 79.61 / 78.03 / 78.81 43.37 / 36.99 / 39.93 40.93 / 52.07 / 45.83 53.31 / 58.41 / 55.75
C+E 78.89 / 75.34 / 77.08 44.18 / 40.89 / 42.47 40.22 / 52.86 / 45.68 52.81 / 59.04 / 55.75
E+M 79.79 / 76.33 / 78.02 40.16 / 38.76 / 39.45 41.34 / 54.69 / 47.09 53.15 / 60.05 / 56.39
C+E+M 79.43 / 76.25 / 77.81 46.18 / 37.46 / 41.37 41.54 / 53.39 / 46.72 53.98 / 58.45 / 56.13
</table>
<tableCaption confidence="0.995047">
Table 5: Comparison of Recall / Precision / F-score results on the development data set. C&amp;C with Stan-
</tableCaption>
<bodyText confidence="0.74136425">
ford basic Dependency format (C), MC with CoNLL-X format (M), and Enju with Predicate Argument
Structure in Enju format (E) are used for the evaluation. Results with Enju output without PAS informa-
tion (BL-E) and the GENIA tagger output (BL-G) are shown as baselines. Results on simple, binding,
regulation, and all events are shown. The best score in each result is shown in bold.
</bodyText>
<table confidence="0.816208833333333">
Simple Binding Regulation All
Task 1
Ours 67.09 / 77.59 / 71.96 49.57 / 51.65 / 50.59 38.42 / 53.95 / 44.88 50.28 / 63.19 / 56.00
Miwa 65.31 / 76.44 / 70.44 52.16 / 53.08 / 52.62 35.93 / 46.66 / 40.60 48.62 / 58.96 / 53.29
Bj¨orne 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
Riedel N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35
</table>
<equation confidence="0.964908">
Task 2
65.77 / 75.29 / 70.21
N/A
47.56 / 49.55 / 48.54
22.35 / 46.99 / 30.29
38.24 / 53.57 / 44.62
25.75 / 40.75 / 31.56
49.48 / 61.87 / 54.99
35.86 / 54.08 / 43.12
</equation>
<bodyText confidence="0.933860086956522">
Ours
Riedel
Table 6: Comparison of Recall / Precision / F-score results on the test data set. MC with CoNLL-X
format and Enju with Predicate Argument Structure in Enju format are used for the evaluation. Results
on simple, binding, regulation, and all events are shown. Results by Miwa et al. (2010) (Miwa), Bj¨orne
et al. (2009) (Bj¨orne), and Riedel et al. (2009) (Riedel) for Task 1 and Task 2 are shown for comparison.
The best score in each result is shown in bold.
ple, is good for extracting regulation events, but
produced weaker results for simple events. The
ensembles of two parser outputs inherit both the
strengths and weaknesses of the outputs in most
cases, and the strengths and weaknesses of the en-
sembles vary depending on the combined parser
outputs. The differences in performance between
ensembles of the outputs of two parsers to the en-
semble of the three parser outputs are +0.01 for
Task 1, and -0.26 for Task 2. This result sug-
gests that adding more different parsers does not
always improve the performance. The ensemble
of three parser outputs, however, shows stable per-
formance across categories, scoring in the top two
for binding, regulation, and all events, in the top
four for simple events.
</bodyText>
<page confidence="0.999027">
43
</page>
<subsectionHeader confidence="0.986133">
4.4 Performance of Event Extraction System
</subsectionHeader>
<bodyText confidence="0.99997645">
Table 6 shows a comparison of performance on
the shared task test data. MC with CoNLL-X for-
mat and Enju with Predicate Argument Structure
in Enju format are used for the evaluation, select-
ing one of the best performing ensemble settings
in Section 4.3. The performance of the best sys-
tems in the original shared task is shown for refer-
ence ((Bj¨orne et al., 2009) in Task 1 and (Riedel
et al., 2009) in Task 2). The event extraction
system with our modifications performed signifi-
cantly better than the best systems in the shared
task, further outperforming the original system
by Miwa et al. (2010). This result shows that
the system applied for the comparison of syntac-
tic parsers achieves state-of-the-art performance at
event extraction. This result also shows that the
system originally developed only for core events
extraction can be easily extended for other argu-
ments simply by treating the other arguments as
additional arguments.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999986333333333">
Many approaches for parser comparison have been
proposed in the BioNLP field. Most compar-
isons have used gold treebanks with intermediate
formats (Clegg and Shepherd, 2007; Pyysalo et
al., 2007). Application-oriented parser compari-
son across several formats was first introduced by
Miyao et al. (2009), who compared eight parsers
and five formats for the protein-protein interaction
(PPI) extraction task. PPI extraction, the recog-
nition of binary relations of between proteins, is
one of the most basic information extraction tasks
in the BioNLP field. Our findings do not con-
flict with those of Miyao et al. Event extraction
can be viewed as an additional extrinsic evalua-
tion task for syntactic parsers, providing more reli-
able and evaluation and a broader perspective into
parser performance. An additional advantage of
application-oriented evaluation on BioNLP shared
task data is the availability of a manually anno-
tated gold standard treebank, the GENIA treebank,
that covers the same set of abstracts as the task
data. This allows the gold treebank to be consid-
ered as an evaluation standard, in addition to com-
parison of performance in the primary task.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995276595745">
We compared five parsers and three formats on a
bio-molecular event extraction task with a state-
of-the-art event extraction system. The specific
task considered was the BioNLP shared task, al-
lowing the use of the GENIA treebank as a gold
standard parse reference. The event extraction sys-
tem, modified for a higher performance and an ad-
ditional subtask, showed high performance on the
shared task subtasks considered. Four of the five
considered parsers were applied using biomedi-
cal models trained on the GENIA treebank, and
they were found to produce similar performance.
Parser ensembles were further shown to allow im-
provement of the performance of the event extrac-
tion system.
The contributions of this paper are 1) the com-
parison of several commonly used parsers on the
event extraction task with a gold treebank, 2)
demonstration of the usefulness of the parser en-
semble on the task, and 3) the introduction of a
state-of-the-art event extraction system. One lim-
itation of this study is that the comparison be-
tween the parsers is not perfect, as the format con-
versions miss some information from the origi-
nal formats and results with different formats de-
pend on the ability of the event extraction sys-
tem to take advantage of their strengths. To max-
imize comparability, the system was designed to
extract features identically from similar parts of
the dependency-based formats, further adding in-
formation provided by other formats, such as the
lexical entries of the Enju format, from external re-
sources. The results of this paper are expected to
be useful as a guide not only for parser selection
for biomedical information extraction but also for
the development of event extraction systems.
The selection of compared parsers and formats
in the present evaluation is somewhat limited. As
future work, it would be informative to extend
the comparison to other syntactic representations,
such as the PTB format. Finally, the evaluation
showed that the system fails to recover approxi-
mately 40% of events even when provided with
manually annotated treebank data, showing that
other methods and resources need to be adopted
to further improve bio-molecular event extraction
systems. Such improvement is left as future work.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.78479525">
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Scientific Research (C) (General) (MEXT, Japan).
</bodyText>
<page confidence="0.999056">
44
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880517647059">
Daniel M. Bikel. 2004. A distributional analysis of a
lexicalized statistical parsing model. In In EMNLP,
pages 182–189.
Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP’09
Shared Task on Event Extraction, pages 10–18.
Andrew B. Clegg and Adrian J. Shepherd. 2007.
Benchmarking natural-language parsers for biolog-
ical applications using dependency graphs. BMC
Bioinformatics, 8.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, September.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings ofNODALIDA 2007, Tartu,
Estonia, May 25-26.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic
dependency based heuristics for biological event ex-
traction. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages
119–127, Boulder, Colorado, June. Association for
Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of bionlp’09 shared task on event extraction. In
BioNLP ’09: Proceedings of the Workshop on
BioNLP, pages 1–9.
David McClosky. 2009. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer Sci-
ence, Brown University.
Makoto Miwa, Rune Sætre, Jin-Dong Kim, and
Jun’ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology
(JBCB), 8(1):131–146, February.
Yusuke Miyao, Kenji Sagae, Rune Sætre, Takuya
Matsuzaki, and Jun ichi Tsujii. 2009. Evalu-
ating contributions of natural language parsers to
protein-protein interaction extraction. Bioinformat-
ics, 25(3):394–400.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007. On the unification of syntactic annotations
under the stanford dependency scheme: A case
study on bioinfer and genia. In Biological, transla-
tional, and clinical language processing, pages 25–
32, Prague, Czech Republic, June. Association for
Computational Linguistics.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In BioNLP ’09:
Proceedings of the Workshop on BioNLP, pages 41–
49, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical do-
main. J. ofBiomedical Informatics, 42(5):852–865.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In EMNLP-CoNLL 2007.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Jun-
fichi Tsujii. 2005. Syntax annotation for the genia
corpus. In Proceedings of the IJCNLP 2005, Com-
panion volume, pages 222–227, Jeju Island, Korea,
October.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun’ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Panayio-
tis Bozanis and Elias N. Houstis, editors, Panhel-
lenic Conference on Informatics, volume 3746 of
Lecture Notes in Computer Science, pages 382–392.
Springer.
</reference>
<page confidence="0.999389">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847714">
<title confidence="0.964518">A Comparative Study of Syntactic Parsers for Event Extraction</title>
<affiliation confidence="0.916454">of Computer Science, the University of Tokyo,</affiliation>
<address confidence="0.922063">Hongo 7-3-1, Bunkyo-ku, Tokyo,</address>
<affiliation confidence="0.993736">of Computer Science, University of Manchester, Center for Text Mining,</affiliation>
<abstract confidence="0.999674884615385">The extraction of bio-molecular events from text is an important task for a number of domain applications such as pathway construction. Several syntactic parsers have been used in Biomedical Natural Language Processing (BioNLP) applications, and the BioNLP 2009 Shared Task results suggest that incorporation of syntactic analysis is important to achieving state-of-the-art performance. Direct comparison of parsers is complicated by to differences in the such as the division between phrase structureand dependencybased analyses and the variety of output formats, structures and representations applied. In this paper, we present a taskoriented comparison of five parsers, measuring their contribution to bio-molecular event extraction using a state-of-the-art event extraction system. The results show that the parsers with domain models using dependency formats provide very similar performance, and that an ensemble of different parsers in different formats can improve the event extraction system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>A distributional analysis of a lexicalized statistical parsing model.</title>
<date>2004</date>
<booktitle>In In EMNLP,</booktitle>
<pages>182--189</pages>
<contexts>
<context position="5526" citStr="Bikel, 2004" startWordPosition="851" endWordPosition="852"> Positive regulation, and Negative regulation) that can take other events as arguments. In the two tasks considered, events are represented with a textual trigger, type, and arguments, where the trigger is a span of text that states the event in text. In Task 1 the event arguments that need to be extracted are restricted to the core arguments Theme and Cause, and secondary arguments (locations and sites) need to be attached in Task 2. 2.2 Parsers and Formats Five parsers and three formats are adopted for the evaluation. The parsers are GDep (Sagae and Tsujii, 2007)2, the Bikel parser (Bikel) (Bikel, 2004)3, the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009)4, the C&amp;C CCG parser, adapted to biomedical text 1http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2http://www.cs.cmu.edu/-sagae/parser/ gdep/ 3http://www.cis.upenn.edu/-dbikel/ software.html 4http://www.cs.brown.edu/-dmcc/ biomedical.html Figure 1: Stanford basic dependency tree Figure 2: CoNLL-X dependency tree Figure 3: Predicate Argument Structure Figure 4: Format conversion dependencies in five parsers. Formats adopted for the evaluation is shown in solid box</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. A distributional analysis of a lexicalized statistical parsing model. In In EMNLP, pages 182–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jari Bj¨orne</author>
<author>Juho Heimonen</author>
<author>Filip Ginter</author>
<author>Antti Airola</author>
<author>Tapio Pahikkala</author>
<author>Tapio Salakoski</author>
</authors>
<title>Extracting complex biological events with rich graphbased feature sets.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP’09 Shared Task on Event Extraction,</booktitle>
<pages>10--18</pages>
<marker>Bj¨orne, Heimonen, Ginter, Airola, Pahikkala, Salakoski, 2009</marker>
<rawString>Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2009. Extracting complex biological events with rich graphbased feature sets. In Proceedings of the BioNLP’09 Shared Task on Event Extraction, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Clegg</author>
<author>Adrian J Shepherd</author>
</authors>
<title>Benchmarking natural-language parsers for biological applications using dependency graphs.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<contexts>
<context position="28687" citStr="Clegg and Shepherd, 2007" startWordPosition="4733" endWordPosition="4736">best systems in the shared task, further outperforming the original system by Miwa et al. (2010). This result shows that the system applied for the comparison of syntactic parsers achieves state-of-the-art performance at event extraction. This result also shows that the system originally developed only for core events extraction can be easily extended for other arguments simply by treating the other arguments as additional arguments. 5 Related Work Many approaches for parser comparison have been proposed in the BioNLP field. Most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the recognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those of Miyao et al. Event extraction can be viewed as an additional extrinsic evaluation task for syntactic parsers, providing more reliable and evaluation and a broader perspective</context>
</contexts>
<marker>Clegg, Shepherd, 2007</marker>
<rawString>Andrew B. Clegg and Adrian J. Shepherd. 2007. Benchmarking natural-language parsers for biological applications using dependency graphs. BMC Bioinformatics, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford typed dependencies manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual. Technical report, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings ofNODALIDA 2007,</booktitle>
<pages>25--26</pages>
<location>Tartu, Estonia,</location>
<contexts>
<context position="8225" citStr="Johansson and Nugues, 2007" startWordPosition="1250" endWordPosition="1253"> Phrase Structure Grammar (HPSG) and produces a format containing predicate argument structures (PAS) along with a phrase structure tree in Enju format. To study the contribution of the formats in which the five parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 4. The Enju PAS output is converted into Penn Treebank format using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006)7, and CoNLLX dependencies are generated from PTB by using Treebank Converter (Johansson and Nugues, 2007)8. We note that all of these conversions can introduce some errors in the conversion process. With the exception of Bikel, all the applied parsers have models specifically adapted for biomedical text. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The results of parsing with these models as provided for the BioNLP Shared Task are used in this comparison. However, we note that the shared task data, drawn from the GENIA event corpus (Kim et al., 2008), </context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings ofNODALIDA 2007, Tartu, Estonia, May 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Syntactic dependency based heuristics for biological event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>119--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="11554" citStr="Kilicoglu and Bergler (2009)" startWordPosition="1774" endWordPosition="1777">viously introduced base system is here improved with two modifications. One modification is removing two classes of features from the original features (for details of the original feature representation, we refer to (Miwa et al., 2010)); specifically the features representing governordependent relationships from the target word, and the features representing each event edges in the complex event detector are removed. The other modification is to use head words in a trigger expression as a gold trigger word. This modification is inspired by the part-of-speech (POS) based selection proposed by Kilicoglu and Bergler (2009). 39 The system uses a head word “in” as a trigger word in a trigger expression “in the presence of” instead of using all the words of the expression. In cases where there is no head word information in a parser output, head words are selected heuristically: if a word does not modify another word in the trigger expression, the word is selected as a head word. The system is also modified to find secondary arguments (Task 2 in the BioNLP 2009 Shared Task). The second arguments are treated as additional arguments in Task 1: the trigger detector finds secondary argument candidates, the event edge </context>
</contexts>
<marker>Kilicoglu, Bergler, 2009</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2009. Syntactic dependency based heuristics for biological event extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 119–127, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus annotation for mining biomedical events from literature.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<pages>9--10</pages>
<contexts>
<context position="8823" citStr="Kim et al., 2008" startWordPosition="1353" endWordPosition="1356">n and Nugues, 2007)8. We note that all of these conversions can introduce some errors in the conversion process. With the exception of Bikel, all the applied parsers have models specifically adapted for biomedical text. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The results of parsing with these models as provided for the BioNLP Shared Task are used in this comparison. However, we note that the shared task data, drawn from the GENIA event corpus (Kim et al., 2008), contains abstracts that are also in the GENIA treebank. This implies that the parsers are likely to perform better on the texts used in the shared task than on other biomedical domain text, and similarly that systems building on their output are expected to achieve best per7http://www-nlp.stanford.edu/software/ lex-parser.shtml 8http://nlp.cs.lth.se/software/ treebank converter/ formance on this data. However, it does not invalidate comparison within the dataset. We further note that the models do not incorporate any knowledge of the event annotations of the shared task. 2.3 Event Extraction</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2008</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008. Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9:10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of bionlp’09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="1853" citStr="Kim et al., 2009" startWordPosition="268" endWordPosition="271">y formats provide very similar performance, and that an ensemble of different parsers in different formats can improve the event extraction system. 1 Introduction Bio-molecular events are useful for modeling and understanding biological systems, and their automatic extraction from text is one of the key tasks in Biomedical Natural Language Processing (BioNLP). In the BioNLP 2009 Shared Task on event extraction, participants constructed event extraction systems using a variety of different parsers, and the results indicated that the use of a parser was correlated with high ranking in the task (Kim et al., 2009). By contrast, the results did not indicate a clear preference for a particular parser, and there has so far been no direct comparison of different parsers for event extraction. While the outputs of parsers applying the same out format can be compared using a gold standard corpus, it is difficult to perform meaningful comparison of parsers applying different frameworks. Additionally, it is still an open question to what extent high performance on a gold standard treebank correlates with usefulness at practical tasks. Taskbased comparisons of parsers provide not only a way to asses parsers acro</context>
<context position="4228" citStr="Kim et al., 2009" startWordPosition="638" endWordPosition="641"> on Biomedical Natural Language Processing, ACL 2010, pages 37–45, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics traction task setting, following that of the BioNLP 2009 Shared Task. Section 2.2 then summarizes the five syntactic parsers and three formats adopted for the comparison. Section 2.3 described how the state-of-the-art event extraction system of Miwa et al. (2010) is modified and used for the comparison. 2.1 Bio-molecular Event Extraction The bio-molecular event extraction task considered in this study is that defined in the BioNLP 2009 Shared Task (Kim et al., 2009)1. The shared task provided common and consistent task definitions, data sets for training and evaluation, and evaluation criteria. The shared task consists of three subtasks: core event extraction (Task 1), augmenting events with secondary arguments (Task 2), and the recognition of speculation and negation of the events (Task 3) (Kim et al., 2009). In this paper we consider Task 1 and Task 2. The shared task defined nine event types, which can be divided into five simple events (Gene expression, Transcription, Protein catabolism, Phosphorylation, and Localization) that take one core argument,</context>
<context position="6941" citStr="Kim et al., 2009" startWordPosition="1037" endWordPosition="1040">5, and the Enju parser with the GENIA model (Miyao et al., 2009)6. The formats are Stanford Dependencies (SD) (Figure 1), the CoNLL-X dependency format (Figure 2) and the predicate-argument structure (PAS) format used by Enju (Figure 3). With the exception of Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. Analysis of system features in the task found that the use of parser output with one of 5http://svn.ask.it.usyd.edu.au/trac/ candc/ 6http://www-tsujii.is.s.u-tokyo.ac.jp/ enju/ 38 the formats considered here correlated with high rank at the task (Kim et al., 2009). A number of these parsers have also been shown to be effective for protein-protein interactions extraction (Miyao et al., 2009). The five parsers operate in a number of different frameworks, reflected in their analyses. GDep is a native dependency parser that produces CoNLLX-format dependency trees. MC and Bikel are phrase-structure parsers, and they produce Penn Treebank (PTB) format analyses. C&amp;C is a deep parser based on Combinatory Categorial Grammar (CCG), and its native output is in a CCGspecific format. The output of C&amp;C is converted into SD by a rule-based conversion script (Rimell a</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of bionlp’09 shared task on event extraction. In BioNLP ’09: Proceedings of the Workshop on BioNLP, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
</authors>
<title>Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="5650" citStr="McClosky, 2009" startWordPosition="866" endWordPosition="867">ts are represented with a textual trigger, type, and arguments, where the trigger is a span of text that states the event in text. In Task 1 the event arguments that need to be extracted are restricted to the core arguments Theme and Cause, and secondary arguments (locations and sites) need to be attached in Task 2. 2.2 Parsers and Formats Five parsers and three formats are adopted for the evaluation. The parsers are GDep (Sagae and Tsujii, 2007)2, the Bikel parser (Bikel) (Bikel, 2004)3, the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009)4, the C&amp;C CCG parser, adapted to biomedical text 1http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2http://www.cs.cmu.edu/-sagae/parser/ gdep/ 3http://www.cis.upenn.edu/-dbikel/ software.html 4http://www.cs.brown.edu/-dmcc/ biomedical.html Figure 1: Stanford basic dependency tree Figure 2: CoNLL-X dependency tree Figure 3: Predicate Argument Structure Figure 4: Format conversion dependencies in five parsers. Formats adopted for the evaluation is shown in solid boxes. SD: Stanford Dependency format, CCG: Combinatory Categorial Grammar output format, PTB: Penn Treebank format, and PAS: P</context>
</contexts>
<marker>McClosky, 2009</marker>
<rawString>David McClosky. 2009. Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing. Ph.D. thesis, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Sætre</author>
<author>Jin-Dong Kim</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Event extraction with complex event classification using rich features.</title>
<date>2010</date>
<journal>Journal of Bioinformatics and Computational Biology (JBCB),</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="4021" citStr="Miwa et al. (2010)" startWordPosition="604" endWordPosition="607">a bio-molecular event extraction task with a state-of-the-art event extraction system. This section explains the details of the comparison. Section 2.1 presents the event ex37 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 37–45, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics traction task setting, following that of the BioNLP 2009 Shared Task. Section 2.2 then summarizes the five syntactic parsers and three formats adopted for the comparison. Section 2.3 described how the state-of-the-art event extraction system of Miwa et al. (2010) is modified and used for the comparison. 2.1 Bio-molecular Event Extraction The bio-molecular event extraction task considered in this study is that defined in the BioNLP 2009 Shared Task (Kim et al., 2009)1. The shared task provided common and consistent task definitions, data sets for training and evaluation, and evaluation criteria. The shared task consists of three subtasks: core event extraction (Task 1), augmenting events with secondary arguments (Task 2), and the recognition of speculation and negation of the events (Task 3) (Kim et al., 2009). In this paper we consider Task 1 and Task</context>
<context position="9463" citStr="Miwa et al. (2010)" startWordPosition="1450" endWordPosition="1453">hat are also in the GENIA treebank. This implies that the parsers are likely to perform better on the texts used in the shared task than on other biomedical domain text, and similarly that systems building on their output are expected to achieve best per7http://www-nlp.stanford.edu/software/ lex-parser.shtml 8http://nlp.cs.lth.se/software/ treebank converter/ formance on this data. However, it does not invalidate comparison within the dataset. We further note that the models do not incorporate any knowledge of the event annotations of the shared task. 2.3 Event Extraction System The system by Miwa et al. (2010) is adopted for the evaluation. The system was originally developed for finding core events (Task 1 in the BioNLP 2009 Shared Task) using Enju and GDep with the native output of these parsers. The system consists of three supervised classification-based modules: a trigger detector, an event edge detector, and a complex event detector. The trigger detector classifies each word into the appropriate event types, the event edge detector classifies each edge between an event and a protein into an argument type, and the complex event detector classifies event candidates constructed by all edge combi</context>
<context position="11162" citStr="Miwa et al., 2010" startWordPosition="1714" endWordPosition="1717">nd the shortest paths among the entities. The feature generation is format-independent regarding the shared properties of different formats, but makes use also of format-specific information when available for extracting features, including the dependency tags, word-related information (e.g. a lexical entry in Enju format), and the constituents and their head information. The previously introduced base system is here improved with two modifications. One modification is removing two classes of features from the original features (for details of the original feature representation, we refer to (Miwa et al., 2010)); specifically the features representing governordependent relationships from the target word, and the features representing each event edges in the complex event detector are removed. The other modification is to use head words in a trigger expression as a gold trigger word. This modification is inspired by the part-of-speech (POS) based selection proposed by Kilicoglu and Bergler (2009). 39 The system uses a head word “in” as a trigger word in a trigger expression “in the presence of” instead of using all the words of the expression. In cases where there is no head word information in a par</context>
<context position="13598" citStr="Miwa et al., 2010" startWordPosition="2119" endWordPosition="2122">annotations of the treebank to be used for reference. The GENIA treebank is converted into the Enju format with Enju. When the trees in the treebank cannot be converted into the Enju format, parse results are used instead. The GENIA treebank is also converted into PTB formatss. The treebank is then converted into the dependency formats with the conversions described in Section 2.2. While based on manually annotated gold data, the converted treebanks are not always correct due to conversion errors. The event extraction system described in Section 2.3 is used with the default settings shown in (Miwa et al., 2010). The positive and negative ex9http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/downloads.shtml 10http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/eval-test.shtml 11http://categorizer.tmit.bme.hu/ ~illes/genia ptb/ BD CD CDP CTD Task 1 55.60 54.35 54.59 54.42 Task 2 53.94 52.65 52.88 52.76 Table 1: Comparison of the F-score results with different Stanford dependency variants on the development data set with the MC parser. Results for basic dependencies (BD), collapsed dependencies (CD), collapsed dependencies with propagation of conjunct dependencies (CDP), and collapsed tree depen</context>
<context position="21980" citStr="Miwa et al., 2010" startWordPosition="3489" endWordPosition="3492">NLL-X format, Enju with CoNLL-X format, and Enju with Predicate Argument Structure in Enju format are used for the parser ensemble. The changes from single-parser results are shown in parentheses. The best score in each task is shown in bold. NIA treebank, but in many cases results with the parsers are only slightly worse than results with the treebank. The results suggest that there is relative little remaining benefit to be gained for event extraction from improving parser performance. This supports the claim that most of the errors in event extraction are not caused by the parse errors in (Miwa et al., 2010). Experiments using the CoNLL-X format produce slightly worse results than for SD with the gold treebank data, which is at variance with the indication from parser-based results with MC and Enju. Thus, the results do not provide any systematic indication suggesting that one dependency format would be superior to the other in use for event extraction. 4.3 Event Extraction with Parser Ensemble The four parser outputs were selected for the evaluation of a parser ensemble: C&amp;C with Stanford basic Dependency format, MC with CoNLLX format, Enju with CoNLL-X format, and Enju with Predicate Argument S</context>
<context position="26589" citStr="Miwa et al. (2010)" startWordPosition="4384" endWordPosition="4387">45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95 Riedel N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35 Task 2 65.77 / 75.29 / 70.21 N/A 47.56 / 49.55 / 48.54 22.35 / 46.99 / 30.29 38.24 / 53.57 / 44.62 25.75 / 40.75 / 31.56 49.48 / 61.87 / 54.99 35.86 / 54.08 / 43.12 Ours Riedel Table 6: Comparison of Recall / Precision / F-score results on the test data set. MC with CoNLL-X format and Enju with Predicate Argument Structure in Enju format are used for the evaluation. Results on simple, binding, regulation, and all events are shown. Results by Miwa et al. (2010) (Miwa), Bj¨orne et al. (2009) (Bj¨orne), and Riedel et al. (2009) (Riedel) for Task 1 and Task 2 are shown for comparison. The best score in each result is shown in bold. ple, is good for extracting regulation events, but produced weaker results for simple events. The ensembles of two parser outputs inherit both the strengths and weaknesses of the outputs in most cases, and the strengths and weaknesses of the ensembles vary depending on the combined parser outputs. The differences in performance between ensembles of the outputs of two parsers to the ensemble of the three parser outputs are +0</context>
<context position="28159" citStr="Miwa et al. (2010)" startWordPosition="4653" endWordPosition="4656"> Extraction System Table 6 shows a comparison of performance on the shared task test data. MC with CoNLL-X format and Enju with Predicate Argument Structure in Enju format are used for the evaluation, selecting one of the best performing ensemble settings in Section 4.3. The performance of the best systems in the original shared task is shown for reference ((Bj¨orne et al., 2009) in Task 1 and (Riedel et al., 2009) in Task 2). The event extraction system with our modifications performed significantly better than the best systems in the shared task, further outperforming the original system by Miwa et al. (2010). This result shows that the system applied for the comparison of syntactic parsers achieves state-of-the-art performance at event extraction. This result also shows that the system originally developed only for core events extraction can be easily extended for other arguments simply by treating the other arguments as additional arguments. 5 Related Work Many approaches for parser comparison have been proposed in the BioNLP field. Most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Application-oriented parser comparison across s</context>
</contexts>
<marker>Miwa, Sætre, Kim, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Rune Sætre, Jin-Dong Kim, and Jun’ichi Tsujii. 2010. Event extraction with complex event classification using rich features. Journal of Bioinformatics and Computational Biology (JBCB), 8(1):131–146, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
</authors>
<title>Kenji Sagae, Rune Sætre, Takuya Matsuzaki, and Jun ichi Tsujii.</title>
<date>2009</date>
<journal>Bioinformatics,</journal>
<volume>25</volume>
<issue>3</issue>
<marker>Miyao, 2009</marker>
<rawString>Yusuke Miyao, Kenji Sagae, Rune Sætre, Takuya Matsuzaki, and Jun ichi Tsujii. 2009. Evaluating contributions of natural language parsers to protein-protein interaction extraction. Bioinformatics, 25(3):394–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Veronika Laippala</author>
<author>Katri Haverinen</author>
<author>Juho Heimonen</author>
<author>Tapio Salakoski</author>
</authors>
<title>On the unification of syntactic annotations under the stanford dependency scheme: A case study on bioinfer and genia.</title>
<date>2007</date>
<booktitle>In Biological, translational, and clinical language processing,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28710" citStr="Pyysalo et al., 2007" startWordPosition="4737" endWordPosition="4740"> task, further outperforming the original system by Miwa et al. (2010). This result shows that the system applied for the comparison of syntactic parsers achieves state-of-the-art performance at event extraction. This result also shows that the system originally developed only for core events extraction can be easily extended for other arguments simply by treating the other arguments as additional arguments. 5 Related Work Many approaches for parser comparison have been proposed in the BioNLP field. Most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the recognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those of Miyao et al. Event extraction can be viewed as an additional extrinsic evaluation task for syntactic parsers, providing more reliable and evaluation and a broader perspective into parser performanc</context>
</contexts>
<marker>Pyysalo, Ginter, Laippala, Haverinen, Heimonen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Veronika Laippala, Katri Haverinen, Juho Heimonen, and Tapio Salakoski. 2007. On the unification of syntactic annotations under the stanford dependency scheme: A case study on bioinfer and genia. In Biological, translational, and clinical language processing, pages 25– 32, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>41--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="26655" citStr="Riedel et al. (2009)" startWordPosition="4395" endWordPosition="4398">8.48 / 51.95 Riedel N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35 Task 2 65.77 / 75.29 / 70.21 N/A 47.56 / 49.55 / 48.54 22.35 / 46.99 / 30.29 38.24 / 53.57 / 44.62 25.75 / 40.75 / 31.56 49.48 / 61.87 / 54.99 35.86 / 54.08 / 43.12 Ours Riedel Table 6: Comparison of Recall / Precision / F-score results on the test data set. MC with CoNLL-X format and Enju with Predicate Argument Structure in Enju format are used for the evaluation. Results on simple, binding, regulation, and all events are shown. Results by Miwa et al. (2010) (Miwa), Bj¨orne et al. (2009) (Bj¨orne), and Riedel et al. (2009) (Riedel) for Task 1 and Task 2 are shown for comparison. The best score in each result is shown in bold. ple, is good for extracting regulation events, but produced weaker results for simple events. The ensembles of two parser outputs inherit both the strengths and weaknesses of the outputs in most cases, and the strengths and weaknesses of the ensembles vary depending on the combined parser outputs. The differences in performance between ensembles of the outputs of two parsers to the ensemble of the three parser outputs are +0.01 for Task 1, and -0.26 for Task 2. This result suggests that ad</context>
<context position="27959" citStr="Riedel et al., 2009" startWordPosition="4621" endWordPosition="4624"> three parser outputs, however, shows stable performance across categories, scoring in the top two for binding, regulation, and all events, in the top four for simple events. 43 4.4 Performance of Event Extraction System Table 6 shows a comparison of performance on the shared task test data. MC with CoNLL-X format and Enju with Predicate Argument Structure in Enju format are used for the evaluation, selecting one of the best performing ensemble settings in Section 4.3. The performance of the best systems in the original shared task is shown for reference ((Bj¨orne et al., 2009) in Task 1 and (Riedel et al., 2009) in Task 2). The event extraction system with our modifications performed significantly better than the best systems in the shared task, further outperforming the original system by Miwa et al. (2010). This result shows that the system applied for the comparison of syntactic parsers achieves state-of-the-art performance at event extraction. This result also shows that the system originally developed only for core events extraction can be easily extended for other arguments simply by treating the other arguments as additional arguments. 5 Related Work Many approaches for parser comparison have </context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A markov logic approach to bio-molecular event extraction. In BioNLP ’09: Proceedings of the Workshop on BioNLP, pages 41– 49, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Porting a lexicalized-grammar parser to the biomedical domain.</title>
<date>2009</date>
<journal>J. ofBiomedical Informatics,</journal>
<volume>42</volume>
<issue>5</issue>
<contexts>
<context position="6324" citStr="Rimell and Clark, 2009" startWordPosition="942" endWordPosition="945">http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2http://www.cs.cmu.edu/-sagae/parser/ gdep/ 3http://www.cis.upenn.edu/-dbikel/ software.html 4http://www.cs.brown.edu/-dmcc/ biomedical.html Figure 1: Stanford basic dependency tree Figure 2: CoNLL-X dependency tree Figure 3: Predicate Argument Structure Figure 4: Format conversion dependencies in five parsers. Formats adopted for the evaluation is shown in solid boxes. SD: Stanford Dependency format, CCG: Combinatory Categorial Grammar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. (C&amp;C) (Rimell and Clark, 2009)5, and the Enju parser with the GENIA model (Miyao et al., 2009)6. The formats are Stanford Dependencies (SD) (Figure 1), the CoNLL-X dependency format (Figure 2) and the predicate-argument structure (PAS) format used by Enju (Figure 3). With the exception of Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. Analysis of system features in the task found that the use of parser output with one of 5http://svn.ask.it.usyd.edu.au/trac/ candc/ 6http://www-tsujii.is.s.u-tokyo.ac.jp/ enju/ 38 the formats considered here correlated with high rank at the task (</context>
<context position="7556" citStr="Rimell and Clark, 2009" startWordPosition="1136" endWordPosition="1139">., 2009). A number of these parsers have also been shown to be effective for protein-protein interactions extraction (Miyao et al., 2009). The five parsers operate in a number of different frameworks, reflected in their analyses. GDep is a native dependency parser that produces CoNLLX-format dependency trees. MC and Bikel are phrase-structure parsers, and they produce Penn Treebank (PTB) format analyses. C&amp;C is a deep parser based on Combinatory Categorial Grammar (CCG), and its native output is in a CCGspecific format. The output of C&amp;C is converted into SD by a rule-based conversion script (Rimell and Clark, 2009). Enju is deep parser based on Head-driven Phrase Structure Grammar (HPSG) and produces a format containing predicate argument structures (PAS) along with a phrase structure tree in Enju format. To study the contribution of the formats in which the five parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 4. The Enju PAS output is converted into Penn Treebank format using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006)7, and CoNLLX dependencies are genera</context>
</contexts>
<marker>Rimell, Clark, 2009</marker>
<rawString>Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. J. ofBiomedical Informatics, 42(5):852–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles. In EMNLP-CoNLL</title>
<date>2007</date>
<contexts>
<context position="5485" citStr="Sagae and Tsujii, 2007" startWordPosition="843" endWordPosition="846"> (Binding), and three regulation events (Regulation, Positive regulation, and Negative regulation) that can take other events as arguments. In the two tasks considered, events are represented with a textual trigger, type, and arguments, where the trigger is a span of text that states the event in text. In Task 1 the event arguments that need to be extracted are restricted to the core arguments Theme and Cause, and secondary arguments (locations and sites) need to be attached in Task 2. 2.2 Parsers and Formats Five parsers and three formats are adopted for the evaluation. The parsers are GDep (Sagae and Tsujii, 2007)2, the Bikel parser (Bikel) (Bikel, 2004)3, the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009)4, the C&amp;C CCG parser, adapted to biomedical text 1http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2http://www.cs.cmu.edu/-sagae/parser/ gdep/ 3http://www.cis.upenn.edu/-dbikel/ software.html 4http://www.cs.brown.edu/-dmcc/ biomedical.html Figure 1: Stanford basic dependency tree Figure 2: CoNLL-X dependency tree Figure 3: Predicate Argument Structure Figure 4: Format conversion dependencies in five parsers. Formats adopted</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuka Tateisi</author>
<author>Akane Yakushiji</author>
<author>Tomoko Ohta</author>
<author>Junfichi Tsujii</author>
</authors>
<title>Syntax annotation for the genia corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the IJCNLP 2005, Companion volume,</booktitle>
<pages>222--227</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="8616" citStr="Tateisi et al., 2005" startWordPosition="1315" endWordPosition="1318">ing the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006)7, and CoNLLX dependencies are generated from PTB by using Treebank Converter (Johansson and Nugues, 2007)8. We note that all of these conversions can introduce some errors in the conversion process. With the exception of Bikel, all the applied parsers have models specifically adapted for biomedical text. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The results of parsing with these models as provided for the BioNLP Shared Task are used in this comparison. However, we note that the shared task data, drawn from the GENIA event corpus (Kim et al., 2008), contains abstracts that are also in the GENIA treebank. This implies that the parsers are likely to perform better on the texts used in the shared task than on other biomedical domain text, and similarly that systems building on their output are expected to achieve best per7http://www-nlp.stanford.edu/software/ lex-parser.shtml 8http://nlp.cs.lth.se/software/ treebank converter/ formance </context>
</contexts>
<marker>Tateisi, Yakushiji, Ohta, Tsujii, 2005</marker>
<rawString>Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Junfichi Tsujii. 2005. Syntax annotation for the genia corpus. In Proceedings of the IJCNLP 2005, Companion volume, pages 222–227, Jeju Island, Korea, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
</authors>
<title>Yuka Tateishi, Jin-Dong Kim, Tomoko Ohta, John McNaught, Sophia Ananiadou, and Jun’ichi Tsujii.</title>
<date>2005</date>
<booktitle>In Panayiotis Bozanis and</booktitle>
<volume>3746</volume>
<pages>382--392</pages>
<editor>Elias N. Houstis, editors,</editor>
<publisher>Springer.</publisher>
<marker>Tsuruoka, 2005</marker>
<rawString>Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, Tomoko Ohta, John McNaught, Sophia Ananiadou, and Jun’ichi Tsujii. 2005. Developing a robust partof-speech tagger for biomedical text. In Panayiotis Bozanis and Elias N. Houstis, editors, Panhellenic Conference on Informatics, volume 3746 of Lecture Notes in Computer Science, pages 382–392. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>