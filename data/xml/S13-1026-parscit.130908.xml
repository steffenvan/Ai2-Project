<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003200">
<title confidence="0.900392">
KLUE-CORE: A regression model of semantic textual similarity
</title>
<author confidence="0.715164">
Paul Greiner and Thomas Proisl and Stefan Evert and Besim Kabashi
</author>
<affiliation confidence="0.585558">
Friedrich-Alexander-Universität Erlangen-Nürnberg
</affiliation>
<address confidence="0.6785795">
Department Germanistik und Komparatistik
Professur für Korpuslinguistik
Bismarckstr. 6
91054 Erlangen, Germany
</address>
<email confidence="0.996867">
{paul.greiner,thomas.proisl,stefan.evert,besim.kabashi}@fau.de
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866933333333">
This paper describes our system entered for the
*SEM 2013 shared task on Semantic Textual
Similarity (STS). We focus on the core task
of predicting the semantic textual similarity of
sentence pairs.
The current system utilizes machine learn-
ing techniques trained on semantic similarity
ratings from the *SEM 2012 shared task; it
achieved rank 20 out of 90 submissions from
35 different teams. Given the simple nature of
our approach, which uses only WordNet and
unannotated corpus data as external resources,
we consider this a remarkably good result, mak-
ing the system an interesting tool for a wide
range of practical applications.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991725">
The *SEM 2013 shared task on Semantic Textual
Similarity (Agirre et al., 2013) required participants
to implement a software system that is able to pre-
dict the semantic textual similarity (STS) of sentence
pairs. Being able to reliably measure semantic simi-
larity can be beneficial for many applications, e.g. in
the domains of MT evaluation, information extrac-
tion, question answering, and summarization.
For the shared task, STS was measured on a scale
ranging from 0 (indicating no similarity at all) to 5
(semantic equivalence). The system predictions were
evaluated against manually annotated data.
</bodyText>
<sectionHeader confidence="0.80955" genericHeader="method">
2 Description of our approach
</sectionHeader>
<bodyText confidence="0.999798266666667">
Our system KLUE-CORE uses two approaches to
estimate STS between pairs of sentences: a distri-
butional bag-of-words model inspired by Schütze
(1998), and a simple alignment model that links each
word in one sentence to the semantically most similar
word in the other sentence. For the alignment model,
word similarities were obtained from WordNet (using
a range of state-of-the-art path-based similarity mea-
sures) and from two distributional semantic models
(DSM).
All similarity scores obtained in this way were
passed to a ridge regression learner in order to obtain
a final STS score. The predictions for new sentence
pairs were then transformed to the range [0,5], as
required by the task definition.
</bodyText>
<subsectionHeader confidence="0.993301">
2.1 The training data
</subsectionHeader>
<bodyText confidence="0.999921642857143">
We trained our system on manually annotated sen-
tence pairs from the STS task at SemEval 2012
(Agirre et al., 2012). Pooling the STS 2012 training
and test data, we obtained 5 data sets from differ-
ent domains, comprising a total of 5343 sentence
pairs annotated with a semantic similarity score in
the range [0,5]. The data sets are paraphrase sen-
tence pairs (MSRpar), sentence pairs from video de-
scriptions (MSRvid), MT evaluation sentence pairs
(MTnews and MTeuroparl), and glosses from two
different lexical semantic resources (OnWN).
All sentence pairs were pre-processed with Tree-
Tagger (Schmid, 1995)1 for part-of-speech annota-
tion and lemmatization.
</bodyText>
<footnote confidence="0.9921025">
1http://www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/treetagger.html
</footnote>
<page confidence="0.830894">
181
</page>
<note confidence="0.3023175">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 181–186, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.997055">
2.2 Similarity on word level
</subsectionHeader>
<bodyText confidence="0.9999772">
Our alignment model (Sec. 2.3.1) is based on similar-
ity scores for pairs of words. We obtained a total of
11 different word similarity measures from WordNet
(Miller et al., 1990) and in a completely unsupervised
manner from distributional semantic models.
</bodyText>
<subsectionHeader confidence="0.840231">
2.2.1 WordNet
</subsectionHeader>
<bodyText confidence="0.998822625">
We computed three state-of-the-art WordNet simi-
larity measures, namely path similarity, Wu-Palmer
similarity and Leacock-Chodorow similarity (Budan-
itsky and Hirst, 2006). As usual, for each pair of
words the synsets with the highest similarity score
were selected. For all three measures, we made use of
the implementations provided as part of the Natural
Language ToolKit for Python (Bird et al., 2009).
</bodyText>
<subsectionHeader confidence="0.67387">
2.2.2 Distributional semantics
</subsectionHeader>
<bodyText confidence="0.999737407407407">
Word similarity scores were also obtained from two
DSM: Distributional Memory (Baroni and Lenci,
2010) and a model compiled from a version of the
English Wikipedia.2 For Distributional Memory, we
chose the collapsed W xW matricization, resulting
in a 30686 x 30686 matrix that was further reduced
to 300 latent dimensions using randomized SVD
(Halko et al., 2009). For the Wikipedia DSM, we
used a L2/R2 context window and mid-frequency
feature terms, resulting in a 77598 x 30484 matrix.
Co-occurrence frequency counts were weighted us-
ing sparse log-likelihood association scores with a
square root transformation, and reduced to 300 latent
dimensions with randomized SVD. In both cases, tar-
get terms are POS-disambiguated lemmas of content
words, and the angle between vectors was used as a
distance measure (equivalent to cosine similarity).
For each DSM, we computed the following se-
mantic distances: (i) angle: the angle between the
two word vectors; (ii) fwdrank: the (logarithm of
the) forward neighbour rank, i.e. which rank the sec-
ond word occupies among the nearest neighbours
of the first word; (iii) bwdrank: the (logarithm of
the) backward neighbour rank, i.e. which rank the
first word occupies among the nearest neighbours of
the second word; (iv) rank: the (logarithm of the)
arithmetic mean of forward and backward neighbour
</bodyText>
<footnote confidence="0.739655666666667">
2For this purpose, we used the pre-processed and linguis-
tically annotated Wackypedia corpus available from http://
wacky.sslmit.unibo.it/.
</footnote>
<bodyText confidence="0.997838461538462">
rank; (v) lowrank: the (logarithm of the) harmonic
mean of forward and backward neighbour rank.
A composite similarity score in the range [0,1]
was obtained by linear regression on all five distance
measures, using the WordSim-353 noun similarity
ratings (Finkelstein et al., 2002) for parameter esti-
mation. This score is referred to as similarity below.
Manual inspection showed that word pairs with simi-
larity &lt; 0.7 were completely unrelated in many cases,
so we also included a “strict” version of similarity
with all lower scores set to 0. We further included
rank and angle, which were linearly transformed to
similarity values in the range [0,1].
</bodyText>
<subsectionHeader confidence="0.999524">
2.3 Similarity on sentence level
</subsectionHeader>
<bodyText confidence="0.9996958">
Similarity scores for sentence pairs were obtained in
two different ways: with a simple alignment model
based on the word similarity scores from Sec. 2.2
(described in Sec. 2.3.1) and with a distributional
bag-of-words model (described in Sec. 2.3.2).
</bodyText>
<subsectionHeader confidence="0.825921">
2.3.1 Similarity by word alignment
</subsectionHeader>
<bodyText confidence="0.999995692307692">
The sentence pairs were preprocessed in the follow-
ing way: input words were transformed to lower-
case; common stopwords were eliminated; and dupli-
cate words within each sentence were deleted. For
the word similarity scores from Sec. 2.2.2, POS-
disambiguated lemmas according to the TreeTagger
annotation were used.
Every word of the first sentence in a given pair
was then compared with every word of the second
sentence, resulting in a matrix of similarity scores
for each of the word similarity measures described
in Sec. 2.2. Since we were not interested in an asym-
metric notion of similarity, matrices were set up so
that the shorter sentence in a pair always corresponds
to the rows of the matrix, transposing the similarity
matrix if necessary. From each matrix, two similar-
ity scores for the sentence pair were computed: the
arithmetic mean of the row maxima (marked as short
in Tab. 4), and the artihmetic mean of the column
maxima (marked as long in Tab. 4).
This approach corresponds to a simple word align-
ment model where each word in the shorter sentence
is aligned to the semantically most similar word in
the longer sentence (short), and vice versa (long).
Note that multiple source words may be aligned to
the same target word, and target words can remain
</bodyText>
<page confidence="0.991279">
182
</page>
<bodyText confidence="0.99943575">
unaligned without penalty. Semantic similarities are
then averaged across all alignment pairs.
In total, we obtained 22 sentence similarity scores
from this approach.
</bodyText>
<subsectionHeader confidence="0.62364">
2.3.2 Distributional similarity
</subsectionHeader>
<bodyText confidence="0.999989170731707">
We computed distributional similarity between the
sentences in each pair directly using bag-of-words
centroid vectors as suggested by Schütze (1998),
based on the two word-level DSM introduced in
Sec. 2.2.2.
For each sentence pair and DSM, we computed (i)
the angle between the centroid vectors of the two sen-
tences and (ii) a z-score relative to all other sentences
in the same data set of the training or test collection.
Both values are measures of semantic distance, but
are automatically transformed into similarity mea-
sures by the regression learner (Sec. 2.4).
For the z-scores, we computed the semantic dis-
tance (i.e. angle) between the first sentence of a given
pair and the second sentences of all word pairs in the
same data set. The resulting list of angles was stan-
dardized to z-scores, and the z-score corresponding
to the second sentence from the given pair was used
as a measure of forward similarity between the first
and second sentence. In the same way, a backward
z-score between the second and first sentence was
determined. We used the average of the forward and
backward z-score as our second STS measure.
The z-transformation was motivated by our obser-
vation that there are substantial differences between
the individual data sets in the STS 2012 training and
test data. For some data sets (MSRpar and MSRvid),
sentences are often almost identical and even a single-
word difference can result in low similarity ratings;
for other data sets (e.g. OnWN), similarity ratings
seem to be based on the general state of affairs de-
scribed by the two sentences rather than their par-
ticular wording of propositional content. By using
other sentences in the same data set as a frame of
reference, corpus-based similarity scores can roughly
be calibrated to the respective notion of STS.
In total, we obtained 4 sentence (dis)similarity
scores from this approach. Because of technical is-
sues, only the z-score measures were used in the
submitted system. The experiments in Sec. 3 also
focus on these z-scores.
</bodyText>
<subsectionHeader confidence="0.955658">
2.4 The regression model
</subsectionHeader>
<bodyText confidence="0.999960833333334">
The 24 individual similarity scores described in
Sec. 2.3.1 and 2.3.2 were combined into a single
STS prediction by supervised regression.
We conducted experiments with various machine
learning algorithms implemented in the Python li-
brary scikit-learn (Pedregosa et al., 2011). In partic-
ular, we tested linear regression, regularized linear
regression (ridge regression), Bayesian ridge regres-
sion, support vector regression and regression trees.
Our final system submitted to the shared task uses
ridge regression, a shrinkage method applied to linear
regression that uses a least-squares regularization on
the regression coefficients (Hastie et al., 2001, 59).
Intuitively speaking, the regularization term discour-
ages large value of the regression coefficients, which
makes the learning technique less prone to overfit-
ting quirks of the training data, especially with large
numbers of features.
We tried to optimise our results by training the indi-
vidual regressors for each test data set on appropriate
portions of the training data. For our task submis-
sion, we used the following training data based on
educated guesses inspired by the very small amount
of development data provied: for the headlines test
set we trained on both glosses and statistical MT
data, for the OnWN and FNWN test sets we trained
on glosses only (OnWN), and for the SMT test set
we trained on statistical MT data only (MTnews and
MTeuroparl). We decided to omit the Microsoft Re-
search Paraphrase Corpus (MSRpar and MSRvid)
because we felt that the types of sentence pairs in this
corpus were too different from the development data.
For our submission, we used all 24 features de-
scribed in Sec. 2.3 as input for the ridge regression
algorithm. Out of 90 submissions by 35 teams, our
system ranked on place 20.3
</bodyText>
<sectionHeader confidence="0.999628" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.824422">
In this section, we describe some post-hoc experi-
ments on the STS 2013 test data, which we performed
in order to find out whether we made good decisions
regarding the machine learning method, training data,
3This paper describes the run listed as KLUE-approach_2 in
the official results. The run KLUE-approach_1 was produced by
the same system without the bag-of-words features (Sec. 2.3.2);
it was only submitted as a safety backup.
</bodyText>
<page confidence="0.998576">
183
</page>
<bodyText confidence="0.999514">
similarity features, and other parameters. Results of
our submitted system are typeset in italics, the best
results in each column are typeset in bold font.
</bodyText>
<subsectionHeader confidence="0.989537">
3.1 Machine learning algorithms
</subsectionHeader>
<bodyText confidence="0.99999625">
Tab. 1 gives an overview of the performance of vari-
ous machine learning algorithms. All regressors were
trained on the same combinations of data sets (see
Sec. 2.4 above) using all available features, and eval-
uated on the STS 2013 test data. Overall, our choice
of ridge regression is justified. Especially for the
OnWN test set, however, support vector regression
is considerably better (it would have achieved rank
11 instead of 17 on this test set). If we had happened
to use the best learning algorithm for each test set,
we would have achieved a mean score of 0.54768
(putting our submission at rank 14 instead of 20).
</bodyText>
<subsectionHeader confidence="0.999169">
3.2 Regularization strength
</subsectionHeader>
<bodyText confidence="0.9999968">
We also experimented with different regularization
strengths, as determined by the parameter a of the
ridge regression algorithm (see Tab. 2). Changing a
from its default value a = 1 does not seem to have
a large impact on the performance of the regressor.
Setting a = 2 for all test sets would have minimally
improved the mean score (rank 19 instead of 20).
Even choosing the optimal a for each test set would
only have resulted in a slightly improved mean score
of 0.53811 (also putting our submission at rank 19).
</bodyText>
<subsectionHeader confidence="0.99989">
3.3 Composition of training data
</subsectionHeader>
<bodyText confidence="0.9999866875">
As described above, we suspected that using different
combinations of the training data for different test
sets might lead to better results. The overview in
Tab. 3 confirms our expectations. We did, however,
fail to correctly guess the optimal combinations for
each test set. We would have obtained the best re-
sults by training on glosses (OnWN) for the headlines
test set (rank 35 instead of 40 in this category), by
training on MSR data (MSRpar and MSRvid) for the
OnWN (rank 11 instead of 17) and FNWN test sets
(rank 9 instead of 10), and by combining glosses and
machine translation data (OnWN, MTnews MTeu-
roparl) for the SMT test set (rank 30 instead of 33).
Had we found the optimal training data for each test
set, our system would have achieved a mean score of
0.55021 (rank 11 instead of 20).
</bodyText>
<subsectionHeader confidence="0.855613">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.9999916875">
For our submission, we used all the features de-
scribed in Sec. 2. Tab. 4 shows what results each
group of features would have achieved by itself (all
runs use ridge regression, default a = 1 and the same
combinations of training data as in our submission).
In Tab. 4, the line labelled wp500 shows the re-
sults obtained using only word-alignment similarity
scores (Sec. 2.3.1) based on the Wikipedia DSM
(Sec. 2.2.2) as features. The following two lines give
separate results for the alignments from shorter to
longer sentence, i.e. row maxima (wp500-short) and
from longer to shorter sentence, i.e. column maxima
(wp500-long), respectively. Below are corresponding
results for word alignments based on Distributional
Memory (dm, dm-short, dm-long) and WordNet simi-
larity as described in Sec. 2.2.1 (WN, WN-short, WN-
long). The line labelled bow represents the two z-
score similarities obtained from distributional bag-of-
words models (Sec. 2.3.2); bow-wp500 (Wikipedia
DSM) and bow-dm (Distributional Memory) each
correspond to a single distributional feature.
Combining all the available features indeed results
in the highest mean score. However, for OnWN and
SMT a subset of the features would have led to better
results. Using only the bag-of-words scores would
have improved the results for the OnWN test set by
a considerable margin (rank 8 instead of 17), using
only the alignment scores based on WordNet would
have improved the results for the SMT test set (rank
17 instead of 33). If we had used the optimal subset
of features for each test set, the mean score would
have increased to 0.55556 (rank 9 instead of 20).
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999975916666667">
Our experiments show that it is essential for high-
quality semantic textual similarity to adapt a corpus-
based system carefully to each particular data set
(choice of training data, feature engineering, tuning
of machine learning algorithm). Many of our edu-
cated guesses for parameter settings turned out to be
fairly close to the optimal values, though there would
have been some room for improvement.
Overall, our simple approach, which makes very
limited use of external resources, performs quite well
– achieving rank 20 out of 90 submissions – and will
be a useful tool for many real-world applications.
</bodyText>
<page confidence="0.993972">
184
</page>
<table confidence="0.99964">
headlines OnWN FNWN SMT mean
Ridge Regression 0.65102 0.68693 0.41887 0.33599 0.53546
Linear Regression 0.65184 0.68118 0.39707 0.32756 0.52966
Bayesian Ridge 0.65164 0.68962 0.42344 0.33003 0.53474
SVM SVR 0.52208 0.73330 0.40479 0.30810 0.49357
Decision Tree 0.29320 0.50633 0.05022 0.17072 0.28510
</table>
<tableCaption confidence="0.997816">
Table 1: Evaluation results for different machine learning algorithms
</tableCaption>
<table confidence="0.983772875">
a headlines OnWN FNWN SMT mean
1 0.65102 0.68693 0.41887 0.33599 0.53546
0.01 0.65184 0.68129 0.39773 0.32773 0.52980
0.1 0.65186 0.68224 0.40246 0.32900 0.53087
0.5 0.65161 0.68492 0.41346 0.33311 0.53374
0.9 0.65114 0.68660 0.41816 0.33560 0.53523
2 0.64941 0.68917 0.42290 0.33830 0.53659
5 0.64394 0.69197 0.42265 0.33669 0.53491
</table>
<tableCaption confidence="0.978396">
Table 2: Evaluation results for different regularization strengths of the ridge regression learner
</tableCaption>
<table confidence="0.999683714285714">
headlines OnWN FNWN SMT mean
def 0.65440 0.68693 0.41887 0.32694 0.53357
smt 0.65322 0.62643 0.24895 0.33599 0.50684
def+smt 0.65102 0.59665 0.24953 0.33867 0.49962
msr 0.63633 0.73396 0.43073 0.33168 0.54185
def+smt+msr 0.65008 0.65093 0.39636 0.28645 0.50777
approach2 0.65102 0.68693 0.41887 0.33599 0.53546
</table>
<tableCaption confidence="0.995944">
Table 3: Evaluation results for different training sets (“approach2” refers to our shared task submission, cf. Sec. 2.4)
</tableCaption>
<table confidence="0.999931142857143">
headlines OnWN FNWN SMT mean
wp500 0.57099 0.59199 0.31740 0.31320 0.46899
wp500-long 0.57837 0.59012 0.30909 0.30075 0.46614
wp500-short 0.58271 0.58845 0.34205 0.29474 0.46794
dm 0.42129 0.55945 0.21139 0.27426 0.38910
dm-long 0.40709 0.56511 0.28993 0.23826 0.38037
dm-short 0.44780 0.53555 0.28709 0.24484 0.38853
WN 0.63654 0.65149 0.41025 0.35624 0.52783
WN-long 0.62749 0.63828 0.39684 0.33399 0.51297
WN-short 0.64986 0.66175 0.41441 0.33350 0.52759
bow 0.52384 0.74046 0.31917 0.24611 0.46808
bow-wp500 0.52726 0.73624 0.32797 0.24460 0.46841
bow-dm 0.21908 0.66873 0.17096 0.20176 0.32138
all 0.65102 0.68693 0.41887 0.33599 0.53546
</table>
<tableCaption confidence="0.999838">
Table 4: Evaluation results for different sets of similarity scores as features (cf. Sec. 3.4)
</tableCaption>
<page confidence="0.998921">
185
</page>
<sectionHeader confidence="0.995854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895607843137">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2012. Semeval-2012 task 6:
A pilot on semantic textual similarity. In First Joint
Conference on Lexical and Computational Semantics,
pages 385–393. Association for Computational Linguis-
tics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional Memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673–712.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. O’Reilly
Media, Sebastopol, CA. Online version available at
http://www.nltk.org/book.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–47.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: The concept revisited.
ACM Transactions on Information Systems, 20(1):116–
131.
N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Find-
ing structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, California Institute
of Technology, September.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2001. The Elements of Statistical Learning. Data Min-
ing, Inference, and Prediction. Springer, New York,
NY.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduc-
tion to WordNet: An on-line lexical database. Interna-
tional Journal of Lexicography, 3(4):235–244.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceedings
of the EACL SIGDAT-Workshop, pages 47–50, Dublin.
Hinrich Schütze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
</reference>
<page confidence="0.998794">
186
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.064611">
<title confidence="0.4798094">KLUE-CORE: A regression model of semantic textual similarity Greiner Proisl Evert Friedrich-Alexander-Universität Department Germanistik und Professur für</title>
<address confidence="0.615716">Bismarckstr. 91054 Erlangen,</address>
<email confidence="0.998475">paul.greiner@fau.de</email>
<email confidence="0.998475">thomas.proisl@fau.de</email>
<email confidence="0.998475">stefan.evert@fau.de</email>
<email confidence="0.998475">besim.kabashi@fau.de</email>
<abstract confidence="0.9995735">This paper describes our system entered for the *SEM 2013 shared task on Semantic Textual Similarity (STS). We focus on the core task of predicting the semantic textual similarity of sentence pairs. The current system utilizes machine learning techniques trained on semantic similarity ratings from the *SEM 2012 shared task; it achieved rank 20 out of 90 submissions from 35 different teams. Given the simple nature of our approach, which uses only WordNet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool for a wide range of practical applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2483" citStr="Agirre et al., 2012" startWordPosition="369" endWordPosition="372">e semantically most similar word in the other sentence. For the alignment model, word similarities were obtained from WordNet (using a range of state-of-the-art path-based similarity measures) and from two distributional semantic models (DSM). All similarity scores obtained in this way were passed to a ridge regression learner in order to obtain a final STS score. The predictions for new sentence pairs were then transformed to the range [0,5], as required by the task definition. 2.1 The training data We trained our system on manually annotated sentence pairs from the STS task at SemEval 2012 (Agirre et al., 2012). Pooling the STS 2012 training and test data, we obtained 5 data sets from different domains, comprising a total of 5343 sentence pairs annotated with a semantic similarity score in the range [0,5]. The data sets are paraphrase sentence pairs (MSRpar), sentence pairs from video descriptions (MSRvid), MT evaluation sentence pairs (MTnews and MTeuroparl), and glosses from two different lexical semantic resources (OnWN). All sentence pairs were pre-processed with TreeTagger (Schmid, 1995)1 for part-of-speech annotation and lemmatization. 1http://www.ims.uni-stuttgart.de/forschung/ ressourcen/wer</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In First Joint Conference on Lexical and Computational Semantics, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1087" citStr="Agirre et al., 2013" startWordPosition="148" endWordPosition="151"> We focus on the core task of predicting the semantic textual similarity of sentence pairs. The current system utilizes machine learning techniques trained on semantic similarity ratings from the *SEM 2012 shared task; it achieved rank 20 out of 90 submissions from 35 different teams. Given the simple nature of our approach, which uses only WordNet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool for a wide range of practical applications. 1 Introduction The *SEM 2013 shared task on Semantic Textual Similarity (Agirre et al., 2013) required participants to implement a software system that is able to predict the semantic textual similarity (STS) of sentence pairs. Being able to reliably measure semantic similarity can be beneficial for many applications, e.g. in the domains of MT evaluation, information extraction, question answering, and summarization. For the shared task, STS was measured on a scale ranging from 0 (indicating no similarity at all) to 5 (semantic equivalence). The system predictions were evaluated against manually annotated data. 2 Description of our approach Our system KLUE-CORE uses two approaches to </context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional Memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="4184" citStr="Baroni and Lenci, 2010" startWordPosition="613" endWordPosition="616"> a completely unsupervised manner from distributional semantic models. 2.2.1 WordNet We computed three state-of-the-art WordNet similarity measures, namely path similarity, Wu-Palmer similarity and Leacock-Chodorow similarity (Budanitsky and Hirst, 2006). As usual, for each pair of words the synsets with the highest similarity score were selected. For all three measures, we made use of the implementations provided as part of the Natural Language ToolKit for Python (Bird et al., 2009). 2.2.2 Distributional semantics Word similarity scores were also obtained from two DSM: Distributional Memory (Baroni and Lenci, 2010) and a model compiled from a version of the English Wikipedia.2 For Distributional Memory, we chose the collapsed W xW matricization, resulting in a 30686 x 30686 matrix that was further reduced to 300 latent dimensions using randomized SVD (Halko et al., 2009). For the Wikipedia DSM, we used a L2/R2 context window and mid-frequency feature terms, resulting in a 77598 x 30484 matrix. Co-occurrence frequency counts were weighted using sparse log-likelihood association scores with a square root transformation, and reduced to 300 latent dimensions with randomized SVD. In both cases, target terms </context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional Memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural Language Processing with Python. O’Reilly</title>
<date>2009</date>
<location>Media, Sebastopol, CA.</location>
<note>Online version available at http://www.nltk.org/book.</note>
<contexts>
<context position="4049" citStr="Bird et al., 2009" startWordPosition="595" endWordPosition="598"> scores for pairs of words. We obtained a total of 11 different word similarity measures from WordNet (Miller et al., 1990) and in a completely unsupervised manner from distributional semantic models. 2.2.1 WordNet We computed three state-of-the-art WordNet similarity measures, namely path similarity, Wu-Palmer similarity and Leacock-Chodorow similarity (Budanitsky and Hirst, 2006). As usual, for each pair of words the synsets with the highest similarity score were selected. For all three measures, we made use of the implementations provided as part of the Natural Language ToolKit for Python (Bird et al., 2009). 2.2.2 Distributional semantics Word similarity scores were also obtained from two DSM: Distributional Memory (Baroni and Lenci, 2010) and a model compiled from a version of the English Wikipedia.2 For Distributional Memory, we chose the collapsed W xW matricization, resulting in a 30686 x 30686 matrix that was further reduced to 300 latent dimensions using randomized SVD (Halko et al., 2009). For the Wikipedia DSM, we used a L2/R2 context window and mid-frequency feature terms, resulting in a 77598 x 30484 matrix. Co-occurrence frequency counts were weighted using sparse log-likelihood assoc</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media, Sebastopol, CA. Online version available at http://www.nltk.org/book.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="3815" citStr="Budanitsky and Hirst, 2006" startWordPosition="554" endWordPosition="558">1: Proceedings of the Main Conference and the Shared Task, pages 181–186, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics 2.2 Similarity on word level Our alignment model (Sec. 2.3.1) is based on similarity scores for pairs of words. We obtained a total of 11 different word similarity measures from WordNet (Miller et al., 1990) and in a completely unsupervised manner from distributional semantic models. 2.2.1 WordNet We computed three state-of-the-art WordNet similarity measures, namely path similarity, Wu-Palmer similarity and Leacock-Chodorow similarity (Budanitsky and Hirst, 2006). As usual, for each pair of words the synsets with the highest similarity score were selected. For all three measures, we made use of the implementations provided as part of the Natural Language ToolKit for Python (Bird et al., 2009). 2.2.2 Distributional semantics Word similarity scores were also obtained from two DSM: Distributional Memory (Baroni and Lenci, 2010) and a model compiled from a version of the English Wikipedia.2 For Distributional Memory, we chose the collapsed W xW matricization, resulting in a 30686 x 30686 matrix that was further reduced to 300 latent dimensions using rando</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>131</pages>
<contexts>
<context position="5844" citStr="Finkelstein et al., 2002" startWordPosition="870" endWordPosition="873">f the) backward neighbour rank, i.e. which rank the first word occupies among the nearest neighbours of the second word; (iv) rank: the (logarithm of the) arithmetic mean of forward and backward neighbour 2For this purpose, we used the pre-processed and linguistically annotated Wackypedia corpus available from http:// wacky.sslmit.unibo.it/. rank; (v) lowrank: the (logarithm of the) harmonic mean of forward and backward neighbour rank. A composite similarity score in the range [0,1] was obtained by linear regression on all five distance measures, using the WordSim-353 noun similarity ratings (Finkelstein et al., 2002) for parameter estimation. This score is referred to as similarity below. Manual inspection showed that word pairs with similarity &lt; 0.7 were completely unrelated in many cases, so we also included a “strict” version of similarity with all lower scores set to 0. We further included rank and angle, which were linearly transformed to similarity values in the range [0,1]. 2.3 Similarity on sentence level Similarity scores for sentence pairs were obtained in two different ways: with a simple alignment model based on the word similarity scores from Sec. 2.2 (described in Sec. 2.3.1) and with a dist</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116– 131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Halko</author>
<author>P G Martinsson</author>
<author>J A Tropp</author>
</authors>
<title>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions.</title>
<date>2009</date>
<tech>Technical Report 2009-05,</tech>
<institution>ACM, California Institute of Technology,</institution>
<contexts>
<context position="4445" citStr="Halko et al., 2009" startWordPosition="656" endWordPosition="659"> for each pair of words the synsets with the highest similarity score were selected. For all three measures, we made use of the implementations provided as part of the Natural Language ToolKit for Python (Bird et al., 2009). 2.2.2 Distributional semantics Word similarity scores were also obtained from two DSM: Distributional Memory (Baroni and Lenci, 2010) and a model compiled from a version of the English Wikipedia.2 For Distributional Memory, we chose the collapsed W xW matricization, resulting in a 30686 x 30686 matrix that was further reduced to 300 latent dimensions using randomized SVD (Halko et al., 2009). For the Wikipedia DSM, we used a L2/R2 context window and mid-frequency feature terms, resulting in a 77598 x 30484 matrix. Co-occurrence frequency counts were weighted using sparse log-likelihood association scores with a square root transformation, and reduced to 300 latent dimensions with randomized SVD. In both cases, target terms are POS-disambiguated lemmas of content words, and the angle between vectors was used as a distance measure (equivalent to cosine similarity). For each DSM, we computed the following semantic distances: (i) angle: the angle between the two word vectors; (ii) fw</context>
</contexts>
<marker>Halko, Martinsson, Tropp, 2009</marker>
<rawString>N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions. Technical Report 2009-05, ACM, California Institute of Technology, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome Friedman</author>
</authors>
<date>2001</date>
<booktitle>The Elements of Statistical Learning. Data Mining, Inference, and Prediction.</booktitle>
<publisher>Springer,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="10710" citStr="Hastie et al., 2001" startWordPosition="1656" endWordPosition="1659">escribed in Sec. 2.3.1 and 2.3.2 were combined into a single STS prediction by supervised regression. We conducted experiments with various machine learning algorithms implemented in the Python library scikit-learn (Pedregosa et al., 2011). In particular, we tested linear regression, regularized linear regression (ridge regression), Bayesian ridge regression, support vector regression and regression trees. Our final system submitted to the shared task uses ridge regression, a shrinkage method applied to linear regression that uses a least-squares regularization on the regression coefficients (Hastie et al., 2001, 59). Intuitively speaking, the regularization term discourages large value of the regression coefficients, which makes the learning technique less prone to overfitting quirks of the training data, especially with large numbers of features. We tried to optimise our results by training the individual regressors for each test data set on appropriate portions of the training data. For our task submission, we used the following training data based on educated guesses inspired by the very small amount of development data provied: for the headlines test set we trained on both glosses and statistica</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2001. The Elements of Statistical Learning. Data Mining, Inference, and Prediction. Springer, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="3554" citStr="Miller et al., 1990" startWordPosition="522" endWordPosition="525">processed with TreeTagger (Schmid, 1995)1 for part-of-speech annotation and lemmatization. 1http://www.ims.uni-stuttgart.de/forschung/ ressourcen/werkzeuge/treetagger.html 181 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 181–186, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics 2.2 Similarity on word level Our alignment model (Sec. 2.3.1) is based on similarity scores for pairs of words. We obtained a total of 11 different word similarity measures from WordNet (Miller et al., 1990) and in a completely unsupervised manner from distributional semantic models. 2.2.1 WordNet We computed three state-of-the-art WordNet similarity measures, namely path similarity, Wu-Palmer similarity and Leacock-Chodorow similarity (Budanitsky and Hirst, 2006). As usual, for each pair of words the synsets with the highest similarity score were selected. For all three measures, we made use of the implementations provided as part of the Natural Language ToolKit for Python (Bird et al., 2009). 2.2.2 Distributional semantics Word similarity scores were also obtained from two DSM: Distributional M</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="10330" citStr="Pedregosa et al., 2011" startWordPosition="1603" endWordPosition="1606">eference, corpus-based similarity scores can roughly be calibrated to the respective notion of STS. In total, we obtained 4 sentence (dis)similarity scores from this approach. Because of technical issues, only the z-score measures were used in the submitted system. The experiments in Sec. 3 also focus on these z-scores. 2.4 The regression model The 24 individual similarity scores described in Sec. 2.3.1 and 2.3.2 were combined into a single STS prediction by supervised regression. We conducted experiments with various machine learning algorithms implemented in the Python library scikit-learn (Pedregosa et al., 2011). In particular, we tested linear regression, regularized linear regression (ridge regression), Bayesian ridge regression, support vector regression and regression trees. Our final system submitted to the shared task uses ridge regression, a shrinkage method applied to linear regression that uses a least-squares regularization on the regression coefficients (Hastie et al., 2001, 59). Intuitively speaking, the regularization term discourages large value of the regression coefficients, which makes the learning technique less prone to overfitting quirks of the training data, especially with large</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in part-of-speech tagging with an application to German.</title>
<date>1995</date>
<booktitle>In Proceedings of the EACL SIGDAT-Workshop,</booktitle>
<pages>47--50</pages>
<location>Dublin.</location>
<contexts>
<context position="2974" citStr="Schmid, 1995" startWordPosition="447" endWordPosition="448">ng data We trained our system on manually annotated sentence pairs from the STS task at SemEval 2012 (Agirre et al., 2012). Pooling the STS 2012 training and test data, we obtained 5 data sets from different domains, comprising a total of 5343 sentence pairs annotated with a semantic similarity score in the range [0,5]. The data sets are paraphrase sentence pairs (MSRpar), sentence pairs from video descriptions (MSRvid), MT evaluation sentence pairs (MTnews and MTeuroparl), and glosses from two different lexical semantic resources (OnWN). All sentence pairs were pre-processed with TreeTagger (Schmid, 1995)1 for part-of-speech annotation and lemmatization. 1http://www.ims.uni-stuttgart.de/forschung/ ressourcen/werkzeuge/treetagger.html 181 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 181–186, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics 2.2 Similarity on word level Our alignment model (Sec. 2.3.1) is based on similarity scores for pairs of words. We obtained a total of 11 different word similarity measures from WordNet (Miller et al., 1990) and in a completely</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements in part-of-speech tagging with an application to German. In Proceedings of the EACL SIGDAT-Workshop, pages 47–50, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="1790" citStr="Schütze (1998)" startWordPosition="257" endWordPosition="258">c textual similarity (STS) of sentence pairs. Being able to reliably measure semantic similarity can be beneficial for many applications, e.g. in the domains of MT evaluation, information extraction, question answering, and summarization. For the shared task, STS was measured on a scale ranging from 0 (indicating no similarity at all) to 5 (semantic equivalence). The system predictions were evaluated against manually annotated data. 2 Description of our approach Our system KLUE-CORE uses two approaches to estimate STS between pairs of sentences: a distributional bag-of-words model inspired by Schütze (1998), and a simple alignment model that links each word in one sentence to the semantically most similar word in the other sentence. For the alignment model, word similarities were obtained from WordNet (using a range of state-of-the-art path-based similarity measures) and from two distributional semantic models (DSM). All similarity scores obtained in this way were passed to a ridge regression learner in order to obtain a final STS score. The predictions for new sentence pairs were then transformed to the range [0,5], as required by the task definition. 2.1 The training data We trained our system</context>
<context position="8156" citStr="Schütze (1998)" startWordPosition="1246" endWordPosition="1247">rd alignment model where each word in the shorter sentence is aligned to the semantically most similar word in the longer sentence (short), and vice versa (long). Note that multiple source words may be aligned to the same target word, and target words can remain 182 unaligned without penalty. Semantic similarities are then averaged across all alignment pairs. In total, we obtained 22 sentence similarity scores from this approach. 2.3.2 Distributional similarity We computed distributional similarity between the sentences in each pair directly using bag-of-words centroid vectors as suggested by Schütze (1998), based on the two word-level DSM introduced in Sec. 2.2.2. For each sentence pair and DSM, we computed (i) the angle between the centroid vectors of the two sentences and (ii) a z-score relative to all other sentences in the same data set of the training or test collection. Both values are measures of semantic distance, but are automatically transformed into similarity measures by the regression learner (Sec. 2.4). For the z-scores, we computed the semantic distance (i.e. angle) between the first sentence of a given pair and the second sentences of all word pairs in the same data set. The res</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Hinrich Schütze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>