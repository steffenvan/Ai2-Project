<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<note confidence="0.996124">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<title confidence="0.998743">
Cross-Cutting Aspects
of
Cross-Language Question Answering Systems
</title>
<author confidence="0.992477">
Bogdan Sacaleanu
</author>
<affiliation confidence="0.773619666666667">
Language Technology Group
DFKI GmbH
Saarbrücken, Germany
</affiliation>
<email confidence="0.968985">
bogdan@dfki.de
</email>
<author confidence="0.971395">
Günter Neumann
</author>
<affiliation confidence="0.757793">
Language Technology Group
DFKI GmbH
Saarbrücken, Germany
</affiliation>
<email confidence="0.980292">
neumann@dfki.de
</email>
<sectionHeader confidence="0.995361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996932">
We describe on-going work in the devel-
opment of a cross-language question-
answering framework for the open do-
main. An overview of the framework is
being provided, some details on the im-
portant concepts of a flexible framework
are presented and two cross-cutting as-
pects (cross-linguality and credibility) for
question-answering systems are up for
discussion.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961666666667">
Different projects, different evaluation forums,
different tasks, different languages, different
document collections, different question types,
different answer processing strategies ... Anyone
familiar with all these concepts knows the com-
plexity and what a daunting prospect of develop-
ing a QA-System easily adaptable to ever chang-
ing requirements this might be. We have started
off with a “prototype-and-go” approach, trying to
keep pace with the emergence of new tasks and
managing the scarcity of your time and of your
resources, to realize later on that what we had is
a bunch of prototypes very tuned to their task
requirements. Trying to adapt them to new re-
quirements seemed often more difficult then
starting off with a new one. Therefore we started
looking for an alternative, which should be more
flexible and should allow us to cover much more
requirements’ variations; in other words we were
considering putting together a Question Answer-
ing framework.
In the rest of the paper we will shortly over-
view the components of such a framework and
will describe the relevant aspects of the solution
offered for each of them, aspects that should ac-
count for a large variety of question types,
document collections and answer processing
techniques, as well as for several languages. We
will continue with a discussion of two issues that
cut across several components of the framework,
namely: cross-linguality and answer credibility,
and will conclude by shortly naming the domains
of usage for the framework and future work.
</bodyText>
<sectionHeader confidence="0.985569" genericHeader="method">
2 Framework Overview
</sectionHeader>
<bodyText confidence="0.999498588235294">
Based on an existing set of cross-language
Question Answering prototypes developed for
different requirements, we began by looking for
the commonalities among them. Following is a
list of reusable components that might be
considered as a starting point in defining a QA
framework (see Figure 1).
Several components along the work-flow of a
typical QA system were identified: a Unit
Alignment component in cross-language envi-
ronments and a Query Expansion compo-
nent for the Question Analysis task; a Unit
Processor and a Query Generator com-
ponent for the Information Retrieval task; a
Mention Chain component for the Answer
Extraction task and a Scoring Strategy for
the Answer Selection task. (see Figure 1)
</bodyText>
<page confidence="0.993775">
15
</page>
<note confidence="0.992452">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<figureCaption confidence="0.99976">
Figure 1: Framework Architecture
</figureCaption>
<bodyText confidence="0.98426975">
Beside the reusability aspect in context of a
QA framework, extensibility is a further
important issue. We have approached it by con-
centrating on a flexible representation format of
the data being passed around in the framework,
namely the representation of the Question-
Object, the InformationUnit and the An-
swerObject.
</bodyText>
<subsectionHeader confidence="0.83806">
2.1 Reusability
</subsectionHeader>
<bodyText confidence="0.999586464285714">
“Reusability is the likelihood a segment of
structured code can be used again to add new
functionalities with slight or no modification”.
(Wikipedia)
In a cross-language setting of a QA system,
which is crossing the language barrier at the user
end rather then at the document end, there are
two approaches for getting the formalized user
information need (QuestionObject) to the
documents’ language: either creating it based on
the question translation or analyzing the original
question and translating the formalized result.
This last approach is convenient when machine
readable dictionaries, part-of-speech taggers and
other bilingual lexical resources (i.e. WordNets)
for both languages are available. For this purpose
a Unit Alignment Component was designed
that produces an alignment of simple (words)
and complex (phrases) syntactic units from the
source to target language.
Query Expansion is another component
present among the support structures of our
framework. Backed by lexico-semantic resources
as EuroWordNet [V98] it can be used for all lan-
guages supported by these resource. For a given
input word, it can return synonyms, hyponyms
and hypernyms according to the following algo-
rithm:
</bodyText>
<equation confidence="0.786727">
if (trgtWord_is_not_ambig)
return Information;
else if (trgtWord_is_ambig)
{
TRANS:
</equation>
<listItem confidence="0.9985295">
1. translate Question
2. do Unit Alignment
3. if (transTrgtWord_is_EWN_aligned)
a. if (alignment_is_not_ambig)
return Information;
b. else if (alignment_is_ambig)
save Alignments;
goto TRANS;
</listItem>
<equation confidence="0.5968685">
}
intersection = intersect_saved_alignments();
</equation>
<construct confidence="0.671257333333333">
if (intersection.size == N) // strict N=1
return Information_for_intersection;
return NULL;
</construct>
<bodyText confidence="0.999307769230769">
An example for the above algorithm up to the
stage 3.a is given in Section 3.1. This represents
the ideal case, when our input can be
disambiguated using the alignments of a question
translation. But more often it is the case of
advancing over this stage to 3.b, when the
ambiguous EuroWordNet alignments are being
saved and a new translation of the question
through other online translation services is
attempted. The idea behind this expansion
method is that lexical diversity of different
translations could narrow down the ambiguity of
a word to a desired level (N).
</bodyText>
<page confidence="0.996277">
16
</page>
<note confidence="0.853998">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.994713169014084">
To select an initial set of information units
relevant to the question, traditional search en-
gines are being used to scale down the search
space. Two important aspects in this process are:
the level of detail at which the indexation units
are chosen and the granularity of the information
units (be it document, paragraph, sentence or
phrase). Two subcomponents are important at
this stage: the Unit Processor and the
Query Generator.
The Unit Processor covers the above-
mentioned aspects: it takes as input an
InformationUnit (i.e., a raw text document)
and it either reduces it to a set of new units (i.e.,
sentences), or it annotates the unit at different
levels (i.e., named entities, grammatical rela-
tions). This way, by chaining different Unit
Processors, you can both reduce the informa-
tion unit size and generate new indexing units.
The Query Generator relies on an ab-
stract description of the processing method to
accordingly generate the IRQuery to make use
of the advanced indexation units. For example,
when named entities were annotated during the
Unit Processor chaining and used as index-
ing units, the Query Generator will adapt
the IRQuery so as to search for an additional
field (neType) that might reflect the expected
answer type for the question considered. We
consider the Query Generator as being the
mediator between the question analysis result
QuestionObject (answer type, constraints)
and the search engine serving the retrieval com-
ponent with information units (documents, sen-
tences). Even more, the Query Generator
relies on an abstract description of the search
engine, too, and can adapt the IRQuery accord-
ing to either a boolean or a ranked search engine.
A Mention Chain component in the An-
swer Extraction task provides an ease of burden
for the Selection task by computing answer ref-
erence chains. This is very helpful for factoid
questions on the Web, and not only, where re-
dundancy of an answer candidate is a good hint
for its potential selection and credibility. A men-
tion chain contains all answers sharing a com-
mon normalized representation, determined ei-
ther through the string similarity of the answers
only or by additionally employing context en-
tailment measures.
The Scoring Strategy component builds
on the mathematical graph theory and reduces
the answer candidate scoring issue to a shortest
path problem on lexical graphs. In most of the
cases the answer “suitability” could be scaled
down to computing a distance metric for the an-
swer and some information in the question (i.e.,
keywords, focus). Both a simple textual distance
measure and another one based on dependency
structures were implemented on these graph
structures, with slight variations making use of
weight and cost properties for graph edges.
Based on available web search API (i.e.,
Google, Yahoo) the Answer Validation
component computes a total frequency count of
co-occurrence for pairs of question and answer,
assuming that the right answer shares more con-
texts with the question than any other candidate
and that the considered answers are semantic
independent and insensitive with respect to the
timeline preferred by the search engines.
</bodyText>
<subsectionHeader confidence="0.989194">
2.2 Extensibility
</subsectionHeader>
<bodyText confidence="0.999640657142857">
“Extensibility is a system design principle where
the implementation takes into consideration fu-
ture growth. ... The central theme is to provide
for change while minimizing impact to existing
system functions.” (Wikipedia)
Extensibility can be approached by two meth-
ods during framework design: through software
design patterns and through a common extensible
data representation format. While we have used
some patterns through the development of reus-
able components like the chaining aspect in the
Unit Processor, the normalization function in the
Mention Chain and the graph design in the Scor-
ing Strategy, we have concentrated more on an
extensible representation format for data being
passed around through the framework: the
QuestionObject, the InformationUnit
and the AnswerObject. For this purpose we
have used XML, XML Schema and data binding
methods (JAXB) to guarantee component life on
an evolving data format. The primary benefit of
allowing extensibility in a format is that it en-
ables a format to evolve without requiring central
control of the format. A secondary benefit is that
it allows the format to stay focused and simple
by pushing specialized cases and complex solu-
tions into optionally supported extensions. W3C
XML Schema provides two features that promote
extensibility in XML vocabularies: the wildcards
xs:any and xs:anyAttribute are used to
allow the occurrence of elements and attributes
from specified namespaces into a given format,
and the xsi:type attribute that can be placed
on an element in an XML instance document to
change its type to a more refined subtype. That
</bodyText>
<page confidence="0.99636">
17
</page>
<note confidence="0.817872">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.99993825">
is, according to user’s need, the data exchanged
among the framework’s components can be ex-
tended, without changing the framework func-
tionality.
</bodyText>
<sectionHeader confidence="0.978235" genericHeader="method">
3 QA Cross-Cutting Aspects
</sectionHeader>
<bodyText confidence="0.999957875">
In a question answering framework there are as-
pects that does not directly relate to the core con-
cerns, but are needed for a proper design. More-
over, they can hardly be pinned down to a com-
ponent, as they cross several core components of
the system. We are talking about concepts like
cross-linguality and credibility – system credibil-
ity reflected in answer credibility.
</bodyText>
<subsectionHeader confidence="0.999437">
3.1 Cross-Linguality
</subsectionHeader>
<bodyText confidence="0.988622333333333">
There are three traditional approaches that count
for cross–linguality in context of information
management systems:
</bodyText>
<listItem confidence="0.9931302">
• translating the queries into the target
language,
• translating the document collection into
the source language or
• translating the queries and the docu-
</listItem>
<bodyText confidence="0.73469825">
ments into an intermediate representation
(inter–lingua).
Two types of translation services are well
known within this context which are based on
</bodyText>
<listItem confidence="0.99394425">
• lexical resources (e.g., dictionaries,
aligned wordnets), or
• machine translation (e.g., example–based
translation).
</listItem>
<bodyText confidence="0.979076071428572">
The only feasible approach when dealing with
huge amounts of data, as is the case for question
answering systems, is translating the question
into the language of the document collection and
the related issue of back-translating the answers
into the language of the user.
We are using two different methods for re-
sponding questions asked in a language different
from the one of the answer-bearing documents.
Both employ online translation services (Alta-
vista, FreeTranslation, etc.) for crossing the lan-
guage barrier, but at different processing steps:
before and after formalizing the user information
need into a QuestionObject.
The before–method translates the question
string in an earlier step, resulting in several
automatic translated strings, of which the best
one is analyzed by the Question Analysis com-
ponent and passed on to the Information Re-
trieval component. This is the strategy we use in
an English–German cross-lingual setting. To be
more precise: the English source question is
translated into several alternative German ques-
tions using online MT services. Each German
question is then parsed with SMES [NP02], our
German parser. The resulting query object is then
weighted according to its linguistic well–
formedness and its completeness wrt. query in-
formation (question type, question focus, an-
swer–type).
The assumption behind this weighting scheme
is that “a translated string is of greater utility for
subsequent processes than another one, if its lin-
guistic analysis is more complete or appropri-
ate.”
The after–method translates the formal-
ized result of the Query Analysis Component by
using the question translations, a language mod-
eling tool and a word alignment tool for creating
a mapping of the formal information need from
the source language into the target language. We
illustrate this strategy in a German–English set-
ting along two lines (using the following German
question as example: In welchem Jahr-
zehnt investierten japanische
Autohersteller sehr stark?):
- translations as returned by the on-line MT
systems are being ranked according to a
language model
In which decade did Japanese
automakers invest very
strongly? (0.7)
In which decade did Japanese
car manufacturers invest
very strongly? (0.8)
- translations with a satisfactory degree of
resemblance to a natural language utterance (i.e.
linguistically well-formedness), given by a
threshold on the language model ranking, are
aligned based on several filters: back-
propagation dictionary filter - based on MRD
(machine readable dictionaries), PoS filter -
based on statistical part-of-speech taggers, and
cognates filter - based on string similarity
measures (dice coefficient and LCSR (lowest
common substring ratio)).
In: [in:1]
welchem: [which:0.5]
Jahrzehnt: [decade:1]
investierten: [invest:1]
</bodyText>
<page confidence="0.987755">
18
</page>
<note confidence="0.829552">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.940574534482759">
japanische: [Japanese:0.5]
Autohersteller:
[car manufacturers:0.8,
auto makers:0.1]
sehr: [very:1]
stark: [strongly:0.5]
The evaluation gives evidence that both
strategies are comparable in results, whereby the
last one is slightly better, due to the fact of not
being forced to choose a best translation, but
working with and combining all the translations
available. That is, considering and combining
several, possible different, translations of the
same question, the chance of detecting a
translation error in an earlier phase of the work–
flow becomes higher and avoids error
propagations through the whole system.
The related issue of back-translating is ex-
plored by looking for parallel data to the an-
swer’s context or metadata, and extracting trans-
lation candidates based on their context and
string surface similarity. For example, in a CLEF
setting, for a German question having as English
answer “Yasser Arafat” we have extracted the
time-stamp of the answer’s context (19.07.1994),
collected all the data with a time-stamp of
07.1994 in the source language, extracted the
named entities of type PERSON and then aligned
“Jassir Arafat” based on its string surface simi-
larity to the initial answer.
The translations and their alignment to the
original question, according to the above-
mentioned after-method, have also a posi-
tive side-effect, namely: some of the aligned
words may have several ranked translations. As
it is the case of the “Autohersteller”, a word
might consider the best ranked alignment (“car
manufacturers”) as its direct translation and the
remaining ones as its expanded words. As such,
given a reliable alignment method, cross-
linguality can prove supportive even for Query
Expansion. Moreover, another method of us-
age can confirm the added value of cross-
linguality for Query Expansion, as de-
scribed below.
For this task we are using the German and the
English wordnets aligned within the EuroWord-
Net [V98] lexical resource. Our goal is to extend
the formalized information need Question
Object with synonyms for the words that are
present in the wordnet.
Considering the ambiguity of words, a WSD
module is required as part of the expansion task.
For this purpose we are using both the original
question and its translations, leveraging the re-
duction in ambiguity gained through translation.
Our devised pseudo-WSD algorithm works
as following:
</bodyText>
<listItem confidence="0.9977985">
1. look up every word from the word-
translation alignment (see example above) in the
lexical resource;
2. if the word is not ambiguous (which is, for
example, the case for Japanese) then extend
the Question Object with its synonyms (e.g.,
[Japanese, Nipponese]);
3. if the word is ambiguous (e.g., invest) then
(3a) for every possible reading of it, get its
aligned German correspondent reading (if it
exists) and look up that reading in the German
original question , e.g.,
1351398: adorn-clothe-invest (EN)
1351223: invest-vest (EN)
1400771: empower-endow-endue-gift-
indue-invest (EN)
1350325: induct-invest-seat (EN)
1293271:
• commit-invest-place-put (EN)
• anlegen-investieren (DE)
</listItem>
<bodyText confidence="0.836801615384615">
(3b) if an aligned reading is found (e.g., Read-
ing-1293271) retain it and add the English
synonyms of it to the Question Object, i.e.,
expand it with:
commit, place, put
Following the question expansion task, the
Question Object has been enriched with new
words that are synonyms of the un–ambiguous
English words and by synonyms of those
ambiguous words, whose meaning(s) have been
found in the original German question. Thus our
expanded example has gained several more
expanded words as follows:
</bodyText>
<subsectionHeader confidence="0.6967735">
{Nipponese,commit,place,put}
3.2 Answer Credibility
</subsectionHeader>
<bodyText confidence="0.9999648">
In the ideal case, a Questions Answering System
(QAS) will deliver correct answers and knows
that they are correct, i.e., it can deliver a proof of
the correctness of the answers. However, at least
for the case of open-domain textual QA
applications, this is out of reach with current
technology. Thus, current QAS can only deliver
answers with certain trustworthyness. Since, a
receiver of an answer usually assumes, that a
QAS tries to identify the best answer possible (at
</bodyText>
<page confidence="0.998188">
19
</page>
<note confidence="0.955362">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.999807333333333">
least for cooperative language games), the QAS
should assign a credibility measure to each
selected answer. The underlying decisions made,
can then also be used for explaining, how the
answer’s credibility was determined.
We view answer credibility as an additional
process to answer extraction and selection that
determines the quality of identified answer
candidates by checking the plausibility of the
answer sources and context on basis of meta
information. For example, useful document and
web page information might be:1
</bodyText>
<listItem confidence="0.997475375">
• The name of the author of this activity;
• Textual fingerprints of authority, e.g.,
“official web page of US government”;
• E-mail address of the contact person for
this activity;
• When was this webpage last updated and
links were checked (also: is there an regu-
lar update);
• The name of the host school or organiza-
tion;
• The link structure of the document, e.g.,
link IN/OUT density, links to relevant
people, other authorities, clusters / hierar-
chies of authorities;
• Text structure, e.g., textual coherence or
style;
</listItem>
<bodyText confidence="0.976117">
Another important source of meta information
relates to ontological knowledge, e.g., the consis-
tency of contextual information with respect to a
domain ontology that defines the scope of the
answer candidates. By this we mean the follow-
ing:
</bodyText>
<listItem confidence="0.997959777777778">
• Given an answer candidate A of a ques-
tion Q (e.g., an instance of a concept in
question) determined by a web-based
QAS.
• Check the textual context of A concern-
ing the mentioning of other relevant
facts/concepts, that can be determined via
access to an external (domain) ontology
using relevant terms from Q and A.
</listItem>
<footnote confidence="0.994499833333333">
1 For information on the topic of Web Credibility, cf.
http://credibility.stanford.edu/ . This URL links to the
Web Credibility Project of the Stanford Persuasive
Technology Lab. Although they do not consider
credibility under a strict QA perspective as we do,
their work and results are a rich source of inspiration.
</footnote>
<bodyText confidence="0.997053595238095">
For example, if the following request is sent to a
web-based QAS: Name IE-systems that are
based on statistical methods. Assume that for
this question, the QAS identifies a list of names
of IE-systems from textual sources as answer
candidates. Assume further, that the QAS has
access to an ontology-based meta-store about
Language Technology terms, cf. [JU05]. Then,
answer credibility checking can use the query
terms IE-system and statistical method for ex-
tracting relevant facts from this LT-store, e.g.,
names of statistical methods, IE experts, IE sub-
tasks or properties of the concept information
extraction. Next, for each answer candidate,
check the textual context of the answer for the
mentioning of these terms and their mutual rela-
tionship. Then a possible credibility heuristics
might decide that the more relevant domain-
knowledge can be identified in the context of the
answer the higher is the credibility of this an-
swer.
These examples demonstrate that a wide vari-
ety of metadata from different levels can be and
should be exploited for determining the credibil-
ity of answers. Since answer credibility is a new
research area, it is still unclear which level of
information is best suited for which kind of ques-
tions and answers. In order to be able to investi-
gate many possible credibility scenarios, we con-
sider answer credibility as a complex abstract
data type or QAS credibility model, QAS-CM
for short. The QAS-CM allows the definition of
different kinds of credibility parameters, which
are related to corresponding meta data and are
orthogonal to the component-oriented view of
the QAS. The values of the parameters might be
computed and evaluated by different compo-
nents, even in parallel. Thus, the credibility of an
answer is a complex value determined through
composition of component-related credibility
values. We are distinguishing two types of meta-
data:
</bodyText>
<listItem confidence="0.931511666666667">
• Static metadata: are available directly
through the textual source of an answer
candidate and are represented in form of
annotations, e.g. HTML/XML tags.
• Dynamic metadata: are computed
online via the components of the QAS,
e.g., linguistic entities, semantic relations,
textual entailment, text structure and co-
herence, topic linkage.
</listItem>
<page confidence="0.984435">
20
</page>
<note confidence="0.972498">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<figureCaption confidence="0.999688">
Figure 2. Credibility Model
</figureCaption>
<bodyText confidence="0.999615">
Following this perspective, we propose the
following structure for the QAS-CM (see Figure
2). We distinguish two major subsystems: the
QAS-CM itself and the QAS runtime instance.
The QAS-CM is further subdivided into three
modules:
</bodyText>
<listItem confidence="0.990773666666667">
1. CM-description
2. Data-Model
3. Process-Model
</listItem>
<bodyText confidence="0.997139">
The CM-description module defines the
credibility properties in a declarative way
independently from the specific QAS instance
used. It defines the name space of the credibility
properties that are to be implemented and the
way they are composed. We assume that each
individual credibility property returns a real
number, i.e., we consider it as a utility function,
cf. [RN03]. Thus, the composition rules define
how a complex value is computed, e.g., by
summation or multiplication. The CM-
description also contains specification about the
actual QAS instance, e.g., the type of document
source or natural language to be processed. The
CM-description serves as a blueprint for the
other two modules, the Data-Model and the
Process-Model.
The Data-Model implements credibility prop-
erties as decision rules over instantiated meta
information taking into account either static or
dynamic meta data. For example a rule like
entails(&lt;SOURCE&gt;,”official
web page of &lt;LOCATION&gt;”)
authority:=aI
assigns the weight aI to the credibility property
with name authority if the document source
of the answer candidate has a tag &lt;source&gt;
which contains a substring “official web
page &lt;LOCATION&gt;”, where &lt;LOCATION&gt;
contains a location name which corresponds to
one mentioned in the Wh-question. For a
question like “Name all German chancellors after
World War II.”, then an answer with a web page
source, that contains something like &lt;SOURCE&gt;
... official web page of Germany ...
&lt;/SOURCE&gt; will receive a credibility value aI.
The Process-Model is related to the compo-
nents of the QAS that have been used for the
processing of a question/answer pair. We assume
that each component can assign a utility measure
for its determined output. In doing so, a compo-
nent should also take into account the size and
utility measure of its input and the ambiguity
degree of its output etc. For example, assume
that the question analysis component (cf. sec. 3)
can recognize the syntactic quality of a parse tree
of a Wh-question, as being complete, par-
tial or null, cf. [NS05a]. Then a possible
decision rule for the Process-Model might be
syntactic_quality(&lt;QUESTION&gt;
, COMPLETE) � QueryAna-
lyser:=aI=1
in case, the parser of the question analysis
component was able to completely parse the Wh-
question &lt;QUESTION&gt;. In a similar way, the
web-validator mentioned in sec. 2.1 can also be
integrated as a credibility function into our QAS-
CM using the following decision rule:
</bodyText>
<page confidence="0.997271">
21
</page>
<note confidence="0.874526">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<equation confidence="0.433719333333333">
validate(Web, &lt;QUESTION&gt;,
&lt;CurrAnsw&gt;) � WebValua-
tor:=aI
</equation>
<bodyText confidence="0.999934333333333">
Note that in case statistical components are used
in QAS, e.g., a stochastic NE-recognizer, then
the probabilities of the those components can be
used for defining the utility values for the
representative decision rules. Furthermore note,
that each decision rule of the Process-Model
corresponds to a single component. The
compositon of several such rules are defined in
the CM-description module.
We are currently implementing QAS-CM
following the same object-oriented framework as
described in sec. 3, which eases integration of the
components of the QAS. The data and the
process model are implemented as a set of
production rules using the RuleML language, cf.
http://www.ruleml.org/. Currently, we define the
values of aI manually. This is more easier to
implement but more tricky to maintain because
of potential mutual interdependencies between
individual values. Therefore, in the next
development cycle of our QAS-CM, we will use
a statistical approach for automatically acquiring
optimal parameter settings. Starting point will be
a question/answer/document corpus. This corpus
can directly be used for training the Data-Model.
In the case of the Process-Model, a Bayesian
Network will be dynamically trained following
ideas from IE-research, cf. [PP03]. Since in this
case we also need output from all major QAS
components, we are integrating an exhaustive
tracing mechanism into our QA framework as a
basis for automatically determining initial
training material.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999910904761905">
The presented QA framework has been used
both in mono-lingual and cross-lingual scenarios
for closed document collections (CLEF collec-
tion) and open document collections (World
Wide Web). In terms of reusability and extensi-
bility, the framework allowed for up to two
weeks of work for building fully functional QA
systems for different use scenarios. In terms of
time performance for systems build upon the
framework, the following figures apply: for sys-
tems used to query the Web a response time of
up to 20 seconds in mono-lingual settings could
be measured; those querying the CLEF document
collection in a mono-lingual setting (German
only) registered a latency of up to 3 seconds and
for a cross-lingual setting of up to 15 seconds.
The qualitative performance has been measured
only on closed document collection and the best
results for 200 questions of the CLEF 2005
evaluation campaign in different use scenarios,
according to [NS05b], were as follows:
</bodyText>
<table confidence="0.986919">
Right Wrong Inexact
DeDe 87 43.5% 100 13
DeEn 51 25.5% 141 8
EnDe 46 23% 141 12
</table>
<sectionHeader confidence="0.94836" genericHeader="conclusions">
Reference
</sectionHeader>
<reference confidence="0.998620172413793">
[JU05] B. Jörg and H. Uszkoreit. The Ontology-
based Architecture of LT World, a Comprehensive
Web Information System for a Science and Tech-
nology Discipline. Leitbild Informations-
kompetenz: Positionen - Praxis - Perspektiven im
europäischen Wissensmarkt. 27. Online Tagung,
2005.
[NP02] G. Neumann and J. Piskorski. A shallow text
processing core engine. Computational Intelli-
gence, 18(3):451–476, 2002.
[NS05a] G. Neumann and S. Sacaleanu. Experiments
on robust NL-question interpretation and multi-
layered document annotation for a cross-language
question/answering system. In Clef 2004, volume
3491. Springer-Verlag LNCS, 2005.
[NS05b] G. Neumann and B. Sacaleanu. DFKI&apos;s LT-
lab at the CLEF 2005 Multiple Language Question
AnsweringTrack. In Working Notes for the CLEF
2005 Workshop, 21-23 September, Vienna, Aus-
tria, 2005.
[PP03] L. Peshkin and A. Pfefer. Bayesian Informa-
tion Extraction Network. In proceedings of IJCAI,
2003.
[RN03] S. Russell and P. Norvig. Artificial Intelli-
gence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, 2nd edition, 2003.
[V98] Vossen, P. (eds) 1998 EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks,
Kluwer Academic Publishers, Dordrecht.
</reference>
<page confidence="0.999035">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.247382">
<note confidence="0.61995">EACL 2006 Workshop on Multilingual Question Answering - MLQA06</note>
<title confidence="0.975921">Cross-Cutting Aspects of Cross-Language Question Answering Systems</title>
<author confidence="0.968429">Bogdan</author>
<affiliation confidence="0.8975075">Language Technology DFKI</affiliation>
<address confidence="0.965718">Saarbrücken, Germany</address>
<email confidence="0.899481">bogdan@dfki.deGünter</email>
<affiliation confidence="0.882462">Language Technology DFKI</affiliation>
<address confidence="0.961765">Saarbrücken, Germany</address>
<email confidence="0.998819">neumann@dfki.de</email>
<abstract confidence="0.991042636363636">We describe on-going work in the development of a cross-language questionanswering framework for the open domain. An overview of the framework is being provided, some details on the important concepts of a flexible framework are presented and two cross-cutting aspects (cross-linguality and credibility) for question-answering systems are up for discussion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Jörg</author>
<author>H Uszkoreit</author>
</authors>
<title>The Ontologybased Architecture of LT World, a Comprehensive Web Information System for a Science and Technology Discipline. Leitbild Informationskompetenz: Positionen - Praxis - Perspektiven im europäischen Wissensmarkt. 27. Online Tagung,</title>
<date>2005</date>
<contexts>
<context position="20898" citStr="[JU05]" startWordPosition="3237" endWordPosition="3237">ty.stanford.edu/ . This URL links to the Web Credibility Project of the Stanford Persuasive Technology Lab. Although they do not consider credibility under a strict QA perspective as we do, their work and results are a rich source of inspiration. For example, if the following request is sent to a web-based QAS: Name IE-systems that are based on statistical methods. Assume that for this question, the QAS identifies a list of names of IE-systems from textual sources as answer candidates. Assume further, that the QAS has access to an ontology-based meta-store about Language Technology terms, cf. [JU05]. Then, answer credibility checking can use the query terms IE-system and statistical method for extracting relevant facts from this LT-store, e.g., names of statistical methods, IE experts, IE subtasks or properties of the concept information extraction. Next, for each answer candidate, check the textual context of the answer for the mentioning of these terms and their mutual relationship. Then a possible credibility heuristics might decide that the more relevant domainknowledge can be identified in the context of the answer the higher is the credibility of this answer. These examples demonst</context>
</contexts>
<marker>[JU05]</marker>
<rawString>B. Jörg and H. Uszkoreit. The Ontologybased Architecture of LT World, a Comprehensive Web Information System for a Science and Technology Discipline. Leitbild Informationskompetenz: Positionen - Praxis - Perspektiven im europäischen Wissensmarkt. 27. Online Tagung, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
<author>J Piskorski</author>
</authors>
<title>A shallow text processing core engine.</title>
<date>2002</date>
<journal>Computational Intelligence,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="12721" citStr="[NP02]" startWordPosition="1966" endWordPosition="1966">at different processing steps: before and after formalizing the user information need into a QuestionObject. The before–method translates the question string in an earlier step, resulting in several automatic translated strings, of which the best one is analyzed by the Question Analysis component and passed on to the Information Retrieval component. This is the strategy we use in an English–German cross-lingual setting. To be more precise: the English source question is translated into several alternative German questions using online MT services. Each German question is then parsed with SMES [NP02], our German parser. The resulting query object is then weighted according to its linguistic well– formedness and its completeness wrt. query information (question type, question focus, answer–type). The assumption behind this weighting scheme is that “a translated string is of greater utility for subsequent processes than another one, if its linguistic analysis is more complete or appropriate.” The after–method translates the formalized result of the Query Analysis Component by using the question translations, a language modeling tool and a word alignment tool for creating a mapping of the fo</context>
</contexts>
<marker>[NP02]</marker>
<rawString>G. Neumann and J. Piskorski. A shallow text processing core engine. Computational Intelligence, 18(3):451–476, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
<author>S Sacaleanu</author>
</authors>
<title>Experiments on robust NL-question interpretation and multilayered document annotation for a cross-language question/answering system.</title>
<date>2005</date>
<booktitle>In Clef 2004, volume 3491. Springer-Verlag LNCS,</booktitle>
<contexts>
<context position="25186" citStr="[NS05a]" startWordPosition="3913" endWordPosition="3913">b page of Germany ... &lt;/SOURCE&gt; will receive a credibility value aI. The Process-Model is related to the components of the QAS that have been used for the processing of a question/answer pair. We assume that each component can assign a utility measure for its determined output. In doing so, a component should also take into account the size and utility measure of its input and the ambiguity degree of its output etc. For example, assume that the question analysis component (cf. sec. 3) can recognize the syntactic quality of a parse tree of a Wh-question, as being complete, partial or null, cf. [NS05a]. Then a possible decision rule for the Process-Model might be syntactic_quality(&lt;QUESTION&gt; , COMPLETE) � QueryAnalyser:=aI=1 in case, the parser of the question analysis component was able to completely parse the Whquestion &lt;QUESTION&gt;. In a similar way, the web-validator mentioned in sec. 2.1 can also be integrated as a credibility function into our QASCM using the following decision rule: 21 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 validate(Web, &lt;QUESTION&gt;, &lt;CurrAnsw&gt;) � WebValuator:=aI Note that in case statistical components are used in QAS, e.g., a stochastic NE-reco</context>
</contexts>
<marker>[NS05a]</marker>
<rawString>G. Neumann and S. Sacaleanu. Experiments on robust NL-question interpretation and multilayered document annotation for a cross-language question/answering system. In Clef 2004, volume 3491. Springer-Verlag LNCS, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
<author>B Sacaleanu</author>
</authors>
<title>DFKI&apos;s LTlab at the CLEF</title>
<date>2005</date>
<booktitle>In Working Notes for the CLEF 2005 Workshop,</booktitle>
<pages>21--23</pages>
<location>Vienna, Austria,</location>
<marker>[NS05b]</marker>
<rawString>G. Neumann and B. Sacaleanu. DFKI&apos;s LTlab at the CLEF 2005 Multiple Language Question AnsweringTrack. In Working Notes for the CLEF 2005 Workshop, 21-23 September, Vienna, Austria, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Peshkin</author>
<author>A Pfefer</author>
</authors>
<title>Bayesian Information Extraction Network.</title>
<date>2003</date>
<booktitle>In proceedings of IJCAI,</booktitle>
<contexts>
<context position="26966" citStr="[PP03]" startWordPosition="4178" endWordPosition="4178">ge, cf. http://www.ruleml.org/. Currently, we define the values of aI manually. This is more easier to implement but more tricky to maintain because of potential mutual interdependencies between individual values. Therefore, in the next development cycle of our QAS-CM, we will use a statistical approach for automatically acquiring optimal parameter settings. Starting point will be a question/answer/document corpus. This corpus can directly be used for training the Data-Model. In the case of the Process-Model, a Bayesian Network will be dynamically trained following ideas from IE-research, cf. [PP03]. Since in this case we also need output from all major QAS components, we are integrating an exhaustive tracing mechanism into our QA framework as a basis for automatically determining initial training material. 4 Results The presented QA framework has been used both in mono-lingual and cross-lingual scenarios for closed document collections (CLEF collection) and open document collections (World Wide Web). In terms of reusability and extensibility, the framework allowed for up to two weeks of work for building fully functional QA systems for different use scenarios. In terms of time performan</context>
</contexts>
<marker>[PP03]</marker>
<rawString>L. Peshkin and A. Pfefer. Bayesian Information Extraction Network. In proceedings of IJCAI, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Russell</author>
<author>P Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach. Prentice-Hall, Englewood Cliffs, NJ, 2nd edition,</title>
<date>2003</date>
<contexts>
<context position="23514" citStr="[RN03]" startWordPosition="3646" endWordPosition="3646">g structure for the QAS-CM (see Figure 2). We distinguish two major subsystems: the QAS-CM itself and the QAS runtime instance. The QAS-CM is further subdivided into three modules: 1. CM-description 2. Data-Model 3. Process-Model The CM-description module defines the credibility properties in a declarative way independently from the specific QAS instance used. It defines the name space of the credibility properties that are to be implemented and the way they are composed. We assume that each individual credibility property returns a real number, i.e., we consider it as a utility function, cf. [RN03]. Thus, the composition rules define how a complex value is computed, e.g., by summation or multiplication. The CMdescription also contains specification about the actual QAS instance, e.g., the type of document source or natural language to be processed. The CM-description serves as a blueprint for the other two modules, the Data-Model and the Process-Model. The Data-Model implements credibility properties as decision rules over instantiated meta information taking into account either static or dynamic meta data. For example a rule like entails(&lt;SOURCE&gt;,”official web page of &lt;LOCATION&gt;”) auth</context>
</contexts>
<marker>[RN03]</marker>
<rawString>S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice-Hall, Englewood Cliffs, NJ, 2nd edition, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
</authors>
<title>EuroWordNet: A Multilingual Database with Lexical Semantic Networks,</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="4418" citStr="[V98]" startWordPosition="670" endWordPosition="670">n the question translation or analyzing the original question and translating the formalized result. This last approach is convenient when machine readable dictionaries, part-of-speech taggers and other bilingual lexical resources (i.e. WordNets) for both languages are available. For this purpose a Unit Alignment Component was designed that produces an alignment of simple (words) and complex (phrases) syntactic units from the source to target language. Query Expansion is another component present among the support structures of our framework. Backed by lexico-semantic resources as EuroWordNet [V98] it can be used for all languages supported by these resource. For a given input word, it can return synonyms, hyponyms and hypernyms according to the following algorithm: if (trgtWord_is_not_ambig) return Information; else if (trgtWord_is_ambig) { TRANS: 1. translate Question 2. do Unit Alignment 3. if (transTrgtWord_is_EWN_aligned) a. if (alignment_is_not_ambig) return Information; b. else if (alignment_is_ambig) save Alignments; goto TRANS; } intersection = intersect_saved_alignments(); if (intersection.size == N) // strict N=1 return Information_for_intersection; return NULL; An example fo</context>
<context position="16392" citStr="[V98]" startWordPosition="2523" endWordPosition="2523">sitive side-effect, namely: some of the aligned words may have several ranked translations. As it is the case of the “Autohersteller”, a word might consider the best ranked alignment (“car manufacturers”) as its direct translation and the remaining ones as its expanded words. As such, given a reliable alignment method, crosslinguality can prove supportive even for Query Expansion. Moreover, another method of usage can confirm the added value of crosslinguality for Query Expansion, as described below. For this task we are using the German and the English wordnets aligned within the EuroWordNet [V98] lexical resource. Our goal is to extend the formalized information need Question Object with synonyms for the words that are present in the wordnet. Considering the ambiguity of words, a WSD module is required as part of the expansion task. For this purpose we are using both the original question and its translations, leveraging the reduction in ambiguity gained through translation. Our devised pseudo-WSD algorithm works as following: 1. look up every word from the wordtranslation alignment (see example above) in the lexical resource; 2. if the word is not ambiguous (which is, for example, th</context>
</contexts>
<marker>[V98]</marker>
<rawString>Vossen, P. (eds) 1998 EuroWordNet: A Multilingual Database with Lexical Semantic Networks, Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>