<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002963">
<title confidence="0.997775">
CMILLS: Adapting Semantic Role Labeling Features to Dependency
Parsing
</title>
<author confidence="0.998588">
Chad Mills Gina-Anne Levow
</author>
<affiliation confidence="0.999944">
University of Washington University of Washington
</affiliation>
<address confidence="0.8313935">
Guggenheim Hall, 4th Floor Guggenheim Hall, 4th Floor
Seattle, WA 98195, USA Seattle, WA 98195, USA
</address>
<email confidence="0.994985">
chills@uw.edu levow@uw.edu
</email>
<sectionHeader confidence="0.996569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998850117647059">
We describe a system for semantic role label-
ing adapted to a dependency parsing frame-
work. Verb arguments are predicted over nodes
in a dependency parse tree instead of nodes in
a phrase-structure parse tree. Our system par-
ticipated in SemEval-2015 shared Task 15,
Subtask 1: CPA parsing and achieved an F-
score of 0.516. We adapted features from prior
semantic role labeling work to the dependency
parsing paradigm, using a series of supervised
classifiers to identify arguments of a verb and
then assigning syntactic and semantic labels.
We found that careful feature selection had a
major impact on system performance. How-
ever, sparse training data still led rule-based
systems like the baseline to be more effective
than learning-based approaches.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998229692307692">
We describe our submission to the SemEval-2015
Task 15, Subtask 1 on Corpus Pattern Analysis
(Baisa et al. 2015). This task is similar to semantic
role labeling but with arguments based on nodes in
dependency parses instead of a syntactic parse tree.
The verb’s arguments are identified and labeled
with both their syntactic and semantic roles.
For example, consider the sentence “But he
said Labour did not agree that Britain could or
should abandon development, either for itself or for
the developing world.” This subtask involves taking
that sentence and making the following determina-
tions relative to the given verb “abandon”:
</bodyText>
<listItem confidence="0.999233">
• “Britain” is the syntactic subject of “abandon”
and falls under the “Institution” semantic type
• “development” is the syntactic object of “aban-
</listItem>
<bodyText confidence="0.9609305">
don” and is of semantic type “Activity”
We organize the remainder of our paper as fol-
lows: Section 2 describes our system, Section 3
presents experiments, and Section 4 concludes.
</bodyText>
<sectionHeader confidence="0.985278" genericHeader="introduction">
2 System Description
</sectionHeader>
<bodyText confidence="0.999820619047619">
Our system consists of a pipelined five-component
system plus source data and resources. A system di-
agram is shown in Figure 1. A cascading series of
MaxEnt classifiers are used to identify arguments,
their syntactic labels, and then their semantic labels.
Each token in an input sentence was a training ex-
ample.
Sketch Engine (Kilgarriff 2014) was used to help
with featurization. All sentences in the training data
were parsed and POS tagged using the Stanford
CoreNLP tools (Manning et al. 2014). This data was
used to generate features which are then supplied to
an Argument Identification Classifier (AIC) that
identifies whether or not a particular token is one of
the relevant verb’s arguments.
For the tokens identified as arguments to the verb,
a Syntax Classifier identifies the syntactic role of
the token. This is done using a multi-class MaxEnt
model with the same features as the AIC plus fea-
tures derived from the AIC’s predictions. A similar
Semantics Classifier follows, taking the Syntax
</bodyText>
<page confidence="0.991932">
433
</page>
<bodyText confidence="0.852511875">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 433–437,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
Classifier’s features and output. Finally, a Seman-
tics Consistency Heuristic Filter is applied to clean
up some of the predictions using a series of heuris-
tics to ensure the system is outputting semantic pre-
dictions that are consistent with the syntax
predictions for the same token.
</bodyText>
<figureCaption confidence="0.975007">
Figure 1: System Architecture Diagram. The input data
</figureCaption>
<bodyText confidence="0.811196222222222">
is parsed by the Stanford Parser and the argument heads
are expanded using the Sketch Engine thesaurus. This
data is then featurized and passed through three succes-
sive classifiers: the Argument Identification Classifier
identifies verb arguments, the Syntax Classifier assigns
syntax labels to the arguments, and the Semantics Clas-
sifier assigns semantic labels to the arguments. Finally,
the Semantics Consistency Heuristic Filter eliminates
some systematic errors in the Semantics Classifier.
</bodyText>
<subsectionHeader confidence="0.961603">
2.1 Featurization
</subsectionHeader>
<bodyText confidence="0.999858653846154">
Many of the features used in our system were in-
spired by the system produced by Toutanova et al.
(2008), which used many features from prior work.
This was a top-performing system and we incorpo-
rated each of the features that applied to the depend-
ency parsing framework adopted in this task. We
then augmented this feature set with a number of
novel additional features. Many of these were adap-
tations of Semantic Role Labeling (SRL) features
from the phrase-structure to dependency parsing
paradigm (Gildea and Jurafsky 2002, Surdeanu et
al. 2003, Pradhan et al. 2004). Others were added to
generalize better to unseen verbs, which is critical
for our task.
Some of our features depend on having a phrase-
structure parse node corresponding to the candidate
dependency parse node. Since dependency parse
nodes each correspond to a token in the sentence,
the tokens corresponding to the candidate node and
its descendants in the dependency parse tree were
identified. Then, in the phrase-structure parse tree,
the lowest ancestor to all of these tokens was taken
to be the phrase-structure parse node best corre-
sponding to the candidate dependency parse node.
The baseline features included some inspired by
Gildea and Jurafsky (2002):
</bodyText>
<listItem confidence="0.980888457142857">
• Phrase Type: the syntactic label of the corre-
sponding node in the parse tree
• Predicate Lemma: lemma of the verb
• Path: the path in the parse tree between the can-
didate syntax node and the verb including the
vertical direction and syntactic parse label of
each node (e.g. “--up--&gt;S--down--&gt;NP”)
• Position: whether the candidate is before or af-
ter the verb in the sentence
• Voice: whether the sentence is active or passive
voice; due to sparse details in Gildea and Juraf-
sky this was based on tgrep search pattern heu-
ristics found in Roland and Jurafsky (2001)
• Head Word of Phrase: the highest token in the
dependency parse under the syntax parse tree
node corresponding to the candidate token
• Sub-Cat CFG: the CFG rule corresponding to
the parent of the verb, defined by the syntactic
node labels of the parent and its children
Additional baseline features were obtained from
Surdeanu et al. (2003) and Pradhan et al. (2004):
• First/Last Word/POS: For the syntactic parse
node corresponding to the candidate node, this
includes four separate features: the first word in
the linear sentence order, its part of speech, the
last word, and its part of speech
• Left/Right Sister Phrase-Type: The Phrase Type
of each of the left and right sisters
• Left/Right Sister Head Word/POS: The word
and POS of the head of the left and right sisters
• Parent Phrase-Type: The Phrase Type of the
parent of the candidate parse node
• Parent POS/Head-Word: The word and part of
speech of the parent of the parse node corre-
sponding to the candidate node
</listItem>
<figure confidence="0.99912725">
Task Data
Sketch Engine
Syntax Classifier
Argument
Identification
Classifier
Featurization
Stanford Parser
Semantics
Consistency
Heuristic Filter
Semantics Classifier
</figure>
<page confidence="0.990934">
434
</page>
<listItem confidence="0.999572571428571">
• Iode-LCA Partial Path: The Path between the
candidate node and the lowest common ances-
tor between the candidate node and the verb
• PP Parent Head Word: The head word of the
parent node in the syntax tree, if that parent is a
prepositional phrase.
• PP IP Head Word/POS: If the syntax parse
</listItem>
<bodyText confidence="0.602438666666667">
node representing the candidate node is a PP,
the head word and POS of the rightmost NP di-
rectly under the PP.
Finally, baseline features that consisted entirely
of pairs of already-mentioned features were also
taken from Xue and Palmer (2004):
</bodyText>
<listItem confidence="0.988540153846154">
• Predicate Lemma &amp; Path
• Predicate Lemma &amp; Head Word of Phrase
• Predicate Lemma &amp; Phrase Type
• Voice &amp; Position
• Predicate Lemma &amp; PP Parent Head Word
We added additional features adapted from the
aforementioned features to generalize better given
the sparse training data relative to other SRL tasks:
• Head POS of Phrase: the tagged POS of the
Head Word of Phrase
• Head Lemma of Phrase: the lemma of the Head
Word of Phrase
• First/Last Lemma: the lemma of the first and
last word under the candidate parse node
• Left/Right Sister Head Lemma: the lemmas of
the Left/Right Sister Head Words
• Parent Head Lemma: the lemma of the Parent
Head Word
• PP Parent Head Lemma/POS: the lemma and
part of speech of the PP Parent Head Word
• PP IP Head Lemma: the lemma of the PP IP
Head Word
• Candidate CFG: the context-free grammar rule
rooted at the syntax parse node corresponding
to the candidate node (one step down from Sub-
Cat CFG)
</listItem>
<bodyText confidence="0.9482425">
Additional features were added to extend these
features or to adapt them to dependency parsing:
</bodyText>
<listItem confidence="0.992548434782608">
• Candidate DP CFG: a CFG-like expansion of
the dependency parse of the candidate node plus
children, each represented by its POS (e.g.
“NNS-&gt;PRP$” or “NNS-&gt;DT JJ NNS”)
• Sub-Cat DP CFG: a similar CFG expansion of
the dependency parse of the parent of the verb
• First/Last DP Word/Lemma/POS – of all of the
descendants of the candidate node in the de-
pendency parse, inclusive, the first/last
word/lemma/POS from the linear sentence
• Dependency Path: the path in the dependency
parse from the candidate node to the verb
• Dependency Iode-LCA Partial Path: path in
the dependency parse from the candidate node
to its lowest common ancestor with the verb
• Dependency Depth: the depth in the depend-
ency parse of the candidate node
• Dependency Descendant Coverage: of all of the
tokens under the candidate syntax parse node,
the percentage of those also under the candidate
node in the dependency parse tree. This
measures the candidate syntax and dependency
parse node alignment.
</listItem>
<bodyText confidence="0.996885117647059">
Additionally, due to the importance of the Pred-
icate Lemma feature in prior SRL work and the need
to generalize entirely to unseen verbs for evaluation
in this task, we used Sketch Engine (Kilgarriff
2014) word sketches for each verb. A word sketch
is obtained for each unseen test verb and the most
similar verb from the training data is used as the
Similar Predicate Lemma feature.
We use a novel similarity function to identify
similar verbs. A word sketch for each verb vi identi-
fies an ordered set of n grammatical relations r1i, r2i,
r3i, ..., rni that tend to co-occur with vi. These are re-
lations like “object”, “subject”, prepositional
phrases head by “of”, etc. The word sketch for each
relation rji associated with vi also includes a signifi-
cance value si(rji). For a given verb vi we calculate a
directional similarity dik with verb vk as:
</bodyText>
<equation confidence="0.9622335">
n
dik = 1(0.8)j−1|si(rji) — sk(rji)|
j=1
|si(rji) — sk(rji) |is defined as zero if the rela-
</equation>
<bodyText confidence="0.953312">
tion rji doesn’t appear in both word sketches. The
final similarity score uik between vi and vk is then:
</bodyText>
<equation confidence="0.832069">
dik + dki
uik — uki —
</equation>
<subsectionHeader confidence="0.983836">
2.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.999928833333333">
We used a series of three classifiers with similar
features, each trained using the mallet implementa-
tion of MaxEnt (McCallum 2002).
First, the AIC is a binary model predicting if a
given candidate token is an argument of the verb. In
the dependency parsing framework used for this
</bodyText>
<page confidence="0.938342">
2
435
</page>
<bodyText confidence="0.996975488888889">
task, a single token in the dependency parse would these cross-validation experiments, for each train-
represent a verbal argument. This was different ing example we used its Similar Predicate Lemma
from previous SRL tasks where a node in the parse in place of its Predicate Lemma feature. This was a
tree was taken as the argument; this is more similar pessimistic assumption that we did not apply to the
to identifying the headword of the phrase that’s an final system submitted for evaluation.1 We suspect
argument rather than identifying the full phrase. this explains why the final f-score on the test data
Each token was treated as one example, with all of was twice as good as that of the cross-validation ex-
the features described in Section 2.1 calculated for periments. The argument identification module per-
each example. We filtered out features that did not formed well on its own with an f-score of 0.627,
appear at least five times in the training data, and which is an upper bound on our overall system per-
trained with the default learning parameters. formance.
Next, the multi-class Syntax Classifier uses the We used a hill climbing heuristic search for the
same features as the AIC plus a binary feature of best possible subset of the available features. This
AIC’s score rounded to the nearest tenth, the AIC’s was a time-consuming process that involved run-
predicted class, and these last two combined. The ning cross-validation for each feature class being
labels predicted were the syntactic label associated evaluated with our three-stage classifier resulting in
with the argument in the train data. 63 classifiers being trained per iteration. All the fea-
Finally, the multi-class Semantics Classifier pre- ture removals or additions that improved perfor-
dicts the semantic label of the argument using the mance were greedily accepted, yielding 22% feature
features from the Syntax Classifier plus its output churn. The best individual feature changes predicted
score rounded to the nearest tenth as a binary fea- 0.5% improvements to overall performance, but to-
ture, its output label, and these last two combined. gether they produced only a 0.9% improvement.
2.3 Semantics Consistency Heuristic Filter We repeated this a second time but only made the
After running the classifiers, overgeneration by the five most valuable changes, yielding a 0.8% point
semantic component was cleaned up using heuris- improvement. We did not have time to continue this
tics. Semantic predictions for tokens without a syn- greedy search, leaving further performance gains
tactic prediction were removed. For tokens with a from searching for the best collection of features un-
syntactic but not semantic label prediction, if the to- realized. We ended our search with 39 feature clas-
ken appeared in the train data with a semantic label ses included, with only 21 of these from the original
the most common one was taken; if not, the most set. Through the course of these experiments, 10 of
prominent distributional synonym (determined by the original feature classes were removed while 18
the Sketch Engine thesaurus) found in the training new feature classes were added in our best model.
data that has a semantic label was used. A final series of experiments were used to heuris-
tically improve the semantic component which was
significantly overgenerating. This yielded the Se-
mantics Consistency Heuristics Filter which results
in a 5% improvement to the overall system perfor-
mance.
The final results on the test data are shown in Ta-
ble 1. The baseline system still outperformed all
teams including ours. The baseline was a heuristic
system that used two dependency parsers to be more
robust to parsing errors. It mapped dependency
parse relations to syntax output directly, with logic
to handle conjunctions, passives, and other phenom-
</bodyText>
<table confidence="0.924617083333333">
ena. Semantic labels were a mixture of hard-coded
3 Experiments
The system was evaluated using leave-one-out
cross-validation on each verb in the train data. For
the initial baseline configuration, only the features
present in prior work were included, with a total of
31 feature classes. This configuration achieved an f-
score of 0.238. The system was then run with our
new features added, which outperformed the base-
line by a relative 4% with an f-score of 0.248. In
1 Predicate Lemma is a critical feature in prior SRL work. In ture. In an attempt to mirror the features and avoid the possibil-
the test data, which only included unseen verbs, we used Sketch ity of cheating during our experiments, we repeated the same
Engine data to identify the verb in the train data most similar to process during the cross-validation experiments, treating the
the verb in the test sentence, the Similar Predicate Lemma fea- other most similar verb in the training data as the Similar Pred-
icate Lemma.
436
values for particular syntactic predictions and the
most common value in the train data for the corre-
sponding word or syntactic label.
Team F-score
Baseline 0.624
FANTASY 0.589
BLCUNLP 0.530
CMILLS (our system) 0.516
</table>
<tableCaption confidence="0.772551857142857">
Table 1: Performance on Test Data. Systems were evalu-
ated on predicting the syntactic and semantic labels for
the arguments of seven test verbs not present in the train
data. Each system was evaluated by independently meas-
uring the f-scores of its syntactic and semantic label pre-
dictions on each verb, averaged together by verb and then
across verbs to arrive at the final f-score.
</tableCaption>
<sectionHeader confidence="0.998638" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99999447368421">
The experiments suggest that more iterations of the
search for the best possible collection of features
could yield significant additional improvements in
system performance. However, we ran out of time
before being able to complete more iterations of the
search. While we trailed the second-place system by
only 1.4% in overall f-score, the first-place system
was ahead by 7.3% indicating significant improve-
ments are still possible.
Additionally, the heuristic baseline outperformed
all systems including ours, indicating that important
patterns and intuitions were not encoded into fea-
tures effectively. Given the sparsity of training data,
it is possible that having more data could have also
helped our approach based on pipelined classifiers.
In the future, we will evaluate using a single dev
set instead of using cross-validation to reduce the
computational cost of experiments. We were con-
cerned about the sparse training data, but given the
missed opportunity to further optimize the feature
sets used by our models due to computational re-
source constraints, a single dev set could have been
a much better approach. We would also like to use
features from the semantic ontology rather than
treating the semantic labels as unrelated tokens.
With our precision and recall within 2% of one
another and relatively low, it would be challenging
to reliably generate real-world lexical entries using
this system, even with a delimited scope. However,
approaches like this could be valuable at giving lex-
icographers a starting point to verify or modify, ra-
ther than starting from scratch.
This was a valuable learning experience, and
while our efforts improved performance over our
own baseline by nearly 12%, there is still plenty of
room to improve and we have a clear path to do so
by incorporating more features and improving ex-
perimental design.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.970809666666667">
Thank you to the anonymous reviewers, Ismail El
Maarouf, and Daniel Cer for their helpful com-
ments. Any mistakes that remain are our own.
</bodyText>
<sectionHeader confidence="0.996746" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999687857142857">
Baisa, Vit, et al. (2015). SemEval-2015 Task 15: A CPA
dictionary-entry-building task. Proceedings of the 9th In-
ternational Workshop on Semantic Evaluation (SemEval
2015). Denver, Co, USA, Association for Computational
Linguistics.
Gildea, Daniel; Jurafsky, Daniel. (2002). &amp;quot;Automatic La-
beling of Semantic Roles.&amp;quot; Computational Linguistics
28(3): 245-288.
Kilgarriff, Adam, et al. The Sketch Engine: ten years on.
In Lexicography (2014): 1-30.
Manning, Christopher; Surdeanu, Mihai; Bauer, John;
Finkel, Jenny; Bethard, Steven; and McClosky, David.
2014. The Stanford CoreNLP Natural Language Pro-
cessing Toolkit. In Proceedings of 52nd Annual Meeting
of the Association for Computational Linguistics: System
Demonstrations, pp. 55-60.
McCallum, Andrew Kachites. &amp;quot;MALLET: A Machine
Learning for Language Toolkit.&amp;quot; http://mal-
let.cs.umass.edu. 2002.
Pradhan, Sameer, et al. (2004). Shallow Semantic Pars-
ing using Support Vector Machines. HLT-NAACL.
Roland, Douglas and Jurafsky, Daniel (2002). &amp;quot;Verb
sense and verb subcategorization probabilities.&amp;quot; The lex-
ical basis of sentence processing: Formal, computational,
and experimental issues 4: 325-45.
Surdeanu, Mihai, et al. (2003). Using predicate-argument
structures for information extraction. Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics-Volume 1, Association for Computa-
tional Linguistics.
Toutanova, Kristina, et al. (2008). &amp;quot;A Global Joint Model
for Semantic Role Labeling.&amp;quot; Computational Linguistics
34(2): 161-191.
Xue, Nianwen and Palmer, Martha (2004). Calibrating
Features for Semantic Role Labeling. EMNLP.
</reference>
<page confidence="0.998632">
437
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976844">
<title confidence="0.9957475">CMILLS: Adapting Semantic Role Labeling Features to Dependency Parsing</title>
<author confidence="0.995693">Chad Mills Gina-Anne Levow</author>
<affiliation confidence="0.999875">University of Washington University of Washington</affiliation>
<address confidence="0.998026">Hall, Floor Guggenheim Hall, Floor Seattle, WA 98195, USA Seattle, WA 98195, USA</address>
<email confidence="0.999689">chills@uw.edulevow@uw.edu</email>
<abstract confidence="0.999546944444444">We describe a system for semantic role labeling adapted to a dependency parsing framework. Verb arguments are predicted over nodes in a dependency parse tree instead of nodes in a phrase-structure parse tree. Our system participated in SemEval-2015 shared Task 15, Subtask 1: CPA parsing and achieved an Fscore of 0.516. We adapted features from prior semantic role labeling work to the dependency parsing paradigm, using a series of supervised classifiers to identify arguments of a verb and then assigning syntactic and semantic labels. We found that careful feature selection had a major impact on system performance. However, sparse training data still led rule-based systems like the baseline to be more effective than learning-based approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vit Baisa</author>
</authors>
<title>SemEval-2015 Task 15: A CPA dictionary-entry-building task.</title>
<date>2015</date>
<booktitle>Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</booktitle>
<institution>Association for Computational Linguistics.</institution>
<location>Denver, Co, USA,</location>
<marker>Baisa, 2015</marker>
<rawString>Baisa, Vit, et al. (2015). SemEval-2015 Task 15: A CPA dictionary-entry-building task. Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, Co, USA, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.&amp;quot;</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<contexts>
<context position="4600" citStr="Gildea and Jurafsky 2002" startWordPosition="715" endWordPosition="718"> Heuristic Filter eliminates some systematic errors in the Semantics Classifier. 2.1 Featurization Many of the features used in our system were inspired by the system produced by Toutanova et al. (2008), which used many features from prior work. This was a top-performing system and we incorporated each of the features that applied to the dependency parsing framework adopted in this task. We then augmented this feature set with a number of novel additional features. Many of these were adaptations of Semantic Role Labeling (SRL) features from the phrase-structure to dependency parsing paradigm (Gildea and Jurafsky 2002, Surdeanu et al. 2003, Pradhan et al. 2004). Others were added to generalize better to unseen verbs, which is critical for our task. Some of our features depend on having a phrasestructure parse node corresponding to the candidate dependency parse node. Since dependency parse nodes each correspond to a token in the sentence, the tokens corresponding to the candidate node and its descendants in the dependency parse tree were identified. Then, in the phrase-structure parse tree, the lowest ancestor to all of these tokens was taken to be the phrase-structure parse node best corresponding to the </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel; Jurafsky, Daniel. (2002). &amp;quot;Automatic Labeling of Semantic Roles.&amp;quot; Computational Linguistics 28(3): 245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>The Sketch Engine: ten years on.</title>
<date>2014</date>
<booktitle>In Lexicography</booktitle>
<pages>1--30</pages>
<contexts>
<context position="2374" citStr="Kilgarriff 2014" startWordPosition="371" endWordPosition="372">itution” semantic type • “development” is the syntactic object of “abandon” and is of semantic type “Activity” We organize the remainder of our paper as follows: Section 2 describes our system, Section 3 presents experiments, and Section 4 concludes. 2 System Description Our system consists of a pipelined five-component system plus source data and resources. A system diagram is shown in Figure 1. A cascading series of MaxEnt classifiers are used to identify arguments, their syntactic labels, and then their semantic labels. Each token in an input sentence was a training example. Sketch Engine (Kilgarriff 2014) was used to help with featurization. All sentences in the training data were parsed and POS tagged using the Stanford CoreNLP tools (Manning et al. 2014). This data was used to generate features which are then supplied to an Argument Identification Classifier (AIC) that identifies whether or not a particular token is one of the relevant verb’s arguments. For the tokens identified as arguments to the verb, a Syntax Classifier identifies the syntactic role of the token. This is done using a multi-class MaxEnt model with the same features as the AIC plus features derived from the AIC’s predictio</context>
<context position="9784" citStr="Kilgarriff 2014" startWordPosition="1605" endWordPosition="1606">dependency parse from the candidate node to its lowest common ancestor with the verb • Dependency Depth: the depth in the dependency parse of the candidate node • Dependency Descendant Coverage: of all of the tokens under the candidate syntax parse node, the percentage of those also under the candidate node in the dependency parse tree. This measures the candidate syntax and dependency parse node alignment. Additionally, due to the importance of the Predicate Lemma feature in prior SRL work and the need to generalize entirely to unseen verbs for evaluation in this task, we used Sketch Engine (Kilgarriff 2014) word sketches for each verb. A word sketch is obtained for each unseen test verb and the most similar verb from the training data is used as the Similar Predicate Lemma feature. We use a novel similarity function to identify similar verbs. A word sketch for each verb vi identifies an ordered set of n grammatical relations r1i, r2i, r3i, ..., rni that tend to co-occur with vi. These are relations like “object”, “subject”, prepositional phrases head by “of”, etc. The word sketch for each relation rji associated with vi also includes a significance value si(rji). For a given verb vi we calculate</context>
</contexts>
<marker>Kilgarriff, 2014</marker>
<rawString>Kilgarriff, Adam, et al. The Sketch Engine: ten years on. In Lexicography (2014): 1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="2528" citStr="Manning et al. 2014" startWordPosition="395" endWordPosition="398">as follows: Section 2 describes our system, Section 3 presents experiments, and Section 4 concludes. 2 System Description Our system consists of a pipelined five-component system plus source data and resources. A system diagram is shown in Figure 1. A cascading series of MaxEnt classifiers are used to identify arguments, their syntactic labels, and then their semantic labels. Each token in an input sentence was a training example. Sketch Engine (Kilgarriff 2014) was used to help with featurization. All sentences in the training data were parsed and POS tagged using the Stanford CoreNLP tools (Manning et al. 2014). This data was used to generate features which are then supplied to an Argument Identification Classifier (AIC) that identifies whether or not a particular token is one of the relevant verb’s arguments. For the tokens identified as arguments to the verb, a Syntax Classifier identifies the syntactic role of the token. This is done using a multi-class MaxEnt model with the same features as the AIC plus features derived from the AIC’s predictions. A similar Semantics Classifier follows, taking the Syntax 433 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pag</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Manning, Christopher; Surdeanu, Mihai; Bauer, John; Finkel, Jenny; Bethard, Steven; and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Kachites. &amp;quot;MALLET: A Machine Learning for Language Toolkit.&amp;quot;</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="10796" citStr="McCallum 2002" startWordPosition="1785" endWordPosition="1786">s like “object”, “subject”, prepositional phrases head by “of”, etc. The word sketch for each relation rji associated with vi also includes a significance value si(rji). For a given verb vi we calculate a directional similarity dik with verb vk as: n dik = 1(0.8)j−1|si(rji) — sk(rji)| j=1 |si(rji) — sk(rji) |is defined as zero if the relation rji doesn’t appear in both word sketches. The final similarity score uik between vi and vk is then: dik + dki uik — uki — 2.2 Classifiers We used a series of three classifiers with similar features, each trained using the mallet implementation of MaxEnt (McCallum 2002). First, the AIC is a binary model predicting if a given candidate token is an argument of the verb. In the dependency parsing framework used for this 2 435 task, a single token in the dependency parse would these cross-validation experiments, for each trainrepresent a verbal argument. This was different ing example we used its Similar Predicate Lemma from previous SRL tasks where a node in the parse in place of its Predicate Lemma feature. This was a tree was taken as the argument; this is more similar pessimistic assumption that we did not apply to the to identifying the headword of the phra</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, Andrew Kachites. &amp;quot;MALLET: A Machine Learning for Language Toolkit.&amp;quot; http://mallet.cs.umass.edu. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
</authors>
<title>Shallow Semantic Parsing using Support Vector Machines.</title>
<date>2004</date>
<publisher>HLT-NAACL.</publisher>
<marker>Pradhan, 2004</marker>
<rawString>Pradhan, Sameer, et al. (2004). Shallow Semantic Parsing using Support Vector Machines. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Roland</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Verb sense and verb subcategorization probabilities.&amp;quot; The lexical basis of sentence processing: Formal, computational, and experimental issues 4:</title>
<date>2002</date>
<pages>325--45</pages>
<marker>Roland, Jurafsky, 2002</marker>
<rawString>Roland, Douglas and Jurafsky, Daniel (2002). &amp;quot;Verb sense and verb subcategorization probabilities.&amp;quot; The lexical basis of sentence processing: Formal, computational, and experimental issues 4: 325-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics-Volume 1, Association for Computational Linguistics.</booktitle>
<marker>Surdeanu, 2003</marker>
<rawString>Surdeanu, Mihai, et al. (2003). Using predicate-argument structures for information extraction. Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics-Volume 1, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
</authors>
<title>A Global Joint Model for Semantic Role Labeling.&amp;quot;</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>2</issue>
<pages>161--191</pages>
<marker>Toutanova, 2008</marker>
<rawString>Toutanova, Kristina, et al. (2008). &amp;quot;A Global Joint Model for Semantic Role Labeling.&amp;quot; Computational Linguistics 34(2): 161-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating Features for Semantic Role Labeling.</title>
<date>2004</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="7563" citStr="Xue and Palmer (2004)" startWordPosition="1214" endWordPosition="1217">tion Stanford Parser Semantics Consistency Heuristic Filter Semantics Classifier 434 • Iode-LCA Partial Path: The Path between the candidate node and the lowest common ancestor between the candidate node and the verb • PP Parent Head Word: The head word of the parent node in the syntax tree, if that parent is a prepositional phrase. • PP IP Head Word/POS: If the syntax parse node representing the candidate node is a PP, the head word and POS of the rightmost NP directly under the PP. Finally, baseline features that consisted entirely of pairs of already-mentioned features were also taken from Xue and Palmer (2004): • Predicate Lemma &amp; Path • Predicate Lemma &amp; Head Word of Phrase • Predicate Lemma &amp; Phrase Type • Voice &amp; Position • Predicate Lemma &amp; PP Parent Head Word We added additional features adapted from the aforementioned features to generalize better given the sparse training data relative to other SRL tasks: • Head POS of Phrase: the tagged POS of the Head Word of Phrase • Head Lemma of Phrase: the lemma of the Head Word of Phrase • First/Last Lemma: the lemma of the first and last word under the candidate parse node • Left/Right Sister Head Lemma: the lemmas of the Left/Right Sister Head Words</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Xue, Nianwen and Palmer, Martha (2004). Calibrating Features for Semantic Role Labeling. EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>