<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040044">
<title confidence="0.989126">
Efficient Large-Scale Parsing a Survey
</title>
<author confidence="0.995056">
John Carroll
</author>
<affiliation confidence="0.9876345">
Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.991205">
Brighton BN1 9QH, UK
</address>
<email confidence="0.957047">
johncaOcogs.susx.ac.uk
</email>
<author confidence="0.99148">
Stephan Oepen
</author>
<affiliation confidence="0.9814315">
Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.75684">
66041 Saarbriicken, Germany
</address>
<email confidence="0.996989">
oe@coli.uni-sb.de
</email>
<sectionHeader confidence="0.979989" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956571428571">
We survey work on the empirical assessment
and comparison of the efficiency of large-scale
parsing systems. We focus on (1) grammars and
data used to assess parser efficiency; (2) meth-
ods and tools for empirical assessment of parser
efficiency; and (3) comparisons of the efficiency
of different large-scale parsing systems.
</bodyText>
<sectionHeader confidence="0.955132" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999945836734694">
Interest in large-scale, grammar-based parsing
has recently seen a large increase, in response
to the complexities of language-based applica-
tion tasks such as speech-to-speech translation,
and enabled by the availability of more pow-
erful computational resources, and by efforts in
large-scale and collaborative grammar engineer-
ing and also in the induction of statistical gram-
mars/parsers from treebanks.
There are two main paradigms in the eval-
uation and comparison of the performance of
parsing algorithms and implemented systems:
(i) the formal, complexity-theoretic analysis
of how an algorithm behaves, typically fo-
cussing on worst-case time and space complex-
ity bounds; and (ii) the empirical study of how
properties of the parser and input (possibly in-
cluding the grammar used) affect actual, ob-
served run-time efficiency.
It has been shown (Maxwell and Kaplan,
1993; Carroll, 1994; van Noord, 1997) that
the theoretical study of algorithms alone does
not (yet) suffice to provide an accurate predic-
tion about how a specific algorithm will per-
form in practice, when used in conjunction
with a specific grammar (or type of grammar),
and when applied to a particular domain and
task. Therefore, empirical assessment of prac-
tical parser performance has become an estab-
lished technique and continues to be the pri-
mary means of comparison among algorithms.
At the same time, system competence (i.e. cov-
erage and overgeneration with respect to a par-
ticular grammar and test set) cannot be de-
coupled from the evaluation of parser perfor-
mance, because two algorithms can only be
compared meaningfully when they really solve
the same problem. This typically means that
they either directly use the same grammar, or at
least achieve demonstrably similar competence
on the same test set.
In the next section, we briefly describe large-
scale grammars and test suites that have been
used in evaluations of parser efficiency. Sec-
tion 3 discusses methods and computational
tools that have been used in such evaluations,
and Section 4 surveys research comparing the
efficiency of different parsers or parsing strate-
gies with large-scale grammars.
</bodyText>
<sectionHeader confidence="0.643213" genericHeader="introduction">
2 Grammars and Data
</sectionHeader>
<bodyText confidence="0.99118625">
A number of large-scale, general-purpose gram-
mars have been used in evaluations of parser ef-
ficiency. We describe their main characteristics
briefly below.&apos;
</bodyText>
<listItem confidence="0.993253333333333">
• The Alvey NL Tools (ANLT) contains a large,
wide-coverage sentence grammar of English
(Grover, Carroll, &amp; Briscoe, 1993), written
in a unification-based metagrammatical for-
malism resembling GPSG. The grammar ex-
pands out to an object grammar of 780 DCG-
</listItem>
<footnote confidence="0.996086666666667">
&apos;While it is the case that most current large-scale
grammar-based parsing systems construct constituent
structure representations that are capable of supporting
semantic interpretation, the English Constraint Gram-
mar (Karlsson, Voutilainen, Heikkila, &amp; Anttila, 1995)
and the Link Grammar (Sleator &amp; Temperley, 1993) sys-
tems are exceptions. Thus, since the motivations behind
these grammars are different we do not consider them
here.
</footnote>
<page confidence="0.999545">
7
</page>
<bodyText confidence="0.999335333333333">
like rules, each category containing on aver-
age around 30 nodes. Associated with the
grammar is a test suite, originally written
by the grammarian to monitor coverage dur-
ing grammar development, containing around
1,400 (mostly grammatical) items.
</bodyText>
<listItem confidence="0.992148210526316">
• The SRI Core Language Engine (CLE) gram-
mar (Alshawi, 1992) is also GPsG-inspired,
but with different treatments of a number of
central syntactic phenomena, such as subcate-
gorisation and unbounded dependencies. The
grammar contains of the order of 150 rules
which map fairly directly into a DCG.
• The LinG0 English grammar (Flickinger &amp;
Sag, 1998) is a broad-coverage HPSG de-
veloped at CSLI Stanford. The grammar
contains roughly 8,000 types and 64 lex-
ical and grammar rules, with an average
feature structure size of around 300 nodes.
Three main test sets have been used for
parser evaluation with this grammar, the
largest—containing 2,100 items—having been
extracted from VerbMobil corpora of tran-
scribed speech and balanced with respect to
sentence length. (Comparable grammars of
German and Japanese, again originally devel-
oped in VerbMobil, are shortly to be avail-
able).
• In the Xerox-led ParGram collaboration
(Butt, King, Niiio, &amp; Segond, 1999), wide-
coverage grammars of English, French, Ger-
man and a number of other languages are be-
ing developed in parallel in the LFG frame-
work, all of the grammars based on a
common set of linguistic principles, with
a commonly-agreed-upon set of grammati-
cal features. Each grammar consists of an
atomic-categoried phrase-structure backbone
augmented with feature annotations.
• The trees in the Penn Treebank induce a large
context-free grammar containing 15,000 rules.
A recent comparison of context-free parsing
strategies (Moore, 2000) has used this gram-
mar, a second one derived from an ATIS tree-
bank (with 4,600 productions), and a third
(24,500 productions) produced by comput-
ing an atomic-categoried backbone from a
unification-based phase structure grammar.
Test sentences for these grammars were de-
rived either from the associated corpora, or
artificially, by using the grammar to stochas-
tically generate random strings.
• The XTAG system grammar (XTAG, 1995) is
a large-scale lexicalised tree adjoining gram-
mar of English, developed by several re-
searchers over the past ten years or so. The
grammar contains of the order of 500 elemen-
tary tree schemata, organised into families;
each lexeme is associated with a number of
these families. Nodes in the tree schemata are
augmented with feature structures so that in-
formation can be passed non-locally between
elementary trees.
</listItem>
<bodyText confidence="0.97353652">
Test suites supplied with grammars have typ-
ically been written by the grammar develop-
ers themselves for the purpose of monitoring
over- and under-generation as the grammar is
changed. However, the test suites have also
been found to be of some value for evaluating
parser efficiency. A major drawback in this con-
text, though, is that each test suite item usually
only contains very limited ambiguity (easing the
task of checking the resulting parses), and is rel-
atively short (so that only one or two construc-
tions are tested at a time). This is also the case
for independently-developed test suites, such as
the TSNLP suites for English, French and Ger-
man (Oepen, Netter, &amp; Klein, 1997). There-
fore, in some parser evaluation work, new suites
of longer sentences have had to be constructed
manually or extracted specially from corpora.
Another important issue is the degree to
which the grammars are available to the gen-
eral NL processing research community. Those
developed within companies are in general more
difficult to obtain, although use for parser evalu-
ation may be easier to negotiate than use within
an actual application system, for instance.
</bodyText>
<sectionHeader confidence="0.972892" genericHeader="method">
3 Methods and Tools
</sectionHeader>
<bodyText confidence="0.9996924">
Previous work on the assessment and compari-
son of large-scale parsers has mostly been con-
cerned with evaluation of parser (or grammati-
cal) coverage, and with correctness of the anal-
yses produced. So, for example, coverage has
been expressed in terms of lists of grammatical
phenomena for which an analysis is provided;
over- and under-generation as the percentage
of grammatical or ungrammatical items from a
given reference set that are or are not assigned
</bodyText>
<page confidence="0.987915">
8
</page>
<bodyText confidence="0.997195217391305">
some sort of analysis; and degree of ambigu-
ity of a grammar in terms of the &apos;parse base&apos;,
the expected number of parses for a given input
length (Carroll, Briscoe, &amp; Sanfilippo, 1998).
Work on quantifying parse correctness has used
various measures of structural consistency with
respect to constituent structure annotations of
a corpus (e.g. exact match, crossing brackets,
tree similarity, and others see Black et al.,
1991, Black, Garside, &amp; Leech, 1993, Grisham,
Macleod, &amp; Sterling, 1992, and Briscoe &amp; Car-
roll, 1993); recently, more general schemes have
been advocated that deploy functor —argument
(dependency) relations as an abstraction over
different phrase structure analyses that a parser
may assign (Lin, 1995; Lehmann et al., 1996;
Carroll et al., 1998). The Penn Treebank and
the SUSANNE corpus are well-established re-
sources for the evaluation of parser accuracy.
In a sharp contrast, there is little exist-
ing methodology, let alone established refer-
ence data or software tools, for the evaluation
and contrastive comparison of parser efficiency.
Although most grammar development environ-
ments and large-scale parsing systems supply fa-
cilities to batch-process a test corpus and record
the results produced by the system, these are
typically restricted to processing a flat, unstruc-
tured input file (listing test sentences, one per
line) and outputting a small number of process-
ing results to a log file.2 Additionally, no met-
rics exist that allow the comparison of parser
efficiency across different grammars and sets
of reference data. We therefore note a strik-
ing methodological and technological deficit in
the area of precise and systematic assessment of
grammar and parser behaviour.
Recently though, a new methodology, termed
competence H performance profiling (Oepen &amp;
Flickinger, 1998; Oepen &amp; Carroll, 2000), has
been proposed that aims to fill this gap. Pro-
files are rich, precise, and structured snapshots
2Some (Meta-)Systems like PLEUK (Calder, 1993) and
HDrug (van Noord &amp; Bouma, 1997) that facilitate the
exploration of multiple descriptive formalisms and pro-
cessing strategies come with slightly more sophisticated
benchmarking facilities and visualisation tools. However,
they still largely operate on monolithic, unannotated in-
put data sets, restrict accounting of system results to
a small number of parameters (e.g. number of analyses,
overall processing time, memory consumption, possibly
the total number of chart edges), and only offer a limited,
predefined choice of analysis techniques.
of parser competence (coverage and correctness)
and performance (efficiency), where the pro-
duction, maintenance, and inspection of pro-
files is supported by a specialised software pack-
age called [incr tsd b0].3 Profiles are stored in
a relational database that serves as the basis
for flexible report generation, visualisation, data
analysis via basic descriptive statistics, and of
course comparison to other profiles. The [incr
tsdb()] package has so far been interfaced with
some eight unification-based grammar develop-
ment and/or parsing systems, and has served
as the &apos;clearing house&apos; in a multi-site collabora-
tive effort on parser benchmarking (Flickinger,
Oepen, Tsujii, &amp; Uszkoreit, 2000), resulting in
useful feedback to all participating groups.
</bodyText>
<sectionHeader confidence="0.997525" genericHeader="method">
4 Efficiency Comparisons
</sectionHeader>
<bodyText confidence="0.999969172413793">
Many parsing algorithms suitable for NL gram-
mars have been proposed over the years, their
proponents often arguing that the number of
computational steps are minimised with respect
to alternative, competing algorithms. However,
such arguments can only be made in the case
of very closely related algorithms; qualitatively
different computations can only reliably be com-
pared empirically. So, for example, generalised
LR parsing was put forward as an improvement
over Earley-style parsing (Tomita, 1987), with a
justification made by running implementations
of the two types of parser on a medium-sized CF
grammar with attribute-value augmentations.
However, comparisons of this type have to be
done with care. The coding of different strate-
gies must use exactly equivalent techniques, and
to be able to make any general claims, the gram-
mar(s) used must be large enough to fully stress
the algorithms In particular, with grammars
admitting less ambiguity, parse time is likely
to increase more slowly with increasing input
length, and also with smaller grammars rule ap-
plication can be constrained tightly with rela-
tively simple predictive techniques. In fact, a
more recent evaluation (Moore, 2000) using a
number of large-scale CF grammars has shown
conclusively that generalised LR parsing is less
efficient than certain left-corner parsing strate-
</bodyText>
<footnote confidence="0.95033675">
2See &apos;Ilttp://www.coli.uni-sb.de/itsdbr for the
(draft) [incr tsdb()] user manual, pronunciation guide-
lines, and instructions on obtaining and installing the
package.
</footnote>
<page confidence="0.996298">
9
</page>
<bodyText confidence="0.999501441176471">
gies.
Moore and Dowding (1991) document a pro-
cess of refining a unification-based (purely
bottom-up) CKY parser (forming part of a
speech understanding system) by incorporating
top-down information to prevent it hypothesis-
ing constituents bottom-up that could not form
part of a complete analysis, given the portions of
rules already partially instantiated. An impor-
tant step was reducing the spurious prediction
of gaps by means of grammar transformations.
The refinement process was guided throughout
by empirical measurements of parser through-
put on a test corpus.
Improvements in efficiency can be gained
by specialising a general-purpose grammar to
a particular corpus. Samuelsson and Rayner
(1991) describe a machine learning technique
that is applied to the CLE grammar to pro-
duce a version of the grammar that parses ATIS
corpus sentences much faster than the original
grammar. In general there are more rules in the
specialised grammar than in the original, but
they are more specific and can thus be applied
more efficiently.
Maxwell and Kaplan (1993) investigate the
interaction between parsing with the CF back-
bone component of a grammar and the resolu-
tion of functional constraints, using a precursor
of the English ParGram grammar. A number of
parsing strategies are evaluated, in combination
with two different unifiers, on a small set of test
sentences. There is a wide gap between the best
and worst performing technique; the differences
can be justified intuitively, but not with any for-
mal analyses of computational complexity.
Carroll (1994) discusses the throughput of
three quite distinct unification-based parsing
algorithms running with the ANLT grammar.
The main findings were that exponential pars-
ing algorithm complexities with respect to
grammar size have little impact on the perfor-
mance of the parsers, since they all achieved rel-
atively good throughput, and parse table sizes
were also quite manageable. Increases in parse
times with longer inputs were also fairly con-
trolled, being roughly only quadratic. In an-
other experiment, running the ANLT grammar
with the CLE parser resulted in very poor per-
formance, suggesting that the parallel develop-
ment of the software and grammars had mad-
vertently caused them to become &apos;tuned&apos; to one
another.
van Noord (1997) presents an efficient imple-
mentation of head-corner parsing, as used in
a prototype spoken language dialogue system.
Memoisation and goal-weakening techniques are
used to reduce parser space requirements; the
head-corner parser also runs faster than imple-
mentations of left-corner, bottom-up and LR
parsers in evaluations using a DCG of Dutch
with speech recogniser word-graph input. A
further set of evaluations use the ANLT gram-
mar, allowing a tentative cross-system compar-
ison with the ANLT parser to be made.
In work concerned with parsing with large-
scale CF grammars, Moore (2000) investigates
empirically the interactions between various
types of grammar factoring and versions of
the left-corner parsing algorithm that differ in
the details of precisely how and in what order
top-down filtering information is applied. Us-
ing three very different grammars, one of the
parser/factoring combinations was found to be
consistently and significantly better than the al-
ternatives, despite being only minimally differ-
ent from the other variants. This strategy was
also shown to outperform several other major
approaches to CF parsing.
Sarkar (2000) evaluates the efficiency of a
chart-based head-corner parsing algorithm on
a corpus of 2,250 Wall Street Journal sen-
tences, using a large-scale grammar (contain-
ing 6,800 elementary tree schemata) extracted
automatically from the Penn Treebank. For
each sentence, parse times were found to corre-
late roughly exponentially with the number of
lexicalised elementary trees selected; there was
little correlation between sentence length and
parse time.
Oepen and Carroll (2000) describe and argue
for a strategy of performance profiling in the en-
gineering of parsing systems for wide-coverage
linguistic grammars. The aim is to characterise
system performance at a very detailed tech-
nical level, but at the same time to abstract
away from idiosyncracies of particular process-
ing systems. Based on insights gained from de-
tailed performance profiles of various parsing
strategies with the LinG0 English grammar, a
novel &apos;hyper-active&apos; parsing strategy is synthe-
sised and evaluated.
</bodyText>
<page confidence="0.99698">
10
</page>
<bodyText confidence="0.999700111111111">
A number of other empirically-driven re-
search efforts into efficient parsing are described
in the same journal special issue (Flickinger
et al., 2000). These include grammar-writing
techniques for improved parser efficiency, new
efficient algorithms for feature structure oper-
ations, fast pre-unification filtering, and tech-
niques for the extraction of CF grammars and
abstract machine compilation for HPsGs.
</bodyText>
<sectionHeader confidence="0.996196" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999755272727273">
Recent interest in large-scale, grammar-based
parsing (in response to the demands of complex
language-based application tasks) has led to re-
newed efforts to develop wide-coverage, general-
purpose grammars, and associated research ef-
forts into efficient parsing with these grammars.
Some initial progress has been made towards
precise empirical assessment of parser efficiency.
However, more work is needed on methods,
standard reference grammars and test data to
facilitate improved comparability.
</bodyText>
<sectionHeader confidence="0.954339" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.978197">
The first author is supported by a UK
EPSRC Advanced Fellowship, and the second
by the Deutsche Forschungsgemeinschaft Col-
laborative Research Division Resource-Adaptive
Cognitive Processes (SFB 378) project B4
(PERFORM).
</bodyText>
<sectionHeader confidence="0.986352" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.881741567164179">
Alshawi, H. (Ed.). (1992). The Core Language
Engine. Cambridge, MA: MIT Press.
Black, E., Abney, S., Flickenger, D. P., Gdaniec,
C., Grishman, R., Harrison, P., Hindle, D.,
Ingria, R., Jelinek, F., Klavans, J., Liberman,
M., Marcus, M., Roukos, S., Santorini, B.,
&amp; Strzalkowski, T. (1991). A procedure for
quantitatively comparing the syntactic cover-
age of English grammars. In Proceedings of
the 4th DARPA speech and natural language
workshop. Pacific Grove, CA: Morgan Kauf-
mann.
Black, E., Garside, R., &amp; Leech, G. (Eds.).
(1993). Statistically-driven computer gram-
mars of English. The IBM — Lancaster
approach. Amsterdam, The Netherlands:
Rodopi.
Briscoe, E., &amp; Carroll, J. (1993). Generalised
probabilistic LR parsing of natural language
(corpora) with unification-based grammars.
Computational Linguistics, 19 (1), 25-60.
Butt, M., King, T. H., Nino, M.-E., &amp; Segond,
F. (1999). A grammar writer&apos;s cookbook.
Stanford, CA: CSLI Publications.
Calder, J. (1993). Graphical interaction with
constraint-based grammars. In Proceedings of
the 3rd Pacific Rim Conference on Computa-
tional Linguistics (pp. 160-169). Vancouver,
BC.
Carroll, J. (1994). Relating complexity to
practical performance in parsing with wide-
coverage unification grammars. In Proceed-
ings of the 32nd Meeting of the Associa-
tion for Computational Linguistics (pp. 287 —
294). Las Cruces, NM.
Carroll, J., Briscoe, E., &amp; Sanfilippo, A. (1998).
Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st International
Conference on Language Resources and Eval-
uation (pp. 447-454). Granada, Spain.
Flickinger, D., Oepen, S., Tsujii, J., &amp; Uszko-
reit, H. (Eds.). (2000). Journal of Natural
Language Engineering. Special Issue on Ef-
ficient processing with HPSG: Methods, sys-
tems, evaluation. Cambridge, UK: Cam-
bridge University Press. (in press)
Flickinger, D. P., &amp; Sag, I. A. (1998). Linguistic
Grammars Online A multi-purpose broad-
coverage computational grammar of English.
In CSLI Bulletin 1999 (pp. 64 — 68). Stanford,
CA: CSLI Publications.
Grisham, R., Macleod, C., &amp; Sterling, J. (1992).
Evaluating parsing strategies using standard-
ized parse files. In Proceedings of the 3rd
ACL Conference on Applied Natural Lan-
guage Processing (pp. 156 — 161). Trento,
Italy.
Grover, C., Carroll, J., &amp; Briscoe, E. (1993).
The Alvey Natural Language Tools grammar
(4th release) (Technical Report No. 284).
University of Cambridge: Computer Labora-
tory.
Karlsson, F., Voutilainen, A., Heikkild, J., &amp;
Anttila, A. (1995). Constraint grammar: a
language-independent system for parsing un-
restricted text. Berlin, Germany: Mouton de
Gruyter.
</reference>
<page confidence="0.997122">
11
</page>
<reference confidence="0.957714565789473">
Lehmann, S., Oepen, S., Regnier-Prost, S., Net-
ter, K., Lux, V., Klein, J., Falkedal, K.,
Fouvry, F., Estival, D., Dauphin, E., Corn-
pagnion, H., Baur, J., Balkan, L., &amp; Arnold,
D. (1996). TSNLP Test Suites for Nat-
ural Language Processing. In Proceedings of
the 16th International Conference on Compu-
tational Linguistics (pp. 711 — 716). Kopen-
hagen, Denmark.
Lin, D. (1995). A dependency-based method
for evaluating broad-coverage parsers. In Pro-
ceedings of the 14th International Joint Con-
ference on Artificial Intelligence (pp. 1420 —
1425). Montreal, Canada.
Maxwell III, J. T., &amp; Kaplan, R. M. (1993). The
interface between phrasal and functional con-
straints. Computational Linguistics, 19 (4),
571-590.
Moore, R. (2000). Improved left-corner chart
parsing for large context-free grammars. In
Proceedings of the 6th International Work-
shop on Parsing Technologies (pp. 171-182).
Trento, Italy.
Moore, R., &amp; Dowding, J. (1991). Efficient
bottom-up parsing. In DARPA Speech and
Natural Language Workshop (pp. 200-203).
Asilomar, CA.
van Noord, G. (1997). An efficient implementa-
tion of the head-corner parser. Computational
Linguistics, 23 (3), 425-456.
van Noord, G., 8z Bouma, G. (1997). Hdrug.
A flexible and extendible development envi-
ronment for natural language processing. In
Proceedings of the Workshop on Computa-
tional Environments for Grammar Develop-
ment and Linguistic Engineering (pp. 91 —
98). Madrid, Spain.
Oepen, S., dgz Carroll, J. (2000). Performance
profiling for parser engineering. Natural Lan-
guage Engineering, 6 (1) (Special Issue on Ef-
ficient Processing with HPSG), 81 — 97.
Oepen, S., &amp; Flickinger, D. P. (1998). To-
wards systematic grammar profiling Test
suite technology ten years after. Journal of
Computer Speech and Language, 12 (4) (Spe-
cial Issue on Evaluation), 411 — 436.
Oepen, S., Netter, K., &amp; Klein, J. (1997).
TSNLP - Test Suites for Natural Language
Processing. In J. Nerbonne (Ed.), Linguistic
Databases (pp. 13-36). Stanford, CA: CSLI
Publications.
Samuelsson, C., &amp; Rayner, M. (1991). Quanti-
tative evaluation of explanation-based learn-
ing as an optimization tool for a large-scale
natural language system. In Proceedings of
the 12th International Joint Conference on
Artificial Intelligence (pp. 609 — 615). Sydney,
Australia.
Sarkar, A. (2000). Practical experiments in
parsing using tree adjoining grammars. In
5th international workshop on tree adjoining
grammars and related formalisms (pp. 193 —
198). Paris France.
Sleator, D., &amp; Temperley, D. (1993). Parsing
English with a link grammar. In Proceedings
of the 3rd International Workshop on Pars-
ing Technologies (pp. 277-292). Tilburg, The
Netherlands.
Tomita, M. (1987). An efficient augmented-
context-free parsing algorithm. Computa-
tional Linguistics, 13 (I), 31-46.
XTAG Group (1995). A lexicalized tree ad-
joining grammar for english (Tech. Rep. No.
IRCS Report 95-03). The Institute for Re-
search in Cognitive Science, University of
Pennsylvania.
</reference>
<page confidence="0.99831">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995308">Efficient Large-Scale Parsing a Survey</title>
<author confidence="0.930889">John</author>
<affiliation confidence="0.95892">Cognitive and Computing University of</affiliation>
<address confidence="0.891472">Brighton BN1 9QH,</address>
<email confidence="0.995777">johncaOcogs.susx.ac.uk</email>
<author confidence="0.97796">Stephan</author>
<affiliation confidence="0.8119305">Computational Saarland</affiliation>
<address confidence="0.94509">66041 Saarbriicken,</address>
<email confidence="0.996579">oe@coli.uni-sb.de</email>
<abstract confidence="0.995999857493858">We survey work on the empirical assessment and comparison of the efficiency of large-scale parsing systems. We focus on (1) grammars and data used to assess parser efficiency; (2) methods and tools for empirical assessment of parser efficiency; and (3) comparisons of the efficiency of different large-scale parsing systems. 1 Background Interest in large-scale, grammar-based parsing has recently seen a large increase, in response to the complexities of language-based application tasks such as speech-to-speech translation, and enabled by the availability of more powerful computational resources, and by efforts in large-scale and collaborative grammar engineering and also in the induction of statistical grammars/parsers from treebanks. There are two main paradigms in the evaluation and comparison of the performance of parsing algorithms and implemented systems: (i) the formal, complexity-theoretic analysis of how an algorithm behaves, typically focussing on worst-case time and space complexity bounds; and (ii) the empirical study of how properties of the parser and input (possibly including the grammar used) affect actual, observed run-time efficiency. It has been shown (Maxwell and Kaplan, 1993; Carroll, 1994; van Noord, 1997) that the theoretical study of algorithms alone does not (yet) suffice to provide an accurate prediction about how a specific algorithm will perform in practice, when used in conjunction with a specific grammar (or type of grammar), and when applied to a particular domain and task. Therefore, empirical assessment of practical parser performance has become an established technique and continues to be the primary means of comparison among algorithms. At the same time, system competence (i.e. coverage and overgeneration with respect to a particular grammar and test set) cannot be decoupled from the evaluation of parser performance, because two algorithms can only be compared meaningfully when they really solve the same problem. This typically means that they either directly use the same grammar, or at least achieve demonstrably similar competence on the same test set. In the next section, we briefly describe largescale grammars and test suites that have been used in evaluations of parser efficiency. Section 3 discusses methods and computational tools that have been used in such evaluations, and Section 4 surveys research comparing the efficiency of different parsers or parsing strategies with large-scale grammars. 2 Grammars and Data A number of large-scale, general-purpose grammars have been used in evaluations of parser efficiency. We describe their main characteristics briefly below.&apos; • The Alvey NL Tools (ANLT) contains a large, wide-coverage sentence grammar of English Carroll, 1993), written in a unification-based metagrammatical forresembling grammar exout to an object grammar of 780 DCG- &apos;While it is the case that most current large-scale grammar-based parsing systems construct constituent structure representations that are capable of supporting semantic interpretation, the English Constraint Grammar (Karlsson, Voutilainen, Heikkila, &amp; Anttila, 1995) and the Link Grammar (Sleator &amp; Temperley, 1993) systems are exceptions. Thus, since the motivations behind these grammars are different we do not consider them here. 7 like rules, each category containing on average around 30 nodes. Associated with the grammar is a test suite, originally written by the grammarian to monitor coverage during grammar development, containing around 1,400 (mostly grammatical) items. • The SRI Core Language Engine (CLE) grammar (Alshawi, 1992) is also GPsG-inspired, but with different treatments of a number of central syntactic phenomena, such as subcategorisation and unbounded dependencies. The grammar contains of the order of 150 rules which map fairly directly into a DCG. • The LinG0 English grammar (Flickinger &amp; 1998) is a broad-coverage developed at CSLI Stanford. The grammar contains roughly 8,000 types and 64 lexical and grammar rules, with an average feature structure size of around 300 nodes. Three main test sets have been used for parser evaluation with this grammar, the largest—containing 2,100 items—having been extracted from VerbMobil corpora of transcribed speech and balanced with respect to sentence length. (Comparable grammars of German and Japanese, again originally developed in VerbMobil, are shortly to be available). • In the Xerox-led ParGram collaboration King, Niiio, 1999), widecoverage grammars of English, French, German and a number of other languages are bedeveloped in parallel in the framework, all of the grammars based on a common set of linguistic principles, with a commonly-agreed-upon set of grammatical features. Each grammar consists of an atomic-categoried phrase-structure backbone augmented with feature annotations. • The trees in the Penn Treebank induce a large context-free grammar containing 15,000 rules. A recent comparison of context-free parsing strategies (Moore, 2000) has used this grammar, a second one derived from an ATIS treebank (with 4,600 productions), and a third (24,500 productions) produced by computing an atomic-categoried backbone from a unification-based phase structure grammar. Test sentences for these grammars were derived either from the associated corpora, or artificially, by using the grammar to stochastically generate random strings. • The XTAG system grammar (XTAG, 1995) is a large-scale lexicalised tree adjoining grammar of English, developed by several researchers over the past ten years or so. The grammar contains of the order of 500 elementary tree schemata, organised into families; each lexeme is associated with a number of these families. Nodes in the tree schemata are augmented with feature structures so that information can be passed non-locally between elementary trees. Test suites supplied with grammars have typically been written by the grammar developers themselves for the purpose of monitoring overand under-generation as the grammar is changed. However, the test suites have also been found to be of some value for evaluating parser efficiency. A major drawback in this context, though, is that each test suite item usually only contains very limited ambiguity (easing the task of checking the resulting parses), and is relatively short (so that only one or two constructions are tested at a time). This is also the case for independently-developed test suites, such as the TSNLP suites for English, French and Ger- (Oepen, Netter, 1997). Therefore, in some parser evaluation work, new suites of longer sentences have had to be constructed manually or extracted specially from corpora. Another important issue is the degree to which the grammars are available to the general NL processing research community. Those developed within companies are in general more difficult to obtain, although use for parser evaluation may be easier to negotiate than use within an actual application system, for instance. 3 Methods and Tools Previous work on the assessment and comparison of large-scale parsers has mostly been concerned with evaluation of parser (or grammatical) coverage, and with correctness of the analyses produced. So, for example, coverage has been expressed in terms of lists of grammatical phenomena for which an analysis is provided; overand under-generation as the percentage of grammatical or ungrammatical items from a given reference set that are or are not assigned 8 some sort of analysis; and degree of ambiguity of a grammar in terms of the &apos;parse base&apos;, the expected number of parses for a given input (Carroll, Briscoe, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contrastive comparison of parser efficiency. Although most grammar development environments and large-scale parsing systems supply facilities to batch-process a test corpus and record the results produced by the system, these are typically restricted to processing a flat, unstructured input file (listing test sentences, one per line) and outputting a small number of processresults to a log Additionally, no metrics exist that allow the comparison of parser efficiency across different grammars and sets of reference data. We therefore note a striking methodological and technological deficit in the area of precise and systematic assessment of grammar and parser behaviour. Recently though, a new methodology, termed H performance profiling Flickinger, 1998; Oepen &amp; Carroll, 2000), has been proposed that aims to fill this gap. Profiles are rich, precise, and structured snapshots (Meta-)Systems like 1993) and HDrug (van Noord &amp; Bouma, 1997) that facilitate the exploration of multiple descriptive formalisms and processing strategies come with slightly more sophisticated benchmarking facilities and visualisation tools. However, they still largely operate on monolithic, unannotated input data sets, restrict accounting of system results to a small number of parameters (e.g. number of analyses, overall processing time, memory consumption, possibly the total number of chart edges), and only offer a limited, predefined choice of analysis techniques. of parser competence (coverage and correctness) and performance (efficiency), where the production, maintenance, and inspection of profiles is supported by a specialised software packcalled tsd Profiles are stored in a relational database that serves as the basis for flexible report generation, visualisation, data analysis via basic descriptive statistics, and of comparison to other profiles. The has so far been interfaced with some eight unification-based grammar development and/or parsing systems, and has served as the &apos;clearing house&apos; in a multi-site collaborative effort on parser benchmarking (Flickinger, Oepen, Tsujii, &amp; Uszkoreit, 2000), resulting in useful feedback to all participating groups. 4 Efficiency Comparisons Many parsing algorithms suitable for NL grammars have been proposed over the years, their proponents often arguing that the number of computational steps are minimised with respect to alternative, competing algorithms. However, such arguments can only be made in the case of very closely related algorithms; qualitatively different computations can only reliably be compared empirically. So, for example, generalised LR parsing was put forward as an improvement over Earley-style parsing (Tomita, 1987), with a justification made by running implementations of the two types of parser on a medium-sized CF grammar with attribute-value augmentations. However, comparisons of this type have to be done with care. The coding of different strategies must use exactly equivalent techniques, and to be able to make any general claims, the grammar(s) used must be large enough to fully stress the algorithms In particular, with grammars admitting less ambiguity, parse time is likely to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. In fact, a more recent evaluation (Moore, 2000) using a number of large-scale CF grammars has shown conclusively that generalised LR parsing is less than certain left-corner parsing stratethe tsdb()] manual, pronunciation guidelines, and instructions on obtaining and installing the package. 9 gies. Moore and Dowding (1991) document a process of refining a unification-based (purely bottom-up) CKY parser (forming part of a speech understanding system) by incorporating top-down information to prevent it hypothesising constituents bottom-up that could not form part of a complete analysis, given the portions of rules already partially instantiated. An important step was reducing the spurious prediction of gaps by means of grammar transformations. The refinement process was guided throughout by empirical measurements of parser throughput on a test corpus. Improvements in efficiency can be gained by specialising a general-purpose grammar to a particular corpus. Samuelsson and Rayner (1991) describe a machine learning technique that is applied to the CLE grammar to produce a version of the grammar that parses ATIS corpus sentences much faster than the original grammar. In general there are more rules in the specialised grammar than in the original, but they are more specific and can thus be applied more efficiently. Maxwell and Kaplan (1993) investigate the interaction between parsing with the CF backbone component of a grammar and the resolution of functional constraints, using a precursor of the English ParGram grammar. A number of parsing strategies are evaluated, in combination with two different unifiers, on a small set of test sentences. There is a wide gap between the best and worst performing technique; the differences can be justified intuitively, but not with any formal analyses of computational complexity. Carroll (1994) discusses the throughput of three quite distinct unification-based parsing algorithms running with the ANLT grammar. The main findings were that exponential parsing algorithm complexities with respect to grammar size have little impact on the performance of the parsers, since they all achieved relatively good throughput, and parse table sizes were also quite manageable. Increases in parse times with longer inputs were also fairly controlled, being roughly only quadratic. In another experiment, running the ANLT grammar with the CLE parser resulted in very poor performance, suggesting that the parallel development of the software and grammars had madvertently caused them to become &apos;tuned&apos; to one another. van Noord (1997) presents an efficient implementation of head-corner parsing, as used in a prototype spoken language dialogue system. Memoisation and goal-weakening techniques are used to reduce parser space requirements; the head-corner parser also runs faster than implementations of left-corner, bottom-up and LR parsers in evaluations using a DCG of Dutch with speech recogniser word-graph input. A further set of evaluations use the ANLT grammar, allowing a tentative cross-system comparison with the ANLT parser to be made. In work concerned with parsing with largescale CF grammars, Moore (2000) investigates empirically the interactions between various types of grammar factoring and versions of the left-corner parsing algorithm that differ in the details of precisely how and in what order top-down filtering information is applied. Using three very different grammars, one of the parser/factoring combinations was found to be consistently and significantly better than the alternatives, despite being only minimally different from the other variants. This strategy was also shown to outperform several other major approaches to CF parsing. Sarkar (2000) evaluates the efficiency of a chart-based head-corner parsing algorithm on a corpus of 2,250 Wall Street Journal sentences, using a large-scale grammar (containing 6,800 elementary tree schemata) extracted automatically from the Penn Treebank. For each sentence, parse times were found to correlate roughly exponentially with the number of lexicalised elementary trees selected; there was little correlation between sentence length and parse time. Oepen and Carroll (2000) describe and argue a strategy of profiling the engineering of parsing systems for wide-coverage linguistic grammars. The aim is to characterise system performance at a very detailed technical level, but at the same time to abstract away from idiosyncracies of particular processing systems. Based on insights gained from detailed performance profiles of various parsing strategies with the LinG0 English grammar, a novel &apos;hyper-active&apos; parsing strategy is synthesised and evaluated. 10 A number of other empirically-driven research efforts into efficient parsing are described in the same journal special issue (Flickinger et al., 2000). These include grammar-writing techniques for improved parser efficiency, new efficient algorithms for feature structure operations, fast pre-unification filtering, and techniques for the extraction of CF grammars and abstract machine compilation for HPsGs. 5 Conclusions Recent interest in large-scale, grammar-based parsing (in response to the demands of complex language-based application tasks) has led to renewed efforts to develop wide-coverage, generalpurpose grammars, and associated research efforts into efficient parsing with these grammars. Some initial progress has been made towards precise empirical assessment of parser efficiency. However, more work is needed on methods, standard reference grammars and test data to facilitate improved comparability.</abstract>
<note confidence="0.508694714285714">Acknowledgements The first author is supported by a UK EPSRC Advanced Fellowship, and the second the Forschungsgemeinschaft Col- Research Division Processes 378) project B4 (PERFORM).</note>
<title confidence="0.6351">References</title>
<affiliation confidence="0.715969">MA: MIT Press.</affiliation>
<address confidence="0.4553156">Black, E., Abney, S., Flickenger, D. P., Gdaniec, C., Grishman, R., Harrison, P., Hindle, D., Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B., &amp; Strzalkowski, T. (1991). A procedure for</address>
<abstract confidence="0.624464285714286">quantitatively comparing the syntactic coverof English grammars. In of DARPA speech and natural language Grove, CA: Morgan Kaufmann. Black, E., Garside, R., &amp; Leech, G. (Eds.). computer gram-</abstract>
<affiliation confidence="0.500172">mars of English. The IBM — Lancaster</affiliation>
<address confidence="0.505397">The Netherlands:</address>
<note confidence="0.762489022727273">Rodopi. E., Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars. Linguistics, 19 (1), M., King, T. H., Nino, M.-E., (1999). A writer&apos;s cookbook. Stanford, CA: CSLI Publications. Calder, J. (1993). Graphical interaction with grammars. In of the 3rd Pacific Rim Conference on Computa- Linguistics 160-169). Vancouver, BC. Carroll, J. (1994). Relating complexity to practical performance in parsing with wideunification grammars. In Proceedings of the 32nd Meeting of the Associafor Computational Linguistics 287 — 294). Las Cruces, NM. E., &amp; Sanfilippo, A. (1998). Parser evaluation: a survey and a new pro- In of the 1st International Conference on Language Resources and Eval- 447-454). Granada, Spain. Flickinger, D., Oepen, S., Tsujii, J., &amp; Uszko- H. (Eds.). (2000). of Natural Language Engineering. Special Issue on Efficient processing with HPSG: Methods, sysevaluation. UK: Cambridge University Press. (in press) D. P., I. A. (1998). Linguistic Grammars Online A multi-purpose broadcoverage computational grammar of English. Bulletin 1999 64 — 68). Stanford, CA: CSLI Publications. R., Macleod, C., Evaluating parsing strategies using standardparse files. In of the 3rd ACL Conference on Applied Natural Lan- Processing 156 — 161). Trento, Italy. C., Carroll, J., E. (1993). The Alvey Natural Language Tools grammar release) Report No. 284).</note>
<affiliation confidence="0.782126">University of Cambridge: Computer Labora-</affiliation>
<abstract confidence="0.713841166666667">tory. Karlsson, F., Voutilainen, A., Heikkild, J., &amp; A. (1995). grammar: a language-independent system for parsing untext. Germany: Mouton de Gruyter.</abstract>
<note confidence="0.934931895833333">11 Lehmann, S., Oepen, S., Regnier-Prost, S., Netter, K., Lux, V., Klein, J., Falkedal, K., Fouvry, F., Estival, D., Dauphin, E., Cornpagnion, H., Baur, J., Balkan, L., &amp; Arnold, (1996). Suites for Language Processing. In of the 16th International Conference on Compu- Linguistics 711 — 716). Kopenhagen, Denmark. Lin, D. (1995). A dependency-based method evaluating broad-coverage parsers. In Proceedings of the 14th International Joint Conon Artificial Intelligence 1420 — 1425). Montreal, Canada. III, J. T., R. M. (1993). The interface between phrasal and functional con- Linguistics, 19 (4), 571-590. Moore, R. (2000). Improved left-corner chart parsing for large context-free grammars. In Proceedings of the 6th International Workon Parsing Technologies 171-182). Trento, Italy. R., J. (1991). Efficient parsing. In Speech and Language Workshop 200-203). Asilomar, CA. van Noord, G. (1997). An efficient implementaof the head-corner parser. 23 (3), van Noord, G., 8z Bouma, G. (1997). Hdrug. A flexible and extendible development environment for natural language processing. In Proceedings of the Workshop on Computational Environments for Grammar Developand Linguistic Engineering 91 — 98). Madrid, Spain. Oepen, S., dgz Carroll, J. (2000). Performance for parser engineering. Language Engineering, 6 (1) (Special Issue on Ef- Processing with HPSG), — 97. Oepen, S., &amp; Flickinger, D. P. (1998). Towards systematic grammar profiling Test technology ten years after. of Computer Speech and Language, 12 (4) (Spe- Issue on Evaluation), — 436. S., Netter, K., J. (1997).</note>
<title confidence="0.782523">Suites for Natural Language</title>
<author confidence="0.763483">In J Nerbonne</author>
<address confidence="0.859278">13-36). Stanford, CA: CSLI</address>
<abstract confidence="0.920250125">Publications. C., M. (1991). Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale language system. In of the 12th International Joint Conference on Intelligence 609 — 615). Sydney, Australia. Sarkar, A. (2000). Practical experiments in parsing using tree adjoining grammars. In 5th international workshop on tree adjoining and related formalisms 193 — 198). Paris France. D., D. (1993). Parsing with a link grammar. In of the 3rd International Workshop on Pars-</abstract>
<note confidence="0.835559818181818">Technologies 277-292). Tilburg, The Netherlands. Tomita, M. (1987). An efficient augmentedparsing algorithm. Computa- Linguistics, 13 (I), Group (1995). A tree adgrammar for english Rep. No. IRCS Report 95-03). The Institute for Research in Cognitive Science, University of Pennsylvania. 12</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>The Core Language Engine.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="3914" citStr="Alshawi, 1992" startWordPosition="592" endWordPosition="593">that are capable of supporting semantic interpretation, the English Constraint Grammar (Karlsson, Voutilainen, Heikkila, &amp; Anttila, 1995) and the Link Grammar (Sleator &amp; Temperley, 1993) systems are exceptions. Thus, since the motivations behind these grammars are different we do not consider them here. 7 like rules, each category containing on average around 30 nodes. Associated with the grammar is a test suite, originally written by the grammarian to monitor coverage during grammar development, containing around 1,400 (mostly grammatical) items. • The SRI Core Language Engine (CLE) grammar (Alshawi, 1992) is also GPsG-inspired, but with different treatments of a number of central syntactic phenomena, such as subcategorisation and unbounded dependencies. The grammar contains of the order of 150 rules which map fairly directly into a DCG. • The LinG0 English grammar (Flickinger &amp; Sag, 1998) is a broad-coverage HPSG developed at CSLI Stanford. The grammar contains roughly 8,000 types and 64 lexical and grammar rules, with an average feature structure size of around 300 nodes. Three main test sets have been used for parser evaluation with this grammar, the largest—containing 2,100 items—having bee</context>
</contexts>
<marker>Alshawi, 1992</marker>
<rawString>Alshawi, H. (Ed.). (1992). The Core Language Engine. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D P Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the 4th DARPA speech and natural language workshop.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Pacific Grove, CA:</location>
<contexts>
<context position="8248" citStr="Black et al., 1991" startWordPosition="1285" endWordPosition="1288">atical phenomena for which an analysis is provided; over- and under-generation as the percentage of grammatical or ungrammatical items from a given reference set that are or are not assigned 8 some sort of analysis; and degree of ambiguity of a grammar in terms of the &apos;parse base&apos;, the expected number of parses for a given input length (Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contras</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Black, E., Abney, S., Flickenger, D. P., Gdaniec, C., Grishman, R., Harrison, P., Hindle, D., Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B., &amp; Strzalkowski, T. (1991). A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of the 4th DARPA speech and natural language workshop. Pacific Grove, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>R Garside</author>
<author>G Leech</author>
</authors>
<title>Statistically-driven computer grammars of English. The IBM — Lancaster approach.</title>
<date>1993</date>
<location>Amsterdam, The Netherlands: Rodopi.</location>
<contexts>
<context position="8279" citStr="Black, Garside, &amp; Leech, 1993" startWordPosition="1289" endWordPosition="1293"> which an analysis is provided; over- and under-generation as the percentage of grammatical or ungrammatical items from a given reference set that are or are not assigned 8 some sort of analysis; and degree of ambiguity of a grammar in terms of the &apos;parse base&apos;, the expected number of parses for a given input length (Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contrastive comparison of parser effic</context>
</contexts>
<marker>Black, Garside, Leech, 1993</marker>
<rawString>Black, E., Garside, R., &amp; Leech, G. (Eds.). (1993). Statistically-driven computer grammars of English. The IBM — Lancaster approach. Amsterdam, The Netherlands: Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>25--60</pages>
<contexts>
<context position="8345" citStr="Briscoe &amp; Carroll, 1993" startWordPosition="1300" endWordPosition="1304">ntage of grammatical or ungrammatical items from a given reference set that are or are not assigned 8 some sort of analysis; and degree of ambiguity of a grammar in terms of the &apos;parse base&apos;, the expected number of parses for a given input length (Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contrastive comparison of parser efficiency. Although most grammar development environments and large-sc</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, E., &amp; Carroll, J. (1993). Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19 (1), 25-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
<author>T H King</author>
<author>M-E Nino</author>
<author>F Segond</author>
</authors>
<title>A grammar writer&apos;s cookbook.</title>
<date>1999</date>
<publisher>CSLI Publications.</publisher>
<location>Stanford, CA:</location>
<marker>Butt, King, Nino, Segond, 1999</marker>
<rawString>Butt, M., King, T. H., Nino, M.-E., &amp; Segond, F. (1999). A grammar writer&apos;s cookbook. Stanford, CA: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Calder</author>
</authors>
<title>Graphical interaction with constraint-based grammars.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd Pacific Rim Conference on Computational Linguistics</booktitle>
<pages>160--169</pages>
<location>Vancouver, BC.</location>
<contexts>
<context position="9803" citStr="Calder, 1993" startWordPosition="1525" endWordPosition="1526">g a small number of processing results to a log file.2 Additionally, no metrics exist that allow the comparison of parser efficiency across different grammars and sets of reference data. We therefore note a striking methodological and technological deficit in the area of precise and systematic assessment of grammar and parser behaviour. Recently though, a new methodology, termed competence H performance profiling (Oepen &amp; Flickinger, 1998; Oepen &amp; Carroll, 2000), has been proposed that aims to fill this gap. Profiles are rich, precise, and structured snapshots 2Some (Meta-)Systems like PLEUK (Calder, 1993) and HDrug (van Noord &amp; Bouma, 1997) that facilitate the exploration of multiple descriptive formalisms and processing strategies come with slightly more sophisticated benchmarking facilities and visualisation tools. However, they still largely operate on monolithic, unannotated input data sets, restrict accounting of system results to a small number of parameters (e.g. number of analyses, overall processing time, memory consumption, possibly the total number of chart edges), and only offer a limited, predefined choice of analysis techniques. of parser competence (coverage and correctness) and</context>
</contexts>
<marker>Calder, 1993</marker>
<rawString>Calder, J. (1993). Graphical interaction with constraint-based grammars. In Proceedings of the 3rd Pacific Rim Conference on Computational Linguistics (pp. 160-169). Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
</authors>
<title>Relating complexity to practical performance in parsing with widecoverage unification grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Meeting of the Association for Computational Linguistics (pp. 287 — 294). Las</booktitle>
<location>Cruces, NM.</location>
<contexts>
<context position="1491" citStr="Carroll, 1994" startWordPosition="213" endWordPosition="214">ts in large-scale and collaborative grammar engineering and also in the induction of statistical grammars/parsers from treebanks. There are two main paradigms in the evaluation and comparison of the performance of parsing algorithms and implemented systems: (i) the formal, complexity-theoretic analysis of how an algorithm behaves, typically focussing on worst-case time and space complexity bounds; and (ii) the empirical study of how properties of the parser and input (possibly including the grammar used) affect actual, observed run-time efficiency. It has been shown (Maxwell and Kaplan, 1993; Carroll, 1994; van Noord, 1997) that the theoretical study of algorithms alone does not (yet) suffice to provide an accurate prediction about how a specific algorithm will perform in practice, when used in conjunction with a specific grammar (or type of grammar), and when applied to a particular domain and task. Therefore, empirical assessment of practical parser performance has become an established technique and continues to be the primary means of comparison among algorithms. At the same time, system competence (i.e. coverage and overgeneration with respect to a particular grammar and test set) cannot b</context>
<context position="14211" citStr="Carroll (1994)" startWordPosition="2187" endWordPosition="2188">an in the original, but they are more specific and can thus be applied more efficiently. Maxwell and Kaplan (1993) investigate the interaction between parsing with the CF backbone component of a grammar and the resolution of functional constraints, using a precursor of the English ParGram grammar. A number of parsing strategies are evaluated, in combination with two different unifiers, on a small set of test sentences. There is a wide gap between the best and worst performing technique; the differences can be justified intuitively, but not with any formal analyses of computational complexity. Carroll (1994) discusses the throughput of three quite distinct unification-based parsing algorithms running with the ANLT grammar. The main findings were that exponential parsing algorithm complexities with respect to grammar size have little impact on the performance of the parsers, since they all achieved relatively good throughput, and parse table sizes were also quite manageable. Increases in parse times with longer inputs were also fairly controlled, being roughly only quadratic. In another experiment, running the ANLT grammar with the CLE parser resulted in very poor performance, suggesting that the </context>
</contexts>
<marker>Carroll, 1994</marker>
<rawString>Carroll, J. (1994). Relating complexity to practical performance in parsing with widecoverage unification grammars. In Proceedings of the 32nd Meeting of the Association for Computational Linguistics (pp. 287 — 294). Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation</booktitle>
<pages>447--454</pages>
<location>Granada,</location>
<contexts>
<context position="8005" citStr="Carroll, Briscoe, &amp; Sanfilippo, 1998" startWordPosition="1249" endWordPosition="1253">us work on the assessment and comparison of large-scale parsers has mostly been concerned with evaluation of parser (or grammatical) coverage, and with correctness of the analyses produced. So, for example, coverage has been expressed in terms of lists of grammatical phenomena for which an analysis is provided; over- and under-generation as the percentage of grammatical or ungrammatical items from a given reference set that are or are not assigned 8 some sort of analysis; and degree of ambiguity of a grammar in terms of the &apos;parse base&apos;, the expected number of parses for a given input length (Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Tree</context>
<context position="8590" citStr="Carroll et al., 1998" startWordPosition="1337" endWordPosition="1340">Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contrastive comparison of parser efficiency. Although most grammar development environments and large-scale parsing systems supply facilities to batch-process a test corpus and record the results produced by the system, these are typically restricted to processing a flat, unstructured input file (listing test sentences, one per line) and outputtin</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>Carroll, J., Briscoe, E., &amp; Sanfilippo, A. (1998). Parser evaluation: a survey and a new proposal. In Proceedings of the 1st International Conference on Language Resources and Evaluation (pp. 447-454). Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>S Oepen</author>
<author>J Tsujii</author>
<author>H Uszkoreit</author>
</authors>
<title>processing with HPSG: Methods, systems, evaluation.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering. Special Issue on Efficient</journal>
<publisher>Cambridge University Press. (in press)</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="11046" citStr="Flickinger, Oepen, Tsujii, &amp; Uszkoreit, 2000" startWordPosition="1701" endWordPosition="1706">d performance (efficiency), where the production, maintenance, and inspection of profiles is supported by a specialised software package called [incr tsd b0].3 Profiles are stored in a relational database that serves as the basis for flexible report generation, visualisation, data analysis via basic descriptive statistics, and of course comparison to other profiles. The [incr tsdb()] package has so far been interfaced with some eight unification-based grammar development and/or parsing systems, and has served as the &apos;clearing house&apos; in a multi-site collaborative effort on parser benchmarking (Flickinger, Oepen, Tsujii, &amp; Uszkoreit, 2000), resulting in useful feedback to all participating groups. 4 Efficiency Comparisons Many parsing algorithms suitable for NL grammars have been proposed over the years, their proponents often arguing that the number of computational steps are minimised with respect to alternative, competing algorithms. However, such arguments can only be made in the case of very closely related algorithms; qualitatively different computations can only reliably be compared empirically. So, for example, generalised LR parsing was put forward as an improvement over Earley-style parsing (Tomita, 1987), with a jus</context>
<context position="17217" citStr="Flickinger et al., 2000" startWordPosition="2643" endWordPosition="2646">gy of performance profiling in the engineering of parsing systems for wide-coverage linguistic grammars. The aim is to characterise system performance at a very detailed technical level, but at the same time to abstract away from idiosyncracies of particular processing systems. Based on insights gained from detailed performance profiles of various parsing strategies with the LinG0 English grammar, a novel &apos;hyper-active&apos; parsing strategy is synthesised and evaluated. 10 A number of other empirically-driven research efforts into efficient parsing are described in the same journal special issue (Flickinger et al., 2000). These include grammar-writing techniques for improved parser efficiency, new efficient algorithms for feature structure operations, fast pre-unification filtering, and techniques for the extraction of CF grammars and abstract machine compilation for HPsGs. 5 Conclusions Recent interest in large-scale, grammar-based parsing (in response to the demands of complex language-based application tasks) has led to renewed efforts to develop wide-coverage, generalpurpose grammars, and associated research efforts into efficient parsing with these grammars. Some initial progress has been made towards pr</context>
</contexts>
<marker>Flickinger, Oepen, Tsujii, Uszkoreit, 2000</marker>
<rawString>Flickinger, D., Oepen, S., Tsujii, J., &amp; Uszkoreit, H. (Eds.). (2000). Journal of Natural Language Engineering. Special Issue on Efficient processing with HPSG: Methods, systems, evaluation. Cambridge, UK: Cambridge University Press. (in press)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Flickinger</author>
<author>I A Sag</author>
</authors>
<title>Linguistic Grammars Online A multi-purpose broadcoverage computational grammar of English.</title>
<date>1998</date>
<booktitle>In CSLI Bulletin</booktitle>
<pages>64--68</pages>
<publisher>CSLI Publications.</publisher>
<location>Stanford, CA:</location>
<contexts>
<context position="4203" citStr="Flickinger &amp; Sag, 1998" startWordPosition="636" endWordPosition="639"> not consider them here. 7 like rules, each category containing on average around 30 nodes. Associated with the grammar is a test suite, originally written by the grammarian to monitor coverage during grammar development, containing around 1,400 (mostly grammatical) items. • The SRI Core Language Engine (CLE) grammar (Alshawi, 1992) is also GPsG-inspired, but with different treatments of a number of central syntactic phenomena, such as subcategorisation and unbounded dependencies. The grammar contains of the order of 150 rules which map fairly directly into a DCG. • The LinG0 English grammar (Flickinger &amp; Sag, 1998) is a broad-coverage HPSG developed at CSLI Stanford. The grammar contains roughly 8,000 types and 64 lexical and grammar rules, with an average feature structure size of around 300 nodes. Three main test sets have been used for parser evaluation with this grammar, the largest—containing 2,100 items—having been extracted from VerbMobil corpora of transcribed speech and balanced with respect to sentence length. (Comparable grammars of German and Japanese, again originally developed in VerbMobil, are shortly to be available). • In the Xerox-led ParGram collaboration (Butt, King, Niiio, &amp; Segond,</context>
</contexts>
<marker>Flickinger, Sag, 1998</marker>
<rawString>Flickinger, D. P., &amp; Sag, I. A. (1998). Linguistic Grammars Online A multi-purpose broadcoverage computational grammar of English. In CSLI Bulletin 1999 (pp. 64 — 68). Stanford, CA: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grisham</author>
<author>C Macleod</author>
<author>J Sterling</author>
</authors>
<title>Evaluating parsing strategies using standardized parse files.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd ACL Conference on Applied Natural Language Processing (pp. 156 — 161).</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="8315" citStr="Grisham, Macleod, &amp; Sterling, 1992" startWordPosition="1294" endWordPosition="1298"> over- and under-generation as the percentage of grammatical or ungrammatical items from a given reference set that are or are not assigned 8 some sort of analysis; and degree of ambiguity of a grammar in terms of the &apos;parse base&apos;, the expected number of parses for a given input length (Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contrastive comparison of parser efficiency. Although most grammar develop</context>
</contexts>
<marker>Grisham, Macleod, Sterling, 1992</marker>
<rawString>Grisham, R., Macleod, C., &amp; Sterling, J. (1992). Evaluating parsing strategies using standardized parse files. In Proceedings of the 3rd ACL Conference on Applied Natural Language Processing (pp. 156 — 161). Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>J Carroll</author>
<author>E Briscoe</author>
</authors>
<title>The Alvey Natural Language Tools grammar (4th release)</title>
<date>1993</date>
<tech>(Technical Report No. 284).</tech>
<institution>University of Cambridge: Computer Laboratory.</institution>
<contexts>
<context position="3038" citStr="Grover, Carroll, &amp; Briscoe, 1993" startWordPosition="458" endWordPosition="462">, we briefly describe largescale grammars and test suites that have been used in evaluations of parser efficiency. Section 3 discusses methods and computational tools that have been used in such evaluations, and Section 4 surveys research comparing the efficiency of different parsers or parsing strategies with large-scale grammars. 2 Grammars and Data A number of large-scale, general-purpose grammars have been used in evaluations of parser efficiency. We describe their main characteristics briefly below.&apos; • The Alvey NL Tools (ANLT) contains a large, wide-coverage sentence grammar of English (Grover, Carroll, &amp; Briscoe, 1993), written in a unification-based metagrammatical formalism resembling GPSG. The grammar expands out to an object grammar of 780 DCG&apos;While it is the case that most current large-scale grammar-based parsing systems construct constituent structure representations that are capable of supporting semantic interpretation, the English Constraint Grammar (Karlsson, Voutilainen, Heikkila, &amp; Anttila, 1995) and the Link Grammar (Sleator &amp; Temperley, 1993) systems are exceptions. Thus, since the motivations behind these grammars are different we do not consider them here. 7 like rules, each category conta</context>
</contexts>
<marker>Grover, Carroll, Briscoe, 1993</marker>
<rawString>Grover, C., Carroll, J., &amp; Briscoe, E. (1993). The Alvey Natural Language Tools grammar (4th release) (Technical Report No. 284). University of Cambridge: Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
<author>A Voutilainen</author>
<author>J Heikkild</author>
<author>A Anttila</author>
</authors>
<title>Constraint grammar: a language-independent system for parsing unrestricted text.</title>
<date>1995</date>
<location>Berlin, Germany: Mouton</location>
<note>de Gruyter.</note>
<marker>Karlsson, Voutilainen, Heikkild, Anttila, 1995</marker>
<rawString>Karlsson, F., Voutilainen, A., Heikkild, J., &amp; Anttila, A. (1995). Constraint grammar: a language-independent system for parsing unrestricted text. Berlin, Germany: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lehmann</author>
<author>S Oepen</author>
<author>S Regnier-Prost</author>
<author>K Netter</author>
<author>V Lux</author>
<author>J Klein</author>
<author>K Falkedal</author>
<author>F Fouvry</author>
<author>D Estival</author>
<author>E Dauphin</author>
<author>H Cornpagnion</author>
<author>J Baur</author>
<author>L Balkan</author>
<author>D Arnold</author>
</authors>
<title>TSNLP Test Suites for Natural Language Processing.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (pp. 711 — 716). Kopenhagen,</booktitle>
<contexts>
<context position="8567" citStr="Lehmann et al., 1996" startWordPosition="1333" endWordPosition="1336">a given input length (Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contrastive comparison of parser efficiency. Although most grammar development environments and large-scale parsing systems supply facilities to batch-process a test corpus and record the results produced by the system, these are typically restricted to processing a flat, unstructured input file (listing test sentences, one </context>
</contexts>
<marker>Lehmann, Oepen, Regnier-Prost, Netter, Lux, Klein, Falkedal, Fouvry, Estival, Dauphin, Cornpagnion, Baur, Balkan, Arnold, 1996</marker>
<rawString>Lehmann, S., Oepen, S., Regnier-Prost, S., Netter, K., Lux, V., Klein, J., Falkedal, K., Fouvry, F., Estival, D., Dauphin, E., Cornpagnion, H., Baur, J., Balkan, L., &amp; Arnold, D. (1996). TSNLP Test Suites for Natural Language Processing. In Proceedings of the 16th International Conference on Computational Linguistics (pp. 711 — 716). Kopenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence</booktitle>
<pages>1420--1425</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="8545" citStr="Lin, 1995" startWordPosition="1331" endWordPosition="1332">parses for a given input length (Carroll, Briscoe, &amp; Sanfilippo, 1998). Work on quantifying parse correctness has used various measures of structural consistency with respect to constituent structure annotations of a corpus (e.g. exact match, crossing brackets, tree similarity, and others see Black et al., 1991, Black, Garside, &amp; Leech, 1993, Grisham, Macleod, &amp; Sterling, 1992, and Briscoe &amp; Carroll, 1993); recently, more general schemes have been advocated that deploy functor —argument (dependency) relations as an abstraction over different phrase structure analyses that a parser may assign (Lin, 1995; Lehmann et al., 1996; Carroll et al., 1998). The Penn Treebank and the SUSANNE corpus are well-established resources for the evaluation of parser accuracy. In a sharp contrast, there is little existing methodology, let alone established reference data or software tools, for the evaluation and contrastive comparison of parser efficiency. Although most grammar development environments and large-scale parsing systems supply facilities to batch-process a test corpus and record the results produced by the system, these are typically restricted to processing a flat, unstructured input file (listin</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Lin, D. (1995). A dependency-based method for evaluating broad-coverage parsers. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (pp. 1420 — 1425). Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Maxwell</author>
<author>R M Kaplan</author>
</authors>
<title>The interface between phrasal and functional constraints.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>571--590</pages>
<contexts>
<context position="1476" citStr="Maxwell and Kaplan, 1993" startWordPosition="209" endWordPosition="212">al resources, and by efforts in large-scale and collaborative grammar engineering and also in the induction of statistical grammars/parsers from treebanks. There are two main paradigms in the evaluation and comparison of the performance of parsing algorithms and implemented systems: (i) the formal, complexity-theoretic analysis of how an algorithm behaves, typically focussing on worst-case time and space complexity bounds; and (ii) the empirical study of how properties of the parser and input (possibly including the grammar used) affect actual, observed run-time efficiency. It has been shown (Maxwell and Kaplan, 1993; Carroll, 1994; van Noord, 1997) that the theoretical study of algorithms alone does not (yet) suffice to provide an accurate prediction about how a specific algorithm will perform in practice, when used in conjunction with a specific grammar (or type of grammar), and when applied to a particular domain and task. Therefore, empirical assessment of practical parser performance has become an established technique and continues to be the primary means of comparison among algorithms. At the same time, system competence (i.e. coverage and overgeneration with respect to a particular grammar and tes</context>
<context position="13711" citStr="Maxwell and Kaplan (1993)" startWordPosition="2106" endWordPosition="2109">mmar transformations. The refinement process was guided throughout by empirical measurements of parser throughput on a test corpus. Improvements in efficiency can be gained by specialising a general-purpose grammar to a particular corpus. Samuelsson and Rayner (1991) describe a machine learning technique that is applied to the CLE grammar to produce a version of the grammar that parses ATIS corpus sentences much faster than the original grammar. In general there are more rules in the specialised grammar than in the original, but they are more specific and can thus be applied more efficiently. Maxwell and Kaplan (1993) investigate the interaction between parsing with the CF backbone component of a grammar and the resolution of functional constraints, using a precursor of the English ParGram grammar. A number of parsing strategies are evaluated, in combination with two different unifiers, on a small set of test sentences. There is a wide gap between the best and worst performing technique; the differences can be justified intuitively, but not with any formal analyses of computational complexity. Carroll (1994) discusses the throughput of three quite distinct unification-based parsing algorithms running with </context>
</contexts>
<marker>Maxwell, Kaplan, 1993</marker>
<rawString>Maxwell III, J. T., &amp; Kaplan, R. M. (1993). The interface between phrasal and functional constraints. Computational Linguistics, 19 (4), 571-590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>Improved left-corner chart parsing for large context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Workshop on Parsing Technologies</booktitle>
<pages>171--182</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="5341" citStr="Moore, 2000" startWordPosition="814" endWordPosition="815">e). • In the Xerox-led ParGram collaboration (Butt, King, Niiio, &amp; Segond, 1999), widecoverage grammars of English, French, German and a number of other languages are being developed in parallel in the LFG framework, all of the grammars based on a common set of linguistic principles, with a commonly-agreed-upon set of grammatical features. Each grammar consists of an atomic-categoried phrase-structure backbone augmented with feature annotations. • The trees in the Penn Treebank induce a large context-free grammar containing 15,000 rules. A recent comparison of context-free parsing strategies (Moore, 2000) has used this grammar, a second one derived from an ATIS treebank (with 4,600 productions), and a third (24,500 productions) produced by computing an atomic-categoried backbone from a unification-based phase structure grammar. Test sentences for these grammars were derived either from the associated corpora, or artificially, by using the grammar to stochastically generate random strings. • The XTAG system grammar (XTAG, 1995) is a large-scale lexicalised tree adjoining grammar of English, developed by several researchers over the past ten years or so. The grammar contains of the order of 500 </context>
<context position="12330" citStr="Moore, 2000" startWordPosition="1900" endWordPosition="1901"> a medium-sized CF grammar with attribute-value augmentations. However, comparisons of this type have to be done with care. The coding of different strategies must use exactly equivalent techniques, and to be able to make any general claims, the grammar(s) used must be large enough to fully stress the algorithms In particular, with grammars admitting less ambiguity, parse time is likely to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. In fact, a more recent evaluation (Moore, 2000) using a number of large-scale CF grammars has shown conclusively that generalised LR parsing is less efficient than certain left-corner parsing strate2See &apos;Ilttp://www.coli.uni-sb.de/itsdbr for the (draft) [incr tsdb()] user manual, pronunciation guidelines, and instructions on obtaining and installing the package. 9 gies. Moore and Dowding (1991) document a process of refining a unification-based (purely bottom-up) CKY parser (forming part of a speech understanding system) by incorporating top-down information to prevent it hypothesising constituents bottom-up that could not form part of a c</context>
<context position="15526" citStr="Moore (2000)" startWordPosition="2392" endWordPosition="2393"> another. van Noord (1997) presents an efficient implementation of head-corner parsing, as used in a prototype spoken language dialogue system. Memoisation and goal-weakening techniques are used to reduce parser space requirements; the head-corner parser also runs faster than implementations of left-corner, bottom-up and LR parsers in evaluations using a DCG of Dutch with speech recogniser word-graph input. A further set of evaluations use the ANLT grammar, allowing a tentative cross-system comparison with the ANLT parser to be made. In work concerned with parsing with largescale CF grammars, Moore (2000) investigates empirically the interactions between various types of grammar factoring and versions of the left-corner parsing algorithm that differ in the details of precisely how and in what order top-down filtering information is applied. Using three very different grammars, one of the parser/factoring combinations was found to be consistently and significantly better than the alternatives, despite being only minimally different from the other variants. This strategy was also shown to outperform several other major approaches to CF parsing. Sarkar (2000) evaluates the efficiency of a chart-b</context>
</contexts>
<marker>Moore, 2000</marker>
<rawString>Moore, R. (2000). Improved left-corner chart parsing for large context-free grammars. In Proceedings of the 6th International Workshop on Parsing Technologies (pp. 171-182). Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
</authors>
<title>Efficient bottom-up parsing.</title>
<date>1991</date>
<booktitle>In DARPA Speech and Natural Language Workshop</booktitle>
<pages>200--203</pages>
<location>Asilomar, CA.</location>
<contexts>
<context position="12680" citStr="Moore and Dowding (1991)" startWordPosition="1946" endWordPosition="1949">mars admitting less ambiguity, parse time is likely to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. In fact, a more recent evaluation (Moore, 2000) using a number of large-scale CF grammars has shown conclusively that generalised LR parsing is less efficient than certain left-corner parsing strate2See &apos;Ilttp://www.coli.uni-sb.de/itsdbr for the (draft) [incr tsdb()] user manual, pronunciation guidelines, and instructions on obtaining and installing the package. 9 gies. Moore and Dowding (1991) document a process of refining a unification-based (purely bottom-up) CKY parser (forming part of a speech understanding system) by incorporating top-down information to prevent it hypothesising constituents bottom-up that could not form part of a complete analysis, given the portions of rules already partially instantiated. An important step was reducing the spurious prediction of gaps by means of grammar transformations. The refinement process was guided throughout by empirical measurements of parser throughput on a test corpus. Improvements in efficiency can be gained by specialising a gen</context>
</contexts>
<marker>Moore, Dowding, 1991</marker>
<rawString>Moore, R., &amp; Dowding, J. (1991). Efficient bottom-up parsing. In DARPA Speech and Natural Language Workshop (pp. 200-203). Asilomar, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>425--456</pages>
<marker>van Noord, 1997</marker>
<rawString>van Noord, G. (1997). An efficient implementation of the head-corner parser. Computational Linguistics, 23 (3), 425-456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
<author>8z Bouma</author>
<author>G</author>
</authors>
<title>Hdrug. A flexible and extendible development environment for natural language processing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Computational Environments for Grammar Development and Linguistic Engineering (pp. 91 — 98).</booktitle>
<location>Madrid,</location>
<marker>van Noord, Bouma, G, 1997</marker>
<rawString>van Noord, G., 8z Bouma, G. (1997). Hdrug. A flexible and extendible development environment for natural language processing. In Proceedings of the Workshop on Computational Environments for Grammar Development and Linguistic Engineering (pp. 91 — 98). Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>dgz Carroll</author>
<author>J</author>
</authors>
<title>Performance profiling for parser engineering.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<booktitle>(Special Issue on Efficient Processing with HPSG), 81 — 97.</booktitle>
<volume>6</volume>
<issue>1</issue>
<marker>Oepen, Carroll, J, 2000</marker>
<rawString>Oepen, S., dgz Carroll, J. (2000). Performance profiling for parser engineering. Natural Language Engineering, 6 (1) (Special Issue on Efficient Processing with HPSG), 81 — 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>D P Flickinger</author>
</authors>
<title>Towards systematic grammar profiling Test suite technology ten years after.</title>
<date>1998</date>
<journal>Journal of Computer Speech and Language,</journal>
<volume>12</volume>
<issue>4</issue>
<pages>436</pages>
<contexts>
<context position="9632" citStr="Oepen &amp; Flickinger, 1998" startWordPosition="1496" endWordPosition="1499">orpus and record the results produced by the system, these are typically restricted to processing a flat, unstructured input file (listing test sentences, one per line) and outputting a small number of processing results to a log file.2 Additionally, no metrics exist that allow the comparison of parser efficiency across different grammars and sets of reference data. We therefore note a striking methodological and technological deficit in the area of precise and systematic assessment of grammar and parser behaviour. Recently though, a new methodology, termed competence H performance profiling (Oepen &amp; Flickinger, 1998; Oepen &amp; Carroll, 2000), has been proposed that aims to fill this gap. Profiles are rich, precise, and structured snapshots 2Some (Meta-)Systems like PLEUK (Calder, 1993) and HDrug (van Noord &amp; Bouma, 1997) that facilitate the exploration of multiple descriptive formalisms and processing strategies come with slightly more sophisticated benchmarking facilities and visualisation tools. However, they still largely operate on monolithic, unannotated input data sets, restrict accounting of system results to a small number of parameters (e.g. number of analyses, overall processing time, memory cons</context>
</contexts>
<marker>Oepen, Flickinger, 1998</marker>
<rawString>Oepen, S., &amp; Flickinger, D. P. (1998). Towards systematic grammar profiling Test suite technology ten years after. Journal of Computer Speech and Language, 12 (4) (Special Issue on Evaluation), 411 — 436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>K Netter</author>
<author>J Klein</author>
</authors>
<title>TSNLP - Test Suites for Natural Language Processing. In</title>
<date>1997</date>
<booktitle>Linguistic Databases</booktitle>
<pages>13--36</pages>
<editor>J. Nerbonne (Ed.),</editor>
<publisher>CSLI Publications.</publisher>
<location>Stanford, CA:</location>
<contexts>
<context position="6874" citStr="Oepen, Netter, &amp; Klein, 1997" startWordPosition="1063" endWordPosition="1067">tten by the grammar developers themselves for the purpose of monitoring over- and under-generation as the grammar is changed. However, the test suites have also been found to be of some value for evaluating parser efficiency. A major drawback in this context, though, is that each test suite item usually only contains very limited ambiguity (easing the task of checking the resulting parses), and is relatively short (so that only one or two constructions are tested at a time). This is also the case for independently-developed test suites, such as the TSNLP suites for English, French and German (Oepen, Netter, &amp; Klein, 1997). Therefore, in some parser evaluation work, new suites of longer sentences have had to be constructed manually or extracted specially from corpora. Another important issue is the degree to which the grammars are available to the general NL processing research community. Those developed within companies are in general more difficult to obtain, although use for parser evaluation may be easier to negotiate than use within an actual application system, for instance. 3 Methods and Tools Previous work on the assessment and comparison of large-scale parsers has mostly been concerned with evaluation</context>
</contexts>
<marker>Oepen, Netter, Klein, 1997</marker>
<rawString>Oepen, S., Netter, K., &amp; Klein, J. (1997). TSNLP - Test Suites for Natural Language Processing. In J. Nerbonne (Ed.), Linguistic Databases (pp. 13-36). Stanford, CA: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
<author>M Rayner</author>
</authors>
<title>Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system.</title>
<date>1991</date>
<booktitle>In Proceedings of the 12th International Joint Conference on Artificial Intelligence (pp. 609 — 615).</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="13353" citStr="Samuelsson and Rayner (1991)" startWordPosition="2045" endWordPosition="2048">-based (purely bottom-up) CKY parser (forming part of a speech understanding system) by incorporating top-down information to prevent it hypothesising constituents bottom-up that could not form part of a complete analysis, given the portions of rules already partially instantiated. An important step was reducing the spurious prediction of gaps by means of grammar transformations. The refinement process was guided throughout by empirical measurements of parser throughput on a test corpus. Improvements in efficiency can be gained by specialising a general-purpose grammar to a particular corpus. Samuelsson and Rayner (1991) describe a machine learning technique that is applied to the CLE grammar to produce a version of the grammar that parses ATIS corpus sentences much faster than the original grammar. In general there are more rules in the specialised grammar than in the original, but they are more specific and can thus be applied more efficiently. Maxwell and Kaplan (1993) investigate the interaction between parsing with the CF backbone component of a grammar and the resolution of functional constraints, using a precursor of the English ParGram grammar. A number of parsing strategies are evaluated, in combinat</context>
</contexts>
<marker>Samuelsson, Rayner, 1991</marker>
<rawString>Samuelsson, C., &amp; Rayner, M. (1991). Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system. In Proceedings of the 12th International Joint Conference on Artificial Intelligence (pp. 609 — 615). Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
</authors>
<title>Practical experiments in parsing using tree adjoining grammars.</title>
<date>2000</date>
<booktitle>In 5th international workshop on tree adjoining grammars and related formalisms (pp. 193 — 198).</booktitle>
<location>Paris</location>
<contexts>
<context position="16088" citStr="Sarkar (2000)" startWordPosition="2475" endWordPosition="2476">h parsing with largescale CF grammars, Moore (2000) investigates empirically the interactions between various types of grammar factoring and versions of the left-corner parsing algorithm that differ in the details of precisely how and in what order top-down filtering information is applied. Using three very different grammars, one of the parser/factoring combinations was found to be consistently and significantly better than the alternatives, despite being only minimally different from the other variants. This strategy was also shown to outperform several other major approaches to CF parsing. Sarkar (2000) evaluates the efficiency of a chart-based head-corner parsing algorithm on a corpus of 2,250 Wall Street Journal sentences, using a large-scale grammar (containing 6,800 elementary tree schemata) extracted automatically from the Penn Treebank. For each sentence, parse times were found to correlate roughly exponentially with the number of lexicalised elementary trees selected; there was little correlation between sentence length and parse time. Oepen and Carroll (2000) describe and argue for a strategy of performance profiling in the engineering of parsing systems for wide-coverage linguistic </context>
</contexts>
<marker>Sarkar, 2000</marker>
<rawString>Sarkar, A. (2000). Practical experiments in parsing using tree adjoining grammars. In 5th international workshop on tree adjoining grammars and related formalisms (pp. 193 — 198). Paris France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sleator</author>
<author>D Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd International Workshop on Parsing Technologies</booktitle>
<pages>277--292</pages>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="3486" citStr="Sleator &amp; Temperley, 1993" startWordPosition="522" endWordPosition="525">. We describe their main characteristics briefly below.&apos; • The Alvey NL Tools (ANLT) contains a large, wide-coverage sentence grammar of English (Grover, Carroll, &amp; Briscoe, 1993), written in a unification-based metagrammatical formalism resembling GPSG. The grammar expands out to an object grammar of 780 DCG&apos;While it is the case that most current large-scale grammar-based parsing systems construct constituent structure representations that are capable of supporting semantic interpretation, the English Constraint Grammar (Karlsson, Voutilainen, Heikkila, &amp; Anttila, 1995) and the Link Grammar (Sleator &amp; Temperley, 1993) systems are exceptions. Thus, since the motivations behind these grammars are different we do not consider them here. 7 like rules, each category containing on average around 30 nodes. Associated with the grammar is a test suite, originally written by the grammarian to monitor coverage during grammar development, containing around 1,400 (mostly grammatical) items. • The SRI Core Language Engine (CLE) grammar (Alshawi, 1992) is also GPsG-inspired, but with different treatments of a number of central syntactic phenomena, such as subcategorisation and unbounded dependencies. The grammar contains</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Sleator, D., &amp; Temperley, D. (1993). Parsing English with a link grammar. In Proceedings of the 3rd International Workshop on Parsing Technologies (pp. 277-292). Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An efficient augmentedcontext-free parsing algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<pages>31--46</pages>
<contexts>
<context position="11634" citStr="Tomita, 1987" startWordPosition="1790" endWordPosition="1791">Tsujii, &amp; Uszkoreit, 2000), resulting in useful feedback to all participating groups. 4 Efficiency Comparisons Many parsing algorithms suitable for NL grammars have been proposed over the years, their proponents often arguing that the number of computational steps are minimised with respect to alternative, competing algorithms. However, such arguments can only be made in the case of very closely related algorithms; qualitatively different computations can only reliably be compared empirically. So, for example, generalised LR parsing was put forward as an improvement over Earley-style parsing (Tomita, 1987), with a justification made by running implementations of the two types of parser on a medium-sized CF grammar with attribute-value augmentations. However, comparisons of this type have to be done with care. The coding of different strategies must use exactly equivalent techniques, and to be able to make any general claims, the grammar(s) used must be large enough to fully stress the algorithms In particular, with grammars admitting less ambiguity, parse time is likely to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tight</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Tomita, M. (1987). An efficient augmentedcontext-free parsing algorithm. Computational Linguistics, 13 (I), 31-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>XTAG Group</author>
</authors>
<title>A lexicalized tree adjoining grammar for english (Tech.</title>
<date>1995</date>
<tech>Rep. No. IRCS Report 95-03).</tech>
<institution>The Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<marker>Group, 1995</marker>
<rawString>XTAG Group (1995). A lexicalized tree adjoining grammar for english (Tech. Rep. No. IRCS Report 95-03). The Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>