<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011140">
<title confidence="0.994071">
Turku: Broad-Coverage Semantic Parsing with Rich Features
</title>
<author confidence="0.991712">
Jenna Kanerva*
</author>
<affiliation confidence="0.89535575">
Department of Information
Technology
University of Turku
Finland
</affiliation>
<email confidence="0.978571">
jmnybl@utu.fi
</email>
<author confidence="0.930293">
Juhani Luotolahti*
</author>
<affiliation confidence="0.891919">
Department of Information
Technology
University of Turku
Finland
</affiliation>
<email confidence="0.993932">
mjluot@utu.fi
</email>
<author confidence="0.995276">
Filip Ginter
</author>
<affiliation confidence="0.8946125">
Department of Information
Technology
University of Turku
Finland
</affiliation>
<email confidence="0.99317">
figint@utu.fi
</email>
<sectionHeader confidence="0.993787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998847909090909">
In this paper we introduce our system ca-
pable of producing semantic parses of sen-
tences using three different annotation for-
mats. The system was used to partic-
ipate in the SemEval-2014 Shared Task
on broad-coverage semantic dependency
parsing and it was ranked third with an
overall Fl-score of 80.49%. The sys-
tem has a pipeline architecture, consisting
of three separate supervised classification
steps.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949">
In the SemEval-2014 Task 8 on semantic parsing,
the objective is to extract for each sentence a rich
set of typed semantic dependencies in three differ-
ent formats: DM, PAS and PCEDT. These formats
differ substantially both in the assignment of se-
mantic heads as well as in the lexicon of seman-
tic dependency types. In the open track of the
shared task, participants were encouraged to use
all resources and tools also beyond the provided
training data. To improve the comparability of the
systems, the organizers provided ready-to-use de-
pendency parses produced using the state-of-the-
art parser of Bohnet and Nivre (2012).
In this paper we describe our entry in the open
track of the shared task. Our system is a pipeline
of three support vector machine classifiers trained
separately for detecting semantic dependencies,
assigning their roles, and selecting the top nodes
of semantic graphs. In this, we loosely follow
the architecture of e.g. the TEES (Bj¨orne et al.,
2012) and EventMine (Miwa et al., 2012) systems,
which were found to be effective in the structurally
</bodyText>
<note confidence="0.736233">
∗These authors contributed equally.
</note>
<footnote confidence="0.948394">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999131545454546">
related task of biomedical event extraction. Sim-
ilar classification approach is shown to be effec-
tive also in semantic parsing by e.g. Zhao et al.
(2009), the winner of the CoNLL’09 Shared Task
on Syntactic and Semantic Dependencies in Mul-
tiple Languages (SRL-only subtask) (Hajiˇc et al.,
2009), where semantic parsing is approached as a
word-pair classification problem and semantic ar-
guments and their roles are predicted simultane-
ously. In preliminary experiments, we also de-
veloped a joint approach to simultaneously iden-
tify semantic dependencies and assign their roles,
but found that the performance of the joint predic-
tion was substantially worse than for the current
pipeline approach. As the source of features, we
rely heavily on the syntactic parses as well as other
external resources such as vector space represen-
tations of words and large-scale syntactic n-gram
statistics.
In the following sections, we describe the three
individual classification steps of our semantic
parsing pipeline.
</bodyText>
<sectionHeader confidence="0.91007" genericHeader="method">
2 Detecting Semantic Dependencies
</sectionHeader>
<bodyText confidence="0.99996">
The first step of our semantic parsing pipeline
is to detect semantic dependencies, i.e. governor-
dependent pairs which has a semantic relation be-
tween them. The first stage covers only the identi-
fication of such dependencies; the labels describ-
ing the semantic roles of the dependents are as-
signed in a later stage.
The semantic dependencies are identified using
a binary support vector machine classifier from the
LIBSVM package (Chang and Lin, 2011). Each
possible combination of two tokens in the sen-
tence is considered to be a candidate for a seman-
tic dependency in both directions, and thus also
included as a training example. No beforehand
pruning of possible candidates is performed dur-
ing training. However, we correct for the over-
whelming number of negative training examples
</bodyText>
<page confidence="0.969392">
678
</page>
<note confidence="0.7319015">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 678–682,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.99997288372093">
by setting the weights of positive and negative ex-
amples used during training, so as to maximize the
unlabeled F1-score on the development set.
Increasing the recall of semantic dependency
detection can be beneficial for the overall perfor-
mance of the pipeline system, since a candidate
lost in the dependency detection stage cannot be
recovered later. We therefore tested the approach
applied, among others by Bj¨orne et al. (2012),
whereby the dependency detection stage heavily
overgenerates candidates and the next stage in the
pipeline is given the option to predict a nega-
tive label, thus removing a candidate dependency.
In preliminary experiments we tried to explicitly
overgenerate the dependency candidates by alter-
ing the classifier threshold, but noticed that heavy
overgeneration of positive examples leads to a de-
creased performance in the role assigning stage.
Instead, the above-mentioned optimization of the
example weights during training results in a clas-
sifier which overgenerates positive examples by
4.4%, achieving the same objective and improving
the overall performance of the system.
Features used during the dependency identifi-
cation are derived from tokens and the syntactic
parse trees provided by the organizers. Our pri-
mary source of features are the syntactic trees,
since 73.2% of semantic dependencies have a cor-
responding undirected syntactic dependency in the
parse tree. Further, the syntactic dependency path
between the governor and the dependent is shorter
than their linear distance in 48.8% of cases (in
43.4% of cases the distance is the same). The final
feature set used in the identification is optimized
by training models with different combinations of
features and selecting the best combination based
on performance on the held-out development set.
Interestingly, the highest performance is achieved
with a rather small set of features, whose full list-
ing is shown in Table 1. The feature vectors are
normalized to unit length prior to classification
and the SVM regularization parameter c is opti-
mized separately for each annotation format.
</bodyText>
<sectionHeader confidence="0.995839" genericHeader="method">
3 Role Assignment
</sectionHeader>
<bodyText confidence="0.999961166666667">
After the semantic governor-dependent pairs are
identified, the next step is to assign a role for
each pair to constitute a full semantic dependency.
This is done by training a multiclass support vec-
tor machine classifier implemented in the SVM-
multiclass package by Joachims (1999). We it-
</bodyText>
<equation confidence="0.980154666666667">
Feature D R T
arg.pos X X
arg.deptype X X
arg.lemma X X
pred.pos X X X
pred.deptype X X X
pred.lemma X X X
pred.is predicate X X
arg.issyntaxdep X
arg.issyntaxgov X
arg.issyntaxsibling X
path.length X X
</equation>
<table confidence="0.711390903225806">
undirected path.deptype X X
directed path.deptype X X
undirected path.pos X X
extended path.deptype X X
simplified path.deptype with len X X
simplified path.deptype wo len X
splitted undirected path.deptype
arg.prev.pos X X
arg.next.pos X X
arg.prev+arg.pos X X
arg.next+arg.pos X X
arg.next+arg+arg.prev.pos X X
pred.prev.pos X X
pred.next.pos X X
pred.prev+pred.pos X X
pred.next+pred.pos X X
pred.next+pred+pred.prev.pos X X
linear route.pos X X
arg.child.pos X
arg.child.deptype X
arg.child.lemma X
pred.child.pos
pred.child.deptype X X
pred.child.lemma X X
syntaxgov.child.deptype X X
vector similarities X X
n-gram frequencies X X
pred.sem role
pred.child.sem role
pred.syntaxsibling.deptype
pred.semanticsibling.sem role
</table>
<tableCaption confidence="0.998198">
Table 1: Features used in the detection of semantic
</tableCaption>
<bodyText confidence="0.967564285714286">
dependencies (D), assigning their roles (R) and top
node detection (T). path refers to syntactic depen-
dencies between the argument and the predicate,
and linear route refers to all tokens between the
argument and the predicate. In top node detection,
where only one token is considered at a time, the
pred is used to represent that token.
</bodyText>
<page confidence="0.996867">
679
</page>
<bodyText confidence="0.99968212">
erate through all identified dependencies, and for
each assign a role, or alternatively classify it as a
negative example. This is to account for the 4.4%
of overgenerated dependencies. However, the pro-
portion of negative classifications should stay rel-
atively low and to ensure this, we downsample the
number of negative examples used in training to
contain only 5% of all negative examples. The
downsampling ratio is optimized on the develop-
ment set using grid search and downsampled train-
ing instances are chosen randomly.
The basic features, shown in Table 1, follow the
same style as in dependency identification. We
also combine some of the basic features by creat-
ing all possible feature pairs in a given set, but do
not perform this with the full set of features. In the
open track, participants are also allowed to use ad-
ditional data and tools beyond the official training
data. In addition to the parse trees, we include also
features utilizing syntactic n-gram frequencies and
vector space similarities.
Google has recently released a large corpus
of syntactic n-grams, a collection of depen-
dency subtrees with frequency counts (Goldberg
and Orwant, 2013). The syntactic n-grams are
induced from the Google Books collection, a
350B token corpus of syntactically parsed text.
In this work we are interested in arcs, which
are (governor, dependent, syntactic relation)
triplets associated with their count.
For each governor-dependent pair, we generate
a set of n-gram features by iterating through all
known dependency types and searching from the
syntactic n-grams how many times (if any) the
governor-dependent pair with the particular de-
pendency type is seen. A separate feature is then
created for each dependency type and the counts
are encoded in feature weights compressed using
w = logio(count). This approach gives us an op-
portunity to include statistical information about
word relations induced from a very large corpus.
Information is captured also outside the particular
syntactic context, as we iterate through all known
dependency types during the process.
Another source of additional data used in role
classification is a publicly available Google News
vector space model1 representing word similari-
ties. The vector space model is induced from the
Google News corpus with the word2vec software
(Mikolov et al., 2013) and negative sampling ar-
</bodyText>
<footnote confidence="0.925937">
1https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.999974529411765">
chitecture, and each vector have 300 dimensions.
The vector space representation gives us an oppor-
tunity to measure word similarities using the stan-
dard cosine similarity function.
The approach to transforming the vector repre-
sentations into features varies with the three dif-
ferent annotation formats. On DM and PAS, we
follow the method of Kanerva and Ginter (2014),
where for each role an average argument vector
is calculated. This is done by averaging all word
vectors seen in the training data as arguments for
the given predicate with a particular role. For each
candidate argument, we can then establish a set of
similarity values to each possible role by taking
the cosine similarity of the argument vector to the
role-wise average vectors. These similarities are
then turned into separate features, where the simi-
larity values are encoded as feature weights.
On PCEDT, preliminary experiments showed
that the best strategy to include word vectors into
classification is by turning them directly into fea-
tures, so that each dimension of the word vector
is represented as a separate feature. Thus, we it-
erate through all 300 vector dimensions and cre-
ate a separate feature representing the position and
value of a particular dimension. Values are again
encoded in feature weights. These features are cre-
ated separately for both the argument and the pred-
icate. The word vectors are pre-normalized to unit
length, so no additional normalization of feature
weights is needed.
Both the n-gram– and vector similarities–based
features give a modest improvement to the classi-
fication performance.
</bodyText>
<sectionHeader confidence="0.978557" genericHeader="method">
4 Detecting Top Nodes
</sectionHeader>
<bodyText confidence="0.999966533333334">
The last step in the pipeline is the detection of
top nodes. A top node is the semantic head or
the structural root of the sentence. Typically each
sentence annotated in the DM and PAS formats
contains one top node, whereas PCEDT sentences
have on average 1.12 top nodes per sentence.
As in the two previous stages, we predict top
nodes by training a support vector machine clas-
sifier, with each token being considered a candi-
date. Because the top node prediction is the last
step performed, in addition to the basic informa-
tion available in the two previous steps, we are
able to use also predicted arguments as features.
Otherwise, the feature set used in top node detec-
tion follows the same style as in the two previous
</bodyText>
<page confidence="0.993619">
680
</page>
<table confidence="0.9987922">
LP LR LF UF
DM 80.94 82.14 81.53 83.48
PAS 87.33 87.76 87.54 88.97
PCEDT 72.42 72.37 72.40 85.86
Overall 80.23 80.76 80.49 86.10
</table>
<tableCaption confidence="0.979757">
Table 2: Overall scores of whole task as well as
</tableCaption>
<bodyText confidence="0.987300071428571">
separately for each annotation format in terms of
labeled precision (LP), recall (LR) and F1-score
(LF) as well as unlabeled F1-score (UF).
tasks, but is substantially smaller (see Table 1). We
also create all possible feature pairs prior to clas-
sification to simulate the use of a second-degree
polynomial kernel.
For each token in the sentence, we predict
whether it is a top node or not. However, in DM
and PAS, where typically only one top node is al-
lowed, we choose only the token with the maxi-
mum positive value to be the final top node. In
PCEDT, we simply let all positive predictions act
as top nodes.
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999992173913044">
The primary evaluation measure is the labeled F1-
score of the predicted dependencies, where the
identification of top nodes is incorporated as an
additional dummy dependency. The overall se-
mantic F1-score of our system is 80.49%. The
prediction performance in DM is 81.53%, in PAS
87.54% and in PCEDT 72.40%. The top nodes
are identified with an overall F1-score of 87.05%.
The unlabeled F1-score reflects the performance
of the dependency detection in isolation from la-
beling task and by comparing the labeled and un-
labeled F1-scores from Table 2 we can see that the
most common mistake relates to the identification
of correct governor-dependent pairs. This is espe-
cially true with the DM and PAS formats where the
difference between labeled and unlabeled scores
is very small (1.9pp and 1.4pp), reflecting high
performance in assigning the roles. Instead, in
PCEDT the role assignment accuracy is substan-
tially below the other two and the difference be-
tween unlabeled and labeled F1-score is as much
as 13.5pp. One likely reason is the higher number
of possible roles defined in the PCEDT format.
</bodyText>
<subsectionHeader confidence="0.923806">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999994642857143">
Naturally, our system generally performs better
with frequently seen semantic roles than roles that
are seen rarely. In the case of DM, the 4 most
common semantic roles cover over 87% of the
gold standard dependencies and are predicted with
a macro F1-score of 85.3%, while the remaining
35 dependency labels found in the gold standard
are predicted at an average rate of 49.4%. To
give this a perspective, the most common 4 roles
have on average 121K training instances, while the
remaining 35 roles have on average about 2000
training instances. For PAS, the 9 most common
labels, which comprise over 80% of all depen-
dencies in the gold standard data and have on av-
erage about 66K training instances per role, are
predicted with an F1-score of 87.6%, while the
remaining 32 labels have on average 4200 train-
ing instance and are predicted with an F1-score of
57.8%. The PCEDT format has the highest num-
ber of possible semantic roles and also lowest cor-
relation between the frequency in training data and
F1-score. For PCEDT, the 11 most common la-
bels, which cover over 80% of all dependencies in
the gold standard, are predicted with an F1-score
of 69.6%, while the remaining 53 roles are pre-
dicted at an average rate of 46.6%. The higher
number of roles also naturally affects the number
of training instances and the 11 most common la-
bels in PCEDT have on average 35K training in-
stances, while the remaining 53 roles have on av-
erage 1600 instances per role.
Similarly, the system performs better with se-
mantic arguments which are nearby the governor.
This is true for both linear distance between the
two tokens and especially for distance measured
by syntactic dependency steps. For example in the
case of DM, semantic dependencies shorter than
3 steps in the syntactic tree cover more than 95%
of the semantic dependencies in the gold standard
and have an F1-score of 75.1%, while the rest have
only 32.6%. The same general pattern is also evi-
dent in the other formats.
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998851222222222">
In this paper we presented our system used to
participate in the SemEval-2014 Shared Task on
broad-coverage semantic dependency parsing. We
built a pipeline of three supervised classifiers to
identify semantic dependencies, assign a role for
each dependency and finally, detect the top nodes.
In addition to basic features used in classifica-
tion we have shown that additional information,
such as frequencies of syntactic n-grams and word
</bodyText>
<page confidence="0.996095">
681
</page>
<bodyText confidence="0.999709666666667">
similarities derived from vector space representa-
tions, can also positively contribute to the classifi-
cation performance.
The overall Fl-score of our system is 80.49%
and it was ranked third in the open track of the
shared task.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99975275">
This work was supported by the Emil Aaltonen
Foundation and the Kone Foundation. Computa-
tional resources were provided by CSC – IT Cen-
ter for Science.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831185185185">
Jari Bj¨orne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP’11 shared task.
BMCBioinformatics, 13(Suppl 11):S4.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1455–1465.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
Syntactic-Ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity, pages
241–247.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, et al. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–18.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Meth-
ods - Support Vector Learning, pages 169–184. MIT
Press.
Jenna Kanerva and Filip Ginter. 2014. Post-hoc ma-
nipulations of vector space models with application
to semantic role labeling. In Proceedings of the 2nd
Workshop on Continuous Vector Space Models and
their Compositionality (CVSC)@ EACL, pages 1–
10.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Workshop Proceedings of
International Conference on Learning Representa-
tions.
Makoto Miwa, Paul Thompson, John McNaught, Dou-
glas Kell, and Sophia Ananiadou. 2012. Extracting
semantically enriched events from biomedical liter-
ature. BMCBioinformatics, 13(1):108.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning:
a huge feature engineering method to semantic de-
pendency parsing. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 55–60.
</reference>
<page confidence="0.997962">
682
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.410720">
<title confidence="0.625395">Turku: Broad-Coverage Semantic Parsing with Rich Features</title>
<affiliation confidence="0.9809825">Department of University of</affiliation>
<email confidence="0.961691">jmnybl@utu.fi</email>
<affiliation confidence="0.9988715">Department of University of</affiliation>
<email confidence="0.827253">mjluot@utu.fi</email>
<author confidence="0.96081">Filip</author>
<affiliation confidence="0.9988765">Department of University of</affiliation>
<email confidence="0.948873">figint@utu.fi</email>
<abstract confidence="0.995487166666667">In this paper we introduce our system capable of producing semantic parses of sentences using three different annotation formats. The system was used to participate in the SemEval-2014 Shared Task on broad-coverage semantic dependency parsing and it was ranked third with an of 80.49%. The system has a pipeline architecture, consisting of three separate supervised classification steps.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jari Bj¨orne</author>
<author>Filip Ginter</author>
<author>Tapio Salakoski</author>
</authors>
<date>2012</date>
<booktitle>University of Turku in the BioNLP’11 shared task. BMCBioinformatics, 13(Suppl 11):S4.</booktitle>
<marker>Bj¨orne, Ginter, Salakoski, 2012</marker>
<rawString>Jari Bj¨orne, Filip Ginter, and Tapio Salakoski. 2012. University of Turku in the BioNLP’11 shared task. BMCBioinformatics, 13(Suppl 11):S4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1455--1465</pages>
<contexts>
<context position="1392" citStr="Bohnet and Nivre (2012)" startWordPosition="206" endWordPosition="209">n In the SemEval-2014 Task 8 on semantic parsing, the objective is to extract for each sentence a rich set of typed semantic dependencies in three different formats: DM, PAS and PCEDT. These formats differ substantially both in the assignment of semantic heads as well as in the lexicon of semantic dependency types. In the open track of the shared task, participants were encouraged to use all resources and tools also beyond the provided training data. To improve the comparability of the systems, the organizers provided ready-to-use dependency parses produced using the state-of-theart parser of Bohnet and Nivre (2012). In this paper we describe our entry in the open track of the shared task. Our system is a pipeline of three support vector machine classifiers trained separately for detecting semantic dependencies, assigning their roles, and selecting the top nodes of semantic graphs. In this, we loosely follow the architecture of e.g. the TEES (Bj¨orne et al., 2012) and EventMine (Miwa et al., 2012) systems, which were found to be effective in the structurally ∗These authors contributed equally. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedin</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1455–1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="3583" citStr="Chang and Lin, 2011" startWordPosition="542" endWordPosition="545"> n-gram statistics. In the following sections, we describe the three individual classification steps of our semantic parsing pipeline. 2 Detecting Semantic Dependencies The first step of our semantic parsing pipeline is to detect semantic dependencies, i.e. governordependent pairs which has a semantic relation between them. The first stage covers only the identification of such dependencies; the labels describing the semantic roles of the dependents are assigned in a later stage. The semantic dependencies are identified using a binary support vector machine classifier from the LIBSVM package (Chang and Lin, 2011). Each possible combination of two tokens in the sentence is considered to be a candidate for a semantic dependency in both directions, and thus also included as a training example. No beforehand pruning of possible candidates is performed during training. However, we correct for the overwhelming number of negative training examples 678 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 678–682, Dublin, Ireland, August 23-24, 2014. by setting the weights of positive and negative examples used during training, so as to maximize the unlabeled F1-score on t</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of Syntactic-Ngrams over time from a very large corpus of English books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>241--247</pages>
<contexts>
<context position="8949" citStr="Goldberg and Orwant, 2013" startWordPosition="1378" endWordPosition="1381">res, shown in Table 1, follow the same style as in dependency identification. We also combine some of the basic features by creating all possible feature pairs in a given set, but do not perform this with the full set of features. In the open track, participants are also allowed to use additional data and tools beyond the official training data. In addition to the parse trees, we include also features utilizing syntactic n-gram frequencies and vector space similarities. Google has recently released a large corpus of syntactic n-grams, a collection of dependency subtrees with frequency counts (Goldberg and Orwant, 2013). The syntactic n-grams are induced from the Google Books collection, a 350B token corpus of syntactically parsed text. In this work we are interested in arcs, which are (governor, dependent, syntactic relation) triplets associated with their count. For each governor-dependent pair, we generate a set of n-gram features by iterating through all known dependency types and searching from the syntactic n-grams how many times (if any) the governor-dependent pair with the particular dependency type is seen. A separate feature is then created for each dependency type and the counts are encoded in fea</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A dataset of Syntactic-Ngrams over time from a very large corpus of English books. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 241–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>169--184</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6449" citStr="Joachims (1999)" startWordPosition="988" endWordPosition="989">d-out development set. Interestingly, the highest performance is achieved with a rather small set of features, whose full listing is shown in Table 1. The feature vectors are normalized to unit length prior to classification and the SVM regularization parameter c is optimized separately for each annotation format. 3 Role Assignment After the semantic governor-dependent pairs are identified, the next step is to assign a role for each pair to constitute a full semantic dependency. This is done by training a multiclass support vector machine classifier implemented in the SVMmulticlass package by Joachims (1999). We itFeature D R T arg.pos X X arg.deptype X X arg.lemma X X pred.pos X X X pred.deptype X X X pred.lemma X X X pred.is predicate X X arg.issyntaxdep X arg.issyntaxgov X arg.issyntaxsibling X path.length X X undirected path.deptype X X directed path.deptype X X undirected path.pos X X extended path.deptype X X simplified path.deptype with len X X simplified path.deptype wo len X splitted undirected path.deptype arg.prev.pos X X arg.next.pos X X arg.prev+arg.pos X X arg.next+arg.pos X X arg.next+arg+arg.prev.pos X X pred.prev.pos X X pred.next.pos X X pred.prev+pred.pos X X pred.next+pred.pos</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support Vector Learning, pages 169–184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenna Kanerva</author>
<author>Filip Ginter</author>
</authors>
<title>Post-hoc manipulations of vector space models with application to semantic role labeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="10555" citStr="Kanerva and Ginter (2014)" startWordPosition="1622" endWordPosition="1625">a publicly available Google News vector space model1 representing word similarities. The vector space model is induced from the Google News corpus with the word2vec software (Mikolov et al., 2013) and negative sampling ar1https://code.google.com/p/word2vec/ chitecture, and each vector have 300 dimensions. The vector space representation gives us an opportunity to measure word similarities using the standard cosine similarity function. The approach to transforming the vector representations into features varies with the three different annotation formats. On DM and PAS, we follow the method of Kanerva and Ginter (2014), where for each role an average argument vector is calculated. This is done by averaging all word vectors seen in the training data as arguments for the given predicate with a particular role. For each candidate argument, we can then establish a set of similarity values to each possible role by taking the cosine similarity of the argument vector to the role-wise average vectors. These similarities are then turned into separate features, where the similarity values are encoded as feature weights. On PCEDT, preliminary experiments showed that the best strategy to include word vectors into class</context>
</contexts>
<marker>Kanerva, Ginter, 2014</marker>
<rawString>Jenna Kanerva and Filip Ginter. 2014. Post-hoc manipulations of vector space models with application to semantic role labeling. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL, pages 1– 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Workshop Proceedings of International Conference on Learning Representations.</booktitle>
<contexts>
<context position="10126" citStr="Mikolov et al., 2013" startWordPosition="1559" endWordPosition="1562">dency type and the counts are encoded in feature weights compressed using w = logio(count). This approach gives us an opportunity to include statistical information about word relations induced from a very large corpus. Information is captured also outside the particular syntactic context, as we iterate through all known dependency types during the process. Another source of additional data used in role classification is a publicly available Google News vector space model1 representing word similarities. The vector space model is induced from the Google News corpus with the word2vec software (Mikolov et al., 2013) and negative sampling ar1https://code.google.com/p/word2vec/ chitecture, and each vector have 300 dimensions. The vector space representation gives us an opportunity to measure word similarities using the standard cosine similarity function. The approach to transforming the vector representations into features varies with the three different annotation formats. On DM and PAS, we follow the method of Kanerva and Ginter (2014), where for each role an average argument vector is calculated. This is done by averaging all word vectors seen in the training data as arguments for the given predicate w</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Workshop Proceedings of International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Paul Thompson</author>
<author>John McNaught</author>
<author>Douglas Kell</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Extracting semantically enriched events from biomedical literature.</title>
<date>2012</date>
<journal>BMCBioinformatics,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="1781" citStr="Miwa et al., 2012" startWordPosition="270" endWordPosition="273"> resources and tools also beyond the provided training data. To improve the comparability of the systems, the organizers provided ready-to-use dependency parses produced using the state-of-theart parser of Bohnet and Nivre (2012). In this paper we describe our entry in the open track of the shared task. Our system is a pipeline of three support vector machine classifiers trained separately for detecting semantic dependencies, assigning their roles, and selecting the top nodes of semantic graphs. In this, we loosely follow the architecture of e.g. the TEES (Bj¨orne et al., 2012) and EventMine (Miwa et al., 2012) systems, which were found to be effective in the structurally ∗These authors contributed equally. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ related task of biomedical event extraction. Similar classification approach is shown to be effective also in semantic parsing by e.g. Zhao et al. (2009), the winner of the CoNLL’09 Shared Task on Syntactic and Semantic Dependencies in Multiple Languages (SRL-only subtask) (Hajiˇc et al.</context>
</contexts>
<marker>Miwa, Thompson, McNaught, Kell, Ananiadou, 2012</marker>
<rawString>Makoto Miwa, Paul Thompson, John McNaught, Douglas Kell, and Sophia Ananiadou. 2012. Extracting semantically enriched events from biomedical literature. BMCBioinformatics, 13(1):108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Multilingual dependency learning: a huge feature engineering method to semantic dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="2246" citStr="Zhao et al. (2009)" startWordPosition="336" endWordPosition="339">g the top nodes of semantic graphs. In this, we loosely follow the architecture of e.g. the TEES (Bj¨orne et al., 2012) and EventMine (Miwa et al., 2012) systems, which were found to be effective in the structurally ∗These authors contributed equally. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ related task of biomedical event extraction. Similar classification approach is shown to be effective also in semantic parsing by e.g. Zhao et al. (2009), the winner of the CoNLL’09 Shared Task on Syntactic and Semantic Dependencies in Multiple Languages (SRL-only subtask) (Hajiˇc et al., 2009), where semantic parsing is approached as a word-pair classification problem and semantic arguments and their roles are predicted simultaneously. In preliminary experiments, we also developed a joint approach to simultaneously identify semantic dependencies and assign their roles, but found that the performance of the joint prediction was substantially worse than for the current pipeline approach. As the source of features, we rely heavily on the syntact</context>
</contexts>
<marker>Zhao, Chen, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong Zhou. 2009. Multilingual dependency learning: a huge feature engineering method to semantic dependency parsing. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 55–60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>