<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.258698">
<title confidence="0.992804">
Active Learning for Constrained Dirichlet Process Mixture Models
</title>
<author confidence="0.977094">
Andreas Vlachos Zoubin Ghahramani Ted Briscoe
</author>
<affiliation confidence="0.9900675">
Computer Laboratory Department of Engineering Computer Laboratory
University of Cambridge University of Cambridge University of Cambridge
</affiliation>
<email confidence="0.987987">
av308@cl.cam.ac.uk zoubin@eng.cam.ac.uk ejb@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807">
Recent work applied Dirichlet Process
Mixture Models to the task of verb cluster-
ing, incorporating supervision in the form
of must-links and cannot-links constraints
between instances. In this work, we intro-
duce an active learning approach for con-
straint selection employing uncertainty-
based sampling. We achieve substantial
improvements over random selection on
two datasets.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940975609756">
Bayesian non-parametric mixture models have the
attractive property that the number of components
used to model the data is not fixed in advance but
is determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel in-
formation. Recent work has applied such mod-
els to various tasks with promising results, e.g.
Teh (2006) and Cohn et al. (2009).
Vlachos et al. (2009) applied the basic model
of this class, the Dirichlet Process Mixture Model
(DPMM), to lexical-semantic verb clustering with
encouraging results. The task involves discov-
ering classes of verbs similar in terms of their
syntactic-semantic properties (e.g. MOTION class
for travel, walk, run, etc.). Such classes can pro-
vide important support for other tasks, such as
word sense disambiguation, parsing and seman-
tic role labeling. (Dang, 2004; Swier and Steven-
son, 2004) Although some fixed classifications are
available these are not comprehensive and are in-
adequate for specific domains.
Furthermore, Vlachos et al. (2009) used a con-
strained version of the DPMM in order to guide
clustering towards some prior intuition or consid-
erations relevant to the specific task at hand. This
supervision was modelled as pairwise constraints
between instances and it informs the model of re-
lations between them that cannot be recovered by
the model on the basis of the feature representa-
tion used. Like other forms of supervision, these
constraints require manual annotation and it is im-
portant to maximize the benefits obtained from it.
Therefore it is natural to consider active learning
(Settles, 2009) in order to focus the supervision on
clusterings on which the model is uncertain.
In this work, we propose a simple yet effec-
tive active learning method employing uncertainty
based sampling. The effectiveness of the AL
method is demonstrated on two datasets, one of
which has multiple gold standards.
</bodyText>
<sectionHeader confidence="0.883858" genericHeader="method">
2 Constrained DPMMs for clustering
</sectionHeader>
<bodyText confidence="0.999842590909091">
In DPMMs, the parameters of each component are
generated by a Dirichlet Process (DP) which can
be seen as a distribution over distributions. Each
instance, represented by its features, is generated
by the component it is assigned to. The compo-
nents discovered correspond to the clusters. The
prior probability of assigning an instance to a par-
ticular component is proportionate to the number
of instances already assigned to it, in other words,
the DPMM exhibits the “rich get richer” prop-
erty. A popular metaphor to describe the DPMM
which exhibits an equivalent clustering property
is the Chinese Restaurant Process (CRP). Cus-
tomers (instances) arrive at a Chinese restaurant
which has an infinite number of tables (compo-
nents). Each customer sits at one of the tables that
is either occupied or vacant with popular tables at-
tracting more customers.
Following Navarro et al. (2006), parameter es-
timation is performed using Gibbs sampling by
sampling the assignment zi of each instance xi
given all the others z−i and the data X:
</bodyText>
<equation confidence="0.9994055">
P(zi = z|z−i, X) a
Azi = z|z−i)P(xi|zi = z, X−i) (1)
</equation>
<page confidence="0.957582">
57
</page>
<note confidence="0.6205025">
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57–61,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.996324125">
In Eq. 1 p(zi = z|z_i) is the CRP prior and
P(xi|zi = z, X_i) is the distribution that gener-
ates instance xi given it has been assigned to com-
ponent z. This sampling scheme is possible be-
cause the assignments in the model are exchange-
able, i.e. their order is not relevant.
The constrained version of the DPMM uses
pairwise constraints over instances in order to
adapt the clustering discovered. Following
Wagstaff &amp; Cardie (2000), a pair of instances is
either linked together (must-link) or not (cannot-
link). For example, charge and run should form a
must-link if the aim is to cluster MOTION verbs
together, but they should form a cannot-link if we
are interested in BILL verbs. All links are as-
sumed to be consistent with each other. In order
to incorporate the constraints in the DPMM, the
Gibbs sampling scheme is modified so that must-
linked instances are generated by the same compo-
nent and cannot-linked instances always by differ-
ent ones. Following Vlachos et al. (2009), for each
instance that does not belong to a linked-group, the
sampler is restricted to choose components that do
not contain instances cannot-linked with it. For
instances in a linked-group, their assignment is
sampled jointly, again taking into account their
cannot-links. This is performed by adding each
instance of the linked-group successively to the
same component. In terms of the CRP metaphor,
customers connected with must-links arrive at the
restaurant and choose a table jointly, respecting
their cannot-links with other customers.
</bodyText>
<sectionHeader confidence="0.992208" genericHeader="method">
3 Active Constraint Selection
</sectionHeader>
<bodyText confidence="0.9999053">
In active learning, the model selects the supervi-
sion to be provided by a human expert. In the con-
text of the DPMMs, the model chooses a pair of
instances for which a must-link or a cannot-link
must be provided. To select the pair, we employ
the simple but effective idea of uncertainty based
sampling. We consider the most informative link
as that on which the model is most uncertain, more
formally the link between instances Vij that maxi-
mizes the following entropy:
</bodyText>
<equation confidence="0.8943415">
Zj = arg max H(zi = zj) (2)
i,j
</equation>
<bodyText confidence="0.999975675">
If we consider clustering as binary classification of
links into must-links and cannot-links, it is equiv-
alent to selecting the pair with the highest label
entropy. During the sampling process used for
parameter inference, component assignments vary
between samples and the components themselves
are not identifiable, i.e. one cannot match the com-
ponents of one sample with those of another. Fur-
thermore, the conditional assignments estimated
during Gibbs sampling (Eq. 1) they do not capture
the uncertainty of the assignments z_i on which
they condition. Therefore, we resort to generating
a set of samples from the (possibly constrained)
DPMM and pick the link on which these sam-
ples maximally disagree, i.e. we approximate the
distribution in Eq. 2 with the probability that in-
stances i, j are in the same cluster or not. Thus,
in a given set of samples the most uncertain link
would be the one between two instances which are
in the same cluster in exactly half of these sam-
ples. Using multiple samples allows us to take into
account the uncertainty in the assignments of the
other instances, as well as the varying number of
components.
Compared to standard pool-based AL, when
clustering with constraints the possible links be-
tween two instances (ignoring transitivity) are
C(N, 2) = N(N − 1)/2 (N is the size of the
dataset) and there is an equal number of candi-
date queries to be considered, as opposed to N
queries in a supervised classification task. Another
interesting difference is that the the AL process
can be initiated without any supervision, since the
DPMM is unsupervised. On the other hand, in
the standard AL scenario a (usually small) labelled
seed set is used. Therefore, we rely exclusively on
the model and the features to guide the constraint
selection process. If the model combined with the
features is not appropriate for the task then the
constraints chosen are unlikely to be useful.
</bodyText>
<sectionHeader confidence="0.989256" genericHeader="method">
4 Datasets and Evaluation
</sectionHeader>
<bodyText confidence="0.999957857142857">
In our experiments we used two verb clustering
datasets, one from general English (Sun et al.,
2008) and one from the biomedical domain (Ko-
rhonen et al., 2006). In both datasets the fea-
tures for each verb are its subcategorization frames
(SCFs) which capture the syntactic context in
which it occurs. They were acquired automati-
cally using a domain-independent statistical pars-
ing toolkit, RASP (Briscoe and Carroll, 2002), and
a classifier which identifies verbal SCFs. As a
consequence, they include some noise due to stan-
dard text processing and parsing errors and due to
the subtlety of the argument-adjunct distinction.
The general English dataset contains 204 verbs
</bodyText>
<page confidence="0.988934">
58
</page>
<bodyText confidence="0.999973205128205">
belonging to 17 fine-grained classes in Levin’s
(Levin, 1993) taxonomy so that each class con-
tains 12 verbs. The biomedical dataset consists of
193 medium to high frequency verbs from a cor-
pus of 2230 full-text articles from 3 biomedical
journals. A team of linguists and biologists cre-
ated a three-level gold standard with 16, 34 and
50 classes. Both datasets were pre-processed us-
ing non-negative matrix factorization (Lin, 2007)
which decomposes a large sparse matrix into two
dense matrices (of lower dimensionality) with
non-negative values. In all experiments 35 dimen-
sions were kept. Preliminary experiments with
different number of dimensions kept did not affect
the performance substantially.
We evaluate our results using three informa-
tion theoretic measures: Variation of Informa-
tion (Meil˘a, 2007), V-measure (Rosenberg and
Hirschberg, 2007) and V-beta (Vlachos et al.,
2009). All three assess the two desirable proper-
ties that a clustering should have with respect to
a gold standard, homogeneity and completeness.
Homogeneity reflects the degree to which each
cluster contains instances from a single class and
is defined as the conditional entropy of the class
distribution of the gold standard given the clus-
tering. Completeness reflects the degree to which
each class is contained in a single cluster and is de-
fined as the conditional entropy of clustering given
the class distribution in the gold standard. V-beta
balances these properties explicitly by taking into
account the ratio of the number of cluster discov-
ered over the number of classes in the gold stan-
dard. While an ideal clustering should have both
properties, naively improving one of them can be
harmful for the other. Compared to the more com-
monly used F-measure (Fung et al., 2003), these
measures have the advantage that they do not as-
sume a mapping between clusters and classes.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999985721311475">
We performed experiments in order to assess the
effectiveness of the AL algorithm for the con-
strained DPMM comparing it to random selection.
In each AL round, we run the Gibbs sampler for
the (constrained) DPMM five times, using 100 it-
erations for burn-in, draw 20 samples from each
run with 5 iterations lag between samples and se-
lect the most uncertain link to be labeled. Fol-
lowing Navarro et al. (2006), the concentration
parameter is inferred from the data using Gibbs
sampling. The performances were averaged across
the collected samples. Random selection was re-
peated three times. The three levels of the biomed-
ical gold standard were used independently and to-
gether with the general English dataset result in
four experimental setups.
The comparison between AL and random se-
lection for each dataset is shown in graphs 1(a)-
1(d) using V-beta, noting that the observations
made hold with all evaluation metrics used. Con-
straints selected via AL improve the performance
rapidly. Indicatively, the performance reached us-
ing 1000 randomly chosen constraints is obtained
using only 110 actively selected ones in the bio-50
dataset. AL performance levels out in later stages
with performance superior to the one achieved us-
ing random selection with the same number of
constraints. The poor performance of random se-
lection is expected, since the unsupervised DPMM
predicts more than 90% of the binary links cor-
rectly. Another interesting observation is that, dur-
ing AL, homogeneity increased faster than com-
pleteness (graphs 1(g) and 1(h)). This suggests
that the features used lead the model towards finer-
grained clusters, which is further confirmed by
the fact that the highest scores on the biomedical
dataset are achieved when comparing against the
finest-grained version of the gold standard. While
it is possible to choose constraints to the model
that would increase completeness with respect to
the gold standard, we argue that this would not al-
low us to obtain obtain insights on the model and
the features used.
We also noticed that the choice of batch size
has a significant effect on the learning rate of the
model. This phenomenon occurs in varying de-
grees in many applications of AL. Manual inspec-
tion of the links chosen at each round revealed that
batches often contained links involving the same
instances. This is expected due to transitivity: if
the link between instances A and B is uncertain
but the link between instances B and C is certain,
then the link between A and C will be uncertain
too. While reducing the batch size leads to bet-
ter learning rates, it requires estimating the model
more often. In order to ameliorate this issue, af-
ter obtaining the label of the most uncertain link,
we remove the samples that disagreed with it and
re-calculate the uncertainty of the remaining links
given the remaining samples. This is repeated un-
til the intended batch size is reached. Thus, we
</bodyText>
<page confidence="0.993686">
59
</page>
<figure confidence="0.999907853658536">
V-beta
V-beta
0 50 100 150 200 250
links
0 50 100 150 200 250
links
0 50 100 150 200 250
links
0 50 100 150 200 250
links
0.9
0.85
0.8
0.75
0.7
0.65
active
random
0.9
0.85
0.8
0.75
0.7
active
random
active
random
active
random
V-beta 0.88
0.84
0.8
0.76
0.72
V-beta 0.75
0.7
0.65
0.6
0.55
0.9
0.85
V-beta
0.8
0.75
0.7
0.65
0.85
0.8
0.75
V-beta
0.7
0.65
0.6
0.55
(b) bio-34
(c) bio-50
hom
comp
(d) gen. English
hom
comp
active10
batch
V-beta
0.85
0.75
0.9
0.8
0.7
(a) bio-16
V-beta
0.85
0.75
0.65
0.9
0.8
0.7
active10
batch
0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250
links links links links
(e) bio-16 (f) bio-34 (g) bio-50 (h) gen. English
</figure>
<figureCaption confidence="0.949083">
Figure 1: (a)-(d): Constrained DPMM learning curves comparing random selection and AL. (e),(f):
Batch selection comparison. (g),(h): Homogeneity and completeness curves during AL.
</figureCaption>
<bodyText confidence="0.990622391304348">
avoid selecting links involving the same instance,
unless their uncertainty was not reduced by the
constraints added. A consideration that arises is
that by reducing the number of samples used for
uncertainty estimation, progressively we are left
with fewer samples to rank the remaining links.
Each labeled link reduces the number of samples
approximately by half since the most uncertain
link is likely to be a must-link in half the sam-
ples and a cannot-lnk in the remaining half. As
a result, for a batch with size JBI the uncertainty
of the last link will be estimated using |S|/2|B|−1
samples. A crude solution would be to generate
enough samples for the desired batch size. How-
ever, obtaining a very large number of samples can
be computationally expensive. Therefore, we set a
threshold for the minimum number of samples to
be used to estimate the link uncertainty and when
it is reached, more samples are generated using the
constraints selected. In graphs 1(e) and 1(f) we
demonstrate the effectiveness of the batch selec-
tion method proposed (labeled “batch”) compared
to naive batch selection (labeled “active10”).
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999963243243243">
We presented an AL method for constrained DP-
MMs employing uncertainty based sampling. We
applied it to two different verb clustering datasets
with 4 gold standards in total and obtained very
good results compared to random selection. The
idea, while explored in the context of verb cluster-
ing with the constrained DPMM, is likely to be ap-
plicable to other models that can incorporate must-
links and cannot-links in MCMC sampling.
Most literature on AL for NLP considers super-
vised methods for classification or sequential tag-
ging. However, AL for clustering is a relatively
under-explored area. Klein et al. (2002) incorpo-
rated actively selected constraints in hierarchical
agglomerative clustering. Basu et al. (2006) have
applied AL to obtain must-links and cannot-links
however, the clustering framework used requires
the number of clusters to be known in advance
which restricts counter-intuitively the clustering
solutions that are discovered. Moreover, semi-
supervised clustering is a form of semi-supervised
learning and in this light, our approach is related
to the work of Zhu et al. (2003).
With respect to the practical application of the
AL method suggested, it is worth noting that in all
our experiments the constraints were obtained for
the respective gold standard of the dataset at ques-
tion and consequently they are all consistent with
each other. However, this assumption might not
hold in case human experts are employed for the
same purpose. In order to use such feedback in the
framework suggested, it is necessary to filter the
constraints provided in order to obtain a consistent
subset. To this end, it would be interesting to in-
vestigate the potential of using “soft” constraints,
i.e. constraints that are provided with relative con-
fidence.
</bodyText>
<sectionHeader confidence="0.989934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999566772727273">
Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond J. Mooney. 2006. Probabilis-
tic semi-supervised clustering with constraints. In
O. Chapelle, B. Schoelkopf, and A. Zien, edi-
tors, Semi-Supervised Learning, pages 73–102. MIT
Press.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499–1504.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548–556.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Benjamin C. M. Fung, Ke Wang, and Martin Ester.
2003. Hierarchical document clustering using fre-
quent itemsets. In Proceedings of SIAM Interna-
tional Conference on Data Mining, pages 59–70.
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of the
Nineteenth International Conference on Machine
Learning, pages 307–314.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006. Automatic classification of verbs in
biomedical texts. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 345–352.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756–2779.
Marina Meil˘a. 2007. Comparing clusterings—an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873–895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using Dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101–122, April.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410–420.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin–Madison.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95–102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 985–992, Sydney, Australia, July.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103–1110.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Combining Active Learning and Semi-
Supervised Learning Using Gaussian Fields and
Harmonic Functions. In ICML workshop on The
Continuum from Labeled to Unlabeled Data in Ma-
chine Learning and Data Mining, pages 58–65.
</reference>
<page confidence="0.999269">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.784468">
<title confidence="0.999886">Active Learning for Constrained Dirichlet Process Mixture Models</title>
<author confidence="0.999933">Andreas Vlachos Zoubin Ghahramani Ted Briscoe</author>
<affiliation confidence="0.999818">Computer Laboratory Department of Engineering Computer Laboratory University of Cambridge University of Cambridge University of</affiliation>
<email confidence="0.794082">av308@cl.cam.ac.ukzoubin@eng.cam.ac.ukejb@cl.cam.ac.uk</email>
<abstract confidence="0.9986409">Recent work applied Dirichlet Process Mixture Models to the task of verb clustering, incorporating supervision in the form between instances. In this work, we introduce an active learning approach for constraint selection employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sugato Basu</author>
<author>Mikhail Bilenko</author>
<author>Arindam Banerjee</author>
<author>Raymond J Mooney</author>
</authors>
<title>Probabilistic semi-supervised clustering with constraints.</title>
<date>2006</date>
<booktitle>Semi-Supervised Learning,</booktitle>
<pages>73--102</pages>
<editor>In O. Chapelle, B. Schoelkopf, and A. Zien, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16153" citStr="Basu et al. (2006)" startWordPosition="2643" endWordPosition="2646"> to two different verb clustering datasets with 4 gold standards in total and obtained very good results compared to random selection. The idea, while explored in the context of verb clustering with the constrained DPMM, is likely to be applicable to other models that can incorporate mustlinks and cannot-links in MCMC sampling. Most literature on AL for NLP considers supervised methods for classification or sequential tagging. However, AL for clustering is a relatively under-explored area. Klein et al. (2002) incorporated actively selected constraints in hierarchical agglomerative clustering. Basu et al. (2006) have applied AL to obtain must-links and cannot-links however, the clustering framework used requires the number of clusters to be known in advance which restricts counter-intuitively the clustering solutions that are discovered. Moreover, semisupervised clustering is a form of semi-supervised learning and in this light, our approach is related to the work of Zhu et al. (2003). With respect to the practical application of the AL method suggested, it is worth noting that in all our experiments the constraints were obtained for the respective gold standard of the dataset at question and consequ</context>
</contexts>
<marker>Basu, Bilenko, Banerjee, Mooney, 2006</marker>
<rawString>Sugato Basu, Mikhail Bilenko, Arindam Banerjee, and Raymond J. Mooney. 2006. Probabilistic semi-supervised clustering with constraints. In O. Chapelle, B. Schoelkopf, and A. Zien, editors, Semi-Supervised Learning, pages 73–102. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="8374" citStr="Briscoe and Carroll, 2002" startWordPosition="1349" endWordPosition="1352">eatures to guide the constraint selection process. If the model combined with the features is not appropriate for the task then the constraints chosen are unlikely to be useful. 4 Datasets and Evaluation In our experiments we used two verb clustering datasets, one from general English (Sun et al., 2008) and one from the biomedical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs 58 belonging to 17 fine-grained classes in Levin’s (Levin, 1993) taxonomy so that each class contains 12 verbs. The biomedical dataset consists of 193 medium to high frequency verbs from a corpus of 2230 full-text articles from 3 biomedical journals. A team of linguists and biologists created a three-level gold standard with 16, 34 and 50 classes. </context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>548--556</pages>
<contexts>
<context position="1123" citStr="Cohn et al. (2009)" startWordPosition="156" endWordPosition="159">is work, we introduce an active learning approach for constraint selection employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 1 Introduction Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are inadequate for specifi</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Investigations into the role of lexical semantics in word sense disambiguation.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="1586" citStr="Dang, 2004" startWordPosition="228" endWordPosition="229"> discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are inadequate for specific domains. Furthermore, Vlachos et al. (2009) used a constrained version of the DPMM in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand. This supervision was modelled as pairwise constraints between instances and it informs the model of relations between them that cannot be recovered by the model on the basis of the feature representation used. Like other forms of supervision, these constraints re</context>
</contexts>
<marker>Dang, 2004</marker>
<rawString>Hoa Trang Dang. 2004. Investigations into the role of lexical semantics in word sense disambiguation. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin C M Fung</author>
<author>Ke Wang</author>
<author>Martin Ester</author>
</authors>
<title>Hierarchical document clustering using frequent itemsets.</title>
<date>2003</date>
<booktitle>In Proceedings of SIAM International Conference on Data Mining,</booktitle>
<pages>59--70</pages>
<contexts>
<context position="10393" citStr="Fung et al., 2003" startWordPosition="1670" endWordPosition="1673">tional entropy of the class distribution of the gold standard given the clustering. Completeness reflects the degree to which each class is contained in a single cluster and is defined as the conditional entropy of clustering given the class distribution in the gold standard. V-beta balances these properties explicitly by taking into account the ratio of the number of cluster discovered over the number of classes in the gold standard. While an ideal clustering should have both properties, naively improving one of them can be harmful for the other. Compared to the more commonly used F-measure (Fung et al., 2003), these measures have the advantage that they do not assume a mapping between clusters and classes. 5 Experiments We performed experiments in order to assess the effectiveness of the AL algorithm for the constrained DPMM comparing it to random selection. In each AL round, we run the Gibbs sampler for the (constrained) DPMM five times, using 100 iterations for burn-in, draw 20 samples from each run with 5 iterations lag between samples and select the most uncertain link to be labeled. Following Navarro et al. (2006), the concentration parameter is inferred from the data using Gibbs sampling. Th</context>
</contexts>
<marker>Fung, Wang, Ester, 2003</marker>
<rawString>Benjamin C. M. Fung, Ke Wang, and Martin Ester. 2003. Hierarchical document clustering using frequent itemsets. In Proceedings of SIAM International Conference on Data Mining, pages 59–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Sepandar D Kamvar</author>
<author>Christopher D Manning</author>
</authors>
<title>From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Nineteenth International Conference on Machine Learning,</booktitle>
<pages>307--314</pages>
<contexts>
<context position="16049" citStr="Klein et al. (2002)" startWordPosition="2630" endWordPosition="2633"> Work We presented an AL method for constrained DPMMs employing uncertainty based sampling. We applied it to two different verb clustering datasets with 4 gold standards in total and obtained very good results compared to random selection. The idea, while explored in the context of verb clustering with the constrained DPMM, is likely to be applicable to other models that can incorporate mustlinks and cannot-links in MCMC sampling. Most literature on AL for NLP considers supervised methods for classification or sequential tagging. However, AL for clustering is a relatively under-explored area. Klein et al. (2002) incorporated actively selected constraints in hierarchical agglomerative clustering. Basu et al. (2006) have applied AL to obtain must-links and cannot-links however, the clustering framework used requires the number of clusters to be known in advance which restricts counter-intuitively the clustering solutions that are discovered. Moreover, semisupervised clustering is a form of semi-supervised learning and in this light, our approach is related to the work of Zhu et al. (2003). With respect to the practical application of the AL method suggested, it is worth noting that in all our experimen</context>
</contexts>
<marker>Klein, Kamvar, Manning, 2002</marker>
<rawString>Dan Klein, Sepandar D. Kamvar, and Christopher D. Manning. 2002. From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 307–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Nigel Collier</author>
</authors>
<title>Automatic classification of verbs in biomedical texts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>345--352</pages>
<contexts>
<context position="8111" citStr="Korhonen et al., 2006" startWordPosition="1308" endWordPosition="1312">g difference is that the the AL process can be initiated without any supervision, since the DPMM is unsupervised. On the other hand, in the standard AL scenario a (usually small) labelled seed set is used. Therefore, we rely exclusively on the model and the features to guide the constraint selection process. If the model combined with the features is not appropriate for the task then the constraints chosen are unlikely to be useful. 4 Datasets and Evaluation In our experiments we used two verb clustering datasets, one from general English (Sun et al., 2008) and one from the biomedical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs 58 belonging to 17 fine-grained classes in Levin’s (Levin, 1993) taxonomy so that each </context>
</contexts>
<marker>Korhonen, Krymolowski, Collier, 2006</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Nigel Collier. 2006. Automatic classification of verbs in biomedical texts. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 345–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: a preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="8688" citStr="Levin, 1993" startWordPosition="1400" endWordPosition="1401">ical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs 58 belonging to 17 fine-grained classes in Levin’s (Levin, 1993) taxonomy so that each class contains 12 verbs. The biomedical dataset consists of 193 medium to high frequency verbs from a corpus of 2230 full-text articles from 3 biomedical journals. A team of linguists and biologists created a three-level gold standard with 16, 34 and 50 classes. Both datasets were pre-processed using non-negative matrix factorization (Lin, 2007) which decomposes a large sparse matrix into two dense matrices (of lower dimensionality) with non-negative values. In all experiments 35 dimensions were kept. Preliminary experiments with different number of dimensions kept did n</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: a preliminary investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Jen Lin</author>
</authors>
<title>Projected gradient methods for nonnegative matrix factorization.</title>
<date>2007</date>
<journal>Neural Compuation,</journal>
<volume>19</volume>
<issue>10</issue>
<contexts>
<context position="9058" citStr="Lin, 2007" startWordPosition="1461" endWordPosition="1462">clude some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs 58 belonging to 17 fine-grained classes in Levin’s (Levin, 1993) taxonomy so that each class contains 12 verbs. The biomedical dataset consists of 193 medium to high frequency verbs from a corpus of 2230 full-text articles from 3 biomedical journals. A team of linguists and biologists created a three-level gold standard with 16, 34 and 50 classes. Both datasets were pre-processed using non-negative matrix factorization (Lin, 2007) which decomposes a large sparse matrix into two dense matrices (of lower dimensionality) with non-negative values. In all experiments 35 dimensions were kept. Preliminary experiments with different number of dimensions kept did not affect the performance substantially. We evaluate our results using three information theoretic measures: Variation of Information (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007) and V-beta (Vlachos et al., 2009). All three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness. Hom</context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>Chih-Jen Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural Compuation, 19(10):2756–2779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meil˘a</author>
</authors>
<title>Comparing clusterings—an information based distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<volume>98</volume>
<issue>5</issue>
<marker>Meil˘a, 2007</marker>
<rawString>Marina Meil˘a. 2007. Comparing clusterings—an information based distance. Journal of Multivariate Analysis, 98(5):873–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel J Navarro</author>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Michael D Lee</author>
</authors>
<title>Modeling individual differences using Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of Mathematical Psychology,</journal>
<volume>50</volume>
<issue>2</issue>
<contexts>
<context position="3560" citStr="Navarro et al. (2006)" startWordPosition="544" endWordPosition="547">rrespond to the clusters. The prior probability of assigning an instance to a particular component is proportionate to the number of instances already assigned to it, in other words, the DPMM exhibits the “rich get richer” property. A popular metaphor to describe the DPMM which exhibits an equivalent clustering property is the Chinese Restaurant Process (CRP). Customers (instances) arrive at a Chinese restaurant which has an infinite number of tables (components). Each customer sits at one of the tables that is either occupied or vacant with popular tables attracting more customers. Following Navarro et al. (2006), parameter estimation is performed using Gibbs sampling by sampling the assignment zi of each instance xi given all the others z−i and the data X: P(zi = z|z−i, X) a Azi = z|z−i)P(xi|zi = z, X−i) (1) 57 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57–61, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics In Eq. 1 p(zi = z|z_i) is the CRP prior and P(xi|zi = z, X_i) is the distribution that generates instance xi given it has been assigned to component z. This sampling scheme is possible because the assignment</context>
<context position="10913" citStr="Navarro et al. (2006)" startWordPosition="1761" endWordPosition="1764">f them can be harmful for the other. Compared to the more commonly used F-measure (Fung et al., 2003), these measures have the advantage that they do not assume a mapping between clusters and classes. 5 Experiments We performed experiments in order to assess the effectiveness of the AL algorithm for the constrained DPMM comparing it to random selection. In each AL round, we run the Gibbs sampler for the (constrained) DPMM five times, using 100 iterations for burn-in, draw 20 samples from each run with 5 iterations lag between samples and select the most uncertain link to be labeled. Following Navarro et al. (2006), the concentration parameter is inferred from the data using Gibbs sampling. The performances were averaged across the collected samples. Random selection was repeated three times. The three levels of the biomedical gold standard were used independently and together with the general English dataset result in four experimental setups. The comparison between AL and random selection for each dataset is shown in graphs 1(a)- 1(d) using V-beta, noting that the observations made hold with all evaluation metrics used. Constraints selected via AL improve the performance rapidly. Indicatively, the per</context>
</contexts>
<marker>Navarro, Griffiths, Steyvers, Lee, 2006</marker>
<rawString>Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers, and Michael D. Lee. 2006. Modeling individual differences using Dirichlet processes. Journal of Mathematical Psychology, 50(2):101–122, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>410--420</pages>
<contexts>
<context position="9480" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="1518" endWordPosition="1521">om 3 biomedical journals. A team of linguists and biologists created a three-level gold standard with 16, 34 and 50 classes. Both datasets were pre-processed using non-negative matrix factorization (Lin, 2007) which decomposes a large sparse matrix into two dense matrices (of lower dimensionality) with non-negative values. In all experiments 35 dimensions were kept. Preliminary experiments with different number of dimensions kept did not affect the performance substantially. We evaluate our results using three information theoretic measures: Variation of Information (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007) and V-beta (Vlachos et al., 2009). All three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness. Homogeneity reflects the degree to which each cluster contains instances from a single class and is defined as the conditional entropy of the class distribution of the gold standard given the clustering. Completeness reflects the degree to which each class is contained in a single cluster and is defined as the conditional entropy of clustering given the class distribution in the gold standard. V-beta balances these proper</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 410–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey. Computer Sciences</title>
<date>2009</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin–Madison.</institution>
<contexts>
<context position="2340" citStr="Settles, 2009" startWordPosition="349" endWordPosition="350">omains. Furthermore, Vlachos et al. (2009) used a constrained version of the DPMM in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand. This supervision was modelled as pairwise constraints between instances and it informs the model of relations between them that cannot be recovered by the model on the basis of the feature representation used. Like other forms of supervision, these constraints require manual annotation and it is important to maximize the benefits obtained from it. Therefore it is natural to consider active learning (Settles, 2009) in order to focus the supervision on clusterings on which the model is uncertain. In this work, we propose a simple yet effective active learning method employing uncertainty based sampling. The effectiveness of the AL method is demonstrated on two datasets, one of which has multiple gold standards. 2 Constrained DPMMs for clustering In DPMMs, the parameters of each component are generated by a Dirichlet Process (DP) which can be seen as a distribution over distributions. Each instance, represented by its features, is generated by the component it is assigned to. The components discovered cor</context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Burr Settles. 2009. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
</authors>
<title>Verb class discovery from rich syntactic data.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="8052" citStr="Sun et al., 2008" startWordPosition="1298" endWordPosition="1301">n a supervised classification task. Another interesting difference is that the the AL process can be initiated without any supervision, since the DPMM is unsupervised. On the other hand, in the standard AL scenario a (usually small) labelled seed set is used. Therefore, we rely exclusively on the model and the features to guide the constraint selection process. If the model combined with the features is not appropriate for the task then the constraints chosen are unlikely to be useful. 4 Datasets and Evaluation In our experiments we used two verb clustering datasets, one from general English (Sun et al., 2008) and one from the biomedical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs 58 belonging to 17 fine-grai</context>
</contexts>
<marker>Sun, Korhonen, Krymolowski, 2008</marker>
<rawString>Lin Sun, Anna Korhonen, and Yuval Krymolowski. 2008. Verb class discovery from rich syntactic data. In Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert S Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="1614" citStr="Swier and Stevenson, 2004" startWordPosition="230" endWordPosition="234"> novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are inadequate for specific domains. Furthermore, Vlachos et al. (2009) used a constrained version of the DPMM in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand. This supervision was modelled as pairwise constraints between instances and it informs the model of relations between them that cannot be recovered by the model on the basis of the feature representation used. Like other forms of supervision, these constraints require manual annotation and </context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>Robert S. Swier and Suzanne Stevenson. 2004. Unsupervised semantic role labelling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 95–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1100" citStr="Teh (2006)" startWordPosition="153" endWordPosition="154">nstances. In this work, we introduce an active learning approach for constraint selection employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 1 Introduction Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and Constrained Dirichlet Process Mixture Models for Verb Clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL workshop on GEometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="1146" citStr="Vlachos et al. (2009)" startWordPosition="160" endWordPosition="163">e an active learning approach for constraint selection employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 1 Introduction Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are inadequate for specific domains. Furthermore,</context>
<context position="4932" citStr="Vlachos et al. (2009)" startWordPosition="780" endWordPosition="783">r to adapt the clustering discovered. Following Wagstaff &amp; Cardie (2000), a pair of instances is either linked together (must-link) or not (cannotlink). For example, charge and run should form a must-link if the aim is to cluster MOTION verbs together, but they should form a cannot-link if we are interested in BILL verbs. All links are assumed to be consistent with each other. In order to incorporate the constraints in the DPMM, the Gibbs sampling scheme is modified so that mustlinked instances are generated by the same component and cannot-linked instances always by different ones. Following Vlachos et al. (2009), for each instance that does not belong to a linked-group, the sampler is restricted to choose components that do not contain instances cannot-linked with it. For instances in a linked-group, their assignment is sampled jointly, again taking into account their cannot-links. This is performed by adding each instance of the linked-group successively to the same component. In terms of the CRP metaphor, customers connected with must-links arrive at the restaurant and choose a table jointly, respecting their cannot-links with other customers. 3 Active Constraint Selection In active learning, the m</context>
<context position="9514" citStr="Vlachos et al., 2009" startWordPosition="1524" endWordPosition="1527">s and biologists created a three-level gold standard with 16, 34 and 50 classes. Both datasets were pre-processed using non-negative matrix factorization (Lin, 2007) which decomposes a large sparse matrix into two dense matrices (of lower dimensionality) with non-negative values. In all experiments 35 dimensions were kept. Preliminary experiments with different number of dimensions kept did not affect the performance substantially. We evaluate our results using three information theoretic measures: Variation of Information (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007) and V-beta (Vlachos et al., 2009). All three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness. Homogeneity reflects the degree to which each cluster contains instances from a single class and is defined as the conditional entropy of the class distribution of the gold standard given the clustering. Completeness reflects the degree to which each class is contained in a single cluster and is defined as the conditional entropy of clustering given the class distribution in the gold standard. V-beta balances these properties explicitly by taking into acc</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and Constrained Dirichlet Process Mixture Models for Verb Clustering. In Proceedings of the EACL workshop on GEometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiri Wagstaff</author>
<author>Claire Cardie</author>
</authors>
<title>Clustering with instance-level constraints.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>1103--1110</pages>
<contexts>
<context position="4383" citStr="Wagstaff &amp; Cardie (2000)" startWordPosition="685" endWordPosition="688">) 57 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57–61, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics In Eq. 1 p(zi = z|z_i) is the CRP prior and P(xi|zi = z, X_i) is the distribution that generates instance xi given it has been assigned to component z. This sampling scheme is possible because the assignments in the model are exchangeable, i.e. their order is not relevant. The constrained version of the DPMM uses pairwise constraints over instances in order to adapt the clustering discovered. Following Wagstaff &amp; Cardie (2000), a pair of instances is either linked together (must-link) or not (cannotlink). For example, charge and run should form a must-link if the aim is to cluster MOTION verbs together, but they should form a cannot-link if we are interested in BILL verbs. All links are assumed to be consistent with each other. In order to incorporate the constraints in the DPMM, the Gibbs sampling scheme is modified so that mustlinked instances are generated by the same component and cannot-linked instances always by different ones. Following Vlachos et al. (2009), for each instance that does not belong to a linke</context>
</contexts>
<marker>Wagstaff, Cardie, 2000</marker>
<rawString>Kiri Wagstaff and Claire Cardie. 2000. Clustering with instance-level constraints. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 1103–1110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>John Lafferty</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Combining Active Learning and SemiSupervised Learning Using Gaussian Fields and Harmonic Functions.</title>
<date>2003</date>
<booktitle>In ICML workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="16533" citStr="Zhu et al. (2003)" startWordPosition="2701" endWordPosition="2704">methods for classification or sequential tagging. However, AL for clustering is a relatively under-explored area. Klein et al. (2002) incorporated actively selected constraints in hierarchical agglomerative clustering. Basu et al. (2006) have applied AL to obtain must-links and cannot-links however, the clustering framework used requires the number of clusters to be known in advance which restricts counter-intuitively the clustering solutions that are discovered. Moreover, semisupervised clustering is a form of semi-supervised learning and in this light, our approach is related to the work of Zhu et al. (2003). With respect to the practical application of the AL method suggested, it is worth noting that in all our experiments the constraints were obtained for the respective gold standard of the dataset at question and consequently they are all consistent with each other. However, this assumption might not hold in case human experts are employed for the same purpose. In order to use such feedback in the framework suggested, it is necessary to filter the constraints provided in order to obtain a consistent subset. To this end, it would be interesting to investigate the potential of using “soft” const</context>
</contexts>
<marker>Zhu, Lafferty, Ghahramani, 2003</marker>
<rawString>Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani. 2003. Combining Active Learning and SemiSupervised Learning Using Gaussian Fields and Harmonic Functions. In ICML workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, pages 58–65.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>