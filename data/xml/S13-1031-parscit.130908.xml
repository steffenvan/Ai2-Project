<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019589">
<note confidence="0.7371025">
CFILT-CORE: Semantic Textual Similarity using Universal Networking
Language
</note>
<author confidence="0.934876">
Avishek Dan Pushpak Bhattacharyya
</author>
<affiliation confidence="0.931228">
IIT Bombay IIT Bombay
Mumbai, India Mumbai, India
</affiliation>
<email confidence="0.99492">
avishekdan@cse.iitb.ac.in pb@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.996594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999682090909091">
This paper describes the system that was sub-
mitted in the *SEM 2013 Semantic Textual
Similarity shared task. The task aims to find
the similarity score between a pair of sen-
tences. We describe a Universal Network-
ing Language (UNL) based semantic extrac-
tion system for measuring the semantic simi-
larity. Our approach combines syntactic and
word level similarity measures along with the
UNL based semantic similarity measures for
finding similarity scores between sentences.
</bodyText>
<sectionHeader confidence="0.99877" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999782">
Semantic Textual Similarity is the task of finding
the degree of semantic equivalence between a pair
of sentences. The core Semantic Textual Similar-
ity shared task of *SEM 2013 (Agirre et al., 2013)
is to generate a score in the range 0-5 for a pair
of sentences depending on their semantic similar-
ity. Textual similarity finds applications in infor-
mation retrieval and it is closely related to textual
entailment. Universal Networking Language (UNL)
(Uchida, 1996) is an ideal mechanism for seman-
tics representation. Our system first converts the
sentences into a UNL graph representation and then
matches the graphs to generate the semantic relat-
edness score. Even though the goal is to judge sen-
tences based on their semantic relatedness, our sys-
tem incorporates some lexical and syntactic similar-
ity measures to make the system robust in the face
of data sparsity.
Section 2 give a brief introduction to UNL. Sec-
tion 3 decribes the English Enconverter developed
</bodyText>
<figureCaption confidence="0.950162">
Figure 1: UNL Graph for ’The boy chased the dog’
Figure 2: UNL Graph for ’The dog was chased by the
boy’
</figureCaption>
<bodyText confidence="0.954153666666667">
by us. Section 4 discusses the various similarity
measures used for the task. Section 5 mentions
the corpus used for training and testing. Section 6
describes the method used to train the system and
Section 7 presents the results obtained on the task
datasets.
</bodyText>
<sectionHeader confidence="0.842986" genericHeader="method">
2 Universal Networking Language
</sectionHeader>
<bodyText confidence="0.998955666666667">
Universal Networking Language (UNL) is an inter-
lingua that represents a sentence in a language inde-
pendent, unambiguous form. The three main build-
ing blocks of UNL are relations, universal words
and attributes. UNL representations have a graphical
structure with concepts being represented as nodes
(universal words) and interactions between concepts
being represented by edges (relations) between the
nodes. Figure 1 shows the UNL graph correspond-
</bodyText>
<page confidence="0.980616">
216
</page>
<subsubsectionHeader confidence="0.252976">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.9630961875">
and the Shared Task, pages 216–220, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
ing to the sentence ’The boy chased the dog.’ The
conversion from a source language to UNL is called
enconversion. The reverse process of generating a
natural language sentence from UNL is called de-
conversion. The enconversion process is markedly
more difficult than the deconversion process due to
the inherent ambiguity and idiosyncracy of natural
language.
UNL representation captures the semantics inde-
pendent of the structure of the language. Figures
1 and 2 show the UNL representation of two struc-
turally different sentences which convey the same
meaning. The UNL graph structure remains the
same with an additional attribute on the main verb
of figure 2 indicating the voice of the sentence.
</bodyText>
<subsectionHeader confidence="0.972011">
2.1 Universal Words
</subsectionHeader>
<bodyText confidence="0.999989769230769">
Universal words (UWs) are language independent
concepts that are linked to various language re-
sources. The UWs used by us are linked to
the Princeton WordNet and various other language
WordNet synsets. UWs consist of a head word
which is the word in its lemma form. For example,
in figure 2 the word chased is shown in its lemma
form as chased. The head word is followed by a
constraint list which is used to disambiguate it. For
example, chase icl (includes) pursue indicates that
chase as a type of pursuing is indicated here. Com-
plex concepts are represented by hypernodes, which
are UNL graphs themselves.
</bodyText>
<subsectionHeader confidence="0.98951">
2.2 Relations
</subsectionHeader>
<bodyText confidence="0.999962833333333">
Relations are two place functions that imdicate the
relationship between UWs. Some of the commonly
used relations are agent (agt), object (obj), instru-
ment (ins), place (plc). For example, in figure 1 the
relation agt between boy and chase indicates that the
boy is the doer of the action.
</bodyText>
<subsectionHeader confidence="0.997298">
2.3 Attribute
</subsectionHeader>
<bodyText confidence="0.9998275">
Attributes are one place functions that convey vari-
ous morphological and pragmatic information. For
example, in figure 1 the attribute past indicates that
the verb is in the past tense.
</bodyText>
<sectionHeader confidence="0.988766" genericHeader="method">
3 UNL Generation
</sectionHeader>
<bodyText confidence="0.999938352941177">
The conversion from English to UNL involves aug-
menting the sentence with various factors such as
POS tags, NER tags and dependency parse tree
relations and paths. The suitable UW generation
is achieved through a word sense disambiguation
(WSD) system trained on a tourism corpus. The
WSD system maps the words to Wordnet 2.1 synset
ids. The attribute and relation generation is achieved
through a combination of rule-base and classifiers
trained on a small corpus. We use a nearest neighbor
classifier trained on the EOLSS corpus for generat-
ing relations. The attributes are generated by con-
ditional random fields trained on the IGLU corpus.
The attribute generation is a word level phenomena,
hence attributes for complex UWs cannot be gener-
ated by the classifiers. The steps are described in
detail.
</bodyText>
<subsectionHeader confidence="0.999814">
3.1 Parts of Speech Tagging
</subsectionHeader>
<bodyText confidence="0.999910666666667">
The Stanford POS tagger using the WSJ corpus
trained PCFG model is used to tag the sentences.
Penn Treebank style tags are generated.
</bodyText>
<subsectionHeader confidence="0.999771">
3.2 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999866">
A Supervised Word Sense Disambiguation (WSD)
tool trained in Tourism domain is used. The WSD
system takes a sequence of tagged words and pro-
vides the WordNet synset ids of all nouns, verbs, ad-
jectives and adverbs in the sequence. The accuracy
of the system is depends on the length of the input
sentence.
</bodyText>
<subsectionHeader confidence="0.99442">
3.3 Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.992127333333333">
Stanford Named Entity Recognizer is used to tag the
words in the sentence. The tags may be PERSON,
LOCATION or ORGANIZATION.
</bodyText>
<subsectionHeader confidence="0.996199">
3.4 Parsing and Clause Marking
</subsectionHeader>
<bodyText confidence="0.999857583333334">
Stanford Parser is used to parse the sentences. Rules
based on the constituency parse are used to identify
the clause boundaries. The dependency parse is used
for clause type detection. It is also used in the later
stages of UNL generation.
The clauses are converted into separate sim-
ple sentences for further processing. Independent
clauses can be trivially separated since they have
complete sentential structure of their own. Depen-
dent clauses are converted into complete sentences
using rules based on the type of clause. For exam-
ple, for the sentence, That he is a good sprinter, is
</bodyText>
<page confidence="0.988614">
217
</page>
<bodyText confidence="0.9998602">
known to all, containing a nominal clause, the sim-
ple sentences obtained are he is a good sprinter and
it is known to all. Here the dependent clause is re-
placed by the anaphora it to generate the sentence
corresponding to the main clause.
</bodyText>
<subsectionHeader confidence="0.900066">
3.5 UW Generation
</subsectionHeader>
<bodyText confidence="0.9999084">
WordNet synset ids obtained from the WSD system
and the parts of speech tags are used to generate the
UWs. The head word is the English sentence in its
lemma form. The constraint list is generated from
the WordNet depending on the POS tag.
</bodyText>
<subsectionHeader confidence="0.986294">
3.6 Relation Generation
</subsectionHeader>
<bodyText confidence="0.999966909090909">
Relations are generated by a combination of rule
base and corpus based techniques. Rules are writ-
ten using parts of speech tags, named entity tags and
parse dependency relations. The corpus based tech-
niques are used when insufficient rules exist for re-
lation generation. We use a corpus of about 28000
sentences consisting of UNL graphs for WordNet
glosses obtained from the UNDL foundation. This
technique tries to find similar examples from the cor-
pus and assigns the observed relation label to the
new part of the sentence.
</bodyText>
<subsectionHeader confidence="0.995746">
3.7 Attribute Generation
</subsectionHeader>
<bodyText confidence="0.999889166666667">
Attributes are a combination of morphological fea-
tures and pragmatic information. Attribute genera-
tion can be considered to be a sequence labeling task
on the words. A conditional random field trained on
the corpus described in section 5.1 is used for at-
tribute generation.
</bodyText>
<sectionHeader confidence="0.991031" genericHeader="method">
4 Similarity Measures
</sectionHeader>
<bodyText confidence="0.999966">
We broadly define three categories of similarity
measures based on our classification of perception
of similarity.
</bodyText>
<subsectionHeader confidence="0.997652">
4.1 Word based Similarity Measure
</subsectionHeader>
<bodyText confidence="0.999976333333333">
Word based similarity measures consider the sen-
tences as sets-of-words. These measures are mo-
tivated by our view that sentences having a lot of
common words will appear quite similar to a human
user. The sentences are tokenized using Stanford
Parser. The Jaccard coefficient (Agirre and Ghosh
and Mooney, 2000) compares the similarity or diver-
sity of two sets. It is the ratio of size of intersection
to the size of union of two sets. We define a new
measure based on the Jaccard similarity coefficient
that captures the relatedness between words. The
tokens in the set are augmented with related words
from Princeton WordNet. (Pedersen and Patward-
han and Michelizzi, 2004) As a preprocessing step,
all the tokens are stemmed using WordNet Stemmer.
For each possible sense of each stem, its synonyms,
antonyms, hypernyms and holonyms are added to
the set as applicable. For example, hypernyms are
added only when the token appears as a noun or verb
in the WordNet. The scoring function used is defined
as
</bodyText>
<equation confidence="0.9994725">
Ext�Sim(S1, S2) = |ExtS1 ∩ ExtS2|
|S1 ∪ S2|
</equation>
<bodyText confidence="0.999799">
The following example illustrates the intuition be-
hind this similarity measure.
</bodyText>
<listItem confidence="0.999577">
• I am cooking chicken in the house.
• I am grilling chicken in the kitchen.
</listItem>
<bodyText confidence="0.999962333333333">
The measure generates a similarity score of 1
since grilling is a kind of cooking (hypernymy) and
kitchen is a part of house (holonymy).
</bodyText>
<subsectionHeader confidence="0.997">
4.2 Syntactic Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999852142857143">
Structural similarity as an indicator of textual sim-
ilarity is captured by the syntactic similarity mea-
sures. Parses are obtained for the pair of English
sentences using Stanford Parser. The parser is run on
the English PCFG model. The dependency graphs of
the two sentences are matched to generate the simi-
larity score. A dependency graph consists of a num-
ber of dependency relations of the form dep(word1,
word2) where dep is the type of relation and word1
and word2 are the words between which the rela-
tion holds. A complete match of a dependency re-
lation contributes 1 to the score whereas a match of
only the words in the relation contributes 0.75 to the
score.
</bodyText>
<equation confidence="0.980717">
2|
SynSim(S1, S2) = |S1 ∪ S2 |+ 0.75∗
E
aES1,bES211a.w1 = b.w1&amp;a.w2 = b.w2]]
|S1 ∪ S2|
</equation>
<bodyText confidence="0.9589095">
Here S1 and S2 represent the set of dependency
relations.
</bodyText>
<page confidence="0.99636">
218
</page>
<bodyText confidence="0.99993725">
An extended syntactic similarity measure in
which exact word matchings are replaced by a match
within a set formed by extending the word with re-
lated words as described in 4.1 is also used.
</bodyText>
<subsectionHeader confidence="0.999434">
4.3 Semantic Similarity Measure
</subsectionHeader>
<bodyText confidence="0.999862">
Semantic similarity measures try to capture the sim-
ilarity in the meaning of the sentences. The UNL
graphs generated for the two sentences are compared
using the formula given below. In addition, syn-
onymy is no more used for enriching the word bank
since UWs by design are mapped to synsets, hence
all synonyms are equivalent in a UNL graph.
</bodyText>
<equation confidence="0.984954166666667">
�
5em5im(51, 52) = |51 n 52|
|51 U 52 |+
aES1,bES2
[[a.w1 = b.w1&amp;a.w2 = b.w2]]
+ 0.75*
|51 U 52|
[[a.r = b.r&amp;a.Ew1 = b.Ew1&amp;a.Ew2 = b.Ew2]]
|51 U 52|
+0.6 *
[[a.Ew1 = b.Ew1&amp;a.Ew2 = b.Ew2]] )
|51 U 52|
</equation>
<sectionHeader confidence="0.996166" genericHeader="method">
5 Corpus
</sectionHeader>
<bodyText confidence="0.999601380952381">
The system is trained on the Semantic Textual Sim-
ilarity 2012 task data. The training dataset consists
of 750 pairs from the MSR-Paraphrase corpus, 750
sentences from the MSR-Video corpus and 734 pairs
from the SMTeuroparl corpus.
The test set contains headlines mined from sev-
eral news sources mined by European Media Moni-
tor, sense definitions from WordNet and OntoNotes,
sense definitions from WordNet and FrameNet, sen-
tences from DARPA GALE HTER and HyTER,
where one sentence is a MT output and the other is
a reference translation.
Each corpus contains pairs of sentences with an
associated score from 0 to 5. The scores are given
based on whether the sentences are on different top-
ics (0), on the same topic but have different con-
tent (1), not equivalent but sharing some details (2),
roughly equivalent with some inportant information
missing or differing (3), mostly important while dif-
fering in some unimportant details (4) or completely
equivalent (5).
</bodyText>
<tableCaption confidence="0.999289">
Table 1: Results
</tableCaption>
<table confidence="0.9978135">
Corpus CFILT Best Results
Headlines 0.5336 0.7642
OnWN 0.2381 0.7529
FNWN 0.2261 0.5818
SMT 0.2906 0.3804
Mean 0.3531 0.6181
</table>
<sectionHeader confidence="0.997022" genericHeader="method">
6 Training
</sectionHeader>
<bodyText confidence="0.9987658">
The several scores are combined by training a Lin-
ear Regression model. We use the inbuilt libaries of
Weka to learn the weights. To compute the proba-
bility of a test sentence pair, the following formula
is used.
</bodyText>
<equation confidence="0.958159333333333">
5
score(51, 52) = c + � Aiscorei(51, 52)
i=1
</equation>
<sectionHeader confidence="0.999741" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999772">
The test dataset contained many very long sentences
which could not be parsed by the Stanford parser
used by the UNL system. In addition, the perfor-
mance of the WSD system led to numerous false
negatives. Hence erroneous output were produced in
these cases. In these cases, the word based similar-
ity measures somewhat stabilized the scores. Table
1 summarizes the results.
The UNL system is not robust enough to han-
dle large sentences with long distance relationships
which leads to poor performance on the OnWN and
FNWN datasets.
</bodyText>
<sectionHeader confidence="0.989774" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998725">
The approach discussed in the paper shows promise
for the small sentences. The ongoing development
of UNL is expected to improve the accuracy of the
system. Tuning the scoring parameters on a develop-
ment set instead of arbitrary values may improve re-
sults. A log-linear model instead of the linear com-
bination of scores may capture the relationships be-
tween the scores in a better way.
</bodyText>
<sectionHeader confidence="0.997232" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.843613">
Eneko Agirre and Daniel Cer and Mona Diab and Aitor
Gonzalez-Agirre and Weiwei Guo. *SEM 2013
</reference>
<page confidence="0.8290525">
(0.75*
219
</page>
<reference confidence="0.999476117647059">
Shared Task: Semantic Textual Similarity, including a
Pilot on Typed-Similarity. *SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics. Association for Computational Linguistics.
Hiroshi Uchida. UNL: Universal Networking Lan-
guageAn Electronic Language for Communica-
tion, Understanding, and Collaboration. 1996.
UNU/IAS/UNL Center, Tokyo.
Alexander Strehl and Joydeep Ghosh and Raymond
Mooney Impact of similarity measures on web-page
clustering. 2000. Workshop on Artificial Intelligence
for Web Search (AAAI 2000).
Ted Pedersen and Siddharth Patwardhan and Jason
Michelizzi WordNet:: Similarity: measuring the re-
latedness of concepts. 2004. Demonstration Papers
at HLT-NAACL 2004. Association for Computational
Linguistics.
</reference>
<page confidence="0.997569">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666967">
<title confidence="0.9978545">CFILT-CORE: Semantic Textual Similarity using Universal Networking Language</title>
<author confidence="0.998281">Avishek Dan Pushpak Bhattacharyya</author>
<affiliation confidence="0.987292">IIT Bombay IIT Bombay</affiliation>
<address confidence="0.98938">Mumbai, India Mumbai, India</address>
<email confidence="0.690159">avishekdan@cse.iitb.ac.inpb@cse.iitb.ac.in</email>
<abstract confidence="0.999224333333333">This paper describes the system that was submitted in the *SEM 2013 Semantic Textual Similarity shared task. The task aims to find the similarity score between a pair of sentences. We describe a Universal Networking Language (UNL) based semantic extraction system for measuring the semantic similarity. Our approach combines syntactic and word level similarity measures along with the UNL based semantic similarity measures for finding similarity scores between sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
</authors>
<booktitle>SEM 2013 Shared Task: Semantic Textual Similarity, including a Pilot on Typed-Similarity. *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, Guo, </marker>
<rawString>Eneko Agirre and Daniel Cer and Mona Diab and Aitor Gonzalez-Agirre and Weiwei Guo. *SEM 2013 Shared Task: Semantic Textual Similarity, including a Pilot on Typed-Similarity. *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Uchida</author>
</authors>
<title>UNL: Universal Networking LanguageAn Electronic Language for Communication, Understanding, and Collaboration.</title>
<date>1996</date>
<publisher>UNU/IAS/UNL</publisher>
<location>Center, Tokyo.</location>
<contexts>
<context position="1166" citStr="Uchida, 1996" startWordPosition="172" endWordPosition="173">ord level similarity measures along with the UNL based semantic similarity measures for finding similarity scores between sentences. 1 Introduction Semantic Textual Similarity is the task of finding the degree of semantic equivalence between a pair of sentences. The core Semantic Textual Similarity shared task of *SEM 2013 (Agirre et al., 2013) is to generate a score in the range 0-5 for a pair of sentences depending on their semantic similarity. Textual similarity finds applications in information retrieval and it is closely related to textual entailment. Universal Networking Language (UNL) (Uchida, 1996) is an ideal mechanism for semantics representation. Our system first converts the sentences into a UNL graph representation and then matches the graphs to generate the semantic relatedness score. Even though the goal is to judge sentences based on their semantic relatedness, our system incorporates some lexical and syntactic similarity measures to make the system robust in the face of data sparsity. Section 2 give a brief introduction to UNL. Section 3 decribes the English Enconverter developed Figure 1: UNL Graph for ’The boy chased the dog’ Figure 2: UNL Graph for ’The dog was chased by the</context>
</contexts>
<marker>Uchida, 1996</marker>
<rawString>Hiroshi Uchida. UNL: Universal Networking LanguageAn Electronic Language for Communication, Understanding, and Collaboration. 1996. UNU/IAS/UNL Center, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Strehl</author>
<author>Joydeep Ghosh</author>
<author>Raymond Mooney</author>
</authors>
<title>Impact of similarity measures on web-page clustering.</title>
<date>2000</date>
<booktitle>Workshop on Artificial Intelligence for Web Search (AAAI</booktitle>
<marker>Strehl, Ghosh, Mooney, 2000</marker>
<rawString>Alexander Strehl and Joydeep Ghosh and Raymond Mooney Impact of similarity measures on web-page clustering. 2000. Workshop on Artificial Intelligence for Web Search (AAAI 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet:: Similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen and Siddharth Patwardhan and Jason Michelizzi WordNet:: Similarity: measuring the relatedness of concepts. 2004. Demonstration Papers at HLT-NAACL 2004. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>