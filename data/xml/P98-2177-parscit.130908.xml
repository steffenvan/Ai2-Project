<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002294">
<title confidence="0.9973555">
Statistical Models for Unsupervised Prepositional Phrase
Attachment
</title>
<author confidence="0.991977">
Adwait Ratnaparkhi
</author>
<affiliation confidence="0.998462">
Dept. of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.9824385">
200 South 33rd Street
Philadelphia, PA 19104-6389
</address>
<email confidence="0.98168">
adwaitOunagi.cis.upenn.edu
</email>
<sectionHeader confidence="0.992926" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823214285714">
We present several unsupervised statistical
models for the prepositional phrase attachment
task that approach the accuracy of the best su-
pervised methods for this task. Our unsuper-
vised approach uses a heuristic based on at-
tachment proximity and trains from raw text
that is annotated with only part-of-speech tags
and morphological base forms, as opposed to
attachment information. It is therefore less
resource-intensive and more portable than pre-
vious corpus-based algorithm proposed for this
task. We present results for prepositional
phrase attachment in both English and Span-
ish.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999815777777778">
Prepositional phrase attachment is the task of
deciding, for a given preposition in a sentence,
the attachment site that corresponds to the
interpretation of the sentence. For example,
the task in the following examples is to de-
cide whether the preposition with modifies the
preceding noun phrase (with head word shirt)
or the preceding verb phrase (with head word
bought or washed).
</bodyText>
<listItem confidence="0.9316625">
1. I bought the shirt with pockets.
2. I washed the shirt with soap.
</listItem>
<bodyText confidence="0.9997781">
In sentence 1, with modifies the noun shirt, since
with pockets describes the shirt. However in sen-
tence 2, with modifies the verb washed since with
soap describes how the shirt is washed. While
this form of attachment ambiguity is usually
easy for people to resolve, a computer requires
detailed knowledge about words (e.g., washed
vs. bought) in order to successfully resolve such
ambiguities and predict the correct interpreta-
tion.
</bodyText>
<sectionHeader confidence="0.996524" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9999585">
Most of the previous successful approaches to
this problem have been statistical or corpus-
based, and they consider only prepositions
whose attachment is ambiguous between a pre-
ceding noun phrase and verb phrase. Previous
work has framed the problem as a classification
task, in which the goal is to predict N or V, cor-
responding to noun or verb attachment, given
the head verb v, the head noun n, the preposi-
tion p, and optionally, the object of the prepo-
sition n2. For example, the (v,n,p, n2) tuples
corresponding to the example sentences are
</bodyText>
<listItem confidence="0.996877">
1. bought shirt with pockets
2. washed shirt with soap
</listItem>
<bodyText confidence="0.996734782608696">
The correct classifications of tuples 1 and 2 are
N and V, respectively.
(Hindle and Rooth, 1993) describes a par-
tially supervised approach in which the FID-
DITCH partial parser was used to extract
(v, n, p) tuples from raw text, where p is a
preposition whose attachment is ambiguous be-
tween the head verb v and the head noun n.
The extracted tuples are then used to con-
struct a classifier, which resolves unseen ambi-
guities at around 80% accuracy. Later work,
such as (Ratnaparkhi et al., 1994; Brill and
Resnik, 1994; Collins and Brooks, 1995; Merlo
et al., 1997; Zavrel and Daelemans, 1997; Franz,
1997), trains and tests on quintuples of the
form (v, n,p, n2, a) extracted from the Penn
treebank(Marcus et al., 1994), and has gradu-
ally improved on this accuracy with other kinds
of statistical learning methods, yielding up to
84.5% accuracy(Collins and Brooks, 1995). Re-
cently, (Stetina and Naga,o, 1997) have reported
88% accuracy by using a corpus-based model in
conjunction with a semantic dictionary.
</bodyText>
<page confidence="0.996212">
1079
</page>
<bodyText confidence="0.999790166666667">
While previous corpus-based methods are
highly accurate for this task, they are difficult
to port to other languages because they re-
quire resources that are expensive to construct
or simply nonexistent in other languages. We
present an unsupervised algorithm for prepo-
sitional phrase attachment in English that re-
quires only an part-of-speech tagger and a mor-
phology database, and is therefore less resource-
intensive and more portable than previous ap-
proaches, which have all required either tree-
banks or partial parsers.
</bodyText>
<sectionHeader confidence="0.959128" genericHeader="method">
3 Unsupervised Prepositional
</sectionHeader>
<subsectionHeader confidence="0.697341">
Phrase Attachment
</subsectionHeader>
<bodyText confidence="0.999936642857143">
The exact task of our algorithm will be to con-
struct a classifier c/ which maps an instance of
an ambiguous prepositional phrase (v, n, p, n2)
to either N or V, corresponding to noun at-
tachment or verb attachment, respectively. In
the full natural language parsing task, there are
more than just two potential attachment sites,
but we limit our task to choosing between a verb
v and a noun n so that we may compare with
previous supervised attempts on this problem.
While we will be given the candidate attach-
ment sites during testing, the training proce-
dure assumes no a priori information about po-
tential attachment sites.
</bodyText>
<subsectionHeader confidence="0.999723">
3.1 Generating Training Data From
Raw Text
</subsectionHeader>
<bodyText confidence="0.999993333333334">
We generate training data from raw text by
using a part-of-speech tagger, a simple chun-
ker, an extraction heuristic, and a morphology
database. The order in which these tools are
applied to raw text is shown in Table 1. The
tagger from (Ratnaparkhi, 1996) first annotates
sentences of raw text with a sequence of part-
of-speech tags. The chunker, implemented with
two small regular expressions, then replaces
simple noun phrases and quantifier phrases with
their head words. The extraction heuristic then
finds head word tuples and their likely attach-
ments from the tagged and chunked text. The
heuristic relies on the observed fact that in En-
glish and in languages with similar word order,
the attachment site of a preposition is usually
located only a few words to the left of the prepo-
sition. Finally, numbers are replaced by a single
token, the text is converted to lower case, and
the morphology database is used to find the base
forms of the verbs and nouns.
The extracted head word tuples differ from
the training data used in previous supervised at-
tempts in an important way. In the supervised
case, both of the potential sites, namely the verb
v and the noun n are known before the attach-
ment is resolved. In the unsupervised case dis-
cussed here, the extraction heuristic only finds
what it thinks are unambiguous cases of prepo-
sitional phrase attachment. Therefore, there is
only one possible attachment site for the prepo-
sition, and either the verb v or the noun n does
not exist, in the case of noun-attached prepo-
sition or a verb-attached preposition, respec-
tively. This extraction heuristic loosely resem-
bles a step in the bootstrapping procedure used
to get training data for the classifier of (Hindle
and Rooth, 1993). In that step, unambiguous
attachments from the FIDDITCH parser&apos;s out-
put are initially used to resolve some of the am-
biguous attachments, and the resolved cases are
iteratively used to disambiguate the remaining
unresolved cases. Our procedure differs criti-
cally from (Hindle and Rooth, 1993) in that we
do not iterate, we extract unambiguous attach-
ments from unparsed input sentences, and we
totally ignore the ambiguous cases. It is the hy-
pothesis of this approach that the information
in just the unambiguous attachment events can
resolve the ambiguous attachment events of the
test data.
</bodyText>
<subsectionHeader confidence="0.918307">
3.1.1 Heuristic Extraction of
Unambiguous Cases
</subsectionHeader>
<bodyText confidence="0.951224">
Given a tagged and chunked sentence, the ex-
traction heuristic returns head word tuples of
the form (v,p, n2) or (n,p, n2), where v is the
verb, n is the noun, p is the preposition, n2 is
the object of the preposition. The main idea
of the extraction heuristic is that an attach-
ment site of a preposition is usually within a
few words to the left of the preposition. We
extract :
(v,p, n2) if
</bodyText>
<listItem confidence="0.9987168">
• p is a preposition (p of)
• v is the first verb that occurs within K
words to the left of p
• v is not a form of the verb to be
• No noun occurs between v and p
</listItem>
<page confidence="0.93668">
1080
</page>
<table confidence="0.9991488">
Tool Output
Raw Text The professional conduct of lawyers in other jurisdictions is guided by Amer-
ican Bar Association rules or by state bar ethics codes, none of which permit
non-lawyers to be partners in law firms .
4.
POS Tagger The/DT professional/JJ conduct/NN of/IN lawyers/NNS in/IN other/JJ
jurisdictions/NNS is/VBZ guided/VBN by/IN American/NNP Bar/NNP
Association/NNP rules/NNS or/CC by/IN state/NN bar/NN ethics/NNS
codes/NNS ,/, none/NN of/IN which/WDT permit/VBP non-lawyers/NNS
to/TO be/VB partners/NNS in/IN law/NN firms/NNS ./.
4
Chunker conduct/NN of/IN lawyers/NNS in/IN jurisdictions/NNS is/VBZ
guided/VBN by/IN rules/NNS or/CC by/IN codes/NNS ,/, none/NN
of/IN which/WDT permit/VBP non-lawyers/NNS to/TO be/VB part-
ners/NNS in/IN firms/NNS ./.
4-
Extraction Heuristic (n =lawyers, p =in, n2 =jurisdictions)
(v =guided, p =by, n2 =rules)
Morphology (n =lawyer, p -=in, n2 =jurisdiction)
(v =guide, p =by, n2 =rule)
</table>
<tableCaption confidence="0.99891">
Table 1: How to obtain training data from raw text
</tableCaption>
<listItem confidence="0.998385166666667">
• n2 is the first noun that occurs within
K words to the right of p
• No verb occurs between p and n2
(n,p, n2) if
• p is a preposition (p 0 of)
• n is the first noun that occurs within
K words to the left of p
• No verb occurs within K words to the
left of p
• n2 is the first noun that occurs within
K words to the right of p
• No verb occurs between p and n2
</listItem>
<bodyText confidence="0.998079727272727">
Table 1 also shows the result of the applying the
extraction heuristic to a sample sentence.
The heuristic ignores cases where p = of,
since such cases are rarely ambiguous, and we
opt to model them deterministically as noun at-
tachments. We will report accuracies (in Sec-
tion 5) on both cases where p = of and where
p 0 of. Also, the heuristic excludes examples
with the verb to be from the training set (but
not the test set) since we found them to be un-
reliable sources of evidence.
</bodyText>
<subsectionHeader confidence="0.999975">
3.2 Accuracy of Extraction Heuristic
</subsectionHeader>
<bodyText confidence="0.9924988">
Applying the extraction heuristic to 970K unan-
notated sentences from the 1988 Wall St. Jour-
nal&apos; data yields approximately 910K unique
head word tuples of the form (v, p, n2) or
(n,p, n2). The extraction heuristic is far from
perfect; when applied to and compared with the
annotated Wall St. Journal data of the Penn
treebank, only 69% of the extracted head word
tuples represent correct attachments.2 The ex-
tracted tuples are meant to be a noisy but abun-
dant substitute for the information that one
might get from a treebank. Tables 2 and 3
list the most frequent extracted head word tu-
ples for unambiguous verb and noun attach-
ments, respectively. Many of the frequent noun-
attached (n,p, n2) tuples, such as num to num,3
are incorrect. The prepositional phrase to num
is usually attached to a verb such as rise or fall
in the Wall St. Journal domain, e.g., Profits
rose 46 % to 52 million.
</bodyText>
<footnote confidence="0.9960914">
&apos;This data is available from the Linguistic Data Con-
sortium, http://www.ldc.upenn.edu
2This accuracy also excludes cases where p = of.
3Recall the num is the token for quantifier phrases
identified by the chunker, like 5 million, or 6 %.
</footnote>
<page confidence="0.834597">
1081
</page>
<table confidence="0.999872636363636">
Frequency Verb Prep Noun2
8110 close at num
1926 reach for comment
1539 rise to num
1438 compare with num
1072 fall to num
970 account for num
887 value at million
839 say in interview
680 compare with million
673 price at num
</table>
<tableCaption confidence="0.661196">
Table 2: Most frequent (v,p, n2) tuples
</tableCaption>
<table confidence="0.999898636363637">
Frequency Noun Prep Noun2
1983 num to num
923 num from num
853 share from million
723 trading on exchange
721 num in num
560 num to month
519 share on revenue
461 num to day
417 trading on yesterday
376 share on sale
</table>
<tableCaption confidence="0.996172">
Table 3: Most frequent (n,p, n2) tuples
</tableCaption>
<sectionHeader confidence="0.993994" genericHeader="method">
4 Statistical Models
</sectionHeader>
<bodyText confidence="0.998701">
While the extracted tuples of the form (n, p, n2)
and (v, p, n2) represent unambiguous noun and
verb attachments in which either the verb or
noun is known, our eventual goal is to resolve
ambiguous attachments in the test data of the
form (v, n,p, n2), in which both the noun n and
verb v are always known. We therefore must
use any information in the unambiguous cases
to resolve the ambiguous cases. A natural way is
to use a classifier that compares the probability
of each outcome:
</bodyText>
<equation confidence="0.8993024">
c/(v, n, p, n2) =
IN
1 arg maxae{N,v} Pr(v, n, p, a)
—
(1)
</equation>
<bodyText confidence="0.999938">
We do not currently use n2 in the probability
model, and we omit it from further discussion.
We can factor Pr(v,n,p, a) as follows:
</bodyText>
<equation confidence="0.994840666666667">
Pr(v, n,p, a) = Pr(v)Pr(n)
Pr(alv,n)
Pr(pla,v, n)
</equation>
<bodyText confidence="0.999984">
The terms Pr(n) and Pr(v) are independent of
the attachment a and need not be computed
in c/ (1), but the estimation of Pr(alv, n) and
Pr(pja,v,n) is problematic since our training
data, i.e., the head words extracted from raw
text, occur with either n or v, but never both
n, v. This leads to make some heuristically mo-
tivated approximations. Let the random vari-
able 0 range over {true, false), and let it de-
note the presence or absence of any preposition
that is unambiguously attached to the noun or
verb in question. Then p(cb = trueln) is the
conditional probability that a particular noun
n in free text has an unambiguous prepositional
phrase attachment. (0 = true will be written
simply as true.) We approximate Pr(alv, n) as
follows:
</bodyText>
<equation confidence="0.9997478">
Pr(trueln)
Z(v, n)
Pr(truelv)
Z(v, n)
Pr(trueln) + Pr(truelv)
</equation>
<bodyText confidence="0.997983875">
The rationale behind this approximation is that
the tendency of a v, n pair towards a noun
(verb) attachment is related to the tendency of
the noun (verb) alone to occur with an unam-
biguous prepositional phrase. The Z(v, n) term
exists only to make the approximation a well
formed probability over a E IN, 171.
We approximate Pr(pla,v,n) as follows:
</bodyText>
<equation confidence="0.937996">
Pr(pla = N, v, n)P...- Pr(pltrue,n)
Pr(p(a = V, v, n)P...- Pr(pltrue,v)
</equation>
<bodyText confidence="0.993043866666667">
The rationale behind these approximations is
that when generating p given a noun (verb) at-
tachment, only the counts involving the noun
(verb) are relevant, assuming also that the noun
(verb) has an attached prepositional phrase, i.e.,
0 = true.
We use word statistics from both the tagged
corpus and the set of extracted head word tuples
to estimate the probability of generating 0 =
true, p, and n2. Counts from the extracted set
of tuples assume that 0 = true, while counts
from the corpus itself may correspond to either
0 = true or 0 = false, depending on if the noun
if p = of
otherwise
</bodyText>
<equation confidence="0.999227">
Pr(a = Nlv,n)
Pr(a = Viv,n)
Z (v ,n) =
</equation>
<page confidence="0.912668">
1082
</page>
<bodyText confidence="0.8471565">
or verb in question is, or is not, respectively,
unambiguously attached to a preposition.
</bodyText>
<subsectionHeader confidence="0.836972">
4.1 Generate 0
</subsectionHeader>
<bodyText confidence="0.9999701">
The quantities Pr(trueln) and Pr(truelv) de-
note the conditional probability that n or v
will occur with some unambiguously attached
preposition, and are estimated as follows:
p, and define cN Ep cAp) as the number
of noun attached tuples. Analogously, define
cv(p) = E, c(v, p, true) and cv = Ep cv (P)•
The counts c(n,p,true) and c(v,p,true) are
from the extracted head word tuples. Using the
above notation, we can interpolate as follows:
</bodyText>
<equation confidence="0.932481166666667">
c(n,p,true) +
c(n,true) +1
c(v,p,true) + cv
19
c(v,true) +1
Pr(trueln) = c(n,true) c(n) &gt; 0 Pr (pItrue, n)
</equation>
<figure confidence="0.7951804">
Pr(truelv) = c(n) otherwise Pr (pltrue, v)
1. .5 c(v) &gt; 0
c(v,true) otherwise
c(v)
.5
</figure>
<bodyText confidence="0.998599333333333">
where c(n) and c(v) are counts from the tagged
corpus, and where c(n, true) and c(v, true) are
counts from the extracted head word tuples.
</bodyText>
<subsectionHeader confidence="0.971434">
4.2 Generate p
</subsectionHeader>
<bodyText confidence="0.999971857142857">
The terms Pr(pin, true) and Pr(plv, true) de-
note the conditional probability that a particu-
lar preposition p will occur as an unambiguous
attachment to n or v. We present two tech-
niques to estimate this probability, one based
on bigram counts and another based on an in-
terpolation method.
</bodyText>
<subsubsectionHeader confidence="0.60454">
4.2.1 Bigram Counts
</subsubsectionHeader>
<bodyText confidence="0.999980142857143">
This technique uses the bigram counts of the
extracted head word tuples, and backs off to
the uniform distribution when the denominator
is zero.
where 7) is the set of possible prepositions,
where all the counts c(...) are from the ex-
tracted head word tuples.
</bodyText>
<subsectionHeader confidence="0.569131">
4.2.2 Interpolation
</subsectionHeader>
<bodyText confidence="0.999873">
This technique is similar to the one in (Hindle
and Rooth, 1993), and interpolates between the
tendencies of the (v, p) and (n, p) bigrams and
the tendency of the type of attachment (e.g., N
or V) towards a particular preposition p. First,
define cN(p) = En c(n,p,true) as the number
of noun attached tuples with the preposition
</bodyText>
<sectionHeader confidence="0.982512" genericHeader="method">
5 Evaluation in English
</sectionHeader>
<bodyText confidence="0.999717970588235">
Approximately 970K unannotated sentences
from the 1988 Wall St. Journal were pro-
cessed in a manner identical to the example sen-
tence in Table 1. The result was approximately
910,000 head word tuples of the form (v ,p, n2)
or (n,p, n2). Note that while the head word
tuples represent correct attachments only 69%
of the time, their quantity is about 45 times
greater than the quantity of data used in previ-
ous supervised approaches. The extracted data
was used as training material for the three clas-
sifiers clbasel Chnterp, and cibigram• Each classi-
fier is constructed as follows:
Cl base This is the &amp;quot;baseline&amp;quot; classifier that pre-
dicts N of p = of, and V otherwise.
c/interp: This classifier has the form of equa-
tion (1), uses the method in section 4.1 to
generate 0, and the method in section 4.2.2
to generate p.
clbigram: This classifier has the form of equa-
tion (1), uses the method in section 4.1 to
generate 0, and the method in section 4.2.1
to generate p.
Table 4 shows accuracies of the classifiers on
the test set of (Ratnaparkhi et al., 1994), which
is derived from the manually annotated attach-
ments in the Penn Treebank Wall St. Journal
data. The Penn Treebank is drawn from the
1989 Wall St. Journal data, so there is no pos-
sibility of overlap with our training data. Fur-
thermore, the extraction heuristic was devel-
oped and tuned on a &amp;quot;development set&amp;quot;, i.e., a
set of annotated examples that did not overlap
with either the test set or the training set.
</bodyText>
<equation confidence="0.996884818181818">
Pr (pltrue, n)
Pr(pltrue,v) =
c(n,p,true) c(n, true) &gt; 0
c(n true)
otherwise
75f
c(v,p,true)
c(v,true)
otherwise
ITT
c(v,true) &gt; 0
</equation>
<page confidence="0.926989">
1083
</page>
<table confidence="0.9820738">
Subset Number of Events dbiqrarn Clinterp Ci base
p = of 925 917 917 917
p of 2172 1620 1618 1263
Total 3097 2537 2535 2180
Accuracy - 81.91% 81.85% 70.39%
</table>
<tableCaption confidence="0.996217">
Table 4: Accuracy of mostly unsupervised classifiers on English Wall St. Journal data
</tableCaption>
<table confidence="0.998508666666667">
Attachment Pr(alv,n) Pr(pla,v,n)
Noun(a = N) .02 .24
Verb(a = V) .30 .44
</table>
<tableCaption confidence="0.900333">
Table 5: The key probabilities for the ambigu-
ous example rise num to num
</tableCaption>
<bodyText confidence="0.97767175">
Table 5 shows the two probabilities Pr(alv, n)
and Pr (pla, v, n), using the same approxima-
tions as c/bigram, for the ambiguous example rise
num to num. (Recall that Pr(v) and Pr(n) are
not needed.) While the tuple (num, to, num) is
more frequent than (rise, to, num), the condi-
tional probabilities prefer a = V, which is the
choice that maximizes Pr (v , n, p, a).
Both classifiers c/interp and c/bigram clearly
outperform the. baseline, but the classifier
does not outperform dbigraml
even
</bodyText>
<subsubsectionHeader confidence="0.705071">
Clinterp
</subsubsectionHeader>
<bodyText confidence="0.999883217391304">
though it interpolates between the less specific
evidence (the preposition counts) and more spe-
cific evidence (the bigram counts). This may be
due to the errors in our extracted training data;
supervised classifiers that train from clean data
typically benefit greatly by combining less spe-
cific evidence with more specific evidence.
Despite the errors in the training data,
the performance of the unsupervised classifiers
(81.9%) begins to approach the best perfor-
mance of the comparable supervised classifiers
(84.5%). (Our goal is to replicate the super-
vision of a treebank, but not a semantic dictio-
nary, so we do not compare against (Stetina and
Nagao, 1997).) Furthermore, we do not use the
second noun n2, whereas the best supervised
methods use this information. Our result shows
that the information in imperfect but abundant
data from unambiguous attachments, as shown
in Tables 2 and 3, is sufficient to resolve ambigu-
ous prepositional phrase attachments at accu-
racies just under the supervised state-of-the-art
accuracy.
</bodyText>
<sectionHeader confidence="0.997261" genericHeader="evaluation">
6 Evaluation in Spanish
</sectionHeader>
<bodyText confidence="0.99997656">
We claim that our approach is portable to lan-
guages with similar word order, and we support
this claim by demonstrating our approach on
the Spanish language. We used the Spanish
tagger and morphological analyzer developed
at the Xerox Research Centre Europe4 and we
modified the extraction heuristic to account for
the new tagset, and to account for the Spanish
equivalents of the words of (i.e., de or del) and
to be (i.e., ser). Chunking was not performed
on the Spanish data. We used 450k sentences
of raw text from the Linguistic Data Consor-
tium&apos;s Spanish News Text Collection to extract
a training set, and we used a non-overlapping
set of 50k sentences from the collection to create
test sets. Three native Spanish speakers were
asked to extract and annotate ambiguous in-
stances of Spanish prepositional phrase attach-
ments. They annotated two sets (using the full
sentence context); one set consisted of all am-
biguous prepositional phrase attachments of the
form (v, n,p, n2), and the other set consisted of
cases where p = con. For testing our classifier,
we used only those judgments on which all three
annotators agreed.
</bodyText>
<subsectionHeader confidence="0.971521">
6.1 Performance
</subsectionHeader>
<bodyText confidence="0.99935375">
The performance of the classifiers Clbigram,
dinterp) and c/base, when trained and tested
on Spanish language data, are shown in Ta-
ble 6. The Spanish test set has fewer ambiguous
prepositions than the English test set, as shown
by the accuracy of Ciba„. However, the accuracy
improvements of Clbygrara over Cl ba„ are statisti-
cally significant for both test sets.5
</bodyText>
<footnote confidence="0.9957635">
4These were supplied by Dr. Lauri Kartunnen during
his visit to Penn.
5Using proportions of changed cases, P = 0.0258 for
the first set, and P = 0.0108 for the set where p = con
</footnote>
<page confidence="0.945014">
1084
</page>
<table confidence="0.999623714285714">
Test Set Subset Number of Events dbigram dinterp clbase
All p p = de del 156 154 154 154
p 0 de del 116 103 97 91
Total 272 257 251 245
Accuracy - 94.5% 92.3% 90.1%
p = con Total 192 166 160 151
Accuracy - 86.4% 83.3% 78.6%
</table>
<tableCaption confidence="0.998842">
Table 6: Accuracy of mostly unsupervised classifiers on Spanish News Data
</tableCaption>
<sectionHeader confidence="0.994483" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999983705882353">
The unsupervised algorithm for prepositional
phrase attachment presented here is the only
algorithm in the published literature that can
significantly outperform the baseline without
using data derived from a treebank or parser.
The accuracy of our technique approaches the
accuracy of the best supervised methods, and
does so with only a tiny fraction of the supervi-
sion. Since only a small part of the extraction
heuristic is specific to English, and since part-
of-speech taggers and morphology databases are
widely available in other languages, our ap-
proach is far more portable than previous ap-
proaches for this problem. We successfully
demonstrated the portability of our approach
by applying it to the prepositional phrase at-
tachment task in the Spanish language.
</bodyText>
<sectionHeader confidence="0.998854" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999878666666667">
We thank Dr. Lauri Kartunnen for lending us
the Spanish natural language tools, and Mike
Collins for helpful discussions on this work.
</bodyText>
<sectionHeader confidence="0.999236" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805163636364">
ACL. 1997. Proceedings of the 35th Annual
Meeting of the ACL, and 8th Conference of
the EACL, Madrid, Spain, July.
Eric Brill and Phil Resnik. 1994. A Rule Based
Approach to Prepositional Phrase Attach-
ment Disambiguation. In Proceedings of the
Fifteenth International Conference on Com-
putational Linguistics (COLING).
Michael Collins and James Brooks. 1995.
Prepositional Phrase Attachment through a
Backed-off Model. In David Yarowsky and
Kenneth Church, editors, Proceedings of the
Third Workshop on Very Large Corpora,
pages 27-38, Cambridge, Massachusetts,
June.
Alexander Franz. 1997. Independence Assump-
tions Considered Harmful. In ACL (ACL,
1997).
Donald Hindle and Mats Rooth. 1993. Struc-
tural Ambiguity and Lexical Relations. Com-
putational Linguistics, 19(1):103-120.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1994. Building
a large annotated corpus of English: the
Penn Treebank. Computational Linguistics,
19(2):313-330.
Paola Merlo, Matthew W. Crocker, and
Cathy Berthouzoz. 1997. Attaching Multiple
Prepositional Phrases: Generalized Backed-
off Estimation. In Claire Cardie and Ralph
Weischedel, editors, Second Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 149-155, Providence, R.I.,
Aug. 1-2.
Adwait Ratnaparkhi, Jeff Reynar, and Salim
Roukos. 1994. A Maximum Entropy Model
for Prepositional Phrase Attachment. In Pro-
ceedings of the Human Language Technology
Workshop, pages 250-255, Plainsboro, N.J.
ARPA.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Part of Speech Tagger. In Eric Brill
and Kenneth Church, editors, Conference on
Empirical Methods in Natural Language Pro-
cessing, University of Pennsylvania, May 17-
18.
Jiri Stetina and Makoto Nagao. 1997. Corpus
Based PP Attachment Ambiguity Resolution
with a Semantic Dictionary. In Jou Zhou and
Kenneth Church, editors, Proceedings of the
Fifth Workshop on Very Large Corpora, pages
66-80, Beijing and Hong Kong, Aug. 18 - 20.
Jakub Zavrel and Walter Daelemans. 1997.
Memory-Based Learning: Using Similarity
for Smoothing. In ACL (ACL, 1997).
</reference>
<page confidence="0.995233">
1085
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526518">
<title confidence="0.9800525">Statistical Models for Unsupervised Prepositional Phrase Attachment</title>
<author confidence="0.986859">Adwait Ratnaparkhi</author>
<affiliation confidence="0.999629">Dept. of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.9980775">200 South 33rd Street Philadelphia, PA 19104-6389</address>
<email confidence="0.999567">adwaitOunagi.cis.upenn.edu</email>
<abstract confidence="0.970040133333333">several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task. We present results for prepositional phrase attachment in both English and Spanish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACL</author>
</authors>
<date>1997</date>
<booktitle>Proceedings of the 35th Annual Meeting of the ACL, and 8th Conference of the EACL,</booktitle>
<location>Madrid, Spain,</location>
<marker>ACL, 1997</marker>
<rawString>ACL. 1997. Proceedings of the 35th Annual Meeting of the ACL, and 8th Conference of the EACL, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Phil Resnik</author>
</authors>
<title>A Rule Based Approach to Prepositional Phrase Attachment Disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="2865" citStr="Brill and Resnik, 1994" startWordPosition="459" endWordPosition="462">uples corresponding to the example sentences are 1. bought shirt with pockets 2. washed shirt with soap The correct classifications of tuples 1 and 2 are N and V, respectively. (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995). Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary. 1079 While previous corpus-based methods are highly accurate for this task, they are difficult to port to ot</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Eric Brill and Phil Resnik. 1994. A Rule Based Approach to Prepositional Phrase Attachment Disambiguation. In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional Phrase Attachment through a Backed-off Model.</title>
<date>1995</date>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<editor>In David Yarowsky and Kenneth Church, editors,</editor>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="2891" citStr="Collins and Brooks, 1995" startWordPosition="463" endWordPosition="466">he example sentences are 1. bought shirt with pockets 2. washed shirt with soap The correct classifications of tuples 1 and 2 are N and V, respectively. (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995). Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary. 1079 While previous corpus-based methods are highly accurate for this task, they are difficult to port to other languages because they</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional Phrase Attachment through a Backed-off Model. In David Yarowsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 27-38, Cambridge, Massachusetts, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Franz</author>
</authors>
<title>Independence Assumptions Considered Harmful.</title>
<date>1997</date>
<booktitle>In ACL (ACL,</booktitle>
<contexts>
<context position="2953" citStr="Franz, 1997" startWordPosition="475" endWordPosition="476"> soap The correct classifications of tuples 1 and 2 are N and V, respectively. (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995). Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary. 1079 While previous corpus-based methods are highly accurate for this task, they are difficult to port to other languages because they require resources that are expensive to construct or simply n</context>
</contexts>
<marker>Franz, 1997</marker>
<rawString>Alexander Franz. 1997. Independence Assumptions Considered Harmful. In ACL (ACL, 1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<date>1993</date>
<booktitle>Structural Ambiguity and Lexical Relations. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="2444" citStr="Hindle and Rooth, 1993" startWordPosition="383" endWordPosition="386">stical or corpusbased, and they consider only prepositions whose attachment is ambiguous between a preceding noun phrase and verb phrase. Previous work has framed the problem as a classification task, in which the goal is to predict N or V, corresponding to noun or verb attachment, given the head verb v, the head noun n, the preposition p, and optionally, the object of the preposition n2. For example, the (v,n,p, n2) tuples corresponding to the example sentences are 1. bought shirt with pockets 2. washed shirt with soap The correct classifications of tuples 1 and 2 are N and V, respectively. (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treeba</context>
<context position="6331" citStr="Hindle and Rooth, 1993" startWordPosition="1036" endWordPosition="1039">th of the potential sites, namely the verb v and the noun n are known before the attachment is resolved. In the unsupervised case discussed here, the extraction heuristic only finds what it thinks are unambiguous cases of prepositional phrase attachment. Therefore, there is only one possible attachment site for the preposition, and either the verb v or the noun n does not exist, in the case of noun-attached preposition or a verb-attached preposition, respectively. This extraction heuristic loosely resembles a step in the bootstrapping procedure used to get training data for the classifier of (Hindle and Rooth, 1993). In that step, unambiguous attachments from the FIDDITCH parser&apos;s output are initially used to resolve some of the ambiguous attachments, and the resolved cases are iteratively used to disambiguate the remaining unresolved cases. Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases. It is the hypothesis of this approach that the information in just the unambiguous attachment events can resolve the ambiguous attachment events of the test data. 3.1.1 H</context>
<context position="15213" citStr="Hindle and Rooth, 1993" startWordPosition="2615" endWordPosition="2618">(plv, true) denote the conditional probability that a particular preposition p will occur as an unambiguous attachment to n or v. We present two techniques to estimate this probability, one based on bigram counts and another based on an interpolation method. 4.2.1 Bigram Counts This technique uses the bigram counts of the extracted head word tuples, and backs off to the uniform distribution when the denominator is zero. where 7) is the set of possible prepositions, where all the counts c(...) are from the extracted head word tuples. 4.2.2 Interpolation This technique is similar to the one in (Hindle and Rooth, 1993), and interpolates between the tendencies of the (v, p) and (n, p) bigrams and the tendency of the type of attachment (e.g., N or V) towards a particular preposition p. First, define cN(p) = En c(n,p,true) as the number of noun attached tuples with the preposition 5 Evaluation in English Approximately 970K unannotated sentences from the 1988 Wall St. Journal were processed in a manner identical to the example sentence in Table 1. The result was approximately 910,000 head word tuples of the form (v ,p, n2) or (n,p, n2). Note that while the head word tuples represent correct attachments only 69%</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural Ambiguity and Lexical Relations. Computational Linguistics, 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="3067" citStr="Marcus et al., 1994" startWordPosition="493" endWordPosition="496">scribes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995). Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary. 1079 While previous corpus-based methods are highly accurate for this task, they are difficult to port to other languages because they require resources that are expensive to construct or simply nonexistent in other languages. We present an unsupervised algorithm for prepositional phrase attachment in English</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Matthew W Crocker</author>
<author>Cathy Berthouzoz</author>
</authors>
<title>Attaching Multiple Prepositional Phrases: Generalized Backedoff Estimation.</title>
<date>1997</date>
<booktitle>In Claire Cardie and Ralph Weischedel, editors, Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>149--155</pages>
<location>Providence, R.I.,</location>
<contexts>
<context position="2911" citStr="Merlo et al., 1997" startWordPosition="467" endWordPosition="470">. bought shirt with pockets 2. washed shirt with soap The correct classifications of tuples 1 and 2 are N and V, respectively. (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995). Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary. 1079 While previous corpus-based methods are highly accurate for this task, they are difficult to port to other languages because they require resources t</context>
</contexts>
<marker>Merlo, Crocker, Berthouzoz, 1997</marker>
<rawString>Paola Merlo, Matthew W. Crocker, and Cathy Berthouzoz. 1997. Attaching Multiple Prepositional Phrases: Generalized Backedoff Estimation. In Claire Cardie and Ralph Weischedel, editors, Second Conference on Empirical Methods in Natural Language Processing, pages 149-155, Providence, R.I., Aug. 1-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeff Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A Maximum Entropy Model for Prepositional Phrase Attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>250--255</pages>
<publisher>ARPA.</publisher>
<location>Plainsboro, N.J.</location>
<contexts>
<context position="2841" citStr="Ratnaparkhi et al., 1994" startWordPosition="455" endWordPosition="458">example, the (v,n,p, n2) tuples corresponding to the example sentences are 1. bought shirt with pockets 2. washed shirt with soap The correct classifications of tuples 1 and 2 are N and V, respectively. (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995). Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary. 1079 While previous corpus-based methods are highly accurate for this task, they are</context>
<context position="16556" citStr="Ratnaparkhi et al., 1994" startWordPosition="2854" endWordPosition="2857">s. The extracted data was used as training material for the three classifiers clbasel Chnterp, and cibigram• Each classifier is constructed as follows: Cl base This is the &amp;quot;baseline&amp;quot; classifier that predicts N of p = of, and V otherwise. c/interp: This classifier has the form of equation (1), uses the method in section 4.1 to generate 0, and the method in section 4.2.2 to generate p. clbigram: This classifier has the form of equation (1), uses the method in section 4.1 to generate 0, and the method in section 4.2.1 to generate p. Table 4 shows accuracies of the classifiers on the test set of (Ratnaparkhi et al., 1994), which is derived from the manually annotated attachments in the Penn Treebank Wall St. Journal data. The Penn Treebank is drawn from the 1989 Wall St. Journal data, so there is no possibility of overlap with our training data. Furthermore, the extraction heuristic was developed and tuned on a &amp;quot;development set&amp;quot;, i.e., a set of annotated examples that did not overlap with either the test set or the training set. Pr (pltrue, n) Pr(pltrue,v) = c(n,p,true) c(n, true) &gt; 0 c(n true) otherwise 75f c(v,p,true) c(v,true) otherwise ITT c(v,true) &gt; 0 1083 Subset Number of Events dbiqrarn Clinterp Ci bas</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A Maximum Entropy Model for Prepositional Phrase Attachment. In Proceedings of the Human Language Technology Workshop, pages 250-255, Plainsboro, N.J. ARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part of Speech Tagger.</title>
<date>1996</date>
<booktitle>In Eric Brill and Kenneth Church, editors, Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>17--18</pages>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="4854" citStr="Ratnaparkhi, 1996" startWordPosition="787" endWordPosition="788">tial attachment sites, but we limit our task to choosing between a verb v and a noun n so that we may compare with previous supervised attempts on this problem. While we will be given the candidate attachment sites during testing, the training procedure assumes no a priori information about potential attachment sites. 3.1 Generating Training Data From Raw Text We generate training data from raw text by using a part-of-speech tagger, a simple chunker, an extraction heuristic, and a morphology database. The order in which these tools are applied to raw text is shown in Table 1. The tagger from (Ratnaparkhi, 1996) first annotates sentences of raw text with a sequence of partof-speech tags. The chunker, implemented with two small regular expressions, then replaces simple noun phrases and quantifier phrases with their head words. The extraction heuristic then finds head word tuples and their likely attachments from the tagged and chunked text. The heuristic relies on the observed fact that in English and in languages with similar word order, the attachment site of a preposition is usually located only a few words to the left of the preposition. Finally, numbers are replaced by a single token, the text is</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part of Speech Tagger. In Eric Brill and Kenneth Church, editors, Conference on Empirical Methods in Natural Language Processing, University of Pennsylvania, May 17-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
<author>Makoto Nagao</author>
</authors>
<title>Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Workshop on Very Large Corpora,</booktitle>
<volume>18</volume>
<pages>66--80</pages>
<editor>In Jou Zhou and Kenneth Church, editors,</editor>
<location>Beijing</location>
<contexts>
<context position="18660" citStr="Stetina and Nagao, 1997" startWordPosition="3213" endWordPosition="3216"> specific evidence (the preposition counts) and more specific evidence (the bigram counts). This may be due to the errors in our extracted training data; supervised classifiers that train from clean data typically benefit greatly by combining less specific evidence with more specific evidence. Despite the errors in the training data, the performance of the unsupervised classifiers (81.9%) begins to approach the best performance of the comparable supervised classifiers (84.5%). (Our goal is to replicate the supervision of a treebank, but not a semantic dictionary, so we do not compare against (Stetina and Nagao, 1997).) Furthermore, we do not use the second noun n2, whereas the best supervised methods use this information. Our result shows that the information in imperfect but abundant data from unambiguous attachments, as shown in Tables 2 and 3, is sufficient to resolve ambiguous prepositional phrase attachments at accuracies just under the supervised state-of-the-art accuracy. 6 Evaluation in Spanish We claim that our approach is portable to languages with similar word order, and we support this claim by demonstrating our approach on the Spanish language. We used the Spanish tagger and morphological ana</context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Jiri Stetina and Makoto Nagao. 1997. Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary. In Jou Zhou and Kenneth Church, editors, Proceedings of the Fifth Workshop on Very Large Corpora, pages 66-80, Beijing and Hong Kong, Aug. 18 - 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-Based Learning: Using Similarity for Smoothing.</title>
<date>1997</date>
<booktitle>In ACL (ACL,</booktitle>
<contexts>
<context position="2939" citStr="Zavrel and Daelemans, 1997" startWordPosition="471" endWordPosition="474">pockets 2. washed shirt with soap The correct classifications of tuples 1 and 2 are N and V, respectively. (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy. Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995). Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary. 1079 While previous corpus-based methods are highly accurate for this task, they are difficult to port to other languages because they require resources that are expensive to constru</context>
</contexts>
<marker>Zavrel, Daelemans, 1997</marker>
<rawString>Jakub Zavrel and Walter Daelemans. 1997. Memory-Based Learning: Using Similarity for Smoothing. In ACL (ACL, 1997).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>