<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000859">
<title confidence="0.963906">
EM Decipherment for Large Vocabularies
</title>
<author confidence="0.915338">
Malte Nuhn and Hermann Ney
</author>
<affiliation confidence="0.9027645">
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
</affiliation>
<email confidence="0.994162">
&lt;surname&gt;@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.993775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999626956521739">
This paper addresses the problem of EM-
based decipherment for large vocabular-
ies. Here, decipherment is essentially
a tagging problem: Every cipher token
is tagged with some plaintext type. As
with other tagging problems, this one can
be treated as a Hidden Markov Model
(HMM), only here, the vocabularies are
large, so the usual O(NV 2) exact EM ap-
proach is infeasible. When faced with
this situation, many people turn to sam-
pling. However, we propose to use a type
of approximate EM and show that it works
well. The basic idea is to collect fractional
counts only over a small subset of links
in the forward-backward lattice. The sub-
set is different for each iteration of EM.
One option is to use beam search to do the
subsetting. The second method restricts
the successor words that are looked at, for
each hypothesis. It does this by consulting
pre-computed tables of likely n-grams and
likely substitutions.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999886956521739">
The decipherment of probabilistic substitution ci-
phers (ciphers in which each plaintext token can
be substituted by any cipher token, following a
distribution p(f|e), cf. Table 2) can be seen as
an important step towards decipherment for MT.
This problem has not been studied explicitly be-
fore. Scaling to larger vocabularies for proba-
bilistic substitution ciphers decipherment is a dif-
ficult problem: The algorithms for 1:1 or homo-
phonic substitution ciphers are not applicable, and
standard algorithms like EM training become in-
tractable when vocabulary sizes go beyond a few
hundred words. In this paper we present an effi-
cient EM based training procedure for probabilis-
tic substitution ciphers which provides high deci-
pherment accuracies while having low computa-
tional requirements. The proposed approach al-
lows using high order n-gram language models,
and is scalable to large vocabulary sizes. We show
improvements in decipherment accuracy in a va-
riety of experiments (including MT) while being
computationally more efficient than previous pub-
lished work on EM-based decipherment.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.98432875">
Several methods exist for deciphering 1:1 substi-
tution ciphers: Ravi and Knight (2008) solve 1:1
substitution ciphers by formulating the decipher-
ment problem as an integer linear program. Cor-
lett and Penn (2010) solve the same problem us-
ing A∗ search. Nuhn et al. (2013) present a beam
search approach that scales to large vocabulary
and high order language models. Even though be-
ing successful, these algorithms are not applicable
to probabilistic substitution ciphers, or any of its
extensions as they occur in decipherment for ma-
chine translation.
EM training for probabilistic ciphers was first
covered in Ravi and Knight (2011). Nuhn et al.
(2012) have given an approximation to exact EM
training using context vectors, allowing to train-
ing models even for larger vocabulary sizes. Ravi
(2013) report results on the OPUS subtitle corpus
using an elaborate hash sampling technique, based
on n-gram language models and context vectors,
that is computationally very efficient.
Conventional beam search is a well studied
topic: Huang et al. (1992) present beam search for
automatic speech recognition, using fine-grained
pruning procedures. Similarly, Young and Young
(1994) present an HMM toolkit, including pruned
forward-backward EM training. Pal et al. (2006)
use beam search for training of CRFs.
</bodyText>
<page confidence="0.979872">
759
</page>
<note confidence="0.5105055">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759–764,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.8398644">
Method Publications Complexity
EM Full (Knight et al., 2006), (Ravi and Knight, 2011) O(NV n)
EM Fixed Candidates (Nuhn et al., 2012) O(N)
EM Beam This Work O(NV )
EM Lookahead This Work O(N)
</table>
<tableCaption confidence="0.8818925">
Table 1: Different approximations to exact EM training for decipherment. N is the cipher sequence
length, V the size of the target vocabulary, and n the order of the language model.
</tableCaption>
<bodyText confidence="0.99784825">
The main contribution of this work is the pre-
selection beam search that—to the best of our
knowledge—was not known in literature before,
and serves as an important step to applying EM
training to the large vocabulary decipherment
problem. Table 1 gives an overview of the EM
based methods. More details are given in Sec-
tion 3.2.
</bodyText>
<sectionHeader confidence="0.992766" genericHeader="method">
3 Probabilistic Substitution Ciphers
</sectionHeader>
<bodyText confidence="0.839479333333333">
We define probabilistic substitutions ciphers us-
ing the following generative story for ciphertext
sequences fN1 :
</bodyText>
<listItem confidence="0.941066">
1. Stochastically generate a plaintext sequence
eN1 according to a bigram1 language model.
2. For each plaintext token en choose a substi-
tution fn with probability P(fn|en, ϑ).
</listItem>
<bodyText confidence="0.596756">
This generative story corresponds to the model
</bodyText>
<equation confidence="0.9963026">
p(eN 1 , fN 1 , ϑ) = p(eN 1 ) � p(fN 1 |eN 1 , ϑ) , (1)
with the zero-order membership model
N
p(fN1 |eN1 , ϑ) = H plex(fn|en, ϑ) (2)
n=1
</equation>
<bodyText confidence="0.91423">
with parameters p(f|e, ϑ) ~ ϑf|e and normaliza-
tion constraints Ve E f ϑf|e = 1, and first-order
plaintext sequence model
</bodyText>
<equation confidence="0.997148333333333">
N
P(eN1 ) = H pLM(en|en−1) . (3)
n=1
</equation>
<bodyText confidence="0.9542768">
Thus, the probabilistic substitution cipher can be
seen as a Hidden Markov Model. Table 2 gives an
overview over the model. We want to find those
parameters ϑ that maximize the marginal distribu-
tion p(fN1 |ϑ):
</bodyText>
<equation confidence="0.574904">
ϑ = arg max
V&apos;
1This can be generalized to n-gram language models.
</equation>
<bodyText confidence="0.998335">
After we obtained the parameters ϑ we
can obtain ei as the Viterbi decoding
arg maxe1N{p(e1 f 1N, ϑ) }
</bodyText>
<subsectionHeader confidence="0.989802">
3.1 Exact EM training
</subsectionHeader>
<bodyText confidence="0.999502777777778">
In the decipherment setting, we are given the ob-
served ciphertext fN1 and the model p(fN1 |eN1 , ϑ)
that explains how the observed ciphertext has been
generated given a latent plaintext eN1 . Marginaliz-
ing the unknown eN1 , we would like to obtain the
maximum likelihood estimate of ϑ as specified in
Equation 4. We iteratively compute the maximum
likelihood estimate by applying the EM algorithm
(Dempster et al., 1977):
</bodyText>
<equation confidence="0.995606571428571">
E pn(e|fN1 ,ϑ)
n:fn=f
pn(e|fN 1 , ϑ) (5)
with
pn(e|fN 1 ,ϑ) = � p(eN1 |fN1)
ϑ(6)
[eN1 :en=e]
</equation>
<bodyText confidence="0.9999404">
being the posterior probability of observing the
plaintext symbol e at position n given the cipher-
text sequence fN1 and the current parameters ϑ.
pn(e|fN1 , ϑ) can be efficiently computed using the
forward-backward algorithm.
</bodyText>
<subsectionHeader confidence="0.999943">
3.2 Approximations to EM-Training
</subsectionHeader>
<bodyText confidence="0.999844818181818">
The computational complexity of EM training
stems from the sum E[eN1 :en=e] contained in the
posterior pn(e|fN1 , ϑ). However, we can approx-
imate this sum (and hope that the EM training
procedure is still working) by only evaluating the
dominating terms, i.e. we only evaluate the sum
for sequences eN1 that have the largest contribu-
tions to E[eN1 :en=e]. Note that due to this approxi-
mation, the new parameter estimates in Equation 5
can become zero. This is a critical issue, since
pairs (e, f) with p(f|e) = 0 cannot recover from
</bodyText>
<equation confidence="0.986819571428571">
p(fN1 , eN1 |ϑI)
{ E
[eN1 ]
} (4)
˜ϑf|e =
E E
f n:fn=f
</equation>
<page confidence="0.977988">
760
</page>
<table confidence="0.9886175">
Sequence of cipher tokens : fN = f1,..., fN
Sequence of plaintext tokens : 1 = e1, ... , eN
eN
1
Joint probability : p(fN1 , eN1 |ϑ) = p(eN1 ) · p(fN1 |eN1 , ϑ)
N
Language model : p(eN1 ) = H pLM(en|en−1)
n=1
N
Membership probabilities : p(fN1 |eN1 , ϑ) = H plex(fn|en, ϑ)
n=1
Paramater Set : ϑ = {ϑf|e}, p(f|e, ϑ) = ϑf|e
Normalization : ∀e : E ϑf|e = 1
f
Probability of cipher sequence : p(fN 1 |ϑ) = E p(fN1 , eN1 |ϑ)
[eN1 ]
</table>
<tableCaption confidence="0.991426">
Table 2: Definition of the probabilistic substitution cipher model. In contrast to simple or homophonic
</tableCaption>
<bodyText confidence="0.977627142857143">
substitution ciphers, each plaintext token can be substituted by multiple cipher text tokens. The parameter
ϑf|e represents the probability of substituting token e with token f.
acquiring zero probability in some early iteration.
In order to allow the lexicon to recover from these
zeros, we use a smoothed lexicon ˆplex(f|e) =
λplex(f|e) + (1 − λ)/|Vf |with λ = 0.9 when
conducting the E-Step.
</bodyText>
<subsectionHeader confidence="0.866123">
3.2.1 Beam Search
</subsectionHeader>
<bodyText confidence="0.99994447826087">
Instead of evaluating the sum for terms with the
exact largest contributions, we restrict ourselves to
terms that are likely to have a large contribution to
the sum, dropping any guarantees about the actual
contribution of these terms.
Beam search is a well known algorithm related
to this idea: We build up sequences ec1 with grow-
ing cardinality c. For each cardinality, only a set
of the B most promising hypotheses is kept. Then
for each active hypothesis of cardinality c, all pos-
sible extensions with substitutions fc+1 → ec+1
are explored. Then in turn only the best B out of
the resulting B · Ve many hypotheses are kept and
the algorithm continues with the next cardinality.
Reaching the full cardinality N, the algorithm ex-
plored B · N · Ve many hypotheses, resulting in a
complexity of O(BNVe).
Even though EM training using beam search
works well, it still suffers from exploring all Ve
possible extensions for each active hypothesis, and
thus scaling linearly with the vocabulary size. Due
to that, standard beam search EM training is too
slow to be used in the decipherment setting.
</bodyText>
<subsectionHeader confidence="0.575514">
3.2.2 Preselection Search
</subsectionHeader>
<bodyText confidence="0.999359277777778">
Instead of evaluating all substitutions fc+1 →
ec+1 ∈ Ve, this algorithm only expands a fixed
number of candidates: For a hypothesis ending in
a language model state σ, we only look at BLM
many successor words ec+1 with the highest LM
probability pLM(ec+1|σ) and at Blex many suc-
cessor words ec+1 with the highest lexical prob-
ability plex(fc+1|ec+1). Altogether, for each hy-
pothesis we only look at (BLM + Blex) many suc-
cessor states. Then, just like in the standard beam
search approach, we prune all explored new hy-
potheses and continue with the pruned set of B
many hypotheses. Thus, for a cipher of length N
we only explore N · B · (BLM + Blex) many hy-
potheses.2
Intuitively speaking, our approach solves the
EM training problem for decipherment using large
vocabularies by focusing only on those substitu-
tions that either seem likely due to the language
model (”What word is likely to follow the cur-
rent partial decipherment?”) or due to the lexicon
model (”Based on my knowledge about the cur-
rent cipher token, what is the most likely substitu-
tion?”).
In order to efficiently find the maximizing e for
pLM(e|σ) and plex(f|e), we build a lookup ta-
ble that contains for each language model state σ
the BLM best successor words e, and a separate
lookup table that contains for each source word f
the Blex highest scoring tokens e. The language
model lookup table remains constant during all it-
erations, while the lexicon lookup table needs to
be updated between each iteration.
Note that the size of the LM lookup table scales
linearly with the number of language model states.
Thus the memory requirements for the lookup ta-
</bodyText>
<footnote confidence="0.929168">
2We always use B = 100, Blex = 5, and BLM = 50.
</footnote>
<page confidence="0.994112">
761
</page>
<figureCaption confidence="0.966569666666667">
Figure 1: Illustration of the search space explored by full search, beam search, and preselection search.
Full search keeps all possible hypotheses at cardinality c and explores all possible substitutions at (c+1).
Beam search only keeps the B most promising hypotheses and then selects the best new hypotheses for
cardinality (c + 1) from all possible substitutions. Preselection search keeps only the B best hypotheses
for every cardinality c and only looks at the (BLex + BLM) most promising substitutions for cardinality
(c + 1) based on the current lexicon (BLex dashed lines) and language model (BLM solid lines).
</figureCaption>
<figure confidence="0.971767416666667">
Full Search
Beam Search Preselection Search
Vocab
f1 f2 f3 f4 f5
e5
e4
start
Sentence
f6
e3
e2
e1
</figure>
<table confidence="0.9673142">
Name Lang. Sent. Words Voc.
VERBMOBIL English 27,862 294,902 3,723
Spanish 13,181 39,185 562
OPUS
English 19,770 61,835 411
</table>
<tableCaption confidence="0.993675">
Table 3: Statistics of the copora used in this pa-
</tableCaption>
<bodyText confidence="0.973971857142857">
per: The VERBMOBIL corpus is used to conduct
experiments on simple substitution ciphers, while
the OPUS corpus is used in our Machine Transla-
tion experiments.
ble do not form a practical problem of our ap-
proach. Figure 1 illustrates full search, beam
search, and our proposed method.
</bodyText>
<sectionHeader confidence="0.998156" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999456285714286">
We first show experiments for data in which the
underlying model is an actual 1:1 substitution ci-
pher. In this case, we report the word accuracy
of the final decipherment. We then show experi-
ments for a simple machine translation task. Here
we report translation quality in BLEU. The cor-
pora used in this paper are shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.995061">
4.1 Simple Substitution Ciphers
</subsectionHeader>
<bodyText confidence="0.999956571428571">
In this set of experiments, we compare the exact
EM training to the approximations presented in
this paper. We use the English side of the German-
English VERBMOBIL corpus (Wahlster, 2000) to
construct a word substitution cipher, by substitut-
ing every word type with a unique number. In or-
der to have a non-parallel setup, we train language
</bodyText>
<table confidence="0.999217">
Vocab LM Method Acc.[%] Time[h]
200 2 exact 97.19 224.88
beam 98.87 9.04
presel. 98.50 4.14
500 2 beam 92.12 24.27
presel. 92.16 4.70
3 661 3 beam 91.16 302.81
presel. 90.92 19.68
4 presel. 92.14 23.72
</table>
<tableCaption confidence="0.8490975">
Table 4: Results for simple substitution ciphers
based on the VERBMOBIL corpus using exact,
beam, and preselection EM. Exact EM is not
tractable for vocabulary sizes above 200.
</tableCaption>
<bodyText confidence="0.999282">
models of order 2, 3 and 4 on the first half of the
corpus and use the second half as ciphertext. Ta-
ble 4 shows the results of our experiments.
Since exact EM is not tractable for vocabulary
sizes beyond 200 words, we train word classes on
the whole corpus and map the words to classes
(consistent along the first and second half of the
corpus). By doing this, we create new simple sub-
stitution ciphers with smaller vocabularies of size
200 and 500. For the smallest setup, we can di-
rectly compare all three EM variants. We also in-
clude experiments on the original corpus with vo-
cabulary size of 3661. When comparing exact EM
training with beam- and preselection EM training,
the first thing we notice is that it takes about 20
times longer to run the exact EM training than
training with beam EM, and about 50 times longer
than the preselection EM training. Interestingly,
</bodyText>
<page confidence="0.989179">
762
</page>
<table confidence="0.9996112">
Model Method BLEU [%] Runtime
2-gram Exact EM(Ravi and Knight, 2011) 15.3 850.0h
whole segment lm Exact EM(Ravi and Knight, 2011) 19.3 850.0h
2-gram Preselection EM (This work) 15.7 1.8h
3-gram Preselection EM (This work) 19.5 1.9h
</table>
<tableCaption confidence="0.928208">
Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on
the Spanish/English OPUS corpus using only non-parallel corpora for training.
</tableCaption>
<bodyText confidence="0.999916225806452">
the accuracy of the approximations to exact EM
training is better than that of the exact EM train-
ing. Even though this needs further investigation,
it is clear that the pruned versions of EM training
find sparser distributions plex(f|e): This is desir-
able in this set of experiments, and could be the
reason for improved performance.
For larger vocabularies, exact EM training is not
tractable anymore. We thus constrain ourselves to
running experiments with beam and preselection
EM training only. Here we can see that the runtime
of the preselection search is roughly the same as
when running on a smaller vocabulary, while the
beam search runtime scales almost linearly with
the vocabulary size. For the full vocabulary of
3661 words, preselection EM using a 4-gram LM
needs less than 7% of the time of beam EM with a
3-gram LM and performs by 1% better in symbol
accuracy.
To summarize: Beam search EM is an or-
der of magnitude faster than exact EM training
while even increasing decipherment accuracy. Our
new preselection search method is in turn or-
ders of magnitudes faster than beam search EM
while even being able to outperform exact EM and
beam EM by using higher order language mod-
els. We were thus able to scale the EM deci-
pherment to larger vocabularies of several thou-
sand words. The runtime behavior is also consis-
tent with the computational complexity discussed
in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.931383">
4.2 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999978081081081">
We show that our algorithm is directly applicable
to the decipherment problem for machine transla-
tion. We use the same simplified translation model
as presented by Ravi and Knight (2011). Because
this translation model allows insertions and dele-
tions, hypotheses of different cardinalities coex-
ist during search. We extend our search approach
such that pruning is done for each cardinality sep-
arately. Other than that, we use the same pres-
election search procedure as used for the simple
substitution cipher task.
We run experiments on the opus corpus as pre-
sented in (Tiedemann, 2009). Table 5 shows pre-
viously published results using EM together with
the results of our new method:
(Ravi and Knight, 2011) is the only publication
that reports results using exact EM training and
only n-gram language models on the target side:
It has an estimated runtime of 850h. All other
published results (using EM training and Bayesian
inference) use context vectors as an additional
source of information: This might be an explana-
tion why Nuhn et al. (2012) and Ravi (2013) are
able to outperform exact EM training as reported
by Ravi and Knight (2011). (Ravi, 2013) reports
the most efficient method so far: It only consumes
about 3h of computation time. However, as men-
tioned before, those results are not directly compa-
rable to our work, since they use additional context
information on the target side.
Our algorithm clearly outperforms the exact
EM training in run time, and even slighlty im-
proves performance in BLEU. Similar to the sim-
ple substitution case, the improved performance
might be caused by inferring a sparser distribution
plex(f|e). However, this requires further investi-
gation.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9993017">
We have shown a conceptually consistent and easy
to implement EM based training method for deci-
pherment that outperforms exact and beam search
EM training for simple substitution ciphers and
decipherment for machine translation, while re-
ducing training time to a fraction of exact and
beam EM. We also point out that the preselection
method presented in this paper is not restricted to
word based translation models and can also be ap-
plied to phrase based translation models.
</bodyText>
<page confidence="0.997543">
763
</page>
<sectionHeader confidence="0.989671" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735714285714">
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040–1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, B, 39.
Xuedong Huang, Fileno Alleva, Hsiao wuen Hon, Mei
yuh Hwang, and Ronald Rosenfeld. 1992. The
sphinx-ii speech recognition system: An overview.
Computer, Speech and Language, 7:137–148.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised Analysis for De-
cipherment Problems. In Proceedings of the
COLING/ACL on Main conference poster sessions,
COLING-ACL ’06, pages 499–506, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156–164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Annual Meeting of the Assoc. for Computational
Linguistics, pages 1569–1576, Sofia, Bulgaria, Au-
gust.
Chris Pal, Charles Sutton, and Andrew McCallum.
2006. Sparse forward-backward using minimum di-
vergence beams for fast training of conditional ran-
dom fields. In International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812–819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12–21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for ma-
chine translation via hash sampling. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 362–371,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
J¨org Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237–248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of speech-to-speech translations. Springer-
Verlag, Berlin.
S.J. Young and Sj Young. 1994. The htk hidden
markov model toolkit: Design and philosophy. En-
tropic Cambridge Research Laboratory, Ltd, 2:2–
44.
</reference>
<page confidence="0.997825">
764
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372171">
<title confidence="0.818608">EM Decipherment for Large Vocabularies Nuhn</title>
<author confidence="0.519329">Human Language Technology</author>
<author confidence="0.519329">Pattern</author>
<affiliation confidence="0.987427">Computer Science Department, RWTH Aachen University, Aachen,</affiliation>
<email confidence="0.995681"><surname>@cs.rwth-aachen.de</email>
<abstract confidence="0.999361208333333">This paper addresses the problem of EMbased decipherment for large vocabularies. Here, decipherment is essentially a tagging problem: Every cipher token is tagged with some plaintext type. As with other tagging problems, this one can be treated as a Hidden Markov Model (HMM), only here, the vocabularies are so the usual EM approach is infeasible. When faced with this situation, many people turn to sampling. However, we propose to use a type of approximate EM and show that it works well. The basic idea is to collect fractional counts only over a small subset of links in the forward-backward lattice. The subset is different for each iteration of EM. One option is to use beam search to do the subsetting. The second method restricts the successor words that are looked at, for each hypothesis. It does this by consulting pre-computed tables of likely n-grams and likely substitutions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Corlett</author>
<author>Gerald Penn</author>
</authors>
<title>An exact A* method for deciphering letter-substitution ciphers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1040--1047</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2462" citStr="Corlett and Penn (2010)" startWordPosition="381" endWordPosition="385">ciphers which provides high decipherment accuracies while having low computational requirements. The proposed approach allows using high order n-gram language models, and is scalable to large vocabulary sizes. We show improvements in decipherment accuracy in a variety of experiments (including MT) while being computationally more efficient than previous published work on EM-based decipherment. 2 Related Work Several methods exist for deciphering 1:1 substitution ciphers: Ravi and Knight (2008) solve 1:1 substitution ciphers by formulating the decipherment problem as an integer linear program. Corlett and Penn (2010) solve the same problem using A∗ search. Nuhn et al. (2013) present a beam search approach that scales to large vocabulary and high order language models. Even though being successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation. EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report resul</context>
</contexts>
<marker>Corlett, Penn, 2010</marker>
<rawString>Eric Corlett and Gerald Penn. 2010. An exact A* method for deciphering letter-substitution ciphers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040–1047, Uppsala, Sweden, July. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, B,</journal>
<volume>39</volume>
<contexts>
<context position="5965" citStr="Dempster et al., 1977" startWordPosition="967" endWordPosition="970">(fN1 |ϑ): ϑ = arg max V&apos; 1This can be generalized to n-gram language models. After we obtained the parameters ϑ we can obtain ei as the Viterbi decoding arg maxe1N{p(e1 f 1N, ϑ) } 3.1 Exact EM training In the decipherment setting, we are given the observed ciphertext fN1 and the model p(fN1 |eN1 , ϑ) that explains how the observed ciphertext has been generated given a latent plaintext eN1 . Marginalizing the unknown eN1 , we would like to obtain the maximum likelihood estimate of ϑ as specified in Equation 4. We iteratively compute the maximum likelihood estimate by applying the EM algorithm (Dempster et al., 1977): E pn(e|fN1 ,ϑ) n:fn=f pn(e|fN 1 , ϑ) (5) with pn(e|fN 1 ,ϑ) = � p(eN1 |fN1) ϑ(6) [eN1 :en=e] being the posterior probability of observing the plaintext symbol e at position n given the ciphertext sequence fN1 and the current parameters ϑ. pn(e|fN1 , ϑ) can be efficiently computed using the forward-backward algorithm. 3.2 Approximations to EM-Training The computational complexity of EM training stems from the sum E[eN1 :en=e] contained in the posterior pn(e|fN1 , ϑ). However, we can approximate this sum (and hope that the EM training procedure is still working) by only evaluating the dominati</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
<author>Fileno Alleva</author>
<author>Hsiao wuen Hon</author>
<author>Mei yuh Hwang</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>The sphinx-ii speech recognition system: An overview.</title>
<date>1992</date>
<journal>Computer, Speech and Language,</journal>
<pages>7--137</pages>
<contexts>
<context position="3299" citStr="Huang et al. (1992)" startWordPosition="515" endWordPosition="518">le to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation. EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient. Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backward EM training. Pal et al. (2006) use beam search for training of CRFs. 759 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759–764, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Method Publications Complexity EM Full (Knight et al., 2006), (Ravi and Knight, 2011) O(NV n) EM Fixed Candidates (Nuhn et al., 20</context>
</contexts>
<marker>Huang, Alleva, Hon, Hwang, Rosenfeld, 1992</marker>
<rawString>Xuedong Huang, Fileno Alleva, Hsiao wuen Hon, Mei yuh Hwang, and Ronald Rosenfeld. 1992. The sphinx-ii speech recognition system: An overview. Computer, Speech and Language, 7:137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Anish Nair</author>
<author>Nishit Rathod</author>
<author>Kenji Yamada</author>
</authors>
<title>Unsupervised Analysis for Decipherment Problems.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06,</booktitle>
<pages>499--506</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3829" citStr="Knight et al., 2006" startWordPosition="588" endWordPosition="591">ally very efficient. Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backward EM training. Pal et al. (2006) use beam search for training of CRFs. 759 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759–764, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Method Publications Complexity EM Full (Knight et al., 2006), (Ravi and Knight, 2011) O(NV n) EM Fixed Candidates (Nuhn et al., 2012) O(N) EM Beam This Work O(NV ) EM Lookahead This Work O(N) Table 1: Different approximations to exact EM training for decipherment. N is the cipher sequence length, V the size of the target vocabulary, and n the order of the language model. The main contribution of this work is the preselection beam search that—to the best of our knowledge—was not known in literature before, and serves as an important step to applying EM training to the large vocabulary decipherment problem. Table 1 gives an overview of the EM based metho</context>
</contexts>
<marker>Knight, Nair, Rathod, Yamada, 2006</marker>
<rawString>Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised Analysis for Decipherment Problems. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 499–506, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Deciphering foreign language by combining language models and context vectors.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>156--164</pages>
<institution>Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic of</location>
<contexts>
<context position="2903" citStr="Nuhn et al. (2012)" startWordPosition="454" endWordPosition="457">ing 1:1 substitution ciphers: Ravi and Knight (2008) solve 1:1 substitution ciphers by formulating the decipherment problem as an integer linear program. Corlett and Penn (2010) solve the same problem using A∗ search. Nuhn et al. (2013) present a beam search approach that scales to large vocabulary and high order language models. Even though being successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation. EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient. Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backward EM training. Pal e</context>
<context position="16692" citStr="Nuhn et al. (2012)" startWordPosition="2822" endWordPosition="2825">same preselection search procedure as used for the simple substitution cipher task. We run experiments on the opus corpus as presented in (Tiedemann, 2009). Table 5 shows previously published results using EM together with the results of our new method: (Ravi and Knight, 2011) is the only publication that reports results using exact EM training and only n-gram language models on the target side: It has an estimated runtime of 850h. All other published results (using EM training and Bayesian inference) use context vectors as an additional source of information: This might be an explanation why Nuhn et al. (2012) and Ravi (2013) are able to outperform exact EM training as reported by Ravi and Knight (2011). (Ravi, 2013) reports the most efficient method so far: It only consumes about 3h of computation time. However, as mentioned before, those results are not directly comparable to our work, since they use additional context information on the target side. Our algorithm clearly outperforms the exact EM training in run time, and even slighlty improves performance in BLEU. Similar to the simple substitution case, the improved performance might be caused by inferring a sparser distribution plex(f|e). Howe</context>
</contexts>
<marker>Nuhn, Mauser, Ney, 2012</marker>
<rawString>Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language models and context vectors. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 156–164, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Julian Schamper</author>
<author>Hermann Ney</author>
</authors>
<title>Beam search for solving substitution ciphers.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>1569--1576</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2521" citStr="Nuhn et al. (2013)" startWordPosition="394" endWordPosition="397"> low computational requirements. The proposed approach allows using high order n-gram language models, and is scalable to large vocabulary sizes. We show improvements in decipherment accuracy in a variety of experiments (including MT) while being computationally more efficient than previous published work on EM-based decipherment. 2 Related Work Several methods exist for deciphering 1:1 substitution ciphers: Ravi and Knight (2008) solve 1:1 substitution ciphers by formulating the decipherment problem as an integer linear program. Corlett and Penn (2010) solve the same problem using A∗ search. Nuhn et al. (2013) present a beam search approach that scales to large vocabulary and high order language models. Even though being successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation. EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash samp</context>
</contexts>
<marker>Nuhn, Schamper, Ney, 2013</marker>
<rawString>Malte Nuhn, Julian Schamper, and Hermann Ney. 2013. Beam search for solving substitution ciphers. In Annual Meeting of the Assoc. for Computational Linguistics, pages 1569–1576, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Pal</author>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Sparse forward-backward using minimum divergence beams for fast training of conditional random fields.</title>
<date>2006</date>
<booktitle>In International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="3515" citStr="Pal et al. (2006)" startWordPosition="544" endWordPosition="547">2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient. Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backward EM training. Pal et al. (2006) use beam search for training of CRFs. 759 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759–764, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Method Publications Complexity EM Full (Knight et al., 2006), (Ravi and Knight, 2011) O(NV n) EM Fixed Candidates (Nuhn et al., 2012) O(N) EM Beam This Work O(NV ) EM Lookahead This Work O(N) Table 1: Different approximations to exact EM training for decipherment. N is the cipher sequence length, V the size of the target vocabulary, and n the o</context>
</contexts>
<marker>Pal, Sutton, McCallum, 2006</marker>
<rawString>Chris Pal, Charles Sutton, and Andrew McCallum. 2006. Sparse forward-backward using minimum divergence beams for fast training of conditional random fields. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Attacking decipherment problems optimally with low-order ngram models.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>812--819</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="2337" citStr="Ravi and Knight (2008)" startWordPosition="362" endWordPosition="365">eyond a few hundred words. In this paper we present an efficient EM based training procedure for probabilistic substitution ciphers which provides high decipherment accuracies while having low computational requirements. The proposed approach allows using high order n-gram language models, and is scalable to large vocabulary sizes. We show improvements in decipherment accuracy in a variety of experiments (including MT) while being computationally more efficient than previous published work on EM-based decipherment. 2 Related Work Several methods exist for deciphering 1:1 substitution ciphers: Ravi and Knight (2008) solve 1:1 substitution ciphers by formulating the decipherment problem as an integer linear program. Corlett and Penn (2010) solve the same problem using A∗ search. Nuhn et al. (2013) present a beam search approach that scales to large vocabulary and high order language models. Even though being successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation. EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to ex</context>
</contexts>
<marker>Ravi, Knight, 2008</marker>
<rawString>Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order ngram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 812–819, Honolulu, Hawaii. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT),</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2883" citStr="Ravi and Knight (2011)" startWordPosition="450" endWordPosition="453">thods exist for deciphering 1:1 substitution ciphers: Ravi and Knight (2008) solve 1:1 substitution ciphers by formulating the decipherment problem as an integer linear program. Corlett and Penn (2010) solve the same problem using A∗ search. Nuhn et al. (2013) present a beam search approach that scales to large vocabulary and high order language models. Even though being successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation. EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient. Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backwar</context>
<context position="13887" citStr="Ravi and Knight, 2011" startWordPosition="2357" endWordPosition="2360"> half of the corpus). By doing this, we create new simple substitution ciphers with smaller vocabularies of size 200 and 500. For the smallest setup, we can directly compare all three EM variants. We also include experiments on the original corpus with vocabulary size of 3661. When comparing exact EM training with beam- and preselection EM training, the first thing we notice is that it takes about 20 times longer to run the exact EM training than training with beam EM, and about 50 times longer than the preselection EM training. Interestingly, 762 Model Method BLEU [%] Runtime 2-gram Exact EM(Ravi and Knight, 2011) 15.3 850.0h whole segment lm Exact EM(Ravi and Knight, 2011) 19.3 850.0h 2-gram Preselection EM (This work) 15.7 1.8h 3-gram Preselection EM (This work) 19.5 1.9h Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on the Spanish/English OPUS corpus using only non-parallel corpora for training. the accuracy of the approximations to exact EM training is better than that of the exact EM training. Even though this needs further investigation, it is clear that the pruned versions of EM training find sparser distributions plex(f|e): This is desirable in t</context>
<context position="15830" citStr="Ravi and Knight (2011)" startWordPosition="2680" endWordPosition="2683">g decipherment accuracy. Our new preselection search method is in turn orders of magnitudes faster than beam search EM while even being able to outperform exact EM and beam EM by using higher order language models. We were thus able to scale the EM decipherment to larger vocabularies of several thousand words. The runtime behavior is also consistent with the computational complexity discussed in Section 3.2. 4.2 Machine Translation We show that our algorithm is directly applicable to the decipherment problem for machine translation. We use the same simplified translation model as presented by Ravi and Knight (2011). Because this translation model allows insertions and deletions, hypotheses of different cardinalities coexist during search. We extend our search approach such that pruning is done for each cardinality separately. Other than that, we use the same preselection search procedure as used for the simple substitution cipher task. We run experiments on the opus corpus as presented in (Tiedemann, 2009). Table 5 shows previously published results using EM together with the results of our new method: (Ravi and Knight, 2011) is the only publication that reports results using exact EM training and only </context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT), pages 12–21, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
</authors>
<title>Scalable decipherment for machine translation via hash sampling.</title>
<date>2013</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>362--371</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3049" citStr="Ravi (2013)" startWordPosition="479" endWordPosition="480"> Corlett and Penn (2010) solve the same problem using A∗ search. Nuhn et al. (2013) present a beam search approach that scales to large vocabulary and high order language models. Even though being successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation. EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient. Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backward EM training. Pal et al. (2006) use beam search for training of CRFs. 759 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Sh</context>
<context position="16708" citStr="Ravi (2013)" startWordPosition="2827" endWordPosition="2828">h procedure as used for the simple substitution cipher task. We run experiments on the opus corpus as presented in (Tiedemann, 2009). Table 5 shows previously published results using EM together with the results of our new method: (Ravi and Knight, 2011) is the only publication that reports results using exact EM training and only n-gram language models on the target side: It has an estimated runtime of 850h. All other published results (using EM training and Bayesian inference) use context vectors as an additional source of information: This might be an explanation why Nuhn et al. (2012) and Ravi (2013) are able to outperform exact EM training as reported by Ravi and Knight (2011). (Ravi, 2013) reports the most efficient method so far: It only consumes about 3h of computation time. However, as mentioned before, those results are not directly comparable to our work, since they use additional context information on the target side. Our algorithm clearly outperforms the exact EM training in run time, and even slighlty improves performance in BLEU. Similar to the simple substitution case, the improved performance might be caused by inferring a sparser distribution plex(f|e). However, this requir</context>
</contexts>
<marker>Ravi, 2013</marker>
<rawString>Sujith Ravi. 2013. Scalable decipherment for machine translation via hash sampling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 362–371, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<volume>volume V,</volume>
<pages>237--248</pages>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<location>Amsterdam/Philadelphia, Borovets, Bulgaria.</location>
<contexts>
<context position="16229" citStr="Tiedemann, 2009" startWordPosition="2746" endWordPosition="2747">n 3.2. 4.2 Machine Translation We show that our algorithm is directly applicable to the decipherment problem for machine translation. We use the same simplified translation model as presented by Ravi and Knight (2011). Because this translation model allows insertions and deletions, hypotheses of different cardinalities coexist during search. We extend our search approach such that pruning is done for each cardinality separately. Other than that, we use the same preselection search procedure as used for the simple substitution cipher task. We run experiments on the opus corpus as presented in (Tiedemann, 2009). Table 5 shows previously published results using EM together with the results of our new method: (Ravi and Knight, 2011) is the only publication that reports results using exact EM training and only n-gram language models on the target side: It has an estimated runtime of 850h. All other published results (using EM training and Bayesian inference) use context vectors as an additional source of information: This might be an explanation why Nuhn et al. (2012) and Ravi (2013) are able to outperform exact EM training as reported by Ravi and Knight (2011). (Ravi, 2013) reports the most efficient </context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of speech-to-speech translations.</title>
<date>2000</date>
<editor>Wolfgang Wahlster, editor.</editor>
<publisher>SpringerVerlag,</publisher>
<location>Berlin.</location>
<marker>2000</marker>
<rawString>Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of speech-to-speech translations. SpringerVerlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Young</author>
<author>Sj Young</author>
</authors>
<title>The htk hidden markov model toolkit: Design and philosophy.</title>
<date>1994</date>
<journal>Entropic Cambridge Research Laboratory, Ltd,</journal>
<volume>2</volume>
<pages>44</pages>
<contexts>
<context position="3426" citStr="Young and Young (1994)" startWordPosition="531" endWordPosition="534"> training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient. Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backward EM training. Pal et al. (2006) use beam search for training of CRFs. 759 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759–764, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Method Publications Complexity EM Full (Knight et al., 2006), (Ravi and Knight, 2011) O(NV n) EM Fixed Candidates (Nuhn et al., 2012) O(N) EM Beam This Work O(NV ) EM Lookahead This Work O(N) Table 1: Different approximations to exact EM training for deciph</context>
</contexts>
<marker>Young, Young, 1994</marker>
<rawString>S.J. Young and Sj Young. 1994. The htk hidden markov model toolkit: Design and philosophy. Entropic Cambridge Research Laboratory, Ltd, 2:2– 44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>