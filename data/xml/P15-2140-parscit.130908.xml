<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002107">
<title confidence="0.9986585">
Semantic Structure Analysis of Noun Phrases
using Abstract Meaning Representation
</title>
<author confidence="0.994897">
Yuichiro Sawai Hiroyuki Shindo Yuji Matsumoto
</author>
<affiliation confidence="0.9984365">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.717091">
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
</address>
<email confidence="0.999427">
{sawai.yuichiro.sn0,shindo,matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998586842105263">
We propose a method for semantic struc-
ture analysis of noun phrases using Ab-
stract Meaning Representation (AMR).
AMR is a graph representation for the
meaning of a sentence, in which noun
phrases (NPs) are manually annotated
with internal structure and semantic re-
lations. We extract NPs from the AMR
corpus and construct a data set of NP
semantic structures. We also propose a
transition-based algorithm which jointly
identifies both the nodes in a semantic
structure tree and semantic relations be-
tween them. Compared to the baseline,
our method improves the performance of
NP semantic structure analysis by 2.7
points, while further incorporating exter-
nal dictionary boosts the performance by
7.1 points.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999668222222222">
Semantic structure analysis of noun phrases (NPs)
is an important research topic, which is beneficial
for various NLP tasks, such as machine transla-
tion and question answering (Nakov and Hearst,
2013; Nakov, 2013). Among the previous works
on NP analysis are internal NP structure analy-
sis (Vadas and Curran, 2007; Vadas and Curran,
2008), noun-noun relation analysis of noun com-
pounds (Girju et al., 2005; Tratz and Hovy, 2010;
Kim and Baldwin, 2013), and predicate-argument
analysis of noun compounds (Lapata, 2002).
The goal of internal NP structure analysis is
to assign bracket information inside an NP (e.g.,
(lung cancer) deaths indicates that the phrase lung
cancer modifies the head deaths). In noun-noun
relation analysis, the goal is to assign one of the
predefined semantic relations to a noun compound
consisting of two nouns (e.g., assigning a relation
</bodyText>
<figureCaption confidence="0.914437">
Figure 1: disaster prevention awareness in AMR.
Predicate-argument relation ARG1, noun-noun re-
lation topic, and internal structure (disaster pre-
vention) awareness are expressed.
</figureCaption>
<bodyText confidence="0.999551333333333">
purpose to a noun compound cooking pot, mean-
ing that pot is used for cooking). On the other
hand, in predicate-argument analysis, the goal is
to decide whether the modifier noun is the subject
or the object of the head noun (e.g., car is the ob-
ject of love in car lover, while child is the subject
of behave in child behavior).
Previous NP researches have mainly focused on
these different subproblems of NP analysis using
different data sets, rather than targeting general
NPs simultaneously. However, these subproblems
of NP analysis tend to be highly intertwined when
processing texts. For the purpose of tackling the
task of combined NP analysis, we make use of the
Abstract Meaning Representation (AMR) corpus.
AMR is a formalism of sentence semantic
structure by directed, acyclic, and rooted graphs,
in which semantic relations such as predicate-
argument relations and noun-noun relations are
expressed. In this paper, we extract substructures
corresponding to NPs (shown in Figure 1) from
the AMR Bank1, and create a data set of NP se-
mantic structures. In general, AMR substructures
are graphs. However, since we found out that NPs
mostly form trees rather than graphs in the AMR
Bank, we can assume that AMR substructures cor-
responding to NPs are trees. Thus, we define our
task as predicting the AMR tree structure, given a
sequence of words in an NP.
The previous method for AMR parsing takes a
</bodyText>
<footnote confidence="0.872999">
1http://amr.isi.edu/
</footnote>
<figure confidence="0.7864185">
disaster prevention awareness
disaster prevent-01 awareness
ARG1
topic
</figure>
<page confidence="0.765586">
851
</page>
<note confidence="0.318993666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 851–856,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<tableCaption confidence="0.643638">
Train Dev Test
3504 463 398
Table 1: Statistics of the extracted NP data
</tableCaption>
<bodyText confidence="0.99996125">
two-step approach: first identifying distinct con-
cepts (nodes) in the AMR graph, then defin-
ing the dependency relations between those con-
cepts (Flanigan et al., 2014). In the concept iden-
tification step, unlike POS tagging, one word is
sometimes assigned with more than one concept,
and the number of possible concepts is far more
than the number of possible parts-of-speech. As
the concept identification accuracy remains low,
such a pipeline method suffers from error propaga-
tion, thus resulting in a suboptimal AMR parsing
performance.
To solve this problem, we extend a transition-
based dependency parsing algorithm, and propose
a novel algorithm which jointly identifies the con-
cepts and the relations in AMR trees. Compared
to the baseline, our method improves the perfor-
mance of AMR analysis of NP semantic structures
by 2.7 points, and using an external dictionary fur-
ther boosts the performance by 7.1 points.
</bodyText>
<sectionHeader confidence="0.948244" genericHeader="method">
2 Abstract Meaning Representation
</sectionHeader>
<subsectionHeader confidence="0.99901">
2.1 Extraction of NPs
</subsectionHeader>
<bodyText confidence="0.999865176470588">
We extract substructures (subtrees) correspond-
ing to NPs from the AMR Bank (LDC2014T12).
In the AMR Bank, there is no alignment be-
tween the words and the concepts (nodes) in the
AMR graphs. We obtain this alignment by using
the rule-based alignment tool by Flanigan et al.
(2014). Then, we use the Stanford Parser (Klein
and Manning, 2003) to obtain constituency trees,
and extract NPs that contain more than one noun
and are not included by another NP. We ex-
clude NPs that contain named entities, because
they would require various kinds of manually
crafted rules for each type of named entity. We
also exclude NPs that contain possessive pronouns
or conjunctions, which prove problematic for the
alignment tool. Table 1 shows the statistics of the
extracted NP data.
</bodyText>
<subsectionHeader confidence="0.996774">
2.2 Previous Method for AMR Analysis
</subsectionHeader>
<bodyText confidence="0.993573076923077">
We adopt the method proposed by Flanigan et
al. (2014) as our baseline, which is a two-step
pipeline method of concept identification step and
Figure 2: Concept identification step of (Flanigan
et al., 2014) for a retired plant worker. ∅ denotes
an empty concept.
relation identification step. Their method is de-
signed for parsing sentences into AMR, but here,
we use this method for parsing NPs.
In their method, concept identification is formu-
lated as a sequence labeling problem (Janssen and
Limnios, 1999) and solved by the Viterbi algo-
rithm. Spans of words in the input sentence are
labeled with concept subgraphs. Figure 2 illus-
trates the concept identification step for an NP a
retired plant worker.
After the concepts have been identified, these
concepts are fixed, and the dependency rela-
tions between them are identified by an algorithm
that finds the maximum spanning connected sub-
graph (Chu and Liu, 1965), which is similar to the
maximum spanning tree (MST) algorithm used for
dependency parsing (McDonald et al., 2005).
They report that using gold concepts yields
much better performance, implying that joint iden-
tification of concepts and relations can be helpful.
</bodyText>
<sectionHeader confidence="0.993534" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.9999605">
In this paper, we propose a novel approach for
mapping the word sequence in an NP to an AMR
tree, where the concepts (nodes) corresponding to
the words and the dependency relations between
those concepts must be identified. We extend the
arc-standard algorithm by Nivre (2004) for AMR
parsing, and propose a transition-based algorithm
which jointly identifies concepts and dependency
</bodyText>
<figureCaption confidence="0.995996">
Figure 4: a retired plant worker in AMR
</figureCaption>
<figure confidence="0.9992235">
a retired
plant
worker
retire-01
plant
work-01
person
ARG0-of
a retired
retire-01
ARG0-of
plant
ARG2
plant
work-01
person
ARG0-of
worker
</figure>
<page confidence="0.971004">
852
</page>
<table confidence="0.996686545454546">
Previous action σ1 σ0 β R
0 (initial state) [a retired plant worker] 0
1 SHIFT(EMPTY(a)) 0 [retired plant worker] 0
2 EMPTY-REDUCE [retired plant worker] 0
3 SHIFT(DICTPRED(retired)) [plant worker] 0
retire-01
4 SHIFT(LEMMA(plant)) [worker] 0
retire-01 plant
5 SHIFT(KNOWN(worker)) [ ] 0
person
plant ARG0-of
work 01
6 LEFT-REDUCE(ARG2, nchild) [ ] {(work-01) ARG2
−−−� (plant)}
person
retire-01 ARG0-of
work-01
7 LEFT-REDUCE(ARG0-of, nroot) person [ ] ARG2
(work-0 1) (plant),
(person) (retire-01)}
ARG0-of
work-01
</table>
<figureCaption confidence="0.9879625">
Figure 3: Derivation of an AMR tree for a retired plant worker (σ0 and σ1 denote the top and the second
top of the stack, respectively.)
</figureCaption>
<equation confidence="0.966279666666667">
Action
SHIFT(c(wi))
LEFT-REDUCE(r, n)
RIGHT-REDUCE(r, n)
EMPTY-REDUCE
Current state
(σ, [wi|β], R)
([σ|ci|cj], β, R)
([σ|ci|cj], β, R)
([σ|φ], β, R)
Next state
([σ|c(wi)], β, R)
([σ|cj],β, R U {nroot(ci) t− � n(cj)})
([σ|ci],β,R U {n(ci) &apos;−� nroot(cj)})
(σ, β, R)
</equation>
<tableCaption confidence="0.912426">
Table 2: Definitions of the actions
</tableCaption>
<bodyText confidence="0.99997012">
relations. Our algorithm is similar to (Hatori et al.,
2011), in which they perform POS tagging and de-
pendency parsing jointly by assigning a POS tag
to a word when performing SHIFT, but differs in
that, unlike POS tagging, one word is sometimes
assigned with more than one concept. In our al-
gorithm, the input words are stored in the buffer
and the identified concepts are stored in the stack.
SHIFT identifies a concept subtree associated with
the top word in the buffer. REDUCE identifies the
dependency relation between the top two concept
subtrees in the stack. Figure 3 illustrates the pro-
cess of deriving an AMR tree for a retired plant
worker, and Figure 4 shows the resulting AMR
tree.
Table 2 shows the definition of each action and
state transition. A state is a triple (σ, β, R), where
σ is a stack of identified concept subtrees, β is a
buffer of words, and R is a set of identified rela-
tions. SHIFT(c(wi)) extracts the top word wi in
the buffer, generates a concept subtree c(wi) from
wi, and pushes c(wi) onto the stack. The concept
subtree c(wi) is generated from wi by using one of
the rules defined in Table 3. LEFT-REDUCE(r, n)
pops the top two subtrees ci, cj from the stack,
identifies the relation r between the root nroot(ci)
of ci and the node n(cj) in cj, adds r to R, and
pushes cj back onto the stack. Here, n denotes a
mapping from a subtree to its specific node, which
allows for attachment to an arbitrary concept in
a subtree. Since the sizes of the subtrees were
at most two in our data set, n ∈ {nroot, nchild},
where nroot is a mapping from a subtree to its root,
and nchild is a mapping from a subtree to the direct
child of its root. RIGHT-REDUCE(r, n) is defined
in the same manner. EMPTY-REDUCE removes an
empty subtree 0 at the top of the stack. EMPTY-
REDUCE is always performed immediately after
SHIFT(0) generates an empty subtree 0. In the
initial state, the stack σ is empty, the buffer β con-
tains all the words in the NP, and the set of the
identified relations R is empty. In the terminal
state, the buffer β is empty and the stack σ con-
tains only one subtree. The resulting AMR tree is
obtained by connecting all the subtrees generated
by the SHIFT actions with all the relations in R.
The previous method could not generate un-
seen concepts in the training data, leading to low
recall in concept identification. In contrast, our
method defines five rules (Table 3), three of which
</bodyText>
<page confidence="0.998299">
853
</page>
<table confidence="0.991165285714286">
Rule Concept subtree to generate Example of subtree generation
EMPTY an empty concept subtree 0 fighters --+ 0
KNOWN a subtree aligning to the word in the training data fighters --+ (person) ARG0-of
LEMMA a subtree with the lemma of the word as the only concept ����--+ (fight-01)  |...
DICTPRED a subtree with a predicate form of the word as the only concept fighters --+ (fighter)
DICTNOUN a subtree with a noun form of the word as the only concept fighters --+ (fight-01)  |(fight-02)  |...
fighters --+ (fight)
</table>
<tableCaption confidence="0.995808">
Table 3: Rules for generating a concept subtree (Vertical lines indicate multiple candidate subtrees.)
</tableCaption>
<bodyText confidence="0.9877375">
(LEMMA, DICTPRED, and DICTNOUN) allow for
generation of unseen concepts from any word.
</bodyText>
<subsectionHeader confidence="0.976395">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.9996906">
The feature set O(s, a) for the current state s and
the next action a is the direct product (all-vs-all
combinations from each set) of the feature set
Ostate(s) for the current state and the feature set
Oaction(s, a) for the next action.
</bodyText>
<equation confidence="0.860152">
O(s, a) = Ostate(s) × Oaction(s, a)
</equation>
<bodyText confidence="0.999200466666667">
Ostate(s) is the union of the feature sets defined
in Table 4, where w(c) denotes the word from
which the subtree c was generated.
Table 5 shows the feature set
Oaction((0&apos;, [wi10], R), a) for each action a, where
rule(wi, c) is a function that returns the rule
which generated the subtree c from the top word
wi in the buffer. In order to allow different SHIFT
actions and different LEFT-RIGHT/REDUCE
actions to partially share features, Table 5 de-
fines features of different granularities for each
action. For example, although SHIFT((run-01))
and SHIFT((sleep-01)) are different actions, they
share the features “S”, “S”◦“DICTPRED” because
they share the generation rule DICTPRED.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999883">
We conduct an experiment using our NP data set
(Table 1). We use the implementation 2 of (Flani-
gan et al., 2014) as our baseline. For the baseline,
we use the features of the default settings.
The method by Flanigan et al. (2014) can only
generate the concepts that appear in the training
data. On the other hand, our method can gen-
erate concepts that do not appear in the training
data using the concept generation rules LEMMA,
DICTPRED, and DICTNOUN in Table 3. For a fair
comparison, first, we only use the rules EMPTY
and KNOWN. Then, we add the rule LEMMA,
which can generate a concept of the lemma of the
</bodyText>
<footnote confidence="0.92443">
2https://github.com/jflanigan/jamr
</footnote>
<table confidence="0.680761142857143">
Name Definition
LEM {w(σ1).lem, w(σ0).lem, β0.lem,
w(σ1).lemow(σ0).lem, w(σ0).lemoβ0.lem}
SUF {w(σ1).suf, w(σ0).suf,β0.suf,
w(σ1).suf o w(σ0).suf, w(σ0).suf o β0.suf}
POS {w(σ1).pos, w(σ0).pos, β0.pos,
w(σ1).pos o w(σ0).pos, w(σ0).pos o β0.pos}
DEP {w(σ1).dep, w(σ0).dep, β0.dep,
w(σ1).depow(σ0).dep, w(σ0).depoβ0.dep}
HEAD {w(σ1).off, w(σ0).off,β0.off,
w(σ1).off o w(σ0).off, w(σ0).off o β0.off}
ROOT {nroot(σ1), nroot(σ0), nroot(σ1) o nroot(σ0)}
MID all words between w(σ1) and w(σ0) U
all words between w(σ0) and β0
</table>
<tableCaption confidence="0.938984">
Table 4: Feature sets for the state (.lem is the
</tableCaption>
<bodyText confidence="0.99482">
lemma, .suf is the prefix of length 3, .pos is the
part-of-speech, .dep is the dependency label to the
parent word, .off is the offset to the parent word,
and ◦ denotes concatenation of features.)
</bodyText>
<tableCaption confidence="0.974123">
Table 5: Feature sets for the action
</tableCaption>
<bodyText confidence="0.997805888888889">
word. Finally, we add the rules DICTPRED and
DICTNOUN. These two rules need conversion from
nouns and adjectives to their verb and noun forms,
For this conversion, we use CatVar v2.1 (Habash
and Dorr, 2003), which lists categorial variations
of words (such as verb run for noun runner). We
also use definitions of the predicates from Prop-
Bank (Palmer et al., 2005), which AMR tries to
reuse as much as possible, and impose constraints
that the defined predicates can only have semantic
relations consistent with the definition.
During the training, we use the max-violation
perceptron (Huang et al., 2012) with beam size 8
and average the parameters. During the testing, we
also perform beam search with beam size 8.
Table 6 shows the overall performance on NP
semantic structure analysis. We evaluate the per-
formance using the Smatch score (Cai and Knight,
</bodyText>
<figure confidence="0.543525545454545">
Action a
SHIFT(c)
LEFT-REDUCE(r, n)
RIGHT-REDUCE(r, n)
EMPTY-REDUCE
φ..tion((σ, [wi|β], R), a)
{“S”, “S” o rule(wi, c),
“S” o rule(wi, c) o c}
{“L-R”, “L-R”or, “L-R”or on}
{“R-R”, “R-R”or, “R-R”oron}
{“E-R”}
</figure>
<page confidence="0.787843">
854
</page>
<table confidence="0.999862">
Method P R Fl
(Flanigan et al., 2014) 75.5 61.1 67.5
Our method (EMPTY/KNOWN) 78.0 63.8 70.2
Our method+LEMMA 75.7 75.2 75.4
Our method+LEMMA/DICT 77.3 77.3 77.3
</table>
<tableCaption confidence="0.947502">
Table 6: Performance on NP semantic structure
analysis
</tableCaption>
<table confidence="0.9999276">
Method P R Fl
(Flanigan et al., 2014) 88.4 71.4 79.0
Our method (EMPTY/KNOWN) 88.9 72.2 79.7
Our method+LEMMA 84.8 84.2 84.5
Our method+LEMMA/DICT 85.8 85.6 85.7
</table>
<tableCaption confidence="0.999854">
Table 7: Performance on concept identification
</tableCaption>
<bodyText confidence="0.999962947368421">
2013), which reports precision, recall, and F1-
score for overlaps of nodes, edges, and roots in
semantic structure graphs. Compared to the base-
line, our method improves both the precision and
recall, resulting in an increasing of F1-score by 2.7
points. When we add the LEMMA rule, the re-
call increases by 11.4 points because the LEMMA
rule can generate concepts that do not appear in
the training data, resulting in a further increase of
F1-score by 5.2 points. Finally, when we add the
DICT rules, the F1-score improves further by 1.9
points.
Table 7 shows the performance on concept iden-
tification. We report precision, recall, and F1-
score against the correct set of concepts. For each
condition, we observe the same tendency in per-
formance increases as Table 6. Thus, we conclude
that our method improves both the concept identi-
fication and relation identification performances.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999957454545455">
In this paper, we used Abstract Meaning Repre-
sentation (AMR) for semantic structure analysis
of noun compounds (NPs). We extracted substruc-
tures corresponding to NPs from the AMR Bank,
and created a data set of NP semantic structures.
Then, we proposed a novel method which jointly
identifies concepts (nodes) and dependency rela-
tions in AMR trees. We confirmed that our method
improves the performance on NP semantic struc-
ture analysis, and that incorporating an external
dictionary further boosts the performance.
</bodyText>
<sectionHeader confidence="0.997807" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.970265">
This work was supported by JSPS KAKENHI
Grant Number 15K16053, 26240035.
</bodyText>
<sectionHeader confidence="0.995635" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999409307692308">
Shu Cai and Kevin Knight. 2013. Smatch: An eval-
uation metric for semantic feature structures. pages
748–752.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrimi-
native graph-based parser for the Abstract Meaning
Representation. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1426–1436.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun com-
pounds. Computer Speech and Language, 19:479–
496.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
the 2003 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 17–23.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint POS tag-
ging and dependency parsing in Chinese. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing, pages 1216–1224.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151.
Jacques Janssen and Nikolaos Limnios. 1999. Semi-
Markov models and applications. Springer, Octo-
ber.
Su Nam Kim and Timothy Baldwin. 2013. A lexi-
cal semantic approach to interpreting and bracketing
English noun compounds. Natural Language Engi-
neering, 19(3):385–407.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 423–430.
Maria Lapata. 2002. The disambiguation of nomi-
nalizations. Computational Linguistics, 28(3):357–
388.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP), pages 523–530.
</reference>
<page confidence="0.98754">
855
</page>
<reference confidence="0.999527827586207">
Preslav I. Nakov and Marti A. Hearst. 2013. Semantic
interpretation of noun compounds using verbal and
other paraphrases. ACM Transactions on Speech
and Language Processing, 10(3):1–51.
Preslav Nakov. 2013. On the interpretation of noun
compounds: Syntax, semantics, and entailment.
Natural Language Engineering, 19(1):291–330.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 678–687.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, pages 240–247.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 335–343.
</reference>
<page confidence="0.999009">
856
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741714">
<title confidence="0.9971615">Semantic Structure Analysis of Noun using Abstract Meaning Representation</title>
<author confidence="0.995821">Yuichiro Sawai Hiroyuki Shindo Yuji Matsumoto</author>
<affiliation confidence="0.999701">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.948922">8916-5 Takayama, Ikoma, Nara, 630-0192,</address>
<abstract confidence="0.98648095">We propose a method for semantic structure analysis of noun phrases using Abstract Meaning Representation (AMR). AMR is a graph representation for the meaning of a sentence, in which noun phrases (NPs) are manually annotated with internal structure and semantic relations. We extract NPs from the AMR corpus and construct a data set of NP semantic structures. We also propose a transition-based algorithm which jointly identifies both the nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>Kevin Knight</author>
</authors>
<title>Smatch: An evaluation metric for semantic feature structures.</title>
<date>2013</date>
<pages>748--752</pages>
<marker>Cai, Knight, 2013</marker>
<rawString>Shu Cai and Kevin Knight. 2013. Smatch: An evaluation metric for semantic feature structures. pages 748–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<pages>1400</pages>
<contexts>
<context position="6624" citStr="Chu and Liu, 1965" startWordPosition="1042" endWordPosition="1045"> method is designed for parsing sentences into AMR, but here, we use this method for parsing NPs. In their method, concept identification is formulated as a sequence labeling problem (Janssen and Limnios, 1999) and solved by the Viterbi algorithm. Spans of words in the input sentence are labeled with concept subgraphs. Figure 2 illustrates the concept identification step for an NP a retired plant worker. After the concepts have been identified, these concepts are fixed, and the dependency relations between them are identified by an algorithm that finds the maximum spanning connected subgraph (Chu and Liu, 1965), which is similar to the maximum spanning tree (MST) algorithm used for dependency parsing (McDonald et al., 2005). They report that using gold concepts yields much better performance, implying that joint identification of concepts and relations can be helpful. 3 Proposed Method In this paper, we propose a novel approach for mapping the word sequence in an NP to an AMR tree, where the concepts (nodes) corresponding to the words and the dependency relations between those concepts must be identified. We extend the arc-standard algorithm by Nivre (2004) for AMR parsing, and propose a transition-</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the Abstract Meaning Representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1426--1436</pages>
<contexts>
<context position="4093" citStr="Flanigan et al., 2014" startWordPosition="628" endWordPosition="631">ng takes a 1http://amr.isi.edu/ disaster prevention awareness disaster prevent-01 awareness ARG1 topic 851 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 851–856, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Train Dev Test 3504 463 398 Table 1: Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts (Flanigan et al., 2014). In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech. As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance. To solve this problem, we extend a transitionbased dependency parsing algorithm, and propose a novel algorithm which jointly identifies the concepts and the relations in AMR trees. Compared to the baseline, our method improves </context>
<context position="5761" citStr="Flanigan et al. (2014)" startWordPosition="901" endWordPosition="904"> alignment tool by Flanigan et al. (2014). Then, we use the Stanford Parser (Klein and Manning, 2003) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP. We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity. We also exclude NPs that contain possessive pronouns or conjunctions, which prove problematic for the alignment tool. Table 1 shows the statistics of the extracted NP data. 2.2 Previous Method for AMR Analysis We adopt the method proposed by Flanigan et al. (2014) as our baseline, which is a two-step pipeline method of concept identification step and Figure 2: Concept identification step of (Flanigan et al., 2014) for a retired plant worker. ∅ denotes an empty concept. relation identification step. Their method is designed for parsing sentences into AMR, but here, we use this method for parsing NPs. In their method, concept identification is formulated as a sequence labeling problem (Janssen and Limnios, 1999) and solved by the Viterbi algorithm. Spans of words in the input sentence are labeled with concept subgraphs. Figure 2 illustrates the concept i</context>
<context position="12652" citStr="Flanigan et al., 2014" startWordPosition="2068" endWordPosition="2072"> for each action a, where rule(wi, c) is a function that returns the rule which generated the subtree c from the top word wi in the buffer. In order to allow different SHIFT actions and different LEFT-RIGHT/REDUCE actions to partially share features, Table 5 defines features of different granularities for each action. For example, although SHIFT((run-01)) and SHIFT((sleep-01)) are different actions, they share the features “S”, “S”◦“DICTPRED” because they share the generation rule DICTPRED. 4 Experiments We conduct an experiment using our NP data set (Table 1). We use the implementation 2 of (Flanigan et al., 2014) as our baseline. For the baseline, we use the features of the default settings. The method by Flanigan et al. (2014) can only generate the concepts that appear in the training data. On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICTPRED, and DICTNOUN in Table 3. For a fair comparison, first, we only use the rules EMPTY and KNOWN. Then, we add the rule LEMMA, which can generate a concept of the lemma of the 2https://github.com/jflanigan/jamr Name Definition LEM {w(σ1).lem, w(σ0).lem, β0.lem, w(σ1).lemow(σ0)</context>
<context position="15095" citStr="Flanigan et al., 2014" startWordPosition="2457" endWordPosition="2460"> relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the performance using the Smatch score (Cai and Knight, Action a SHIFT(c) LEFT-REDUCE(r, n) RIGHT-REDUCE(r, n) EMPTY-REDUCE φ..tion((σ, [wi|β], R), a) {“S”, “S” o rule(wi, c), “S” o rule(wi, c) o c} {“L-R”, “L-R”or, “L-R”or on} {“R-R”, “R-R”or, “R-R”oron} {“E-R”} 854 Method P R Fl (Flanigan et al., 2014) 75.5 61.1 67.5 Our method (EMPTY/KNOWN) 78.0 63.8 70.2 Our method+LEMMA 75.7 75.2 75.4 Our method+LEMMA/DICT 77.3 77.3 77.3 Table 6: Performance on NP semantic structure analysis Method P R Fl (Flanigan et al., 2014) 88.4 71.4 79.0 Our method (EMPTY/KNOWN) 88.9 72.2 79.7 Our method+LEMMA 84.8 84.2 84.5 Our method+LEMMA/DICT 85.8 85.6 85.7 Table 7: Performance on concept identification 2013), which reports precision, recall, and F1- score for overlaps of nodes, edges, and roots in semantic structure graphs. Compared to the baseline, our method improves both the precision and recall, resulting </context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the Abstract Meaning Representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1426–1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<pages>496</pages>
<contexts>
<context position="1437" citStr="Girju et al., 2005" startWordPosition="210" endWordPosition="213"> Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation Figure 1: disaster prevention awareness in AMR. Predicate-argument relation ARG1, noun-noun relation topic, and internal structure (disast</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19:479– 496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Bonnie Dorr</author>
</authors>
<title>A categorial variation database for English.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>17--23</pages>
<contexts>
<context position="14194" citStr="Habash and Dorr, 2003" startWordPosition="2310" endWordPosition="2313"> o β0.off} ROOT {nroot(σ1), nroot(σ0), nroot(σ1) o nroot(σ0)} MID all words between w(σ1) and w(σ0) U all words between w(σ0) and β0 Table 4: Feature sets for the state (.lem is the lemma, .suf is the prefix of length 3, .pos is the part-of-speech, .dep is the dependency label to the parent word, .off is the offset to the parent word, and ◦ denotes concatenation of features.) Table 5: Feature sets for the action word. Finally, we add the rules DICTPRED and DICTNOUN. These two rules need conversion from nouns and adjectives to their verb and noun forms, For this conversion, we use CatVar v2.1 (Habash and Dorr, 2003), which lists categorial variations of words (such as verb run for noun runner). We also use definitions of the predicates from PropBank (Palmer et al., 2005), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the </context>
</contexts>
<marker>Habash, Dorr, 2003</marker>
<rawString>Nizar Habash and Bonnie Dorr. 2003. A categorial variation database for English. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 17–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint POS tagging and dependency parsing in Chinese.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1216--1224</pages>
<contexts>
<context position="8483" citStr="Hatori et al., 2011" startWordPosition="1333" endWordPosition="1336">01 ARG0-of work-01 7 LEFT-REDUCE(ARG0-of, nroot) person [ ] ARG2 (work-0 1) (plant), (person) (retire-01)} ARG0-of work-01 Figure 3: Derivation of an AMR tree for a retired plant worker (σ0 and σ1 denote the top and the second top of the stack, respectively.) Action SHIFT(c(wi)) LEFT-REDUCE(r, n) RIGHT-REDUCE(r, n) EMPTY-REDUCE Current state (σ, [wi|β], R) ([σ|ci|cj], β, R) ([σ|ci|cj], β, R) ([σ|φ], β, R) Next state ([σ|c(wi)], β, R) ([σ|cj],β, R U {nroot(ci) t− � n(cj)}) ([σ|ci],β,R U {n(ci) &apos;−� nroot(cj)}) (σ, β, R) Table 2: Definitions of the actions relations. Our algorithm is similar to (Hatori et al., 2011), in which they perform POS tagging and dependency parsing jointly by assigning a POS tag to a word when performing SHIFT, but differs in that, unlike POS tagging, one word is sometimes assigned with more than one concept. In our algorithm, the input words are stored in the buffer and the identified concepts are stored in the stack. SHIFT identifies a concept subtree associated with the top word in the buffer. REDUCE identifies the dependency relation between the top two concept subtrees in the stack. Figure 3 illustrates the process of deriving an AMR tree for a retired plant worker, and Figu</context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2011</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2011. Incremental joint POS tagging and dependency parsing in Chinese. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1216–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="14593" citStr="Huang et al., 2012" startWordPosition="2374" endWordPosition="2377">or the action word. Finally, we add the rules DICTPRED and DICTNOUN. These two rules need conversion from nouns and adjectives to their verb and noun forms, For this conversion, we use CatVar v2.1 (Habash and Dorr, 2003), which lists categorial variations of words (such as verb run for noun runner). We also use definitions of the predicates from PropBank (Palmer et al., 2005), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the performance using the Smatch score (Cai and Knight, Action a SHIFT(c) LEFT-REDUCE(r, n) RIGHT-REDUCE(r, n) EMPTY-REDUCE φ..tion((σ, [wi|β], R), a) {“S”, “S” o rule(wi, c), “S” o rule(wi, c) o c} {“L-R”, “L-R”or, “L-R”or on} {“R-R”, “R-R”or, “R-R”oron} {“E-R”} 854 Method P R Fl (Flanigan et al., 2014) 75.5 61.1 67.5 Our method (EMPTY/KNOWN) 78.0 63.8 70.2 Our method+LEMMA 75.7 75.2 75.4 Our method</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Janssen</author>
<author>Nikolaos Limnios</author>
</authors>
<title>SemiMarkov models and applications.</title>
<date>1999</date>
<publisher>Springer,</publisher>
<contexts>
<context position="6216" citStr="Janssen and Limnios, 1999" startWordPosition="974" endWordPosition="977"> for the alignment tool. Table 1 shows the statistics of the extracted NP data. 2.2 Previous Method for AMR Analysis We adopt the method proposed by Flanigan et al. (2014) as our baseline, which is a two-step pipeline method of concept identification step and Figure 2: Concept identification step of (Flanigan et al., 2014) for a retired plant worker. ∅ denotes an empty concept. relation identification step. Their method is designed for parsing sentences into AMR, but here, we use this method for parsing NPs. In their method, concept identification is formulated as a sequence labeling problem (Janssen and Limnios, 1999) and solved by the Viterbi algorithm. Spans of words in the input sentence are labeled with concept subgraphs. Figure 2 illustrates the concept identification step for an NP a retired plant worker. After the concepts have been identified, these concepts are fixed, and the dependency relations between them are identified by an algorithm that finds the maximum spanning connected subgraph (Chu and Liu, 1965), which is similar to the maximum spanning tree (MST) algorithm used for dependency parsing (McDonald et al., 2005). They report that using gold concepts yields much better performance, implyi</context>
</contexts>
<marker>Janssen, Limnios, 1999</marker>
<rawString>Jacques Janssen and Nikolaos Limnios. 1999. SemiMarkov models and applications. Springer, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>A lexical semantic approach to interpreting and bracketing English noun compounds.</title>
<date>2013</date>
<journal>Natural Language Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="1483" citStr="Kim and Baldwin, 2013" startWordPosition="218" endWordPosition="221">oves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation Figure 1: disaster prevention awareness in AMR. Predicate-argument relation ARG1, noun-noun relation topic, and internal structure (disaster prevention) awareness are expressed. purpos</context>
</contexts>
<marker>Kim, Baldwin, 2013</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2013. A lexical semantic approach to interpreting and bracketing English noun compounds. Natural Language Engineering, 19(3):385–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="5240" citStr="Klein and Manning, 2003" startWordPosition="814" endWordPosition="817">nd the relations in AMR trees. Compared to the baseline, our method improves the performance of AMR analysis of NP semantic structures by 2.7 points, and using an external dictionary further boosts the performance by 7.1 points. 2 Abstract Meaning Representation 2.1 Extraction of NPs We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12). In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs. We obtain this alignment by using the rule-based alignment tool by Flanigan et al. (2014). Then, we use the Stanford Parser (Klein and Manning, 2003) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP. We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity. We also exclude NPs that contain possessive pronouns or conjunctions, which prove problematic for the alignment tool. Table 1 shows the statistics of the extracted NP data. 2.2 Previous Method for AMR Analysis We adopt the method proposed by Flanigan et al. (2014) as our baseline, which is a two-step pipeline method of concept identification</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The disambiguation of nominalizations.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>388</pages>
<contexts>
<context position="1549" citStr="Lapata, 2002" startWordPosition="228" endWordPosition="229"> further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation Figure 1: disaster prevention awareness in AMR. Predicate-argument relation ARG1, noun-noun relation topic, and internal structure (disaster prevention) awareness are expressed. purpose to a noun compound cooking pot, meaning that pot is used for coo</context>
</contexts>
<marker>Lapata, 2002</marker>
<rawString>Maria Lapata. 2002. The disambiguation of nominalizations. Computational Linguistics, 28(3):357– 388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav I Nakov</author>
<author>Marti A Hearst</author>
</authors>
<title>Semantic interpretation of noun compounds using verbal and other paraphrases.</title>
<date>2013</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="1231" citStr="Nakov and Hearst, 2013" startWordPosition="176" endWordPosition="179"> corpus and construct a data set of NP semantic structures. We also propose a transition-based algorithm which jointly identifies both the nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to </context>
</contexts>
<marker>Nakov, Hearst, 2013</marker>
<rawString>Preslav I. Nakov and Marti A. Hearst. 2013. Semantic interpretation of noun compounds using verbal and other paraphrases. ACM Transactions on Speech and Language Processing, 10(3):1–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>On the interpretation of noun compounds: Syntax, semantics, and entailment.</title>
<date>2013</date>
<journal>Natural Language Engineering,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1245" citStr="Nakov, 2013" startWordPosition="180" endWordPosition="181">data set of NP semantic structures. We also propose a transition-based algorithm which jointly identifies both the nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compoun</context>
</contexts>
<marker>Nakov, 2013</marker>
<rawString>Preslav Nakov. 2013. On the interpretation of noun compounds: Syntax, semantics, and entailment. Natural Language Engineering, 19(1):291–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="7181" citStr="Nivre (2004)" startWordPosition="1133" endWordPosition="1134">maximum spanning connected subgraph (Chu and Liu, 1965), which is similar to the maximum spanning tree (MST) algorithm used for dependency parsing (McDonald et al., 2005). They report that using gold concepts yields much better performance, implying that joint identification of concepts and relations can be helpful. 3 Proposed Method In this paper, we propose a novel approach for mapping the word sequence in an NP to an AMR tree, where the concepts (nodes) corresponding to the words and the dependency relations between those concepts must be identified. We extend the arc-standard algorithm by Nivre (2004) for AMR parsing, and propose a transition-based algorithm which jointly identifies concepts and dependency Figure 4: a retired plant worker in AMR a retired plant worker retire-01 plant work-01 person ARG0-of a retired retire-01 ARG0-of plant ARG2 plant work-01 person ARG0-of worker 852 Previous action σ1 σ0 β R 0 (initial state) [a retired plant worker] 0 1 SHIFT(EMPTY(a)) 0 [retired plant worker] 0 2 EMPTY-REDUCE [retired plant worker] 0 3 SHIFT(DICTPRED(retired)) [plant worker] 0 retire-01 4 SHIFT(LEMMA(plant)) [worker] 0 retire-01 plant 5 SHIFT(KNOWN(worker)) [ ] 0 person plant ARG0-of wo</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="14352" citStr="Palmer et al., 2005" startWordPosition="2337" endWordPosition="2340">he state (.lem is the lemma, .suf is the prefix of length 3, .pos is the part-of-speech, .dep is the dependency label to the parent word, .off is the offset to the parent word, and ◦ denotes concatenation of features.) Table 5: Feature sets for the action word. Finally, we add the rules DICTPRED and DICTNOUN. These two rules need conversion from nouns and adjectives to their verb and noun forms, For this conversion, we use CatVar v2.1 (Habash and Dorr, 2003), which lists categorial variations of words (such as verb run for noun runner). We also use definitions of the predicates from PropBank (Palmer et al., 2005), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the performance using the Smatch score (Cai and Knight, Action a SHIFT(c) LEFT-REDUCE(r, n) RIGHT-REDUCE(r, n) EMPTY-REDUCE φ..tion((σ, [wi|β], R), a) {“S”, “S” o</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A taxonomy, dataset, and classifier for automatic noun compound interpretation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>678--687</pages>
<contexts>
<context position="1459" citStr="Tratz and Hovy, 2010" startWordPosition="214" endWordPosition="217">eline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation Figure 1: disaster prevention awareness in AMR. Predicate-argument relation ARG1, noun-noun relation topic, and internal structure (disaster prevention) awarene</context>
</contexts>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2010. A taxonomy, dataset, and classifier for automatic noun compound interpretation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Adding noun phrase structure to the penn treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="1345" citStr="Vadas and Curran, 2007" startWordPosition="195" endWordPosition="198">ntly identifies both the nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation Figure 1: disaster prevention awareness in AMR</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007. Adding noun phrase structure to the penn treebank. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Parsing noun phrase structure with CCG.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>335--343</pages>
<contexts>
<context position="1370" citStr="Vadas and Curran, 2008" startWordPosition="199" endWordPosition="202"> nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 1 Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation Figure 1: disaster prevention awareness in AMR. Predicate-argument rela</context>
</contexts>
<marker>Vadas, Curran, 2008</marker>
<rawString>David Vadas and James R. Curran. 2008. Parsing noun phrase structure with CCG. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 335–343.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>