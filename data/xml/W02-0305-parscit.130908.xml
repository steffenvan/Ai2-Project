<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002816">
<note confidence="0.601317333333333">
Proceedings of the Workshop on Natural Language Processing in
the Biomedical Domain, Philadelphia, July 2002, pp. 29-36.
Association for Computational Linguistics.
</note>
<title confidence="0.988524">
MPLUS: A Probabilistic Medical Language Understanding System
</title>
<author confidence="0.99953">
Lee M. Christensen, Peter J. Haug, and Marcelo Fiszman
</author>
<affiliation confidence="0.999458">
Department of Medical Informatics, LDS Hospital/University of Utah, Salt Lake City, UT
</affiliation>
<email confidence="0.999308">
E-mail: ldlchris@ihc.com, ldphaug@ihc.com, ldmfiszm@ihc.com
</email>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995574">
This paper describes the basic philosophy
and implementation of MPLUS (M+), a
robust medical text analysis tool that uses a
semantic model based on Bayesian
Networks (BNs). BNs provide a concise
and useful formalism for representing
semantic patterns in medical text, and for
recognizing and reasoning over those
patterns. BNs are noise-tolerant, and
facilitate the training of M+.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999445547169812">
In the field of medical informatics,
computerized tools are being developed that
depend on databases of clinical information.
These include alerting systems for improved
patient care, data mining systems for quality
assurance and research, and diagnostic systems
for more complex medical decision support.
These systems require data that is appropriately
structured and coded. Since a large portion of
the information stored in patient databases is in
the form of free text, manually coding this
information in a format accessible to these tools
can be time consuming and expensive. In recent
years, natural language processing (NLP)
methodologies have been studied as a means of
automating this task. There have been many
projects involving automated medical language
analysis, including deciphering pathology
reports (Smart and Roux, 1995), physical exam
findings (Lin et al., 1991), and radiology reports
(Friedman et al., 1994; Ranum, 1989; Koehler,
1998).
M+ is the latest in a line of NLP tools
developed at LDS Hospital in Salt Lake City,
Utah. Its predecessors include SPRUS (Ranum,
1989) and SymText (Koehler, 1998). These
tools have been used in the realm of radiology
reports, admitting diagnoses (Haug et al., 1997),
radiology utilization review (Fiszman, 2002)
and syndromic detection (Chapman et al.,
2002). Some of the character of these tools
derives from common characteristics of
radiology reports, their initial target domain.
Because of the off-the-cuff nature of
radiology dictation, a report will frequently
contain text that is telegraphic or otherwise not
well formed grammatically. Our desire was not
only to take advantage of phrasal structure to
discover semantic patterns in text, but also to be
able to infer those patterns from lexical and
contextual cues when necessary.
Most NLP systems capable of semantic
analysis employ representational formalisms
with ties to classical logic, including semantic
grammars (Friedman et al., 1994), unification-
based semantics (Moore, 1989), and description
logics (Romacker and Hahn, 2000). M+ and its
predecessors employ Bayesian Networks (Pearl,
1988), a methodology outside this tradition.
This study discusses the philosophy and
implementation of M+, and attempts to show
how Bayesian Networks can be useful in
medical text analysis.
</bodyText>
<sectionHeader confidence="0.957402" genericHeader="introduction">
2 The M+ Semantic Model
</sectionHeader>
<subsectionHeader confidence="0.935306">
2.1 Semantic Bayesian Networks
</subsectionHeader>
<bodyText confidence="0.99806546875">
M+ uses Bayesian Networks (BNs) to represent
the basic semantic types and relations within a
medical domain such as chest radiology reports.
M+ BNs are structurally similar to semantic
networks, in that they are implemented as
directed acyclic graphs, with nodes
representing word and concept types, and links
representing relations between those types. BNs
also have a character as frames or slot-filler
representations (Minsky, 1975). Each node is
treated as a variable, with an associated list of
possible values. For instance a node
representing &amp;quot;disease severity&amp;quot; might include
the possible values {&amp;quot;severe&amp;quot;, &amp;quot;moderate&amp;quot;,
&amp;quot;mild&amp;quot;}. Each value has a probability, either
assigned or inferred, of being the true value of
that node.
In addition to providing a framework
for representation, a BN is also a probabilistic
inference engine. The probability of each
possible value of a node is conditioned on the
probabilities of the values of neighboring nodes,
through a training process that learns a Bayesian
joint probability function from a set of training
cases. After a BN is trained, a node can be
assigned a value by setting the probability of
that value to 1, and the probabilities of the
alternate values to 0. This results in a cascading
update of the value probabilities in all
unassigned nodes, in effect predicting what the
values of the unassigned nodes should be, given
the initial assignments. The sum of the
probabilities for the values of a given node is
constrained to equal 1, making the values
mutually exclusive, and reflecting uncertainty if
more than one value has a nonzero probability.
Please note that in this paper, &amp;quot;BN instance&amp;quot;
refers to the state of a BN after assignments
have been made.
A training case for a BN is a list of node
/ value assignments. For instance, consider a
simple BN for chest anatomy phrases, as shown
in Figure 1.
UMLS metathesaurus. By convention, concept
names in M+ are preceded with an asterisk.
A medical domain is represented in M+
as a network of BNs, with word-level and lower
concept-level BNs providing input to higher
concept-level BNs. Figure 2 shows a partial
view of the network of BNs used to model the
M+ Head CT (Computerized Tomography)
domain, instantiated with the phrase &amp;quot;temporal
subdural hemorrhage&amp;quot;. Each BN instance is
shown with a list of nodes and most probable
values. Note that input nodes of higher BNs in
this model have the same name as, and take
input from, the summary nodes of lower BNs.
Word level BNs have input nodes named
&amp;quot;head&amp;quot;, &amp;quot;mod1&amp;quot; and &amp;quot;mod2&amp;quot;, corresponding to
the syntactic head and modifiers of a phrase.
Each node in a BN has a distinguished &amp;quot;null&amp;quot;
value, whose meaning is that no information
relevant to that node, explicit or inferable, is
present in the represented phrase.
</bodyText>
<figureCaption confidence="0.988318">
Figure 1. BN for simple chest anatomy phrases.
</figureCaption>
<bodyText confidence="0.999782">
A training case for this BN applied to
the phrase &amp;quot;right upper lobe&amp;quot; could be:
</bodyText>
<equation confidence="0.80888125">
side=right
verticality=upper
location=lobe
interpretation= *right-upper-lobe
</equation>
<bodyText confidence="0.926264">
In the context of the Bayesian learning,
this case has an effect similar to a production
rule which states &amp;quot;If you find the words &apos;right&apos;,
&apos;upper&apos; and &apos;lobe&apos; together in a phrase, infer the
</bodyText>
<footnote confidence="0.7795083">
meaning *right-upper-lobe&amp;quot;. After training on
this case, assigning one or more values from this
case would increase the probabilities of the
other values; for instance assigning side=
&amp;quot;right&amp;quot; would increase the probability of the
value interpretation= *right-upper-lobe.
Interpretive concepts such as *right-
upper-lobe are atomic symbols which are either
invented by the human trainer, or else obtained
from a medical knowledge database such as the
</footnote>
<figureCaption confidence="0.9709065">
Figure 2. Network of M+ BNs, applied to
&amp;quot;temporal subdural hemorrhage&amp;quot;.
</figureCaption>
<bodyText confidence="0.9996628">
One way in which M+ differs from its
predecessor SymText (Koehler, 1998) is in the
size and modularity of its semantic BNs. The
SymText BNs group observation and disease
concepts together with state (&amp;quot;present&amp;quot;,
&amp;quot;absent&amp;quot;), change-of-state (&amp;quot;old&amp;quot;, &amp;quot;chronic&amp;quot;),
anatomic location and other concept types. M+
trades the inferential advantages of such
monolithic BNs for the modularity and
composability of smaller BNs such as those
shown in figure 2. Figure 3 shows a single
instance of the SymText Chest Radiology
Findings BN, instantiated with the sentence
&amp;quot;There is dense infiltrative opacity in the right
upper lobe&amp;quot;.
</bodyText>
<figure confidence="0.965619966666666">
*observations : *localized upper lobe infiltrate (0.888)
*state : *present (0.989)
state term : null (0.966)
*topic concept : *poorly-marginated opacity (0.877)
topic term : opacity (1.0)
topic modifier : infiltrative (1.0)
*measurement concept : *null (0.999)
measurement term : null (0.990)
first value : null (0.998)
second value : null (0.999)
values link : null (0.999)
size descriptor : null (0.999)
*tissue concept : *lung parenchyma (0.906)
tissue term : alveolar (1.0)
*severity concept : *high severity (0.893)
severity term : dense (1.0)
*anatomic concept : *right upper lobe (0.999)
*anatomic link concept : *involving (1.0)
anatomic link term : in (1.0)
anatomic location term : lobe (1.0)
anatomic location modifier : null (0.999)
anatomic modifier side : right (1.0)
anatomic modifier superior/inferior : upper (1.0)
anatomic modifier lateral/medial : null (0.999)
anatomic modifier anterior/posterior : null (0.999)
anatomic modifier central/peripheral : null (0.955)
*change concept : *null (0.569)
change with time : null (0.567)
change degree : null (0.904)
change quality : null (0.923)
</figure>
<figureCaption confidence="0.997698">
Figure 3. SymText BN instantiation.
</figureCaption>
<subsectionHeader confidence="0.995314">
2.2 Parse-Driven BN Instantiation
</subsectionHeader>
<bodyText confidence="0.999977454545455">
M+ BNs are instantiated as part of the
syntactic parse process. M+ syntactic and
semantic analyses are interleaved, in contrast
with NLP systems that perform semantic
analysis after the parse has finished.
M+ uses a bottom-up chart parser, with
a context free grammar (CFG). As a word such
as &amp;quot;right&amp;quot; is recognized by the parser, a word-
level phrase object is created and a BN instance
containing the assignment side= &amp;quot;right&amp;quot; is
attached to that phrase. As larger grammatical
patterns are recognized, the BN instances
attached to subphrases within those patterns are
unified and attached to the new phrases, as
described in section 3. The result of this
process is a set of completed BN instances, as
illustrated in figure 2. Each BN instance is a
template containing word and concept-level
value assignments, and the interpretive concepts
inferred from those assignments. The templates
themselves are nested in a symbolic expression,
as described in section 2.3, to facilitate
composing multiple BN instances in
representations of arbitrary complexity.
Each phrase recognized by the parser is
assigned a probability, based on a weighted sum
of the joint probabilities of its associated BN
instances, and adjusted for various syntactic and
semantic constraint violations. Phrases are
processed in order of probability; thus the parse
involves a semantically-guided best-first search.
Syntactic and semantic analysis in M+
are mutually constraining. If a grammatically
possible phrase is uninterpretable, i.e. if its
subphrase interpretations cannot be unified, it is
rejected. If the interpretation has a low
probability, the phrase is less likely to appear in
the final parse tree. On the other hand,
interpretations are constructed as phrases are
recognized. The exception to this rule is when
an ungrammatical fragment of text is
encountered. M+ then uses a semantically-
guided phrase repair procedure not described in
this paper.
</bodyText>
<subsectionHeader confidence="0.997637">
2.3 The M+ Abstract Semantic Language
</subsectionHeader>
<bodyText confidence="0.991277184782609">
The probabilistic reasoning afforded by BNs is
superior to classical logic in important ways
(Pearl, 1988). However, BNs are limited in
expressive power relative to first-order logics
(Koller and Pfeffer, 1997), and commercially
available implementations lack the flexibility of
symbolic languages. Friedman et al have made
considerable headway in giving BNs many
useful characteristics of first order languages, in
what they call probabilistic relational models, or
PRMs (e.g. Friedman at al. 1999).
While we are waiting for industry-
standard PRMs, we have tried to make our
semantic BNs more useful by combining them
with a first-order language, called the M+
Abstract Semantic Language (ASL),
implemented within M+. Specifically, BNs are
treated as object types within the ASL. There is
a &amp;quot;chest anatomy&amp;quot; type, for instance, and a
&amp;quot;chest radiology findings&amp;quot; type, corresponding
to BNs of those same names. The interpretation
of a phrase is an expression in the ASL,
containing predicates that state the relation of
BN instances to one another, and to the phrase
they describe. For instance, the interpretation of
&amp;quot;hazy right lower lobe opacity&amp;quot; could be the
expression
(and (head-of #phrase1 #find1)
(located-at #find1 #loc1))
where #phrase1 identifies a syntactic phrase
object, and #find1 and #loc1 are tokens
representing instances of the findings BN
(instanced with the words &amp;quot;hazy&amp;quot; and &amp;quot;opacity&amp;quot;)
and the anatomic BN (instanced with &amp;quot;right&amp;quot;,
&amp;quot;lower&amp;quot; and &amp;quot;lobe&amp;quot;), respectively. The relation
&apos;head-of&apos; denotes that the findings BN is the
main or &amp;quot;head&amp;quot; BN for that phrase. Conversely,
&amp;quot;hazy right lower lobe opacity&amp;quot; can be thought
of as a findings-type phrase, with an anatomic-
type modifier.
This expression captures the abstract or
&amp;quot;skeletal&amp;quot; structure of the interpretation, while
the BN instances contain the details and specific
inferences. One can think of the meaning of an
expression like (located-at #find1 #loc1) in
abstract terms, e.g. &amp;quot;some-finding located-at
some-location&amp;quot;. Alternatively, the meaning of a
BN token might be thought of as the most
probable interpretive concept within that BN
instance. In this case, (located-at #find1 #loc1)
could mean &amp;quot;*localized-infiltrate located-at
*left-lower-lobe&amp;quot;.
Because the object types in the ASL are
the abstract concept types represented by the
BNs, semantic rules formulated in this language
constitute an &amp;quot;abstract semantic grammar&amp;quot;
(ASG). The ASG recognizes patterns of
semantic relations among the BNs, and supports
analysis and inference based on those patterns.
It also permits rule-based control over the
creation, instantiation, and use of the BNs,
including defining pathways for information
sharing among BNs using virtual evidence
(Pearl, 1988).
One use of the ASG is in post-parse
processing of interpretations. After the M+
parser has constructed an interpretation, post-
parse ASG productions may augment or alter
this interpretation. One rule instructs &amp;quot;If two
pathological conditions exist in a &apos;consistent-
with&apos; relation, and the first condition has a state
modifier (i.e. *present or *absent), and the
second condition does not, apply the first
condition&apos;s state to the second condition&amp;quot;.
For instance, in the ambiguous sentence
&amp;quot;There is no opacity consistent with
pneumonia&amp;quot;, if the parser doesn&apos;t correctly
determine the scope of &amp;quot;no&amp;quot;, it may produce the
an interpretation in which *pneumonia lacks a
state modifier, and is therefore inferred (by
default) to be present. This rule correctly
attaches (state-of *pneumonia *absent) to this
interpretation.
One important consequence of the
modularity of the M+ BNs, and of the ability to
nest them within the ASL, is that M+ can
compose BN instances in expressions of
arbitrary complexity. For instance, it is
straightforward to represent the multiple
anatomic concepts in the phrase &amp;quot;opacity in the
inferior segment of the left upper lobe, adjacent
to the heart&amp;quot;:
</bodyText>
<equation confidence="0.964606">
(and (head-of #phrase1 #find1)
(located-at #find1 #anat1)
(qualified-by #anat1 #anat2)
(adjacent-to #anat1 #anat3))
</equation>
<bodyText confidence="0.9999518">
where the interpretive concepts of #anat1,
#anat2 and #anat3 are *left-upper-lobe,
*inferior-segment, and *heart, respectively.
The set of binary predicates that
constitutes a phrase interpretation in M+ forms a
directed acyclic graph; thus we can refer to the
interpretation as an interpretation graph. The
interpretation graph of a new phrase is formed
by unifying the graphs of its subphrases, as
described in section 3.
</bodyText>
<subsectionHeader confidence="0.999469">
2.4 Advantages of Bayesian Networks
</subsectionHeader>
<bodyText confidence="0.999805285714286">
As mentioned, a BN training case bears a
similarity to a production rule. It would be
straightforward to implement the training cases
as a set of rules, and apply them to text analysis
using a deductive reasoning engine. However,
Bayesian reasoning has important advantages
over first order logic, including:
</bodyText>
<listItem confidence="0.953085409090909">
1- BNs are able to respond gracefully to
input &amp;quot;noise&amp;quot;. A semantic BN may produce
reasonable inferences from phrasal patterns that
only partially match any given training case, or
that overlap different cases, or that contain
words in an unexpected order. For instance,
having trained on multi-word phrases containing
&amp;quot;opacity&amp;quot;, the single word &amp;quot;opacity&amp;quot; could raise
the probabilities of several interpretations such
as *localized-infiltrate and *parenchymal-
abnormality, both of which are reasonable
hypotheses for the underlying cause of opacity
on a chest x-ray film.
2- Bayesian inference works bi-
directionally; i.e. it is abductive as well as
deductive. If instead of assigning word-level
nodes, one assigns the value of the summary
node, the probability of word values having a
high correlation with that summary will
increase. For instance, assigning the value
*localized-infiltrate will raise the probability
that the topic word is &amp;quot;opacity&amp;quot;.
</listItem>
<bodyText confidence="0.999775166666667">
Bi-directional inference provides a
means for modeling the effects of lexical
context. A value assignment made to one word
node can alter value probabilities at unassigned
word nodes, in a path of inference that passes
through the connecting concept nodes. For
instance, if a BN were trained on &amp;quot;right upper
lobe&amp;quot; and &amp;quot;left upper lobe&amp;quot;, but had never seen
the term &amp;quot;bilateral&amp;quot;, applying the BN to the
phrase &amp;quot;bilateral upper lobes&amp;quot; would increase
the probabilities of both &amp;quot;left&amp;quot; and &amp;quot;right&amp;quot;,
suggesting that &amp;quot;bilateral&amp;quot; is semantically
similar to &amp;quot;left&amp;quot; and &amp;quot;right&amp;quot;. This is one
approach to guessing the node assignments of
unknown words, a step in the direction of
automated learning of new training cases.
Similarly, if the system encounters a
phrase with a misspelling such as &amp;quot;rght upper
lobe&amp;quot;, by noting the orthographic similarity of
&amp;quot;rght&amp;quot; to &amp;quot;right&amp;quot; and the fact that &amp;quot;right&amp;quot; is
highly predicted from surrounding words, it can
determine that &amp;quot;rght&amp;quot; is a misspelling of &amp;quot;right&amp;quot;.
The spell checker currently used by M+
employs this technique.
</bodyText>
<sectionHeader confidence="0.8243005" genericHeader="method">
3 Generating Interpretation
Graphs
</sectionHeader>
<bodyText confidence="0.999994914285714">
As mentioned, in M+ the interpretation graph of
a phrase is created by unifying the graphs of its
child phrases. High joint probabilities in the
resulting BN instances are one source of
evidence that the words thus brought together
exist in the expected semantic pattern.
However, corroborating evidence must be
sought in the syntax of the text. Words which
appear together in a training phrase may not be
in that same relation in a given text. For
instance, &amp;quot;no&amp;quot; and &amp;quot;pneumonia&amp;quot; support
different conclusions in &amp;quot;no evidence of
pneumonia&amp;quot; and &amp;quot;patient has pneumonia with no
apparent complicating factors&amp;quot;. M+ therefore
only attempts to unify sub-interpretations that
appear, on syntactic grounds, to be talking about
the same things. This is less constraining than
production rules that look for words in a
specific order, but more constraining than
simply pulling key words out of a string of text.
The following are examples of rules
used to guide the unification of ASL
interpretation graphs. For convenience, several
shorthand functional notations are used: If P
represents a phrase on the parse chart, root-
bn(P) represents the root or head BN instance in
P&apos;s interpretation graph, and type-of(root-bn(P))
is the BN type of root-bn(P). If A and B are
sibling child phrases of parent phrase C, then C
= parent-phrase(A,B). Note that for
convenience, BN instances in the interpretation
graphs in Figures 4 - 6 are represented
alternately as the words slotted in those
instances, and as the most probable interpretive
concepts inferred by those instances.
</bodyText>
<subsectionHeader confidence="0.987442">
3.1 Same-type Unification
</subsectionHeader>
<bodyText confidence="0.985155909090909">
If phrase A syntactically modifies phrase B,
then M+ assumes that some semantic relation
exists between A and B. The nature of that
relation is partly determinable from type-
of(root-bn(A)) and type-of(root-bn(B)). If type-
of(root-bn(A)) = type-of(root-bn(B)), that
relation is simply one where root-bn(A) and
root-bn(B) are partial descriptions of a single
concept. If root-bn(A) and root-bn(B) are
unifiable, M+ composes their input to form
root-bn(parent-phrase(A,B)).
If in addition there are two unifiable
same-type BN instances X and Y linked to root-
bn(A) and root-bn(B) respectively, via arcs of
the same name, then X and Y also describe a
single concept, and the arcs describe a single
relationship. For instance, if X and Y describe
the anatomic locations of root-bn(A) and root-
bn(B), and if root-bn(A) and root-bn(B) are
partial descriptions of a single &amp;quot;finding&amp;quot;, then X
and Y are partial descriptions of a single
anatomic location, and ought to be unified.
</bodyText>
<figureCaption confidence="0.976821">
Figure 4: Same-type unification
</figureCaption>
<bodyText confidence="0.999919166666667">
In figure 4, in the Chest X-ray domain,
the phrase &amp;quot;bilateral hazy lower lobe opacity&amp;quot; is
interpreted by unifying the interpretations of its
subphrases &amp;quot;bilateral hazy&amp;quot; and &amp;quot;lower lobe
opacity&amp;quot;. Note that without any corresponding
syntactic transformation, this rule brings about a
&amp;quot;virtual transformation&amp;quot;, whereby words are
grouped together within BN instances in a
manner that reflects the conceptual structure of
the text. In this example &amp;quot;bilateral hazy lower
lobe opacity&amp;quot; is treated as (&amp;quot;bilateral lower
lobe&amp;quot;) (&amp;quot;hazy opacity&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.996742">
3.2 Different-type Unification
</subsectionHeader>
<bodyText confidence="0.99886165">
If phrase A syntactically modifies phrase B, and
type-of(root-bn(A)) &lt;&gt; type-of(root-bn(B)),
then root-bn(A) and root-bn(B) represent
different concepts within some semantic
relation. M+ uses the ASG to identify that
relation and to add it to the interpretation graph
in the form of a path of named arcs connecting
root-bn(A) and root-bn(B). This path may
include implicit connecting BN instances.
For instance, to interpret &amp;quot;subdural
hemorrhage&amp;quot; in the Head CT domain, M+
attempts to unify the graphs for the subphrases
&amp;quot;subdural&amp;quot; and &amp;quot;hemorrhage&amp;quot;, where type-
of(root-bn(&amp;quot;subdural&amp;quot;)) = location, and type-
of(root-bn(&amp;quot;hemorrhage&amp;quot;)) = topic. M+
identifies the connecting path for these two
types as shown in figure 2, and adds that path to
the interpretation as shown in figure 5. Note
that this path contains instances of the
&amp;quot;observation&amp;quot; and &amp;quot;anatomy&amp;quot; BN types.
</bodyText>
<figureCaption confidence="0.963434">
Figure 6: Grammar rule - based unification.
</figureCaption>
<sectionHeader confidence="0.994439" genericHeader="method">
4 M+ Implementation
</sectionHeader>
<bodyText confidence="0.999944875">
M+ is written in Common Lisp, with some C
routines for BN access. The M+ architecture
consists of six basic components: The parser,
concept space, rule base, lexicon, ASL inference
engine, and Bayesian network component.
As mentioned, the parser is an
implementation of a bottom up chart parser with
context free grammar.
The concept space is a table of symbols
representing types, objects and relations within
the ASL. These include BN names, BN node
value names, inter-BN relation names, and a
small ontology of useful concepts such as those
related to time.
The rule base contains rules, which
comprise the syntactic grammar and ASG.
The lexicon is a table of Lisp-readable
word information entries, obtained in part from
the UMLS Specialist Lexicon.
The ASL inference engine combines
symbolic unification with backward-chaining
inference. It can be used to match an ASG
pattern against an interpretation graph, and to
perform tests associated with grammar rules.
</bodyText>
<figureCaption confidence="0.98068">
Figure 5. Different-type unification.
</figureCaption>
<subsectionHeader confidence="0.987559">
3.3 Grammar Rule Based Unification The Bayesian network component utilizes
</subsectionHeader>
<bodyText confidence="0.999963666666667">
Individual grammar rules in M+ can recognize
semantic relations, and add connecting arcs to
the interpretation graph. For instance, M+ has a
rule which recognizes findings-type phrases
connected with strings of the &amp;quot;suggesting&amp;quot;
variety, and connects their graphs with a
&apos;consistent-with&apos; arc. This is used to interpret
&amp;quot;opacity suggesting possible infarct&amp;quot; in the
Head CT domain, as shown in figure 6.
the Norsys Netica(TM) API, and includes a set of
Lisp and C language routines for instantiating
and retrieving probabilities from BNs.
</bodyText>
<sectionHeader confidence="0.99061" genericHeader="method">
5 Training M+
</sectionHeader>
<bodyText confidence="0.999981340909091">
Porting M+ to a new medical domain involves
gathering a corpus of training sentences for the
domain, using the Netica(TM) graphical interface
to create domain-specific BNs, and generating
training cases for the new BNs.
The most time-consuming task is the
creation of training cases. We have developed a
prototype version of a Web-based tool which
largely automates this task. The basic idea is to
enable M+ to guess the BN value assignments
of unknown words, then use it to parse phrases
similar to phrases already seen. For instance,
having been trained on the phrase &amp;quot;right upper
lobe&amp;quot;, the parser is able to produce reasonable
parses, with some &amp;quot;guessed&amp;quot; value assignments,
for &amp;quot;left upper lobe&amp;quot;, &amp;quot;right middle lobe&amp;quot;,
&amp;quot;bilateral lungs&amp;quot;, etc. The BN assignments
produced by the parse are output as tentative
new cases to be reviewed and corrected by the
human trainer.
The training process begins with an
initial set of interpreted &amp;quot;seed&amp;quot; phrases. From
this set, the tool can apply the parser to phrases
similar to this set, and so semi-automatically
traverse ever widening semantically contiguous
areas within the space of corpus phrases. As the
training proceeds, the role of the human trainer
increasingly becomes one of providing
correction and interpretations for semantic
patterns the system is increasingly able to
discover on its own.
To parse phrases containing unknown
words, M+ uses a technique based on a variation
of the vector space model of lexical semantic
similarity (Manning and Schutze, 1999). As
M+ encounters an unknown word, it gathers a
list of training corpus words judged similar to
that word, as predicted by the vector space
measure. It then identifies BN nodes whose
known values significantly overlap with this list,
and provisionally assigns the unknown word as
a new value for those nodes. The assignment
resulting in the best parsetree is selected for the
new provisional training case.
</bodyText>
<sectionHeader confidence="0.999594" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999875909090909">
M+ was evaluated for the extraction of
American College of Radiology (ACR)
utilization review codes from Head CT reports
(Fiszman, 2002). The ACR codes compare the
outcome in a report with the suspected diagnosis
provided by emergency department physicians.
If the outcome relates to the suspected diagnosis
then the report should be encoded as positive
(P). If the outcome is negative and does not
relate to the suspected diagnosis then the report
should be encoded as negative (N). In order to
extract those ACR codes we trained M+ to
extract eleven broad disease concepts, then
inferred the ACR codes based on the application
of a rule to the M+ output: If any of the
concepts was present, the report was considered
positive, else the report was considered
negative.
Twenty six hundred head CT scan
reports were used for this evaluation. Six
hundred reports were randomly selected for
testing, and the rest were used to train M+ in
this domain. The performance of M+ on this
task was measured against that of four board
certified physicians, using a gold standard based
on majority vote, as described in (Fiszman,
2002). For each subject we calculated recall,
precision and specificity with their respective 95
% confidence intervals for the capture of ACR
utilization codes.
From 600 head CT reports, 67 were
judged to be positive (P) by the gold standard
physicians and 534 were judged to be negative
(N). Therefore the positive rate for head CT in
this sample was 11%. Recall, precision and
specificity for every subject are presented with
their respective 95% confidence intervals in
Table 1. The physicians had an average recall of
88% (CI, 84% to 92.%), an average precision of
86% (CI, 81% to 90%), and average specificity
of 98% (CI, 97% to 99%). M+ had recall of
87% (CI, 78% to 95%), precision of 85% (CI,
77% to 94%) and specificity of 98% (CI, 97%
to 99).
</bodyText>
<tableCaption confidence="0.995975">
Table 1. Results of ACR utilization code study.
</tableCaption>
<subsectionHeader confidence="0.435629">
Subject Recall Specificity Precision
</subsectionHeader>
<bodyText confidence="0.219332">
Physician1 0.83 0.99 0.91
</bodyText>
<equation confidence="0.308480111111111">
(0.74-0.92) (0.98-1.00) (0.84-0.99)
Physician2 0.88 0.98 0.84
(0.81-0.97) (0.97-0.99) (0.75-0.93)
Physician3 0.93 0.98 0.86
(0.87-1.00) (0.97-0.99) (0.78-0.95)
Physician4 0.88 0.97 0.81
(0.96-0.99) (0.96-0.99) (0.71-0.90)
M+ 0.87 0.98 0.85
(0.78-0.95) (0.97-0.99) (0.77-0.94)
</equation>
<bodyText confidence="0.9996445">
The results on Head CT reports are
encouraging, but there are limitations. We only
evaluated 600 reports, because it&apos;s very hard to
get physicians to produce gold standard data for
medical reports. The prevalence of positive
reports is only 11% and reflects the fact that the
individual brain conditions have very low
prevalence.
</bodyText>
<sectionHeader confidence="0.998728" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999957">
M+ and its predecessors have demonstrated that
BNs provide a useful semantic model for
medical text processing. In practice, a medical
NLP system will frequently encounter missing
and unknown words, unknown and
ungrammatical phrase structures, and
telegraphic usages. Knowledge databases will
be imperfect and incomplete. Using BNs for
semantic representation brings a noise-tolerant,
partial match-tolerant, context-sensitive
character to the recognition of semantic
patterns, and to relevant inferences based on
those patterns. In addition, BNs can be used to
guess the semantic types of unknown words,
providing a basis for bootstrapping the system&apos;s
semantic knowledge.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99889525">
Many thanks to Wendy W. Chapman for her
advice and input in this paper, and her efforts to
make M+ a useful addition to the RODS project
at the University of Pittsburgh.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681736111111">
Chapman W., Christensen L. M., Wagner M., Haug
P. J., Ivanov O., Dowling J. N., Olszewski R. T.
2002. Syndromic Detection from Free-text
Triage Diagnoses: Evaluation of a Medical
Language Processing System before Deployment
in the Winter Olympics. Proc AMIA Symp.
(submitted).
Chomsky, Noam. 1965. Aspects of the theory of
syntax. Special technical report (Massachusetts
Institute of Technology, Research Laboratory of
Electronics); no. 11. Cambridge, MA: MIT Press.
Fiszman M., Blatter D.D., Christensen L.M., Oderich
G., Macedo T., Eidelwein A.P., Haug P.J. 2002.
Utilization review of head CT scans: value of a
medical language processing system. American
Journal of Roentgenology (AJR). (submitted)
Friedman C, Alderson PO, Austin JH, Cimino JJ,
Johnson SB. 1994, A general natural-language
text processor for clinical radiology. J Am Med
Inform Assoc. Mar-Apr;1(2) pp. 161-74.
Friedman N., Getoor L., Koller D. and Pfeffer A.
1999. Learning Probabilistic Relational Models.
Proceedings of the 16th International Joint
Conference on Artificial Intelligence (IJCAI):
pp. 1300-1307.
Haug P. J., Christensen L., Gundersen M., Clemons
B., Koehler S., Bauer K. 1997. A natural
language parsing system for encoding admitting
diagnoses. Proc AMIA Symp. 81: pp. 4-8.
Koehler, S. B. 1998. SymText: A natural language
understanding system for encoding free text
medical data. Ph.D. Dissertation, University of
Utah.
Koller D., and Pfeffer A. 1997. Object-Oriented
Bayesian Networks. Proceedings of the 13th
Annual Conference on Uncertainty in AI: pp.
302-313.
Lin R, Lenert L, Middleton B, Shiffman S. A free-
text processing system to capture physical
findings: Canonical Phrase Identification System
(CAPIS). Proc Annu Symp Comput Appl Med
Care. pp. 843-7.
Manning C. D. and Schutze H. 1999. Foundations of
Statistical Natural Language Processing. MIT
Press.
Minsky, M. 1975. A framework for representing
knowledge. In The Psychology of Human Vision,
ed. P. H. Winston, pp. 211-277. McGraw Hill.
Moore, R. C. 1989. Unification-based Semantic
Interpretation. Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics, pp33-41.
Pearl, Judea. 1988. Probabilistic inference in
intelligent systems. Networks of plausible
inference: Morgan Kaufmann.
Ranum D.L. 1989. Knowledge-based understanding
of radiology text. Comput Methods Programs
Biomed. Oct-Nov;30(2-3) pp.209-215.
Romacker, Martin and Hahn, Udo. 2000. An
empirical assessment of semantic interpretation.
ANLP/NAACL 2000 -- Proceedings of the 6th
Applied Natural Language Processing
Conference &amp; the 1st Conference of the North
American Chapter of the Association for
Computational Linguistics. pp. 327-334.
Schank, R.C. and R. Abelson. 1997. Scripts, Plans,
Goals, and Understanding. Hillsdale, NJ:
Lawrence Erlbaum.
Smart, J. F. and M. Roux. 1995. A model for
medical knowledge representation application to
the analysis of descriptive pathology reports.
Methods Inf Med. Sep;34(4) pp. 352-60.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.947499666666667">Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain, Philadelphia, July 2002, pp. 29-36. Association for Computational Linguistics.</note>
<title confidence="0.997052">MPLUS: A Probabilistic Medical Language Understanding System</title>
<author confidence="0.999975">Lee M Christensen</author>
<author confidence="0.999975">Peter J Haug</author>
<author confidence="0.999975">Marcelo Fiszman</author>
<affiliation confidence="0.93525">Department of Medical Informatics, LDS Hospital/University of Utah, Salt Lake City, UT</affiliation>
<email confidence="0.99933">E-mail:ldlchris@ihc.com,ldphaug@ihc.com,ldmfiszm@ihc.com</email>
<abstract confidence="0.993587476874005">This paper describes the basic philosophy and implementation of MPLUS (M+), a robust medical text analysis tool that uses a semantic model based on Bayesian Networks (BNs). BNs provide a concise and useful formalism for representing semantic patterns in medical text, and for recognizing and reasoning over those patterns. BNs are noise-tolerant, and facilitate the training of M+. In the field of medical informatics, computerized tools are being developed that depend on databases of clinical information. These include alerting systems for improved patient care, data mining systems for quality assurance and research, and diagnostic systems for more complex medical decision support. These systems require data that is appropriately structured and coded. Since a large portion of the information stored in patient databases is in the form of free text, manually coding this information in a format accessible to these tools can be time consuming and expensive. In recent years, natural language processing (NLP) methodologies have been studied as a means of automating this task. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegraphic or otherwise not well formed grammatically. Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary. Most NLP systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (Friedman et al., 1994), unificationbased semantics (Moore, 1989), and description logics (Romacker and Hahn, 2000). M+ and its predecessors employ Bayesian Networks (Pearl, 1988), a methodology outside this tradition. This study discusses the philosophy and implementation of M+, and attempts to show how Bayesian Networks can be useful in medical text analysis. M+ Semantic Model 2.1 Semantic Bayesian Networks M+ uses Bayesian Networks (BNs) to represent the basic semantic types and relations within a medical domain such as chest radiology reports. M+ BNs are structurally similar to semantic networks, in that they are implemented as directed acyclic graphs, with nodes representing word and concept types, and links representing relations between those types. BNs also have a character as frames or slot-filler representations (Minsky, 1975). Each node is treated as a variable, with an associated list of possible values. For instance a representing &amp;quot;disease severity&amp;quot; might include the possible values {&amp;quot;severe&amp;quot;, &amp;quot;moderate&amp;quot;, &amp;quot;mild&amp;quot;}. Each value has a probability, either assigned or inferred, of being the true value of that node. In addition to providing a framework for representation, a BN is also a probabilistic inference engine. The probability of each possible value of a node is conditioned on the probabilities of the values of neighboring nodes, through a training process that learns a Bayesian joint probability function from a set of training cases. After a BN is trained, a node can be assigned a value by setting the probability of that value to 1, and the probabilities of the alternate values to 0. This results in a cascading update of the value probabilities in all unassigned nodes, in effect predicting what the values of the unassigned nodes should be, given the initial assignments. The sum of probabilities for the values of a given node is constrained to equal 1, making the values mutually exclusive, and reflecting uncertainty if more than one value has a nonzero probability. Please note that in this paper, &amp;quot;BN instance&amp;quot; refers to the state of a BN after assignments have been made. A training case for a BN is a list of node / value assignments. For instance, consider a simple BN for chest anatomy phrases, as shown in Figure 1. UMLS metathesaurus. By convention, concept names in M+ are preceded with an asterisk. A medical domain is represented in M+ as a network of BNs, with word-level and lower concept-level BNs providing input to higher concept-level BNs. Figure 2 shows a partial view of the network of BNs used to model the M+ Head CT (Computerized Tomography) domain, instantiated with the phrase &amp;quot;temporal subdural hemorrhage&amp;quot;. Each BN instance is shown with a list of nodes and most probable values. Note that input nodes of higher BNs in this model have the same name as, and take input from, the summary nodes of lower BNs. Word level BNs have input nodes named &amp;quot;head&amp;quot;, &amp;quot;mod1&amp;quot; and &amp;quot;mod2&amp;quot;, corresponding to the syntactic head and modifiers of a phrase. Each node in a BN has a distinguished &amp;quot;null&amp;quot; value, whose meaning is that no information relevant to that node, explicit or inferable, is present in the represented phrase. Figure 1. BN for simple chest anatomy phrases. A training case for this BN applied to the phrase &amp;quot;right upper lobe&amp;quot; could be: side=right verticality=upper location=lobe interpretation= *right-upper-lobe In the context of the Bayesian learning, this case has an effect similar to a production which states you find the words &apos;right&apos;, &apos;upper&apos; and &apos;lobe&apos; together in a phrase, infer the After training on this case, assigning one or more values from this case would increase the probabilities of the values; for instance assigning increase the probability of the Interpretive concepts such as *rightupper-lobe are atomic symbols which are either invented by the human trainer, or else obtained from a medical knowledge database such as the Figure 2. Network of M+ BNs, applied to &amp;quot;temporal subdural hemorrhage&amp;quot;. One way in which M+ differs from its predecessor SymText (Koehler, 1998) is in the size and modularity of its semantic BNs. The SymText BNs group observation and disease concepts together with state (&amp;quot;present&amp;quot;, &amp;quot;absent&amp;quot;), change-of-state (&amp;quot;old&amp;quot;, &amp;quot;chronic&amp;quot;), anatomic location and other concept types. M+ trades the inferential advantages of such monolithic BNs for the modularity and composability of smaller BNs such as those shown in figure 2. Figure 3 shows a single instance of the SymText Chest Radiology Findings BN, instantiated with the sentence &amp;quot;There is dense infiltrative opacity in the right upper lobe&amp;quot;. *observations : *localized upper lobe infiltrate (0.888) *state : *present (0.989) state term : null (0.966) *topic concept : *poorly-marginated opacity (0.877) topic term : opacity (1.0) topic modifier : infiltrative (1.0) *measurement concept : *null (0.999) measurement term : null (0.990) first value : null (0.998) second value : null (0.999) values link : null (0.999) size descriptor : null (0.999) *tissue concept : *lung parenchyma (0.906) tissue term : alveolar (1.0) *severity concept : *high severity (0.893) severity term : dense (1.0) *anatomic concept : *right upper lobe (0.999) *anatomic link concept : *involving (1.0) anatomic link term : in (1.0) anatomic location term : lobe (1.0) anatomic location modifier : null (0.999) anatomic modifier side : right (1.0) anatomic modifier superior/inferior : upper (1.0) anatomic modifier lateral/medial : null (0.999) anatomic modifier anterior/posterior : null (0.999) anatomic modifier central/peripheral : null (0.955) *change concept : *null (0.569) change with time : null (0.567) change degree : null (0.904) change quality : null (0.923) Figure 3. SymText BN instantiation. 2.2 Parse-Driven BN Instantiation M+ BNs are instantiated as part of the syntactic parse process. M+ syntactic and semantic analyses are interleaved, in contrast with NLP systems that perform semantic analysis after the parse has finished. M+ uses a bottom-up chart parser, with a context free grammar (CFG). As a word such as &amp;quot;right&amp;quot; is recognized by the parser, a wordlevel phrase object is created and a BN instance the assignment &amp;quot;right&amp;quot; attached to that phrase. As larger grammatical patterns are recognized, the BN instances attached to subphrases within those patterns are unified and attached to the new phrases, as described in section 3. The result of this process is a set of completed BN instances, as illustrated in figure 2. Each BN instance is a template containing word and concept-level value assignments, and the interpretive concepts inferred from those assignments. The templates themselves are nested in a symbolic expression, as described in section 2.3, to facilitate composing multiple BN instances in representations of arbitrary complexity. Each phrase recognized by the parser is assigned a probability, based on a weighted sum of the joint probabilities of its associated BN instances, and adjusted for various syntactic and semantic constraint violations. Phrases are processed in order of probability; thus the parse involves a semantically-guided best-first search. Syntactic and semantic analysis in M+ are mutually constraining. If a grammatically possible phrase is uninterpretable, i.e. if its subphrase interpretations cannot be unified, it is rejected. If the interpretation has a probability, the phrase is less likely to appear in the final parse tree. On the other hand, interpretations are constructed as phrases are recognized. The exception to this rule is when an ungrammatical fragment of text is encountered. M+ then uses a semanticallyguided phrase repair procedure not described in this paper. 2.3 The M+ Abstract Semantic Language The probabilistic reasoning afforded by BNs is superior to classical logic in important ways (Pearl, 1988). However, BNs are limited in expressive power relative to first-order logics (Koller and Pfeffer, 1997), and commercially available implementations lack the flexibility of symbolic languages. Friedman et al have made considerable headway in giving BNs many useful characteristics of first order languages, in what they call probabilistic relational models, or PRMs (e.g. Friedman at al. 1999). While we are waiting for industrystandard PRMs, we have tried to make our semantic BNs more useful by combining them with a first-order language, called the M+ Abstract Semantic Language (ASL), implemented within M+. Specifically, BNs are treated as object types within the ASL. There is a &amp;quot;chest anatomy&amp;quot; type, for instance, and a &amp;quot;chest radiology findings&amp;quot; type, corresponding to BNs of those same names. The interpretation of a phrase is an expression in the ASL, containing predicates that state the relation of BN instances to one another, and to the phrase they describe. For instance, the interpretation of &amp;quot;hazy right lower lobe opacity&amp;quot; could be the expression (and (head-of #phrase1 #find1) (located-at #find1 #loc1)) where #phrase1 identifies a syntactic phrase object, and #find1 and #loc1 are tokens representing instances of the findings BN (instanced with the words &amp;quot;hazy&amp;quot; and &amp;quot;opacity&amp;quot;) and the anatomic BN (instanced with &amp;quot;right&amp;quot;, &amp;quot;lower&amp;quot; and &amp;quot;lobe&amp;quot;), respectively. The relation &apos;head-of&apos; denotes that the findings BN is the main or &amp;quot;head&amp;quot; BN for that phrase. Conversely, &amp;quot;hazy right lower lobe opacity&amp;quot; can be thought of as a findings-type phrase, with an anatomictype modifier. This expression captures the abstract or &amp;quot;skeletal&amp;quot; structure of the interpretation, while the BN instances contain the details and specific inferences. One can think of the meaning of an expression like (located-at #find1 #loc1) in abstract terms, e.g. &amp;quot;some-finding located-at some-location&amp;quot;. Alternatively, the meaning of a BN token might be thought of as the most probable interpretive concept within that BN instance. In this case, (located-at #find1 #loc1) could mean &amp;quot;*localized-infiltrate located-at *left-lower-lobe&amp;quot;. Because the object types in the ASL are the abstract concept types represented by the BNs, semantic rules formulated in this language constitute an &amp;quot;abstract semantic grammar&amp;quot; (ASG). The ASG recognizes patterns of semantic relations among the BNs, and supports analysis and inference based on those patterns. It also permits rule-based control over the creation, instantiation, and use of the BNs, including defining pathways for information sharing among BNs using virtual evidence (Pearl, 1988). One use of the ASG is in post-parse processing of interpretations. After the M+ parser has constructed an interpretation, postparse ASG productions may augment or alter interpretation. One rule instructs two pathological conditions exist in a &apos;consistentwith&apos; relation, and the first condition has a state *present or and the second condition does not, apply the first state to the second For instance, in the ambiguous sentence &amp;quot;There is no opacity consistent with pneumonia&amp;quot;, if the parser doesn&apos;t correctly determine the scope of &amp;quot;no&amp;quot;, it may produce the an interpretation in which *pneumonia lacks a state modifier, and is therefore inferred (by default) to be present. This rule correctly attaches (state-of *pneumonia *absent) to this interpretation. One important consequence of the modularity of the M+ BNs, and of the ability to nest them within the ASL, is that M+ can compose BN instances in expressions of arbitrary complexity. For instance, it straightforward to represent the multiple anatomic concepts in the phrase &amp;quot;opacity in the inferior segment of the left upper lobe, adjacent to the heart&amp;quot;: (and (head-of #phrase1 #find1) (located-at #find1 #anat1) (qualified-by #anat1 #anat2) (adjacent-to #anat1 #anat3)) where the interpretive concepts of #anat1, *inferior-segment, and *heart, respectively. The set of binary predicates that constitutes a phrase interpretation in M+ forms a directed acyclic graph; thus we can refer to the interpretation as an interpretation graph. The interpretation graph of a new phrase is formed by unifying the graphs of its subphrases, as described in section 3. 2.4 Advantages of Bayesian Networks As mentioned, a BN training case bears a similarity to a production rule. It would be straightforward to implement the training cases as a set of rules, and apply them to text analysis using a deductive reasoning engine. However, Bayesian reasoning has important advantages over first order logic, including: 1- BNs are able to respond gracefully to input &amp;quot;noise&amp;quot;. A semantic BN may produce reasonable inferences from phrasal patterns that only partially match any given training case, or that overlap different cases, or that contain words in an unexpected order. For instance, having trained on multi-word phrases containing &amp;quot;opacity&amp;quot;, the single word &amp;quot;opacity&amp;quot; could raise the probabilities of several interpretations such as *localized-infiltrate and *parenchymalabnormality, both of which are reasonable hypotheses for the underlying cause of opacity on a chest x-ray film. 2- Bayesian inference works bidirectionally; i.e. it is abductive as well as deductive. If instead of assigning word-level nodes, one assigns the value of the summary node, the probability of word values having a high correlation with that summary will increase. For instance, assigning the value *localized-infiltrate will raise the probability that the topic word is &amp;quot;opacity&amp;quot;. Bi-directional inference provides a means for modeling the effects of lexical context. A value assignment made to one word node can alter value probabilities at unassigned word nodes, in a path of inference that passes through the connecting concept nodes. For instance, if a BN were trained on &amp;quot;right upper lobe&amp;quot; and &amp;quot;left upper lobe&amp;quot;, but had never seen the term &amp;quot;bilateral&amp;quot;, applying the BN to the phrase &amp;quot;bilateral upper lobes&amp;quot; would increase the probabilities of both &amp;quot;left&amp;quot; and &amp;quot;right&amp;quot;, suggesting that &amp;quot;bilateral&amp;quot; is semantically similar to &amp;quot;left&amp;quot; and &amp;quot;right&amp;quot;. This is approach to guessing the node assignments of unknown words, a step in the direction of automated learning of new training cases. Similarly, if the system encounters a phrase with a misspelling such as &amp;quot;rght upper lobe&amp;quot;, by noting the orthographic similarity of &amp;quot;rght&amp;quot; to &amp;quot;right&amp;quot; and the fact that &amp;quot;right&amp;quot; is highly predicted from surrounding words, it can determine that &amp;quot;rght&amp;quot; is a misspelling of &amp;quot;right&amp;quot;. The spell checker currently used by M+ employs this technique. Interpretation Graphs As mentioned, in M+ the interpretation graph of a phrase is created by unifying the graphs of its child phrases. High joint probabilities in the resulting BN instances are one source of evidence that the words thus brought together exist in the expected semantic pattern. However, corroborating evidence must be sought in the syntax of the text. Words which appear together in a training phrase may not be in that same relation in a given text. For instance, &amp;quot;no&amp;quot; and &amp;quot;pneumonia&amp;quot; support conclusions in &amp;quot;noevidence of pneumonia&amp;quot;and &amp;quot;patient has pneumoniawith apparent complicating factors&amp;quot;. M+ therefore only attempts to unify sub-interpretations that appear, on syntactic grounds, to be talking about the same things. This is less constraining than production rules that look for words in a specific order, but more constraining than simply pulling key words out of a string of text. The following are examples of rules used to guide the unification of ASL interpretation graphs. For convenience, several shorthand functional notations are used: If P represents a phrase on the parse chart, rootbn(P) represents the root or head BN instance in P&apos;s interpretation graph, and type-of(root-bn(P)) is the BN type of root-bn(P). If A and B are sibling child phrases of parent phrase C, then C = parent-phrase(A,B). Note that convenience, BN instances in the interpretation graphs in Figures 4 - 6 are represented alternately as the words slotted in those instances, and as the most probable interpretive concepts inferred by those instances. 3.1 Same-type Unification If phrase A syntactically modifies phrase B, then M+ assumes that some semantic relation exists between A and B. The nature of that relation is partly determinable from typeof(root-bn(A)) and type-of(root-bn(B)). If typeof(root-bn(A)) = type-of(root-bn(B)), that relation is simply one where root-bn(A) and root-bn(B) are partial descriptions of a single concept. If root-bn(A) and root-bn(B) are unifiable, M+ composes their input to form root-bn(parent-phrase(A,B)). If in addition there are two unifiable same-type BN instances X and Y linked to rootbn(A) and root-bn(B) respectively, via arcs of the same name, then X and Y also describe a single concept, and the arcs describe a single relationship. For instance, if X and Y describe the anatomic locations of root-bn(A) and rootbn(B), and if root-bn(A) and root-bn(B) are partial descriptions of a single &amp;quot;finding&amp;quot;, then X and Y are partial descriptions of a single anatomic location, and ought to be unified. Figure 4: Same-type unification In figure 4, in the Chest X-ray domain, the phrase &amp;quot;bilateral hazy lower lobe opacity&amp;quot; is interpreted by unifying the interpretations of its subphrases &amp;quot;bilateral hazy&amp;quot; and &amp;quot;lower lobe opacity&amp;quot;. Note that without any corresponding syntactic transformation, this rule brings about a &amp;quot;virtual transformation&amp;quot;, whereby words are grouped together within BN instances in a manner that reflects the conceptual structure of the text. In this example &amp;quot;bilateral hazy lower lobe opacity&amp;quot; is treated as (&amp;quot;bilateral lower lobe&amp;quot;) (&amp;quot;hazy opacity&amp;quot;). 3.2 Different-type Unification If phrase A syntactically modifies phrase B, and type-of(root-bn(A)) &lt;&gt; type-of(root-bn(B)), then root-bn(A) and root-bn(B) represent different concepts within some semantic relation. M+ uses the ASG to identify that relation and to add it to the interpretation graph in the form of a path of named arcs connecting root-bn(A) and root-bn(B). This path may include implicit connecting BN instances. For instance, to interpret &amp;quot;subdural hemorrhage&amp;quot; in the Head CT domain, M+ attempts to unify the graphs for the subphrases &amp;quot;subdural&amp;quot; and &amp;quot;hemorrhage&amp;quot;, where type- = location, and typeof(root-bn(&amp;quot;hemorrhage&amp;quot;)) = topic. identifies the connecting path for these two types as shown in figure 2, and adds that path to the interpretation as shown in figure 5. Note that this path contains instances of the &amp;quot;observation&amp;quot; and &amp;quot;anatomy&amp;quot; BN types. Figure 6: Grammar rule based unification. Implementation M+ is written in Common Lisp, with some C routines for BN access. The M+ architecture consists of six basic components: The parser, concept space, rule base, lexicon, ASL inference engine, and Bayesian network component. As mentioned, the parser is an implementation of a bottom up chart parser with context free grammar. The concept space is a table of symbols representing types, objects and relations within the ASL. These include BN names, BN node value names, inter-BN relation names, and a small ontology of useful concepts such as those related to time. The rule base contains rules, which comprise the syntactic grammar and ASG. The lexicon is a table of Lisp-readable word information entries, obtained in part from the UMLS Specialist Lexicon. The ASL inference engine combines symbolic unification with backward-chaining inference. It can be used to match an ASG pattern against an interpretation graph, and to perform tests associated with grammar rules. Figure 5. Different-type unification. Grammar Rule Based Unification Bayesian network component utilizes Individual grammar rules in M+ can recognize semantic relations, and add connecting arcs to the interpretation graph. For instance, M+ has a rule which recognizes findings-type phrases connected with strings of the &amp;quot;suggesting&amp;quot; variety, and connects their graphs with a &apos;consistent-with&apos; arc. This is used to interpret &amp;quot;opacity suggesting possible infarct&amp;quot; in the Head CT domain, as shown in figure 6. Norsys API, and includes a set of Lisp and C language routines for instantiating and retrieving probabilities from BNs. M+ Porting M+ to a new medical domain involves gathering a corpus of training sentences for the using the graphical interface to create domain-specific BNs, and generating training cases for the new BNs. The most time-consuming task is the creation of training cases. We have developed a prototype version of a Web-based tool which largely automates this task. The basic idea is to enable M+ to guess the BN value assignments of unknown words, then use it to parse phrases similar to phrases already seen. For instance, having been trained on the phrase &amp;quot;right upper lobe&amp;quot;, the parser is able to produce reasonable parses, with some &amp;quot;guessed&amp;quot; value assignments, for &amp;quot;left upper lobe&amp;quot;, &amp;quot;right middle lobe&amp;quot;, &amp;quot;bilateral lungs&amp;quot;, etc. The BN assignments produced by the parse are output as tentative new cases to be reviewed and corrected by the human trainer. The training process begins with an initial set of interpreted &amp;quot;seed&amp;quot; phrases. From this set, the tool can apply the parser to phrases similar to this set, and so semi-automatically traverse ever widening semantically contiguous areas within the space of corpus phrases. As the training proceeds, the role of the human trainer increasingly becomes one of providing correction and interpretations for semantic patterns the system is increasingly able to discover on its own. To parse phrases containing unknown words, M+ uses a technique based on a variation of the vector space model of lexical semantic similarity (Manning and Schutze, 1999). As M+ encounters an unknown word, it gathers a list of training corpus words judged similar to that word, as predicted by the vector space measure. It then identifies BN nodes whose known values significantly overlap with this list, and provisionally assigns the unknown word as a new value for those nodes. The assignment resulting in the best parsetree is selected for the new provisional training case. M+ was evaluated for the extraction of American College of Radiology (ACR) utilization review codes from Head CT reports (Fiszman, 2002). The ACR codes compare the outcome in a report with the suspected diagnosis provided by emergency department physicians. If the outcome relates to the suspected diagnosis then the report should be encoded as positive (P). If the outcome is negative and does not relate to the suspected diagnosis then the report should be encoded as negative (N). In order to extract those ACR codes we trained M+ to extract eleven broad disease concepts, then inferred the ACR codes based on the application of a rule to the M+ output: If any of the concepts was present, the report was considered positive, else the report was considered negative. Twenty six hundred head CT scan reports were used for this evaluation. Six hundred reports were randomly selected for testing, and the rest were used to train M+ in this domain. The performance of M+ on this task was measured against that of four board certified physicians, using a gold standard based on majority vote, as described in (Fiszman, 2002). For each subject we calculated recall, precision and specificity with their respective 95 % confidence intervals for the capture of ACR utilization codes. From 600 head CT reports, 67 were judged to be positive (P) by the gold standard physicians and 534 were judged to be negative (N). Therefore the positive rate for head CT in this sample was 11%. Recall, precision and specificity for every subject are presented with their respective 95% confidence intervals in Table 1. The physicians had an average recall of 88% (CI, 84% to 92.%), an average precision of 86% (CI, 81% to 90%), and average specificity of 98% (CI, 97% to 99%). M+ had recall of 87% (CI, 78% to 95%), precision of 85% (CI, 77% to 94%) and specificity of 98% (CI, 97% to 99). Table 1. Results of ACR utilization code study.</abstract>
<note confidence="0.6570985">Subject Recall Specificity Precision Physician1 0.83 0.99 0.91</note>
<phone confidence="0.850574333333333">(0.74-0.92) (0.98-1.00) (0.84-0.99) Physician2 0.88 0.98 0.84 (0.81-0.97) (0.97-0.99) (0.75-0.93) Physician3 0.93 0.98 0.86 (0.87-1.00) (0.97-0.99) (0.78-0.95) Physician4 0.88 0.97 0.81 (0.96-0.99) (0.96-0.99) (0.71-0.90) M+ 0.87 0.98 0.85 (0.78-0.95) (0.97-0.99) (0.77-0.94)</phone>
<abstract confidence="0.998502607142857">The results on Head CT reports are encouraging, but there are limitations. We only evaluated 600 reports, because it&apos;s very hard to get physicians to produce gold standard data for medical reports. The prevalence of positive reports is only 11% and reflects the fact that the individual brain conditions have very low prevalence. M+ and its predecessors have demonstrated that BNs provide a useful semantic model for medical text processing. In practice, a medical NLP system will frequently encounter missing and unknown words, unknown ungrammatical phrase structures, and telegraphic usages. Knowledge databases will be imperfect and incomplete. Using BNs for semantic representation brings a noise-tolerant, partial match-tolerant, character to the recognition of semantic patterns, and to relevant inferences based on those patterns. In addition, BNs can be used to guess the semantic types of unknown words, providing a basis for bootstrapping the system&apos;s semantic knowledge. Many thanks to Wendy W. Chapman for her advice and input in this paper, and her efforts to make M+ a useful addition to the RODS project at the University of Pittsburgh.</abstract>
<note confidence="0.581027071428571">References Chapman W., Christensen L. M., Wagner M., Haug P. J., Ivanov O., Dowling J. N., Olszewski R. T. 2002. Syndromic Detection from Free-text Triage Diagnoses: Evaluation of a Medical Language Processing System before Deployment the Winter Olympics. AMIA Symp. (submitted). Noam. 1965. of the theory of technical report (Massachusetts Institute of Technology, Research Laboratory of Electronics); no. 11. Cambridge, MA: MIT Press. Fiszman M., Blatter D.D., Christensen L.M., Oderich G., Macedo T., Eidelwein A.P., Haug P.J. 2002.</note>
<abstract confidence="0.884315333333333">Utilization review of head CT scans: value of a language processing system. of Roentgenology (AJR).</abstract>
<author confidence="0.6835055">A general natural-language</author>
<note confidence="0.921692444444444">processor for clinical radiology. Am Med Assoc. pp. 161-74. Friedman N., Getoor L., Koller D. and Pfeffer A. 1999. Learning Probabilistic Relational Models. Proceedings of the 16th International Joint on Artificial Intelligence pp. 1300-1307. Haug P. J., Christensen L., Gundersen M., Clemons B., Koehler S., Bauer K. 1997. A natural</note>
<abstract confidence="0.851677333333333">language parsing system for encoding admitting AMIA Symp. pp. 4-8. Koehler, S. B. 1998. SymText: A natural language understanding system for encoding free text medical data. Ph.D. Dissertation, University of Utah.</abstract>
<note confidence="0.774515823529412">Koller D., and Pfeffer A. 1997. Object-Oriented Networks. of the 13th Conference on Uncertainty in pp. 302-313. Lin R, Lenert L, Middleton B, Shiffman S. A freetext processing system to capture physical findings: Canonical Phrase Identification System Annu Symp Comput Appl Med 843-7. Manning C. D. and Schutze H. 1999. Foundations of Statistical Natural Language Processing. MIT Press. Minsky, M. 1975. A framework for representing In Psychology of Human ed. P. H. Winston, pp. 211-277. McGraw Hill. Moore, R. C. 1989. Unification-based Semantic of the 27th Annual</note>
<title confidence="0.522089">Meeting of the Association for Computational</title>
<author confidence="0.487928">Probabilistic inference in</author>
<abstract confidence="0.969318571428572">intelligent systems. Networks of plausible inference: Morgan Kaufmann. Ranum D.L. 1989. Knowledge-based understanding radiology text. Methods Programs pp.209-215. Romacker, Martin and Hahn, Udo. 2000. An empirical assessment of semantic interpretation.</abstract>
<note confidence="0.368451583333333">ANLP/NAACL 2000 -- Proceedings of the 6th Applied Natural Language Processing Conference &amp; the 1st Conference of the North American Chapter of the Association for Linguistics. 327-334. R.C. and R. Abelson. 1997. Plans, and Understanding. NJ: Lawrence Erlbaum. Smart, J. F. and M. Roux. 1995. A model for medical knowledge representation application to the analysis of descriptive pathology reports. Inf Sep;34(4) pp. 352-60.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Chapman</author>
<author>L M Christensen</author>
<author>M Wagner</author>
<author>P J Haug</author>
<author>O Ivanov</author>
<author>J N Dowling</author>
<author>R T Olszewski</author>
</authors>
<title>Syndromic Detection from Free-text Triage Diagnoses: Evaluation of a Medical Language Processing System before Deployment in the Winter Olympics.</title>
<date>2002</date>
<booktitle>Proc AMIA Symp. (submitted).</booktitle>
<contexts>
<context position="2153" citStr="Chapman et al., 2002" startWordPosition="310" endWordPosition="313">k. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegraphic or otherwise not well formed grammatically. Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary. Most NLP systems capable of semantic analysis employ representational formalisms with ties to classical logic, includi</context>
</contexts>
<marker>Chapman, Christensen, Wagner, Haug, Ivanov, Dowling, Olszewski, 2002</marker>
<rawString>Chapman W., Christensen L. M., Wagner M., Haug P. J., Ivanov O., Dowling J. N., Olszewski R. T. 2002. Syndromic Detection from Free-text Triage Diagnoses: Evaluation of a Medical Language Processing System before Deployment in the Winter Olympics. Proc AMIA Symp. (submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the theory of syntax. Special technical report</title>
<date>1965</date>
<booktitle>of Electronics); no. 11.</booktitle>
<publisher>MIT Press.</publisher>
<institution>Massachusetts Institute of Technology, Research Laboratory</institution>
<location>Cambridge, MA:</location>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, Noam. 1965. Aspects of the theory of syntax. Special technical report (Massachusetts Institute of Technology, Research Laboratory of Electronics); no. 11. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fiszman</author>
<author>D D Blatter</author>
<author>L M Christensen</author>
<author>G Oderich</author>
<author>T Macedo</author>
<author>A P Eidelwein</author>
<author>P J Haug</author>
</authors>
<title>Utilization review of head CT scans: value of a medical language processing system.</title>
<date>2002</date>
<journal>American Journal of Roentgenology</journal>
<marker>Fiszman, Blatter, Christensen, Oderich, Macedo, Eidelwein, Haug, 2002</marker>
<rawString>Fiszman M., Blatter D.D., Christensen L.M., Oderich G., Macedo T., Eidelwein A.P., Haug P.J. 2002. Utilization review of head CT scans: value of a medical language processing system. American Journal of Roentgenology (AJR). (submitted)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
<author>Alderson PO</author>
<author>Austin JH</author>
<author>Cimino JJ</author>
<author>Johnson SB</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<journal>J Am Med Inform Assoc.</journal>
<volume>1</volume>
<issue>2</issue>
<pages>161--74</pages>
<contexts>
<context position="1763" citStr="Friedman et al., 1994" startWordPosition="248" endWordPosition="251">se systems require data that is appropriately structured and coded. Since a large portion of the information stored in patient databases is in the form of free text, manually coding this information in a format accessible to these tools can be time consuming and expensive. In recent years, natural language processing (NLP) methodologies have been studied as a means of automating this task. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently </context>
</contexts>
<marker>Friedman, PO, JH, JJ, SB, 1994</marker>
<rawString>Friedman C, Alderson PO, Austin JH, Cimino JJ, Johnson SB. 1994, A general natural-language text processor for clinical radiology. J Am Med Inform Assoc. Mar-Apr;1(2) pp. 161-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friedman</author>
<author>L Getoor</author>
<author>D Koller</author>
<author>A Pfeffer</author>
</authors>
<title>Learning Probabilistic Relational Models.</title>
<date>1999</date>
<booktitle>Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI):</booktitle>
<pages>1300--1307</pages>
<marker>Friedman, Getoor, Koller, Pfeffer, 1999</marker>
<rawString>Friedman N., Getoor L., Koller D. and Pfeffer A. 1999. Learning Probabilistic Relational Models. Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI): pp. 1300-1307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Haug</author>
<author>L Christensen</author>
<author>M Gundersen</author>
<author>B Clemons</author>
<author>S Koehler</author>
<author>K Bauer</author>
</authors>
<title>A natural language parsing system for encoding admitting diagnoses.</title>
<date>1997</date>
<booktitle>Proc AMIA Symp.</booktitle>
<volume>81</volume>
<pages>4--8</pages>
<contexts>
<context position="2060" citStr="Haug et al., 1997" startWordPosition="298" endWordPosition="301">anguage processing (NLP) methodologies have been studied as a means of automating this task. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegraphic or otherwise not well formed grammatically. Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary. Most NLP systems capable </context>
</contexts>
<marker>Haug, Christensen, Gundersen, Clemons, Koehler, Bauer, 1997</marker>
<rawString>Haug P. J., Christensen L., Gundersen M., Clemons B., Koehler S., Bauer K. 1997. A natural language parsing system for encoding admitting diagnoses. Proc AMIA Symp. 81: pp. 4-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Koehler</author>
</authors>
<title>SymText: A natural language understanding system for encoding free text medical data.</title>
<date>1998</date>
<institution>Ph.D. Dissertation, University of Utah.</institution>
<contexts>
<context position="1792" citStr="Koehler, 1998" startWordPosition="254" endWordPosition="255">opriately structured and coded. Since a large portion of the information stored in patient databases is in the form of free text, manually coding this information in a format accessible to these tools can be time consuming and expensive. In recent years, natural language processing (NLP) methodologies have been studied as a means of automating this task. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegrap</context>
<context position="6953" citStr="Koehler, 1998" startWordPosition="1077" endWordPosition="1078">n a phrase, infer the meaning *right-upper-lobe&amp;quot;. After training on this case, assigning one or more values from this case would increase the probabilities of the other values; for instance assigning side= &amp;quot;right&amp;quot; would increase the probability of the value interpretation= *right-upper-lobe. Interpretive concepts such as *rightupper-lobe are atomic symbols which are either invented by the human trainer, or else obtained from a medical knowledge database such as the Figure 2. Network of M+ BNs, applied to &amp;quot;temporal subdural hemorrhage&amp;quot;. One way in which M+ differs from its predecessor SymText (Koehler, 1998) is in the size and modularity of its semantic BNs. The SymText BNs group observation and disease concepts together with state (&amp;quot;present&amp;quot;, &amp;quot;absent&amp;quot;), change-of-state (&amp;quot;old&amp;quot;, &amp;quot;chronic&amp;quot;), anatomic location and other concept types. M+ trades the inferential advantages of such monolithic BNs for the modularity and composability of smaller BNs such as those shown in figure 2. Figure 3 shows a single instance of the SymText Chest Radiology Findings BN, instantiated with the sentence &amp;quot;There is dense infiltrative opacity in the right upper lobe&amp;quot;. *observations : *localized upper lobe infiltrate (0.888</context>
</contexts>
<marker>Koehler, 1998</marker>
<rawString>Koehler, S. B. 1998. SymText: A natural language understanding system for encoding free text medical data. Ph.D. Dissertation, University of Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>A Pfeffer</author>
</authors>
<title>Object-Oriented Bayesian Networks.</title>
<date>1997</date>
<booktitle>Proceedings of the 13th Annual Conference on Uncertainty in AI:</booktitle>
<pages>302--313</pages>
<contexts>
<context position="10855" citStr="Koller and Pfeffer, 1997" startWordPosition="1669" endWordPosition="1672">ied, it is rejected. If the interpretation has a low probability, the phrase is less likely to appear in the final parse tree. On the other hand, interpretations are constructed as phrases are recognized. The exception to this rule is when an ungrammatical fragment of text is encountered. M+ then uses a semanticallyguided phrase repair procedure not described in this paper. 2.3 The M+ Abstract Semantic Language The probabilistic reasoning afforded by BNs is superior to classical logic in important ways (Pearl, 1988). However, BNs are limited in expressive power relative to first-order logics (Koller and Pfeffer, 1997), and commercially available implementations lack the flexibility of symbolic languages. Friedman et al have made considerable headway in giving BNs many useful characteristics of first order languages, in what they call probabilistic relational models, or PRMs (e.g. Friedman at al. 1999). While we are waiting for industrystandard PRMs, we have tried to make our semantic BNs more useful by combining them with a first-order language, called the M+ Abstract Semantic Language (ASL), implemented within M+. Specifically, BNs are treated as object types within the ASL. There is a &amp;quot;chest anatomy&amp;quot; typ</context>
</contexts>
<marker>Koller, Pfeffer, 1997</marker>
<rawString>Koller D., and Pfeffer A. 1997. Object-Oriented Bayesian Networks. Proceedings of the 13th Annual Conference on Uncertainty in AI: pp. 302-313.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Lin</author>
<author>L Lenert</author>
<author>B Middleton</author>
<author>S Shiffman</author>
</authors>
<title>A freetext processing system to capture physical findings: Canonical Phrase Identification System (CAPIS).</title>
<booktitle>Proc Annu Symp Comput Appl Med Care.</booktitle>
<pages>843--7</pages>
<marker>Lin, Lenert, Middleton, Shiffman, </marker>
<rawString>Lin R, Lenert L, Middleton B, Shiffman S. A freetext processing system to capture physical findings: Canonical Phrase Identification System (CAPIS). Proc Annu Symp Comput Appl Med Care. pp. 843-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="24681" citStr="Manning and Schutze, 1999" startWordPosition="3803" endWordPosition="3806">cess begins with an initial set of interpreted &amp;quot;seed&amp;quot; phrases. From this set, the tool can apply the parser to phrases similar to this set, and so semi-automatically traverse ever widening semantically contiguous areas within the space of corpus phrases. As the training proceeds, the role of the human trainer increasingly becomes one of providing correction and interpretations for semantic patterns the system is increasingly able to discover on its own. To parse phrases containing unknown words, M+ uses a technique based on a variation of the vector space model of lexical semantic similarity (Manning and Schutze, 1999). As M+ encounters an unknown word, it gathers a list of training corpus words judged similar to that word, as predicted by the vector space measure. It then identifies BN nodes whose known values significantly overlap with this list, and provisionally assigns the unknown word as a new value for those nodes. The assignment resulting in the best parsetree is selected for the new provisional training case. 6 Evaluation M+ was evaluated for the extraction of American College of Radiology (ACR) utilization review codes from Head CT reports (Fiszman, 2002). The ACR codes compare the outcome in a re</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>Manning C. D. and Schutze H. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minsky</author>
</authors>
<title>A framework for representing knowledge.</title>
<date>1975</date>
<booktitle>In The Psychology of Human Vision,</booktitle>
<pages>211--277</pages>
<editor>ed. P. H. Winston,</editor>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="3628" citStr="Minsky, 1975" startWordPosition="531" endWordPosition="532">he philosophy and implementation of M+, and attempts to show how Bayesian Networks can be useful in medical text analysis. 2 The M+ Semantic Model 2.1 Semantic Bayesian Networks M+ uses Bayesian Networks (BNs) to represent the basic semantic types and relations within a medical domain such as chest radiology reports. M+ BNs are structurally similar to semantic networks, in that they are implemented as directed acyclic graphs, with nodes representing word and concept types, and links representing relations between those types. BNs also have a character as frames or slot-filler representations (Minsky, 1975). Each node is treated as a variable, with an associated list of possible values. For instance a node representing &amp;quot;disease severity&amp;quot; might include the possible values {&amp;quot;severe&amp;quot;, &amp;quot;moderate&amp;quot;, &amp;quot;mild&amp;quot;}. Each value has a probability, either assigned or inferred, of being the true value of that node. In addition to providing a framework for representation, a BN is also a probabilistic inference engine. The probability of each possible value of a node is conditioned on the probabilities of the values of neighboring nodes, through a training process that learns a Bayesian joint probability function f</context>
</contexts>
<marker>Minsky, 1975</marker>
<rawString>Minsky, M. 1975. A framework for representing knowledge. In The Psychology of Human Vision, ed. P. H. Winston, pp. 211-277. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>Unification-based Semantic Interpretation.</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="2839" citStr="Moore, 1989" startWordPosition="413" endWordPosition="414"> of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegraphic or otherwise not well formed grammatically. Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary. Most NLP systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (Friedman et al., 1994), unificationbased semantics (Moore, 1989), and description logics (Romacker and Hahn, 2000). M+ and its predecessors employ Bayesian Networks (Pearl, 1988), a methodology outside this tradition. This study discusses the philosophy and implementation of M+, and attempts to show how Bayesian Networks can be useful in medical text analysis. 2 The M+ Semantic Model 2.1 Semantic Bayesian Networks M+ uses Bayesian Networks (BNs) to represent the basic semantic types and relations within a medical domain such as chest radiology reports. M+ BNs are structurally similar to semantic networks, in that they are implemented as directed acyclic gr</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Moore, R. C. 1989. Unification-based Semantic Interpretation. Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pp33-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic inference in intelligent systems. Networks of plausible inference:</title>
<date>1988</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2953" citStr="Pearl, 1988" startWordPosition="429" endWordPosition="430">eport will frequently contain text that is telegraphic or otherwise not well formed grammatically. Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary. Most NLP systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (Friedman et al., 1994), unificationbased semantics (Moore, 1989), and description logics (Romacker and Hahn, 2000). M+ and its predecessors employ Bayesian Networks (Pearl, 1988), a methodology outside this tradition. This study discusses the philosophy and implementation of M+, and attempts to show how Bayesian Networks can be useful in medical text analysis. 2 The M+ Semantic Model 2.1 Semantic Bayesian Networks M+ uses Bayesian Networks (BNs) to represent the basic semantic types and relations within a medical domain such as chest radiology reports. M+ BNs are structurally similar to semantic networks, in that they are implemented as directed acyclic graphs, with nodes representing word and concept types, and links representing relations between those types. BNs al</context>
<context position="10751" citStr="Pearl, 1988" startWordPosition="1656" endWordPosition="1657">ly possible phrase is uninterpretable, i.e. if its subphrase interpretations cannot be unified, it is rejected. If the interpretation has a low probability, the phrase is less likely to appear in the final parse tree. On the other hand, interpretations are constructed as phrases are recognized. The exception to this rule is when an ungrammatical fragment of text is encountered. M+ then uses a semanticallyguided phrase repair procedure not described in this paper. 2.3 The M+ Abstract Semantic Language The probabilistic reasoning afforded by BNs is superior to classical logic in important ways (Pearl, 1988). However, BNs are limited in expressive power relative to first-order logics (Koller and Pfeffer, 1997), and commercially available implementations lack the flexibility of symbolic languages. Friedman et al have made considerable headway in giving BNs many useful characteristics of first order languages, in what they call probabilistic relational models, or PRMs (e.g. Friedman at al. 1999). While we are waiting for industrystandard PRMs, we have tried to make our semantic BNs more useful by combining them with a first-order language, called the M+ Abstract Semantic Language (ASL), implemented</context>
<context position="13366" citStr="Pearl, 1988" startWordPosition="2048" endWordPosition="2049">N instance. In this case, (located-at #find1 #loc1) could mean &amp;quot;*localized-infiltrate located-at *left-lower-lobe&amp;quot;. Because the object types in the ASL are the abstract concept types represented by the BNs, semantic rules formulated in this language constitute an &amp;quot;abstract semantic grammar&amp;quot; (ASG). The ASG recognizes patterns of semantic relations among the BNs, and supports analysis and inference based on those patterns. It also permits rule-based control over the creation, instantiation, and use of the BNs, including defining pathways for information sharing among BNs using virtual evidence (Pearl, 1988). One use of the ASG is in post-parse processing of interpretations. After the M+ parser has constructed an interpretation, postparse ASG productions may augment or alter this interpretation. One rule instructs &amp;quot;If two pathological conditions exist in a &apos;consistentwith&apos; relation, and the first condition has a state modifier (i.e. *present or *absent), and the second condition does not, apply the first condition&apos;s state to the second condition&amp;quot;. For instance, in the ambiguous sentence &amp;quot;There is no opacity consistent with pneumonia&amp;quot;, if the parser doesn&apos;t correctly determine the scope of &amp;quot;no&amp;quot;, i</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Pearl, Judea. 1988. Probabilistic inference in intelligent systems. Networks of plausible inference: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Ranum</author>
</authors>
<title>Knowledge-based understanding of radiology text.</title>
<date>1989</date>
<booktitle>Comput Methods Programs Biomed. Oct-Nov;30(2-3)</booktitle>
<pages>209--215</pages>
<contexts>
<context position="1776" citStr="Ranum, 1989" startWordPosition="252" endWordPosition="253"> that is appropriately structured and coded. Since a large portion of the information stored in patient databases is in the form of free text, manually coding this information in a format accessible to these tools can be time consuming and expensive. In recent years, natural language processing (NLP) methodologies have been studied as a means of automating this task. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text </context>
</contexts>
<marker>Ranum, 1989</marker>
<rawString>Ranum D.L. 1989. Knowledge-based understanding of radiology text. Comput Methods Programs Biomed. Oct-Nov;30(2-3) pp.209-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Romacker</author>
<author>Udo Hahn</author>
</authors>
<title>An empirical assessment of semantic interpretation. ANLP/NAACL</title>
<date>2000</date>
<booktitle>Proceedings of the 6th Applied Natural Language Processing Conference &amp; the 1st Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<pages>327--334</pages>
<contexts>
<context position="2889" citStr="Romacker and Hahn, 2000" startWordPosition="418" endWordPosition="421">arget domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegraphic or otherwise not well formed grammatically. Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary. Most NLP systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (Friedman et al., 1994), unificationbased semantics (Moore, 1989), and description logics (Romacker and Hahn, 2000). M+ and its predecessors employ Bayesian Networks (Pearl, 1988), a methodology outside this tradition. This study discusses the philosophy and implementation of M+, and attempts to show how Bayesian Networks can be useful in medical text analysis. 2 The M+ Semantic Model 2.1 Semantic Bayesian Networks M+ uses Bayesian Networks (BNs) to represent the basic semantic types and relations within a medical domain such as chest radiology reports. M+ BNs are structurally similar to semantic networks, in that they are implemented as directed acyclic graphs, with nodes representing word and concept typ</context>
</contexts>
<marker>Romacker, Hahn, 2000</marker>
<rawString>Romacker, Martin and Hahn, Udo. 2000. An empirical assessment of semantic interpretation. ANLP/NAACL 2000 -- Proceedings of the 6th Applied Natural Language Processing Conference &amp; the 1st Conference of the North American Chapter of the Association for Computational Linguistics. pp. 327-334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
<author>R Abelson</author>
</authors>
<date>1997</date>
<booktitle>Scripts, Plans, Goals, and Understanding.</booktitle>
<location>Hillsdale, NJ: Lawrence Erlbaum.</location>
<marker>Schank, Abelson, 1997</marker>
<rawString>Schank, R.C. and R. Abelson. 1997. Scripts, Plans, Goals, and Understanding. Hillsdale, NJ: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Smart</author>
<author>M Roux</author>
</authors>
<title>A model for medical knowledge representation application to the analysis of descriptive pathology reports.</title>
<date>1995</date>
<journal>Methods Inf Med.</journal>
<volume>34</volume>
<issue>4</issue>
<pages>352--60</pages>
<contexts>
<context position="1674" citStr="Smart and Roux, 1995" startWordPosition="234" endWordPosition="237">rance and research, and diagnostic systems for more complex medical decision support. These systems require data that is appropriately structured and coded. Since a large portion of the information stored in patient databases is in the form of free text, manually coding this information in a format accessible to these tools can be time consuming and expensive. In recent years, natural language processing (NLP) methodologies have been studied as a means of automating this task. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target dom</context>
</contexts>
<marker>Smart, Roux, 1995</marker>
<rawString>Smart, J. F. and M. Roux. 1995. A model for medical knowledge representation application to the analysis of descriptive pathology reports. Methods Inf Med. Sep;34(4) pp. 352-60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>