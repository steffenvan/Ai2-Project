<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014141">
<title confidence="0.978712">
Toward Using Morphology in French-English Phrase-based SMT
</title>
<author confidence="0.993263">
Marine Carpuat
</author>
<affiliation confidence="0.886268666666667">
Center for Computational Learning Systems
Columbia University
475 Riverside Drive, New York, NY 10115
</affiliation>
<email confidence="0.999097">
marine@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99980825">
We describe the system used in our sub-
mission to the WMT-2009 French-English
translation task. We use the Moses phrase-
based Statistical Machine Translation sys-
tem with two simple modications of the
decoding input and word-alignment strat-
egy based on morphology, and analyze
their impact on translation quality.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996272">
In this first participation to the French-English
translation task at WMT, our goal was to build a
standard phrase-based statistical machine transla-
tion system and study the impact of French mor-
phological variations at different stages of training
and decoding.
Many strategies have been proposed to inte-
grate morphology information in SMT, including
factored translation models (Koehn and Hoang,
2007), adding a translation dictionary containing
inflected forms to the training data (Schwenk et
al., 2008), entirely replacing surface forms by
representations built on lemmas and POS tags
(Popovi´c and Ney, 2004), morphemes learned in
an unsupervised manner (Virpojia et al., 2007),
and using Porter stems and even 4-letter prefixes
for word alignment (Watanabe et al., 2006). In
non-European languages, such as Arabic, heavy
effort has been put in identifying appropriate in-
put representations to improve SMT quality (e.g.,
Sadat and Habash (2006))
As a first step toward using morphology infor-
mation in our French-English SMT system, this
submission focused on studying the impact of
*The author was partially funded by GALE DARPA Con-
tract No. HR0011-06-C-0023. Any opinions, findings and
conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the
views of the Defense Advanced Research Projects Agency.
different input representations for French based
on the POS and lemmatization provided by the
Treetagger tool (Schmid, 1994). In the WMT09
French-English data sets, we observe that more
than half of the words that are unknown in the
translation lexicon actually occur in the training
data under different inflected forms. We show that
combining a lemma backoff strategy at decoding
time and improving alignments by generalizing
across verb surface forms improves OOV rates and
translation quality.
</bodyText>
<sectionHeader confidence="0.855022" genericHeader="method">
2 Translation system
</sectionHeader>
<subsectionHeader confidence="0.993216">
2.1 Data sets
</subsectionHeader>
<bodyText confidence="0.952171785714286">
We use a subset of the data made available for the
official French to English translation task. The
evaluation test set consists of French news data
from September to October 2008, however the
bulk of the training data is not from the same do-
main. The translation model was trained on the
Europarl corpus (europarl-v4) and the small news
commentary corpus (news-commentary09). Fol-
lowing D´echelotte et al. (2008), we learn a sin-
gle phrase table and reordering model rather than
one for each domain, as it was found to yield bet-
ter performance in a very similar setting. The
language model was trained on the English side
of these parallel corpora augmented with non-
parallel English news data (news-train08.en). Pa-
rameter tuning was performed on the designated
development data, which is also in the news do-
main: news-dev2009a was used as the develop-
ment set and news-dev2009b as the test set.
Using those data sets, there is therefore a mis-
match between the training and evaluation do-
mains, as in the domain adaptation tasks of the
previous WMT evaluations. A large automatically
extracted parallel corpus was made available, but
we were not able to use it due to time constraints.
Additional use of this in-domain data would im-
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 150–154,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.997251">
150
</page>
<bodyText confidence="0.876235">
prove coverage and translation quality.
</bodyText>
<subsectionHeader confidence="0.995542">
2.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.992376866666667">
French and English corpora processing followed
the same three steps:
First, long sentences are resegmented using
simple punctuation-based heuristics.
Second, tokenization, POS tagging and lemma-
tization are performed with Treetagger (Schmid,
1994) using the standard French and English pa-
rameter files1. Treetagger is based on Hidden
Markov Models where transition probabilities are
estimated with decision trees. The POS tag set
consists of 33 tags which capture tense informa-
tion for verbs, but not gender and number.
Third, sentence-initial capitalized words are
normalized to their most frequent form as reported
by Zollmann et al. (2006).
</bodyText>
<subsectionHeader confidence="0.999483">
2.3 Core system
</subsectionHeader>
<bodyText confidence="0.999877592592593">
We use the Moses phrase-based statistical machine
translation system (Koehn et al., 2007) and follow
standard training, tuning and decoding strategies.
The translation model consists of a stan-
dard Moses phrase-table with lexicalized reorder-
ing. Bidirectional word alignments obtained with
GIZA++ are intersected using the grow-diag-final
heuristic. Translations of phrases of up to 7 words
long are collected and scored with translation pro-
bilities and lexical weighting.
The English language model is a 4-gram model
with Kneser-Ney smoothing, built with the SRI
language modeling toolkit (Stolcke, 2002).
The loglinear model feature weights were
learned using minimum error rate training
(MERT) (Och, 2003) with BLEU score (Papineni
et al., 2002) as the objective function.
Other decoding parameters were selected man-
ually on an earlier version of the system trained
and evaluated on the single-domain Europarl data.
While the configuration achieved competitive re-
sults on the previous, it is not be optimal for this
domain adaptation task.
We will first conduct an analysis of this core
SMT system, and experiment with two modifi-
cations of input representation for decoding and
alignment respectively.
</bodyText>
<footnote confidence="0.960526">
1www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
</footnote>
<table confidence="0.992291333333334">
OOV verbs w/ surface w/ lemma+
form in POS in
training training
corpus corpus
dev2009a 21 (28%) 48 (63%)
dev2009b 16 (24%) 33 (49%)
</table>
<tableCaption confidence="0.813995333333333">
Table 1: Unknown verbs statistics
3 Many unknown words are (almost)
seen in training
</tableCaption>
<bodyText confidence="0.999937740740741">
Our baseline system is set up to copy unknown
words to the output. This is a helpful strategy to
translate unknown names and cognates, but is far
from optimal. In this section, we take a closer look
at those unknown words.
About 25% of the dev and test set sentences
contain at least one unknown token. After elim-
inating number expressions, which can be handled
with translation rules, the majority of unknown
words are content words, nouns, verbs and adjec-
tives. As reported in Table 1, we find that many of
the verbs that are not in the phrase-table vocabu-
lary were actually seen in the training data in the
exact same form: they are therefore out of vocabu-
lary due to alignment errors. In addition, for more
than half of the unknown verb occurrences, an-
other inflexion form for the same lemma and POS
tag are observed in the training corpus.
Using only the surface form of words therefore
leads us to ignore potentially useful information
available in our training corpus. Additional train-
ing data would naturally improve coverage, but
will not cover all possible morphological varia-
tions of all verbs, especially for tenses and persons
that are not used frequently in news coverage. It
is therefore necessary to generalize beyond word
surface forms.
</bodyText>
<sectionHeader confidence="0.900016" genericHeader="method">
4 Using morphological information in
decoding
</sectionHeader>
<bodyText confidence="0.999929777777778">
A simple strategy for handling unknown words
at decoding time consists in replacing their oc-
currences in the test set with their lemma, when
it is part of the translation lexicon vocabulary.
Unlike with factored models (Koehn and Hoang,
2007) or additional translation lexicons (Schwenk
et al., 2008), we do not generate the surface form
back from the lemma translation, which means
that tense, gender and number information are
</bodyText>
<page confidence="0.991398">
151
</page>
<table confidence="0.999669333333334">
news-dev2009a representation OOV % METEOR BLEU NIST
baseline surface form only 2.24 49.05 20.45 6.135
decoding lemma backoff 2.13 49.12 20.44 6.143
lemma+POS for all 2.24 48.87 20.36 6.145
word alignment lemma+POS for adj 2.25 48.94 20.46 6.131
lemma+POS for verbs 2.21 49.05 20.47 6.137
backoff + all 2.10 48.97 20.36 6.147
decoding + alignment backoff + adj 2.12 49.05 20.48 6.140
backoff + verbs 2.08 49.15 20.50 6.148
news-dev2009b representation OOV % METEOR BLEU NIST
baseline surface form only 2.52 49.60 21.10 6.211
decoding lemma backoff 2.43 49.66 21.02 6.210
lemma+POS for all 2.53 49.56 21.03 6.199
word alignment lemma+POS for adj 2.52 49.74 21.00 6.213
lemma+POS for verbs 2.47 49.73 21.10 6.217
backoff + all 2.44 49.59 20.92 6.194
decoding+alignment backoff + adj 2.43 49.80 21.03 6.217
backoff + verbs 2.39 49.80 21.03 6.217
</table>
<tableCaption confidence="0.89708">
Table 2: Evaluation of the decoding backoff strategy, the modified word alignment strategy and their
combination
</tableCaption>
<table confidence="0.99961955">
Input Mˆeme s’il d´emissionnait, la situation ne changerait pas.
Baseline even if it d´emissionnait, the situation will not change.
Lemma backoff even if it resign, the situation will not change.
Reference even if he resigned, the situation would remain the same.
Input Tant que tu gagnes, on te laisse en paix
Baseline As you gagnes, it leaves you in peace
Lemma backoff As you win, it leaves you in peace
Reference As Long as You Gain, We Let You
Input Le groupe a r´eagi comme il faut, il a sorti un nouveau et meilleur disque.
Baseline The group has reacted properly, it has emerged a new and better records.
Lemma+POS for verbs The group has reacted properly, it has produced a new and better records.
Reference The group responded with a new and even better CD.
Input Un trader qui ne prend pas de vacances est un trader qui ne veut pas laisser son book a` un autre”,
Baseline conclut Kerviel.
Lemma+POS for verbs A senior trader which does not take holiday is a senior trader which does not allow his book to another,
Reference ” concludes Kerviel.
A senior trader which does not take holiday is a senior trader who do not wish to leave his book to
another, ” concludes Kerviel.
A broker who does not take vacations is a broker who does not want anybody to look into his records,”
Kerviel concluded.
</table>
<tableCaption confidence="0.994845">
Table 3: Examples of improved translations by morphological analysis
</tableCaption>
<table confidence="0.999630307692308">
Input 54 pour cent ne font pas du tout confiance au premier ministre et 27 pour cent au pr´esident du Fidesz.
Baseline 54% are not all confidence to Prime Minister and the President of Fidesz 1.27%.
Backoff + verbs 54% do not all confidence to Prime Minister and 27% to the President of Fidesz.
Reference Fifty-four percent said they did not trust the PM, while 27 percent said they mistrusted the Fidesz
chairman.
Input Le prsident V´aclav Klaus s’est nouveau prononc sur la probl´ematique du rchauffement plantaire.
Baseline President V´aclav Klaus has once again voted on the problem of global warming.
Backoff+verbs President V´aclav Klaus has again pronounced on the problem of global warming.
Reference President V´aclav Klaus has again commented on the problem of global warming.
Input Mais les sup´erieurs ´etaient au courant de tout, ou plutˆot, ils s’en doutaient.
Baseline But superiors were aware of everything, or rather, they knew.
Backoff+verbs But superiors were aware of everything, or rather, they doubted.
Reference But his superiors are said to have known, or rather suspected the whole thing.
</table>
<tableCaption confidence="0.999721">
Table 4: Examples of translations that are not improved morphological analysis
</tableCaption>
<page confidence="0.998096">
152
</page>
<bodyText confidence="0.996084666666667">
lost. However, imperfect lemma translations can
be more useful to understand the meaning of the
input sentence than copying the unknown word to
the output.
We report the impact of this strategy on auto-
matic evaluation scores in the decoding section of
Table 2. Since only a small subset of the test sen-
tences are affected by the change, the score vari-
ation is small, but the OOV rate decreases and
translation quality is not degraded. In addition
to the BLEU and NIST n-gram precision metrics
which only count exact matches between system
output and reference, we report METEOR scores
which take into account matches after lemmatiza-
tion using both the Porter stemmer and the Word-
Net lemmas (Banerjee and Lavie, 2005). The im-
provement in METEOR scores results from more
matches with the references, yielding both im-
proved precision and recall.
Manual inspection of the output sentences
shows that the translations are better to the human
eye and potentially more useful to subsequent text
understanding applications (Table 3).
5 Using morphological information in
word-alignment
In this experiment, we would like to use morpho-
logical analysis to alleviate the alignment errors
because of which some words from the parallel
corpus are not in the phrase-table. We adopt a
two-step approach: (1) before word alignment, re-
place surface forms by lemma and POS tags. In
our experiments, this replacement is performed for
3 categories of words: verbs only, adjectives only
and all words. (2) the phrase-table and reorder-
ing models are learned as usual using word surface
forms, but with the alignment links from step 1.
In constrast with Watanabe et al. (2006), we at-
tempt to generalize for specific word categories
only, rather than use lemmas across all surface
forms, as we found in earlier experiments that this
approach did not help translation quality in our
particular setting.
Unlike other approaches which use morpholog-
ical analysis to change the representation of the
input (e.g., Popovi´c and Ney (2004), Sadat and
Habash (2006), Virpojia et al. (2007)), our system
still uses word surface forms as input during de-
coding. This is a constraint imposed by the rela-
tively coarse analysis given by the default Treetag-
ger lemmas and POS tags. Since they do not cap-
ture information that is crucial in translation such
as number and gender, we need to keep surface
forms as the input for translation.
The impact of this strategy on automatic eval-
uation metrics is reported in the word alignment
section of Table 2. Note that all experiments were
performed using the parameters learned by MERT
on news-dev2009a using the baseline configura-
tion. Again the impact in numbers is small, but
does not degrade translation quality. The ME-
TEOR score is slightly improved on the real test
set. As expected given our POS tag set, it seems
better to restrict the modifications of the input for
word alignment to verbs or adjectives.
This simple modification of the training proce-
dure improves the coverage of the phrase-table,
but the OOV rate remains higher than with the
lemma backoff strategy. For the news-dev2009b
test set, 1186 additional phrases are available in
the phrase-table after replacing verb surface forms
by their lemma and POS combination. About half
of the test sentences are changed. As reflected by
the scores, most of the changes are small and do
not yield significantly different sentences. How-
ever, some translations are improved as can be
seen in Table 3.
The impact of both strategies combined is re-
ported in the decoding + alignment section of Ta-
ble 2. Tables 3 and 4 show positive and negative
examples of translations using the best combina-
tion.
</bodyText>
<sectionHeader confidence="0.996155" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9992751875">
We have described the system used for our sub-
mission, which is based on Moses with two sim-
ple modifications of the decoding input and word-
alignment strategy in order to improve coverage
without using additional training data. While
the improvements on automatic metrics are small,
manual inspection suggests that better morpholog-
ical analysis for the French side has potential to
improve translation quality. In future work, we
plan to improve the core model by including the
new large in-domain parallel corpus in training,
and to further experiment with French input rep-
resentations at different stages of training and de-
coding using more expressive POS tags such as
the MULTITAG tag set (Allauzen and Bonneau-
Maynard, 2008).
</bodyText>
<page confidence="0.998858">
153
</page>
<sectionHeader confidence="0.983146" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.8467743">
Alexandre Allauzen and H´el`ene Bonneau-
Maynard. Training and evaluation of pos
taggers on the french multitag corpus. In
European Language Resources Association
(ELRA), editor, Proceedings of the Sixth Inter-
national Language Resources and Evaluation
(LREC’08), Marrakech, Morocco, may 2008.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric for MT evaluation with
improved correlation with human judgement.
</bodyText>
<reference confidence="0.979315869047619">
In Proceedings of Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or
Summarization at the 43th Annual Meeting of
the Association of Computational Linguistics
(ACL-2005), Ann Arbor, Michigan, June 2005.
Daniel D´echelotte, Gilles Adda, Alexandre Al-
lauzen, H´el`ene Bonneau-Maynard, Olivier Gal-
ibert, Jean-Luc Gauvain, Philippe Langlais, and
Franc¸ois Yvon. LIMSIs statistical translation
systems for WMT08. In Proceedings of the
Third Workshop on Statistical Machine Trans-
lation, pages 107–110, Columbus, Ohio, 2008.
Philipp Koehn and Hieu Hoang. Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868–876, 2007.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. Moses: Open source toolkit for statisti-
cal machine translation. In Annual Meeting of
the Association for Computational Linguistics
(ACL), demonstration session, Prague, Czech
Republic, June 2007.
Franz Josef Och. Minimum error rate training in
statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167,
2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, 2002.
Maja Popovi´c and Hermann Ney. Towards the use
of word stems and suffixes for statistical ma-
chine translation. In Proceedings of the 4th In-
ternational Conference on Language Resources
and Evaluation (LREC-2004), 2004.
Fatiha Sadat and Nizar Habash. Combination of
arabic preprocessing schemes for statistical ma-
chine translation. In ACL-44: Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, pages 1–8, Morristown, NJ, USA, 2006.
Association for Computational Linguistics.
Helmut Schmid. Probabilistic part–of–speech tag-
ging using decision trees. In Proceedings of the
Conference on New Methods in Language Pro-
cessin, pages 44–49, Manchester, UK, 1994.
Holger Schwenk, Jean-Baptiste Fouet, and Jean
Senellart. First steps towards a general pur-
pose French/English statistical machine transla-
tion system. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages
119–122, Columbus, Ohio, June 2008. Associ-
ation for Computational Linguistics.
Andreas Stolcke. SRILM—an extensible lan-
guage modeling toolkit. In International Con-
ference on Spoken Language Processing, Den-
ver, Colorado, September 2002.
Sami Virpojia, Jaako J. V¨ayrynen, Mathias Creutz,
and Markus Sadeniemi. Morphology-aware sta-
tistical machine translation based on morphs in-
duced in an unsupervised manner. In Machine
Translation Summit XI, pages 491–498, Copen-
hagen, Denmark, September 2007.
Taro Watanabe, Hajime Tsukada, and Hideki
Isozaki. Ntt system description for the wmt2006
shared task. In Proceedings on the Workshop
on Statistical Machine Translation, pages 122–
125, New York City, June 2006. Association for
Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Stephan
Vogel, and Alex Waibel. The CMU-UKA Syn-
tax Augmented Machine Translation System
for IWSLT-06. In Proceedings of the Interna-
tional Workshop on Spoken Language Transla-
tion, pages 138–144, Kyoto, Japan, 2006.
</reference>
<page confidence="0.999771">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545373">
<title confidence="0.997061">Toward Using Morphology in French-English Phrase-based SMT</title>
<author confidence="0.97548">Marine</author>
<affiliation confidence="0.781432">Center for Computational Learning Columbia</affiliation>
<address confidence="0.998815">475 Riverside Drive, New York, NY</address>
<email confidence="0.998963">marine@ccls.columbia.edu</email>
<abstract confidence="0.999615">We describe the system used in our submission to the WMT-2009 French-English translation task. We use the Moses phrasebased Statistical Machine Translation system with two simple modications of the decoding input and word-alignment strategy based on morphology, and analyze their impact on translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2005</date>
<booktitle>In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005),</booktitle>
<location>Ann Arbor, Michigan,</location>
<marker>2005</marker>
<rawString>In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005), Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D´echelotte</author>
<author>Gilles Adda</author>
</authors>
<title>Alexandre Allauzen, H´el`ene Bonneau-Maynard, Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais, and Franc¸ois Yvon. LIMSIs statistical translation systems for WMT08.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>107--110</pages>
<location>Columbus, Ohio,</location>
<marker>D´echelotte, Adda, 2008</marker>
<rawString>Daniel D´echelotte, Gilles Adda, Alexandre Allauzen, H´el`ene Bonneau-Maynard, Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais, and Franc¸ois Yvon. LIMSIs statistical translation systems for WMT08. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 107–110, Columbus, Ohio, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<contexts>
<context position="938" citStr="Koehn and Hoang, 2007" startWordPosition="131" endWordPosition="134">Moses phrasebased Statistical Machine Translation system with two simple modications of the decoding input and word-alignment strategy based on morphology, and analyze their impact on translation quality. 1 Introduction In this first participation to the French-English translation task at WMT, our goal was to build a standard phrase-based statistical machine translation system and study the impact of French morphological variations at different stages of training and decoding. Many strategies have been proposed to integrate morphology information in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information</context>
<context position="7579" citStr="Koehn and Hoang, 2007" startWordPosition="1176" endWordPosition="1179">ly useful information available in our training corpus. Additional training data would naturally improve coverage, but will not cover all possible morphological variations of all verbs, especially for tenses and persons that are not used frequently in news coverage. It is therefore necessary to generalize beyond word surface forms. 4 Using morphological information in decoding A simple strategy for handling unknown words at decoding time consists in replacing their occurrences in the test set with their lemma, when it is part of the translation lexicon vocabulary. Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al., 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are 151 news-dev2009a representation OOV % METEOR BLEU NIST baseline surface form only 2.24 49.05 20.45 6.135 decoding lemma backoff 2.13 49.12 20.44 6.143 lemma+POS for all 2.24 48.87 20.36 6.145 word alignment lemma+POS for adj 2.25 48.94 20.46 6.131 lemma+POS for verbs 2.21 49.05 20.47 6.137 backoff + all 2.10 48.97 20.36 6.147 decoding + alignment backoff + adj 2.12 49.05 20.48 6.140 backoff + verbs 2.08 49.15</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876, 2007.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4650" citStr="Koehn et al., 2007" startWordPosition="709" endWordPosition="712">on-based heuristics. Second, tokenization, POS tagging and lemmatization are performed with Treetagger (Schmid, 1994) using the standard French and English parameter files1. Treetagger is based on Hidden Markov Models where transition probabilities are estimated with decision trees. The POS tag set consists of 33 tags which capture tense information for verbs, but not gender and number. Third, sentence-initial capitalized words are normalized to their most frequent form as reported by Zollmann et al. (2006). 2.3 Core system We use the Moses phrase-based statistical machine translation system (Koehn et al., 2007) and follow standard training, tuning and decoding strategies. The translation model consists of a standard Moses phrase-table with lexicalized reordering. Bidirectional word alignments obtained with GIZA++ are intersected using the grow-diag-final heuristic. Translations of phrases of up to 7 words long are collected and scored with translation probilities and lexical weighting. The English language model is a 4-gram model with Kneser-Ney smoothing, built with the SRI language modeling toolkit (Stolcke, 2002). The loglinear model feature weights were learned using minimum error rate training </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="5268" citStr="Och, 2003" startWordPosition="800" endWordPosition="801">low standard training, tuning and decoding strategies. The translation model consists of a standard Moses phrase-table with lexicalized reordering. Bidirectional word alignments obtained with GIZA++ are intersected using the grow-diag-final heuristic. Translations of phrases of up to 7 words long are collected and scored with translation probilities and lexical weighting. The English language model is a 4-gram model with Kneser-Ney smoothing, built with the SRI language modeling toolkit (Stolcke, 2002). The loglinear model feature weights were learned using minimum error rate training (MERT) (Och, 2003) with BLEU score (Papineni et al., 2002) as the objective function. Other decoding parameters were selected manually on an earlier version of the system trained and evaluated on the single-domain Europarl data. While the configuration achieved competitive results on the previous, it is not be optimal for this domain adaptation task. We will first conduct an analysis of this core SMT system, and experiment with two modifications of input representation for decoding and alignment respectively. 1www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ OOV verbs w/ surface w/ lemma+ form in POS in tr</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="5308" citStr="Papineni et al., 2002" startWordPosition="805" endWordPosition="808">g and decoding strategies. The translation model consists of a standard Moses phrase-table with lexicalized reordering. Bidirectional word alignments obtained with GIZA++ are intersected using the grow-diag-final heuristic. Translations of phrases of up to 7 words long are collected and scored with translation probilities and lexical weighting. The English language model is a 4-gram model with Kneser-Ney smoothing, built with the SRI language modeling toolkit (Stolcke, 2002). The loglinear model feature weights were learned using minimum error rate training (MERT) (Och, 2003) with BLEU score (Papineni et al., 2002) as the objective function. Other decoding parameters were selected manually on an earlier version of the system trained and evaluated on the single-domain Europarl data. While the configuration achieved competitive results on the previous, it is not be optimal for this domain adaptation task. We will first conduct an analysis of this core SMT system, and experiment with two modifications of input representation for decoding and alignment respectively. 1www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ OOV verbs w/ surface w/ lemma+ form in POS in training training corpus corpus dev2009a 2</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Towards the use of word stems and suffixes for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC-2004),</booktitle>
<marker>Popovi´c, Ney, 2004</marker>
<rawString>Maja Popovi´c and Hermann Ney. Towards the use of word stems and suffixes for statistical machine translation. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC-2004), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatiha Sadat</author>
<author>Nizar Habash</author>
</authors>
<title>Combination of arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="1485" citStr="Sadat and Habash (2006)" startWordPosition="213" endWordPosition="216">mation in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of *The author was partially funded by GALE DARPA Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. different input representations for French based on the POS and lemmatization provided by the Treetagger tool (Schmid, 1994). In the WMT09 French-English data sets, we observe that mo</context>
<context position="13329" citStr="Sadat and Habash (2006)" startWordPosition="2124" endWordPosition="2127">d for 3 categories of words: verbs only, adjectives only and all words. (2) the phrase-table and reordering models are learned as usual using word surface forms, but with the alignment links from step 1. In constrast with Watanabe et al. (2006), we attempt to generalize for specific word categories only, rather than use lemmas across all surface forms, as we found in earlier experiments that this approach did not help translation quality in our particular setting. Unlike other approaches which use morphological analysis to change the representation of the input (e.g., Popovi´c and Ney (2004), Sadat and Habash (2006), Virpojia et al. (2007)), our system still uses word surface forms as input during decoding. This is a constraint imposed by the relatively coarse analysis given by the default Treetagger lemmas and POS tags. Since they do not capture information that is crucial in translation such as number and gender, we need to keep surface forms as the input for translation. The impact of this strategy on automatic evaluation metrics is reported in the word alignment section of Table 2. Note that all experiments were performed using the parameters learned by MERT on news-dev2009a using the baseline config</context>
</contexts>
<marker>Sadat, Habash, 2006</marker>
<rawString>Fatiha Sadat and Nizar Habash. Combination of arabic preprocessing schemes for statistical machine translation. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 1–8, Morristown, NJ, USA, 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part–of–speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the Conference on New Methods in Language Processin,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2026" citStr="Schmid, 1994" startWordPosition="297" endWordPosition="298">ut representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of *The author was partially funded by GALE DARPA Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. different input representations for French based on the POS and lemmatization provided by the Treetagger tool (Schmid, 1994). In the WMT09 French-English data sets, we observe that more than half of the words that are unknown in the translation lexicon actually occur in the training data under different inflected forms. We show that combining a lemma backoff strategy at decoding time and improving alignments by generalizing across verb surface forms improves OOV rates and translation quality. 2 Translation system 2.1 Data sets We use a subset of the data made available for the official French to English translation task. The evaluation test set consists of French news data from September to October 2008, however th</context>
<context position="4148" citStr="Schmid, 1994" startWordPosition="633" endWordPosition="634">orpus was made available, but we were not able to use it due to time constraints. Additional use of this in-domain data would imProceedings of the Fourth Workshop on Statistical Machine Translation, pages 150–154, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 150 prove coverage and translation quality. 2.2 Preprocessing French and English corpora processing followed the same three steps: First, long sentences are resegmented using simple punctuation-based heuristics. Second, tokenization, POS tagging and lemmatization are performed with Treetagger (Schmid, 1994) using the standard French and English parameter files1. Treetagger is based on Hidden Markov Models where transition probabilities are estimated with decision trees. The POS tag set consists of 33 tags which capture tense information for verbs, but not gender and number. Third, sentence-initial capitalized words are normalized to their most frequent form as reported by Zollmann et al. (2006). 2.3 Core system We use the Moses phrase-based statistical machine translation system (Koehn et al., 2007) and follow standard training, tuning and decoding strategies. The translation model consists of a</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. Probabilistic part–of–speech tagging using decision trees. In Proceedings of the Conference on New Methods in Language Processin, pages 44–49, Manchester, UK, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean-Baptiste Fouet</author>
<author>Jean Senellart</author>
</authors>
<title>First steps towards a general purpose French/English statistical machine translation system.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>119--122</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1042" citStr="Schwenk et al., 2008" startWordPosition="146" endWordPosition="149">t and word-alignment strategy based on morphology, and analyze their impact on translation quality. 1 Introduction In this first participation to the French-English translation task at WMT, our goal was to build a standard phrase-based statistical machine translation system and study the impact of French morphological variations at different stages of training and decoding. Many strategies have been proposed to integrate morphology information in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of *The author was par</context>
<context position="7637" citStr="Schwenk et al., 2008" startWordPosition="1184" endWordPosition="1187">itional training data would naturally improve coverage, but will not cover all possible morphological variations of all verbs, especially for tenses and persons that are not used frequently in news coverage. It is therefore necessary to generalize beyond word surface forms. 4 Using morphological information in decoding A simple strategy for handling unknown words at decoding time consists in replacing their occurrences in the test set with their lemma, when it is part of the translation lexicon vocabulary. Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al., 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are 151 news-dev2009a representation OOV % METEOR BLEU NIST baseline surface form only 2.24 49.05 20.45 6.135 decoding lemma backoff 2.13 49.12 20.44 6.143 lemma+POS for all 2.24 48.87 20.36 6.145 word alignment lemma+POS for adj 2.25 48.94 20.46 6.131 lemma+POS for verbs 2.21 49.05 20.47 6.137 backoff + all 2.10 48.97 20.36 6.147 decoding + alignment backoff + adj 2.12 49.05 20.48 6.140 backoff + verbs 2.08 49.15 20.50 6.148 news-dev2009b representation OOV % METEOR BLE</context>
</contexts>
<marker>Schwenk, Fouet, Senellart, 2008</marker>
<rawString>Holger Schwenk, Jean-Baptiste Fouet, and Jean Senellart. First steps towards a general purpose French/English statistical machine translation system. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 119–122, Columbus, Ohio, June 2008. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="5165" citStr="Stolcke, 2002" startWordPosition="785" endWordPosition="786">re system We use the Moses phrase-based statistical machine translation system (Koehn et al., 2007) and follow standard training, tuning and decoding strategies. The translation model consists of a standard Moses phrase-table with lexicalized reordering. Bidirectional word alignments obtained with GIZA++ are intersected using the grow-diag-final heuristic. Translations of phrases of up to 7 words long are collected and scored with translation probilities and lexical weighting. The English language model is a 4-gram model with Kneser-Ney smoothing, built with the SRI language modeling toolkit (Stolcke, 2002). The loglinear model feature weights were learned using minimum error rate training (MERT) (Och, 2003) with BLEU score (Papineni et al., 2002) as the objective function. Other decoding parameters were selected manually on an earlier version of the system trained and evaluated on the single-domain Europarl data. While the configuration achieved competitive results on the previous, it is not be optimal for this domain adaptation task. We will first conduct an analysis of this core SMT system, and experiment with two modifications of input representation for decoding and alignment respectively. </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. SRILM—an extensible language modeling toolkit. In International Conference on Spoken Language Processing, Denver, Colorado, September 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpojia</author>
<author>Jaako J V¨ayrynen</author>
<author>Mathias Creutz</author>
<author>Markus Sadeniemi</author>
</authors>
<title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
<date>2007</date>
<booktitle>In Machine Translation Summit XI,</booktitle>
<pages>491--498</pages>
<location>Copenhagen, Denmark,</location>
<marker>Virpojia, V¨ayrynen, Creutz, Sadeniemi, 2007</marker>
<rawString>Sami Virpojia, Jaako J. V¨ayrynen, Mathias Creutz, and Markus Sadeniemi. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. In Machine Translation Summit XI, pages 491–498, Copenhagen, Denmark, September 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Ntt system description for the wmt2006 shared task.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>122--125</pages>
<location>New York City,</location>
<contexts>
<context position="1312" citStr="Watanabe et al., 2006" startWordPosition="187" endWordPosition="190">ystem and study the impact of French morphological variations at different stages of training and decoding. Many strategies have been proposed to integrate morphology information in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of *The author was partially funded by GALE DARPA Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. different </context>
<context position="12950" citStr="Watanabe et al. (2006)" startWordPosition="2064" endWordPosition="2067">ing morphological information in word-alignment In this experiment, we would like to use morphological analysis to alleviate the alignment errors because of which some words from the parallel corpus are not in the phrase-table. We adopt a two-step approach: (1) before word alignment, replace surface forms by lemma and POS tags. In our experiments, this replacement is performed for 3 categories of words: verbs only, adjectives only and all words. (2) the phrase-table and reordering models are learned as usual using word surface forms, but with the alignment links from step 1. In constrast with Watanabe et al. (2006), we attempt to generalize for specific word categories only, rather than use lemmas across all surface forms, as we found in earlier experiments that this approach did not help translation quality in our particular setting. Unlike other approaches which use morphological analysis to change the representation of the input (e.g., Popovi´c and Ney (2004), Sadat and Habash (2006), Virpojia et al. (2007)), our system still uses word surface forms as input during decoding. This is a constraint imposed by the relatively coarse analysis given by the default Treetagger lemmas and POS tags. Since they </context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. Ntt system description for the wmt2006 shared task. In Proceedings on the Workshop on Statistical Machine Translation, pages 122– 125, New York City, June 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU-UKA Syntax Augmented Machine Translation System for IWSLT-06.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>138--144</pages>
<location>Kyoto, Japan,</location>
<contexts>
<context position="4543" citStr="Zollmann et al. (2006)" startWordPosition="693" endWordPosition="696">corpora processing followed the same three steps: First, long sentences are resegmented using simple punctuation-based heuristics. Second, tokenization, POS tagging and lemmatization are performed with Treetagger (Schmid, 1994) using the standard French and English parameter files1. Treetagger is based on Hidden Markov Models where transition probabilities are estimated with decision trees. The POS tag set consists of 33 tags which capture tense information for verbs, but not gender and number. Third, sentence-initial capitalized words are normalized to their most frequent form as reported by Zollmann et al. (2006). 2.3 Core system We use the Moses phrase-based statistical machine translation system (Koehn et al., 2007) and follow standard training, tuning and decoding strategies. The translation model consists of a standard Moses phrase-table with lexicalized reordering. Bidirectional word alignments obtained with GIZA++ are intersected using the grow-diag-final heuristic. Translations of phrases of up to 7 words long are collected and scored with translation probilities and lexical weighting. The English language model is a 4-gram model with Kneser-Ney smoothing, built with the SRI language modeling t</context>
</contexts>
<marker>Zollmann, Venugopal, Vogel, Waibel, 2006</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Stephan Vogel, and Alex Waibel. The CMU-UKA Syntax Augmented Machine Translation System for IWSLT-06. In Proceedings of the International Workshop on Spoken Language Translation, pages 138–144, Kyoto, Japan, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>