<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<note confidence="0.587722">
*SEM 2012 Shared Task: Resolving the Scope and Focus of Negation
</note>
<author confidence="0.917186">
Roser Morante Eduardo Blanco
</author>
<affiliation confidence="0.911648">
CLiPS - University of Antwerp Lymba Corporation
</affiliation>
<address confidence="0.922478">
Prinsstraat 13, B-2000 Antwerp, Belgium Richardson, TX 75080 USA
</address>
<email confidence="0.994579">
Roser.Morante@ua.ac.be eduardo@lymba.com
</email>
<sectionHeader confidence="0.99366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997473">
The Joint Conference on Lexical and Compu-
tational Semantics (*SEM) each year hosts a
shared task on semantic related topics. In its
first edition held in 2012, the shared task was
dedicated to resolving the scope and focus of
negation. This paper presents the specifica-
tions, datasets and evaluation criteria of the
task. An overview of participating systems is
provided and their results are summarized.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952918367347">
Semantic representation of text has received consid-
erable attention these past years. While early shal-
low approaches have been proven useful for several
natural language processing applications (Wu and
Fung, 2009; Surdeanu et al., 2003; Shen and La-
pata, 2007), the field is moving towards analyzing
and processing complex linguistic phenomena, such
as metaphor (Shutova, 2010) or modality and nega-
tion (Morante and Sporleder, 2012).
The *SEM 2012 Shared Task is devoted to nega-
tion, specifically, to resolving its scope and focus.
Negation is a grammatical category that comprises
devices used to reverse the truth value of proposi-
tions. Broadly speaking, scope is the part of the
meaning that is negated and focus the part of the
scope that is most prominently or explicitly negated
(Huddleston and Pullum, 2002). Although negation
is a very relevant and complex semantic aspect of
language, current proposals to annotate meaning ei-
ther dismiss negation or only treat it in a partial man-
ner.
The interest in automatically processing nega-
tion originated in the medical domain (Chapman
et al., 2001), since clinical reports and discharge
summaries must be reliably interpreted and indexed.
The annotation of negation and hedge cues and their
scope in the BioScope corpus (Vincze et al., 2008)
represented a pioneering effort. This corpus boosted
research on scope resolution, especially since it was
used in the CoNLL 2010 Shared Task (CoNLL
ST 2010) on hedge detection (Farkas et al., 2010).
Negation has also been studied in sentiment analy-
sis (Wiegand et al., 2010) as a means to determine
the polarity of sentiments and opinions.
Whereas several scope detectors have been de-
veloped using BioScope (Morante and Daelemans,
2009; Velldal et al., 2012), there is a lack of cor-
pora and tools to process negation in general domain
texts. This is why we have prepared new corpora
for scope and focus detection. Scope is annotated
in Conan Doyle stories (CD-SCO corpus). For each
negation, the cue, its scope and the negated event, if
any, are marked as shown in example (1a). Focus is
annotated on top of PropBank, which uses the WSJ
section of the Penn TreeBank (PB-FOC corpus). Fo-
cus annotation is restricted to verbal negations an-
notated with MNEG in PropBank, and all the words
belonging to a semantic role are selected as focus.
An annotated example is shown in (1b)1.
</bodyText>
<listItem confidence="0.597314">
(1) a. [John had] never [said as much before]
b. John had never said {as much} before
</listItem>
<bodyText confidence="0.999850857142857">
The rest of this paper is organized as follows.
The two proposed tasks are described in Section 2,
and the corpora in Section 3. Participating systems
and their results are summarized in Section 4. The
approaches used by participating systems are de-
scribed in Section 5, as well as the analysis of re-
sults. Finally, Section 6 concludes the paper.
</bodyText>
<footnote confidence="0.686208666666667">
1Throughout this paper, negation cues are marked in bold
letters, scopes are enclosed in square brackets and negated
events are underlined; focus is enclosed in curly brackets.
</footnote>
<page confidence="0.945747">
265
</page>
<note confidence="0.980554">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 265–274,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.797414" genericHeader="method">
2 Task description
</sectionHeader>
<bodyText confidence="0.995013">
The *SEM 2012 Shared Task2 was dedicated to re-
solving the scope and focus of negation (Task 1 and
2 respectively). Participants were allowed to engage
in any combination of tasks and submit at most two
runs per task. A pilot task combining scope and
focus detection was initially planned, but was can-
celled due to lack of participation. We received a
total of 14 runs, 12 for scope detection (7 closed, 5
open) and 2 for focus detection (0 closed, 2 open).
Submissions fall into two tracks:
</bodyText>
<listItem confidence="0.783018571428571">
• Closed track. Systems are built using exclusively
the annotations provided in the training set and are
tuned with the development set. Systems that do
not use external tools to process the input text or
that modify the annotations provided (e.g., simplify
parse tree, concatenate lists of POS tags, ) fall under
this track.
• Open track. Systems can make use of any external
resource or tool. For example, if a team uses an ex-
ternal semantic parser, named entity recognizer or
obtains the lemma for each token by querying ex-
ternal resources, it falls under the open track. The
tools used cannot have been developed or tuned us-
ing the annotations of the test set.
</listItem>
<bodyText confidence="0.999529125">
Regardless of the track, teams were allowed to
submit their final results on the test set using a sys-
tem trained on both the training and development
sets. The data format is the same as in several pre-
vious CoNLL Shared Tasks (Surdeanu et al., 2008).
Sentences are separated by a blank line. Each sen-
tence consists of a sequence of tokens, and a new
line is used for each token.
</bodyText>
<subsectionHeader confidence="0.98379">
2.1 Task 1: Scope Resolution
</subsectionHeader>
<bodyText confidence="0.999904">
Task 1 aimed at resolving the scope of negation cues
and detecting negated events. The task is divided
into 3 subtasks:
</bodyText>
<listItem confidence="0.9129189">
1. Identifying negation cues, i.e., words that express
negation. Cues can be single words (e.g., never),
multiwords (e.g., no longer, by no means), or affixes
(e.g.l im-, -less). Note that negation cues can be
discontinuous, e.g., neither [... ] nor.
2. Resolving the scope of negation. This subtask ad-
dresses the problem of determining which tokens
within a sentence are affected by the negation cue.
A scope is a sequence of tokens that can be discon-
tinuous.
</listItem>
<footnote confidence="0.805496">
2www.clips.ua.ac.be/sem2012-st-neg/
</footnote>
<bodyText confidence="0.796151">
3. Identifying the negated event or property, if any.
The negated event or property is always within the
scope of a cue. Only factual events can be negated.
For the sentence in (2), systems have to identify
no and nothing as negation cues, after his habit he
said and after mine I asked questions as scopes, and
said and asked as negated events.
(2) [After his habit he said] nothing, and after mine I
asked no questions.
After his habit he said nothing, and [after mine I
asked] no [questions].
</bodyText>
<subsectionHeader confidence="0.839495">
2.1.1 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.999790625">
Previously, scope resolvers have been evaluated at
either the token or scope level. The token level eval-
uation checks whether each token is correctly la-
beled (inside or outside the scope), while the scope
level evaluation checks whether the full scope is cor-
rectly labeled. The CoNLL 2010 ST introduced pre-
cision and recall at scope level as performance mea-
sures and established the following requirements: A
true positive (TP) requires an exact match for both
the negation cue and the scope. False positives (FP)
occur when a system predicts a non-existing scope
in gold, or when it incorrectly predicts a scope exist-
ing in gold because: (1) the negation cue is correct
but the scope is incorrect; (2) the cue is incorrect
but the scope is correct; (3) both cue and scope are
incorrect. These three scenarios also trigger a false
negative (FN). Finally, FN also occur when the gold
annotations specify a scope but the system makes no
such prediction (Farkas et al., 2010).
As we see it, the CONLL 2010 ST evaluation
requirements were somewhat strict because for a
scope to be counted as TP, the negation cue had
to be correctly identified (strict match) as well as
the punctuation tokens within the scope. Addi-
tionally, this evaluation penalizes partially correct
scopes more than fully missed scopes, since partially
correct scopes count as FP and FN, whereas missed
scopes count only as FN. This is a standard prob-
lem when applying the F measures to the evaluation
of sequences. For this shared task we have adopted
a slightly different approach based on the following
criteria:
</bodyText>
<listItem confidence="0.987008">
• Punctuation tokens are ignored.
• We provide a scope level measure that does not re-
quire strict cue match. To count a scope as TP this
</listItem>
<page confidence="0.992707">
266
</page>
<bodyText confidence="0.857128">
measure requires that only one cue token is cor-
rectly identified, instead of all cue tokens.
</bodyText>
<listItem confidence="0.984936666666667">
• To count a negated event as TP we do not require
correct identification of the cue.
• To evaluate cues, scopes and negated events, partial
matches are not counted as FP, only as FN. This is to
avoid penalizing partial matches more than missed
matches.
</listItem>
<bodyText confidence="0.78072">
The following evaluation measures have been
used to evaluate the systems:
</bodyText>
<listItem confidence="0.999533944444444">
• Cue-level F1-measures (Cue).
• Scope-level F1-measures that require only partial
cue match (Scope NCM).
• Scope-level F1-measures that require strict cue
match (Scope CM). In this case, all tokens of the
cue have to be correctly identified.
• F1-measure over negated events (Negated), com-
puted independently from cues and from scopes.
• Global F1-measure of negation (Global): the three
elements of the negation — cue, scope and negated
event — all have to be correctly identified (strict
match).
• F1-measure over scope tokens (Scope tokens). The
total of scope tokens in a sentence is the sum of to-
kens of all scopes. For example, if a sentence has
two scopes, one of five tokens and another of seven
tokens, then the total of scope tokens is twelve.
• Percentage of correct negation sentences (CNS).
</listItem>
<bodyText confidence="0.99995405">
A second version of the measures (Cue/Scope
CM/Scope NCM/Negated/Global-B) was calculated
and provided to participants, but was not used to
rank the systems, because it was introduced in the
last period of the development phase following the
request of a participant team. In the B version of the
measures, precision is not counted as (TP/(TP+FP)),
but as (TP / total of system predictions), counting in
this way the percentage of perfect matches among
all the system predictions. Providing this version of
the measures also allowed us to compare the results
of the two versions and to check if systems would
be ranked in a different position depending on the
version.
Even though we believe that relaxing scope eval-
uation by ignoring punctuation marks and relaxing
the strict cue match requirement is a positive feature
of our evaluation, we need to explore further in order
to define a scope evaluation measure that captures
the impact of partial matches in the scores.
</bodyText>
<subsectionHeader confidence="0.998975">
2.2 Task 2: Focus Detection
</subsectionHeader>
<bodyText confidence="0.999915681818182">
This task tackles focus of negation detection. Both
scope and focus are tightly connected. Scope is the
part of the meaning that is negated and focus is that
part of the scope that is most prominently or explic-
itly negated (Huddleston and Pullum, 2002). Focus
can also be defined as the element of the scope that is
intended to be interpreted as false to make the over-
all negative true.
Detecting focus of negation is useful for retriev-
ing the numerous words that contribute to implicit
positive meanings within a negation. Consider the
statement The government didn’t release the UFO
files {until 2008}. The focus is until 2008, yielding
the interpretation The government released the UFO
files, but not until 1998. Once the focus is resolved,
the verb release, its AGENT The government and its
THEME the UFO files are positive; only the TEMPO-
RAL information until 2008 remains negated.
We only target verbal negations and focus is al-
ways the full text of a semantic role. Some examples
of annotation and their interpretation (Int) using fo-
cus detection are provided in (3–5).
</bodyText>
<listItem confidence="0.998128416666667">
(3) Even if that deal isn’t {revived}, NBC hopes to
find another.
Int: Even if that deal is suppressed, NBC hopes to
find another.
(4) A decision isn’t expected {until some time next
year}.
Int: A decision is expected at some time next year.
(5) ... it told the SEC it couldn’t provide financial
statements by the end of its first extension
“{without unreasonable burden or expense}”.
Int: It could provide them by that time with a huge
overhead.
</listItem>
<subsectionHeader confidence="0.561326">
2.2.1 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.99995025">
Task 2 is evaluated using precision, recall and Fl.
Submissions are ranked by Fl. For each negation,
the predicted focus is considered correct if it is a per-
fect match with the gold annotations.
</bodyText>
<sectionHeader confidence="0.987722" genericHeader="method">
3 Data Sets
</sectionHeader>
<bodyText confidence="0.999993">
We have released two datasets, which will be avail-
able from the web site of the task: CD-SCO for
scope detection and PB-FOC for focus detection.
The next two sections introduce the datasets.
</bodyText>
<page confidence="0.993932">
267
</page>
<figureCaption confidence="0.996924">
Figure 1: Example sentence from CD-SCO.
</figureCaption>
<table confidence="0.916506">
questions
WL2 108 0 After After IN (S(S(PP*
WL2 108 1 his his PRP$ (NP*
WL2 108 2 habit habit NN *))
WL2 108 3 he he PRP (NP*)
WL2 108 4 said say VBD (VP*
WL2 108 5 nothing nothing NN (NP*)))
WL2 108 6 , , , *
WL2 108 7 and and CC *
WL2 108 8 after after IN (S(PP*
WL2 108 9 mine mine NN (NP*))
WL2 108 10 I I PRP (NP*)
WL2 108 11 asked ask VBD (VP*
WL2 108 12 no no DT (NP*
WL2 108 13 questions question NNS *)))
WL2 108 14 . . . *)
</table>
<figure confidence="0.979598818181818">
nothing
after
mine
I
asked asked
no
After
his
habit
he
said said
</figure>
<subsectionHeader confidence="0.99818">
3.1 CD-SCO: Scope Annotation
</subsectionHeader>
<bodyText confidence="0.999961758620689">
The corpus for Task 1 is CD-SCO, a corpus of Co-
nan Doyle stories. The training corpus contains The
Hound of the Baskervilles, the development corpus,
The Adventure of Wisteria Lodge, and the test corpus
The Adventure of the Red Circle and The Adventure
of the Cardboard Box. The original texts are freely
available from the Gutenberg Project.3
CD-SCO is annotated with negation cues and
their scope, as well as the event or property that is
negated. The cues are the words that express nega-
tion and the scope is the part of a sentence that is
affected by the negation cues. The negated event
or property is the main event or property actually
negated by the negation cue. An event can be a pro-
cess, an action, or a state.
Figure 1 shows an example sentence. Column 1
contains the name of the file, column 2 the sentence
#, column 3 the token #, column 4 the word, column
5 the lemma, column 6 the PoS, column 7 the parse
tree information and columns 8 to end the negation
information. If a sentence does not contain a nega-
tion, column 8 contains “***” and there are no more
columns. If it does contain negations, the informa-
tion for each one is encoded in three columns: nega-
tion cue, scope, and negated event respectively.
The annotation of cues and scopes is inspired by
the BioScope corpus, but there are several differ-
ences. First and foremost, BioScope does not an-
notate the negated event or property. Another im-
</bodyText>
<footnote confidence="0.925715">
3http://www.gutenberg.org/browse/
authors/d\#a37238
</footnote>
<table confidence="0.999654111111111">
Training Dev. Test
# tokens 65,450 13,566 19,216
# sentences 3644 787 1089
# negation sent. 848 144 235
% negation sent. 23.27 18.29 21.57
# cues 984 173 264
# unique cues 30 20 20
#scopes 887 168 249
# negated 616 122 173
</table>
<tableCaption confidence="0.999695">
Table 1: CD-SCO Corpus statistics.
</tableCaption>
<bodyText confidence="0.99994325">
portant difference concerns the scope model itself:
in CD-SCO, the cue is not considered to be part of
the scope. Furthermore, scopes can be discontinu-
ous and all arguments of the negated event are con-
sidered to be part of the scope, including the subject,
which is kept out of the scope in BioScope. A final
difference is that affixal negation is annotated in CD-
SCO, as in (6).
</bodyText>
<listItem confidence="0.8423925">
(6) [He] declares that he heard cries but [is] un[{able}
to state from what direction they came].
</listItem>
<bodyText confidence="0.999754777777778">
Statistics for the corpus is presented in Table 1.
More information about the annotation guidelines is
provided by Morante et al. (2011) and Morante and
Daelemans (2012), including inter-annotator agree-
ment.
The corpus was preprocessed at the University
of Oslo. Tokenization was obtained by the PTB-
compliant tokenizer that is part of the LinGO En-
glish Resource Grammar. 4
</bodyText>
<footnote confidence="0.974786">
4http://moin.delph-in.net/
</footnote>
<page confidence="0.99418">
268
</page>
<bodyText confidence="0.9891105">
Apart from the gold annotations, the corpus was
provided to participants with additional annotations:
</bodyText>
<listItem confidence="0.982246454545454">
• Lemmatization using the GENIA tagger (Tsuruoka
and Tsujii, 2005), version 3.0.1, with the ’-nt’ com-
mand line option. GENIA PoS tags are comple-
mented with TnT PoS tags for increased compati-
bility with the original PTB.
• Parsing with the Charniak and Johnson (2005) re-
ranking parser.5 For compatibility with PTB con-
ventions, the top-level nodes in parse trees (‘S1’),
were removed. The conversion of PTB-style syntax
trees into CoNLL-style format was performed using
the CoNLL 2005 Shared Task software.6
</listItem>
<subsectionHeader confidence="0.998387">
3.2 PB-FOC: Focus Annotation
</subsectionHeader>
<bodyText confidence="0.991497352941177">
We have adapted the only previous annotation effort
targeting focus of negation for PB-FOC (Blanco and
Moldovan, 2011). This corpus provides focus an-
notation on top of PropBank. It targets exclusively
verbal negations marked with MNEG in PropBank
and selects as focus the semantic role containing the
most likely focus. The motivation behind their ap-
proach, annotation guidelines and examples can be
found in the aforementioned paper.
We gathered all negations from sections 02–21,
23 and 24 and discarded negations for which the fo-
cus or PropBank annotations were not sound, leav-
ing 3,544 instances.7 For each verbal negation, PB-
FOC provides the current sentence, and the previous
and next sentences as context. For each sentence,
along with the gold focus annotations, PB-FOC con-
tains the following additional annotations:
</bodyText>
<listItem confidence="0.998360222222222">
• Token number;
• POS tags using the Brill tagger (Brill, 1992);
• Named Entities using the Stanford named en-
tity recognizer recognizer (Finkel et al., 2005);
• Chunks using the chunker by Phan (2006);
• Syntactic tree using the Charniak parser (Char-
niak, 2000);
• Dependency tree derived from the syntactic
tree (de Marneffe et al., 2006);
</listItem>
<footnote confidence="0.972494">
ErgTokenization, http://moin.delph-in.net/
ReppTop
5November 2009 release available from Brown University.
6http://www.lsi.upc.edu/˜srlconll/
srlconll-1.1.tgz
7The original focus annotation targeted the 3,993 negations
marked with MNEG in the whole PropBank.
</footnote>
<table confidence="0.999944526315789">
Train Devel Test
1 role 2,210 515 672
2 roles 89 15 38
3 roles 3 0 2
All 2,302 530 712
Semantic roles focus belongs to A1 980 222 309
AM-NEG 592 138 172
AM-TMP 161 35 46
AM-MNR 127 27 38
A2 112 28 36
A0 94 23 31
None 88 19 35
AM-ADV 78 23 26
C-A1 46 6 16
AM-PNC 33 8 12
AM-LOC 25 4 10
A4 11 2 5
R-A1 10 2 2
Other 40 8 16
</table>
<tableCaption confidence="0.858986">
Table 2: Basic numeric analysis for PB-FOC. The first 4
rows indicate the number of unique roles each negation
belongs to, the rest indicate the counts for each role.
</tableCaption>
<listItem confidence="0.9647008">
• Semantic roles using the labeler described by
(Punyakanok et al., 2008); and
• Verbal negation, indicates with ‘N’ if that token
correspond to a verbal negation for which focus
must be predicted.
</listItem>
<bodyText confidence="0.998415230769231">
Figure 2 provides a sample of PB-FOC. Know-
ing that the original focus annotations were done on
top of PropBank and that focus corresponds to a sin-
gle role, semantic role information is key to predict
the focus. In Table 2, we show some basic numeric
analysis regarding focus annotation and the automat-
ically obtained semantic role labels. Most instances
of focus belong to a single role in the three splits
and the most common role focus belongs to is A1,
followed by AM-NEG, M-TMP and M-MNR. Note
that some instances have at least one word that does
not belong to any role (88 in training, 19 in develop-
ment and 35 in test).
</bodyText>
<sectionHeader confidence="0.981396" genericHeader="method">
4 Submissions and results
</sectionHeader>
<bodyText confidence="0.9999492">
A total of 14 runs were submitted: 12 for scope de-
tection and 2 for focus detection. The unbalanced
number of submissions might be due to the fact that
both tasks are relatively new and the tight timeline
(six weeks) under which systems were developed.
</bodyText>
<page confidence="0.992726">
269
</page>
<table confidence="0.9670148125">
Marketers 1 NNS O B-NP (S1(S(NP*) 2 nsubj (A0*) * - *
believe 2 VBP O B-VP (VP* 0 root (V*) * - *
most 3 RBS O B-NP (SBAR(S(NP* 4 amod (A1* (A0* - FOCUS
Americans 4 NNPS O I-NP *) 7 nsubj * *) - FOCUS
wo 5 MD O B-VP (VP* 7 aux * (AM-MOD*) -*
n’t 6 RB O I-VP * 7 neg * (AM-NEG*) - *
make 7 VB O I-VP (VP* 2 ccomp * (V*) N *
the 8 DT O B-NP (NP* 10 det * (A1* - *
convenience 9 NN O I-NP * 10 nn * * - *
trade-off 10 NN O I-NP *)))))) 7 dobj *) *) - *
... 11 : O O * 2 punct * * -*
. 12 . O O *)) 2 punct * * - *
Figure 2: Example sentence from PB-FOC.
Team Prec. Rec. F1
Open UConcordia, run 1 60.00 56.88 58.40
UConcordia, run 2 59.85 56.74 58.26
</table>
<tableCaption confidence="0.999501">
Table 3: Official results for Task 2.
</tableCaption>
<bodyText confidence="0.999755">
Some participants showed interest in the second task
and expressed that they did not participate because
of lack of time. In this section, we present the results
for each task.
</bodyText>
<subsectionHeader confidence="0.981463">
4.1 Task 1
</subsectionHeader>
<bodyText confidence="0.999963833333333">
Six teams (UiO1, UiO2, FBK, UWashington,
UMichigan, UABCoRAL) submitted results for the
closed track with a total of seven runs, and four
teams (UiO2, UGroningen, UCM-1, UCM-2) sub-
mitted results for the open track with a total of five
runs. The evaluation results are provided in Ta-
ble 4, which contains the official results, and Table 5,
which contains the results for evaluation measures
B.
The best Global score in the closed track was ob-
tained by UiO1 (57.63 F1). The best score for Cues
was obtained by FBK (92.34 F1), for Scopes CM
by UiO2 (73.39 F1), for Scopes NCM by UWash-
ington (72.40 F1), and for Negated by UiO1 (67.02
F1). The best Global score in the open track was ob-
tained by UiO2 (54.82 F1), as well as the best scores
for Cues (91.31 F1), Scopes CM (72.39 F1), Scopes
NCM (72.39 F1), and Negated (61.79 F1).
</bodyText>
<subsectionHeader confidence="0.833267">
4.2 Task 2
</subsectionHeader>
<bodyText confidence="0.9998795">
Only one team participated in Task 2, UConcordia
from CLaC Lab at Concordia University. They sub-
mitted two runs and the official results are summa-
rized in Table 3. Their best run scored 58.40 F1.
</bodyText>
<sectionHeader confidence="0.955917" genericHeader="evaluation">
5 Approaches and analysis
</sectionHeader>
<bodyText confidence="0.998822333333333">
In this section we summarize the methodologies ap-
plied by participants to solve the tasks and we ana-
lyze the results.
</bodyText>
<subsectionHeader confidence="0.943114">
5.1 Task 1
</subsectionHeader>
<bodyText confidence="0.999914068965517">
To solve Task 1 most teams develop a three module
pipeline with a module per subtask. Scope resolu-
tion and negated event detection are independent of
each other and both depend on cue detection. An
exception is the UiO1 system, which incorporates a
module for factuality detection. Most systems ap-
ply machine learning algorithms, either Conditional
Random Fields (CRFs) or Support Vector Machines
(SVMs), while less systems implement a rule-based
approach. Syntax information is widely employed,
either in the form of rules or incorporated in the
learning model. Multi-word and affixal negation
cues receive a special treatment in most cases, and
scopes are generally postprocessed.
The systems that participate in the closed track
are machine learning based. The UiO1 system is an
adaptation of another system (Velldal et al., 2012),
which combines SVM cue classification with SVM-
based ranking of syntactic constituents for scope
resolution. The approach is extended to identify
negated events by first classifying negations as fac-
tual or non-factual, and then applying an SVM
ranker over candidate events. The original treat-
ment of factuality in this system results in the high-
est score for both the negated event subtask and the
global task.
The UiO2 system combines SVM cue classifica-
tion with CRF-based sequence labeling. An original
aspect of the UiO2 approach is the model represen-
</bodyText>
<page confidence="0.992577">
270
271
</page>
<figure confidence="0.985636587155963">
43.83
26.81
27.23
34.04
35.74
40.00
42.13
41.28
7.66
11.91
18.72
27.23
% CNS
Global
Prec. Rec. F1
41.90 45.08 43.43
Global B
7.66 7.58 7.62
12.34 10.98 11.62
Prec. Rec. F1
25.70 27.65 26.64
38.03 40.91 39.42
38.03 40.91 39.42
20.50 21.59 21.03
27.59 27.27 27.43
39.08 42.05 40.51
30.00 28.41 29.18
32.98 35.61 34.24
42.18 43.94 43.04
Negated
Prec. Rec. F1
58.60 75.00 65.79
Negated B
Prec. Rec. F1
60.39 56.71 58.49
67.16 52.63 59.01
52.66 52.05 52.35
55.22 65.29 59.83
53.90 50.92 52.37
66.67 12.72 21.36
63.82 57.40 60.44
62.50 38.46 47.62
38.25 52.24 44.16
57.62 72.89 64.36
44.44 21.18 28.69
Scope Tokens
Prec. Rec. F1
56.55 60.64 58.52
Scopes B NCM
Prec. Rec. F1
12.26 12.85 12.55
58.23 58.23 58.23
59.32 62.65 60.94
39.84 40.96 40.39
59.54 62.65 61.06
56.90 54.62 55.74
55.51 50.60 52.94
59.26 64.26 61.66
59.30 61.45 60.36
45.67 46.59 46.13
41.63 38.96 40.25
Scopes NCM
Prec. Rec. F1
56.55 60.64 58.52
Scopes B CM
Prec. Rec. F1
12.26 12.85 12.55
58.23 58.23 58.23
59.32 62.65 60.94
39.84 40.96 40.39
59.30 61.45 60.36
59.54 62.65 61.06
55.23 53.01 54.10
55.51 50.60 52.94
58.52 63.45 60.89
45.67 46.59 46.13
41.20 38.55 39.83
Scopes CM
Prec. Rec. F1
86.97 93.56 90.14
Cues B
89.09 92.80 90.91
Prec. Rec. F1
91.63 91.29 91.46
86.97 93.56 90.14
83.91 82.95 83.43
86.69 91.29 88.93
85.82 84.85 85.33
92.80 87.88 90.27
86.97 93.56 90.14
85.26 92.05 88.52
79.58 85.61 82.48
72.34 64.39 68.13
Cues
Prec. Rec. F1
UABCoRAL
UGroningen r1
UMichigan
UWashington
UGroningen r2
UiO2
UiO1 r1
UiO2
FBK
UCM-1
UCM-2
UiO1 r2
Official results for Task 1
Participating institutions:
</figure>
<affiliation confidence="0.6975895">
UiO: University of Oslo; FBK: Fondazione Bruno Kessler &amp;University of Trento; UWashington: University of Washington; UMichigan: University of Michigan; UABCoRAL:
CoRAL Lab University of Alabama; UGroningen: University of Groningen; UCM: Complutense University of Madrid.
</affiliation>
<tableCaption confidence="0.998756">
Table 5: Results withevaluation measures B. Precision is calculated as: true positives / total of system predictions. “r1” stands for run 1 nd “r2” for run 2. “CM”
stands for Cue Match and “NCM” stands for No Cue Match.
Table 4: Official results. “r1” stands for run 1 nd “r2” for run 2. CNS stands for Correct Negation Sentences. “CM” stands for Cue Match and “NCM” stands for
</tableCaption>
<table confidence="0.987348726027397">
No Cue Match.
79.87 45.08 57.63
66.36 27.65 39.04
84.27 28.41 42.49
84.96 36.36 50.93
83.45 43.94 57.57
78.26 40.91 53.73
74.02 35.61 48.09
78.72 42.05 54.82
37.74 7.58 12.62
66.28 21.59 32.57
72.00 27.27 39.56
42.65 10.98 17.46
60.58 75.00 67.02
65.00 38.46 48.33
50.00 52.24 51.10
58.04 50.92 54.25
64.14 56.71 60.20
68.18 52.63 59.40
60.50 72.89 66.12
66.90 57.40 61.79
53.94 52.05 52.98
66.67 12.72 21.36
56.63 65.29 60.65
46.15 21.18 29.03
75.87 90.08 82.37
85.37 68.86 76.23
84.85 80.66 82.70
83.26 83.77 83.51
81.53 82.44 81.98
86.03 81.55 83.73
81.99 88.81 85.26
82.25 82.16 82.20
69.69 70.30 69.99
58.30 67.70 62.65
85.37 68.53 76.03
69.20 82.27 75.17
83.89 60.64 70.39
90.00 50.60 64.78
82.90 64.26 72.40
88.96 58.23 70.39
85.71 62.65 72.39
87.43 61.45 72.17
79.53 54.62 64.76
85.71 62.65 72.39
66.90 38.96 49.24
82.86 46.59 59.64
76.12 40.96 53.26
46.38 12.85 20.12
83.89 60.64 70.39
90.00 50.60 64.78
82.72 63.45 71.81
88.96 58.23 70.39
85.71 62.65 72.39
87.43 61.45 72.17
79.04 53.01 63.46
85.71 62.65 72.39
67.13 38.55 48.98
82.86 46.59 59.64
76.12 40.96 53.26
46.38 12.85 20.12
89.17 93.56 91.31
85.93 85.61 85.77
94.31 87.88 90.98
88.04 92.05 90.00
93.41 91.29 92.34
89.17 93.56 91.31
91.42 92.80 92.10
89.17 93.56 91.31
86.90 82.95 84.88
81.34 64.39 71.88
89.26 91.29 90.26
88.89 84.85 86.82
</table>
<figure confidence="0.921809916666667">
UiO1 r2
UABCoRAL
UMichigan
UWashington
FBK
UiO2
UiO1 r1
UiO2
UGroningen r1
UCM-2
UCM-1
UGroningen r2
</figure>
<bodyText confidence="0.999953295454545">
tation for scopes and negated events, where tokens
are assigned a set of labels that attempts to de-
scribe their behavior within the mechanics of nega-
tion. After unseen sequences are labeled, in-scope
and negated tokens are assigned to their respective
cues using simple post-processing heuristics.
The FBK system consists of three different CRF
classifiers, as well as the UMichigan. A character-
istic of the cue model of the UMichigan system is
that tokens are assigned five labels in order to rep-
resent the different types of negation. Similarly, the
UWashington system has a CRF sequence tagger for
scope and negated event detection, while the cue de-
tector learns regular expression matching rules from
the training set. The UABCoRAL system follows
the same strategy, but instead of CRFs it employs
SVM Light.
The resources utilized by participants in the open
track are diverse. UiO2 reparsed the data with Malt-
Parser in order to obtain dependency graphs. For the
rest, the system is the same as in the closed track.
The global results obtained by this system in the
closed track are higher than the results obtained in
the open track, which is mostly due to a higher per-
formance of the scope resolution module. This is the
only machine learning system in the open track and
the highest performing one.
The UGroningen system is based on tools that
produce complex semantic representations. The sys-
tem employs the C&amp;C tools8 for parsing and Boxer9
to produce semantic representations in the form of
Discourse Representation Structures (DRSs). For
cue detection, the DRSs are converted to flat, non-
recursive structures, called Discourse Representa-
tion Graphs (DRGs). These DRGs allow for cue de-
tection by means of labelled tuples. Scope detection
is done by gathering the tokens that occur within the
scope of the negated DRSs. For negated event detec-
tion, a basic algorithm takes the detected scope and
returns the negated event based on information from
the syntax tree within the scope.
UCM-1 and UCM-2 are rule-based systems that
rely heavily on information from the syntax tree.
The UCM-1 system was initially designed for pro-
</bodyText>
<footnote confidence="0.9916675">
8http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/Documentation
9http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/boxer
</footnote>
<bodyText confidence="0.999913895833333">
cessing opinionated texts. It applies a dictionary ap-
proach to cue detection, with the detection of affixal
cues being performed using WordNet. Non-affixal
cue detection is performed by consulting a prede-
fined list of cues. It then uses information from the
syntax tree in order to get a first approximation to
the scope, which is later refined using a set of post-
processing rules. In the case of the UCM-2 system
an algorithm detects negation cues and their scope
by traversing Minipar dependency structures. Fi-
nally, the scope is refined with post-processing rules
that take into account the information provided by
the first algorithm and linguistic clause boundaries.
If we compare tracks, the Global best results ob-
tained in the closed track (57.63 F1) are higher than
the Global best results obtained in the open track
(54.82 F1). If we compare approaches, the best re-
sults in the two tracks are obtained with machine
learning-based systems. The rule-based systems
participating in the open track clearly score lower
(39.56 F1 the best) than the machine learning-based
system (54.82 F1).
Regarding subtasks, systems achieve higher re-
sults in the cue detection task (92.34 F1 the best) and
lower results in the scope resolution (72.40 F1 the
best) and negated event detection (67.02 F1 the best)
tasks. This is not surprising, not only because of
the error propagation effect, but also because the set
of negation cues is closed and comprises mostly sin-
gle tokens, whereas scope sequences are longer. The
best results in cue detection are obtained by the FBK
system that uses CRFs and applies a special proce-
dure to detect the negation cues that are subtokens.
The best scores for scope resolution (72.40, 72.39
F1) are obtained by two machine learning compo-
nents. UWashington uses CRFs with features de-
rived from the syntax tree. UiO2 uses CRFs mod-
els with syntactic and lexical features for scopes, to-
gether with a set of labels aimed at capturing the
behavior of certain tokens within the mechanics of
negation. The best scores for negated events (67.02
F1) are obtained by the UiO1 system that first clas-
sifies negations as factual or non-factual, and then
applies an SVM ranker over candidate events.
Finally, we would like to draw the attention to the
different scores obtained depending on the evalua-
tion measure used. When scope resolution is evalu-
ated with the Scope (NCM, CM) measure, results
</bodyText>
<page confidence="0.987301">
272
</page>
<bodyText confidence="0.999715666666667">
are much lower than when using the Scope To-
kens measure, which does not reflect the ability of
systems to deal with sequences. Another observa-
tion is related to the difference in precision scores
between the two versions of the evaluation mea-
sures. Whereas for Cues and Negated the differ-
ences are not so big because most cues and negated
events span over a single token, for Scopes they are.
The best Scope NCM precision score is 90.00 %,
whereas the best Scope NCM B precision score is
59.54 %. This shows that the scores can change
considerably depending on how partial matches are
counted (as FP and FN, or only as FN). As a final
remark it is worth noting that the ranking of systems
does not change when using the B measures.
</bodyText>
<subsectionHeader confidence="0.929612">
5.2 Task 2
</subsectionHeader>
<bodyText confidence="0.99997875">
UConcordia submitted two runs in the open track.
Both of them follow the same three component ap-
proach. First, negation cues are detected. Second,
the scope of negation is extracted based on depen-
dency relations and heuristics defined by Kilicoglu
and Bergler (2011). Third, the focus of negation
is determined within the elements belonging to the
scope following three heuristics.
</bodyText>
<sectionHeader confidence="0.999537" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999819918918919">
In this paper we presented the description of the first
*SEM Shared Task on Resolving the Scope and Fo-
cus of Negation, which consisted of two different
tasks related to different aspects of negation: Task 1
on resolving the scope of negation, and Task 2 on
detecting the focus of negation. Task 1 was di-
vided into three subtasks: identifying negation cues,
resolving their scope, and identifying the negated
event. Two new datasets have been produced for this
Shared Task: the CD-SCO corpus of Conan Doyle
stories annotated with scopes, and the PB-FOC cor-
pus, which provides focus annotation on top of Prop-
Bank. New evaluation software was also developed
for this task. The datasets and the evaluation soft-
ware will be available on the web site of the Shared
Task. As far as we know, this is the first task that fo-
cuses on resolving the focus and scope of negation.
A total of 14 runs were submitted, 12 for scope
detection and 2 for focus detection. Of these, four
runs are from systems that take a rule-based ap-
proach, two runs from hybrid systems, and the rest
from systems that take a machine learning approach
using SVMs or CRFs. Most participants designed a
three component architecture.
For a future edition of the shared task we would
like to unify the annotation schemes of the two cor-
pora, namely the annotation of focus in PB-FOC and
negated events in CD-SCO. The annotation of more
data with both scope and focus would allow us to
study the two aspects jointly. We would also like to
provide better evaluation measures for scope reso-
lution. Currently, scopes are evaluated in terms of
F1, which demands a division of errors into the cat-
egories TP/FP/TN/FN borrowed from the evaluation
of information retrieval systems. These categories
are not completely appropriate to be assigned to se-
quence tasks, such as scope resolution.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99977">
We are very grateful to Vivek Srikumar for pre-
processing the PB-FOC corpus with the Illinois se-
mantic role labeler, and to Stephan Oepen for pre-
processing the CD-SCO corpus. We also thank the
*SEM organisers and the ST participants. Roser
Morante’s research was funded by the University of
Antwerp (GOA project BIOGRAPH).
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871619047619">
Eduardo Blanco and Dan Moldovan. 2011. Semantic
Representation of Negation Using Focus Detection. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 581–589, Portland, Ore-
gon, USA. Association for Computational Linguistics.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the third conference on Applied
natural language processing, ANLC ’92, pages 152–
155, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. J Biomed Inform,
34:301–310.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173–180, Ann Arbor.
</reference>
<page confidence="0.980587">
273
</page>
<reference confidence="0.999906201923077">
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Lin-
guistics conference, NAACL 2000, pages 132–139,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology. The Stanford Natural
Language Processing Group.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1–12, Uppsala, Sweden.
Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 363–370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press.
Halil Kilicoglu and Sabine Bergler. 2011. Effective bio-
event extraction using trigger words and syntactic de-
pendencies. Computational Intelligence, 27(4):583–
609.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the 13th Conference on Natu-
ral Language Learning, pages 21–29, Boulder, CO.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation cues and
their scope in Conan Doyle stories. In Proceedings
of LREC 2012, Istambul.
Roser Morante and Caroline Sporleder. 2012. Special is-
sue on modality and negation: An introduction. Com-
putational Linguistics.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope. guidelines v1.0. Technical Report Series CTR-
003, CLiPS, University of Antwerp, Antwerp, April.
Xuan-Hieu Phan. 2006. Crfchunker: Crf english phrase
chunker.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287, June.
Dan Shen and Mirella Lapata. 2007. Using Semantic
Roles to Improve Question Answering. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EM NLP-CoNLL),
pages 12–21.
Ekaterina Shutova. 2010. Models of Metaphor in NLP.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 688–697,
Uppsala, Sweden. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using Predicate-Argument Struc-
tures for Information Extraction. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 8–15, Sapporo, Japan. Asso-
ciation for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning, page 159177, Manchester.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 467–474, Vancouver.
Erik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of syntax. Computational Lin-
guistics.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60–68,
Uppsala, Sweden. University of Antwerp.
Dekai Wu and Pascale Fung. 2009. Semantic Roles
for SMT: A Hybrid Two-Pass Model. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Companion
Volume: Short Papers, pages 13–16, Boulder, Col-
orado. Association for Computational Linguistics.
</reference>
<page confidence="0.998302">
274
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.825336">
<title confidence="0.991826">SEM 2012 Shared Task: Resolving the Scope and Focus of Negation</title>
<author confidence="0.998268">Roser Morante Eduardo Blanco</author>
<affiliation confidence="0.960943">CLiPS - University of Antwerp Lymba Corporation</affiliation>
<address confidence="0.974755">Prinsstraat 13, B-2000 Antwerp, Belgium Richardson, TX 75080 USA</address>
<email confidence="0.911265">Roser.Morante@ua.ac.beeduardo@lymba.com</email>
<abstract confidence="0.9945831">The Joint Conference on Lexical and Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eduardo Blanco</author>
<author>Dan Moldovan</author>
</authors>
<title>Semantic Representation of Negation Using Focus Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>581--589</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="16385" citStr="Blanco and Moldovan, 2011" startWordPosition="2794" endWordPosition="2797">ENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 3.2 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus provides focus annotation on top of PropBank. It targets exclusively verbal negations marked with MNEG in PropBank and selects as focus the semantic role containing the most likely focus. The motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For e</context>
</contexts>
<marker>Blanco, Moldovan, 2011</marker>
<rawString>Eduardo Blanco and Dan Moldovan. 2011. Semantic Representation of Negation Using Focus Detection. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 581–589, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the third conference on Applied natural language processing, ANLC ’92,</booktitle>
<pages>152--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17155" citStr="Brill, 1992" startWordPosition="2919" endWordPosition="2920">ntic role containing the most likely focus. The motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5November 2009 release available from Brown University. 6http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. Train Devel Test 1 role 2,210 515 672 2 roles 89 15 38 3 roles </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the third conference on Applied natural language processing, ANLC ’92, pages 152– 155, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>Will Bridewell</author>
<author>Paul Hanbury</author>
<author>Gregory F Cooper</author>
<author>Bruce G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>J Biomed Inform,</journal>
<pages>34--301</pages>
<contexts>
<context position="1775" citStr="Chapman et al., 2001" startWordPosition="272" endWordPosition="275">tion, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScop</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. J Biomed Inform, 34:301–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor.</location>
<contexts>
<context position="15998" citStr="Charniak and Johnson (2005)" startWordPosition="2736" endWordPosition="2739">nte and Daelemans (2012), including inter-annotator agreement. The corpus was preprocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4http://moin.delph-in.net/ 268 Apart from the gold annotations, the corpus was provided to participants with additional annotations: • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 3.2 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus provides focus annotation on top of PropBank. It targets exclusively verbal negations marked with MNEG in PropBank and selects as focus the semantic role containing the most likely focus. The motivat</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173–180, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17353" citStr="Charniak, 2000" startWordPosition="2951" endWordPosition="2953">ons 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5November 2009 release available from Brown University. 6http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. Train Devel Test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 All 2,302 530 712 Semantic roles focus belongs to A1 980 222 309 AM-NEG 592 138 172 AM-TMP 161 35 46 AM-MNR 127 27 38 A2 112 28 36 A0 94 23 31 None 88 19 35 AM-ADV 78 23 26 C-A1 46 6 16 AM-PNC</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL 2000, pages 132–139, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology. The Stanford Natural Language Processing Group.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology. The Stanford Natural Language Processing Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>1--12</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 1–12, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17249" citStr="Finkel et al., 2005" startWordPosition="2932" endWordPosition="2935">tation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5November 2009 release available from Brown University. 6http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. Train Devel Test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 All 2,302 530 712 Semantic roles focus belongs to A1 980 222 309 AM-NEG 592 138 172 AM-T</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 363–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Huddleston</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1490" citStr="Huddleston and Pullum, 2002" startWordPosition="226" endWordPosition="229">ications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the Co</context>
<context position="10753" citStr="Huddleston and Pullum, 2002" startWordPosition="1800" endWordPosition="1803">different position depending on the version. Even though we believe that relaxing scope evaluation by ignoring punctuation marks and relaxing the strict cue match requirement is a positive feature of our evaluation, we need to explore further in order to define a scope evaluation measure that captures the impact of partial matches in the scores. 2.2 Task 2: Focus Detection This task tackles focus of negation detection. Both scope and focus are tightly connected. Scope is the part of the meaning that is negated and focus is that part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Focus can also be defined as the element of the scope that is intended to be interpreted as false to make the overall negative true. Detecting focus of negation is useful for retrieving the numerous words that contribute to implicit positive meanings within a negation. Consider the statement The government didn’t release the UFO files {until 2008}. The focus is until 2008, yielding the interpretation The government released the UFO files, but not until 1998. Once the focus is resolved, the verb release, its AGENT The government and its THEME the UFO files are positive; only the TEMPORAL info</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>Rodney D. Huddleston and Geoffrey K. Pullum. 2002. The Cambridge Grammar of the English Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Effective bioevent extraction using trigger words and syntactic dependencies.</title>
<date>2011</date>
<journal>Computational Intelligence,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>609</pages>
<contexts>
<context position="32042" citStr="Kilicoglu and Bergler (2011)" startWordPosition="5487" endWordPosition="5490">s they are. The best Scope NCM precision score is 90.00 %, whereas the best Scope NCM B precision score is 59.54 %. This shows that the scores can change considerably depending on how partial matches are counted (as FP and FN, or only as FN). As a final remark it is worth noting that the ranking of systems does not change when using the B measures. 5.2 Task 2 UConcordia submitted two runs in the open track. Both of them follow the same three component approach. First, negation cues are detected. Second, the scope of negation is extracted based on dependency relations and heuristics defined by Kilicoglu and Bergler (2011). Third, the focus of negation is determined within the elements belonging to the scope following three heuristics. 6 Conclusions In this paper we presented the description of the first *SEM Shared Task on Resolving the Scope and Focus of Negation, which consisted of two different tasks related to different aspects of negation: Task 1 on resolving the scope of negation, and Task 2 on detecting the focus of negation. Task 1 was divided into three subtasks: identifying negation cues, resolving their scope, and identifying the negated event. Two new datasets have been produced for this Shared Tas</context>
</contexts>
<marker>Kilicoglu, Bergler, 2011</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2011. Effective bioevent extraction using trigger words and syntactic dependencies. Computational Intelligence, 27(4):583– 609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>A metalearning approach to processing the scope of negation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Natural Language Learning,</booktitle>
<pages>21--29</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="2405" citStr="Morante and Daelemans, 2009" startWordPosition="372" endWordPosition="375">ince clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC corpus). Focus annotation is restricted to verbal negations annotated with MNEG in PropBank, and all the words belonging to a semantic role are selected as focus</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. A metalearning approach to processing the scope of negation. In Proceedings of the 13th Conference on Natural Language Learning, pages 21–29, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>ConanDoyle-neg: Annotation of negation cues and their scope in Conan Doyle stories.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC 2012, Istambul.</booktitle>
<contexts>
<context position="15395" citStr="Morante and Daelemans (2012)" startWordPosition="2643" endWordPosition="2646"> concerns the scope model itself: in CD-SCO, the cue is not considered to be part of the scope. Furthermore, scopes can be discontinuous and all arguments of the negated event are considered to be part of the scope, including the subject, which is kept out of the scope in BioScope. A final difference is that affixal negation is annotated in CDSCO, as in (6). (6) [He] declares that he heard cries but [is] un[{able} to state from what direction they came]. Statistics for the corpus is presented in Table 1. More information about the annotation guidelines is provided by Morante et al. (2011) and Morante and Daelemans (2012), including inter-annotator agreement. The corpus was preprocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4http://moin.delph-in.net/ 268 Apart from the gold annotations, the corpus was provided to participants with additional annotations: • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (20</context>
</contexts>
<marker>Morante, Daelemans, 2012</marker>
<rawString>Roser Morante and Walter Daelemans. 2012. ConanDoyle-neg: Annotation of negation cues and their scope in Conan Doyle stories. In Proceedings of LREC 2012, Istambul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Caroline Sporleder</author>
</authors>
<title>Special issue on modality and negation: An introduction. Computational Linguistics.</title>
<date>2012</date>
<contexts>
<context position="1108" citStr="Morante and Sporleder, 2012" startWordPosition="162" endWordPosition="165">of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processi</context>
</contexts>
<marker>Morante, Sporleder, 2012</marker>
<rawString>Roser Morante and Caroline Sporleder. 2012. Special issue on modality and negation: An introduction. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Sarah Schrauwen</author>
<author>Walter Daelemans</author>
</authors>
<title>Annotation of negation cues and their scope. guidelines v1.0.</title>
<date>2011</date>
<tech>Technical Report Series CTR003, CLiPS,</tech>
<institution>University of Antwerp,</institution>
<location>Antwerp,</location>
<contexts>
<context position="15362" citStr="Morante et al. (2011)" startWordPosition="2638" endWordPosition="2641">istics. portant difference concerns the scope model itself: in CD-SCO, the cue is not considered to be part of the scope. Furthermore, scopes can be discontinuous and all arguments of the negated event are considered to be part of the scope, including the subject, which is kept out of the scope in BioScope. A final difference is that affixal negation is annotated in CDSCO, as in (6). (6) [He] declares that he heard cries but [is] un[{able} to state from what direction they came]. Statistics for the corpus is presented in Table 1. More information about the annotation guidelines is provided by Morante et al. (2011) and Morante and Daelemans (2012), including inter-annotator agreement. The corpus was preprocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4http://moin.delph-in.net/ 268 Apart from the gold annotations, the corpus was provided to participants with additional annotations: • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing </context>
</contexts>
<marker>Morante, Schrauwen, Daelemans, 2011</marker>
<rawString>Roser Morante, Sarah Schrauwen, and Walter Daelemans. 2011. Annotation of negation cues and their scope. guidelines v1.0. Technical Report Series CTR003, CLiPS, University of Antwerp, Antwerp, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
</authors>
<title>Crfchunker: Crf english phrase chunker.</title>
<date>2006</date>
<contexts>
<context position="17292" citStr="Phan (2006)" startWordPosition="2942" endWordPosition="2943">forementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5November 2009 release available from Brown University. 6http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. Train Devel Test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 All 2,302 530 712 Semantic roles focus belongs to A1 980 222 309 AM-NEG 592 138 172 AM-TMP 161 35 46 AM-MNR 127 27 38 A2 112 28 36 </context>
</contexts>
<marker>Phan, 2006</marker>
<rawString>Xuan-Hieu Phan. 2006. Crfchunker: Crf english phrase chunker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="18253" citStr="Punyakanok et al., 2008" startWordPosition="3114" endWordPosition="3117">ed the 3,993 negations marked with MNEG in the whole PropBank. Train Devel Test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 All 2,302 530 712 Semantic roles focus belongs to A1 980 222 309 AM-NEG 592 138 172 AM-TMP 161 35 46 AM-MNR 127 27 38 A2 112 28 36 A0 94 23 31 None 88 19 35 AM-ADV 78 23 26 C-A1 46 6 16 AM-PNC 33 8 12 AM-LOC 25 4 10 A4 11 2 5 R-A1 10 2 2 Other 40 8 16 Table 2: Basic numeric analysis for PB-FOC. The first 4 rows indicate the number of unique roles each negation belongs to, the rest indicate the counts for each role. • Semantic roles using the labeler described by (Punyakanok et al., 2008); and • Verbal negation, indicates with ‘N’ if that token correspond to a verbal negation for which focus must be predicted. Figure 2 provides a sample of PB-FOC. Knowing that the original focus annotations were done on top of PropBank and that focus corresponds to a single role, semantic role information is key to predict the focus. In Table 2, we show some basic numeric analysis regarding focus annotation and the automatically obtained semantic role labels. Most instances of focus belong to a single role in the three splits and the most common role focus belongs to is A1, followed by AM-NEG,</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using Semantic Roles to Improve Question Answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EM NLP-CoNLL),</booktitle>
<pages>12--21</pages>
<contexts>
<context position="936" citStr="Shen and Lapata, 2007" startWordPosition="136" endWordPosition="140">cs (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and com</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using Semantic Roles to Improve Question Answering. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EM NLP-CoNLL), pages 12–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Models of Metaphor in NLP.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>688--697</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<contexts>
<context position="1053" citStr="Shutova, 2010" startWordPosition="155" endWordPosition="156">dicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Models of Metaphor in NLP. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 688–697, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using Predicate-Argument Structures for Information Extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan.</location>
<contexts>
<context position="912" citStr="Surdeanu et al., 2003" startWordPosition="132" endWordPosition="135">d Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using Predicate-Argument Structures for Information Extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 8–15, Sapporo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The conll2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>159177</pages>
<location>Manchester.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, page 159177, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>467--474</pages>
<location>Vancouver.</location>
<contexts>
<context position="15798" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2701" endWordPosition="2704">s] un[{able} to state from what direction they came]. Statistics for the corpus is presented in Table 1. More information about the annotation guidelines is provided by Morante et al. (2011) and Morante and Daelemans (2012), including inter-annotator agreement. The corpus was preprocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4http://moin.delph-in.net/ 268 Apart from the gold annotations, the corpus was provided to participants with additional annotations: • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 3.2 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 467–474, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Lilja Øvrelid</author>
<author>Jonathon Read</author>
<author>Stephan Oepen</author>
</authors>
<title>Speculation and negation: Rules, rankers, and the role of syntax.</title>
<date>2012</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="2428" citStr="Velldal et al., 2012" startWordPosition="376" endWordPosition="379">charge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC corpus). Focus annotation is restricted to verbal negations annotated with MNEG in PropBank, and all the words belonging to a semantic role are selected as focus. An annotated example </context>
<context position="22181" citStr="Velldal et al., 2012" startWordPosition="3850" endWordPosition="3853">he UiO1 system, which incorporates a module for factuality detection. Most systems apply machine learning algorithms, either Conditional Random Fields (CRFs) or Support Vector Machines (SVMs), while less systems implement a rule-based approach. Syntax information is widely employed, either in the form of rules or incorporated in the learning model. Multi-word and affixal negation cues receive a special treatment in most cases, and scopes are generally postprocessed. The systems that participate in the closed track are machine learning based. The UiO1 system is an adaptation of another system (Velldal et al., 2012), which combines SVM cue classification with SVMbased ranking of syntactic constituents for scope resolution. The approach is extended to identify negated events by first classifying negations as factual or non-factual, and then applying an SVM ranker over candidate events. The original treatment of factuality in this system results in the highest score for both the negated event subtask and the global task. The UiO2 system combines SVM cue classification with CRF-based sequence labeling. An original aspect of the UiO2 approach is the model represen270 271 43.83 26.81 27.23 34.04 35.74 40.00 4</context>
</contexts>
<marker>Velldal, Øvrelid, Read, Oepen, 2012</marker>
<rawString>Erik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan Oepen. 2012. Speculation and negation: Rules, rankers, and the role of syntax. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gyorgy Szarvas</author>
<author>Richard Farkas</author>
<author>Gyorgy Mora</author>
<author>Janos Csirik</author>
</authors>
<title>The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>11--9</pages>
<contexts>
<context position="1968" citStr="Vincze et al., 2008" startWordPosition="302" endWordPosition="305">rt of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and f</context>
</contexts>
<marker>Vincze, Szarvas, Farkas, Mora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gyorgy Mora, and Janos Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9(Suppl 11):S9+.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Alexandra Balahur</author>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
<author>Andr´es Montoyo</author>
</authors>
<title>A survey on the role of negation in sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,</booktitle>
<pages>60--68</pages>
<institution>Uppsala, Sweden. University of Antwerp.</institution>
<contexts>
<context position="2244" citStr="Wiegand et al., 2010" startWordPosition="347" endWordPosition="350">negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC </context>
</contexts>
<marker>Wiegand, Balahur, Roth, Klakow, Montoyo, 2010</marker>
<rawString>Michael Wiegand, Alexandra Balahur, Benjamin Roth, Dietrich Klakow, and Andr´es Montoyo. 2010. A survey on the role of negation in sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60–68, Uppsala, Sweden. University of Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic Roles for SMT: A Hybrid Two-Pass Model.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado.</location>
<contexts>
<context position="889" citStr="Wu and Fung, 2009" startWordPosition="128" endWordPosition="131">rence on Lexical and Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 13–16, Boulder, Colorado. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>