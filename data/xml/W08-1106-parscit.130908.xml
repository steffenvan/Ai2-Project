<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002005">
<title confidence="0.9967835">
Extractive vs. NLG-based Abstractive Summarization of Evaluative
Text: The Effect of Corpus Controversiality
</title>
<author confidence="0.992022">
Giuseppe Carenini and Jackie Chi Kit Cheung1
</author>
<affiliation confidence="0.9992605">
Department of Computer Science
University of British Columbia
</affiliation>
<address confidence="0.926068">
Vancouver, B.C. V6T 1Z4, Canada
</address>
<email confidence="0.99889">
{carenini,cckitpw}@cs.ubc.ca
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998439375">
Extractive summarization is the strategy of
concatenating extracts taken from a corpus
into a summary, while abstractive summariza-
tion involves paraphrasing the corpus using
novel sentences. We define a novel measure
of corpus controversiality of opinions con-
tained in evaluative text, and report the results
of a user study comparing extractive and
NLG-based abstractive summarization at dif-
ferent levels of controversiality. While the ab-
stractive summarizer performs better overall,
the results suggest that the margin by which
abstraction outperforms extraction is greater
when controversiality is high, providing a
context in which the need for generation-
based methods is especially great.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989571428571">
There are two main approaches to the task of sum-
marization—extraction and abstraction (Hahn and
Mani, 2000). Extraction involves concatenating ex-
tracts taken from the corpus into a summary,
whereas abstraction involves generating novel sen-
tences from information extracted from the corpus.
It has been observed that in the context of multi-
document summarization of news articles, extrac-
tion may be inappropriate because it may produce
summaries which are overly verbose or biased to-
wards some sources (Barzilay et al., 1999). How-
ever, there has been little work identifying specific
factors which might affect the performance of each
strategy in summarizing evaluative documents
</bodyText>
<footnote confidence="0.654382">
1Authors are listed in alphabetical order.
</footnote>
<bodyText confidence="0.99991596">
containing opinions and preferences, such as cus-
tomer reviews or blogs. This work aims to address
this gap by exploring one dimension along which
the effectiveness of the two paradigms could vary;
namely, the controversiality of the opinions con-
tained in the corpus.
In this paper, we make the following contribu-
tions. Firstly, we define a measure of controver-
siality of opinions in the corpus based on informa-
tion entropy. Secondly, we run a user study to test
the hypothesis that a controversial corpus has
greater need of abstractive methods and conse-
quently of NLG techniques. Intuitively, extracting
sentences from multiple users whose opinions are
diverse and wide-ranging may not reflect the over-
all opinion, whereas it may be adequate content-
wise if opinions are roughly the same across users.
As a secondary contribution, we propose a method
for structuring text when summarizing controver-
sial corpora. This method is used in our study for
generating abstractive summaries.
The results of the user study support our hy-
pothesis that a NLG summarizer outperforms an
extractive summarizer by more when the contro-
versiality is high.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999965833333333">
There has been little work comparing extractive
and abstractive multi-document summarization. A
previous study on summarizing evaluative text
(Carenini et. al, 2006) showed that extraction and
abstraction performed about equally well, though
for different reasons. The study, however, did not
</bodyText>
<page confidence="0.998249">
33
</page>
<bodyText confidence="0.999945445945946">
look at the effect of the controversiality of the cor-
pus on the relative performance of the two strate-
gies.
To the best of our knowledge, the task of mea-
suring the controversiality of opinions in a corpus
has not been studied before. Some well known
measures are related to this task, including vari-
ance, information entropy, and measures of inter-
rater reliability. (e.g. Fleiss&apos; Kappa (Fleiss, 1971),
Krippendorff&apos;s Alpha (Krippendorff, 1980)). How-
ever, these existing measures do not satisfy certain
properties that a sound measure of controversiality
should possess, prompting us to develop our own
based on information entropy.
Summary evaluation is a challenging open re-
search area. Existing methods include soliciting
human judgements, task-based approaches, and au-
tomatic approaches.
Task-based evaluation measures the effective-
ness of a summarizer for its intended purpose. (e.g.
(McKeown et al., 2005)) This approach, however,
is less applicable in this work because we are inter-
ested in evaluating specific properties of the sum-
mary such as the grammaticality and the content,
which may be difficult to evaluate with an overall
task-based approach. Furthermore, the design of
the task may intrinsically favour abstractive or ex-
tractive summarization. As an extreme example,
asking for a list of specific comments from users
would clearly favour extractive summarization.
Another method for summary evaluation is the
Pyramid method (Nenkova and Passonneau, 2004),
which takes into account the fact that human sum-
maries with different content can be equally infor-
mative. Multiple human summaries are taken to be
models, and chunks of meaning known as Summa-
ry Content Units (SCU) are manually identified.
Peer summaries are evaluated based on how many
SCUs they share with the model summaries, and
the number of model summaries in which these
SCUs are found. Although this method has been
tested in DUC 2006 and DUC 2005 (Passonneau et
al., 2006), (Passonneau et al., 2005) in the domain
of news articles, it has not been tested for evalua-
tive text. A pilot study that we conducted on a set
of customer reviews on a product using the Pyra-
mid method revealed several problems specific to
the evaluative domain. For example, summaries
which misrepresented the polarity of the evalua-
tions for a certain feature were not penalized, and
human summaries sometimes produced contradic-
tory statements about the distribution of the opin-
ions. In one case, one model summary claimed that
a feature is positively rated, while another claimed
the opposite, whereas the machine summary indi-
cated that this feature drew mixed reviews. Clear-
ly, only one of these positions should be regarded
as correct. Further work is needed to resolve these
problems.
There are also automatic methods for summary
evaluation, such as ROUGE (Lin, 2004), which
gives a score based on the similarity in the se-
quences of words between a human-written model
summary and the machine summary. While
ROUGE scores have been shown to often correlate
quite well with human judgements (Nenkova et al.,
2007), they do not provide insights into the specif-
ic strengths and weaknesses of the summary.
The method of summarization evaluation used
in this work is to ask users to complete a question-
naire about summaries that they are presented with.
The questionnaire consists of questions asking for
Likert ratings and is adapted from the question-
naire in (Carenini et al., 2006).
</bodyText>
<sectionHeader confidence="0.992763" genericHeader="method">
3 Representative Systems
</sectionHeader>
<bodyText confidence="0.99939156">
In our user study, we compare an abstractive and
an extractive multi-document summarizer that are
both developed specifically for the evaluative do-
main. These summarizers have been found to pro-
duce quantitatively similar results, and both signif-
icantly outperform a baseline summarizer, which is
the MEAD summarization framework with all op-
tions set to the default (Radev et al., 2000).
Both summarizers rely on information extrac-
tion from the corpus. First, sentences with opinions
need to be identified, along with the features of the
entity that are evaluated, the strength, and polarity
(positive or negative) of the evaluation. For in-
stance, in a corpus of customer reviews, the sen-
tence “Excellent picture quality - on par with my
Pioneer, Panasonic, and JVC players.” contains an
opinion on the feature picture quality of a DVD
player, and is a very positive evaluation (+3 on a
scale from -3 to +3). We rely on methods from pre-
vious work for these tasks (Hu and Liu, 2004).
Once these features, called Crude Features (CFs),
are extracted, they are mapped onto a taxonomy of
User Defined Features (UDFs), so named because
they can be defined by the user. This mapping pro-
vides a better conceptual organization of the CFs
</bodyText>
<page confidence="0.998378">
34
</page>
<bodyText confidence="0.99976">
by grouping together semantically similar CFs,
such as jpeg picture and jpeg slide show under the
UDF JPEG. For the purposes of our study, feature
extraction, polarity/strength identification and the
mapping from CFs to UDFs are not done automati-
cally as in (Hu and Liu, 2004) and (Carenini et al,
2005). Instead, “gold standard” annotations by hu-
mans are used in order to focus on the effect of the
summarization strategy.
</bodyText>
<subsectionHeader confidence="0.998938">
3.1 Abstractive Summarizer: SEA
</subsectionHeader>
<bodyText confidence="0.999664088235294">
The abstractive summarizer is the Summarizer of
Evaluative Arguments (SEA), adapted from GEA,
a system for generating evaluative text tailored to
the user&apos;s preferences (Carenini and Moore, 2006).
In SEA, units of content are organized by
UDFs. The importance of each UDF is based on
the number and strength of evaluations of CFs
mapped to this UDF, as well as the importance of
its children UDFs. Content selection consists of re-
peating the following two steps until the desired
number of UDFs have been selected: (i) greedily
selecting the most important UDF (ii) recalculating
the measure of importance scores for the remaining
UDFs.
The content structuring, microplanning, and re-
alization stages of SEA are adapted from GEA.
Each selected UDF is realized in the final summary
by one clause, generated from a template pattern
based on the number and distribution of
polarity/strength evaluations of the UDF. For ex-
ample, the UDF video output with an average po-
larity/strength of near -3 might be realized as “sev-
eral customers found the video output to be terri-
ble.”
While experimenting with the SEA summariz-
er, we noticed that the document structuring of
Customers had mixed opinions about the Apex AD2600.
Although several customers found the video output to be
poor and some customers disliked the user interface, cus-
tomers had mixed opinions about the range of compatible
disc formats. However, users did agree on some things.
Some users found the extra features to be very good even
though customers had mixed opinions about the supplied
universal remote control.
</bodyText>
<figureCaption confidence="0.7925374">
Figure 1: SEA summary of a controversial corpus with
a document structuring problem. Controversial and un-
controversial features are interwoven. See Figure 3 for
an example of a summary structured with our alterna-
tive strategy.
</figureCaption>
<bodyText confidence="0.99997345">
SEA summaries, which is adapted from GEA and
is based on guidelines from argumentation theory
(Carenini and Moore, 2000), sometimes sounded
unnatural. We found that controversially rated
UDF features (roughly balanced positive and nega-
tive evaluations) were treated as contrasts to those
which were uncontroversially rated (either mostly
positive, or mostly negative evaluations). In SEA,
contrast relations between features are realized by
cue phrases signalling contrast such as “however”
and “although”. These cue phrases appear to signal
a contrast that is too strong for the relation between
controversial and uncontroversial features. An ex-
ample of a SEA summary suffering from this prob-
lem can be found in Figure 1.
To solve this problem, we devised an alterna-
tive content structure for controversial corpora, in
which all controversial features appear first, fol-
lowed by all positively and negatively evaluated
features.
</bodyText>
<subsectionHeader confidence="0.999313">
3.2 Extractive Summarizer: MEAD*
</subsectionHeader>
<bodyText confidence="0.999985038461538">
The extractive approach is represented by
MEAD*, which is adapted from the open source
summarization framework MEAD (Radev et al.,
2000).
After information extraction, MEAD* orders
CFs by the number of sentences evaluating that
CF, and selects a sentence from each CF until the
word limit has been reached. The sentence that is
selected for each CF is the one with the highest
sum of polarity/strength evaluations for any fea-
ture, so sentences that mention more CFs tend to
be selected. The selected sentences are then or-
dered according to the UDF hierarchy by a depth-
first traversal through the UDF tree so that more
abstract features tend to precede more specific
ones.
MEAD* does not have a special mechanism to
deal with controversial features. It is not clear how
overall controversiality of a feature can be effec-
tively expressed with extraction, as each sentence
conveys a specific and unique opinion. One could
include two sentences of opposite polarity for each
controversial feature. However, in several cases
that we considered, this produced extremely inco-
herent text that did not seem to convey the gist of
the overall controversiality of the feature.
</bodyText>
<page confidence="0.995301">
35
</page>
<subsectionHeader confidence="0.991433">
3.3 Links to the Corpus
</subsectionHeader>
<bodyText confidence="0.999981466666667">
In common with the previous study on which this
is based, both the SEA and MEAD* summaries
contain “clickable footnotes” which are links back
into an original user review, with a relevant sen-
tence highlighted. These footnotes serve to provide
details for the abstractive SEA summarizer, and
context for the sentences chosen by the extractive
MEAD* summarizer. They also aid the partici-
pants of the user study in checking the contents of
the summary. The sample sentences for SEA are
selected by a method similar to the MEAD* sen-
tence selection algorithm. One of the questions in
the questionnaire provided to users targets the ef-
fectiveness of the footnotes as an aid to the sum-
mary.
</bodyText>
<sectionHeader confidence="0.987102" genericHeader="method">
4 Measuring Controversiality
</sectionHeader>
<bodyText confidence="0.99989117948718">
The opinion sentences in the corpus are annotated
with the CF that they evaluate as well as the
strength, from 1 to 3, and polarity, positive or neg-
ative, of the evaluation. It is natural then, to base a
measure of controversiality on these annotations.
To measure the controversiality of a corpus, we
first measure the controversiality of each of the
features in the corpus. We list two properties that a
measure of feature controversiality should satisfy.
Strength-sensitivity: The measure should be
sensitive to the strength of the evaluations. e.g. Po-
larity/strength (P/S) evaluations of -2 and +2
should be less controversial than -3 and +3
Polarity-sensitivity: The measure should be sen-
sitive the polarity of the evaluations. e.g. P/S eval-
uations of -1 and +1 should be more controversial
than +1 and +3.
The rationale for this property is that positive
and negative evaluations are fundamentally differ-
ent, and this distinction is more important than the
difference in intensity. Thus, though a numerical
scale would suggest that -1 and +1 are as distant as
+1 and +3, a suitable controversiality measure
should not treat them so.
In addition, the overall measure of corpus con-
troversiality should also satisfy the following two
features.
CF-weighting: CFs should be weighted by the
number of evaluations they contain when calculat-
ing the overall value of controversiality for the cor-
pus.
CF-independence: The controversiality of indi-
vidual CFs should not affect each other.
An alternative is to calculate controversiality by
UDFs instead of CFs. However, not all CFs
mapped to the same UDF represent the same con-
cept. For example, the CFs picture clarity and col-
or signal are both mapped to the UDF video out-
put.
</bodyText>
<subsectionHeader confidence="0.990974">
4.1 Existing Measures of Variability
</subsectionHeader>
<bodyText confidence="0.99996355">
Since the problem of measuring the variability of a
distribution has been well studied, we first exam-
ined existing metrics including variance, entropy,
kappa, weighted kappa, Krippendorff’s alpha, and
information entropy. Each of these, however, is
problematic in their canonical form, leading us to
devise a new metric based on information entropy
which satisfies the above properties. Existing met-
rics will now be examined in turn.
Variance: Variance does not satisfy polarity-
sensitivity, as the statistic only takes into account
the difference of each data point to the mean, and
the sign of the data point plays no role.
Information Entropy: The canonical form of in-
formation entropy does not satisfy strength or po-
larity sensitivity, because the measure considers
the discrete values of the distribution to be an un-
ordered set.
Measures of Inter-rater Reliability: Many mea-
sures exist to assess inter-rater agreement or dis-
agreement, which is the task of measuring how
similarly two or more judges rate one or more sub-
jects beyond chance (dis)agreement. Various ver-
sions of Kappa and Krippendorff&apos;s Alpha (Krip-
pendorff, 1980), which have shown to be equiva-
lent in their most generalized forms (Passonneau,
1997), can be modified to satisfy all the properties
listed above. However, there are important differ-
ences between the tasks of measuring controver-
siality and measuring inter-rater reliability. Kappa
and Krippendorff&apos;s Alpha correct for chance agree-
ment between raters, which is appropriate in the
context of inter-rater reliability calculations, be-
cause judges are asked to give their opinions on
items that are given to them. In contrast, expres-
sions of opinion are volunteered by users, and
users self-select the features they comment on.
Thus, it is reasonable to assume that they never
randomly select an evaluation for a feature, and
chance agreement does not exist.
</bodyText>
<page confidence="0.993857">
36
</page>
<sectionHeader confidence="0.580806" genericHeader="method">
4.2 Entropy-based Controversiality
</sectionHeader>
<bodyText confidence="0.999951714285714">
We define here our novel measure of controver-
siality, which is based on information entropy be-
cause it can be more easily adapted to measure
controversiality. As has been stated, entropy in its
original form over the evaluations of a CF is not
sensitive to strength or polarity. To correct this, we
first aggregate the positive and negative evalua-
tions for each CF separately, and then calculate the
entropy based on the resultant Bernoulli distribu-
tion.
Let ps(cfj) be the set of polarity/strength evalua-
tions for cfj. Let the importance of a feature,
imp(cfj), be the sum of the absolute values of the
polarity/strength evaluations for cfj.
</bodyText>
<equation confidence="0.993886285714286">
impcf j= ∑ ∣psk∣
ps k∈ pscfj
Define:
imp _ poscf j= ∑ ∣psk∣
psk ∈ pscfj∧ psk0
imp_neg cf j= ∑ ∣psk∣
psk∈ ps cf j∧psk0
</equation>
<bodyText confidence="0.99989075">
Now, calculate the entropy of the Bernoulli dis-
tribution corresponding to the importance of the
two polarities to satisfy polarity-sensitivity. That
is, Bernoulli with parameter
</bodyText>
<equation confidence="0.7259105">
j=imp_ poscf j/impcf j
H j=−j log2j−1−jlog21−j
</equation>
<bodyText confidence="0.99984675">
Next, we scale this score by the importance of
the evaluations divided by the maximum possible
importance for this number of evaluations to satis-
fy strength-sensitivity. Since our scale is from -3 to
</bodyText>
<figureCaption confidence="0.990473333333333">
Figure 2: Sample feature controversiality scores for
three different distributions of polarity/strength evalua-
tions.
</figureCaption>
<bodyText confidence="0.980378382352941">
+3, the maximum possible importance for a feature
is three times the number of evaluations.
max_impcf j=3×∣ps cf j∣
Then the controversiality of a feature is:
contro cf imp cf j ×H j
j = max _impcf j
The case corresponding to the highest possible
feature controversiality, then, would be the bi-
modal case with equal numbers of evaluations on
the extreme positive and negative bins (Figure 2).
Note, however, that controversiality is not simply
bimodality. A unimodal normal-like distribution of
evaluations centred on zero, for example, should
intuitively be somewhat controversial, because
there are equal numbers of positive and negative
evaluations. Our entropy-based feature controver-
siality measure is able to take this into account.
To calculate the controversiality of the corpus,
a weighted average is taken over the CF controver-
siality scores, with the weight being equal to one
less than the number of evaluations for that CF.
We subtract one to eliminate any CF where only
one evaluation is made, as that CF has an entropy
score of one by default before scaling by impor-
tance. This procedure satisfies properties CF-
weighting and CF-independence.
wcf j=∣ps cf j∣−1
controcorpus=∑ wcf j×controcf j
∑ wcf j
Although the annotations in this corpus range
from -3 to +3, it would be easy to rescale opinion
annotations of different corpora to apply this met-
ric. Note that empirically, this measure correlates
highly with Kappa and Krippendorff&apos;s Alpha.
</bodyText>
<sectionHeader confidence="0.98073" genericHeader="method">
5 User Study
</sectionHeader>
<bodyText confidence="0.999926090909091">
Our main hypothesis that extractive summarization
is outperformed even more in the case of contro-
versial corpora was tested by a user study, which
compared the results of MEAD* and the modified
SEA. First, ten subsets of 30 user reviews were se-
lected from the corpus of 101 reviews of the Apex
AD2600 DVD player from amazon.com by
stochastic local search. Five of these subsets are
controversial, with controversiality scores between
0.83 and 0.88, and five of these are uncontrover-
sial, with controversiality scores of 0. A set of thir-
</bodyText>
<page confidence="0.998883">
37
</page>
<note confidence="0.289369">
SEA
</note>
<bodyText confidence="0.998440142857143">
Customers had mixed opinions about the Apex AD2600 1,2
possibly because users were divided on the range of compatible
disc formats 3,4 and there was disagreement among the users
about the video output 5,6. However, users did agree on some
things. Some purchasers found the extra features 7 to be very
good and some customers really liked the surround sound sup-
port 8 and thought the user interface 9 was poor.
</bodyText>
<sectionHeader confidence="0.354186" genericHeader="method">
MEAD*
</sectionHeader>
<bodyText confidence="0.999140444444444">
When we tried to hook up the first one , it was broken - the
motor would not eject discs or close the door . 1 The build
quality feels solid , it does n&apos;t shake or whine while playing
discs , and the picture and sound is top notch ( both dts and
dd5.1 sound good ) . 2 The progressive scan option can be
turned off easily by a button on the remote control which is
one of the simplest and easiest remote controls i have ever
seen or used . 3 It plays original dvds and cds and plays
mp3s and jpegs . 4
</bodyText>
<figureCaption confidence="0.9967545">
Figure 3: Sample SEA and MEAD* summaries for a controversial corpus. The numbers within the summaries are
footnotes linking the summary to an original user review from the corpus.
</figureCaption>
<bodyText confidence="0.9993055">
ty user reviews per subcorpus was needed to create
a summary of sufficient length, which in our case
was about 80 words in length.
Twenty university students were recruited and
presented with two summaries of the same subcor-
pus, one generated from SEA and one from
MEAD*. We generated ten subcorpora in total, so
each subcorpus was assigned to two participants.
One of these participants was shown the SEA sum-
mary first, and the other was shown the MEAD*
summary first, to eliminate the order of presenta-
tion as a source of variation.
The participants were asked to take on the role
of an employee of Apex, and told that they would
have to write a summary for the quality assurance
department of the company about the product in
question. The purpose of this was to prime them to
look for information that should be included in a
summary of this corpus. They were given thirty
minutes to read the reviews, and take notes.
They were then presented with a questionnaire
on the summaries, consisting of ten Likert rating
questions. Five of these questions targeted the lin-
guistic quality of the summary, based on linguistic
well-formedness questions used at DUC 2005, one
targeted the “clickable footnotes” linking to sample
sentences in the summary (see section 3.3), and
three evaluated the contents of the summary. The
three questions targeted Recall, Precision, and the
general Accuracy of the summary contents respec-
tively. The tenth question asked for a general over-
all quality judgement of the summary.
After familiarizing themselves with the ques-
tionnaire, the participants were presented with the
two summaries in sequence, and asked to fill out
the questionnaire while reading the summary. They
were allowed to return to the original set of re-
views during this time. Lastly, they were given an
additional questionnaire which asked them to com-
pare the two summaries that they were shown.
Questions in the questionnaire not found in
(Carenini et al., 2006) are attached in Appendix A.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999861">
6.1 Quantitative Results
</subsectionHeader>
<bodyText confidence="0.999935233333333">
We convert the Likert responses from a scale from
Strongly Disagree to Strong Agree to a scale from
1 to 5, with 1 corresponding to Strongly Disagree,
and 5 to Strongly Agree. We group the ten ques-
tions into four categories: linguistic (questions 1 to
5), content (questions 6 to 8), footnote (question
9), and overall (question 10). See Table 1 for a
breakdown of the responses for each question at
each controversiality level.
For our analysis, we adopt a two-step approach
that has been applied in Computational Linguistics
(Di Eugenio et al., 2002) as well as in HCI (Hinck-
ley et al., 1997).
First, we perform a two-way Analysis of Vari-
ance (ANOVA) test using the average response of
the questions in each category. The two factors are
controversiality of the corpus (high or low) as in-
dependent samples, and the summarizer (SEA or
MEAD*) as repeated measures. We repeat this
procedure for the average of the ten questions,
termed Macro below. The p-values of these tests
are summarized in Table 2.
The results of the ANOVA tests indicate that
SEA significantly outperforms MEAD* in terms of
linguistic and overall quality, as well as for all the
questions combined. It does not significantly out-
perform MEAD* by content, or in the amount that
the included sample sentences linked to by the
footnotes aid the summary. No significant differ-
ences are found in the performance of the summa-
</bodyText>
<page confidence="0.997954">
38
</page>
<table confidence="0.999609176470588">
Controversial Uncontroversial
SEA MEAD* (SEA – MEAD*;SEA MEAD* (SEA – MEAD*;
Question Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev.
Grammaticality 4.5 0.53 3.4 1.26 1.1 0.99 4.2 0.92 2.78 1.3 1.56 1.51
Non-redundancy 4.2 0.92 4 1.07 0.25 1.58 3.7 0.95 3.8 1.14 -0.1 1.45
Referential clarity 4.5 0.53 3.44 1.33 1 1.22 4.2 1.03 3.5 1.18 0.7 1.34
Focus 4.11 1.27 2.1 0.88 2.22 0.83 3.9 1.1 2.6 1.35 1.3 1.57
Structure and Coherence 4.1 0.99 1.9 0.99 2.2 1.14 3.8 1.4 2.3 1.06 1.5 1.9
Linguistic 4.29 0.87 2.91 1.35 1.39 1.34 3.96 1.07 3 1.29 0.98 1.63
Recall 2.8 1.32 1.8 1.23 1 1.33 2.5 1.27 2.5 1.43 0 1.89
Precision 3.9 1.1 2.7 1.64 1.2 1.23 3.5 1.27 3.3 0.95 0.2 1.93
Accuracy 3.4 0.97 3.3 1.57 0.1 1.2 3.1 1.52 3.2 1.03 -0.1 2.28
Content 3.37 1.19 2.6 1.57 0.77 1.3 3.03 1.38 3 1.17 0.03 1.97
Footnote 4 1.05 3.9 0.88 0.1 1.66 3.6 1.07 3.5 1.35 0.1 1.6
Overall 3.8 0.79 2.4 1.17 1.4 1.07 3.2 1.23 2.7 0.82 0.5 1.84
Macro – Footnote 3.92 1.06 2.75 1.41 1.17 1.32 3.57 1.26 2.97 1.2 0.61 1.81
Macro 3.93 1.05 2.87 1.4 1.06 1.39 3.57 1.24 3.02 1.22 0.56 1.79
</table>
<tableCaption confidence="0.9635075">
Table 1: Breakdown of average Likert question responses for each summary at the two levels of controversiali-
ty as well as the difference between SEA and MEAD*.
</tableCaption>
<table confidence="0.999962285714286">
Question Controver- Summarizer Controversiality
Set siality x Summarizer
Linguistic 0.7226 &lt;0.0001 0.2639
Content 0.9215 0.1906 0.2277
Footnote 0.2457 0.7805 1
Overall 0.6301 0.0115 0.2000
Macro 0.7127 0.0003 0.1655
</table>
<tableCaption confidence="0.99813">
Table 2: Two-way ANOVA p-values.
</tableCaption>
<bodyText confidence="0.999902219512195">
rizers over the two levels of controversiality for
any of the question sets .
While the average differences in scores be-
tween the SEA and MEAD* summarizers are
greater in the controversial case for the linguistic,
content, and macro averages as well as the ques-
tion on the overall quality, the p-values for interac-
tion between the two factors in the two-way ANO-
VA test are not significant.
For the second step of the analysis, we use a
one-tailed sign test (Siegel and Castellan, 1988)
over the difference in performance of the summa-
rizers at the two levels of controversiality for the
questions in the questionnaire. We encode + in the
case where the difference between SEA and
MEAD* is greater for a question in the controver-
sial setting, – if the difference is smaller, and we
discard a question if the difference is the same (e.g.
the Footnote question). Since the Overall question
is likely correlated with the responses of the other
questions, we did not include it in the test. After
discarding the Footnote question, the p-value over
the remaining eight questions is 0.0352, which
lends support to our hypothesis that the abstraction
is better by more when the corpus is controversial.
We also analyze the users&apos; summary prefer-
ences at the two levels of controversiality. A strong
preference for SEA is encoded as a 5, while a
strong preference for MEAD* is encoded as a 1,
with 3 being neutral. Using a two-tailed unpaired
two-sample t-test, we do not find a significant dif-
ference in the participants&apos; summary preferences
(p=0.6237). However, participants sometimes pre-
ferred summaries for reasons other than linguistic
or content quality, or may base their judgement
only on one aspect of the summary. For instance,
one participant rated SEA at least as well as
MEAD* in all questions except Footnote, yet pre-
ferred MEAD* to SEA overall precisely because
MEAD* was felt to have made better use of the
footnotes than SEA.
</bodyText>
<subsectionHeader confidence="0.998726">
6.2 Qualitative Results
</subsectionHeader>
<bodyText confidence="0.999535928571429">
The qualitative comments that participants were
asked to provide along with the Likert scores con-
firmed the observations that led us to formulate the
initial hypothesis.
In the controversial subcorpora, participants
generally agreed that the abstractive nature of
SEA&apos;s generated text was an advantage. For exam-
ple, one participant lauded SEA for attempting to
“synthesize the reviews” and said that it “did re-
flect the mixed nature of the reviews, and covered
some common complaints.” The participant, how-
ever, said that SEA “was somewhat misleading in
that it understated the extent to which reviews were
negative. In particular, agreement was reported on
</bodyText>
<page confidence="0.997902">
39
</page>
<bodyText confidence="0.999890142857143">
some features where none existed, and problems
with reliability were not mentioned.”
Participants disagreed on the information cover-
age of the MEAD* summary. One participant said
that MEAD* includes “almost all the information
about the Apex 2600 DVD player”, while another
said that it “does not reflect all information from
the customer reviews.”
In the uncontroversial subcorpora, more users
criticized SEA for its inaccuracy in content selec-
tion. One participant felt that SEA “made general-
izations that were not precise or accurate.” Partici-
pants had specific comments about the features
that SEA mentioned that they did not consider im-
portant. For example, one comment was that
“Compatibility with CDs was not a general prob-
lem, nor were issues with the remote control, or
video output (when it worked).” MEAD* was criti-
cized for being “overly specific”, but users praised
MEAD* for being “not at all redundant”, and said
that it “included information I felt was important.”
</bodyText>
<sectionHeader confidence="0.995789" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989322580645">
We have explored the controversiality of opinions
in a corpus of evaluative text as an aspect which
may determine how well abstractive and extractive
summarization strategies perform. We have pre-
sented a novel measure of controversiality, and re-
ported on the results of a user study which suggest
that abstraction by NLG outperforms extraction by
a larger amount in more controversial corpora. We
have also presented a document structuring strate-
gy for summarization of controversial corpora.
Our work has implications in practical deci-
sions on summarization strategy choice; an extrac-
tive approach, which may be easier to implement
because of its lack of requirement for natural lan-
guage generation, may suffice if the controversiali-
ty of opinions in a corpus is sufficiently low.
A future approach to summarization of evalua-
tive text might combine extraction and abstraction
in order to combine the different strengths that
each bring to the summary. The controversiality of
the corpus might be one factor determining the mix
of abstraction and extraction in the summary. The
footnotes linking to sample sentences in the corpus
in SEA are already one form of this combined ap-
proach. Further work is needed to integrate this
text into the summary itself, possibly in a modified
form.
As a final note to our user study, further studies
should be done with different corpora and summa-
rization systems to increase the external validity of
our results.
</bodyText>
<sectionHeader confidence="0.990279" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996738">
We would like to thank Raymond T. Ng, Gabriel
Murray and Lucas Rizoli for their helpful com-
ments. This work was partially funded by the Nat-
ural Sciences and Engineering Research Council of
Canada.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844">
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context of
multi-document summarization. In Proc. 37th ACL,
pages 550–557.
Giuseppe Carenini and Johanna D. Moore. 2006. Gener-
ating and evaluating evaluative arguments. Artificial
Intelligence, 170(11):925-952.
Giuseppe Carenini, Raymond Ng and Adam Pauls.
2006. Multi-document summarization of evaluative
text. In Proc. 11th EACL 2006, pages 305-312.
Giuseppe Carenini, Raymond T. Ng and Ed Zwart.
2005. Extracting Knowledge from Evaluative Text.
In Proc. 3rd International Conference on Knowledge
Capture, pages 11-18.
Giuseppe Carenini and Johanna D. Moore. 2000. A
strategy for generating evaluative arguments. In First
INLG, pages 47-54, Mitzpe Ramon, Israel.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural language
generation for intelligent tutoring systems. In INL-
G02, The 2nd INLG, pages 120-127.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin.
76:378-382.
U. Hahn and I. Mani. 2000. The challenges of automatic
summarization. IEEE Computer, 33(11):29-36.
Ken Hinckley, Randy Pausch, Dennis Proffitt, James
Patten, and Neal Kassell. 1997. Cooperative bimanu-
al action. In Proc. CHI Conference.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proc. 10th ACM SIGKDD
conference, pages 168-177.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage Publications, Bev-
erly Hills, CA.
Chin-Yew Lin. 2004. ROUGE: A Package for Automat-
ic Evaluation of Summaries. In Proc. of Workshop
on Text Summarization Branches Out, Post-Confer-
ence Workshop of ACL 2004, Barcelona, Spain.
</reference>
<page confidence="0.967371">
40
</page>
<reference confidence="0.991493323529412">
2005. Linguistic quality questions from the 2005 docu-
ment understanding conference.
http://duc.nist.gov/duc2005/quality-questions.txt
Kathleen McKeown, Rebecca Passonneau, David Elson,
Ani Nenkova, and Julia Hirschberg. 2005. Do sum-
maries help? A task-based evaluation of multi-docu-
ment summarization. In Proc. SIGIR 2005.
Ani Nenkova, Rebecca J. Passonneau, and K. McKe-
own. 2007. The pyramid method: incorporating hu-
man content selection variation in summarization
evaluation. ACM Transactions on Speech and Lan-
guage Processing, 4(2).
Ani Nenkova and Rebecca J. Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proc. NAACL/HLT.
Rebecca J. Passonneau, Kathleen McKeown, Sergey
Sigleman, and Adam Goodkind. 2006. Applying the
pyramid method in the 2006 Document Understand-
ing Conference. In Proc. DUC&apos;06.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the pyra-
mid method in DUC 2005. In Proc. DUC&apos;05.
Rebecca J. Passonneau. 1997. Applying Reliability Met-
rics to Co-Reference Annotation. Department of
Computer Science, Columbia University, TR
CUCS-017-97.
Dragomir Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-based summarization
of multiple documents: sentence extraction, utility-
based evaluation, and user studies. In Proc.
ANLP/NAACL Workshop on Automatic Summariza-
tion.
S. Siegel and N. J. Castellan, Jr. 1988. Nonparametric
statistics for the behaviorial sciences. McGraw Hill.
</reference>
<sectionHeader confidence="0.922265" genericHeader="method">
Appendix A. Additional Questions
</sectionHeader>
<bodyText confidence="0.989747">
Footnotes: a) Did you use the footnotes when re-
viewing the summary?
b) Answer this question only if you answered
“Yes” to the previous question. The clickable foot-
notes were a helpful addition to the summary.
</bodyText>
<sectionHeader confidence="0.64228" genericHeader="method">
Summary Comparison Questions:
</sectionHeader>
<listItem confidence="0.736886888888889">
1) List any Pros and Cons you can think of for
each of the summaries. Point form is okay.
2) Overall, which summary did you prefer?
3) Why did you prefer this summary? (If the
reason overlaps with some points from question 1,
put a star next to those points in the chart.)
4) Do you have any other comments about the
reviews or summaries, the tasks, or the experiment
in general? If so, please write them below.
</listItem>
<page confidence="0.999058">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920046">
<title confidence="0.989231">Extractive vs. NLG-based Abstractive Summarization of Text: The Effect of Corpus Controversiality</title>
<author confidence="0.99503">Carenini</author>
<author confidence="0.99503">Jackie Chi Kit</author>
<affiliation confidence="0.9999675">Department of Computer University of British</affiliation>
<address confidence="0.999638">Vancouver, B.C. V6T 1Z4, Canada</address>
<email confidence="0.998076">carenini@cs.ubc.ca</email>
<email confidence="0.998076">cckitpw@cs.ubc.ca</email>
<abstract confidence="0.996838647058824">Extractive summarization is the strategy of concatenating extracts taken from a corpus into a summary, while abstractive summarization involves paraphrasing the corpus using novel sentences. We define a novel measure of corpus controversiality of opinions contained in evaluative text, and report the results of a user study comparing extractive and NLG-based abstractive summarization at different levels of controversiality. While the abstractive summarizer performs better overall, the results suggest that the margin by which abstraction outperforms extraction is greater when controversiality is high, providing a context in which the need for generationbased methods is especially great.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proc. 37th ACL,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="1519" citStr="Barzilay et al., 1999" startWordPosition="214" endWordPosition="217">iding a context in which the need for generationbased methods is especially great. 1 Introduction There are two main approaches to the task of summarization—extraction and abstraction (Hahn and Mani, 2000). Extraction involves concatenating extracts taken from the corpus into a summary, whereas abstraction involves generating novel sentences from information extracted from the corpus. It has been observed that in the context of multidocument summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al., 1999). However, there has been little work identifying specific factors which might affect the performance of each strategy in summarizing evaluative documents 1Authors are listed in alphabetical order. containing opinions and preferences, such as customer reviews or blogs. This work aims to address this gap by exploring one dimension along which the effectiveness of the two paradigms could vary; namely, the controversiality of the opinions contained in the corpus. In this paper, we make the following contributions. Firstly, we define a measure of controversiality of opinions in the corpus based on</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Regina Barzilay, Kathleen R. McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proc. 37th ACL, pages 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Johanna D Moore</author>
</authors>
<title>Generating and evaluating evaluative arguments.</title>
<date>2006</date>
<journal>Artificial Intelligence,</journal>
<pages>170--11</pages>
<contexts>
<context position="8512" citStr="Carenini and Moore, 2006" startWordPosition="1333" endWordPosition="1336">tically similar CFs, such as jpeg picture and jpeg slide show under the UDF JPEG. For the purposes of our study, feature extraction, polarity/strength identification and the mapping from CFs to UDFs are not done automatically as in (Hu and Liu, 2004) and (Carenini et al, 2005). Instead, “gold standard” annotations by humans are used in order to focus on the effect of the summarization strategy. 3.1 Abstractive Summarizer: SEA The abstractive summarizer is the Summarizer of Evaluative Arguments (SEA), adapted from GEA, a system for generating evaluative text tailored to the user&apos;s preferences (Carenini and Moore, 2006). In SEA, units of content are organized by UDFs. The importance of each UDF is based on the number and strength of evaluations of CFs mapped to this UDF, as well as the importance of its children UDFs. Content selection consists of repeating the following two steps until the desired number of UDFs have been selected: (i) greedily selecting the most important UDF (ii) recalculating the measure of importance scores for the remaining UDFs. The content structuring, microplanning, and realization stages of SEA are adapted from GEA. Each selected UDF is realized in the final summary by one clause, </context>
</contexts>
<marker>Carenini, Moore, 2006</marker>
<rawString>Giuseppe Carenini and Johanna D. Moore. 2006. Generating and evaluating evaluative arguments. Artificial Intelligence, 170(11):925-952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Adam Pauls</author>
</authors>
<title>Multi-document summarization of evaluative text.</title>
<date>2006</date>
<booktitle>In Proc. 11th EACL</booktitle>
<pages>305--312</pages>
<contexts>
<context position="6603" citStr="Carenini et al., 2006" startWordPosition="1021" endWordPosition="1024">(Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary. The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006). 3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000). Both summarizers rely on information extraction from the corpus. First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the stren</context>
<context position="23096" citStr="Carenini et al., 2006" startWordPosition="3724" endWordPosition="3727">eted Recall, Precision, and the general Accuracy of the summary contents respectively. The tenth question asked for a general overall quality judgement of the summary. After familiarizing themselves with the questionnaire, the participants were presented with the two summaries in sequence, and asked to fill out the questionnaire while reading the summary. They were allowed to return to the original set of reviews during this time. Lastly, they were given an additional questionnaire which asked them to compare the two summaries that they were shown. Questions in the questionnaire not found in (Carenini et al., 2006) are attached in Appendix A. 6 Results 6.1 Quantitative Results We convert the Likert responses from a scale from Strongly Disagree to Strong Agree to a scale from 1 to 5, with 1 corresponding to Strongly Disagree, and 5 to Strongly Agree. We group the ten questions into four categories: linguistic (questions 1 to 5), content (questions 6 to 8), footnote (question 9), and overall (question 10). See Table 1 for a breakdown of the responses for each question at each controversiality level. For our analysis, we adopt a two-step approach that has been applied in Computational Linguistics (Di Eugen</context>
</contexts>
<marker>Carenini, Ng, Pauls, 2006</marker>
<rawString>Giuseppe Carenini, Raymond Ng and Adam Pauls. 2006. Multi-document summarization of evaluative text. In Proc. 11th EACL 2006, pages 305-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Ed Zwart</author>
</authors>
<title>Extracting Knowledge from Evaluative Text.</title>
<date>2005</date>
<booktitle>In Proc. 3rd International Conference on Knowledge Capture,</booktitle>
<pages>11--18</pages>
<contexts>
<context position="8164" citStr="Carenini et al, 2005" startWordPosition="1281" endWordPosition="1284">ly on methods from previous work for these tasks (Hu and Liu, 2004). Once these features, called Crude Features (CFs), are extracted, they are mapped onto a taxonomy of User Defined Features (UDFs), so named because they can be defined by the user. This mapping provides a better conceptual organization of the CFs 34 by grouping together semantically similar CFs, such as jpeg picture and jpeg slide show under the UDF JPEG. For the purposes of our study, feature extraction, polarity/strength identification and the mapping from CFs to UDFs are not done automatically as in (Hu and Liu, 2004) and (Carenini et al, 2005). Instead, “gold standard” annotations by humans are used in order to focus on the effect of the summarization strategy. 3.1 Abstractive Summarizer: SEA The abstractive summarizer is the Summarizer of Evaluative Arguments (SEA), adapted from GEA, a system for generating evaluative text tailored to the user&apos;s preferences (Carenini and Moore, 2006). In SEA, units of content are organized by UDFs. The importance of each UDF is based on the number and strength of evaluations of CFs mapped to this UDF, as well as the importance of its children UDFs. Content selection consists of repeating the follo</context>
</contexts>
<marker>Carenini, Ng, Zwart, 2005</marker>
<rawString>Giuseppe Carenini, Raymond T. Ng and Ed Zwart. 2005. Extracting Knowledge from Evaluative Text. In Proc. 3rd International Conference on Knowledge Capture, pages 11-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Johanna D Moore</author>
</authors>
<title>A strategy for generating evaluative arguments.</title>
<date>2000</date>
<booktitle>In First INLG,</booktitle>
<pages>47--54</pages>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="10236" citStr="Carenini and Moore, 2000" startWordPosition="1615" endWordPosition="1618">liked the user interface, customers had mixed opinions about the range of compatible disc formats. However, users did agree on some things. Some users found the extra features to be very good even though customers had mixed opinions about the supplied universal remote control. Figure 1: SEA summary of a controversial corpus with a document structuring problem. Controversial and uncontroversial features are interwoven. See Figure 3 for an example of a summary structured with our alternative strategy. SEA summaries, which is adapted from GEA and is based on guidelines from argumentation theory (Carenini and Moore, 2000), sometimes sounded unnatural. We found that controversially rated UDF features (roughly balanced positive and negative evaluations) were treated as contrasts to those which were uncontroversially rated (either mostly positive, or mostly negative evaluations). In SEA, contrast relations between features are realized by cue phrases signalling contrast such as “however” and “although”. These cue phrases appear to signal a contrast that is too strong for the relation between controversial and uncontroversial features. An example of a SEA summary suffering from this problem can be found in Figure </context>
</contexts>
<marker>Carenini, Moore, 2000</marker>
<rawString>Giuseppe Carenini and Johanna D. Moore. 2000. A strategy for generating evaluative arguments. In First INLG, pages 47-54, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
<author>Michael J Trolio</author>
</authors>
<title>The DIAG experiments: Natural language generation for intelligent tutoring systems.</title>
<date>2002</date>
<booktitle>In INLG02, The 2nd INLG,</booktitle>
<pages>120--127</pages>
<marker>Di Eugenio, Glass, Trolio, 2002</marker>
<rawString>Barbara Di Eugenio, Michael Glass, and Michael J. Trolio. 2002. The DIAG experiments: Natural language generation for intelligent tutoring systems. In INLG02, The 2nd INLG, pages 120-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin.</journal>
<pages>76--378</pages>
<contexts>
<context position="3566" citStr="Fleiss, 1971" startWordPosition="539" endWordPosition="540">arization. A previous study on summarizing evaluative text (Carenini et. al, 2006) showed that extraction and abstraction performed about equally well, though for different reasons. The study, however, did not 33 look at the effect of the controversiality of the corpus on the relative performance of the two strategies. To the best of our knowledge, the task of measuring the controversiality of opinions in a corpus has not been studied before. Some well known measures are related to this task, including variance, information entropy, and measures of interrater reliability. (e.g. Fleiss&apos; Kappa (Fleiss, 1971), Krippendorff&apos;s Alpha (Krippendorff, 1980)). However, these existing measures do not satisfy certain properties that a sound measure of controversiality should possess, prompting us to develop our own based on information entropy. Summary evaluation is a challenging open research area. Existing methods include soliciting human judgements, task-based approaches, and automatic approaches. Task-based evaluation measures the effectiveness of a summarizer for its intended purpose. (e.g. (McKeown et al., 2005)) This approach, however, is less applicable in this work because we are interested in eva</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin. 76:378-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>I Mani</author>
</authors>
<title>The challenges of automatic summarization.</title>
<date>2000</date>
<journal>IEEE Computer,</journal>
<pages>33--11</pages>
<contexts>
<context position="1102" citStr="Hahn and Mani, 2000" startWordPosition="150" endWordPosition="153">fine a novel measure of corpus controversiality of opinions contained in evaluative text, and report the results of a user study comparing extractive and NLG-based abstractive summarization at different levels of controversiality. While the abstractive summarizer performs better overall, the results suggest that the margin by which abstraction outperforms extraction is greater when controversiality is high, providing a context in which the need for generationbased methods is especially great. 1 Introduction There are two main approaches to the task of summarization—extraction and abstraction (Hahn and Mani, 2000). Extraction involves concatenating extracts taken from the corpus into a summary, whereas abstraction involves generating novel sentences from information extracted from the corpus. It has been observed that in the context of multidocument summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al., 1999). However, there has been little work identifying specific factors which might affect the performance of each strategy in summarizing evaluative documents 1Authors are listed in alpha</context>
</contexts>
<marker>Hahn, Mani, 2000</marker>
<rawString>U. Hahn and I. Mani. 2000. The challenges of automatic summarization. IEEE Computer, 33(11):29-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hinckley</author>
<author>Randy Pausch</author>
<author>Dennis Proffitt</author>
<author>James Patten</author>
<author>Neal Kassell</author>
</authors>
<title>Cooperative bimanual action.</title>
<date>1997</date>
<booktitle>In Proc. CHI Conference.</booktitle>
<contexts>
<context position="23754" citStr="Hinckley et al., 1997" startWordPosition="3837" endWordPosition="3841">ts 6.1 Quantitative Results We convert the Likert responses from a scale from Strongly Disagree to Strong Agree to a scale from 1 to 5, with 1 corresponding to Strongly Disagree, and 5 to Strongly Agree. We group the ten questions into four categories: linguistic (questions 1 to 5), content (questions 6 to 8), footnote (question 9), and overall (question 10). See Table 1 for a breakdown of the responses for each question at each controversiality level. For our analysis, we adopt a two-step approach that has been applied in Computational Linguistics (Di Eugenio et al., 2002) as well as in HCI (Hinckley et al., 1997). First, we perform a two-way Analysis of Variance (ANOVA) test using the average response of the questions in each category. The two factors are controversiality of the corpus (high or low) as independent samples, and the summarizer (SEA or MEAD*) as repeated measures. We repeat this procedure for the average of the ten questions, termed Macro below. The p-values of these tests are summarized in Table 2. The results of the ANOVA tests indicate that SEA significantly outperforms MEAD* in terms of linguistic and overall quality, as well as for all the questions combined. It does not significant</context>
</contexts>
<marker>Hinckley, Pausch, Proffitt, Patten, Kassell, 1997</marker>
<rawString>Ken Hinckley, Randy Pausch, Dennis Proffitt, James Patten, and Neal Kassell. 1997. Cooperative bimanual action. In Proc. CHI Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. 10th ACM SIGKDD conference,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="7610" citStr="Hu and Liu, 2004" startWordPosition="1188" endWordPosition="1191">t (Radev et al., 2000). Both summarizers rely on information extraction from the corpus. First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the strength, and polarity (positive or negative) of the evaluation. For instance, in a corpus of customer reviews, the sentence “Excellent picture quality - on par with my Pioneer, Panasonic, and JVC players.” contains an opinion on the feature picture quality of a DVD player, and is a very positive evaluation (+3 on a scale from -3 to +3). We rely on methods from previous work for these tasks (Hu and Liu, 2004). Once these features, called Crude Features (CFs), are extracted, they are mapped onto a taxonomy of User Defined Features (UDFs), so named because they can be defined by the user. This mapping provides a better conceptual organization of the CFs 34 by grouping together semantically similar CFs, such as jpeg picture and jpeg slide show under the UDF JPEG. For the purposes of our study, feature extraction, polarity/strength identification and the mapping from CFs to UDFs are not done automatically as in (Hu and Liu, 2004) and (Carenini et al, 2005). Instead, “gold standard” annotations by huma</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proc. 10th ACM SIGKDD conference, pages 168-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="3609" citStr="Krippendorff, 1980" startWordPosition="543" endWordPosition="544">izing evaluative text (Carenini et. al, 2006) showed that extraction and abstraction performed about equally well, though for different reasons. The study, however, did not 33 look at the effect of the controversiality of the corpus on the relative performance of the two strategies. To the best of our knowledge, the task of measuring the controversiality of opinions in a corpus has not been studied before. Some well known measures are related to this task, including variance, information entropy, and measures of interrater reliability. (e.g. Fleiss&apos; Kappa (Fleiss, 1971), Krippendorff&apos;s Alpha (Krippendorff, 1980)). However, these existing measures do not satisfy certain properties that a sound measure of controversiality should possess, prompting us to develop our own based on information entropy. Summary evaluation is a challenging open research area. Existing methods include soliciting human judgements, task-based approaches, and automatic approaches. Task-based evaluation measures the effectiveness of a summarizer for its intended purpose. (e.g. (McKeown et al., 2005)) This approach, however, is less applicable in this work because we are interested in evaluating specific properties of the summary </context>
<context position="15872" citStr="Krippendorff, 1980" startWordPosition="2523" endWordPosition="2525">y takes into account the difference of each data point to the mean, and the sign of the data point plays no role. Information Entropy: The canonical form of information entropy does not satisfy strength or polarity sensitivity, because the measure considers the discrete values of the distribution to be an unordered set. Measures of Inter-rater Reliability: Many measures exist to assess inter-rater agreement or disagreement, which is the task of measuring how similarly two or more judges rate one or more subjects beyond chance (dis)agreement. Various versions of Kappa and Krippendorff&apos;s Alpha (Krippendorff, 1980), which have shown to be equivalent in their most generalized forms (Passonneau, 1997), can be modified to satisfy all the properties listed above. However, there are important differences between the tasks of measuring controversiality and measuring inter-rater reliability. Kappa and Krippendorff&apos;s Alpha correct for chance agreement between raters, which is appropriate in the context of inter-rater reliability calculations, because judges are asked to give their opinions on items that are given to them. In contrast, expressions of opinion are volunteered by users, and users self-select the fe</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proc. of Workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5992" citStr="Lin, 2004" startWordPosition="921" endWordPosition="922"> For example, summaries which misrepresented the polarity of the evaluations for a certain feature were not penalized, and human summaries sometimes produced contradictory statements about the distribution of the opinions. In one case, one model summary claimed that a feature is positively rated, while another claimed the opposite, whereas the machine summary indicated that this feature drew mixed reviews. Clearly, only one of these positions should be regarded as correct. Further work is needed to resolve these problems. There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary. The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proc. of Workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<title>Linguistic quality questions from the 2005 document understanding conference.</title>
<date>2005</date>
<note>http://duc.nist.gov/duc2005/quality-questions.txt</note>
<marker>2005</marker>
<rawString>2005. Linguistic quality questions from the 2005 document understanding conference. http://duc.nist.gov/duc2005/quality-questions.txt</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Rebecca Passonneau</author>
<author>David Elson</author>
<author>Ani Nenkova</author>
<author>Julia Hirschberg</author>
</authors>
<title>Do summaries help? A task-based evaluation of multi-document summarization.</title>
<date>2005</date>
<booktitle>In Proc. SIGIR</booktitle>
<contexts>
<context position="4076" citStr="McKeown et al., 2005" startWordPosition="608" endWordPosition="611">uding variance, information entropy, and measures of interrater reliability. (e.g. Fleiss&apos; Kappa (Fleiss, 1971), Krippendorff&apos;s Alpha (Krippendorff, 1980)). However, these existing measures do not satisfy certain properties that a sound measure of controversiality should possess, prompting us to develop our own based on information entropy. Summary evaluation is a challenging open research area. Existing methods include soliciting human judgements, task-based approaches, and automatic approaches. Task-based evaluation measures the effectiveness of a summarizer for its intended purpose. (e.g. (McKeown et al., 2005)) This approach, however, is less applicable in this work because we are interested in evaluating specific properties of the summary such as the grammaticality and the content, which may be difficult to evaluate with an overall task-based approach. Furthermore, the design of the task may intrinsically favour abstractive or extractive summarization. As an extreme example, asking for a list of specific comments from users would clearly favour extractive summarization. Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact tha</context>
</contexts>
<marker>McKeown, Passonneau, Elson, Nenkova, Hirschberg, 2005</marker>
<rawString>Kathleen McKeown, Rebecca Passonneau, David Elson, Ani Nenkova, and Julia Hirschberg. 2005. Do summaries help? A task-based evaluation of multi-document summarization. In Proc. SIGIR 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca J Passonneau</author>
<author>K McKeown</author>
</authors>
<title>The pyramid method: incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="6236" citStr="Nenkova et al., 2007" startWordPosition="960" endWordPosition="963">e, one model summary claimed that a feature is positively rated, while another claimed the opposite, whereas the machine summary indicated that this feature drew mixed reviews. Clearly, only one of these positions should be regarded as correct. Further work is needed to resolve these problems. There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary. The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006). 3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain. These summarizers have been found to produce quantita</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca J. Passonneau, and K. McKeown. 2007. The pyramid method: incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proc. NAACL/HLT.</booktitle>
<contexts>
<context position="4637" citStr="Nenkova and Passonneau, 2004" startWordPosition="693" endWordPosition="696"> a summarizer for its intended purpose. (e.g. (McKeown et al., 2005)) This approach, however, is less applicable in this work because we are interested in evaluating specific properties of the summary such as the grammaticality and the content, which may be difficult to evaluate with an overall task-based approach. Furthermore, the design of the task may intrinsically favour abstractive or extractive summarization. As an extreme example, asking for a list of specific comments from users would clearly favour extractive summarization. Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact that human summaries with different content can be equally informative. Multiple human summaries are taken to be models, and chunks of meaning known as Summary Content Units (SCU) are manually identified. Peer summaries are evaluated based on how many SCUs they share with the model summaries, and the number of model summaries in which these SCUs are found. Although this method has been tested in DUC 2006 and DUC 2005 (Passonneau et al., 2006), (Passonneau et al., 2005) in the domain of news articles, it has not been tested for evaluative text. A pilot study </context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proc. NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Kathleen McKeown</author>
<author>Sergey Sigleman</author>
<author>Adam Goodkind</author>
</authors>
<title>Applying the pyramid method in the 2006 Document Understanding Conference. In</title>
<date>2006</date>
<booktitle>Proc. DUC&apos;06.</booktitle>
<contexts>
<context position="5119" citStr="Passonneau et al., 2006" startWordPosition="776" endWordPosition="779">sers would clearly favour extractive summarization. Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact that human summaries with different content can be equally informative. Multiple human summaries are taken to be models, and chunks of meaning known as Summary Content Units (SCU) are manually identified. Peer summaries are evaluated based on how many SCUs they share with the model summaries, and the number of model summaries in which these SCUs are found. Although this method has been tested in DUC 2006 and DUC 2005 (Passonneau et al., 2006), (Passonneau et al., 2005) in the domain of news articles, it has not been tested for evaluative text. A pilot study that we conducted on a set of customer reviews on a product using the Pyramid method revealed several problems specific to the evaluative domain. For example, summaries which misrepresented the polarity of the evaluations for a certain feature were not penalized, and human summaries sometimes produced contradictory statements about the distribution of the opinions. In one case, one model summary claimed that a feature is positively rated, while another claimed the opposite, whe</context>
</contexts>
<marker>Passonneau, McKeown, Sigleman, Goodkind, 2006</marker>
<rawString>Rebecca J. Passonneau, Kathleen McKeown, Sergey Sigleman, and Adam Goodkind. 2006. Applying the pyramid method in the 2006 Document Understanding Conference. In Proc. DUC&apos;06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
<author>Sergey Sigleman</author>
</authors>
<title>Applying the pyramid method in DUC</title>
<date>2005</date>
<booktitle>In Proc. DUC&apos;05.</booktitle>
<contexts>
<context position="5146" citStr="Passonneau et al., 2005" startWordPosition="780" endWordPosition="783">xtractive summarization. Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact that human summaries with different content can be equally informative. Multiple human summaries are taken to be models, and chunks of meaning known as Summary Content Units (SCU) are manually identified. Peer summaries are evaluated based on how many SCUs they share with the model summaries, and the number of model summaries in which these SCUs are found. Although this method has been tested in DUC 2006 and DUC 2005 (Passonneau et al., 2006), (Passonneau et al., 2005) in the domain of news articles, it has not been tested for evaluative text. A pilot study that we conducted on a set of customer reviews on a product using the Pyramid method revealed several problems specific to the evaluative domain. For example, summaries which misrepresented the polarity of the evaluations for a certain feature were not penalized, and human summaries sometimes produced contradictory statements about the distribution of the opinions. In one case, one model summary claimed that a feature is positively rated, while another claimed the opposite, whereas the machine summary in</context>
</contexts>
<marker>Passonneau, Nenkova, McKeown, Sigleman, 2005</marker>
<rawString>Rebecca J. Passonneau, Ani Nenkova, Kathleen McKeown, and Sergey Sigleman. 2005. Applying the pyramid method in DUC 2005. In Proc. DUC&apos;05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>Applying Reliability Metrics to Co-Reference Annotation.</title>
<date>1997</date>
<pages>017--97</pages>
<institution>Department of Computer Science, Columbia University, TR</institution>
<contexts>
<context position="15958" citStr="Passonneau, 1997" startWordPosition="2538" endWordPosition="2539">data point plays no role. Information Entropy: The canonical form of information entropy does not satisfy strength or polarity sensitivity, because the measure considers the discrete values of the distribution to be an unordered set. Measures of Inter-rater Reliability: Many measures exist to assess inter-rater agreement or disagreement, which is the task of measuring how similarly two or more judges rate one or more subjects beyond chance (dis)agreement. Various versions of Kappa and Krippendorff&apos;s Alpha (Krippendorff, 1980), which have shown to be equivalent in their most generalized forms (Passonneau, 1997), can be modified to satisfy all the properties listed above. However, there are important differences between the tasks of measuring controversiality and measuring inter-rater reliability. Kappa and Krippendorff&apos;s Alpha correct for chance agreement between raters, which is appropriate in the context of inter-rater reliability calculations, because judges are asked to give their opinions on items that are given to them. In contrast, expressions of opinion are volunteered by users, and users self-select the features they comment on. Thus, it is reasonable to assume that they never randomly sele</context>
</contexts>
<marker>Passonneau, 1997</marker>
<rawString>Rebecca J. Passonneau. 1997. Applying Reliability Metrics to Co-Reference Annotation. Department of Computer Science, Columbia University, TR CUCS-017-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In Proc. ANLP/NAACL Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="7015" citStr="Radev et al., 2000" startWordPosition="1084" endWordPosition="1087">rs to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006). 3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000). Both summarizers rely on information extraction from the corpus. First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the strength, and polarity (positive or negative) of the evaluation. For instance, in a corpus of customer reviews, the sentence “Excellent picture quality - on par with my Pioneer, Panasonic, and JVC players.” contains an opinion on the feature picture quality of a DVD player, and is a very positive evaluation (+3 on a scale from -3 to +3). We rely on methods from previous work for these tasks (Hu and Liu, 2004). Onc</context>
<context position="11214" citStr="Radev et al., 2000" startWordPosition="1760" endWordPosition="1763">s “however” and “although”. These cue phrases appear to signal a contrast that is too strong for the relation between controversial and uncontroversial features. An example of a SEA summary suffering from this problem can be found in Figure 1. To solve this problem, we devised an alternative content structure for controversial corpora, in which all controversial features appear first, followed by all positively and negatively evaluated features. 3.2 Extractive Summarizer: MEAD* The extractive approach is represented by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000). After information extraction, MEAD* orders CFs by the number of sentences evaluating that CF, and selects a sentence from each CF until the word limit has been reached. The sentence that is selected for each CF is the one with the highest sum of polarity/strength evaluations for any feature, so sentences that mention more CFs tend to be selected. The selected sentences are then ordered according to the UDF hierarchy by a depthfirst traversal through the UDF tree so that more abstract features tend to precede more specific ones. MEAD* does not have a special mechanism to deal with controversi</context>
</contexts>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>Dragomir Radev, Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies. In Proc. ANLP/NAACL Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegel</author>
<author>N J Castellan</author>
</authors>
<title>Nonparametric statistics for the behaviorial sciences.</title>
<date>1988</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="26543" citStr="Siegel and Castellan, 1988" startWordPosition="4335" endWordPosition="4338">ontent 0.9215 0.1906 0.2277 Footnote 0.2457 0.7805 1 Overall 0.6301 0.0115 0.2000 Macro 0.7127 0.0003 0.1655 Table 2: Two-way ANOVA p-values. rizers over the two levels of controversiality for any of the question sets . While the average differences in scores between the SEA and MEAD* summarizers are greater in the controversial case for the linguistic, content, and macro averages as well as the question on the overall quality, the p-values for interaction between the two factors in the two-way ANOVA test are not significant. For the second step of the analysis, we use a one-tailed sign test (Siegel and Castellan, 1988) over the difference in performance of the summarizers at the two levels of controversiality for the questions in the questionnaire. We encode + in the case where the difference between SEA and MEAD* is greater for a question in the controversial setting, – if the difference is smaller, and we discard a question if the difference is the same (e.g. the Footnote question). Since the Overall question is likely correlated with the responses of the other questions, we did not include it in the test. After discarding the Footnote question, the p-value over the remaining eight questions is 0.0352, wh</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>S. Siegel and N. J. Castellan, Jr. 1988. Nonparametric statistics for the behaviorial sciences. McGraw Hill.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>