<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000076">
<title confidence="0.646739">
F2 — New Technique for Recognition of User Emotional States in
Spoken Dialogue Systems
</title>
<author confidence="0.90325">
Ram6n L6pez-C6zar
</author>
<affiliation confidence="0.84979">
Dept. of Languages and
Computer Systems, CTIC-
UGR, University of Granada,
Spain
</affiliation>
<email confidence="0.995629">
rlopezc@ugr.es
</email>
<author confidence="0.991073">
Jan Silovsky
</author>
<affiliation confidence="0.90056925">
Institute of Information
Technology and Electronics,
Technical University of
Liberec, Czech Republic
</affiliation>
<email confidence="0.996255">
jan.silovsky@tul.cz
</email>
<author confidence="0.993263">
David Griol
</author>
<affiliation confidence="0.959103">
Dept. of Computer Science
Carlos III University of
</affiliation>
<address confidence="0.925695">
Madrid, Spain
</address>
<email confidence="0.997053">
dgriol@inf.uc3m.es
</email>
<sectionHeader confidence="0.994992" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9964792">
In this paper we propose a new technique to
enhance emotion recognition by combining
in different ways what we call emotion pre-
dictions. The technique is called F2 as the
combination is based on a double fusion
process. The input to the first fusion phase is
the output of a number of classifiers which
deal with different types of information re-
garding each sentence uttered by the user.
The output of this process is the input to the
second fusion stage, which provides as out-
put the most likely emotional category. Ex-
periments have been carried out using a pre-
viously-developed spoken dialogue system
designed for the fast food domain. Results
obtained considering three and two emo-
tional categories show that our technique
outperforms the standard single fusion tech-
nique by 2.25% and 3.35% absolute, respec-
tively.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867945945946">
Automatic recognition of user emotional states
is a very challenging task that has attracted the
attention of the research community for several
decades. The goal is to design methods to
make computers interact more naturally with
human beings. This is a very complex task due
to a variety of reasons. One is the absence of a
generally agreed definition of emotion and of
qualitatively different types of emotion. An-
other is that we still have an incomplete under-
standing of how humans process emotions, as
even people have difficulty in distinguishing
between them. Thus, in many cases a given
emotion is perceived differently by different
people.
Studies in emotion recognition made by the
research community have been applied to en-
hance the quality or efficiency of several ser-
vices provided by computers. For example,
these have been applied to spoken dialogue
systems (SDSs) used in automated call-centres,
where the goal is to detect problems in the in-
teraction and, if appropriate, transfer the call
automatically to a human operator.
The remainder of the paper is organised as
follows. Section 2 addresses related work on
the application of emotion recognition to
SDSs. Section 3 focuses on the proposed tech-
nique, describing the classifiers and fusion
methods employed in the current implementa-
tion. Section 4 discusses our speech database
and its emotional annotation. Section 5 pre-
sents the experiments, comparing results ob-
tained using the standard single fusion tech-
nique with the proposed double fusion. Finally,
Section 6 presents the conclusions and outlines
possibilities for future work.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99996575">
Many studies can be found in the literature
addressing potential improvements to SDSs by
recognising user emotional states. A diversity
of speech databases, features used for training
and recognition, number of emotional catego-
ries, and recognition methods have been pro-
posed. For example, Batliner et al. (2003) em-
ployed three different databases to detect trou-
bles in communication. One was collected
from a single experienced actor who was told
to express anger because of system malfunc-
tions. Other was collected from naive speakers
who were asked to read neutral and emotional
sentences. The third database was collected
using a WOZ scenario designed to deliberately
provoke user reactions to system malfunctions.
The study focused on detecting two emotion
categories: emotional (e.g. anger) and neutral,
employing classifiers that dealt with prosodic,
linguistic, and discourse information.
</bodyText>
<note confidence="0.622686">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 281–288,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</note>
<page confidence="0.997866">
281
</page>
<bodyText confidence="0.999518903225806">
Liscombe et al. (2005) made experiments
with a corpus of 5,690 dialogues collected with
the “How May I Help You” system, and con-
sidered seven emotional categories: posi-
tive/neutral, somewhat frustrated, very frus-
trated, somewhat angry, very angry, somewhat
other negative, and very other negative. They
employed standard lexical, prosodic and con-
textual features.
Devillers and Vidrascu (2006) employed
human-to-human dialogues on a financial task,
and considered four emotional categories: an-
ger, fear, relief and sadness. Emotion classifi-
cation was carried out considering linguistic
information and paralinguistic cues.
Ai et al. (2006) used a database collected
from 100 dialogues between 20 students and a
spoken dialogue tutor, and for classification
employed lexical items, prosody, user gender,
beginning and ending time of turns, user turns
in the dialogue, and system/user performance
features. Four emotional categories were con-
sidered: uncertain, certain, mixed and neutral.
Morrison et al. (2007) compared two emo-
tional speech data sources The former was col-
lected from a call-centre in which customers
talked directly to a customer service represen-
tative. The second database was collected from
12 non-professional actors and actresses who
simulated six emotional categories: anger, dis-
gust, fear, happiness, sadness and surprise.
</bodyText>
<sectionHeader confidence="0.962482" genericHeader="method">
3 The proposed technique
</sectionHeader>
<bodyText confidence="0.999973636363637">
The technique that we propose in this paper to
enhance emotion recognition in SDSs consid-
ers that a set of classifiers 91 = {C1, C2, ..., Cm}
receive as input feature vectors f related to
each sentence uttered by the user. As a result,
each classifier generates one emotion predic-
tion, which is a vector of pairs (hi, pi), i = 1...S,
where hi is an emotional category (e.g. Angry),
pi is the probability of the utterance belonging
to hi in accordance with the classifier, and S is
the number of emotional categories considered,
which forms the set E = {e1, e2, ..., eS}.
The emotion predictions generated by the
classifiers make up the input to the first fusion
stage, which we call Fusion-0. This stage em-
ploys n fusion methods called F0i, i = 1...n, to
generate other predictions: vectors of pairs
(h0j,k , p0j,k), j = 1...n, k = 1...S, where h0j,k is an
emotional category, and p0j,k is the probability
of the utterance belonging to h0j,k in accordance
with the fusion method F0j.
The second fusion stage, called Fusion-1,
receives the predictions provided by Fusion-0
and generates the pair (h11,1 , p11,1), where h11,1
is the emotional category with highest prob-
ability, p11,1. This emotional category is deter-
mined employing a fusion method called F11,
and represents the user’s emotional state de-
duced by the technique. The best combination
of fusion methods to be used in Fusion-0 (F01,
F02,...,F0j, 1 ≤ j ≤ n) and the best fusion method
to be used in Fusion-1 (F11) must be experi-
mentally determined.
</bodyText>
<subsectionHeader confidence="0.997627">
3.1 Classifiers
</subsectionHeader>
<bodyText confidence="0.999399">
In the current implementation our technique
employs four classifiers, which deal with pros-
ody, acoustics, lexical items and dialogue acts
regarding each utterance.
</bodyText>
<subsectionHeader confidence="0.911743">
3.1.1 Prosodic classifier
</subsectionHeader>
<bodyText confidence="0.99995565">
The input to our prosodic classifier is an n-
dimensional feature vector obtained from
global statistics of pitch and energy, and fea-
tures derived from the duration of
voiced/unvoiced segments in each utterance.
After carrying out experiments to find the ap-
propriate feature set for the classifier, we de-
cided to use the following 11 features: pitch
mean, minimum and maximum, pitch deriva-
tives mean, mean and variance of absolute val-
ues of pitch derivatives, energy maximum,
mean of absolute value of energy derivatives,
correlation of pitch and energy derivatives,
average length of voiced segments, and dura-
tion of longest monotonous segment.
The classifier employs gender-dependent
Gaussian Mixture Models (GMMs) to repre-
sent emotional categories. The likelihood for
the n-dimensional feature vector (x), given an
emotional category λ, is defined as:
</bodyText>
<equation confidence="0.994934">
Q
P(x ) ∑
λ = wlPl x
( )
l=1
</equation>
<bodyText confidence="0.981060666666667">
i.e., a weighted linear combination of Q uni-
modal Gaussian densities Pl(x). The density
function Pl(x) is defined as:
</bodyText>
<equation confidence="0.9991481">
1  1 ′
P x
( ) exp ( ) ( ) 
=  − − Σ −
− 1
x � x �
l l l l
( )n
2 det 2
π Σ l
</equation>
<bodyText confidence="0.988656">
where the $l’s are mean vectors and the Σl‘s
covariance matrices. The emotional category
</bodyText>
<page confidence="0.994309">
282
</page>
<bodyText confidence="0.9998565">
deduced by the classifier, h, is decided accord-
ing to the following expression:
where λS represents the models for the emo-
tional categories considered, and the max func-
tion is computed employing the EM (Expecta-
tion-Maximization) algorithm. To compute the
probabilities pi for the emotion prediction of
the classifier we use the following expression:
</bodyText>
<equation confidence="0.929819666666667">
S
pi fli / E flk (2)
k=1
</equation>
<bodyText confidence="0.99981875">
where βi is the log-likelihood of hi, S is the
number of emotional categories considered,
and the βk’s are the log-likelihoods of these
emotional categories.
</bodyText>
<subsectionHeader confidence="0.921797">
3.1.2 Acoustic classifier
</subsectionHeader>
<bodyText confidence="0.99991678125">
Prosodic features are nowadays among the
most popular features for emotion recognition
(Dellaert et al. 1996; Luengo et al. 2005).
However, several authors have evaluated other
features. For example, Nwe et al. (2003) em-
ployed several short-term spectral features and
observed that Logarithmic Frequency Power
Coefficients (LFPCs) provide better perform-
ance than Mel-Frequency Cepstral Coefficient
(MFCCs) or Linear Prediction Cepstral Coeffi-
cients (LPCCs). Experiments carried out with
our speech database (which will be discussed
in Section 4) have confirmed this observation.
However, we have also noted that when we
used the first and second derivatives, the best
results were obtained for MFCCs. Hence, we
decided to use 39-feature MFCCs (13 MFCCs,
delta and delta-delta) for classification.
The emotion patterns of the input utterances
are modelled by gender-dependent GMMs, as
with the prosodic classifier, but each input ut-
terance is represented employing a sequence of
feature vectors x = {x1,...,xT} instead of one n-
dimensional vector. We assume mutual inde-
pendence of the feature vectors in x, and com-
pute the log-likelihood for an emotional cate-
gory λ as follows:
The emotional category deduced by the classi-
fier, h, is decided employing Eq. (1), whereas
Eq. (2) is used to compute the probabilities for
the prediction, i.e. for the vector of pairs (hi,
pi).
</bodyText>
<subsectionHeader confidence="0.979736">
3.1.3 Lexical classifier
</subsectionHeader>
<bodyText confidence="0.999892159090909">
A number of previous studies on emotion rec-
ognition take into account information about
the kinds of word uttered by the users, assum-
ing that there is a relationship between words
and emotion categories. For example, swear
words and insults can be considered as convey-
ing a negative emotion (Lee and Narayanan,
2005). Analysis of our dialogue corpus (which
will be discussed in Section 4) has shown that
users did not utter swear words or insults dur-
ing the interaction with the Saplen system.
Nevertheless, there were particular moments in
the interaction at which their emotional state
changed from Neutral to Tired or Angry. These
moments correspond to dialogue states where
the system had problems in recognising the
sentences uttered by the users.
The reasons for these problems are basically
two. On the one hand, most users spoke with
strong southern Spanish accents, characterised
by the deletion of the final s of plural words,
and an exchange of the phonemes s and c in
many words. On the other hand, there are
words in the system’s vocabulary that are very
similar acoustically.
Hence, our goal has been to automatically
find these words by means of a study of the
speech recognition results, and deduce the
emotional category for each input utterance
from the emotional information associated
with the words in the recognition result. To do
this we have followed the study of Lee and
Narayanan (2005), which employs the infor-
mation-theoretic concept of emotional sali-
ence. The emotional salience of a word for a
given emotional category can be defined as the
mutual information between the word and the
emotional category. Let W be a sentence
(speech recognition result) comprised of a se-
quence of n words: W = w1 w2 ...wn, and E a
set of emotional categories, E = {e1, e2, ... ,eS}.
The mutual information between the word wi
and an emotional category ej is defined as fol-
lows:
</bodyText>
<equation confidence="0.50322775">
h = arg max P(x AS )
S
(1)
T mutual _ Information(wi , ej) = log P(ej  |wi )
P(x ) E
A = log P(xt A)
t=1
P(ej )
</equation>
<page confidence="0.993309">
283
</page>
<bodyText confidence="0.999977">
where P(ej  |wi) is the posterior probability that
a sentence containing the word wi implies the
emotional category ej, and P(ej) represents the
prior probability of the emotional category.
Taking into account the previous definitions,
we have defined the emotional salience of the
word wi for an emotional category ej as fol-
lows:
After the salient words for each emotional
category have been identified employing a
training corpus, we can carry out emotion rec-
ognition at the sentence level, considering that
each word in the sentence is independent of the
rest. The goal is to map the sentence W to any
of the emotional categories in E. To do this, we
compute an activation value ak for each emo-
tional category as follows:
</bodyText>
<equation confidence="0.96926175">
n
ak =∑ I m wmk wk
+
m=1
</equation>
<bodyText confidence="0.964124307692308">
where k = 1...S, n is the number of words in
W, Im represents an indicator that has the value
1 if wk is a salient word for the emotional cate-
gory (i.e. salience(wi,ej) ≠ 0) and the value 0
otherwise; wmk is the connection weight be-
tween the word and the emotional category,
and wk represents bias. We define the connec-
tion weight wmk as:
wmk =mutual_ Information(wm , ek )
whereas the bias is computed as:
wk = log P(ek) . Finally, the emotional cate-
gory deduced by the classifier, h, is the one
with highest activation value ak:
</bodyText>
<equation confidence="0.899335">
h = arg max(ak )
k
</equation>
<bodyText confidence="0.999836666666667">
To compute the probabilities pi‘s for the emo-
tion prediction, we use the following expres-
sion:
</bodyText>
<equation confidence="0.938967">
S
pi = ai /
∑ aj
j=1
</equation>
<bodyText confidence="0.999848">
where ai represents the activation value of hi,
and the aj’s are the activation values of the S
emotional categories considered.
</bodyText>
<subsectionHeader confidence="0.847992">
3.1.4 Dialogue acts classifier
</subsectionHeader>
<bodyText confidence="0.999864055555556">
A dialogue act can be defined as the function
performed by an utterance within the context
of a dialogue, for example, greeting, closing,
suggestion, rejection, repeat, rephrase, confir-
mation, specification, disambiguation, or help
(Batliner et al. 2003; Lee and Narayanan,
2005; Liscombe et al. 2005).
Our dialogue acts classifier is inspired by
the study of Liscombe et al. (2005), where the
sequential structure of each dialogue is mod-
elled by a sequence of dialogue acts. A differ-
ence is that they assigned one or more labels
related to dialogue acts to each user utterance,
and did not assign labels to system prompts,
whereas we assign just one label to each sys-
tem prompt and none to user utterances. This
decision is made from the examination of our
dialogue corpus. We have observed that users
got tired or angry if the system generated the
same prompt repeatedly (i.e. repeated the same
dialogue act) to try to get a particular data
item. For example, if it had difficulty in obtain-
ing a telephone number then it employed sev-
eral dialogue turns to obtain the number and
confirm it, which annoyed the users, especially
if they had employed other turns previously to
correct misunderstandings. Hence, our dia-
logue act classifier aims to predict these nega-
tive emotional states by detecting successive
repetitions of the same system’s prompt types
(e.g. prompts to get the telephone number).
In accordance with our approach, the emo-
tional category of a user’s dialogue turn, En, is
that which maximises the posterior probability
given a sequence of the most recent system
prompts:
</bodyText>
<equation confidence="0.9808955">
En = argmaxP(Ek  |DAn−(L* 2−1),... ,DAn−3,DAn−1)
k
</equation>
<bodyText confidence="0.9998678">
where the prompt sequence is represented by a
sequence of dialogue acts (DAi’s) and L is the
length of the sequence, i.e. the number of sys-
tem’s dialogue turns in the sequence. Note that
if L = 1 then the decision about En depends
only on the previous system prompt. In other
words, the emotional category obtained is that
with the greatest probability given just the pre-
vious system turn in the dialogue. The prob-
ability of the considered emotional categories
</bodyText>
<figure confidence="0.980573076923077">
,
)
=
salience(
ej
wi
,
× mutual
)
_Information
ej
P(ej  |wi)
(wi
</figure>
<page confidence="0.99256">
284
</page>
<bodyText confidence="0.999884888888889">
given a sequence of dialogue acts is obtained
by employing a training dialogue corpus.
By means of this equation, we decide the
most likely emotional category for the input
utterance, selecting the category with the high-
est probability given the sequence of dialogue
acts of length L. This probability is used to
create the pair (hi, pi) to be included in the
emotion prediction.
</bodyText>
<subsectionHeader confidence="0.818592">
3.2 Fusion methods
</subsectionHeader>
<bodyText confidence="0.999992285714286">
In the current implementation our technique
employs the three fusion methods discussed in
this section. When used in Fusion-0, these
methods are employed to combine the predic-
tions provided by the classifiers. When used in
Fusion-1, they are used to combine the predic-
tions generated by Fusion-0.
</bodyText>
<subsectionHeader confidence="0.977841">
3.2.1 Average of probabilities (AP)
</subsectionHeader>
<bodyText confidence="0.9999215">
This method combines the predictions by aver-
aging their probabilities. To do this we con-
sider that each input utterance is represented
by feature vectors x1,...,xm from feature spaces
X1,...,Xm, where m is the number of classifiers.
We also assume that each input utterance be-
longs to one of S emotional categories hi, i =
1...S. In each of the m feature spaces a classi-
fier can be created that approximates the poste-
rior probability P(hi  |xk) as follows:
</bodyText>
<equation confidence="0.9036322">
f i x =P h x +ε x
k k
( ) (  |) k ( k)
k
i i
</equation>
<bodyText confidence="0.869251090909091">
where k ( k )
ε i x is the error made by classifier
k. We estimate P(hi  |xk) by k ( k )
f i x and as-
suming a zero-mean error for k ( k)
ε i x , we av-
erage all the k ( k )
f i x ’s to obtain a less error-
sensitive estimation. In this way we obtain the
following mean combination rule to decide the
most likely emotional category:
</bodyText>
<subsectionHeader confidence="0.984441">
3.2.2 Multiplication of probabilities (MP)
</subsectionHeader>
<bodyText confidence="0.999898666666667">
Assuming that the feature spaces X1,...,Xm are
different and independent, the probabilities can
be written as follows:
</bodyText>
<equation confidence="0.9982035">
, xm  |hi)
P(x1  |hi) × P(x2  |hi) × ... × P(xm  |hi)
</equation>
<bodyText confidence="0.9998555">
Using Bayes rule we can obtain the following
equation, which we use to decide the most
likely emotional category for each input utter-
ance (represented as feature vectors x1,...,xm):
</bodyText>
<subsectionHeader confidence="0.705858">
3.2.3 Unweighted vote (UV)
</subsectionHeader>
<bodyText confidence="0.999955833333333">
This method combines the emotion predictions
by counting the number of classifiers (if used
in Fusion-0) or fusion methods (if used in Fu-
sion-1) that consider an emotional category hi
as the most likely for the input utterance. If we
consider three emotional categories X, Y and Z,
hi is decided as follows:
where m is the number of classifiers or fusion
methods employed (e.g., in our experiments, X
= Neutral, Y = Tired and Z = Angry). The
probability pi for hi to be included in the emo-
tion prediction is computed as follows:
</bodyText>
<equation confidence="0.974695">
3
hi  |X, Y, Z) = Vhi / ∑ Vhj
j=1
</equation>
<bodyText confidence="0.999962142857143">
where Vhi is the number of votes for hi, and the
Vhj‘s are the number of votes for the 3 emo-
tional categories. If we consider two emotional
categories X and Y, the most likely emotional
category hi and its probability pi are analo-
gously computed (e.g., in our experiments, X =
Non-negative and Y = Negative).
</bodyText>
<sectionHeader confidence="0.990165" genericHeader="method">
4 Emotional speech database
</sectionHeader>
<bodyText confidence="0.999826">
Our emotional speech database has been con-
structed from a corpus of 440 telephone-based
dialogues between students of the University
of Granada and the Saplen system, which was
</bodyText>
<equation confidence="0.953876788461539">
h x
 |, ... , )
1 xm = f x
i
m k=1
m
1 k
i
∑ )
k
(
(
P
1
k
)
m
P(
∏
hi  |x
/ P( hi)
/ ( )
P h i &apos;
)
m
x
=
,
1
x,
. . .
)
P(hi |
1
m

&apos;
k
∑ ∏

i &apos; k &apos;
k
P(hi&apos;  |x
m m m m
X if ∑Xj ≥ ∑ Yj and ∑ X j ≥ ∑ Zj
j=1 j=1 j=1 j=1
m m m m
Y if ∑Yj ≥ ∑ X j and ∑Yj ≥ ∑ Zj
j=1 j=1 j=1 j=1
m m m m
Z if ∑Zj ≥ ∑Xi and ∑Zj ≥ ∑ Yj
j=1 j=1 j=1 j=1
</equation>
<figure confidence="0.895253823529412">








=
hi

(
P
. . .
x1 ,
(
P
</figure>
<page confidence="0.996451">
285
</page>
<bodyText confidence="0.999445212121212">
previously developed in our lab for the fast
food domain (López-Cózar et al. 1997; López-
Cózar and Callejas, 2006). Each dialogue was
stored in a log file in text format that includes
each system prompt (e.g. Would you like to
drink anything?), the type of prompt (e.g. Any-
FoodOrDrinkToOrder?), the name of the voice
samples file (utterance) that stores the user re-
sponse to the prompt, and the speech recogni-
tion result for the utterance. The dialogue cor-
pus contains 7,923 utterances, 50.3% of which
were recorded by male users and the remaining
by female users.
The utterances have been annotated by 4 la-
bellers (2 male and 2 female). The order of the
utterances has been randomly chosen to avoid
influencing the labellers by the situation in the
dialogues, thus minimising the effect of dis-
course context. The labellers have initially as-
signed one label to each utterance, either
&lt;NEUTRAL&gt;, &lt;TIRED&gt; or &lt;ANGRY&gt; according
to the perceived emotional state of the user.
One of these labels has been finally assigned to
each utterance according to the majority opin-
ion of the labellers, so that 81% of the utter-
ances are annotated as ‘Neutral’, 9.5% as
‘Tired’ and 9.4% as ‘Angry’. This shows that
the database is clearly unbalanced in terms of
emotional categories.
To measure the amount of agreement be-
tween the labellers we employed the Kappa
statistic (K), which is computed as follows
(Cohen, 1960):
</bodyText>
<equation confidence="0.991316">
P(A) − P(E)
1( )
−P E
</equation>
<bodyText confidence="0.998543461538462">
where P(A) is the proportion of times that the
labellers agree, and P(E) is the proportion of
times we would expect the labellers to agree by
chance. We obtained that K = 0.48 and K =
0.45 for male and female labellers, respec-
tively, which according to Landis and Koch
(1977) represents moderate agreement.
d the three fusion methods dis-
cussed in Section 3.2.
employing the former category set will be
called 3-emotion experiments, whereas
those employing the latter category will be
called 2-emotion experiments.
</bodyText>
<listItem confidence="0.9185965">
• The four classifiers described in Section
3.1, an
</listItem>
<bodyText confidence="0.99420945">
tains the
response to the prompt, and the
result provided by the
speech recog-
niser (sentence in text format). The type of
each prompt is used to create a sequence of
dialogue acts of length L, which is the input to
the dialogue acts classifier. The voice samples
file is the input to the prosodic and acoustic
classifiers, and the speech recognition result is
the input to the lexical classifier. This proce-
dure is repeated for all the dialogues in the cor-
pus.
Experimental results have been obtained us-
ing 5-fold cross-validation, with each partition
containing the utteran
user’s
system’s
ces corresponding to 88
different dialogues in the corpus.
</bodyText>
<subsectionHeader confidence="0.992016">
5.1 Performance of Fusion-0
</subsectionHeader>
<bodyText confidence="0.6157285">
K
d employing:
</bodyText>
<sectionHeader confidence="0.997557" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.993525333333333">
The main goal of the experiments has been to
test the proposed technique using our emo-
tional speech database, an
</bodyText>
<listItem confidence="0.944151">
• Three emotional categories (Neutral, An-
gry and Tired) on the one hand, and two
emotional categories (Non-negative and
Negative) on the other. The experi
</listItem>
<bodyText confidence="0.997075193548387">
In the 3-emotion experiments we consider that
an input utterance is correctly classified if the
emotional category deduced by the technique
matches the label assigned to the utterance. In
the 2-emotion experiments, the utterance is
considered to be correctly classified if either
the deduced emotional category is Non-
negative and the label is Neutral, or the cate-
gory is Negative and the label is Tired or An-
gry.
To carry out training and testing we have
used a script that takes as its input a set of la-
belled dialogues in a corpus, and processes
each dialogue by locating within it, from the
beginning to the end, each prompt of the
Saplen system, the voice samples file that con-
Table 1 sets out the average results obtained
for Fusion-0 considering several combinations
of the classifiers and employing the three fu-
sion methods. As can be observed, MP is the
best fusion method, with average classification
rates of 89.08% and 87.43% for the 2 and 3
emotion experiments, respectively. The best
classification rates (92.23% and 90.67%) are
obtained by employing the four classifiers,
which means that the four types of informa-
tion considered (acoustic, prosodic, lexical and
related to dialogue acts) are really useful to
enhan
ce classification rates.
ments
</bodyText>
<page confidence="0.992912">
286
</page>
<table confidence="0.999983615384615">
Fusion Classifiers 2 emot. 3 emot.
Method
AP Aco, Pro 84.15 82.46
Lex, Pro 85.04 82.71
DA, Pro 90.49 87.48
Aco, Lex, Pro 89.20 86.17
Aco, DA, Pro 90.24 88.56
DA, Lex, Pro 90.02 88.02
Aco, DA, Lex, Pro 90.49 88.32
Average 88.66 86.25
MP Aco, Pro 84.15 82.86
Lex, Pro 85.16 83.71
DA, Pro 91.49 89.78
Aco, Lex, Pro 89.17 87.91
Aco, DA, Pro 91.33 89.23
DA, Lex, Pro 90.06 87.82
Aco, DA, Lex, Pro 92.23 90.67
Average 89.08 87.43
UV Aco, Pro 88.64 85.19
Lex, Pro 86.40 83.01
DA, Pro 88.20 84.92
Aco, Lex, Pro 88.76 85.54
Aco, DA, Pro 88.91 85.89
DA, Lex, Pro 88.47 85.61
Aco, DA, Lex, Pro 89.04 87.56
Average 88.35 85.39
</table>
<tableCaption confidence="0.999848">
Table 1: Performance of Fusion-0 (results in %).
</tableCaption>
<subsectionHeader confidence="0.995546">
5.2 Performance of Fusion-1
</subsectionHeader>
<bodyText confidence="0.999646">
Table 2 shows the average results obtained
when Fusion-1 is used to combine the predic-
tions of Fusion-0. The three fusion methods are
tested in Fusion-1, with Fusion-0 employing
four combinations of these methods: AP,MP;
AP,UV; MP,UV; and AP,MP,UV. In all cases
Fusion-0 uses the four classifiers as this is the
configuration that provides the highest classifi-
cation accuracy according to the previous sec-
tion.
Comparison of both tables shows that Fu-
sion-1 clearly outperforms Fusion-0. The best
results are attained for MP, which means that
this method is preferable when the data contain
small errors (emotion predictions generated by
Fusion-0 with accuracy rates around 90%).
To find the reasons for these enhancements
we have analysed the confusion matrix of Fu-
sion-1 using MP. The study reveals that for the
2-emotion experiments this fusion stage works
very well in predicting the Non-negative cate-
gory, very slightly enhancing the classification
rate of Fusion-0 (96.58% vs. 95.93%), whereas
the classification rate of the Negative category
is the same as that obtained by Fusion-0
(88.91%). Overall, the best performance of
Fusion-1 employing MP (94.48%) outdoes that
of Fusion-0 employing AP (90.49%) and MP
(92.23%).
Regarding the 3-emotion experiments, our
analysis shows that using MP, Fusion-1
slightly lowers the classification rate of the
Neutral category obtained by Fusion-0
(97.79% vs. 97.9%), but slightly raises the rate
of the Tired category (93.62% vs. 93.26%),
and the Angry category (77.49% vs. 76.81%).
Overall, the performance of Fusion-1 employ-
ing MP (94.02%) outdoes that of Fusion-0 em-
ploying AP (88.32%) and MP (90.67%).
</bodyText>
<table confidence="0.998390888888889">
Fusion methods Fusion method Fusion method
used in Fusion-0 used in Fusion-1 used in Fusion-1
(2 emotions) (3 emotions)
AP MP UV AP MP UV
AP,MP 93.68 94.48 93.53 91.77 94.02 90.96
AP,UV 93.20 93.23 93.20 91.65 93.13 90.10
MP,UV 93.34 94.38 93.20 91.27 93.98 89.48
AP,MP,UV 93.23 94.36 93.17 91.57 93.97 89.06
Average 93.40 94.11 93.28 91.57 93.78 89.90
</table>
<tableCaption confidence="0.998845">
Table 2: Performance of Fusion-1 (results in %).
</tableCaption>
<sectionHeader confidence="0.992874" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999968111111111">
Our experimental results show that the pro-
posed technique is useful to improve the classi-
fication rates of the standard fusion technique,
which employs just one fusion stage. Compar-
ing results in Table 1 and Table 2 we can ob-
serve that for the 2-emotion experiments, Fu-
sion-1 enhances Fusion-0 by 2.25% absolute
(from 92.23% to 94.48%), while for the 3-
emotion experiments, the improvement is
3.35% absolute (from 90.67% to 94.02%).
These improvements are obtained by employ-
ing AP and MP in Fusion-0 to combine the
emotion predictions of the four classifiers, and
using MP in Fusion-1 to combine the outputs
of Fusion-0.
The reason for these improvements is that
the double fusion process (Fusion-0 and Fu-
sion-1) allows us to benefit from the advan-
</bodyText>
<page confidence="0.983148">
287
</page>
<bodyText confidence="0.999983936170213">
tages of using different methods to combine
information. According to our results, the best
methods are AP and MP. The former allows
gaining maximally from the independent data
representation available, which are the input to
Fusion-0 (in our study, prosody, acoustics,
speech recognition errors, and dialogue acts).
The latter provides better results when the data
contain small errors, which occurs when the
predictions provided by Fusion-0 are the input
to Fusion-1.
Future work will include testing the tech-
nique employing information sources not con-
sidered in this study. The sources we have
dealt with in the experiments (prosodic, acous-
tic, lexical, and dialogue acts) are those most
commonly employed in previous studies.
However, there are also studies that suggest
using other information sources, such as speak-
ing style, subject and problem identification,
and non-verbal cues.
Another future work is to test the technique
employing other methods for classification and
information fusion. For example, it is known
that people are usually confused when they try
to determine the emotional state of a speaker,
given that the difference between some emo-
tions is not always clear. Hence, it would be
interesting to investigate the performance of
the technique employing classification algo-
rithms that deal with this vague boundary, such
as fuzzy inference methods, and using boosting
methods for improving the accuracy of the
classifiers.
Finally, in terms of application of the tech-
nique to improve the system-user interaction,
we will evaluate different dialogue manage-
ment strategies to enable the system’s adapta-
tion to negative emotional states of users (Uni-
versity students). For example, a dialogue
management strategy could be as follows: i) if
the emotional state is Tired begin the following
prompt apologising, and transfer the call to a
human operator if this state is recognised twice
consecutively, and ii) if the emotional state is
Angry apologise and transfer the call to a hu-
man operator immediately.
</bodyText>
<sectionHeader confidence="0.998757" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.903453">
This research has been funded by Spanish pro-
ject HADA TIN2007-64718, and the Czech
Grant Agency project no. 102/08/0707.
</bodyText>
<sectionHeader confidence="0.977616" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999954632653061">
Ai, H., Litman, D. J., Forbes-Riley, K., Rotaru, M.,
Tetreault, J., Purandare, A. 2006. Using system
and user performance features to improve emo-
tion detection in spoken tutoring systems. Proc.
of Interspeech, pp. 797-800.
Batliner, A., Fischer, K., Huber, R., Spilker, J.,
Nöth, E. 2003. How to find trouble in communi-
cation. Speech Communication, vol. 40, pp. 117-
143.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educat. Psychol. Measurement,
vol. 20, pp. 37-46.
Dellaert, F., Polzin, T., Waibel, A. 1996. Recogniz-
ing emotion in speech. Proc. of ICSLP, pp.
1970-1973.
Devillers, L., Vidrascu, L. 2006. Real-life emotions
detection with lexical and paralinguistic cues on
human-human call center dialogs. Proc. of Inter-
speech, pp. 801-804.
Landis, J. R., Koch, G. G. 1977. The measurement
of observer agreement for categorical data. Bio-
metrics, vol. 33, pp. 159-174.
Lee, C. M., Narayanan, S. S. 2005. Toward detect-
ing emotions in spoken dialogs. IEEE Transac-
tions on Speech and Audio Processing, vol.
13(2), pp. 293-303.
Liscombe, J., Riccardi, G., Hakkani-Tizr, D. 2005.
Using context to improve emotion detection in
spoken dialogue systems. Proc. of Interspeech,
pp. 1845-1848.
López-Cózar, R., García, P., Díaz, J., Rubio, A. J.
1997. A voice activated dialog system for fast-
food restaurant applications. Proc. of Eu-
rospeech, pp. 1783-1786.
López-Cózar, R., Callejas, Z. 2006. Combining
Language Models in the Input Interface of a
Spoken Dialogue System. Computer Speech and
Language, 20, pp. 420-440.
Luengo, I., Navas, E., Hernáez, I, Sanchez, J. 2005.
Automatic emotion recognition using prosodic
parameters. Proc. of Interspeech, pp.493-496.
Morrison, D., Wang, R., De Silva, L. C. 2007. En-
semble methods for spoken emotion recognition
in call-centres. Speech Communication, vol.
49(2) pp. 98-112.
Nwe, T. L., Foo, S. V., De Silva, L. C. 2003.
Speech emotion recognition using hidden
Markov models. Speech Communication, vol.
41(4), pp. 603-623.
</reference>
<page confidence="0.99713">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.163872">
<title confidence="0.99495">New Technique for Recognition of User Emotional States Spoken Dialogue Systems</title>
<author confidence="0.75637">Ramn</author>
<affiliation confidence="0.87620125">Dept. of Languages Systems, UGR, University of Spain</affiliation>
<email confidence="0.714846">rlopezc@ugr.es</email>
<author confidence="0.992378">Jan</author>
<affiliation confidence="0.978489333333333">Institute of Information Technology and Technical University</affiliation>
<address confidence="0.989529">Liberec, Czech Republic</address>
<email confidence="0.983313">jan.silovsky@tul.cz</email>
<author confidence="0.757">David</author>
<affiliation confidence="0.999135">Dept. of Computer Carlos III University</affiliation>
<address confidence="0.981237">Madrid, Spain</address>
<email confidence="0.923033">dgriol@inf.uc3m.es</email>
<abstract confidence="0.989566571428571">In this paper we propose a new technique to enhance emotion recognition by combining different ways what we call pre- The technique is called as the combination is based on a double fusion process. The input to the first fusion phase is the output of a number of classifiers which deal with different types of information regarding each sentence uttered by the user. The output of this process is the input to the second fusion stage, which provides as output the most likely emotional category. Experiments have been carried out using a previously-developed spoken dialogue system designed for the fast food domain. Results obtained considering three and two emotional categories show that our technique outperforms the standard single fusion technique by 2.25% and 3.35% absolute, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>D J Litman</author>
<author>K Forbes-Riley</author>
<author>M Rotaru</author>
<author>J Tetreault</author>
<author>A Purandare</author>
</authors>
<title>Using system and user performance features to improve emotion detection in spoken tutoring systems.</title>
<date>2006</date>
<booktitle>Proc. of Interspeech,</booktitle>
<pages>797--800</pages>
<contexts>
<context position="4655" citStr="Ai et al. (2006)" startWordPosition="708" endWordPosition="711">made experiments with a corpus of 5,690 dialogues collected with the “How May I Help You” system, and considered seven emotional categories: positive/neutral, somewhat frustrated, very frustrated, somewhat angry, very angry, somewhat other negative, and very other negative. They employed standard lexical, prosodic and contextual features. Devillers and Vidrascu (2006) employed human-to-human dialogues on a financial task, and considered four emotional categories: anger, fear, relief and sadness. Emotion classification was carried out considering linguistic information and paralinguistic cues. Ai et al. (2006) used a database collected from 100 dialogues between 20 students and a spoken dialogue tutor, and for classification employed lexical items, prosody, user gender, beginning and ending time of turns, user turns in the dialogue, and system/user performance features. Four emotional categories were considered: uncertain, certain, mixed and neutral. Morrison et al. (2007) compared two emotional speech data sources The former was collected from a call-centre in which customers talked directly to a customer service representative. The second database was collected from 12 non-professional actors and</context>
</contexts>
<marker>Ai, Litman, Forbes-Riley, Rotaru, Tetreault, Purandare, 2006</marker>
<rawString>Ai, H., Litman, D. J., Forbes-Riley, K., Rotaru, M., Tetreault, J., Purandare, A. 2006. Using system and user performance features to improve emotion detection in spoken tutoring systems. Proc. of Interspeech, pp. 797-800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>K Fischer</author>
<author>R Huber</author>
<author>J Spilker</author>
<author>E Nöth</author>
</authors>
<title>How to find trouble in communication.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<volume>40</volume>
<pages>117--143</pages>
<contexts>
<context position="3205" citStr="Batliner et al. (2003)" startWordPosition="498" endWordPosition="501">ction 4 discusses our speech database and its emotional annotation. Section 5 presents the experiments, comparing results obtained using the standard single fusion technique with the proposed double fusion. Finally, Section 6 presents the conclusions and outlines possibilities for future work. 2 Related work Many studies can be found in the literature addressing potential improvements to SDSs by recognising user emotional states. A diversity of speech databases, features used for training and recognition, number of emotional categories, and recognition methods have been proposed. For example, Batliner et al. (2003) employed three different databases to detect troubles in communication. One was collected from a single experienced actor who was told to express anger because of system malfunctions. Other was collected from naive speakers who were asked to read neutral and emotional sentences. The third database was collected using a WOZ scenario designed to deliberately provoke user reactions to system malfunctions. The study focused on detecting two emotion categories: emotional (e.g. anger) and neutral, employing classifiers that dealt with prosodic, linguistic, and discourse information. Proceedings of </context>
<context position="14068" citStr="Batliner et al. 2003" startWordPosition="2306" endWordPosition="2309">uced by the classifier, h, is the one with highest activation value ak: h = arg max(ak ) k To compute the probabilities pi‘s for the emotion prediction, we use the following expression: S pi = ai / ∑ aj j=1 where ai represents the activation value of hi, and the aj’s are the activation values of the S emotional categories considered. 3.1.4 Dialogue acts classifier A dialogue act can be defined as the function performed by an utterance within the context of a dialogue, for example, greeting, closing, suggestion, rejection, repeat, rephrase, confirmation, specification, disambiguation, or help (Batliner et al. 2003; Lee and Narayanan, 2005; Liscombe et al. 2005). Our dialogue acts classifier is inspired by the study of Liscombe et al. (2005), where the sequential structure of each dialogue is modelled by a sequence of dialogue acts. A difference is that they assigned one or more labels related to dialogue acts to each user utterance, and did not assign labels to system prompts, whereas we assign just one label to each system prompt and none to user utterances. This decision is made from the examination of our dialogue corpus. We have observed that users got tired or angry if the system generated the sam</context>
</contexts>
<marker>Batliner, Fischer, Huber, Spilker, Nöth, 2003</marker>
<rawString>Batliner, A., Fischer, K., Huber, R., Spilker, J., Nöth, E. 2003. How to find trouble in communication. Speech Communication, vol. 40, pp. 117-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educat. Psychol. Measurement,</journal>
<volume>20</volume>
<pages>37--46</pages>
<contexts>
<context position="20819" citStr="Cohen, 1960" startWordPosition="3584" endWordPosition="3585"> discourse context. The labellers have initially assigned one label to each utterance, either &lt;NEUTRAL&gt;, &lt;TIRED&gt; or &lt;ANGRY&gt; according to the perceived emotional state of the user. One of these labels has been finally assigned to each utterance according to the majority opinion of the labellers, so that 81% of the utterances are annotated as ‘Neutral’, 9.5% as ‘Tired’ and 9.4% as ‘Angry’. This shows that the database is clearly unbalanced in terms of emotional categories. To measure the amount of agreement between the labellers we employed the Kappa statistic (K), which is computed as follows (Cohen, 1960): P(A) − P(E) 1( ) −P E where P(A) is the proportion of times that the labellers agree, and P(E) is the proportion of times we would expect the labellers to agree by chance. We obtained that K = 0.48 and K = 0.45 for male and female labellers, respectively, which according to Landis and Koch (1977) represents moderate agreement. d the three fusion methods discussed in Section 3.2. employing the former category set will be called 3-emotion experiments, whereas those employing the latter category will be called 2-emotion experiments. • The four classifiers described in Section 3.1, an tains the </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. 1960. A coefficient of agreement for nominal scales. Educat. Psychol. Measurement, vol. 20, pp. 37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Dellaert</author>
<author>T Polzin</author>
<author>A Waibel</author>
</authors>
<title>Recognizing emotion in speech.</title>
<date>1996</date>
<booktitle>Proc. of ICSLP,</booktitle>
<pages>1970--1973</pages>
<contexts>
<context position="8946" citStr="Dellaert et al. 1996" startWordPosition="1427" endWordPosition="1430">ing to the following expression: where λS represents the models for the emotional categories considered, and the max function is computed employing the EM (Expectation-Maximization) algorithm. To compute the probabilities pi for the emotion prediction of the classifier we use the following expression: S pi fli / E flk (2) k=1 where βi is the log-likelihood of hi, S is the number of emotional categories considered, and the βk’s are the log-likelihoods of these emotional categories. 3.1.2 Acoustic classifier Prosodic features are nowadays among the most popular features for emotion recognition (Dellaert et al. 1996; Luengo et al. 2005). However, several authors have evaluated other features. For example, Nwe et al. (2003) employed several short-term spectral features and observed that Logarithmic Frequency Power Coefficients (LFPCs) provide better performance than Mel-Frequency Cepstral Coefficient (MFCCs) or Linear Prediction Cepstral Coefficients (LPCCs). Experiments carried out with our speech database (which will be discussed in Section 4) have confirmed this observation. However, we have also noted that when we used the first and second derivatives, the best results were obtained for MFCCs. Hence, </context>
</contexts>
<marker>Dellaert, Polzin, Waibel, 1996</marker>
<rawString>Dellaert, F., Polzin, T., Waibel, A. 1996. Recognizing emotion in speech. Proc. of ICSLP, pp. 1970-1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Devillers</author>
<author>L Vidrascu</author>
</authors>
<title>Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs.</title>
<date>2006</date>
<booktitle>Proc. of Interspeech,</booktitle>
<pages>801--804</pages>
<contexts>
<context position="4409" citStr="Devillers and Vidrascu (2006)" startWordPosition="674" endWordPosition="677">ormation. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 281–288, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 281 Liscombe et al. (2005) made experiments with a corpus of 5,690 dialogues collected with the “How May I Help You” system, and considered seven emotional categories: positive/neutral, somewhat frustrated, very frustrated, somewhat angry, very angry, somewhat other negative, and very other negative. They employed standard lexical, prosodic and contextual features. Devillers and Vidrascu (2006) employed human-to-human dialogues on a financial task, and considered four emotional categories: anger, fear, relief and sadness. Emotion classification was carried out considering linguistic information and paralinguistic cues. Ai et al. (2006) used a database collected from 100 dialogues between 20 students and a spoken dialogue tutor, and for classification employed lexical items, prosody, user gender, beginning and ending time of turns, user turns in the dialogue, and system/user performance features. Four emotional categories were considered: uncertain, certain, mixed and neutral. Morris</context>
</contexts>
<marker>Devillers, Vidrascu, 2006</marker>
<rawString>Devillers, L., Vidrascu, L. 2006. Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs. Proc. of Interspeech, pp. 801-804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<pages>159--174</pages>
<contexts>
<context position="21118" citStr="Landis and Koch (1977)" startWordPosition="3640" endWordPosition="3643">llers, so that 81% of the utterances are annotated as ‘Neutral’, 9.5% as ‘Tired’ and 9.4% as ‘Angry’. This shows that the database is clearly unbalanced in terms of emotional categories. To measure the amount of agreement between the labellers we employed the Kappa statistic (K), which is computed as follows (Cohen, 1960): P(A) − P(E) 1( ) −P E where P(A) is the proportion of times that the labellers agree, and P(E) is the proportion of times we would expect the labellers to agree by chance. We obtained that K = 0.48 and K = 0.45 for male and female labellers, respectively, which according to Landis and Koch (1977) represents moderate agreement. d the three fusion methods discussed in Section 3.2. employing the former category set will be called 3-emotion experiments, whereas those employing the latter category will be called 2-emotion experiments. • The four classifiers described in Section 3.1, an tains the response to the prompt, and the result provided by the speech recogniser (sentence in text format). The type of each prompt is used to create a sequence of dialogue acts of length L, which is the input to the dialogue acts classifier. The voice samples file is the input to the prosodic and acoustic</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis, J. R., Koch, G. G. 1977. The measurement of observer agreement for categorical data. Biometrics, vol. 33, pp. 159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S S Narayanan</author>
</authors>
<title>Toward detecting emotions in spoken dialogs.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>293--303</pages>
<contexts>
<context position="10550" citStr="Lee and Narayanan, 2005" startWordPosition="1680" endWordPosition="1683">re vectors in x, and compute the log-likelihood for an emotional category λ as follows: The emotional category deduced by the classifier, h, is decided employing Eq. (1), whereas Eq. (2) is used to compute the probabilities for the prediction, i.e. for the vector of pairs (hi, pi). 3.1.3 Lexical classifier A number of previous studies on emotion recognition take into account information about the kinds of word uttered by the users, assuming that there is a relationship between words and emotion categories. For example, swear words and insults can be considered as conveying a negative emotion (Lee and Narayanan, 2005). Analysis of our dialogue corpus (which will be discussed in Section 4) has shown that users did not utter swear words or insults during the interaction with the Saplen system. Nevertheless, there were particular moments in the interaction at which their emotional state changed from Neutral to Tired or Angry. These moments correspond to dialogue states where the system had problems in recognising the sentences uttered by the users. The reasons for these problems are basically two. On the one hand, most users spoke with strong southern Spanish accents, characterised by the deletion of the fina</context>
<context position="14093" citStr="Lee and Narayanan, 2005" startWordPosition="2310" endWordPosition="2313">, h, is the one with highest activation value ak: h = arg max(ak ) k To compute the probabilities pi‘s for the emotion prediction, we use the following expression: S pi = ai / ∑ aj j=1 where ai represents the activation value of hi, and the aj’s are the activation values of the S emotional categories considered. 3.1.4 Dialogue acts classifier A dialogue act can be defined as the function performed by an utterance within the context of a dialogue, for example, greeting, closing, suggestion, rejection, repeat, rephrase, confirmation, specification, disambiguation, or help (Batliner et al. 2003; Lee and Narayanan, 2005; Liscombe et al. 2005). Our dialogue acts classifier is inspired by the study of Liscombe et al. (2005), where the sequential structure of each dialogue is modelled by a sequence of dialogue acts. A difference is that they assigned one or more labels related to dialogue acts to each user utterance, and did not assign labels to system prompts, whereas we assign just one label to each system prompt and none to user utterances. This decision is made from the examination of our dialogue corpus. We have observed that users got tired or angry if the system generated the same prompt repeatedly (i.e.</context>
</contexts>
<marker>Lee, Narayanan, 2005</marker>
<rawString>Lee, C. M., Narayanan, S. S. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, vol. 13(2), pp. 293-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liscombe</author>
<author>G Riccardi</author>
<author>D Hakkani-Tizr</author>
</authors>
<title>Using context to improve emotion detection in spoken dialogue systems.</title>
<date>2005</date>
<booktitle>Proc. of Interspeech,</booktitle>
<pages>1845--1848</pages>
<contexts>
<context position="4038" citStr="Liscombe et al. (2005)" startWordPosition="620" endWordPosition="623">aive speakers who were asked to read neutral and emotional sentences. The third database was collected using a WOZ scenario designed to deliberately provoke user reactions to system malfunctions. The study focused on detecting two emotion categories: emotional (e.g. anger) and neutral, employing classifiers that dealt with prosodic, linguistic, and discourse information. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 281–288, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 281 Liscombe et al. (2005) made experiments with a corpus of 5,690 dialogues collected with the “How May I Help You” system, and considered seven emotional categories: positive/neutral, somewhat frustrated, very frustrated, somewhat angry, very angry, somewhat other negative, and very other negative. They employed standard lexical, prosodic and contextual features. Devillers and Vidrascu (2006) employed human-to-human dialogues on a financial task, and considered four emotional categories: anger, fear, relief and sadness. Emotion classification was carried out considering linguistic information and paralinguistic cues.</context>
<context position="14116" citStr="Liscombe et al. 2005" startWordPosition="2314" endWordPosition="2317">est activation value ak: h = arg max(ak ) k To compute the probabilities pi‘s for the emotion prediction, we use the following expression: S pi = ai / ∑ aj j=1 where ai represents the activation value of hi, and the aj’s are the activation values of the S emotional categories considered. 3.1.4 Dialogue acts classifier A dialogue act can be defined as the function performed by an utterance within the context of a dialogue, for example, greeting, closing, suggestion, rejection, repeat, rephrase, confirmation, specification, disambiguation, or help (Batliner et al. 2003; Lee and Narayanan, 2005; Liscombe et al. 2005). Our dialogue acts classifier is inspired by the study of Liscombe et al. (2005), where the sequential structure of each dialogue is modelled by a sequence of dialogue acts. A difference is that they assigned one or more labels related to dialogue acts to each user utterance, and did not assign labels to system prompts, whereas we assign just one label to each system prompt and none to user utterances. This decision is made from the examination of our dialogue corpus. We have observed that users got tired or angry if the system generated the same prompt repeatedly (i.e. repeated the same dial</context>
</contexts>
<marker>Liscombe, Riccardi, Hakkani-Tizr, 2005</marker>
<rawString>Liscombe, J., Riccardi, G., Hakkani-Tizr, D. 2005. Using context to improve emotion detection in spoken dialogue systems. Proc. of Interspeech, pp. 1845-1848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R López-Cózar</author>
<author>P García</author>
<author>J Díaz</author>
<author>A J Rubio</author>
</authors>
<title>A voice activated dialog system for fastfood restaurant applications.</title>
<date>1997</date>
<booktitle>Proc. of Eurospeech,</booktitle>
<pages>1783--1786</pages>
<contexts>
<context position="19499" citStr="López-Cózar et al. 1997" startWordPosition="3359" endWordPosition="3362"> speech database has been constructed from a corpus of 440 telephone-based dialogues between students of the University of Granada and the Saplen system, which was h x |, ... , ) 1 xm = f x i m k=1 m 1 k i ∑ ) k ( ( P 1 k ) m P( ∏ hi |x / P( hi) / ( ) P h i &apos; ) m x = , 1 x, . . . ) P(hi | 1 m  &apos; k ∑ ∏  i &apos; k &apos; k P(hi&apos; |x m m m m X if ∑Xj ≥ ∑ Yj and ∑ X j ≥ ∑ Zj j=1 j=1 j=1 j=1 m m m m Y if ∑Yj ≥ ∑ X j and ∑Yj ≥ ∑ Zj j=1 j=1 j=1 j=1 m m m m Z if ∑Zj ≥ ∑Xi and ∑Zj ≥ ∑ Yj j=1 j=1 j=1 j=1         = hi  ( P . . . x1 , ( P 285 previously developed in our lab for the fast food domain (López-Cózar et al. 1997; LópezCózar and Callejas, 2006). Each dialogue was stored in a log file in text format that includes each system prompt (e.g. Would you like to drink anything?), the type of prompt (e.g. AnyFoodOrDrinkToOrder?), the name of the voice samples file (utterance) that stores the user response to the prompt, and the speech recognition result for the utterance. The dialogue corpus contains 7,923 utterances, 50.3% of which were recorded by male users and the remaining by female users. The utterances have been annotated by 4 labellers (2 male and 2 female). The order of the utterances has been randoml</context>
</contexts>
<marker>López-Cózar, García, Díaz, Rubio, 1997</marker>
<rawString>López-Cózar, R., García, P., Díaz, J., Rubio, A. J. 1997. A voice activated dialog system for fastfood restaurant applications. Proc. of Eurospeech, pp. 1783-1786.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R López-Cózar</author>
<author>Z Callejas</author>
</authors>
<title>Combining Language Models in the Input Interface of a Spoken Dialogue System.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<pages>420--440</pages>
<marker>López-Cózar, Callejas, 2006</marker>
<rawString>López-Cózar, R., Callejas, Z. 2006. Combining Language Models in the Input Interface of a Spoken Dialogue System. Computer Speech and Language, 20, pp. 420-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Luengo</author>
<author>E Navas</author>
<author>I Hernáez</author>
<author>J Sanchez</author>
</authors>
<title>Automatic emotion recognition using prosodic parameters.</title>
<date>2005</date>
<booktitle>Proc. of Interspeech,</booktitle>
<pages>493--496</pages>
<contexts>
<context position="8967" citStr="Luengo et al. 2005" startWordPosition="1431" endWordPosition="1434">xpression: where λS represents the models for the emotional categories considered, and the max function is computed employing the EM (Expectation-Maximization) algorithm. To compute the probabilities pi for the emotion prediction of the classifier we use the following expression: S pi fli / E flk (2) k=1 where βi is the log-likelihood of hi, S is the number of emotional categories considered, and the βk’s are the log-likelihoods of these emotional categories. 3.1.2 Acoustic classifier Prosodic features are nowadays among the most popular features for emotion recognition (Dellaert et al. 1996; Luengo et al. 2005). However, several authors have evaluated other features. For example, Nwe et al. (2003) employed several short-term spectral features and observed that Logarithmic Frequency Power Coefficients (LFPCs) provide better performance than Mel-Frequency Cepstral Coefficient (MFCCs) or Linear Prediction Cepstral Coefficients (LPCCs). Experiments carried out with our speech database (which will be discussed in Section 4) have confirmed this observation. However, we have also noted that when we used the first and second derivatives, the best results were obtained for MFCCs. Hence, we decided to use 39-</context>
</contexts>
<marker>Luengo, Navas, Hernáez, Sanchez, 2005</marker>
<rawString>Luengo, I., Navas, E., Hernáez, I, Sanchez, J. 2005. Automatic emotion recognition using prosodic parameters. Proc. of Interspeech, pp.493-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Morrison</author>
<author>R Wang</author>
<author>L C De Silva</author>
</authors>
<title>Ensemble methods for spoken emotion recognition in call-centres.</title>
<date>2007</date>
<journal>Speech Communication,</journal>
<volume>49</volume>
<issue>2</issue>
<pages>98--112</pages>
<marker>Morrison, Wang, De Silva, 2007</marker>
<rawString>Morrison, D., Wang, R., De Silva, L. C. 2007. Ensemble methods for spoken emotion recognition in call-centres. Speech Communication, vol. 49(2) pp. 98-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Nwe</author>
<author>S V Foo</author>
<author>L C De Silva</author>
</authors>
<title>Speech emotion recognition using hidden Markov models.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<volume>41</volume>
<issue>4</issue>
<pages>603--623</pages>
<marker>Nwe, Foo, De Silva, 2003</marker>
<rawString>Nwe, T. L., Foo, S. V., De Silva, L. C. 2003. Speech emotion recognition using hidden Markov models. Speech Communication, vol. 41(4), pp. 603-623.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>