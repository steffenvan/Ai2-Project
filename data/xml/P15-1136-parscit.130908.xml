<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.99239">
Entity-Centric Coreference Resolution with Model Stacking
</title>
<author confidence="0.994957">
Kevin Clark
</author>
<affiliation confidence="0.9873095">
Computer Science Department
Stanford University
</affiliation>
<email confidence="0.988434">
kevclark@cs.stanford.edu
</email>
<author confidence="0.992972">
Christopher D. Manning
</author>
<affiliation confidence="0.9868795">
Computer Science Department
Stanford University
</affiliation>
<email confidence="0.995573">
manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.997349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931263157895">
Mention pair models that predict whether
or not two mentions are coreferent have
historically been very effective for coref-
erence resolution, but do not make use
of entity-level information. However, we
show that the scores produced by such
models can be aggregated to define pow-
erful entity-level features between clusters
of mentions. Using these features, we
train an entity-centric coreference system
that learns an effective policy for building
up coreference chains incrementally. The
mention pair scores are also used to prune
the search space the system works in, al-
lowing for efficient training with an exact
loss function. We evaluate our system on
the English portion of the 2012 CoNLL
Shared Task dataset and show that it im-
proves over the current state of the art.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956694915254">
Coreference resolution, the task of identifying
mentions in a text that refer to the same real world
entity, is an important aspect of text understanding
and has numerous applications. Many approaches
to coreference resolution learn a scoring function
defined over mention pairs to guide the corefer-
ence decisions (Soon et al., 2001; Ng and Cardie,
2002; Bengtson and Roth, 2008). However, such
systems do not make use of entity-level informa-
tion, i.e., features between clusters of mentions in-
stead of pairs.
Using entity-level information is valuable be-
cause it allows early coreference decisions to in-
form later ones. For example, finding that Clin-
ton and she corefer makes it more likely that Clin-
ton corefers with Hillary Clinton than Bill Clin-
ton due to gender agreement constraints. Such in-
formation has been incorporated successfully into
entity-centric coreference systems that build up
coreference clusters incrementally, using the in-
formation from the partially completed corefer-
ence chains produced so far to guide later deci-
sions (Raghunathan et al., 2010; Stoyanov and
Eisner, 2012; Ma et al., 2014).
However, defining useful features between clus-
ters of mentions and learning an effective policy
for incrementally building up clusters can be chal-
lenging, and many recent state-of-the-art systems
work entirely or almost entirely over pairs of men-
tions (Fernandes et al., 2012; Durrett and Klein,
2013; Chang et al., 2013). In this paper we in-
troduce a novel coreference system that combines
the advantages of mention pair and entity-centric
systems with model stacking. We first propose
two mention pair models designed to capture dif-
ferent linguistic phenomena in coreference resolu-
tion. We then describe how the probabilities pro-
duced by these models can be used to generate
expressive features between clusters of mentions.
Using these features, we train an entity-centric in-
cremental coreference system.
The entity-centric system builds up coreference
chains with agglomerative clustering: each men-
tion starts in its own cluster and then pairs of clus-
ters are merged each step. We train an agent to
determine whether it is desirable to merge a par-
ticular pair of clusters using an imitation learning
algorithm based on DAgger (Ross et al., 2011).
Previous incremental coreference systems heuris-
tically define which actions are beneficial for the
agent to perform, but we instead propose a way
of assigning exact costs to actions based on coref-
erence evaluation metrics, adding a concept of
the severity of a mistake. Furthermore, rather
than considering all pairs of clusters as candidate
merges, we use the scores of the pairwise mod-
els to reduce the search space, first by providing
an ordering over which merges are considered and
secondly by discarding merges that are not likely
</bodyText>
<page confidence="0.92702">
1405
</page>
<note confidence="0.975880333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1405–1415,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999801433333333">
to be good. This greatly reduces the time it takes
to run the agent, making learning computationally
feasible.
Imitation learning is challenging because it is
a non-i.i.d. learning problem; the distribution of
states seen by the agent depends on the agent’s pa-
rameters. Model stacking offers a way of decom-
posing the learning problem by training pairwise
models with many parameters in a straightforward
supervised learning setting and using their outputs
for training a much simpler model in the more
difficult imitation learning setting. Furthermore,
mention pair scores can produce powerful features
for training the agent because the scores indicate
which mention pairs between the clusters in ques-
tion are relevant; high scoring and low scoring
pairs can indicate when a merge should be forced
or disallowed while other mention pairs may pro-
vide little useful information.
We run experiments on the English portion of
the 2012 CoNLL Shared Task dataset. The entity-
centric clustering algorithm greatly outperforms
commonly used heuristic methods for coordinat-
ing pairwise scores to produce a coreference par-
tition. We also show that combining the scores
of different pairwise models designed to capture
different aspects coreference results in significant
gains in accuracy. Our final system gets a com-
bined score of 63.02 on the dataset, substantially
outperforming other state of the art systems.
</bodyText>
<sectionHeader confidence="0.988515" genericHeader="method">
2 Mention Pair Models
</sectionHeader>
<bodyText confidence="0.999847136363636">
Mention pair models predict whether or not a
given pair of mentions belong in the same coref-
erence cluster. We incorporate two different men-
tion pair models into our system. However, other
pairwise models could easily be added; one advan-
tage of our model stacking approach is that it can
combine different simple classifiers in a modular
way.
Our two models are designed to capture dif-
ferent aspects of coreference. The first one is
built to predict coreference for all of the candi-
date antecedents of a mention. This makes it use-
ful for providing scores when the current mention
has clear coreference links to many previous men-
tions. For example President Clinton might be
linked to the president, Bill Clinton, and Mr. Pres-
ident.
However, mentions often only have one clear
antecedent. This is especially common in pronom-
inal anaphora resolution, such as in the sentence
Bill arrived, but nobody saw him. The pronoun
him is directly referring back to a previous part of
the discourse, not some entity that other mentions
may also refer to. However, there still might be
coreference links between him and previous men-
tions in the text because of transitivity: any other
mention about Bill would be coreferent with him.
For such mentions, there may be very little ev-
idence in the discourse to suggest a coreference
link, so attempting to train a model to predict these
will bear little fruit. With this as motivation, we
also train a model to predict only one correct an-
tecedent of the current mention.
We found a classification model to be well
suited for the first task and and a ranking model to
be well suited for the second one. These two mod-
els differ only in the training criteria used. Both
models use a logistic classifier to assign a proba-
bility to a mention m and candidate antecedent a
representing the likelihood that the two mentions
are coreferent. The candidate antecedent a may
take on the value NA indicating that m has no an-
tecedent. The probability of coreference takes the
standard logistic form:
</bodyText>
<equation confidence="0.668933">
pO(a, m) = (1 + eOl f(a,m))−1
</equation>
<bodyText confidence="0.999979909090909">
where f(a, m) is a vector of feature functions on
a and m and 0 are the feature weights we wish
to learn. Let M denote the set of all mentions
in the training set, T (m) denote the set of true an-
tecedents of a mention m (i.e., mentions that occur
before m in the text that are coreferent with m or
{NA} if m has no antecedent), and F(m) denote
the set of false antecedents of m. We want to find
a parameter vector 0 that assigns high probabili-
ties to the candidate antecedents in T (m) and low
probabilities to the ones in F(m).
</bodyText>
<subsectionHeader confidence="0.912453">
2.1 Classification Model
</subsectionHeader>
<bodyText confidence="0.993667333333333">
For the classification model, we consider each pair
of mentions independently with the goal of pre-
dicting coreference correctly for as many of them
as possible. The model is trained by minimiz-
ing negative conditional log likelihood augmented
with L1 regularization:
</bodyText>
<equation confidence="0.9579868">
Lc(0c) = − E � E log pO.(t, m)
mEM tET (m)
E )
+ log(1 − pO.(f, m)) + A||0c||1
fEF(m)
</equation>
<page confidence="0.957606">
1406
</page>
<bodyText confidence="0.999464333333333">
By summing over all candidate antecedents, the
objective encourages the model to produce good
probabilities for all of them.
</bodyText>
<subsectionHeader confidence="0.996088">
2.2 Ranking Model
</subsectionHeader>
<bodyText confidence="0.999792136363637">
For the ranking model, candidate antecedents for a
mention are considered simultaneously and com-
pete with each other to be matched with the cur-
rent mention. This makes the model well suited
to the task of finding a single best antecedent for
a mention. A natural learning objective for such
a model would be a max-margin training crite-
ria that encourages separation between the highest
scoring true antecedent and highest scoring false
antecedent of the current mention. However, we
found such models to be poor at producing scores
useful for a downstream clustering model because
a max-margin objective encourages scores for true
antecedents to be high only relative to other can-
didate antecedents. It is much more beneficial
to have mention pair scores that are comparable
across different mentions as well as different can-
didate antecedents. For this reason, we instead
train the model with an objective that maximizes
the conditional log likelihood of the highest scor-
ing true and false antecedents under the logistic
model:
</bodyText>
<equation confidence="0.980582">
� (max log pθ, (t, m)
Lr(θr) = − tET (m)
mEM
+ min )
fEF(m) log(1 − pθ,(f,m)) + A||θr||1
</equation>
<bodyText confidence="0.997621">
For both models, we set A = 0.001 and opti-
mize their objectives using AdaGrad (Duchi et al.,
2011).
</bodyText>
<subsectionHeader confidence="0.972627">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.981284">
Our mention pair models use a variety of common
features for mention pair classification (for more
details see (Bengtson and Roth, 2008; Stoyanov et
al., 2010; Lee et al., 2011; Recasens et al., 2013)).
These include
</bodyText>
<listItem confidence="0.98873075">
• Distance features, e.g., the distance between
the two mentions in sentences or number of
mentions.
• Syntactic features, e.g., number of embed-
ded NPs under a mention, POS tags of the
first, last, and head word.
• Semantic features, e.g., named entity type,
speaker identification.
• Rule-based features, e.g., exact and partial
string matching.
• Lexical Features, e.g., the first, last, and
head word of the current mention.
</listItem>
<bodyText confidence="0.998958333333333">
We also employ a feature conjunction scheme sim-
ilar to the one described by Durrett and Klein
(2013).
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="method">
3 Entity-Centric Coreference Model
</sectionHeader>
<bodyText confidence="0.9999695">
Mention pair scores alone are not enough to pro-
duce a final set of coreference clusters because
they do not enforce transitivity: if the pair of men-
tions (a, b) and the pair of mentions (b, c) are
deemed coreferent by the model, there is no guar-
antee that the model will also classify (a, c) as
coreferent. Thus a second step is needed to co-
ordinate the scores to produce a final coreference
partition. A widely used approach for this is best-
first clustering (Ng and Cardie, 2002). For each
mention, the best-first algorithm assigns the most
probable preceding mention classified as corefer-
ent with it as the antecedent.
The primary weakness of this approach is that it
only relies on local information to make decisions,
so it cannot consolidate information at the entity
level. As a result, coreference chains produced by
such algorithms can exhibit low coherency. For
example, a cluster may consist of [Hillary Clin-
ton, Clinton, he] because the coreference decision
between Hillary Clinton and Clinton is made in-
dependently of the one between Clinton and he.
To tackle this problem, we build an entity-
centric model that operates between pairs of clus-
ters instead of pairs of mentions, guided by scores
produced by the pairwise models. It builds up
clusters of mentions believed to refer to the same
entity as it goes, relying on the partially formed
clusters produced so far to make decisions. For
example, the system could reject linking [Hillary
Clinton] with [Clinton, he] because of the low
score between the pair (Hillary Clinton, he).
Our entity-centric “agent” builds up coreference
chains with agglomerative clustering. It begins
in a start state where each mention is in a sepa-
rate single-element cluster. At each step, it ob-
serves the current state s, which consists of all par-
tially formed coreference clusters produced so far,
and selects some action a which merges two exist-
ing clusters. The action will result in a new state
with new candidate actions and the process is re-
peated. The model is entity-centric in that it builds
</bodyText>
<page confidence="0.966474">
1407
</page>
<bodyText confidence="0.999452666666667">
up clusters of mentions representing entities and
merges clusters if it predicts they are representing
the same one.
</bodyText>
<subsectionHeader confidence="0.997127">
3.1 Test-time Inference
</subsectionHeader>
<bodyText confidence="0.999987655172414">
The agent assigns a score to each action a using a
linear model with feature function fe and weight
vector θe: sθe(a) = θTe fe(a). A particular setting
of θe defines a policy 7r that determines which ac-
tion a = 7r(s) the agent will take in state s. This
policy is to greedily take highest scoring candidate
action available from the current state.
Rather than using all possible cluster merges as
the candidate set of actions the agent selects from,
we use the scores produced by mention pair mod-
els to reduce the search space. First, we order all
mention pairs in the document in descending or-
der according to their pairwise scores. This causes
clustering to occur in an easy-first fashion, where
harder decisions are delayed until more informa-
tion is available. Secondly, we discard all men-
tion pairs that score below a threshold t under
the assumption that the clusters containing these
pairs are unlikely to be coreferent. In our experi-
ments we were able able set t so that over 95% of
pairs were removed with no decrease in accuracy.
Lastly, we iterate through this list of pairs in or-
der. For each pair, we make a binary decision on
whether or not the clusters containing these pairs
should be merged. This formulates the agent’s task
so it only has two actions to chose from instead of
a number of actions proportional to the number of
clusters squared. Algorithm 1 shows the full test-
time procedure.
</bodyText>
<subsectionHeader confidence="0.9876425">
3.2 Learning
Imitation Learning with DAgger
</subsectionHeader>
<bodyText confidence="0.989870772727273">
We face a sequential prediction problem where fu-
ture observations (visited states) depend on previ-
ous actions. This is challenging because it violates
the common i.i.d. assumptions made in statistical
learning. Imitation learning, where expert demon-
strations of good behavior are used to teach the
agent, has proven very useful in practice for this
sort of problem (Argall et al., 2009). We use imita-
tion learning to set the parameters θe of our agent
by training it to classify whether a particular action
is the one an expert policy would take in the cur-
rent state. In particular, we use θe as parameters
for a binary logistic classifier that predicts which
action (merge or do not merge) matches the expert
policy.
Algorithm 1 Inference method: agglomerative
clustering
Input: Set of mentions in document M, pair-
wise classifier with parameters θc, agent with
parameters θe, cutoff threshold t
Output: Clustering C
Initialize list of mention pairs P —* []
</bodyText>
<equation confidence="0.795575166666667">
for each pair (mi, mj) E M2 with i &lt; j do
if pθ,(mi, mj) &gt; t then
P.append((mi, mj))
end if
end for
Sort P in descending order according to pθ,
</equation>
<bodyText confidence="0.495688333333333">
Initialize C —* initial clustering with each men-
tion in M in its own cluster
for (mi, mj) E P do
</bodyText>
<construct confidence="0.815278666666667">
if C[mi] =� C[mj]
and sθe(C[mi], C[mj]) &gt; 0 then
DoMerge(C[mi], C[mj], C)
</construct>
<listItem confidence="0.3005395">
end if
end for
</listItem>
<bodyText confidence="0.999978">
We found the DAgger (Ross et al., 2011) imita-
tion learning method (see Algorithm 2) to be effec-
tive for this task. DAgger is an iterative algorithm
that aggregates a dataset D consisting of states and
the actions performed by the expert policy in those
states. At each iteration, it first samples a trajec-
tory of states visited by the current policy by run-
ning the policy to completion from the start state.
It then labels those states with the best action ac-
cording to the expert policy, adds those labeled ex-
amples to the dataset, and then trains a new clas-
sifier over the dataset to get a new policy. When
producing a trajectory to train on, the expert pol-
icy is stochastically mixed with the current policy;
with probability Qi the expert’s action is chosen in-
stead of the current policy’s. We set Q so it decays
exponentially as the iteration number increases.
By sampling trajectories under the current
policy, DAgger exposes the system to states at
train time similar to the ones it will face at test
time. In contrast, training the agent on the gold
labels alone would unrealistically teach it to make
decisions under the assumption that all previous
decisions were correct, potentially causing it to
over-rely on information from past actions. This is
especially problematic in coreference, where the
error rate is quite high. Even when using DAgger,
</bodyText>
<page confidence="0.969075">
1408
</page>
<bodyText confidence="0.9995498">
this problem could exist to a lesser degree if the
model heavily overfits to the training data. How-
ever, the agent has a small number of parameters
thanks to our model stacking approach, reducing
the risk of this happening.
</bodyText>
<figure confidence="0.839354928571429">
Algorithm 2 Learning method: DAgger
Input: initial policy ˆπ1, expert policy π∗
Output: final policy ˆπN
Initialize D ← ∅
for i = 1 to N do
Let πi = βiπ∗ + (1 − βi)ˆπi
Sample a trajectory under the current policy
using πi
Get dataset Di = (s, π∗(s)) of states visited
by πi and actions given by the expert
Aggregate datasets: D ← D U Di
Train classifier ˆπi+1 on D
end for
Assigning Costs to Actions
</figure>
<bodyText confidence="0.999497">
A key aspect of incrementally building corefer-
ence clusters is that some local decisions are much
more important than others. For example, a merge
between two large clusters influences the score far
more than a merge between two small ones. Ad-
ditionally, getting early decisions correct is crucial
because later actions are dependent on early ones,
causing errors to compound if mistakes are made
early. To capture this, we take an approach in-
spired by the SEARN learning algorithm (Daum´e
et al., 2009) and add costs to the actions in the
aggregated dataset. We then train the agent to do
cost-sensitive classification. Using these costs, we
simply define the expert policy as the policy that
takes the action with the lowest cost at each step.
We want our costs to represent how a partic-
ular local decision will affect the final score of
the coreference system. Unfortunately, standard
coreference evaluation metrics do not decompose
over cluster merges. Instead, we compute the loss
of an action by “rolling out” the current policy to
completion. More concretely, let m be a function
(such as a coreference evaluation metric) that as-
signs scores to states; we are interested in reach-
ing a final state for which m is high. Suppose we
are assigning costs to the set of actions A(s) that
can be taken from some state s. For each action
a E A(s), we apply that action to s to get a new
state s0, run the current policy ˆπi from s0 to com-
pletion, and then compute the value of m on the
resulting final state. This gives exactly the final
score the system would get if it made the action a
from state s and then continued under the current
policy. Let fm(s, a) denote this value for a par-
ticular metric, state, and action. We assign each
action the regret r associated with taking that ac-
tion under the current policy as a cost:
</bodyText>
<equation confidence="0.9647845">
r(s, a) = max fm(s, a0) − fm(s, a)
a�∈A(s)
</equation>
<bodyText confidence="0.9999535">
The “rolling out” procedure means we naively
have to visit O(t2) states each iteration instead of
t, where t is the length of a trajectory. However,
the highly constrained action space described in
section 3.1 combined with the use of memoization
allows the algorithm to still run efficiently.
</bodyText>
<subsectionHeader confidence="0.958857">
Improving Runtime with Memoization
</subsectionHeader>
<bodyText confidence="0.9995352">
During training, the agent will see many of the
same states and actions multiple times. We can ex-
ploit this with memoization, significantly improv-
ing the algorithm’s runtime. In particular, we store
the following values:
</bodyText>
<listItem confidence="0.964265">
• Given a state s and action a, the value of the
cost function, r(s, a).
• Given an action a, the score the model as-
signs that action, sθe(a).
• Given an action a, the result of the feature
function on that action, fe(a).
</listItem>
<bodyText confidence="0.999864166666667">
The first two values depend on the current model,
so the saved values must be cleared between iter-
ations of training. In experiments on the develop-
ment set of the CoNLL 2012 corpus, these tables
had 76%, 94%, and 93% hit rates respectively af-
ter 50 passes over the dataset.
</bodyText>
<subsectionHeader confidence="0.932909">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999988416666667">
Our agent uses features that are derived from the
scores produced by the two mention pair mod-
els. Although these scores only operate on men-
tion pairs, they are combined to capture cluster-
level interactions by being aggregated in differ-
ent ways over pairs of mentions from the clus-
ters. Mention pair scores can produce powerful
features for training the agent because they show
which mention pairs between the clusters in ques-
tion are relevant, and often a small subset of the
mention pairs provide far more information than
the rest. For example, a strong negative pairwise
</bodyText>
<page confidence="0.995065">
1409
</page>
<figureCaption confidence="0.9836935">
Figure 1: Examples of features generated for a candidate cluster merge. Weights on edges are the
probabilities of coreference produced by a mention pair model.
</figureCaption>
<bodyText confidence="0.99190975">
link like Hillary Clinton and he should disallow
a merge, while other mention pairs, such as two
instances of the pronoun she far apart in the text,
might provide very little information. Using the
mention pair models for probabilities, we compute
the following features over all pairs of mentions
between the clusters (i.e., each mention is in a dif-
ferent cluster).
</bodyText>
<listItem confidence="0.944648142857143">
• The minimum and maximum probability of
coreference.
• The average probability and average log
probability of coreference.
• The average probability and log proba-
bility of coreference for a particular pair
of grammatical types of mentions (either
</listItem>
<bodyText confidence="0.99887775">
pronoun or non-pronoun). For exam-
ple, Avg-Prob non-pronoun pronoun
gives the average probability of coreference
when the candidate antecedent is not a pro-
noun and the candidate anaphor is a pronoun.
Note that the averaged features have a natural
probabilistic interpretation; the average probabil-
ity corresponds to the expected number of coref-
erence links between the involved mention pairs
while the average log probability corresponds to
the probability that all mention pairs will have a
coreference link. All of these features are com-
puted twice: once with the classification model
and once with the ranking model.
We also compute the following features based
on other aspects of the current state:
</bodyText>
<listItem confidence="0.994863666666667">
• Whether a preceding mention pair in the
list of mention pairs has the same candidate
anaphor as the current one.
• The index of the current mention pair in the
list divided by the size of the list, i.e., what
percentage of the list have we seen so far.
• The number of mentions in the current docu-
ment.
• The probability of the first-occurring men-
</listItem>
<bodyText confidence="0.983846916666667">
tion in the second-occurring cluster not be-
ing anaphoric (i.e., pθ,(NA, m)). This pre-
vents producing clusters that, for example,
start with a pronoun.
Lastly, we take one feature conjunction with
a boolean representing whether both clusters are
size 1. In total, there are only 56 features af-
ter the feature conjunction. However, these fea-
tures provide strong signal because they are di-
rectly related to the probabilities of mentions be-
ing coreferent. In contrast, the pairwise models
use thousands of features (after feature conjunc-
tions), including lexical features that are extremely
sparse. The pairwise models can easily exploit
this much bigger feature set because they oper-
ate in a classic supervised learning setting. The
entity-centric model, on the other hand, learns in
a much more challenging non-i.i.d. setting. Model
stacking avoids the difficulty of directly training
the entity-centric model with a large set of weak
features by decomposing the task into first learn-
ing to produce good pairwise scores and then us-
ing those scores to generate a manageable set of
strong features.
</bodyText>
<subsectionHeader confidence="0.983451">
3.4 Training Details
</subsectionHeader>
<bodyText confidence="0.999752666666667">
Because the entity-centric agent relies on the out-
put of pairwise classifiers, they should not be
trained on the same data. Therefore we split the
</bodyText>
<page confidence="0.975438">
1410
</page>
<bodyText confidence="0.99999225">
training set into two sections and use one for train-
ing the pairwise models and the other for training
the agent. When evaluating on the development
set, we use 80% of the documents in the training
set to train the mention pair models and the rest
to train the entity-centric model. When evaluating
on the test set we use the whole training set for
the mention pair models and the development set
for the entity-centric model. We also tried using
cross-validation instead of a single split, but found
this did not improve performance, which we be-
lieve to be because this trains the agent with dif-
ferent pairwise models than the ones used at test
time.
For our initial policy ˆπ1, we set the parame-
ters of the agent so it operates with simple best-
first clustering (initializing all feature weights to
0 except for the maximum-score, anaphor-seen,
and bias features). For m, the performance met-
ric determining the action costs, we use a linear
combination of the B3 (Bagga and Baldwin, 1998)
and MUC (Vilain et al., 1995) metrics, which are
both commonly used for evaluating coreference
systems. The other metric used in our evaluation,
Entity-based CEAFE (CEAFφ4) (Luo, 2005), was
not used because it is expensive to compute. We
found weighting B3 three times as much as MUC
to be effective on the development set.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.987945">
Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999963125">
We apply our model to the English portion of
the CoNLL 2012 Shared Task data (Pradhan et
al., 2012), which is derived from the OntoNotes
corpus (Hovy et al., 2006). The data is split into
a training set of 2802 documents, development
set of 343 documents, and a test set of 345
documents. We use the provided preprocessing
for parse trees, named entity tags, etc. The models
are evaluated using three of the most popular
metrics for coreference resolution: MUC, B3, and
Entity-based CEAFE (CEAFφ4). We also include
the average F1 score (CoNLL F1) of these three
metrics, as is commonly done in CoNLL Shared
Tasks. We used the most recent version of the
CoNLL scorer (version 8.01), which implements
the original definitions of these metrics.
</bodyText>
<subsectionHeader confidence="0.707386">
Mention Detection
</subsectionHeader>
<bodyText confidence="0.986085">
Our experiments were run using system-produced
predicted mentions. We used the rule-based
</bodyText>
<table confidence="0.999649166666667">
MUC B3 CEAFφ4 Avg.
Classification, B.F. 72.00 60.01 55.63 62.55
Ranking, B.F. 71.91 60.63 56.38 62.97
Classification, E.C. 72.34 61.46 57.16 63.65
Ranking, E.C. 72.37 61.34 57.13 63.61
Both, E.C. 72.52 62.02 57.69 64.08
</table>
<tableCaption confidence="0.679814">
Table 1: Metric scores on the development set
for the classification and ranking pairwise mod-
els when using best-first clustering (B.F.) or the
entity-centric model (E.C.).
</tableCaption>
<bodyText confidence="0.997643166666667">
mention detection algorithm from Raghunathan
et al. (2010), which first extracts pronouns and
maximal NP projections as candidate mentions
and then filters this set with rules that remove
spurious mentions such as numeric entities or
pleonastic it pronouns.
</bodyText>
<subsectionHeader confidence="0.471005">
Comparison of Models
</subsectionHeader>
<bodyText confidence="0.998875033333333">
We compare the effectiveness of the entity-centric
model with the commonly used best-first cluster-
ing approach, which assigns mentions the high-
est scoring previous mention as the antecedent.
Unlike the entity-centric model, the best-first ap-
proach only relies on local information to make
decisions. We also compare the effectiveness of
the ranking and classification pairwise models.
Table 1 shows the results of these models on the
development set.
The entity-centric model outperforms best-
first clustering for both mention pair models,
demonstrating the utility of a learned, incremental
clustering algorithm. The improvement is much
greater for the classification pairwise model, caus-
ing it to outperform the ranking model with the
entity-centric clustering algorithm even though
it performs significantly worse than the ranking
model with best-first clustering. This suggests
that although the ranking model is better at finding
a single correct antecedent for a mention, the
classification model is more useful for producing
cluster-level features. Incorporating probabilities
from both pairwise models further improved
scores over using either model alone, indicating
that the mention pair classifiers were successful
in learning scoring functions useful in different
circumstances.
Incorporating other Entity-Level Features
Although the entity-centric model has so far only
</bodyText>
<page confidence="0.962278">
1411
</page>
<table confidence="0.999622666666667">
MUC B3 CEAFφ4 Avg.
Scores Only 72.52 62.02 57.69 64.08
+Agreement 72.59 61.98 57.58 64.05
</table>
<tableCaption confidence="0.987657">
Table 2: Metric scores on the development set for
</tableCaption>
<bodyText confidence="0.983712">
the entity-centric model with and without the ad-
dition of entity-level agreement features.
used features derived from the scores produced
by mention pair models, other entity-level features
could easily be added. We experiment with this by
adding four cluster-level agreement features based
on gender, number, animacy, and named entity
type. Each of these features can take on three val-
ues: “same” (e.g., both clusters have gender value
feminine), “compatible” (e.g., one cluster has gen-
der value feminine while the other has value un-
known), or “incompatible” (one cluster has gender
value feminine while the other has value mascu-
line). The cluster-level value for a particular fea-
ture is the most common value among mentions in
that cluster (e.g., if a cluster has 2 masculine men-
tions, 1 feminine mention, and 1 unknown men-
tion) the value is considered masculine. Table 2
shows the results.
Adding the additional features had no substan-
tial impact on scores, suggesting that features de-
rived from pairwise scores are sufficient for cap-
turing this kind of entity-level information. A
disagreement between clusters necessarily means
there will be disagreements between some of the
involved mentions, so features like the average and
minimum probability between mention pairs will
have lower values when a disagreement is present.
</bodyText>
<subsectionHeader confidence="0.980735">
Final System Performance
</subsectionHeader>
<bodyText confidence="0.99996505">
In Table 3 we compare the results of our system
with the following state-of-the-art approaches: the
JOINT and INDEP models of the Berkeley sys-
tem (Durrett and Klein, 2014) (the JOINT model
jointly does NER and entity linking along with
coreference); the Prune-and-Score system (Ma et
al., 2014); the HOTCoref system (Bj¨orkelund and
Kuhn, 2014); the CPL3M sytem (Chang et al.,
2013); and Fernandes et al. We use the full entity-
centric clustering algorithm drawing upon scores
from both pairwise models. We do not make use
of agreement features, as these did not increase ac-
curacy and complicate the system. Our final model
substantially outperforms the other systems on the
CoNLL F1 score. The largest improvement is in
the B3 metric, which is unsurprising because the
entity-centric model primarily optimizes for this
during training. However, our model also achieves
the highest CEAFφ4 F1 and second highest MUC
F1 scores among the other systems.
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999502113636364">
Both mention pair (Soon et al., 2001; Ng and
Cardie, 2002; Bengtson and Roth, 2008; Stoyanov
et al., 2010; Bj¨orkelund and Farkas, 2012) and
mention ranking models (Denis and Baldridge,
2007b; Rahman and Ng, 2009) have been widely
used for coreference resolution, and there have
been many proposed ways of post-processing the
pairwise scores to make predictions. Despite
their simplicity, closest-first clustering (Soon et
al., 2001) and best-first clustering (Ng and Cardie,
2002) are arguably the most widely used of these
approaches. Other work uses global inference
with integer linear programming to enforce tran-
sitivity (Denis and Baldridge, 2007a; Finkel and
Manning, 2008), graph partitioning algorithms
(McCallum and Wellner, 2005; Nicolae and Nico-
lae, 2006), the Dempster-Shafer rule (Kehler,
1997; Bean and Riloff, 2004), or correlational
clustering (McCallum and Wellner, 2003; Finley
and Joachims, 2005). In contrast to these methods,
our entity-centric model directly learns how to use
pairwise scores to produce a coreference partition
that scores highly according to an evaluation met-
ric, and can use the outputs of more than one men-
tion pair model.
Recently, coreference models using latent an-
tecedents have gained in popularity and achieved
state-of-the-art results (Fernandes et al., 2012;
Durrett and Klein, 2013; Chang et al., 2013;
Bj¨orkelund and Kuhn, 2014). These learn a scor-
ing function over mention pairs, but are trained to
maximize a global objective function instead of
pairwise accuracy. Unlike in our system, these
methods typically consider one pair of mentions
at a time during inference.
Several works have explored using non-local
entity-level features in mention-entity models that
assign a single mention to a (partially completed)
cluster (Luo et al., 2004; Yang et al., 2008; Rah-
man and Ng, 2011). Our system, however, builds
clusters incrementally through merge operations,
and so can operate in an easy-first fashion. Raghu-
nathan et al. (2010) take this approach with a
rule-based system that runs in multiple passes
</bodyText>
<page confidence="0.974803">
1412
</page>
<table confidence="0.986768888888889">
MUC F1 Prec. B3 F1 CEAFφ4 CoNLL
Prec. Rec. Rec. Prec. Rec. F1 Avg. F1
Fernandes et al. 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65
Chang et al. - - 69.48 - - 57.44 - - 53.07 60.00
Bj¨orkelund &amp; Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63
Ma et al. 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56
Durrett &amp; Klein (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23
Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71
This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02
</table>
<tableCaption confidence="0.999974">
Table 3: Comparison of this work with other state-of-the-art approaches on the test set.
</tableCaption>
<bodyText confidence="0.999939720930233">
and Stoyanov and Eisner (2012) train a classi-
fier to do this with a structured perceptron algo-
rithm. Entity-level information has also been suc-
cessfully incorporated in coreference systems us-
ing joint inference (McCallum and Wellner, 2003;
Culotta et al., 2006; Poon and Domingos, 2008;
Haghighi and Klein, 2010), but these approaches
do not directly learn parameters tuned so the sys-
tem runs effectively at test time, while our imita-
tion learning approach does.
Imitation learning has been employed to train
coreference resolvers on trajectories of decisions
similar to those that would be seen at test-time by
Daum´e et al. (2005) and Ma et al. (2014). Other
works use structured perceptron models for the
same purpose (Stoyanov and Eisner, 2012; Fer-
nandes et al., 2012; Bj¨orkelund and Kuhn, 2014).
These systems all heuristically determine which
actions are desirable for the system to perform.
In contrast, our approach directly computes a cost
for actions based on coreference evaluation met-
rics. This means our system directly learns which
actions lead to good clusterings instead of which
look good locally according to a heuristic. Fur-
thermore, the costs provide our system a measure
of the severity of a mistake, which we argue is very
beneficial for the coreference task.
Our model stacking approach further distin-
guishes this work by providing a new way of defin-
ing cluster-level features. The majority of useful
features for coreference systems operate on pairs
of mentions (in one of our experiments we show
the addition of classic entity-level features does
not improve our system), but incremental corefer-
ence systems must make decisions involving many
mention pairs. Other incremental coreference sys-
tems either incorporate features from a single pair
(Stoyanov and Eisner, 2012) or average features
across all pairs in the involved clusters (Ma et
al., 2014). Our system instead combines informa-
tion from the involved mention pairs in a variety
of ways with with higher order features produced
from the scores of mention pair models.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999992473684211">
We introduced a new approach to coreference res-
olution that trains an entity-centric system using
the scores produced by mention pair models as
features. The brunt of task-specific learning oc-
curs within the mention pair models, which are
trained in a straightforward supervised manner.
Guided by the pairwise scores, our entity-centric
agent then learns an effective procedure for build-
ing up coreference clusters incrementally, using
previous decisions to inform later ones. The agent
benefits from using multiple mention pair mod-
els designed to capture different aspects of coref-
erence. Experiments show that the agent, which
learns how to coordinate mention pair scores, out-
performs the commonly used best-first method.
We evaluate our final system on the English por-
tion of the CoNLL 2012 Shared Task and report a
significant improvement over the current state of
the art.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999985181818182">
We thank the anonymous reviewers for their
thoughtful comments. Stanford University grate-
fully acknowledges the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Deep
Exploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL)
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
</bodyText>
<page confidence="0.984073">
1413
</page>
<sectionHeader confidence="0.996089" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999090942857143">
Brenna D Argall, Sonia Chernova, Manuela Veloso,
and Brett Browning. 2009. A survey of robot learn-
ing from demonstration. Robotics and Autonomous
Systems, 57(5):469–483.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.
David L Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Human Language Technology
and North American Association for Computational
Linguistics (HLT-NAACL), pages 297–304.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 294–303.
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Conference on Computational Natu-
ral Language Learning - Shared Task, pages 49–55.
Anders Bj¨orkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference resolution
with latent antecedents and non-local features. In
Association of Computational Linguistics (ACL).
Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model for coref-
erence resolution. In Empirical Methods in Natural
Language Processing (EMNLP), pages 601–612.
Aron Culotta, Michael Wick, Robert Hall, and An-
drew McCallum. 2006. First-order probabilistic
models for coreference resolution. In Human Lan-
guage Technology and North American Association
for Computational Linguistics (HLT-NAACL), pages
81–88.
Hal Daum´e III and Daniel Marcu. 2005. A large-
scale exploration of effective global features for a
joint entity detection and tracking model. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 97–104.
Hal Daum´e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297–325.
Pascal Denis and Jason Baldridge. 2007a. Joint de-
termination of anaphoricity and coreference resolu-
tion using integer programming. In Human Lan-
guage Technology and North American Association
for Computational Linguistics (HLT-NAACL), pages
236–243.
Pascal Denis and Jason Baldridge. 2007b. A rank-
ing approach to pronoun resolution. In International
Joint Conferences on Artificial Intelligence (IJCAI),
pages 1588–1593.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.
Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1971–1982.
Greg Durrett and Dan Klein. 2014. A joint model
for entity analysis: Coreference, typing, and linking.
Transactions of the Association for Computational
Linguistics (TACL), 2:477–490.
Eraldo Rezende Fernandes, C´ıcero Nogueira Dos San-
tos, and Ruy Luiz Milidi´u. 2012. Latent structure
perceptron with feature induction for unrestricted
coreference resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Conference on Computa-
tional Natural Language Learning - Shared Task,
pages 41–48.
Jenny Rose Finkel and Christopher D Manning. 2008.
Enforcing transitivity in coreference resolution. In
Association for Computational Linguistics (ACL),
Short Paper, pages 45–48.
Thomas Finley and Thorsten Joachims. 2005. Super-
vised clustering with support vector machines. In
Proceedings of the 22nd international conference on
Machine learning, pages 217–224.
Aria Haghighi and Dan Klein. 2010. Coreference
resolution in a modular, entity-centered model. In
Human Language Technology and North American
Association for Computational Linguistics (HLT-
NAACL), pages 385–393.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Human Language Technology
and North American Association for Computational
Linguistics (HLT-NAACL), pages 57–60.
Andrew Kehler. 1997. Probabilistic coreference in in-
formation extraction. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 163–
173.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 28–34.
</reference>
<page confidence="0.871633">
1414
</page>
<reference confidence="0.999832483516483">
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Association for Computational
Linguistics (ACL), page 135.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Empirical Methods in Natural
Language Processing (EMNLP), pages 25–32.
Chao Ma, Janardhan Rao Doppa, J Walker Orr,
Prashanth Mannem, Xiaoli Fern, Tom Dietterich,
and Prasad Tadepalli. 2014. Prune-and-score:
Learning for greedy coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Andrew McCallum and Ben Wellner. 2003. Toward
conditional models of identity uncertainty with ap-
plication to proper noun coreference. In Proceed-
ings of the IJCAI Workshop on Information Integra-
tion on the Web.
Andrew McCallum and Ben Wellner. 2005. Condi-
tional models of identity uncertainty with applica-
tion to noun coreference. In Advances in Neural
Information Processing Systems (NIPS), pages 905–
912.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Association of Computational Linguistics (ACL),
pages 104–111.
Cristina Nicolae and Gabriel Nicolae. 2006. Bestcut:
A graph algorithm for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 275–283.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 650–659.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Conference on Com-
putational Natural Language Learning - Shared
Task, pages 1–40.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A
multi-pass sieve for coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 492–501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Empirical Methods
in Natural Language Processing (EMNLP), pages
968–977.
Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: a cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research (JAIR), pages 469–521.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Human Language Technology and North American
Association for Computational Linguistics (HLT-
NAACL), pages 627–633.
St´ephane Ross, Geoffrey J Gordon, and J Andrew Bag-
nell. 2011. A reduction of imitation learning and
structured prediction to no-regret online learning.
In Artificial Intelligence and Statistics (AISTATS),
pages 627–633.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In COLING, pages 2519–
2534.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Reconcile: A coreference resolution research plat-
form. Computer Science Technical Report, Cornell
University, Ithaca, NY.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th conference on Message understand-
ing, pages 45–52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-mention
model for coreference resolution with inductive
logic programming. In Association of Computa-
tional Linguistics (ACL), pages 843–851.
</reference>
<page confidence="0.992954">
1415
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445300">
<title confidence="0.999893">Entity-Centric Coreference Resolution with Model Stacking</title>
<author confidence="0.990479">Kevin</author>
<affiliation confidence="0.9093405">Computer Science Stanford</affiliation>
<email confidence="0.999742">kevclark@cs.stanford.edu</email>
<author confidence="0.99666">D Christopher</author>
<affiliation confidence="0.9577465">Computer Science Stanford</affiliation>
<email confidence="0.999666">manning@cs.stanford.edu</email>
<abstract confidence="0.999559235294118">Mention pair models that predict whether or not two mentions are coreferent have historically been very effective for coreference resolution, but do not make use of entity-level information. However, we show that the scores produced by such models can be aggregated to define powerful entity-level features between clusters of mentions. Using these features, we train an entity-centric coreference system that learns an effective policy for building up coreference chains incrementally. The mention pair scores are also used to prune the search space the system works in, allowing for efficient training with an exact loss function. We evaluate our system on</abstract>
<note confidence="0.688191666666667">the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brenna D Argall</author>
<author>Sonia Chernova</author>
<author>Manuela Veloso</author>
<author>Brett Browning</author>
</authors>
<title>A survey of robot learning from demonstration. Robotics and Autonomous Systems,</title>
<date>2009</date>
<pages>57--5</pages>
<contexts>
<context position="14679" citStr="Argall et al., 2009" startWordPosition="2420" endWordPosition="2423">lates the agent’s task so it only has two actions to chose from instead of a number of actions proportional to the number of clusters squared. Algorithm 1 shows the full testtime procedure. 3.2 Learning Imitation Learning with DAgger We face a sequential prediction problem where future observations (visited states) depend on previous actions. This is challenging because it violates the common i.i.d. assumptions made in statistical learning. Imitation learning, where expert demonstrations of good behavior are used to teach the agent, has proven very useful in practice for this sort of problem (Argall et al., 2009). We use imitation learning to set the parameters θe of our agent by training it to classify whether a particular action is the one an expert policy would take in the current state. In particular, we use θe as parameters for a binary logistic classifier that predicts which action (merge or do not merge) matches the expert policy. Algorithm 1 Inference method: agglomerative clustering Input: Set of mentions in document M, pairwise classifier with parameters θc, agent with parameters θe, cutoff threshold t Output: Clustering C Initialize list of mention pairs P —* [] for each pair (mi, mj) E M2 </context>
</contexts>
<marker>Argall, Chernova, Veloso, Browning, 2009</marker>
<rawString>Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. 2009. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5):469–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="25161" citStr="Bagga and Baldwin, 1998" startWordPosition="4234" endWordPosition="4237"> models and the development set for the entity-centric model. We also tried using cross-validation instead of a single split, but found this did not improve performance, which we believe to be because this trains the agent with different pairwise models than the ones used at test time. For our initial policy ˆπ1, we set the parameters of the agent so it operates with simple bestfirst clustering (initializing all feature weights to 0 except for the maximum-score, anaphor-seen, and bias features). For m, the performance metric determining the action costs, we use a linear combination of the B3 (Bagga and Baldwin, 1998) and MUC (Vilain et al., 1995) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAFφ4) (Luo, 2005), was not used because it is expensive to compute. We found weighting B3 three times as much as MUC to be effective on the development set. 4 Experiments and Results Experimental Setup We apply our model to the English portion of the CoNLL 2012 Shared Task data (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The data is split into a training set of 2802 documents, developm</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>297--304</pages>
<contexts>
<context position="31730" citStr="Bean and Riloff, 2004" startWordPosition="5247" endWordPosition="5250">009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pair</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>David L Bean and Ellen Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL), pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>294--303</pages>
<contexts>
<context position="1419" citStr="Bengtson and Roth, 2008" startWordPosition="208" endWordPosition="211">ce the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art. 1 Introduction Coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. Many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the coreference decisions (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). However, such systems do not make use of entity-level information, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially comple</context>
<context position="10004" citStr="Bengtson and Roth, 2008" startWordPosition="1629" endWordPosition="1632">pair scores that are comparable across different mentions as well as different candidate antecedents. For this reason, we instead train the model with an objective that maximizes the conditional log likelihood of the highest scoring true and false antecedents under the logistic model: � (max log pθ, (t, m) Lr(θr) = − tET (m) mEM + min ) fEF(m) log(1 − pθ,(f,m)) + A||θr||1 For both models, we set A = 0.001 and optimize their objectives using AdaGrad (Duchi et al., 2011). 2.3 Features Our mention pair models use a variety of common features for mention pair classification (for more details see (Bengtson and Roth, 2008; Stoyanov et al., 2010; Lee et al., 2011; Recasens et al., 2013)). These include • Distance features, e.g., the distance between the two mentions in sentences or number of mentions. • Syntactic features, e.g., number of embedded NPs under a mention, POS tags of the first, last, and head word. • Semantic features, e.g., named entity type, speaker identification. • Rule-based features, e.g., exact and partial string matching. • Lexical Features, e.g., the first, last, and head word of the current mention. We also employ a feature conjunction scheme similar to the one described by Durrett and Kl</context>
<context position="30981" citStr="Bengtson and Roth, 2008" startWordPosition="5138" endWordPosition="5141">titycentric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), </context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP), pages 294–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Datadriven multilingual coreference resolution using resolver stacking.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning - Shared Task,</booktitle>
<pages>49--55</pages>
<marker>Bj¨orkelund, Farkas, 2012</marker>
<rawString>Anders Bj¨orkelund and Rich´ard Farkas. 2012. Datadriven multilingual coreference resolution using resolver stacking. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning - Shared Task, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
</authors>
<title>Learning structured perceptrons for coreference resolution with latent antecedents and non-local features.</title>
<date>2014</date>
<booktitle>In Association of Computational Linguistics (ACL).</booktitle>
<marker>Bj¨orkelund, Kuhn, 2014</marker>
<rawString>Anders Bj¨orkelund and Jonas Kuhn. 2014. Learning structured perceptrons for coreference resolution with latent antecedents and non-local features. In Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Rajhans Samdani</author>
<author>Dan Roth</author>
</authors>
<title>A constrained latent variable model for coreference resolution.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>601--612</pages>
<contexts>
<context position="2476" citStr="Chang et al., 2013" startWordPosition="373" endWordPosition="376">ncorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference resolution. We then describe how the probabilities produced by these models can be used to generate expressive features between clusters of mentions. Using these features, we train an entity-centric incremental coreference system. The entity-centric system builds up coreference chains with agglomerative clustering: each mention starts in its </context>
<context position="30317" citStr="Chang et al., 2013" startWordPosition="5029" endWordPosition="5032">essarily means there will be disagreements between some of the involved mentions, so features like the average and minimum probability between mention pairs will have lower values when a disagreement is present. Final System Performance In Table 3 we compare the results of our system with the following state-of-the-art approaches: the JOINT and INDEP models of the Berkeley system (Durrett and Klein, 2014) (the JOINT model jointly does NER and entity linking along with coreference); the Prune-and-Score system (Ma et al., 2014); the HOTCoref system (Bj¨orkelund and Kuhn, 2014); the CPL3M sytem (Chang et al., 2013); and Fernandes et al. We use the full entitycentric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair </context>
<context position="32251" citStr="Chang et al., 2013" startWordPosition="5327" endWordPosition="5330"> 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first </context>
</contexts>
<marker>Chang, Samdani, Roth, 2013</marker>
<rawString>Kai-Wei Chang, Rajhans Samdani, and Dan Roth. 2013. A constrained latent variable model for coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP), pages 601–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Robert Hall</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2006</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="33895" citStr="Culotta et al., 2006" startWordPosition="5601" endWordPosition="5604">0 57.94 68.75 44.34 53.91 61.56 Durrett &amp; Klein (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23 Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71 This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (2005) and Ma et al. (2014). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014). These systems all heuristically determine which actions ar</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2006</marker>
<rawString>Aron Culotta, Michael Wick, Robert Hall, and Andrew McCallum. 2006. First-order probabilistic models for coreference resolution. In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A largescale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>97--104</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. A largescale exploration of effective global features for a joint entity detection and tracking model. In Empirical Methods in Natural Language Processing (EMNLP), pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>75</volume>
<issue>3</issue>
<marker>Langford, Marcu, 2009</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning, 75(3):297–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>236--243</pages>
<contexts>
<context position="31089" citStr="Denis and Baldridge, 2007" startWordPosition="5154" endWordPosition="5157">ement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer r</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007a. Joint determination of anaphoricity and coreference resolution using integer programming. In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL), pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>A ranking approach to pronoun resolution.</title>
<date>2007</date>
<booktitle>In International Joint Conferences on Artificial Intelligence (IJCAI),</booktitle>
<pages>1588--1593</pages>
<contexts>
<context position="31089" citStr="Denis and Baldridge, 2007" startWordPosition="5154" endWordPosition="5157">ement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer r</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007b. A ranking approach to pronoun resolution. In International Joint Conferences on Artificial Intelligence (IJCAI), pages 1588–1593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="9854" citStr="Duchi et al., 2011" startWordPosition="1605" endWordPosition="1608">ective encourages scores for true antecedents to be high only relative to other candidate antecedents. It is much more beneficial to have mention pair scores that are comparable across different mentions as well as different candidate antecedents. For this reason, we instead train the model with an objective that maximizes the conditional log likelihood of the highest scoring true and false antecedents under the logistic model: � (max log pθ, (t, m) Lr(θr) = − tET (m) mEM + min ) fEF(m) log(1 − pθ,(f,m)) + A||θr||1 For both models, we set A = 0.001 and optimize their objectives using AdaGrad (Duchi et al., 2011). 2.3 Features Our mention pair models use a variety of common features for mention pair classification (for more details see (Bengtson and Roth, 2008; Stoyanov et al., 2010; Lee et al., 2011; Recasens et al., 2013)). These include • Distance features, e.g., the distance between the two mentions in sentences or number of mentions. • Syntactic features, e.g., number of embedded NPs under a mention, POS tags of the first, last, and head word. • Semantic features, e.g., named entity type, speaker identification. • Rule-based features, e.g., exact and partial string matching. • Lexical Features, e</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1971--1982</pages>
<contexts>
<context position="2455" citStr="Durrett and Klein, 2013" startWordPosition="369" endWordPosition="372">ch information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference resolution. We then describe how the probabilities produced by these models can be used to generate expressive features between clusters of mentions. Using these features, we train an entity-centric incremental coreference system. The entity-centric system builds up coreference chains with agglomerative clustering: each m</context>
<context position="10614" citStr="Durrett and Klein (2013)" startWordPosition="1730" endWordPosition="1733">and Roth, 2008; Stoyanov et al., 2010; Lee et al., 2011; Recasens et al., 2013)). These include • Distance features, e.g., the distance between the two mentions in sentences or number of mentions. • Syntactic features, e.g., number of embedded NPs under a mention, POS tags of the first, last, and head word. • Semantic features, e.g., named entity type, speaker identification. • Rule-based features, e.g., exact and partial string matching. • Lexical Features, e.g., the first, last, and head word of the current mention. We also employ a feature conjunction scheme similar to the one described by Durrett and Klein (2013). 3 Entity-Centric Coreference Model Mention pair scores alone are not enough to produce a final set of coreference clusters because they do not enforce transitivity: if the pair of mentions (a, b) and the pair of mentions (b, c) are deemed coreferent by the model, there is no guarantee that the model will also classify (a, c) as coreferent. Thus a second step is needed to coordinate the scores to produce a final coreference partition. A widely used approach for this is bestfirst clustering (Ng and Cardie, 2002). For each mention, the best-first algorithm assigns the most probable preceding me</context>
<context position="32231" citStr="Durrett and Klein, 2013" startWordPosition="5323" endWordPosition="5326">ms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can opera</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP), pages 1971–1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>A joint model for entity analysis: Coreference, typing, and linking.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>2--477</pages>
<contexts>
<context position="30106" citStr="Durrett and Klein, 2014" startWordPosition="4996" endWordPosition="4999">e additional features had no substantial impact on scores, suggesting that features derived from pairwise scores are sufficient for capturing this kind of entity-level information. A disagreement between clusters necessarily means there will be disagreements between some of the involved mentions, so features like the average and minimum probability between mention pairs will have lower values when a disagreement is present. Final System Performance In Table 3 we compare the results of our system with the following state-of-the-art approaches: the JOINT and INDEP models of the Berkeley system (Durrett and Klein, 2014) (the JOINT model jointly does NER and entity linking along with coreference); the Prune-and-Score system (Ma et al., 2014); the HOTCoref system (Bj¨orkelund and Kuhn, 2014); the CPL3M sytem (Chang et al., 2013); and Fernandes et al. We use the full entitycentric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the en</context>
</contexts>
<marker>Durrett, Klein, 2014</marker>
<rawString>Greg Durrett and Dan Klein. 2014. A joint model for entity analysis: Coreference, typing, and linking. Transactions of the Association for Computational Linguistics (TACL), 2:477–490.</rawString>
</citation>
<citation valid="true">
<title>Eraldo Rezende Fernandes, C´ıcero Nogueira Dos Santos, and Ruy Luiz Milidi´u.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning - Shared Task,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="33665" citStr="(2012)" startWordPosition="5568" endWordPosition="5568">.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65 Chang et al. - - 69.48 - - 57.44 - - 53.07 60.00 Bj¨orkelund &amp; Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 Ma et al. 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56 Durrett &amp; Klein (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23 Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71 This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (200</context>
</contexts>
<marker>2012</marker>
<rawString>Eraldo Rezende Fernandes, C´ıcero Nogueira Dos Santos, and Ruy Luiz Milidi´u. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning - Shared Task, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Association for Computational Linguistics (ACL), Short Paper,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="31579" citStr="Finkel and Manning, 2008" startWordPosition="5226" endWordPosition="5229">02; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art resu</context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D Manning. 2008. Enforcing transitivity in coreference resolution. In Association for Computational Linguistics (ACL), Short Paper, pages 45–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Finley</author>
<author>Thorsten Joachims</author>
</authors>
<title>Supervised clustering with support vector machines.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>217--224</pages>
<contexts>
<context position="31815" citStr="Finley and Joachims, 2005" startWordPosition="5258" endWordPosition="5261">roposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accura</context>
</contexts>
<marker>Finley, Joachims, 2005</marker>
<rawString>Thomas Finley and Thorsten Joachims. 2005. Supervised clustering with support vector machines. In Proceedings of the 22nd international conference on Machine learning, pages 217–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLTNAACL),</booktitle>
<pages>385--393</pages>
<contexts>
<context position="33947" citStr="Haghighi and Klein, 2010" startWordPosition="5609" endWordPosition="5612"> (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23 Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71 This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (2005) and Ma et al. (2014). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014). These systems all heuristically determine which actions are desirable for the system to perform. In contrast, </context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technology and North American Association for Computational Linguistics (HLTNAACL), pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>57--60</pages>
<contexts>
<context position="25694" citStr="Hovy et al., 2006" startWordPosition="4324" endWordPosition="4327">ing the action costs, we use a linear combination of the B3 (Bagga and Baldwin, 1998) and MUC (Vilain et al., 1995) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAFφ4) (Luo, 2005), was not used because it is expensive to compute. We found weighting B3 three times as much as MUC to be effective on the development set. 4 Experiments and Results Experimental Setup We apply our model to the English portion of the CoNLL 2012 Shared Task data (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The data is split into a training set of 2802 documents, development set of 343 documents, and a test set of 345 documents. We use the provided preprocessing for parse trees, named entity tags, etc. The models are evaluated using three of the most popular metrics for coreference resolution: MUC, B3, and Entity-based CEAFE (CEAFφ4). We also include the average F1 score (CoNLL F1) of these three metrics, as is commonly done in CoNLL Shared Tasks. We used the most recent version of the CoNLL scorer (version 8.01), which implements the original definitions of these metrics. Mention Detection Our</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL), pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>Probabilistic coreference in information extraction.</title>
<date>1997</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>163--173</pages>
<contexts>
<context position="31706" citStr="Kehler, 1997" startWordPosition="5245" endWordPosition="5246">hman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring fu</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>Andrew Kehler. 1997. Probabilistic coreference in information extraction. In Empirical Methods in Natural Language Processing (EMNLP), pages 163– 173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<contexts>
<context position="10045" citStr="Lee et al., 2011" startWordPosition="1637" endWordPosition="1640"> mentions as well as different candidate antecedents. For this reason, we instead train the model with an objective that maximizes the conditional log likelihood of the highest scoring true and false antecedents under the logistic model: � (max log pθ, (t, m) Lr(θr) = − tET (m) mEM + min ) fEF(m) log(1 − pθ,(f,m)) + A||θr||1 For both models, we set A = 0.001 and optimize their objectives using AdaGrad (Duchi et al., 2011). 2.3 Features Our mention pair models use a variety of common features for mention pair classification (for more details see (Bengtson and Roth, 2008; Stoyanov et al., 2010; Lee et al., 2011; Recasens et al., 2013)). These include • Distance features, e.g., the distance between the two mentions in sentences or number of mentions. • Syntactic features, e.g., number of embedded NPs under a mention, POS tags of the first, last, and head word. • Semantic features, e.g., named entity type, speaker identification. • Rule-based features, e.g., exact and partial string matching. • Lexical Features, e.g., the first, last, and head word of the current mention. We also employ a feature conjunction scheme similar to the one described by Durrett and Klein (2013). 3 Entity-Centric Coreference </context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Conference on Computational Natural Language Learning: Shared Task, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>135</pages>
<contexts>
<context position="32695" citStr="Luo et al., 2004" startWordPosition="5395" endWordPosition="5398">ference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. Raghunathan et al. (2010) take this approach with a rule-based system that runs in multiple passes 1412 MUC F1 Prec. B3 F1 CEAFφ4 CoNLL Prec. Rec. Rec. Prec. Rec. F1 Avg. F1 Fernandes et al. 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65 Chang et al. - - 69.48 - - 57.44 - - 53.07 60.00 Bj¨orkelund &amp; Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 Ma et al. 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 </context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the bell tree. In Association for Computational Linguistics (ACL), page 135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="25346" citStr="Luo, 2005" startWordPosition="4264" endWordPosition="4265">e this trains the agent with different pairwise models than the ones used at test time. For our initial policy ˆπ1, we set the parameters of the agent so it operates with simple bestfirst clustering (initializing all feature weights to 0 except for the maximum-score, anaphor-seen, and bias features). For m, the performance metric determining the action costs, we use a linear combination of the B3 (Bagga and Baldwin, 1998) and MUC (Vilain et al., 1995) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAFφ4) (Luo, 2005), was not used because it is expensive to compute. We found weighting B3 three times as much as MUC to be effective on the development set. 4 Experiments and Results Experimental Setup We apply our model to the English portion of the CoNLL 2012 Shared Task data (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The data is split into a training set of 2802 documents, development set of 343 documents, and a test set of 345 documents. We use the provided preprocessing for parse trees, named entity tags, etc. The models are evaluated using three of the most po</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Empirical Methods in Natural Language Processing (EMNLP), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Ma</author>
<author>Janardhan Rao Doppa</author>
<author>J Walker Orr</author>
<author>Prashanth Mannem</author>
<author>Xiaoli Fern</author>
<author>Tom Dietterich</author>
<author>Prasad Tadepalli</author>
</authors>
<title>Prune-and-score: Learning for greedy coreference resolution.</title>
<date>2014</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2153" citStr="Ma et al., 2014" startWordPosition="323" endWordPosition="326">ad of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference resolution. We then d</context>
<context position="30229" citStr="Ma et al., 2014" startWordPosition="5015" endWordPosition="5018"> capturing this kind of entity-level information. A disagreement between clusters necessarily means there will be disagreements between some of the involved mentions, so features like the average and minimum probability between mention pairs will have lower values when a disagreement is present. Final System Performance In Table 3 we compare the results of our system with the following state-of-the-art approaches: the JOINT and INDEP models of the Berkeley system (Durrett and Klein, 2014) (the JOINT model jointly does NER and entity linking along with coreference); the Prune-and-Score system (Ma et al., 2014); the HOTCoref system (Bj¨orkelund and Kuhn, 2014); the CPL3M sytem (Chang et al., 2013); and Fernandes et al. We use the full entitycentric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and</context>
<context position="34288" citStr="Ma et al. (2014)" startWordPosition="5666" endWordPosition="5669">a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (2005) and Ma et al. (2014). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014). These systems all heuristically determine which actions are desirable for the system to perform. In contrast, our approach directly computes a cost for actions based on coreference evaluation metrics. This means our system directly learns which actions lead to good clusterings instead of which look good locally according to a heuristic. Furthermore, the costs provide our system a measure of the severity of a mistake, which we argue is very benefic</context>
<context position="35511" citStr="Ma et al., 2014" startWordPosition="5858" endWordPosition="5861"> the coreference task. Our model stacking approach further distinguishes this work by providing a new way of defining cluster-level features. The majority of useful features for coreference systems operate on pairs of mentions (in one of our experiments we show the addition of classic entity-level features does not improve our system), but incremental coreference systems must make decisions involving many mention pairs. Other incremental coreference systems either incorporate features from a single pair (Stoyanov and Eisner, 2012) or average features across all pairs in the involved clusters (Ma et al., 2014). Our system instead combines information from the involved mention pairs in a variety of ways with with higher order features produced from the scores of mention pair models. 6 Conclusion We introduced a new approach to coreference resolution that trains an entity-centric system using the scores produced by mention pair models as features. The brunt of task-specific learning occurs within the mention pair models, which are trained in a straightforward supervised manner. Guided by the pairwise scores, our entity-centric agent then learns an effective procedure for building up coreference clust</context>
</contexts>
<marker>Ma, Doppa, Orr, Mannem, Fern, Dietterich, Tadepalli, 2014</marker>
<rawString>Chao Ma, Janardhan Rao Doppa, J Walker Orr, Prashanth Mannem, Xiaoli Fern, Tom Dietterich, and Prasad Tadepalli. 2014. Prune-and-score: Learning for greedy coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proceedings of the IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="31787" citStr="McCallum and Wellner, 2003" startWordPosition="5254" endWordPosition="5257">, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective functio</context>
<context position="33873" citStr="McCallum and Wellner, 2003" startWordPosition="5597" endWordPosition="5600">81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56 Durrett &amp; Klein (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23 Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71 This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (2005) and Ma et al. (2014). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014). These systems all heuristically dete</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>Andrew McCallum and Ben Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proceedings of the IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>905--912</pages>
<contexts>
<context position="31638" citStr="McCallum and Wellner, 2005" startWordPosition="5233" endWordPosition="5236">elund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang</context>
</contexts>
<marker>McCallum, Wellner, 2005</marker>
<rawString>Andrew McCallum and Ben Wellner. 2005. Conditional models of identity uncertainty with application to noun coreference. In Advances in Neural Information Processing Systems (NIPS), pages 905– 912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Association of Computational Linguistics (ACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1393" citStr="Ng and Cardie, 2002" startWordPosition="204" endWordPosition="207"> prune the search space the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art. 1 Introduction Coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. Many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the coreference decisions (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). However, such systems do not make use of entity-level information, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information</context>
<context position="11131" citStr="Ng and Cardie, 2002" startWordPosition="1823" endWordPosition="1826">n. We also employ a feature conjunction scheme similar to the one described by Durrett and Klein (2013). 3 Entity-Centric Coreference Model Mention pair scores alone are not enough to produce a final set of coreference clusters because they do not enforce transitivity: if the pair of mentions (a, b) and the pair of mentions (b, c) are deemed coreferent by the model, there is no guarantee that the model will also classify (a, c) as coreferent. Thus a second step is needed to coordinate the scores to produce a final coreference partition. A widely used approach for this is bestfirst clustering (Ng and Cardie, 2002). For each mention, the best-first algorithm assigns the most probable preceding mention classified as coreferent with it as the antecedent. The primary weakness of this approach is that it only relies on local information to make decisions, so it cannot consolidate information at the entity level. As a result, coreference chains produced by such algorithms can exhibit low coherency. For example, a cluster may consist of [Hillary Clinton, Clinton, he] because the coreference decision between Hillary Clinton and Clinton is made independently of the one between Clinton and he. To tackle this pro</context>
<context position="30956" citStr="Ng and Cardie, 2002" startWordPosition="5134" endWordPosition="5137">l. We use the full entitycentric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Fi</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Association of Computational Linguistics (ACL), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Nicolae</author>
<author>Gabriel Nicolae</author>
</authors>
<title>Bestcut: A graph algorithm for coreference resolution.</title>
<date>2006</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>275--283</pages>
<contexts>
<context position="31666" citStr="Nicolae and Nicolae, 2006" startWordPosition="5237" endWordPosition="5241">mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund a</context>
</contexts>
<marker>Nicolae, Nicolae, 2006</marker>
<rawString>Cristina Nicolae and Gabriel Nicolae. 2006. Bestcut: A graph algorithm for coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP), pages 275–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with markov logic.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>650--659</pages>
<contexts>
<context position="33920" citStr="Poon and Domingos, 2008" startWordPosition="5605" endWordPosition="5608">.91 61.56 Durrett &amp; Klein (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23 Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71 This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (2005) and Ma et al. (2014). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014). These systems all heuristically determine which actions are desirable for the syste</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with markov logic. In Empirical Methods in Natural Language Processing (EMNLP), pages 650–659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning - Shared Task,</booktitle>
<pages>1--40</pages>
<contexts>
<context position="25630" citStr="Pradhan et al., 2012" startWordPosition="4313" endWordPosition="4316">or-seen, and bias features). For m, the performance metric determining the action costs, we use a linear combination of the B3 (Bagga and Baldwin, 1998) and MUC (Vilain et al., 1995) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAFφ4) (Luo, 2005), was not used because it is expensive to compute. We found weighting B3 three times as much as MUC to be effective on the development set. 4 Experiments and Results Experimental Setup We apply our model to the English portion of the CoNLL 2012 Shared Task data (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The data is split into a training set of 2802 documents, development set of 343 documents, and a test set of 345 documents. We use the provided preprocessing for parse trees, named entity tags, etc. The models are evaluated using three of the most popular metrics for coreference resolution: MUC, B3, and Entity-based CEAFE (CEAFφ4). We also include the average F1 score (CoNLL F1) of these three metrics, as is commonly done in CoNLL Shared Tasks. We used the most recent version of the CoNLL scorer (version 8.01), which implements </context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning - Shared Task, pages 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multi-pass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>492--501</pages>
<contexts>
<context position="2108" citStr="Raghunathan et al., 2010" startWordPosition="315" endWordPosition="318">on, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic p</context>
<context position="26832" citStr="Raghunathan et al. (2010)" startWordPosition="4502" endWordPosition="4505">.01), which implements the original definitions of these metrics. Mention Detection Our experiments were run using system-produced predicted mentions. We used the rule-based MUC B3 CEAFφ4 Avg. Classification, B.F. 72.00 60.01 55.63 62.55 Ranking, B.F. 71.91 60.63 56.38 62.97 Classification, E.C. 72.34 61.46 57.16 63.65 Ranking, E.C. 72.37 61.34 57.13 63.61 Both, E.C. 72.52 62.02 57.69 64.08 Table 1: Metric scores on the development set for the classification and ranking pairwise models when using best-first clustering (B.F.) or the entity-centric model (E.C.). mention detection algorithm from Raghunathan et al. (2010), which first extracts pronouns and maximal NP projections as candidate mentions and then filters this set with rules that remove spurious mentions such as numeric entities or pleonastic it pronouns. Comparison of Models We compare the effectiveness of the entity-centric model with the commonly used best-first clustering approach, which assigns mentions the highest scoring previous mention as the antecedent. Unlike the entity-centric model, the best-first approach only relies on local information to make decisions. We also compare the effectiveness of the ranking and classification pairwise mo</context>
<context position="32885" citStr="Raghunathan et al. (2010)" startWordPosition="5425" endWordPosition="5429">lund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. Raghunathan et al. (2010) take this approach with a rule-based system that runs in multiple passes 1412 MUC F1 Prec. B3 F1 CEAFφ4 CoNLL Prec. Rec. Rec. Prec. Rec. F1 Avg. F1 Fernandes et al. 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65 Chang et al. - - 69.48 - - 57.44 - - 53.07 60.00 Bj¨orkelund &amp; Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 Ma et al. 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56 Durrett &amp; Klein (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23 Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71 This work</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multi-pass sieve for coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP), pages 492–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>968--977</pages>
<contexts>
<context position="31112" citStr="Rahman and Ng, 2009" startWordPosition="5158" endWordPosition="5161"> not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP), pages 968–977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Narrowing the modeling gap: a cluster-ranking approach to coreference resolution.</title>
<date>2011</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>469--521</pages>
<contexts>
<context position="32736" citStr="Rahman and Ng, 2011" startWordPosition="5403" endWordPosition="5407">ts have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. Raghunathan et al. (2010) take this approach with a rule-based system that runs in multiple passes 1412 MUC F1 Prec. B3 F1 CEAFφ4 CoNLL Prec. Rec. Rec. Prec. Rec. F1 Avg. F1 Fernandes et al. 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65 Chang et al. - - 69.48 - - 57.44 - - 53.07 60.00 Bj¨orkelund &amp; Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 Ma et al. 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56 Durrett &amp; Klein (INDEP.) 72.2</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Narrowing the modeling gap: a cluster-ranking approach to coreference resolution. Journal of Artificial Intelligence Research (JAIR), pages 469–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLTNAACL),</booktitle>
<pages>627--633</pages>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In Human Language Technology and North American Association for Computational Linguistics (HLTNAACL), pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Ross</author>
<author>Geoffrey J Gordon</author>
<author>J Andrew Bagnell</author>
</authors>
<title>A reduction of imitation learning and structured prediction to no-regret online learning.</title>
<date>2011</date>
<booktitle>In Artificial Intelligence and Statistics (AISTATS),</booktitle>
<pages>627--633</pages>
<contexts>
<context position="3304" citStr="Ross et al., 2011" startWordPosition="505" endWordPosition="508">different linguistic phenomena in coreference resolution. We then describe how the probabilities produced by these models can be used to generate expressive features between clusters of mentions. Using these features, we train an entity-centric incremental coreference system. The entity-centric system builds up coreference chains with agglomerative clustering: each mention starts in its own cluster and then pairs of clusters are merged each step. We train an agent to determine whether it is desirable to merge a particular pair of clusters using an imitation learning algorithm based on DAgger (Ross et al., 2011). Previous incremental coreference systems heuristically define which actions are beneficial for the agent to perform, but we instead propose a way of assigning exact costs to actions based on coreference evaluation metrics, adding a concept of the severity of a mistake. Furthermore, rather than considering all pairs of clusters as candidate merges, we use the scores of the pairwise models to reduce the search space, first by providing an ordering over which merges are considered and secondly by discarding merges that are not likely 1405 Proceedings of the 53rd Annual Meeting of the Associatio</context>
<context position="15620" citStr="Ross et al., 2011" startWordPosition="2595" endWordPosition="2598"> policy. Algorithm 1 Inference method: agglomerative clustering Input: Set of mentions in document M, pairwise classifier with parameters θc, agent with parameters θe, cutoff threshold t Output: Clustering C Initialize list of mention pairs P —* [] for each pair (mi, mj) E M2 with i &lt; j do if pθ,(mi, mj) &gt; t then P.append((mi, mj)) end if end for Sort P in descending order according to pθ, Initialize C —* initial clustering with each mention in M in its own cluster for (mi, mj) E P do if C[mi] =� C[mj] and sθe(C[mi], C[mj]) &gt; 0 then DoMerge(C[mi], C[mj], C) end if end for We found the DAgger (Ross et al., 2011) imitation learning method (see Algorithm 2) to be effective for this task. DAgger is an iterative algorithm that aggregates a dataset D consisting of states and the actions performed by the expert policy in those states. At each iteration, it first samples a trajectory of states visited by the current policy by running the policy to completion from the start state. It then labels those states with the best action according to the expert policy, adds those labeled examples to the dataset, and then trains a new classifier over the dataset to get a new policy. When producing a trajectory to trai</context>
</contexts>
<marker>Ross, Gordon, Bagnell, 2011</marker>
<rawString>St´ephane Ross, Geoffrey J Gordon, and J Andrew Bagnell. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In Artificial Intelligence and Statistics (AISTATS), pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1372" citStr="Soon et al., 2001" startWordPosition="200" endWordPosition="203">es are also used to prune the search space the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art. 1 Introduction Coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. Many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the coreference decisions (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). However, such systems do not make use of entity-level information, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, </context>
<context position="30935" citStr="Soon et al., 2001" startWordPosition="5130" endWordPosition="5133"> and Fernandes et al. We use the full entitycentric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Jason Eisner</author>
</authors>
<title>Easy-first coreference resolution.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>2519--2534</pages>
<contexts>
<context position="2135" citStr="Stoyanov and Eisner, 2012" startWordPosition="319" endWordPosition="322"> clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference res</context>
<context position="33665" citStr="Stoyanov and Eisner (2012)" startWordPosition="5565" endWordPosition="5568">ndes et al. 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65 Chang et al. - - 69.48 - - 57.44 - - 53.07 60.00 Bj¨orkelund &amp; Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 Ma et al. 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56 Durrett &amp; Klein (INDEP.) 72.27 69.30 70.75 60.92 55.73 58.21 55.33 54.14 54.73 61.23 Durrett &amp; Klein (JOINT) 72.61 69.91 71.24 61.18 56.43 58.71 56.17 54.23 55.18 61.71 This work 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (200</context>
<context position="35431" citStr="Stoyanov and Eisner, 2012" startWordPosition="5844" endWordPosition="5847">e our system a measure of the severity of a mistake, which we argue is very beneficial for the coreference task. Our model stacking approach further distinguishes this work by providing a new way of defining cluster-level features. The majority of useful features for coreference systems operate on pairs of mentions (in one of our experiments we show the addition of classic entity-level features does not improve our system), but incremental coreference systems must make decisions involving many mention pairs. Other incremental coreference systems either incorporate features from a single pair (Stoyanov and Eisner, 2012) or average features across all pairs in the involved clusters (Ma et al., 2014). Our system instead combines information from the involved mention pairs in a variety of ways with with higher order features produced from the scores of mention pair models. 6 Conclusion We introduced a new approach to coreference resolution that trains an entity-centric system using the scores produced by mention pair models as features. The brunt of task-specific learning occurs within the mention pair models, which are trained in a straightforward supervised manner. Guided by the pairwise scores, our entity-ce</context>
</contexts>
<marker>Stoyanov, Eisner, 2012</marker>
<rawString>Veselin Stoyanov and Jason Eisner. 2012. Easy-first coreference resolution. In COLING, pages 2519– 2534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Reconcile: A coreference resolution research platform.</title>
<date>2010</date>
<journal>Computer Science</journal>
<tech>Technical Report,</tech>
<institution>Cornell University,</institution>
<location>Ithaca, NY.</location>
<contexts>
<context position="10027" citStr="Stoyanov et al., 2010" startWordPosition="1633" endWordPosition="1636">arable across different mentions as well as different candidate antecedents. For this reason, we instead train the model with an objective that maximizes the conditional log likelihood of the highest scoring true and false antecedents under the logistic model: � (max log pθ, (t, m) Lr(θr) = − tET (m) mEM + min ) fEF(m) log(1 − pθ,(f,m)) + A||θr||1 For both models, we set A = 0.001 and optimize their objectives using AdaGrad (Duchi et al., 2011). 2.3 Features Our mention pair models use a variety of common features for mention pair classification (for more details see (Bengtson and Roth, 2008; Stoyanov et al., 2010; Lee et al., 2011; Recasens et al., 2013)). These include • Distance features, e.g., the distance between the two mentions in sentences or number of mentions. • Syntactic features, e.g., number of embedded NPs under a mention, POS tags of the first, last, and head word. • Semantic features, e.g., named entity type, speaker identification. • Rule-based features, e.g., exact and partial string matching. • Lexical Features, e.g., the first, last, and head word of the current mention. We also employ a feature conjunction scheme similar to the one described by Durrett and Klein (2013). 3 Entity-Ce</context>
<context position="31004" citStr="Stoyanov et al., 2010" startWordPosition="5142" endWordPosition="5145">gorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algo</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Reconcile: A coreference resolution research platform. Computer Science Technical Report, Cornell University, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th conference on Message understanding,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="25191" citStr="Vilain et al., 1995" startWordPosition="4240" endWordPosition="4243">r the entity-centric model. We also tried using cross-validation instead of a single split, but found this did not improve performance, which we believe to be because this trains the agent with different pairwise models than the ones used at test time. For our initial policy ˆπ1, we set the parameters of the agent so it operates with simple bestfirst clustering (initializing all feature weights to 0 except for the maximum-score, anaphor-seen, and bias features). For m, the performance metric determining the action costs, we use a linear combination of the B3 (Bagga and Baldwin, 1998) and MUC (Vilain et al., 1995) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAFφ4) (Luo, 2005), was not used because it is expensive to compute. We found weighting B3 three times as much as MUC to be effective on the development set. 4 Experiments and Results Experimental Setup We apply our model to the English portion of the CoNLL 2012 Shared Task data (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The data is split into a training set of 2802 documents, development set of 343 documents, and </context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the 6th conference on Message understanding, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Jun Lang</author>
<author>Chew Lim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>An entity-mention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In Association of Computational Linguistics (ACL),</booktitle>
<pages>843--851</pages>
<contexts>
<context position="32714" citStr="Yang et al., 2008" startWordPosition="5399" endWordPosition="5402">ng latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. Raghunathan et al. (2010) take this approach with a rule-based system that runs in multiple passes 1412 MUC F1 Prec. B3 F1 CEAFφ4 CoNLL Prec. Rec. Rec. Prec. Rec. F1 Avg. F1 Fernandes et al. 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65 Chang et al. - - 69.48 - - 57.44 - - 53.07 60.00 Bj¨orkelund &amp; Kuhn 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 Ma et al. 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56 Durrett</context>
</contexts>
<marker>Yang, Su, Lang, Tan, Liu, Li, 2008</marker>
<rawString>Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting Liu, and Sheng Li. 2008. An entity-mention model for coreference resolution with inductive logic programming. In Association of Computational Linguistics (ACL), pages 843–851.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>