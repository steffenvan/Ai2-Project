<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978905">
Efficient Hierarchical Entity Classifier Using Conditional Random Fields
</title>
<author confidence="0.986263">
Koen Deschacht
</author>
<affiliation confidence="0.8791425">
Interdisciplinary Centre for Law &amp; IT
Katholieke Universiteit Leuven
</affiliation>
<address confidence="0.902875">
Tiensestraat 41, 3000 Leuven, Belgium
</address>
<email confidence="0.994663">
koen.deschacht@law.kuleuven.ac.be
</email>
<author confidence="0.880864">
Marie-Francine Moens
</author>
<affiliation confidence="0.812364">
Interdisciplinary Centre for Law &amp; IT
Katholieke Universiteit Leuven
</affiliation>
<address confidence="0.92663">
Tiensestraat 41, 3000 Leuven, Belgium
</address>
<email confidence="0.996868">
marie-france.moens@law.kuleuven.be
</email>
<sectionHeader confidence="0.99383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996590615384615">
In this paper we develop an automatic
classifier for a very large set of labels, the
WordNet synsets. We employ Conditional
Random Fields (CRFs) because of their
flexibility to include a wide variety of non-
independent features. Training CRFs on a
big number of labels proved a problem be-
cause of the large training cost. By tak-
ing into account the hypernym/hyponym
relation between synsets in WordNet, we
reduced the complexity of training from
O(TM2NG) to O(T (logM)2NG) with
only a limited loss in accuracy.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998535">
The work described in this paper was carried out
during the CLASS project1. The central objec-
tive of this project is to develop advanced learning
methods that allow images, video and associated
text to be analyzed and structured automatically.
One of the goals of the project is the alignment of
visual and textual information. We will, for exam-
ple, learn the correspondence between faces in an
image and persons described in surrounding text.
The role of the authors in the CLASS project is
mainly on information extraction from text.
In the first phase of the project we build a clas-
sifier for automatic identification and categoriza-
tion of entities in texts which we report here. This
classifier extracts entities from text, and assigns a
label to these entities chosen from an inventory
of possible labels. This task is closely related to
both named entity recognition (NER), which tra-
ditionally assigns nouns to a small number of cate-
gories and word sense disambiguation (Agirre and
</bodyText>
<footnote confidence="0.932099">
1http://class.inrialpes.fr/
</footnote>
<bodyText confidence="0.99662905882353">
Rigau, 1996; Yarowsky, 1995), where the sense
for a word is chosen from a much larger inventory
of word senses.
We will employ a probabilistic model that’s
been used successfully in NER (Conditional Ran-
dom Fields) and use this with an extensive inven-
tory of word senses (the WordNet lexical database)
to perform entity detection.
In section 2 we describe WordNet and it’s use
for entity categorization. Section 3 gives an
overview of Conditional Random Fields and sec-
tion 4 explains how the parameters of this model
are estimated during training. We will drastically
reduce the computational complexity of training in
section 5. Section 6 describes the implementation
of this method, section 7 the obtained results and
finally section 8 future work.
</bodyText>
<sectionHeader confidence="0.993257" genericHeader="introduction">
2 WordNet
</sectionHeader>
<bodyText confidence="0.999827578947368">
WordNet (Fellbaum et al., 1998) is a lexical
database whose design is inspired by psycholin-
guistic theories of human lexical memory. English
nouns, verbs, adjectives and adverbs are organized
in synsets. A synset is a collection of words that
have a close meaning and that represent an under-
lying concept. An example of such a synset is
“person, individual, someone, somebody, mortal,
soul”. All these words refer to a human being.
WordNet (v2.1) contains 155.327 words, which
are organized in 117.597 synsets. WordNet de-
fines a number of relations between synsets. For
nouns the most important relation is the hyper-
nym/hyponym relation. A noun X is a hypernym
of a noun Y if Y is a subtype or instance of X. For
example, “bird” is a hypernym of “penguin” (and
“penguin” is a hyponym of “bird”). This relation
organizes the synsets in a hierarchical tree (Hayes,
1999), of which a fragment is pictured in fig. 1.
</bodyText>
<page confidence="0.99046">
33
</page>
<note confidence="0.9750715">
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 33–40,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.969776">
Figure 1: Fragment of the hypernym/hyponym
tree
</figureCaption>
<bodyText confidence="0.999392777777778">
This tree has a depth of 18 levels and maximum
width of 17837 synsets (fig. 2).
We will build a classifier using CRFs that tags
noun phrases in a text with their WordNet synset.
This will enable us to recognize entities, and to
classify the entities in certain groups. Moreover,
it allows learning the context pattern of a certain
meaning of a word. Take for example the sentence
“The ambulance took the remains of the bomber
to the morgue.” Having every noun phrase tagged
with it’s WordNet synset reveals that in this sen-
tence, “bomber” is “a person who plants bombs”
(and not “a military aircraft that drops bombs dur-
ing flight”). Using the hypernym/hyponym rela-
tions from WordNet, we can also easily find out
that “ambulance” is a kind of “car”, which in turn
is a kind of “conveyance, transport” which in turn
is a “physical object”.
</bodyText>
<sectionHeader confidence="0.988331" genericHeader="method">
3 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.99916135">
Conditional random fields (CRFs) (Lafferty et al.,
2001; Jordan, 1999; Wallach, 2004) is a statistical
method based on undirected graphical models. Let
X be a random variable over data sequences to be
labeled and Y a random variable over correspond-
ing label sequences. All components YZ of Y are
assumed to range over a finite label alphabet K.
In this paper X will range over the sentences of
a text, tagged with POS-labels and Y ranges over
the synsets to be recognized in these sentences.
We define G = (V, E) to be an undirected
graph such that there is a node v E V correspond-
ing to each of the random variables representing an
element Y� of Y . If each random variable Y� obeys
the Markov property with respect to G (e.g., in a
first order model the transition probability depends
only on the neighboring state), then the model
(Y, X) is a Conditional Random Field. Although
the structure of the graph G may be arbitrary, we
limit the discussion here to graph structures in
</bodyText>
<figureCaption confidence="0.947999">
Figure 2: Number of synsets per level in WordNet
</figureCaption>
<bodyText confidence="0.95422344">
which the nodes corresponding to elements of Y
form a simple first-order Markov chain.
A CRF defines a conditional probability distri-
bution p(Y |X) of label sequences given input se-
quences. We assume that the random variable se-
quences X and Y have the same length and use
x = (x1, ..., xT ) and y = (y1, ..., yT) for an input
sequence and label sequence respectively. Instead
of defining a joint distribution over both label and
observation sequences, the model defines a condi-
tional probability over labeled sequences. A novel
observation sequence x is labeled with y, so that
the conditional probability p(y|x) is maximized.
We define a set of K binary-valued features or
feature functions fk(yt−1, yt, x) that each express
some characteristic of the empirical distribution of
the training data that should also hold in the model
distribution. An example of such a feature is
1 if x has POS ‘NN’ and
yt is concept ‘entity’
0 otherwise
Feature functions can depend on the previous
(yt−1) and the current (yt) state. Considering K
feature functions, the conditional probability dis-
tribution defined by the CRF is
</bodyText>
<equation confidence="0.9654858">
( XT )
XK
1
p(y|x) = Z(x)exp λkfk(yt−1, yt, x)
t=1k=1
</equation>
<bodyText confidence="0.996119666666667">
where λj is a parameter to model the observed
statistics and Z(x) is a normalizing constant com-
puted as
</bodyText>
<equation confidence="0.9939435">
exp(t=1
X X λkfk(yt−1, yt, x)
</equation>
<bodyText confidence="0.998337666666667">
This method can be thought of a generalization
of both the Maximum Entropy Markov model
(MEMM) and the Hidden Markov model (HMM).
</bodyText>
<equation confidence="0.996749444444444">
fk(yt−1, yt, x) =



T K
X
Z(x) =
YEY
k=1
</equation>
<page confidence="0.991099">
34
</page>
<bodyText confidence="0.999773565217391">
It brings together the best of discriminative mod-
els and generative models: (1) It can accommo-
date many statistically correlated features of the
inputs, contrasting with generative models, which
often require conditional independent assumptions
in order to make the computations tractable and (2)
it has the possibility of context-dependent learning
by trading off decisions at different sequence posi-
tions to obtain a global optimal labeling. Because
CRFs adhere to the maximum entropy principle,
they offer a valid solution when learning from in-
complete information. Given that in information
extraction tasks, we often lack an annotated train-
ing set that covers all possible extraction patterns,
this is a valuable asset.
Lafferty et al. (Lafferty et al., 2001) have shown
that CRFs outperform both MEMM and HMM
on synthetic data and on a part-of-speech tagging
task. Furthermore, CRFs have been used success-
fully in information extraction (Peng and McCal-
lum, 2004), named entity recognition (Li and Mc-
Callum, 2003; McCallum and Li, 2003) and sen-
tence parsing (Sha and Pereira, 2003).
</bodyText>
<sectionHeader confidence="0.984885" genericHeader="method">
4 Parameter estimation
</sectionHeader>
<bodyText confidence="0.999994818181818">
In this section we’ll explain to some detail how to
derive the parameters θ = {λk}, given the train-
ing data. The problem can be considered as a con-
strained optimization problem, where we have to
find a set of parameters which maximizes the log
likelihood of the conditional distribution (McCal-
lum, 2003). We are confronted with the problem
of efficiently calculating the expectation of each
feature function with respect to the CRF model
distribution for every observation sequence x in
the training data. Formally, we are given a set
</bodyText>
<subsectionHeader confidence="0.559907">
N
</subsectionHeader>
<bodyText confidence="0.9994735">
of training examples D = {x(i), y(i) } i=1 where
each x(i) = {x 1i), x2i), ..., xTz) } is a sequence
of inputs and y(i) = {yii), y2i), ..., yT2) } is a se-
quence of the desired labels. We will estimate the
parameters by penalized maximum likelihood, op-
timizing the function:
</bodyText>
<equation confidence="0.922099">
N
l(θ) = log p(y(i)|x(i)) (3)
i=1
</equation>
<bodyText confidence="0.6388795">
After substituting the CRF model (2) in the like-
lihood (3), we get the following expression:
</bodyText>
<equation confidence="0.9998976">
l(θ) = EN T K λkfk(y(i)
i=1 E E t−1, y(i)
t=1 k=1 t , x(i))
− EN log Z(x(i))
i=1
</equation>
<bodyText confidence="0.986613814814815">
The function l(θ) cannot be maximized in closed
form, so numerical optimization is used. The par-
tial derivates are:
(4)
Using these derivates, we can iteratively adjust
the parameters θ (with Limited-Memory BFGS
(Byrd et al., 1994)) until l(θ) has reached an opti-
mum. During each iteration we have to calculate
p(y′, y|x(i)). This can be done, as for the Hid-
den Markov Model, using the forward-backward
algorithm (Baum and Petrie, 1966; Forney, 1996).
This algorithm has a computational complexity of
O(TM2) (where T is the length of the sequence
and M the number of the labels). We have to exe-
cute the forward-backward algorithm once for ev-
ery training instance during every iteration. The
total cost of training a linear-chained CRFs is thus:
O(TM2NG)
where N is the number of training examples and G
the number of iterations. We’ve experienced that
this complexity is an important delimiting factor
when learning a big collection of labels. Employ-
ing CRFs to learn the 95076 WordNet synsets with
20133 training examples was not feasible on cur-
rent hardware. In the next section we’ll describe
the method we’ve implemented to drastically re-
duce this complexity.
</bodyText>
<sectionHeader confidence="0.937611" genericHeader="method">
5 Reducing complexity
</sectionHeader>
<bodyText confidence="0.999934857142857">
In this section we’ll see how we create groups of
features for every label that enable an important
reduction in complexity of both labeling and train-
ing. We’ll first discuss how these groups of fea-
tures are created (section 5.1) and then how both
labeling (section 5.2) and training (section 5.3) are
performed using these groups.
</bodyText>
<equation confidence="0.999719818181818">
∂l(θ) = EN T fk(y(i)
∂λk i=1 E t , y(i)
t−1, x(i))
E fk(y′, y, x(i))p(y′, y|x(i))
y,y′
−
T
E
t=1
EN
i=1
</equation>
<page confidence="0.993171">
35
</page>
<figureCaption confidence="0.998364">
Figure 3: Fragment of the tree used for labeling
</figureCaption>
<subsectionHeader confidence="0.994859">
5.1 Hierarchical feature selection
</subsectionHeader>
<bodyText confidence="0.999897541666666">
To reduce the complexity of CRFs, we assign a
selection of features to every node in the hierar-
chical tree. As discussed in section 2 WordNet de-
fines a relation between synsets which organises
the synsets in a tree. In its current form this tree
does not meet our needs: we need a tree where
every label used for labeling corresponds to ex-
actly one leaf-node, and no label corresponds to
a non-leaf node. We therefor modify the existing
tree. We create a new top node (“top”) and add the
original tree as defined by WordNet as a subtree to
this top-node. We add leaf-nodes corresponding
to the labels “NONE”, “ADJ”, “ADV”, “VERB”
to the top-node and for the other labels (the noun
synsets) we add a leaf-node to the node represent-
ing the corresponding synset. For example, we
add a node corresponding to the label “ENTITY”
to the node “entity”. Fig. 3 pictures a fraction of
this tree. Nodes corresponding to a label have an
uppercase name, nodes not corresponding to a la-
bel have a lowercase name.
We use v to denote nodes of the tree. We call
the top concept vtop and the concept v+ the parent
of v, which is the parent of v−. We call Av the
collection of ancestors of a concept v, including v
itself.
We will now show how we transform a regular
CRF in a CRF that uses hierarchical feature selec-
tion. We first notice that we can rewrite eq. 2 as
collection of features fk(yt−1, yt, x) for which it is
possible to find a node vt−1 and input x for which
fk(vt−1, v, x) =� 0. If v is a non-leaf node, we de-
fine Fv as the collection of features fk(yt−1, yt, x)
(1) which are elements of Fv− for every child node
v− of v and (2) for every v−1 and v−2 , children of
v, it is valid that for every previous label vt−1 and
input x fk(vt−1, v−1 , x) =fk(vt−1, v−2 , x).
Informally, Fv is the collection of features
which are useful to evaluate for a certain node. For
the leaf-nodes, this is the collection of features that
can possibly return a non-zero value. For non-leaf
nodes, it’s useful to evaluate features belonging to
Fv when they have the same value for all the de-
scendants of that node (which we can put to good
use, see further).
We define Fv′ = Fv \Fv+ where v+ is the parent
of label v. For the top node vtop we define F′vtop =
Fvtop. We also set
</bodyText>
<equation confidence="0.975416">
G′(yt−1, yt, x) = exp
</equation>
<bodyText confidence="0.9959484">
We’ve now organised the collection of features in
such a way that we can use the hierarchical rela-
tions defined by WordNet when determining the
probability of a certain labeling y. We first see
that
</bodyText>
<table confidence="0.877587454545455">
we can now determine the probability of a labeling
y, given input x
 X Akfk(yt−1, yt, x) 
 fk∈F′yt 
 
G(yt−1, yt, x) = exp  X Akfk(yt−1, yt, x) 
 fk∈Fyt 
 
= G(yt−1, y+t , x)G′(yt−1, yt, x)
��� Y= G′(yt−1, v, x)
v∈Ayt
</table>
<bodyText confidence="0.999231166666667">
We rewrite this equation because it will enable
us to reduce the complexity of CRFs and it has
the property that p(yt|yt−1, x) ^ G(yt−1, yt, x)
which we will use in section 5.3.
We now define a collection of features Fv for
every node v. If v is leaf-node, we define Fv as the
</bodyText>
<equation confidence="0.370951">
t=1 v∈Ayt
</equation>
<bodyText confidence="0.999871428571429">
This formula has exactly the same result as eq. 2.
Because we assigned a collection of features to ev-
ery node, we can discard parts of the search space
when searching for possible labelings, obtaining
an important reduction in complexity. We elab-
orate this idea in the following sections for both
labeling and training.
</bodyText>
<equation confidence="0.9994633">
T
p(y|x) = Z(x) Y G(yt−1, yt, x)
K
with G(yt−1, yt, x) = exp( P Akfk(yt−1, yt, x))
k=1
t=1
YT
1 Y
p(y|x) = G′(yt−1, v, x) (5)
Z(x)
</equation>
<page confidence="0.98116">
36
</page>
<subsectionHeader confidence="0.993396">
5.2 Labeling
</subsectionHeader>
<bodyText confidence="0.999980523809524">
The standard method to label a sentence with
CRFs is by using the Viterbi algorithm (Forney,
1973; Viterbi, 1967) which has a computational
complexity of O(TM2). The basic idea to reduce
this computational complexity is to select the best
labeling in a number of iterations. In the first itera-
tion, we label every word in a sentence with a label
chosen from the top-level labels. After choosing
the best labeling, we refine our choice (choose a
child label of the previous chosen label) in subse-
quent iterations until we arrive at a synset which
has no children. In every iteration we only have
to choose from a very small number of labels, thus
breaking down the problem of selecting the correct
label from a large number of labels in a number of
smaller problems.
Formally, when labeling a sentence we find the
label sequence y such that y has the maximum
probability of all labelings. We will estimate the
best labeling in an iterative way: we start with
the best labeling ytop−1 = {ytop−1
</bodyText>
<equation confidence="0.997691">
1 , ..., ytop−1
T }
</equation>
<bodyText confidence="0.9011155">
choosing only from the children ytop−1
t of the top
node. The probability of this labeling ytop−1
is
</bodyText>
<subsectionHeader confidence="0.644467">
After selecting a labeling
</subsectionHeader>
<bodyText confidence="0.91727875">
with maximum
probability, we proceed by selecting a labeling
with maximum probability etc.. We pro-
ceed using this method until we reach a labeling
in which every yt is a node which has no children
and return this labeling as the final labeling.
The assumption we make here is that if a node
v is selected at position t of the most probable la-
beling
the children
have a larger prob-
ability of being selected at position t in the most
probable labeling
We reduce the num-
ber of labels we take into consideration by stating
that for every concept v for which v
</bodyText>
<equation confidence="0.867513">
we
set
, x) = 0 for every child
of v.
</equation>
<bodyText confidence="0.8677925">
This reduces the space of possible labelings dras-
tically, re
</bodyText>
<equation confidence="0.991151555555556">
ytop−2
ytop−3
ytop−s
v−
ytop−s−1.
=�ytop−s
t ,
G′(yt−1,v−t
v−
</equation>
<bodyText confidence="0.92843">
ducing the computational complexity of
thm
On every level we have to execute the
Viterbi algori
logq(M).
thm for q labels, thus resulting in a
total complexity of
37 Figure 4: Nodes that need to be taken into account
during the forward-backward algori
ity of the calculation of the partial derivates
(eq. 4). The predominant factor with regard to
the computational complexity in the evaluation of
this equation is the calculation of
Recall we do this with the forward-backward al-
gorithm, which has a computational complexity
of O(TM2). We reduce the number of labels to
improve performance. We will do this by mak-
ing the same assumption as in the previous sec-
tion: for every
</bodyText>
<equation confidence="0.937861967741936">
∂l(θ)
∂λ�
p(yt−1,y|x(i)).
concept v at level s, for which
v =� ytop−s
t ,
T
p(
y
t
op−1|
x) =
′
(x)
′
(yt
−1
, yt
op−
1 , x)
t=1
where
Z′(x)
is an appropriate normalizing con-
stant. We now select a labeling
ytop−2
so that on
every position t node
ytop−2 is a child of ytop−1
t .
t
</equation>
<bodyText confidence="0.586658">
The probabilty of this labeling is (following eq. 5)
</bodyText>
<equation confidence="0.996893166666667">
T
p(ytop−2|x) = 1
Z′(x) t=1
v∈A�top−2
t
G′(yt−1, v, x)
</equation>
<bodyText confidence="0.780437">
the Viterbi algorithm. If q is the average number
of children of a concept, the depth of the tree is
</bodyText>
<equation confidence="0.959592">
O(T logq(M)q2) (6)
</equation>
<subsectionHeader confidence="0.566027">
5.3 Training
</subsectionHeader>
<bodyText confidence="0.952156714285714">
We will now discuss how we reduce the compu-
tational complexity of training. As explained in
section 4 we have to estimate the parameters Ak
that optimize the function l(B). We will show here
how we can reduce the computational complex-
every child
of v. Since (as noted in sect.
</bodyText>
<equation confidence="0.859252166666666">
5.2)
x)
vt, x), this has the
consequence that
x) = 0 and that
p(vt,
</equation>
<bodyText confidence="0.881923888888889">
= 0. Fig. 4 gives a graphical repre-
sentation of this reduction of the search space. The
correct label here is
,the grey nodes
have anon-zero p(vt,
and the white nodes
have a zero p(vt,
In the forward backward algorithm we only
have to account every node v that has anon-zero
p(v,
As can be easily seen from fig. 4,
the number of nodes is
where q is the
average number of children of a concept. The to-
tal complexity of running the forward-backward
algorithm is
Since we have to
run this algorithm once for every
</bodyText>
<equation confidence="0.966250384615385">
v−
p(vt|yt−1,
≈G(yt−1,
p(vt|yt−1,
yt−1|x)
“LABEL1”
yt−1|x)
yt−1|x).
yt−1|x).
qlogqM,
O(T(q logqM)2).
gradient compu-
we set G′(yt−1, v−t , x) = 0 for
</equation>
<figureCaption confidence="0.99768">
Figure 5: Time needed for one training cycle Figure 6: Time needed for labeling
</figureCaption>
<bodyText confidence="0.9889045">
tation for every training instance we find the total
training cost
</bodyText>
<equation confidence="0.904093">
O(T(q logQM)2NG) (7)
</equation>
<sectionHeader confidence="0.992885" genericHeader="method">
6 Implementation
</sectionHeader>
<bodyText confidence="0.999902931034483">
To implement the described method we need two
components: an interface to the WordNet database
and an implementation of CRFs using a hierar-
chical model. JWordNet is a Java interface to
WordNet developed by Oliver Steele (which can
be found on http://jwn.sourceforge.
net/). We used this interface to extract the Word-
Net hierarchy.
An implementation of CRFs using the hierar-
chical model was obtained by adapting the Mallet2
package. The Mallet package (McCallum, 2002)
is an integrated collection of Java code useful for
statistical natural language processing, document
classification, clustering, and information extrac-
tion. It also offers an efficient implementation of
CRFs. We’ve adapted this implementation so it
creates hierarchical selections of features which
are then used for training and labeling.
We used the Semcor corpus (Fellbaum et al.,
1998; Landes et al., 1998) for training. This cor-
pus, which was created by the Princeton Univer-
sity, is a subset of the English Brown corpus con-
taining almost 700,000 words. Every sentence in
the corpus is noun phrase chunked. The chunks
are tagged by POS and both noun and verb phrases
are tagged with their WordNet sense. Since we do
not want to learn a classification for verb synsets,
we replace the tags of the verbs with one tag
“VERB”.
</bodyText>
<footnote confidence="0.900888">
2http://mallet.cs.umass.edu/
</footnote>
<sectionHeader confidence="0.999278" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999816914285714">
The major goal of this paper was to build a clas-
sifier that could learn all the WordNet synsets in a
reasonable amount of time. We will first discuss
the improvement in time needed for training and
labeling and then discuss accuracy.
We want to test the influence of the number of
labels on the time needed for training. Therefor,
we created different training sets, all of which had
the same input (246 sentences tagged with POS la-
bels), but a different number of labels. The first
training set only had 5 labels (“ADJ”, “ADV”,
“VERB”, “entity” and “NONE”). The second had
the same labels except we replaced the label “en-
tity” with either “physical entity”, “abstract entity”
or “thing”. We continued this procedure, replac-
ing parent nouns labels with their children (i.e.
hyponyms) for subsequent training sets. We then
trained both a CRF using a hierarchical feature se-
lection and a standard CRF on these training sets.
Fig. 5 shows the time needed for one iteration
of training with different numbers of labels. We
can see how the time needed for training slowly
increases for the CRF using hierarchical feature
selection but increases fast when using a standard
CRF. This is conform to eq. 7.
Fig. 6 shows the average time needed for la-
beling a sentence. Here again the time increases
slowly for a CRF using hierarchical feature selec-
tion, but increases fast for a standard CRF, con-
form to eq. 6.
Finally, fig 7 shows the error rate (on the train-
ing data) after each training cycle. We see that a
standard CRF and a CRF using hierarchical fea-
ture selection perform comparable. Note that fig
7 gives the error rate on the training data but this
</bodyText>
<page confidence="0.996998">
38
</page>
<bodyText confidence="0.99993965625">
can differ considerable from the error rate on un-
seen data.
After these tests on a small section of the Sem-
cor corpus, we trained a CRF using hierarchi-
cal feature selection on 7/8 of the full corpus.
We trained for 23 iterations, which took approx-
imately 102 hours. Testing the model on the re-
maining 1/8 of the corpus resulted in an accuracy
of 77.82%. As reported in (McCarthy et al., 2004),
a baseline approach that ignors context but simply
assigns the most likely sense to a given word ob-
tains a accuracy of 67%. We did not have the pos-
sibility to compare the accuracy of this model with
a standard CRF, since as already stated, training
such a CRF takes impractically long, but we can
compare our systems with existing WSD-systems.
Mihalcea and Moldovan (Mihalcea and Moldovan,
1999) use the semantic density between words to
determine the word sense. They achieve an ac-
curacy of 86.5% (testing on the first two tagged
files of the Semcor corpus). Wilks and Stevenson
(Wilks and Stevenson, 1998) use a combination
of knowledge sources and achieve an accuracy of
92%3. Note that both these methods use additional
knowledge apart from the WordNet hierarchy.
The sentences in the training and testing sets
were already (perfectly) POS-tagged and noun
chunked, and that in a real-life situation addi-
tional preprocessing by a POS-tagger (such as the
LT-POS-tagger4) and noun chunker (such as de-
scribed in (Ramshaw and Marcus, 1995)) which
will introduce additional errors.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="discussions">
8 Future work
</sectionHeader>
<bodyText confidence="0.999911416666667">
In this section we’ll discuss some of the work we
plan to do in the future. First of all we wish to
evaluate our algorithm on standard test sets, such
as the data of the Senseval conference5, which
tests performance on word sense disambiguation,
and the data of the CoNLL 2003 shared task6, on
named entity recognition.
An important weakness of our algorithm is the
fact that, to label a sentence, we have to traverse
the hierarchy tree and choose the correct synsets
at every level. An error at a certain level can not
be recovered. Therefor, we would like to perform
</bodyText>
<footnote confidence="0.985136333333333">
3This method was tested on the Semcore corpus, but use
the word senses of the Longman Dictionary of Contemporary
English
4http://www.ltg.ed.ac.uk/software/
5http://www.senseval.org/
6http://www.cnts.ua.ac.be/conll2003/
</footnote>
<figureCaption confidence="0.999869">
Figure 7: Error rate during training
</figureCaption>
<bodyText confidence="0.999827291666667">
some a of beam-search (Bisiani, 1992), keeping
a number of best labelings at every level. We
strongly suspect this will have a positive impact
on the accuracy of our algorithm.
As already mentioned, this work is carried out
during the CLASS project. In the second phase
of this project we will discover classes and at-
tributes of entities in texts. To accomplish this
we will not only need to label nouns with their
synset, but we also need to label verbs, adjec-
tives and adverbs. This can become problem-
atic as WordNet has no hypernym/hyponym rela-
tion (or equivalent) for the synsets of adjectives
and adverbs. WordNet has an equivalent relation
for verbs (hypernym/troponym), but this structures
the verb synsets in a big number of loosely struc-
tured trees, which is less suitable for the described
method. VerbNet (Kipper et al., 2000) seems a
more promising resource to use when classify-
ing verbs, and we will also investigate the use
of other lexical databases, such as ThoughtTrea-
sure (Mueller, 1998), Cyc (Lenat, 1995), Open-
mind Commonsense (Stork, 1999) and FrameNet
(Baker et al., 1998).
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999856">
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978).
</bodyText>
<sectionHeader confidence="0.983575" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.734719">
Eneko Agirre and German Rigau. 1996. Word sense
disambiguation using conceptual density. In Pro-
ceedings of the 16th International Conference on
</bodyText>
<page confidence="0.99818">
39
</page>
<bodyText confidence="0.764849727272727">
Computational Linguistics (Coling’96), pages 16–
22, Copenhagen, Denmark.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley Framenet project. In Proceedings of the
COLING-ACL.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced lex-
icons. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of CoNLL-2003, pages 188–191.
Edmonton, Canada.
</bodyText>
<reference confidence="0.996630191489362">
L. E. Baum and T. Petrie. 1966. Statistical in-
ference for probabilistic functions of finite state
markov chains. Annals of Mathematical Statistics,,
37:1554–1563.
R. Bisiani. 1992. Beam search. In S. C. Shapiro,
editor, Encyclopedia of Artificial Intelligence, New
York. Wiley-Interscience.
Richard H. Byrd, Jorge Nocedal, and Robert B. Schn-
abel. 1994. Representations of quasi-newton matri-
ces and their use in limited memory methods. Math.
Program., 63(2):129–156.
C. Fellbaum, J. Grabowski, and S. Landes. 1998. Per-
formance and confidence in a semantic annotation
task. In C. Fellbaum, editor, WordNet: An Elec-
tronic Lexical Database. The MIT Press.
G. D. Forney. 1973. The viterbi algorithm. In Pro-
ceeding of the IEEE, pages 268 – 278.
G. D. Forney. 1996. The forward-backward algo-
rithm. In Proceedings of the 34th Allerton Confer-
ence on Communications, Control and Computing,
pages 432–446.
Brian Hayes. 1999. The web of words. American
Scientist, 87(2):108–112, March-April.
Michael I. Jordan, editor. 1999. Learning in Graphical
Models. The MIT Press, Cambridge.
K. Kipper, H.T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. Proceedings
of the Seventh National Conference on Artificial In-
telligence (AAAI-2000).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning.
S. Landes, C. Leacock, and R.I. Tengi. 1998. Build-
ing semantic concordances. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. The MIT
Press.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11):32–38.
Wei Li and Andrew McCallum. 2003. Rapid develop-
ment of hindi named entity recognition using con-
ditional random fields and feature induction. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 2(3):290–294.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of the
Nineteenth Conference on Uncertainty in Artificial
Intelligence.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Using automatically acquired predominant
senses for word sense disambiguation. In Proceed-
ings of the ACL SENSEVAL-3 workshop, pages 151–
154, Barcelona, Spain.
R. Mihalcea and D.I. Moldovan. 1999. A method
for word sense disambiguation of unrestricted text.
In Proceedings of the 37th conference on Associa-
tion for Computational Linguistics, pages 152–158.
Association for Computational Linguistics Morris-
town, NJ, USA.
Erik T. Mueller. 1998. Natural language processing
with ThoughtTreasure. Signiform, New York.
F. Peng and A. McCallum. 2004. Accurate infor-
mation extraction from research papers using con-
ditional random fields. In Proceedings of Human
Language Technology Conference and North Amer-
ican Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 329–336.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceed-
ings of the Third ACL Workshop on Very Large Cor-
pora, pages 82–94. Cambridge MA, USA.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of Human
Language Technology, HLT-NAACL.
D. Stork. 1999. The openmind initiative. IEEE Intelli-
gent Systems &amp; their applications, 14(3):19–20.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimal decoding algo-
rithm. IEEE Trans. Informat. Theory, 13:260–269.
Hanna M. Wallach. 2004. Conditional random fields:
An introduction. Technical Report MS-CIS-04-21.,
University of Pennsylvania CIS.
Y. Wilks and M. Stevenson. 1998. Word sense disam-
biguation using optimised combinations of knowl-
edge sources. Proceedings of COLING/ACL, 98.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189–196.
</reference>
<page confidence="0.998613">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.587733">
<title confidence="0.99945">Efficient Hierarchical Entity Classifier Using Conditional Random Fields</title>
<author confidence="0.972642">Koen Deschacht</author>
<affiliation confidence="0.9925845">Interdisciplinary Centre for Law &amp; IT Katholieke Universiteit Leuven</affiliation>
<address confidence="0.993006">Tiensestraat 41, 3000 Leuven, Belgium</address>
<email confidence="0.98184">koen.deschacht@law.kuleuven.ac.be</email>
<author confidence="0.660552">Marie-Francine Moens</author>
<affiliation confidence="0.9716265">Interdisciplinary Centre for Law &amp; IT Katholieke Universiteit Leuven</affiliation>
<address confidence="0.975246">Tiensestraat 41, 3000 Leuven, Belgium</address>
<email confidence="0.973572">marie-france.moens@law.kuleuven.be</email>
<abstract confidence="0.999630769230769">In this paper we develop an automatic classifier for a very large set of labels, the WordNet synsets. We employ Conditional Random Fields (CRFs) because of their flexibility to include a wide variety of nonindependent features. Training CRFs on a big number of labels proved a problem because of the large training cost. By taking into account the hypernym/hyponym relation between synsets in WordNet, we reduced the complexity of training from only a limited loss in accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>T Petrie</author>
</authors>
<title>Statistical inference for probabilistic functions of finite state markov chains.</title>
<date>1966</date>
<journal>Annals of Mathematical Statistics,,</journal>
<pages>37--1554</pages>
<contexts>
<context position="9758" citStr="Baum and Petrie, 1966" startWordPosition="1632" endWordPosition="1635">i)) (3) i=1 After substituting the CRF model (2) in the likelihood (3), we get the following expression: l(θ) = EN T K λkfk(y(i) i=1 E E t−1, y(i) t=1 k=1 t , x(i)) − EN log Z(x(i)) i=1 The function l(θ) cannot be maximized in closed form, so numerical optimization is used. The partial derivates are: (4) Using these derivates, we can iteratively adjust the parameters θ (with Limited-Memory BFGS (Byrd et al., 1994)) until l(θ) has reached an optimum. During each iteration we have to calculate p(y′, y|x(i)). This can be done, as for the Hidden Markov Model, using the forward-backward algorithm (Baum and Petrie, 1966; Forney, 1996). This algorithm has a computational complexity of O(TM2) (where T is the length of the sequence and M the number of the labels). We have to execute the forward-backward algorithm once for every training instance during every iteration. The total cost of training a linear-chained CRFs is thus: O(TM2NG) where N is the number of training examples and G the number of iterations. We’ve experienced that this complexity is an important delimiting factor when learning a big collection of labels. Employing CRFs to learn the 95076 WordNet synsets with 20133 training examples was not feas</context>
</contexts>
<marker>Baum, Petrie, 1966</marker>
<rawString>L. E. Baum and T. Petrie. 1966. Statistical inference for probabilistic functions of finite state markov chains. Annals of Mathematical Statistics,, 37:1554–1563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bisiani</author>
</authors>
<title>Beam search.</title>
<date>1992</date>
<booktitle>Encyclopedia of Artificial Intelligence,</booktitle>
<editor>In S. C. Shapiro, editor,</editor>
<location>New York. Wiley-Interscience.</location>
<contexts>
<context position="24071" citStr="Bisiani, 1992" startWordPosition="4191" endWordPosition="4192">d the data of the CoNLL 2003 shared task6, on named entity recognition. An important weakness of our algorithm is the fact that, to label a sentence, we have to traverse the hierarchy tree and choose the correct synsets at every level. An error at a certain level can not be recovered. Therefor, we would like to perform 3This method was tested on the Semcore corpus, but use the word senses of the Longman Dictionary of Contemporary English 4http://www.ltg.ed.ac.uk/software/ 5http://www.senseval.org/ 6http://www.cnts.ua.ac.be/conll2003/ Figure 7: Error rate during training some a of beam-search (Bisiani, 1992), keeping a number of best labelings at every level. We strongly suspect this will have a positive impact on the accuracy of our algorithm. As already mentioned, this work is carried out during the CLASS project. In the second phase of this project we will discover classes and attributes of entities in texts. To accomplish this we will not only need to label nouns with their synset, but we also need to label verbs, adjectives and adverbs. This can become problematic as WordNet has no hypernym/hyponym relation (or equivalent) for the synsets of adjectives and adverbs. WordNet has an equivalent </context>
</contexts>
<marker>Bisiani, 1992</marker>
<rawString>R. Bisiani. 1992. Beam search. In S. C. Shapiro, editor, Encyclopedia of Artificial Intelligence, New York. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard H Byrd</author>
<author>Jorge Nocedal</author>
<author>Robert B Schnabel</author>
</authors>
<title>Representations of quasi-newton matrices and their use in limited memory methods.</title>
<date>1994</date>
<journal>Math. Program.,</journal>
<volume>63</volume>
<issue>2</issue>
<contexts>
<context position="9554" citStr="Byrd et al., 1994" startWordPosition="1597" endWordPosition="1600">uence of inputs and y(i) = {yii), y2i), ..., yT2) } is a sequence of the desired labels. We will estimate the parameters by penalized maximum likelihood, optimizing the function: N l(θ) = log p(y(i)|x(i)) (3) i=1 After substituting the CRF model (2) in the likelihood (3), we get the following expression: l(θ) = EN T K λkfk(y(i) i=1 E E t−1, y(i) t=1 k=1 t , x(i)) − EN log Z(x(i)) i=1 The function l(θ) cannot be maximized in closed form, so numerical optimization is used. The partial derivates are: (4) Using these derivates, we can iteratively adjust the parameters θ (with Limited-Memory BFGS (Byrd et al., 1994)) until l(θ) has reached an optimum. During each iteration we have to calculate p(y′, y|x(i)). This can be done, as for the Hidden Markov Model, using the forward-backward algorithm (Baum and Petrie, 1966; Forney, 1996). This algorithm has a computational complexity of O(TM2) (where T is the length of the sequence and M the number of the labels). We have to execute the forward-backward algorithm once for every training instance during every iteration. The total cost of training a linear-chained CRFs is thus: O(TM2NG) where N is the number of training examples and G the number of iterations. We</context>
</contexts>
<marker>Byrd, Nocedal, Schnabel, 1994</marker>
<rawString>Richard H. Byrd, Jorge Nocedal, and Robert B. Schnabel. 1994. Representations of quasi-newton matrices and their use in limited memory methods. Math. Program., 63(2):129–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
<author>J Grabowski</author>
<author>S Landes</author>
</authors>
<title>Performance and confidence in a semantic annotation task.</title>
<date>1998</date>
<editor>In C. Fellbaum, editor,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2733" citStr="Fellbaum et al., 1998" startWordPosition="420" endWordPosition="423">cessfully in NER (Conditional Random Fields) and use this with an extensive inventory of word senses (the WordNet lexical database) to perform entity detection. In section 2 we describe WordNet and it’s use for entity categorization. Section 3 gives an overview of Conditional Random Fields and section 4 explains how the parameters of this model are estimated during training. We will drastically reduce the computational complexity of training in section 5. Section 6 describes the implementation of this method, section 7 the obtained results and finally section 8 future work. 2 WordNet WordNet (Fellbaum et al., 1998) is a lexical database whose design is inspired by psycholinguistic theories of human lexical memory. English nouns, verbs, adjectives and adverbs are organized in synsets. A synset is a collection of words that have a close meaning and that represent an underlying concept. An example of such a synset is “person, individual, someone, somebody, mortal, soul”. All these words refer to a human being. WordNet (v2.1) contains 155.327 words, which are organized in 117.597 synsets. WordNet defines a number of relations between synsets. For nouns the most important relation is the hypernym/hyponym rel</context>
<context position="19591" citStr="Fellbaum et al., 1998" startWordPosition="3425" endWordPosition="3428">und on http://jwn.sourceforge. net/). We used this interface to extract the WordNet hierarchy. An implementation of CRFs using the hierarchical model was obtained by adapting the Mallet2 package. The Mallet package (McCallum, 2002) is an integrated collection of Java code useful for statistical natural language processing, document classification, clustering, and information extraction. It also offers an efficient implementation of CRFs. We’ve adapted this implementation so it creates hierarchical selections of features which are then used for training and labeling. We used the Semcor corpus (Fellbaum et al., 1998; Landes et al., 1998) for training. This corpus, which was created by the Princeton University, is a subset of the English Brown corpus containing almost 700,000 words. Every sentence in the corpus is noun phrase chunked. The chunks are tagged by POS and both noun and verb phrases are tagged with their WordNet sense. Since we do not want to learn a classification for verb synsets, we replace the tags of the verbs with one tag “VERB”. 2http://mallet.cs.umass.edu/ 7 Results The major goal of this paper was to build a classifier that could learn all the WordNet synsets in a reasonable amount of </context>
</contexts>
<marker>Fellbaum, Grabowski, Landes, 1998</marker>
<rawString>C. Fellbaum, J. Grabowski, and S. Landes. 1998. Performance and confidence in a semantic annotation task. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Forney</author>
</authors>
<title>The viterbi algorithm.</title>
<date>1973</date>
<booktitle>In Proceeding of the IEEE,</booktitle>
<pages>268--278</pages>
<contexts>
<context position="14588" citStr="Forney, 1973" startWordPosition="2539" endWordPosition="2540">f v is leaf-node, we define Fv as the t=1 v∈Ayt This formula has exactly the same result as eq. 2. Because we assigned a collection of features to every node, we can discard parts of the search space when searching for possible labelings, obtaining an important reduction in complexity. We elaborate this idea in the following sections for both labeling and training. T p(y|x) = Z(x) Y G(yt−1, yt, x) K with G(yt−1, yt, x) = exp( P Akfk(yt−1, yt, x)) k=1 t=1 YT 1 Y p(y|x) = G′(yt−1, v, x) (5) Z(x) 36 5.2 Labeling The standard method to label a sentence with CRFs is by using the Viterbi algorithm (Forney, 1973; Viterbi, 1967) which has a computational complexity of O(TM2). The basic idea to reduce this computational complexity is to select the best labeling in a number of iterations. In the first iteration, we label every word in a sentence with a label chosen from the top-level labels. After choosing the best labeling, we refine our choice (choose a child label of the previous chosen label) in subsequent iterations until we arrive at a synset which has no children. In every iteration we only have to choose from a very small number of labels, thus breaking down the problem of selecting the correct </context>
</contexts>
<marker>Forney, 1973</marker>
<rawString>G. D. Forney. 1973. The viterbi algorithm. In Proceeding of the IEEE, pages 268 – 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Forney</author>
</authors>
<title>The forward-backward algorithm.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Allerton Conference on Communications, Control and Computing,</booktitle>
<pages>432--446</pages>
<contexts>
<context position="9773" citStr="Forney, 1996" startWordPosition="1636" endWordPosition="1637">ituting the CRF model (2) in the likelihood (3), we get the following expression: l(θ) = EN T K λkfk(y(i) i=1 E E t−1, y(i) t=1 k=1 t , x(i)) − EN log Z(x(i)) i=1 The function l(θ) cannot be maximized in closed form, so numerical optimization is used. The partial derivates are: (4) Using these derivates, we can iteratively adjust the parameters θ (with Limited-Memory BFGS (Byrd et al., 1994)) until l(θ) has reached an optimum. During each iteration we have to calculate p(y′, y|x(i)). This can be done, as for the Hidden Markov Model, using the forward-backward algorithm (Baum and Petrie, 1966; Forney, 1996). This algorithm has a computational complexity of O(TM2) (where T is the length of the sequence and M the number of the labels). We have to execute the forward-backward algorithm once for every training instance during every iteration. The total cost of training a linear-chained CRFs is thus: O(TM2NG) where N is the number of training examples and G the number of iterations. We’ve experienced that this complexity is an important delimiting factor when learning a big collection of labels. Employing CRFs to learn the 95076 WordNet synsets with 20133 training examples was not feasible on current</context>
</contexts>
<marker>Forney, 1996</marker>
<rawString>G. D. Forney. 1996. The forward-backward algorithm. In Proceedings of the 34th Allerton Conference on Communications, Control and Computing, pages 432–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Hayes</author>
</authors>
<title>The web of words.</title>
<date>1999</date>
<journal>American Scientist,</journal>
<volume>87</volume>
<issue>2</issue>
<contexts>
<context position="3570" citStr="Hayes, 1999" startWordPosition="564" endWordPosition="565">meaning and that represent an underlying concept. An example of such a synset is “person, individual, someone, somebody, mortal, soul”. All these words refer to a human being. WordNet (v2.1) contains 155.327 words, which are organized in 117.597 synsets. WordNet defines a number of relations between synsets. For nouns the most important relation is the hypernym/hyponym relation. A noun X is a hypernym of a noun Y if Y is a subtype or instance of X. For example, “bird” is a hypernym of “penguin” (and “penguin” is a hyponym of “bird”). This relation organizes the synsets in a hierarchical tree (Hayes, 1999), of which a fragment is pictured in fig. 1. 33 Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 33–40, Sydney, July 2006. c�2006 Association for Computational Linguistics Figure 1: Fragment of the hypernym/hyponym tree This tree has a depth of 18 levels and maximum width of 17837 synsets (fig. 2). We will build a classifier using CRFs that tags noun phrases in a text with their WordNet synset. This will enable us to recognize entities, and to classify the entities in certain groups. Moreover, it allows learning the context pattern of a certain meaning of a word. Take</context>
</contexts>
<marker>Hayes, 1999</marker>
<rawString>Brian Hayes. 1999. The web of words. American Scientist, 87(2):108–112, March-April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan</author>
<author>editor</author>
</authors>
<title>Learning in Graphical Models.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Jordan, editor, 1999</marker>
<rawString>Michael I. Jordan, editor. 1999. Learning in Graphical Models. The MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>Classbased construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-2000).</booktitle>
<contexts>
<context position="24871" citStr="Kipper et al., 2000" startWordPosition="4325" endWordPosition="4328">ed out during the CLASS project. In the second phase of this project we will discover classes and attributes of entities in texts. To accomplish this we will not only need to label nouns with their synset, but we also need to label verbs, adjectives and adverbs. This can become problematic as WordNet has no hypernym/hyponym relation (or equivalent) for the synsets of adjectives and adverbs. WordNet has an equivalent relation for verbs (hypernym/troponym), but this structures the verb synsets in a big number of loosely structured trees, which is less suitable for the described method. VerbNet (Kipper et al., 2000) seems a more promising resource to use when classifying verbs, and we will also investigate the use of other lexical databases, such as ThoughtTreasure (Mueller, 1998), Cyc (Lenat, 1995), Openmind Commonsense (Stork, 1999) and FrameNet (Baker et al., 1998). Acknowledgments The work reported in this paper was supported by the EU-IST project CLASS (Cognitive-Level Annotation using Latent Statistical Structure, IST027978). References Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of the 16th International Conference on 39 Computational Lin</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, H.T. Dang, and M. Palmer. 2000. Classbased construction of a verb lexicon. Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4738" citStr="Lafferty et al., 2001" startWordPosition="759" endWordPosition="762"> context pattern of a certain meaning of a word. Take for example the sentence “The ambulance took the remains of the bomber to the morgue.” Having every noun phrase tagged with it’s WordNet synset reveals that in this sentence, “bomber” is “a person who plants bombs” (and not “a military aircraft that drops bombs during flight”). Using the hypernym/hyponym relations from WordNet, we can also easily find out that “ambulance” is a kind of “car”, which in turn is a kind of “conveyance, transport” which in turn is a “physical object”. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001; Jordan, 1999; Wallach, 2004) is a statistical method based on undirected graphical models. Let X be a random variable over data sequences to be labeled and Y a random variable over corresponding label sequences. All components YZ of Y are assumed to range over a finite label alphabet K. In this paper X will range over the sentences of a text, tagged with POS-labels and Y ranges over the synsets to be recognized in these sentences. We define G = (V, E) to be an undirected graph such that there is a node v E V corresponding to each of the random variables representing an element Y� of Y . If e</context>
<context position="7957" citStr="Lafferty et al., 2001" startWordPosition="1316" endWordPosition="1319">he inputs, contrasting with generative models, which often require conditional independent assumptions in order to make the computations tractable and (2) it has the possibility of context-dependent learning by trading off decisions at different sequence positions to obtain a global optimal labeling. Because CRFs adhere to the maximum entropy principle, they offer a valid solution when learning from incomplete information. Given that in information extraction tasks, we often lack an annotated training set that covers all possible extraction patterns, this is a valuable asset. Lafferty et al. (Lafferty et al., 2001) have shown that CRFs outperform both MEMM and HMM on synthetic data and on a part-of-speech tagging task. Furthermore, CRFs have been used successfully in information extraction (Peng and McCallum, 2004), named entity recognition (Li and McCallum, 2003; McCallum and Li, 2003) and sentence parsing (Sha and Pereira, 2003). 4 Parameter estimation In this section we’ll explain to some detail how to derive the parameters θ = {λk}, given the training data. The problem can be considered as a constrained optimization problem, where we have to find a set of parameters which maximizes the log likelihoo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Landes</author>
<author>C Leacock</author>
<author>R I Tengi</author>
</authors>
<title>Building semantic concordances.</title>
<date>1998</date>
<editor>In C. Fellbaum, editor,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="19613" citStr="Landes et al., 1998" startWordPosition="3429" endWordPosition="3432">eforge. net/). We used this interface to extract the WordNet hierarchy. An implementation of CRFs using the hierarchical model was obtained by adapting the Mallet2 package. The Mallet package (McCallum, 2002) is an integrated collection of Java code useful for statistical natural language processing, document classification, clustering, and information extraction. It also offers an efficient implementation of CRFs. We’ve adapted this implementation so it creates hierarchical selections of features which are then used for training and labeling. We used the Semcor corpus (Fellbaum et al., 1998; Landes et al., 1998) for training. This corpus, which was created by the Princeton University, is a subset of the English Brown corpus containing almost 700,000 words. Every sentence in the corpus is noun phrase chunked. The chunks are tagged by POS and both noun and verb phrases are tagged with their WordNet sense. Since we do not want to learn a classification for verb synsets, we replace the tags of the verbs with one tag “VERB”. 2http://mallet.cs.umass.edu/ 7 Results The major goal of this paper was to build a classifier that could learn all the WordNet synsets in a reasonable amount of time. We will first di</context>
</contexts>
<marker>Landes, Leacock, Tengi, 1998</marker>
<rawString>S. Landes, C. Leacock, and R.I. Tengi. 1998. Building semantic concordances. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Lenat</author>
</authors>
<title>Cyc: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="25058" citStr="Lenat, 1995" startWordPosition="4358" endWordPosition="4359">ir synset, but we also need to label verbs, adjectives and adverbs. This can become problematic as WordNet has no hypernym/hyponym relation (or equivalent) for the synsets of adjectives and adverbs. WordNet has an equivalent relation for verbs (hypernym/troponym), but this structures the verb synsets in a big number of loosely structured trees, which is less suitable for the described method. VerbNet (Kipper et al., 2000) seems a more promising resource to use when classifying verbs, and we will also investigate the use of other lexical databases, such as ThoughtTreasure (Mueller, 1998), Cyc (Lenat, 1995), Openmind Commonsense (Stork, 1999) and FrameNet (Baker et al., 1998). Acknowledgments The work reported in this paper was supported by the EU-IST project CLASS (Cognitive-Level Annotation using Latent Statistical Structure, IST027978). References Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of the 16th International Conference on 39 Computational Linguistics (Coling’96), pages 16– 22, Copenhagen, Denmark. C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley Framenet project. In Proceedings of the COLING-ACL. Andrew McCallu</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>D. B. Lenat. 1995. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Rapid development of hindi named entity recognition using conditional random fields and feature induction.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="8210" citStr="Li and McCallum, 2003" startWordPosition="1356" endWordPosition="1360"> positions to obtain a global optimal labeling. Because CRFs adhere to the maximum entropy principle, they offer a valid solution when learning from incomplete information. Given that in information extraction tasks, we often lack an annotated training set that covers all possible extraction patterns, this is a valuable asset. Lafferty et al. (Lafferty et al., 2001) have shown that CRFs outperform both MEMM and HMM on synthetic data and on a part-of-speech tagging task. Furthermore, CRFs have been used successfully in information extraction (Peng and McCallum, 2004), named entity recognition (Li and McCallum, 2003; McCallum and Li, 2003) and sentence parsing (Sha and Pereira, 2003). 4 Parameter estimation In this section we’ll explain to some detail how to derive the parameters θ = {λk}, given the training data. The problem can be considered as a constrained optimization problem, where we have to find a set of parameters which maximizes the log likelihood of the conditional distribution (McCallum, 2003). We are confronted with the problem of efficiently calculating the expectation of each feature function with respect to the CRF model distribution for every observation sequence x in the training data. </context>
</contexts>
<marker>Li, McCallum, 2003</marker>
<rawString>Wei Li and Andrew McCallum. 2003. Rapid development of hindi named entity recognition using conditional random fields and feature induction. ACM Transactions on Asian Language Information Processing (TALIP), 2(3):290–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="19201" citStr="McCallum, 2002" startWordPosition="3371" endWordPosition="3372">g cycle Figure 6: Time needed for labeling tation for every training instance we find the total training cost O(T(q logQM)2NG) (7) 6 Implementation To implement the described method we need two components: an interface to the WordNet database and an implementation of CRFs using a hierarchical model. JWordNet is a Java interface to WordNet developed by Oliver Steele (which can be found on http://jwn.sourceforge. net/). We used this interface to extract the WordNet hierarchy. An implementation of CRFs using the hierarchical model was obtained by adapting the Mallet2 package. The Mallet package (McCallum, 2002) is an integrated collection of Java code useful for statistical natural language processing, document classification, clustering, and information extraction. It also offers an efficient implementation of CRFs. We’ve adapted this implementation so it creates hierarchical selections of features which are then used for training and labeling. We used the Semcor corpus (Fellbaum et al., 1998; Landes et al., 1998) for training. This corpus, which was created by the Princeton University, is a subset of the English Brown corpus containing almost 700,000 words. Every sentence in the corpus is noun phr</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="8210" citStr="McCallum, 2003" startWordPosition="1358" endWordPosition="1360">ons to obtain a global optimal labeling. Because CRFs adhere to the maximum entropy principle, they offer a valid solution when learning from incomplete information. Given that in information extraction tasks, we often lack an annotated training set that covers all possible extraction patterns, this is a valuable asset. Lafferty et al. (Lafferty et al., 2001) have shown that CRFs outperform both MEMM and HMM on synthetic data and on a part-of-speech tagging task. Furthermore, CRFs have been used successfully in information extraction (Peng and McCallum, 2004), named entity recognition (Li and McCallum, 2003; McCallum and Li, 2003) and sentence parsing (Sha and Pereira, 2003). 4 Parameter estimation In this section we’ll explain to some detail how to derive the parameters θ = {λk}, given the training data. The problem can be considered as a constrained optimization problem, where we have to find a set of parameters which maximizes the log likelihood of the conditional distribution (McCallum, 2003). We are confronted with the problem of efficiently calculating the expectation of each feature function with respect to the CRF model distribution for every observation sequence x in the training data. </context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>A. McCallum. 2003. Efficiently inducing features of conditional random fields. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Using automatically acquired predominant senses for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL SENSEVAL-3 workshop,</booktitle>
<pages>151--154</pages>
<location>Barcelona,</location>
<contexts>
<context position="22113" citStr="McCarthy et al., 2004" startWordPosition="3872" endWordPosition="3875"> the error rate (on the training data) after each training cycle. We see that a standard CRF and a CRF using hierarchical feature selection perform comparable. Note that fig 7 gives the error rate on the training data but this 38 can differ considerable from the error rate on unseen data. After these tests on a small section of the Semcor corpus, we trained a CRF using hierarchical feature selection on 7/8 of the full corpus. We trained for 23 iterations, which took approximately 102 hours. Testing the model on the remaining 1/8 of the corpus resulted in an accuracy of 77.82%. As reported in (McCarthy et al., 2004), a baseline approach that ignors context but simply assigns the most likely sense to a given word obtains a accuracy of 67%. We did not have the possibility to compare the accuracy of this model with a standard CRF, since as already stated, training such a CRF takes impractically long, but we can compare our systems with existing WSD-systems. Mihalcea and Moldovan (Mihalcea and Moldovan, 1999) use the semantic density between words to determine the word sense. They achieve an accuracy of 86.5% (testing on the first two tagged files of the Semcor corpus). Wilks and Stevenson (Wilks and Stevens</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004. Using automatically acquired predominant senses for word sense disambiguation. In Proceedings of the ACL SENSEVAL-3 workshop, pages 151– 154, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D I Moldovan</author>
</authors>
<title>A method for word sense disambiguation of unrestricted text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th conference on Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22510" citStr="Mihalcea and Moldovan, 1999" startWordPosition="3940" endWordPosition="3943">e selection on 7/8 of the full corpus. We trained for 23 iterations, which took approximately 102 hours. Testing the model on the remaining 1/8 of the corpus resulted in an accuracy of 77.82%. As reported in (McCarthy et al., 2004), a baseline approach that ignors context but simply assigns the most likely sense to a given word obtains a accuracy of 67%. We did not have the possibility to compare the accuracy of this model with a standard CRF, since as already stated, training such a CRF takes impractically long, but we can compare our systems with existing WSD-systems. Mihalcea and Moldovan (Mihalcea and Moldovan, 1999) use the semantic density between words to determine the word sense. They achieve an accuracy of 86.5% (testing on the first two tagged files of the Semcor corpus). Wilks and Stevenson (Wilks and Stevenson, 1998) use a combination of knowledge sources and achieve an accuracy of 92%3. Note that both these methods use additional knowledge apart from the WordNet hierarchy. The sentences in the training and testing sets were already (perfectly) POS-tagged and noun chunked, and that in a real-life situation additional preprocessing by a POS-tagger (such as the LT-POS-tagger4) and noun chunker (such</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>R. Mihalcea and D.I. Moldovan. 1999. A method for word sense disambiguation of unrestricted text. In Proceedings of the 37th conference on Association for Computational Linguistics, pages 152–158. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik T Mueller</author>
</authors>
<title>Natural language processing with ThoughtTreasure. Signiform,</title>
<date>1998</date>
<location>New York.</location>
<contexts>
<context position="25039" citStr="Mueller, 1998" startWordPosition="4355" endWordPosition="4356"> label nouns with their synset, but we also need to label verbs, adjectives and adverbs. This can become problematic as WordNet has no hypernym/hyponym relation (or equivalent) for the synsets of adjectives and adverbs. WordNet has an equivalent relation for verbs (hypernym/troponym), but this structures the verb synsets in a big number of loosely structured trees, which is less suitable for the described method. VerbNet (Kipper et al., 2000) seems a more promising resource to use when classifying verbs, and we will also investigate the use of other lexical databases, such as ThoughtTreasure (Mueller, 1998), Cyc (Lenat, 1995), Openmind Commonsense (Stork, 1999) and FrameNet (Baker et al., 1998). Acknowledgments The work reported in this paper was supported by the EU-IST project CLASS (Cognitive-Level Annotation using Latent Statistical Structure, IST027978). References Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of the 16th International Conference on 39 Computational Linguistics (Coling’96), pages 16– 22, Copenhagen, Denmark. C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley Framenet project. In Proceedings of the COLING-</context>
</contexts>
<marker>Mueller, 1998</marker>
<rawString>Erik T. Mueller. 1998. Natural language processing with ThoughtTreasure. Signiform, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>A McCallum</author>
</authors>
<title>Accurate information extraction from research papers using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>329--336</pages>
<contexts>
<context position="8161" citStr="Peng and McCallum, 2004" startWordPosition="1348" endWordPosition="1352">rning by trading off decisions at different sequence positions to obtain a global optimal labeling. Because CRFs adhere to the maximum entropy principle, they offer a valid solution when learning from incomplete information. Given that in information extraction tasks, we often lack an annotated training set that covers all possible extraction patterns, this is a valuable asset. Lafferty et al. (Lafferty et al., 2001) have shown that CRFs outperform both MEMM and HMM on synthetic data and on a part-of-speech tagging task. Furthermore, CRFs have been used successfully in information extraction (Peng and McCallum, 2004), named entity recognition (Li and McCallum, 2003; McCallum and Li, 2003) and sentence parsing (Sha and Pereira, 2003). 4 Parameter estimation In this section we’ll explain to some detail how to derive the parameters θ = {λk}, given the training data. The problem can be considered as a constrained optimization problem, where we have to find a set of parameters which maximizes the log likelihood of the conditional distribution (McCallum, 2003). We are confronted with the problem of efficiently calculating the expectation of each feature function with respect to the CRF model distribution for ev</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>F. Peng and A. McCallum. 2004. Accurate information extraction from research papers using conditional random fields. In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 329–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<location>Cambridge MA, USA.</location>
<contexts>
<context position="23153" citStr="Ramshaw and Marcus, 1995" startWordPosition="4044" endWordPosition="4047">density between words to determine the word sense. They achieve an accuracy of 86.5% (testing on the first two tagged files of the Semcor corpus). Wilks and Stevenson (Wilks and Stevenson, 1998) use a combination of knowledge sources and achieve an accuracy of 92%3. Note that both these methods use additional knowledge apart from the WordNet hierarchy. The sentences in the training and testing sets were already (perfectly) POS-tagged and noun chunked, and that in a real-life situation additional preprocessing by a POS-tagger (such as the LT-POS-tagger4) and noun chunker (such as described in (Ramshaw and Marcus, 1995)) which will introduce additional errors. 8 Future work In this section we’ll discuss some of the work we plan to do in the future. First of all we wish to evaluate our algorithm on standard test sets, such as the data of the Senseval conference5, which tests performance on word sense disambiguation, and the data of the CoNLL 2003 shared task6, on named entity recognition. An important weakness of our algorithm is the fact that, to label a sentence, we have to traverse the hierarchy tree and choose the correct synsets at every level. An error at a certain level can not be recovered. Therefor, </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L.A. Ramshaw and M.P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora, pages 82–94. Cambridge MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology, HLT-NAACL.</booktitle>
<contexts>
<context position="8279" citStr="Sha and Pereira, 2003" startWordPosition="1369" endWordPosition="1372">to the maximum entropy principle, they offer a valid solution when learning from incomplete information. Given that in information extraction tasks, we often lack an annotated training set that covers all possible extraction patterns, this is a valuable asset. Lafferty et al. (Lafferty et al., 2001) have shown that CRFs outperform both MEMM and HMM on synthetic data and on a part-of-speech tagging task. Furthermore, CRFs have been used successfully in information extraction (Peng and McCallum, 2004), named entity recognition (Li and McCallum, 2003; McCallum and Li, 2003) and sentence parsing (Sha and Pereira, 2003). 4 Parameter estimation In this section we’ll explain to some detail how to derive the parameters θ = {λk}, given the training data. The problem can be considered as a constrained optimization problem, where we have to find a set of parameters which maximizes the log likelihood of the conditional distribution (McCallum, 2003). We are confronted with the problem of efficiently calculating the expectation of each feature function with respect to the CRF model distribution for every observation sequence x in the training data. Formally, we are given a set N of training examples D = {x(i), y(i) }</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of Human Language Technology, HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Stork</author>
</authors>
<title>The openmind initiative.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems &amp; their applications,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="25094" citStr="Stork, 1999" startWordPosition="4363" endWordPosition="4364"> verbs, adjectives and adverbs. This can become problematic as WordNet has no hypernym/hyponym relation (or equivalent) for the synsets of adjectives and adverbs. WordNet has an equivalent relation for verbs (hypernym/troponym), but this structures the verb synsets in a big number of loosely structured trees, which is less suitable for the described method. VerbNet (Kipper et al., 2000) seems a more promising resource to use when classifying verbs, and we will also investigate the use of other lexical databases, such as ThoughtTreasure (Mueller, 1998), Cyc (Lenat, 1995), Openmind Commonsense (Stork, 1999) and FrameNet (Baker et al., 1998). Acknowledgments The work reported in this paper was supported by the EU-IST project CLASS (Cognitive-Level Annotation using Latent Statistical Structure, IST027978). References Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of the 16th International Conference on 39 Computational Linguistics (Coling’96), pages 16– 22, Copenhagen, Denmark. C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley Framenet project. In Proceedings of the COLING-ACL. Andrew McCallum and Wei Li. 2003. Early results fo</context>
</contexts>
<marker>Stork, 1999</marker>
<rawString>D. Stork. 1999. The openmind initiative. IEEE Intelligent Systems &amp; their applications, 14(3):19–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Trans. Informat. Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="14604" citStr="Viterbi, 1967" startWordPosition="2541" endWordPosition="2542">de, we define Fv as the t=1 v∈Ayt This formula has exactly the same result as eq. 2. Because we assigned a collection of features to every node, we can discard parts of the search space when searching for possible labelings, obtaining an important reduction in complexity. We elaborate this idea in the following sections for both labeling and training. T p(y|x) = Z(x) Y G(yt−1, yt, x) K with G(yt−1, yt, x) = exp( P Akfk(yt−1, yt, x)) k=1 t=1 YT 1 Y p(y|x) = G′(yt−1, v, x) (5) Z(x) 36 5.2 Labeling The standard method to label a sentence with CRFs is by using the Viterbi algorithm (Forney, 1973; Viterbi, 1967) which has a computational complexity of O(TM2). The basic idea to reduce this computational complexity is to select the best labeling in a number of iterations. In the first iteration, we label every word in a sentence with a label chosen from the top-level labels. After choosing the best labeling, we refine our choice (choose a child label of the previous chosen label) in subsequent iterations until we arrive at a synset which has no children. In every iteration we only have to choose from a very small number of labels, thus breaking down the problem of selecting the correct label from a lar</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Trans. Informat. Theory, 13:260–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Conditional random fields: An introduction.</title>
<date>2004</date>
<tech>Technical Report MS-CIS-04-21.,</tech>
<institution>University of Pennsylvania CIS.</institution>
<contexts>
<context position="4768" citStr="Wallach, 2004" startWordPosition="765" endWordPosition="766"> of a word. Take for example the sentence “The ambulance took the remains of the bomber to the morgue.” Having every noun phrase tagged with it’s WordNet synset reveals that in this sentence, “bomber” is “a person who plants bombs” (and not “a military aircraft that drops bombs during flight”). Using the hypernym/hyponym relations from WordNet, we can also easily find out that “ambulance” is a kind of “car”, which in turn is a kind of “conveyance, transport” which in turn is a “physical object”. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001; Jordan, 1999; Wallach, 2004) is a statistical method based on undirected graphical models. Let X be a random variable over data sequences to be labeled and Y a random variable over corresponding label sequences. All components YZ of Y are assumed to range over a finite label alphabet K. In this paper X will range over the sentences of a text, tagged with POS-labels and Y ranges over the synsets to be recognized in these sentences. We define G = (V, E) to be an undirected graph such that there is a node v E V corresponding to each of the random variables representing an element Y� of Y . If each random variable Y� obeys t</context>
</contexts>
<marker>Wallach, 2004</marker>
<rawString>Hanna M. Wallach. 2004. Conditional random fields: An introduction. Technical Report MS-CIS-04-21., University of Pennsylvania CIS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>M Stevenson</author>
</authors>
<title>Word sense disambiguation using optimised combinations of knowledge sources.</title>
<date>1998</date>
<booktitle>Proceedings of COLING/ACL,</booktitle>
<pages>98</pages>
<contexts>
<context position="22722" citStr="Wilks and Stevenson, 1998" startWordPosition="3976" endWordPosition="3979">thy et al., 2004), a baseline approach that ignors context but simply assigns the most likely sense to a given word obtains a accuracy of 67%. We did not have the possibility to compare the accuracy of this model with a standard CRF, since as already stated, training such a CRF takes impractically long, but we can compare our systems with existing WSD-systems. Mihalcea and Moldovan (Mihalcea and Moldovan, 1999) use the semantic density between words to determine the word sense. They achieve an accuracy of 86.5% (testing on the first two tagged files of the Semcor corpus). Wilks and Stevenson (Wilks and Stevenson, 1998) use a combination of knowledge sources and achieve an accuracy of 92%3. Note that both these methods use additional knowledge apart from the WordNet hierarchy. The sentences in the training and testing sets were already (perfectly) POS-tagged and noun chunked, and that in a real-life situation additional preprocessing by a POS-tagger (such as the LT-POS-tagger4) and noun chunker (such as described in (Ramshaw and Marcus, 1995)) which will introduce additional errors. 8 Future work In this section we’ll discuss some of the work we plan to do in the future. First of all we wish to evaluate our </context>
</contexts>
<marker>Wilks, Stevenson, 1998</marker>
<rawString>Y. Wilks and M. Stevenson. 1998. Word sense disambiguation using optimised combinations of knowledge sources. Proceedings of COLING/ACL, 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1970" citStr="Yarowsky, 1995" startWordPosition="297" endWordPosition="298">rounding text. The role of the authors in the CLASS project is mainly on information extraction from text. In the first phase of the project we build a classifier for automatic identification and categorization of entities in texts which we report here. This classifier extracts entities from text, and assigns a label to these entities chosen from an inventory of possible labels. This task is closely related to both named entity recognition (NER), which traditionally assigns nouns to a small number of categories and word sense disambiguation (Agirre and 1http://class.inrialpes.fr/ Rigau, 1996; Yarowsky, 1995), where the sense for a word is chosen from a much larger inventory of word senses. We will employ a probabilistic model that’s been used successfully in NER (Conditional Random Fields) and use this with an extensive inventory of word senses (the WordNet lexical database) to perform entity detection. In section 2 we describe WordNet and it’s use for entity categorization. Section 3 gives an overview of Conditional Random Fields and section 4 explains how the parameters of this model are estimated during training. We will drastically reduce the computational complexity of training in section 5.</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>