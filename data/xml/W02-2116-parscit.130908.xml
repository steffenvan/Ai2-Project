<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000219">
<title confidence="0.9936175">
The DIAG experiments:
Natural Language Generation for Intelligent Tutoring Systems
</title>
<author confidence="0.995434">
Barbara Di Eugenio and Michael Glass and Michael J. Trolio
</author>
<affiliation confidence="0.854347333333333">
Computer Science Department
University of Illinois
Chicago, IL, 60607, USA
</affiliation>
<email confidence="0.999194">
{bdieugen,mglass}@cs.uic.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956272727273">
We added a sentence planning component
to an existing ITS that teaches students
how to troubleshoot mechanical systems.
We evaluated the original version of the
system and the enhanced one via a user
study in which we collected performance,
learning and usability metrics. We show
that on the whole the enhanced system is
better than the original one. We discuss
how to use the binomial cumulative distri-
bution to assess cumulative effects.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999796375">
Intelligent Tutoring Systems (ITSs) help students
master a certain topic. Research on the next genera-
tion of ITSs (Evens et al., 1993; Ros´e and Freedman,
2000; Aleven, 2001; Graesser et al., 2001) explores
NL as one of the keys to bridge the gap between cur-
rent ITSs and human tutors (Anderson et al., 1995).
Our work is the first to show that it is the NL in-
teraction that improves students’ learning or at least
the students’ experience with the system. We added
NLG capabilities to an existing ITS. We focused on
sentence planning, and specifically, on aggregation.
We then conducted a systematic evaluation that pit-
ted the original version of the system against the
enhanced one. We show that on the whole the en-
hanced system outperforms the original one.
Our work is also relevant to evaluating NLG sys-
tems in general, as we show how to use the binomial
cumulative distribution function to assess cumula-
tive effects.
We will first discuss DIAG, the ITS we are using,
and the sentence planning component we added to
DIAG. We will then describe the formal evaluation
we conducted. We will conclude by discussing re-
lated work and our current work.
</bodyText>
<sectionHeader confidence="0.794832" genericHeader="method">
2 Language Generation for DIAG
</sectionHeader>
<bodyText confidence="0.999676407407408">
DIAG (Towne, 1997) is a shell to build ITSs that
teach students to troubleshoot complex systems such
as home heating and circuitry. Authors build interac-
tive graphical models of systems, and build lessons
based on these graphical models (see Figure 1).
A DIAG application presents a student with a se-
ries of troubleshooting problems of increasing diffi-
culty. The student tests indicators and tries to infer
which faulty part (RU) may cause the detected ab-
normal states. RU stands for replaceable unit, be-
cause the only course of action for the student to
fix the problem is to replace faulty components in
the graphical simulation. Figure 1 shows the fur-
nace system, one subsystem of the home heating
system in our DIAG application. Figure 1 includes
indicators such as the gauge labeled Water Temper-
ature, replaceable units, and other complex modules
(Oil Burner) that contain indicators and replaceable
units. Complex components are zoomable.
At any point, the student can consult the built-in
tutor via the Consult menu (cf. the Consult button
in Figure 1). For example, if an indicator shows an
abnormal reading, s/he can ask the tutor for a hint
regarding which RUs may cause the problem. After
deciding which content to communicate, the original
DIAG system (DIAG-orig) uses very simple tem-
plates to assemble the text to present to the student.
</bodyText>
<figureCaption confidence="0.999136">
Figure 1: A screen from a DIAG application on home heating
</figureCaption>
<bodyText confidence="0.9999655">
As a result, the feedback provided by DIAG-orig is
repetitive, both inter- and intra-turn. In many cases,
the feedback presents a long list of parts. The top
part of Figure 2 shows the reply provided by DIAG-
orig to a request of information regarding the indi-
cator named “Visual Combustion Check”.
</bodyText>
<subsectionHeader confidence="0.972185">
2.1 The sentence planner
</subsectionHeader>
<bodyText confidence="0.999932">
We set out to rapidly improve DIAG’s feedback
mechanism. Our main goals were to to assess
whether simple NLG techniques would lead to mea-
surable improvements in the system’s output, and to
conduct a systematic evaluation that would focus on
language only. Thus, we did not change the tutoring
strategy, or alter the interaction between student and
system in any way. Rather, we concentrated on im-
proving each turn by avoiding excessive repetitions.
We chose to achieve this by: introducing syntac-
tic aggregation (Dalianis, 1996; Huang and Fiedler,
1996; Shaw, 1998; Reape and Mellish, 1998) and
what we call functional aggregation, namely, group-
ing parts according to the structure of the system;
and improving the format of the output. The bot-
tom part of Figure 2 shows the revised output pro-
duced by DIAG-NLP. The RUs under discussion are
grouped by the system modules that contain them
(Oil Burner and Furnace System), and by the like-
lihood that a certain RU causes the observed symp-
toms. In contrast to the original answer, the revised
answer singles out the Ignitor Assembly, the only
RU that cannot cause the symptom.
As our sentence planner, we use EXEMPLARS
(White and Caldwell, 1998), an object-oriented, rule
based generator. It mixes template-style and more
sophisticated types of text planning. The rules
(called exemplars) are meant to capture an exem-
plary way of achieving a communicative goal in
a given communicative context. The text planner
selects rules by traversing the exemplar specializa-
tion hierarchy, and evaluating the applicability con-
ditions associated with each exemplar.
In DIAG-NLP, exemplars are of two main types,
description and aggregation / layout. The four de-
scription exemplars are used when the full descrip-
tion of a part is required, such as whether the part is
in a normal state, its current reading, and, if abnor-
mal, what the normal state should be. The eight ag-
gregation exemplars are used to group large lists of
parts into smaller lists. They allow composite aggre-
gation, so that nested lists are created. The topmost
aggregation exemplar is AggByType, which controls
nesting of embedded lists. AggByType has 6 daugh-
ters, among them: AggByType that groups parts by
part type, i.e., separates indicators from RUs; Ag-
gByContainer that accepts a list of parts, classifies
</bodyText>
<table confidence="0.79050003125">
The visual combustion check is igniting which is abnormal in this startup mode (normal
is combusting)
Oil Nozzle always
produces this abnormality when it fails.
Oil Supply Valve always
produces this abnormality when it fails.
Oil pump always
produces this abnormality when it fails.
Oil Filter always
produces this abnormality when it fails.
System Control Module sometimes
produces this abnormality when it fails.
Ignitor Assembly never
produces this abnormality when it fails.
Burner Motor always
produces this abnormality when it fails.
and, maybe others affect this test.
The visual combustion check indicator is igniting which is abnormal in startup mode.
Normal in this mode is combusting.
Within the Oil Burner
These replaceable units always produce this abnormal indication when they fail:
Oil Nozzle;
Oil Supply Valve;
Oil pump;
Oil Filter;
Burner Motor.
The Ignitor assembly replaceable unit never produces this abnormal indication
when it fails.
Within the furnace system,
The System Control Module replaceable unit sometimes produces this abnormal
indication when it fails.
Also, other parts may affect this indicator.
</table>
<figureCaption confidence="0.997764">
Figure 2: Original (top) and enhanced (bottom) answers to the same Consult Indicator query
</figureCaption>
<bodyText confidence="0.998016928571428">
each part by the system module that contains it, and
then creates a set of lists by module; AggByFufer
that groups RUs according to the likelihood of being
at fault for a specific symptom. The eighth aggrega-
tion exemplar deals with formatting, namely, creat-
ing vertical lists, spacing, etc.
The most frequent application of the aggregation
rules is to group parts according to the system mod-
ule they belong to, and within each module, to group
RUs by how likely it is they may cause the observed
symptom, cf. Figure 2.
Figure 3 illustrates a simple exemplar for describ-
ing one indicator within a list of items.1 An exem-
plar is a Java class with an evalConstraints()
</bodyText>
<footnote confidence="0.931744">
1An exemplar that performs aggregation would be more rel-
evant. However, for expository purposes and because of space
constraints we chose a much simpler description exemplar.
</footnote>
<bodyText confidence="0.99979">
method as its applicability condition, and an
apply() method, responsible for generating an
XML representation of the desired text. Portions de-
limited by &lt;&lt;+ and +&gt;&gt; are annotated XML state-
ments; braces invoke subsidiary exemplars for gen-
erating included text.
The implementation took a graduate student six
months. Most of the effort was devoted not to writ-
ing exemplars, but to making DIAG and EXEM-
PLARS communicate.
After a student query, DIAG collects all the infor-
mation it needs to communicate to the student, and
writes it to a text file that is then passed to EXEM-
PLARS. A portion of the text file that DIAG passes
to EXEMPLARS for the example in Figure 2 is as
follows (ConsultIndicatoris the type of query
asked by the student):
</bodyText>
<figure confidence="0.831287076923077">
exemplar DescribeIndicator(Vector lists, int index, String tense)
extends DescribePart
{
boolean evalConstraints() {
return ((Part)lists.elementAt(index) instanceof Indicator);}
void apply() {
Indicator ind = (Indicator)lists.elementAt(index);
&lt;&lt;+The {ind.getName()} indicator {tense} {ind.getState()}+&gt;&gt;
if ((ind.getState()).equals(ind.getNormalState()))
&lt;&lt;+ which is normal in {ind.getMode()} mode.+&gt;&gt;
else
&lt;&lt;+ which is abnormal in {ind.getMode()} mode.ˆNormal in this
mode is {ind.getNormalState()}.+&gt;&gt;}}
</figure>
<figureCaption confidence="0.987572">
Figure 3: Exemplar for describing an indicator
</figureCaption>
<figure confidence="0.987252">
ConsultIndicator Indicator
name Visual combustion check
state igniting
modeName startup
normalState combusting
-- --
ConsultIndicator ReplUnit
name Oil Nozzle
fufer always
-- --
ConsultIndicator ReplUnit
name System Control Module
fufer sometimes
-- --
ConsultIndicator ReplUnit
name Ignitor assembly
</figure>
<figureCaption confidence="0.480647">
fufer no effect
</figureCaption>
<bodyText confidence="0.99985">
The attribute fufer2 represents the strength of
the causal connection between the failure of an RU
and an observed symptom.3 The order of the in-
formation in the text file mirrors the order in which
DIAG assembles the information, which is also di-
rectly mirrored in the feedback provided by DIAG-
orig (see top of Figure 2).
EXEMPLARS performs essentially three tasks:
1) it determines the specific exemplars needed; 2)
it adds the chosen exemplars to the sentence planner
as a goal; 3) it linearizes and lexicalizes the feedback
in its final form, writing it to a file which is passed
back to DIAG for display in the appropriate window.
In this version of DIAG-NLP, morphology, lex-
ical realization and referring expression generation
</bodyText>
<footnote confidence="0.989961">
2The name comes from DIAG.
3These strenghts are entered at development time via the
specialized editors that the DIAG authoring system provides.
</footnote>
<bodyText confidence="0.9979025">
were all directly encoded in the appropriate exem-
plars. We already have a third version of the sys-
tem in which referring expressions are generated in
a principled way, see Section 5.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.996728137931034">
Our empirical evaluation is a between-subject study:
one group interacts with DIAG-orig and the other
with DIAG-NLP. The 34 subjects (17 per group)
were all science or engineering majors affiliated
with our university. Each subject read some short
material about home heating, went through the first
problem as a trial run, then continued through the
curriculum on his/her own. The curriculum consists
of three problems of increasing difficulty. As there
was no time limit, every student solved every prob-
lem. At the end of the experiment, each subject was
administered a questionnaire.
A log was collected for each subject including, for
each problem: whether the problem was solved; to-
tal time, and time spent reading feedback; how many
and which indicators and RUs the subject consults
DIAG about; how many, and which RUs the subject
replaces.
The questionnaire is divided into three parts. The
first part tests the subject’s understanding of the do-
main. Because the questions are open ended, this
part was scored as if grading an essay. The second
part asks the subject to rate the system’s feedback
along four dimensions on a scale from 1 to 5 (see
Table 3). The third part concerns whether subjects
remember their actions, specifically, the RUs they
replaced. We quantify the subjects’ recollections in
terms of precision and recall with respect to the log
that the system collects. In Table 2, we report the F-
</bodyText>
<equation confidence="0.5604055">
measure ((����)PR
��P �R , with , 3 = 1) that smooths pre-
</equation>
<bodyText confidence="0.636354">
cision and recall.
</bodyText>
<subsectionHeader confidence="0.527397">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.995951">
Tables 1, 2, and 3 show the results for the cumula-
tive measures across the three problems (individual
problems show the same trends).
</bodyText>
<table confidence="0.999585333333333">
DIAG-orig DIAG-NLP
Total Time 29.8’ 28.0’
Feedback Time 6.9’ 5.4’
Indicator consultations 11.4 5.9
RU consultations 19.2 18.1
Parts replaced 3.85 3.33
</table>
<tableCaption confidence="0.984014">
Table 1: Performance measures
</tableCaption>
<table confidence="0.999397333333333">
DIAG-orig DIAG-NLP
Essay score 81/100 83/100
RU recollection .72 .63
</table>
<tableCaption confidence="0.980214">
Table 2: Learning and recollection measures
</tableCaption>
<table confidence="0.9997348">
DIAG-orig DIAG-NLP
Usefulness 4.35 4.47
Helped stay on right track 4.35 4.35
Not misleading 4.00 4.12
Conciseness 3.47 3.76
</table>
<tableCaption confidence="0.999498">
Table 3: Usability measures
</tableCaption>
<bodyText confidence="0.999275">
Although individually all but one or two measures
favor DIAG-NLP, differences are not statistically
significant. Indicator consultations comes closest to
significance with a non-significant trend in favor of
DIAG-NLP (Mann-Whitney test, U=98, p=0.11).
We therefore apply the binomial cumulative dis-
tribution function (BCDF) —the one-tailed Sign
Test (Siegel and Castellan, 1988) —to assess
whether DIAG-NLP is better than DIAG-orig. This
test measures the likelihood that DIAG-NLP could
have beat DIAG-orig on m or more out of n inde-
pendent measures under the null hypothesis that the
two systems are equal. This test is insensitive to the
magnitude of differences in each measure, noticing
only which condition represents a win ((Di Eugenio
et al., 2002) discusses the BCDF further).
</bodyText>
<tableCaption confidence="0.959659">
Table 4 combines the independent measures from
Tables 1, 2, and 3, showing which condition was
</tableCaption>
<table confidence="0.999750636363636">
DIAG-orig DIAG-NLP
Total Time p
Indicator consultations p
RU consultations p
Parts replaced p
Essay score p p
RU recollection p
Usefulness p p
Helped stay on right track p
Not misleading p
Conciseness
</table>
<tableCaption confidence="0.997366">
Table 4: Successes for each system
</tableCaption>
<bodyText confidence="0.999900181818182">
more successful. The result shows 9/10 (or 8/10)
wins for DIAG-NLP. Since one measure was tied,
we report two sets of probabilities assuming that the
tied measure favored DIAG-orig or DIAG-NLP re-
spectively.
The probability of 9/10 (or 8/10) successes for
DIAG-NLP under the null hypothesis is p = 0.01
(or 0.054), showing a significant (or marginally
significant) win for DIAG-NLP. If we question
whether Total Time is independent of the other mea-
sures, then p = 0.02 (or 0.09) for 8/9 (or 7/9) wins,
which is at best a statistically significant and at worst
a marginally significant win for DIAG-NLP.
Had we followed the customary practice of dis-
carding the tied measure (Siegel and Castellan,
1988),4 DIAG-NLP would win 8/9, p = 0.02, or
7/8, p = 0.035 (depending on the inclusion of Total
Time), which are both significant.
We can then conclude that the better measures for
DIAG-NLP, albeit individually not statistically sig-
nificant, cumulatively show that DIAG-NLP outper-
forms DIAG-orig.
</bodyText>
<subsectionHeader confidence="0.986079">
3.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999968111111111">
Our work contributes to both evaluating NLG for
ITSs, and evaluating NLG in general.
We developed the set of metrics we collected
partly based on the literature (e.g., time on task),
partly because of their significance for troubleshoot-
ing (e.g., parts replaced). RU recollection, which
measures what students remember of their own ac-
tions, was suggested to us by a colleague in psychol-
ogy as a possible correlate of learning. Among the
</bodyText>
<footnote confidence="0.655572">
4Discarding tied measures appears to be discarding support
for the null hypothesis, so we do not argue for this approach (Di
Eugenio et al., 2002).
</footnote>
<bodyText confidence="0.999948517857143">
measures we collected, only Essay score (and possi-
bly RU recollection) directly addresses learning, the
others pertain to task performance or user satisfac-
tion. ITSs are frequently evaluated in terms of pre-
/post-test scores, where the same test is given to the
student before and after using the ITS. In our case,
the most appropriate pre-/post-test would have been
a troubleshooting problem. However, we felt the bi-
nary measure “problem solved/ not solved” would
be too rough an assessment to be useful, not to men-
tion that this choice would have meant one fewer
problem in the curriculum.
We contend that performance and usability mea-
sures are important for an ITS, as they provide indi-
rect evidence of its effectiveness. For example, the
lower number of indicator and RU consultations in
DIAG-ILP is evidence in favor of the effectiveness
of the aggregated feedback: because the feedback
highlights what is important, subjects can focus their
troubleshooting without asking as many questions of
the system. Equally important is the lower number
of RU replacements. This metric includes the mis-
takes a student makes, i.e., the parts replaced that
are not responsible for the problem. When repairing
a real system, replacing parts that are actually work-
ing should clearly be kept to a minimum. We claim
that an ITS whose NL feedback leads the student
more effectively towards the solution of a problem
is a better ITS, even if students learn as much in ei-
ther version of the ITS. This holds for usability as
well. In a real setting, students should be more will-
ing to sit down with a system that they perceive as
more friendly and usable than a system that engen-
ders similar learning gains, but is harder to use.
Concerning evaluation of NLG in general, a com-
mon way of assessing whether system B is better
than system A is to collect a number of measures,
hoping that there will be at least one statistically sig-
nificant measure in favor of system B and no signif-
icant measure in favor of system A. However, real-
ity is often murkier than this ideal result. A typi-
cal result of an evaluation may be that out of twelve
measures ten favor B and two favor A, but only two
show statistical significance and those two point to
opposite conclusions. The BCDF is an appropriate
way of assessing whether B outperforms A on the
whole. Using the BCDF addresses a different type
of cumulative effect than e.g. PARADISE (Walker
et al., 1997). This comprehensive framework for
dialogue evaluation combines various measures to
yield a cumulative score for each of the systems be-
ing evaluated. However, the cumulative scores are
then arranged in pairs, and their difference tested for
statistical significance. The BCDF is used not to ob-
tain a single score, but to assess what the measures
collectively say on the performance of the system.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.995807428571429">
Our work touches on three issues: aggregation; eval-
uation of NLG systems; and work on evaluating NL
interfaces for ITSs.
Part of our rules implement standard types of ag-
gregation such as simple conjunction and conjunc-
tion via shared participants (Reiter and Dale, 2000).
We also introduced what we call functional aggre-
gation (perhaps a type of conceptual aggregation
(Reape and Mellish, 1998)). Although it introduces
semantic elements that are outside the purview of
syntactic aggregation, it appears to be preferred by
humans over syntactic aggregation (see Section 5).
Evaluation is of great interest for the language
generation community (Dale and Mellish, 1998),
and much progress has been made in the last few
years. Language generation systems have been eval-
uated e.g. by using human judges to assess the qual-
ity of the texts produced (Coch, 1996; Lester and
Porter, 1997; Harvey and Carberry, 1998); by com-
paring the system’s performance to that of humans
(Yeh and Mellish, 1997); or through task efficacy
measures (Young, 1997; Carenini and Moore, 2000;
Reiter et al., 2001). We have shown how different
measures can be combined to assess what they col-
lectively say on the performance of a system.
Regarding evaluation of NL interfaces for ITSs,
no experiment like ours has been published that
compares two versions of the same system, one
of which uses a NL interface.5 For example, the
CIRCSIM-Tutor system (Evens et al., 1993) which
teaches medical physiology has been used with med-
ical students, but it was never evaluated vs. a less-
sophisticated version; the ANDES system which
teaches physics, and its NL version ATLAS (Van-
Lehn et al., 2000) have been evaluated, but only in a
</bodyText>
<footnote confidence="0.839896">
5One relevant experiment is (Trafton et al., 1997), but their
system does not really qualify as an ITS.
</footnote>
<listItem confidence="0.269662">
very small pilot study (Graesser et al., 2001).
5 Current and future work
</listItem>
<bodyText confidence="0.993110261538461">
We are currently pursuing two lines of research.
First, we added some sophistication to our sen-
tence planner. DIAG-NLP2 is a third fully imple-
mented version of the system (Haller et al., 2002).
DIAG-NLP2 incorporates the GNOME algorithm
(Kibble and Power, 2000) to generate referring ex-
pressions, including references to whole proposi-
tions such as This is caused ..., and models a few
rhetorical relations such as contrast and concession.
DIAG-NLP2 couples EXEMPLARS to a knowl-
edge base built via the SNePS representation sys-
tem (Shapiro and Rapaport, 1992). SNePS makes
it easy to represent and reason about entire proposi-
tions, not just about objects.
Second, we have conducted a constrained data
collection to uncover empirical evidence for the
rules we implemented in EXEMPLARS. Doing the
implementation first and then looking for empiri-
cal evidence may appear backwards. As one of our
goals was to rapidly improve DIAG-orig’s output
and evaluate the improvement, we could not wait
for the result of an empirical investigation. In this,
our work follows much work on aggregation (Dalia-
nis, 1996; Huang and Fiedler, 1996; Shaw, 1998), in
which aggregation rules and heuristics are plausible,
but are not based on any hard evidence. Even when
corpus studies are used (Dalianis, 1996; Harvey and
Carberry, 1998), they are not completely convinc-
ing. Aggregation rules could be posited from a cor-
pus only if we knew the underlying representation
the text had been aggregated from, which is usually
not the case. The data collection we conducted was
meant to address this last issue as well.
To understand how a human tutor may verbalize
a collection of facts, we collected 23 tutoring dia-
logues (for a total of 270 tutor turns) between a stu-
dent interacting with the DIAG application on home
heating and a human tutor. The tutor and the stu-
dent are in different rooms, sharing images of the
same DIAG tutoring screen. When the student con-
sults DIAG, the tutor sees the information that DIAG
would use in generating its advice — exactly the
same information that DIAG gives to EXEMPLARS
in DIAG-NLP. The tutor then types a response that
substitutes for DIAG’s response. Although we can-
not constrain the tutor to mention all and only the
facts that DIAG would have communicated, we can
still analyze how the tutor uses the information pro-
vided by DIAG.
We have recently developed a coding scheme and
started annotating the data. As a preliminary obser-
vation, the most striking pattern is that the humans
eschew syntactic aggregation of part lists and instead
describe functional aggregations of parts. This lends
support to our rule that groups parts according to the
system hierarchical structure. For example, the same
assemblage of parts, i.e., oil nozzle, supply valve,
pump, filter, etc, can be described as the other items
on the fuel line or as the path of the oil flow.
Acknowledgements. This work is supported by grants
N00014-99-1-0930 and N00014-00-1-0640 from the Office of
Naval Research, and partly by grant EIA-9802090 from the Na-
tional Science Foundation. We are grateful to CoGenTex Inc.,
in particular to Mike White, for making EXEMPLARS avail-
able to us; and to Michael Scott for suggesting the BCDF.
</bodyText>
<sectionHeader confidence="0.998174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999518741935484">
Vincent Aleven, editor. 2001. Workshop on Tutorial Di-
alogue Systems, San Antonio, TX, May. The Interna-
tional Society of Artificial Intelligence in Education.
John R. Anderson, Albert T. Corbett, Kenneth R.
Koedinger, and R. Pelletier. 1995. Cognitive tutors:
Lessons learned. Journal of the Learning Sciences,
4(2):167–207.
Giuseppe Carenini and Johanna D. Moore. 2000. An em-
pirical study of the influence of argument conciseness
on argument effectiveness. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, Hong Kong.
Jos´e Coch. 1996. Evaluating and comparing three text-
production techniques. In COLING96, Proceedings of
the Sixteenth International Conference on Computa-
tional Linguistics, pages 249–254, Copenhagen, Den-
mark, August.
Robert Dale and Chris Mellish. 1998. Towards the eval-
uation of natural language generation. In Proceedings
of the First International Conference on Language Re-
sources and Evaluation, Grenada, Spain, May.
Hercules Dalianis. 1996. Concise Natural Language
Generation from Formal Specifications. Ph.D. thesis,
Department of Computer and Systems Science, Sto-
cholm UNiversity. Technical Report 96-008.
Barbara Di Eugenio, Michael Glass, and Michael J. Scott.
2002. The binomial cumulative distribution, or, is my
system better than yours? In LREC2002, Proceedings
of the Third International Conference on Language
Resources and Evaluation, Las Palmas, Spain.
Martha W. Evens, John Spitkovsky, Patrick Boyle,
Joel A. Michael, and Allen A. Rovick. 1993. Syn-
thesizing tutorial dialogues. In Proceedings of the Fif-
teenth Annual Conference of the Cognitive Science So-
ciety, pages 137–140, Hillsdale, New Jersey. Lawrence
Erlbaum Associates.
Arthur Graesser, Kurt VanLehn, Carolyn P. Ros´e,
Pamela W. Jordan, and Derek Harter. 2001. Intelli-
gent tutoring systems with conversational dialogue. AI
Magazine, 22(4):39–52.
Susan Haller, Barbara Di Eugenio, and Michael J. Trolio.
2002. Generating natural language aggregations us-
ing a propositional representation of sets. In FLAIRS
2002, the 15th International Florida AI Research Sym-
posium, Pensacola Beach, FL, May.
Terrence Harvey and Sandra Carberry. 1998. Inte-
grating text plans for conciseness and coherence. In
ACL/COLING 98, Proceedings of the 36th Annual
Meeting ofthe Associationfor Computational Linguis-
tics, pages 512–518, Montreal, Canada.
Xiaoron Huang and Armin Fiedler. 1996. Paraphrasing
and aggregating argumentative text using text struc-
ture. In Proceedings of the 8th International Work-
shop on Natural Language Generation, pages 21–30,
Sussex, UK.
Rodger Kibble and Richard Power. 2000. Nominal gen-
eration in GNOME and ICONOCLAST. Technical re-
port, Information Technology Research Institute, Uni-
versity of Brighton, Brighton, UK.
James C. Lester and Bruce W. Porter. 1997. Developing
and empirically evaluating robust explanation genera-
tors: the KNIGHT experiments. Computational Lin-
guistics, 23(1):65–102. Special Issue on Empirical
Studies in Discourse.
Mike Reape and Chris Mellish. 1998. Just what is
aggregation anyway? In Proceedings of the Eu-
ropean Workshop on Natural Language Generation,
Toulouse, France.
Ehud Reiter and Robert Dale. 2000. Building Natu-
ral Language Generation Systems. Studies in Natural
Language Processing. Cambridge University Press.
Ehud Reiter, Roma Robertson, A. Scott Lennox, and
Liesl Osman. 2001. Using a Randomised Con-
trolled Clinical Trial to Evaluate an NLG System. In
ACL-2001, Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics, pages
434–441, Toulouse, France.
Carolyn P. Ros´e and Reva Freedman, editors. 2000.
Building Dialogue Systems for Tutorial Applications
(AAAI Fall Symposium). American Association for Ar-
tificial Intelligence.
Stuart Shapiro and William Rapaport. 1992. The SNePS
Family. Computers and Mathematics with Applica-
tions, Special Issue on Semantic Networks in Artificial
Intelligence, Part 1, 23(2–5).
James Shaw. 1998. Segregatory coordination and ellipsis
in text generation. In Proceedings of the 36th Annual
Meeting ofthe Associationfor Computational Linguis-
tics, pages 1220–1226, Montreal, Canada.
Sidney Siegel and N. John Castellan, Jr. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw
Hill.
Douglas M. Towne. 1997. Approximate reasoning tech-
niques for intelligent diagnostic instruction. Interna-
tional Journal ofArtificial Intelligence in Education.
J. G. Trafton, K. Wauchope, P. Raymond, B. Deubner,
J. Stroup, and E. Marsch. 1997. How natural is natural
language for intelligent tutoring systems? InProceed-
ings ofthe Annual Conference of the Cognitive Science
Society.
K. VanLehn, R. Freedman, P. W. Jordan, C. Murray,
C. Oran, M. Ringenberg, C. P. Ros´e, K. Schultze,
R. Shelby, D. Treacy, A. Weinstein, and M. Winters-
gill. 2000. Fading and deepening: The next steps for
ANDES and other model-tracing tutors. In Proceed-
ings of the Intelligent Tutoring Systems Conference.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,
and Alicia Abella. 1997. PARADISE: A Framework
for Evaluating Spoken Dialogue Agents. In ACL-
EACL97, Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics, pages
271–280.
Michael White and Ted Caldwell. 1998. Exemplars: A
practical, extensible framework for dynamic text gen-
eration. In Proceedings of the Ninth International
Workshop on Natural Language Generation, pages
266–275, Niagara-on-the-Lake, Canada.
Ching-Long Yeh and Chris Mellish. 1997. An empirical
study on the generation of anaphora in Chinese. Com-
putational Linguistics, 23(1):169–190. Special Issue
on Empirical Studies in Discourse.
R. Michael Young. 1997. Generating Descriptions of
Complex Activities. Ph.D. thesis, Intelligent Systems
Program, University of Pittsburgh.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937057">
<title confidence="0.9986105">The DIAG experiments: Natural Language Generation for Intelligent Tutoring Systems</title>
<author confidence="0.999257">Di_Eugenio Glass J</author>
<affiliation confidence="0.9998265">Computer Science University of</affiliation>
<address confidence="0.98579">Chicago, IL, 60607,</address>
<abstract confidence="0.99610975">We added a sentence planning component to an existing ITS that teaches students how to troubleshoot mechanical systems. We evaluated the original version of the system and the enhanced one via a user study in which we collected performance, learning and usability metrics. We show that on the whole the enhanced system is better than the original one. We discuss how to use the binomial cumulative distribution to assess cumulative effects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2001</date>
<booktitle>Workshop on Tutorial Dialogue Systems,</booktitle>
<editor>Vincent Aleven, editor.</editor>
<location>San Antonio, TX,</location>
<marker>2001</marker>
<rawString>Vincent Aleven, editor. 2001. Workshop on Tutorial Dialogue Systems, San Antonio, TX, May. The International Society of Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Anderson</author>
<author>Albert T Corbett</author>
<author>Kenneth R Koedinger</author>
<author>R Pelletier</author>
</authors>
<title>Cognitive tutors: Lessons learned.</title>
<date>1995</date>
<journal>Journal of the Learning Sciences,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="1019" citStr="Anderson et al., 1995" startWordPosition="156" endWordPosition="159">nal version of the system and the enhanced one via a user study in which we collected performance, learning and usability metrics. We show that on the whole the enhanced system is better than the original one. We discuss how to use the binomial cumulative distribution to assess cumulative effects. 1 Introduction Intelligent Tutoring Systems (ITSs) help students master a certain topic. Research on the next generation of ITSs (Evens et al., 1993; Ros´e and Freedman, 2000; Aleven, 2001; Graesser et al., 2001) explores NL as one of the keys to bridge the gap between current ITSs and human tutors (Anderson et al., 1995). Our work is the first to show that it is the NL interaction that improves students’ learning or at least the students’ experience with the system. We added NLG capabilities to an existing ITS. We focused on sentence planning, and specifically, on aggregation. We then conducted a systematic evaluation that pitted the original version of the system against the enhanced one. We show that on the whole the enhanced system outperforms the original one. Our work is also relevant to evaluating NLG systems in general, as we show how to use the binomial cumulative distribution function to assess cumul</context>
</contexts>
<marker>Anderson, Corbett, Koedinger, Pelletier, 1995</marker>
<rawString>John R. Anderson, Albert T. Corbett, Kenneth R. Koedinger, and R. Pelletier. 1995. Cognitive tutors: Lessons learned. Journal of the Learning Sciences, 4(2):167–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Johanna D Moore</author>
</authors>
<title>An empirical study of the influence of argument conciseness on argument effectiveness.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="19384" citStr="Carenini and Moore, 2000" startWordPosition="3138" endWordPosition="3141">hat are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (VanLehn et al., 2000) have</context>
</contexts>
<marker>Carenini, Moore, 2000</marker>
<rawString>Giuseppe Carenini and Johanna D. Moore. 2000. An empirical study of the influence of argument conciseness on argument effectiveness. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Coch</author>
</authors>
<title>Evaluating and comparing three textproduction techniques.</title>
<date>1996</date>
<booktitle>In COLING96, Proceedings of the Sixteenth International Conference on Computational Linguistics,</booktitle>
<pages>249--254</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="19176" citStr="Coch, 1996" startWordPosition="3107" endWordPosition="3108">s (Reiter and Dale, 2000). We also introduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches </context>
</contexts>
<marker>Coch, 1996</marker>
<rawString>Jos´e Coch. 1996. Evaluating and comparing three textproduction techniques. In COLING96, Proceedings of the Sixteenth International Conference on Computational Linguistics, pages 249–254, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Chris Mellish</author>
</authors>
<title>Towards the evaluation of natural language generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<location>Grenada, Spain,</location>
<contexts>
<context position="18989" citStr="Dale and Mellish, 1998" startWordPosition="3071" endWordPosition="3074">n; evaluation of NLG systems; and work on evaluating NL interfaces for ITSs. Part of our rules implement standard types of aggregation such as simple conjunction and conjunction via shared participants (Reiter and Dale, 2000). We also introduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experime</context>
</contexts>
<marker>Dale, Mellish, 1998</marker>
<rawString>Robert Dale and Chris Mellish. 1998. Towards the evaluation of natural language generation. In Proceedings of the First International Conference on Language Resources and Evaluation, Grenada, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hercules Dalianis</author>
</authors>
<title>Concise Natural Language Generation from Formal Specifications.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Systems Science, Stocholm UNiversity.</institution>
<contexts>
<context position="4152" citStr="Dalianis, 1996" startWordPosition="682" endWordPosition="683">tion regarding the indicator named “Visual Combustion Check”. 2.1 The sentence planner We set out to rapidly improve DIAG’s feedback mechanism. Our main goals were to to assess whether simple NLG techniques would lead to measurable improvements in the system’s output, and to conduct a systematic evaluation that would focus on language only. Thus, we did not change the tutoring strategy, or alter the interaction between student and system in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998; Reape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As </context>
<context position="21291" citStr="Dalianis, 1996" startWordPosition="3454" endWordPosition="3456">se built via the SNePS representation system (Shapiro and Rapaport, 1992). SNePS makes it easy to represent and reason about entire propositions, not just about objects. Second, we have conducted a constrained data collection to uncover empirical evidence for the rules we implemented in EXEMPLARS. Doing the implementation first and then looking for empirical evidence may appear backwards. As one of our goals was to rapidly improve DIAG-orig’s output and evaluate the improvement, we could not wait for the result of an empirical investigation. In this, our work follows much work on aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998), in which aggregation rules and heuristics are plausible, but are not based on any hard evidence. Even when corpus studies are used (Dalianis, 1996; Harvey and Carberry, 1998), they are not completely convincing. Aggregation rules could be posited from a corpus only if we knew the underlying representation the text had been aggregated from, which is usually not the case. The data collection we conducted was meant to address this last issue as well. To understand how a human tutor may verbalize a collection of facts, we collected 23 tutoring dialogues (for</context>
</contexts>
<marker>Dalianis, 1996</marker>
<rawString>Hercules Dalianis. 1996. Concise Natural Language Generation from Formal Specifications. Ph.D. thesis, Department of Computer and Systems Science, Stocholm UNiversity. Technical Report 96-008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
<author>Michael J Scott</author>
</authors>
<title>The binomial cumulative distribution, or, is my system better than yours?</title>
<date>2002</date>
<booktitle>In LREC2002, Proceedings of the Third International Conference on Language Resources and Evaluation,</booktitle>
<location>Las Palmas,</location>
<marker>Di Eugenio, Glass, Scott, 2002</marker>
<rawString>Barbara Di Eugenio, Michael Glass, and Michael J. Scott. 2002. The binomial cumulative distribution, or, is my system better than yours? In LREC2002, Proceedings of the Third International Conference on Language Resources and Evaluation, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha W Evens</author>
<author>John Spitkovsky</author>
<author>Patrick Boyle</author>
<author>Joel A Michael</author>
<author>Allen A Rovick</author>
</authors>
<title>Synthesizing tutorial dialogues.</title>
<date>1993</date>
<booktitle>In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>137--140</pages>
<location>Hillsdale, New Jersey. Lawrence Erlbaum Associates.</location>
<contexts>
<context position="844" citStr="Evens et al., 1993" startWordPosition="124" endWordPosition="127">,mglass}@cs.uic.edu Abstract We added a sentence planning component to an existing ITS that teaches students how to troubleshoot mechanical systems. We evaluated the original version of the system and the enhanced one via a user study in which we collected performance, learning and usability metrics. We show that on the whole the enhanced system is better than the original one. We discuss how to use the binomial cumulative distribution to assess cumulative effects. 1 Introduction Intelligent Tutoring Systems (ITSs) help students master a certain topic. Research on the next generation of ITSs (Evens et al., 1993; Ros´e and Freedman, 2000; Aleven, 2001; Graesser et al., 2001) explores NL as one of the keys to bridge the gap between current ITSs and human tutors (Anderson et al., 1995). Our work is the first to show that it is the NL interaction that improves students’ learning or at least the students’ experience with the system. We added NLG capabilities to an existing ITS. We focused on sentence planning, and specifically, on aggregation. We then conducted a systematic evaluation that pitted the original version of the system against the enhanced one. We show that on the whole the enhanced system ou</context>
<context position="19761" citStr="Evens et al., 1993" startWordPosition="3202" endWordPosition="3205"> of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (VanLehn et al., 2000) have been evaluated, but only in a 5One relevant experiment is (Trafton et al., 1997), but their system does not really qualify as an ITS. very small pilot study (Graesser et al., 2001). 5 Current and future work We are currently pursuing two lines of research. First, we added some sophistication to our sentence planner. DIAG-NLP2 is a third fully implemented version of the syst</context>
</contexts>
<marker>Evens, Spitkovsky, Boyle, Michael, Rovick, 1993</marker>
<rawString>Martha W. Evens, John Spitkovsky, Patrick Boyle, Joel A. Michael, and Allen A. Rovick. 1993. Synthesizing tutorial dialogues. In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, pages 137–140, Hillsdale, New Jersey. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Graesser</author>
<author>Kurt VanLehn</author>
<author>Carolyn P Ros´e</author>
<author>Pamela W Jordan</author>
<author>Derek Harter</author>
</authors>
<title>Intelligent tutoring systems with conversational dialogue.</title>
<date>2001</date>
<journal>AI Magazine,</journal>
<volume>22</volume>
<issue>4</issue>
<marker>Graesser, VanLehn, Ros´e, Jordan, Harter, 2001</marker>
<rawString>Arthur Graesser, Kurt VanLehn, Carolyn P. Ros´e, Pamela W. Jordan, and Derek Harter. 2001. Intelligent tutoring systems with conversational dialogue. AI Magazine, 22(4):39–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Haller</author>
<author>Barbara Di Eugenio</author>
<author>Michael J Trolio</author>
</authors>
<title>Generating natural language aggregations using a propositional representation of sets.</title>
<date>2002</date>
<booktitle>In FLAIRS 2002, the 15th International Florida AI Research Symposium,</booktitle>
<location>Pensacola Beach, FL,</location>
<marker>Haller, Di Eugenio, Trolio, 2002</marker>
<rawString>Susan Haller, Barbara Di Eugenio, and Michael J. Trolio. 2002. Generating natural language aggregations using a propositional representation of sets. In FLAIRS 2002, the 15th International Florida AI Research Symposium, Pensacola Beach, FL, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terrence Harvey</author>
<author>Sandra Carberry</author>
</authors>
<title>Integrating text plans for conciseness and coherence.</title>
<date>1998</date>
<booktitle>In ACL/COLING 98, Proceedings of the 36th Annual Meeting ofthe Associationfor Computational Linguistics,</booktitle>
<pages>512--518</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="19229" citStr="Harvey and Carberry, 1998" startWordPosition="3113" endWordPosition="3116">troduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical student</context>
<context position="21505" citStr="Harvey and Carberry, 1998" startWordPosition="3487" endWordPosition="3490">trained data collection to uncover empirical evidence for the rules we implemented in EXEMPLARS. Doing the implementation first and then looking for empirical evidence may appear backwards. As one of our goals was to rapidly improve DIAG-orig’s output and evaluate the improvement, we could not wait for the result of an empirical investigation. In this, our work follows much work on aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998), in which aggregation rules and heuristics are plausible, but are not based on any hard evidence. Even when corpus studies are used (Dalianis, 1996; Harvey and Carberry, 1998), they are not completely convincing. Aggregation rules could be posited from a corpus only if we knew the underlying representation the text had been aggregated from, which is usually not the case. The data collection we conducted was meant to address this last issue as well. To understand how a human tutor may verbalize a collection of facts, we collected 23 tutoring dialogues (for a total of 270 tutor turns) between a student interacting with the DIAG application on home heating and a human tutor. The tutor and the student are in different rooms, sharing images of the same DIAG tutoring scr</context>
</contexts>
<marker>Harvey, Carberry, 1998</marker>
<rawString>Terrence Harvey and Sandra Carberry. 1998. Integrating text plans for conciseness and coherence. In ACL/COLING 98, Proceedings of the 36th Annual Meeting ofthe Associationfor Computational Linguistics, pages 512–518, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoron Huang</author>
<author>Armin Fiedler</author>
</authors>
<title>Paraphrasing and aggregating argumentative text using text structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the 8th International Workshop on Natural Language Generation,</booktitle>
<pages>21--30</pages>
<location>Sussex, UK.</location>
<contexts>
<context position="4177" citStr="Huang and Fiedler, 1996" startWordPosition="684" endWordPosition="687">he indicator named “Visual Combustion Check”. 2.1 The sentence planner We set out to rapidly improve DIAG’s feedback mechanism. Our main goals were to to assess whether simple NLG techniques would lead to measurable improvements in the system’s output, and to conduct a systematic evaluation that would focus on language only. Thus, we did not change the tutoring strategy, or alter the interaction between student and system in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998; Reape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we </context>
<context position="21316" citStr="Huang and Fiedler, 1996" startWordPosition="3457" endWordPosition="3460"> SNePS representation system (Shapiro and Rapaport, 1992). SNePS makes it easy to represent and reason about entire propositions, not just about objects. Second, we have conducted a constrained data collection to uncover empirical evidence for the rules we implemented in EXEMPLARS. Doing the implementation first and then looking for empirical evidence may appear backwards. As one of our goals was to rapidly improve DIAG-orig’s output and evaluate the improvement, we could not wait for the result of an empirical investigation. In this, our work follows much work on aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998), in which aggregation rules and heuristics are plausible, but are not based on any hard evidence. Even when corpus studies are used (Dalianis, 1996; Harvey and Carberry, 1998), they are not completely convincing. Aggregation rules could be posited from a corpus only if we knew the underlying representation the text had been aggregated from, which is usually not the case. The data collection we conducted was meant to address this last issue as well. To understand how a human tutor may verbalize a collection of facts, we collected 23 tutoring dialogues (for a total of 270 tutor tur</context>
</contexts>
<marker>Huang, Fiedler, 1996</marker>
<rawString>Xiaoron Huang and Armin Fiedler. 1996. Paraphrasing and aggregating argumentative text using text structure. In Proceedings of the 8th International Workshop on Natural Language Generation, pages 21–30, Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodger Kibble</author>
<author>Richard Power</author>
</authors>
<title>Nominal generation in GNOME and ICONOCLAST.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Information Technology Research Institute, University of Brighton,</institution>
<location>Brighton, UK.</location>
<contexts>
<context position="20454" citStr="Kibble and Power, 2000" startWordPosition="3319" endWordPosition="3322"> but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (VanLehn et al., 2000) have been evaluated, but only in a 5One relevant experiment is (Trafton et al., 1997), but their system does not really qualify as an ITS. very small pilot study (Graesser et al., 2001). 5 Current and future work We are currently pursuing two lines of research. First, we added some sophistication to our sentence planner. DIAG-NLP2 is a third fully implemented version of the system (Haller et al., 2002). DIAG-NLP2 incorporates the GNOME algorithm (Kibble and Power, 2000) to generate referring expressions, including references to whole propositions such as This is caused ..., and models a few rhetorical relations such as contrast and concession. DIAG-NLP2 couples EXEMPLARS to a knowledge base built via the SNePS representation system (Shapiro and Rapaport, 1992). SNePS makes it easy to represent and reason about entire propositions, not just about objects. Second, we have conducted a constrained data collection to uncover empirical evidence for the rules we implemented in EXEMPLARS. Doing the implementation first and then looking for empirical evidence may app</context>
</contexts>
<marker>Kibble, Power, 2000</marker>
<rawString>Rodger Kibble and Richard Power. 2000. Nominal generation in GNOME and ICONOCLAST. Technical report, Information Technology Research Institute, University of Brighton, Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James C Lester</author>
<author>Bruce W Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: the KNIGHT experiments.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<note>Special Issue on Empirical Studies in Discourse.</note>
<contexts>
<context position="19201" citStr="Lester and Porter, 1997" startWordPosition="3109" endWordPosition="3112">d Dale, 2000). We also introduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has be</context>
</contexts>
<marker>Lester, Porter, 1997</marker>
<rawString>James C. Lester and Bruce W. Porter. 1997. Developing and empirically evaluating robust explanation generators: the KNIGHT experiments. Computational Linguistics, 23(1):65–102. Special Issue on Empirical Studies in Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
<author>Chris Mellish</author>
</authors>
<title>Just what is aggregation anyway?</title>
<date>1998</date>
<booktitle>In Proceedings of the European Workshop on Natural Language Generation,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="4215" citStr="Reape and Mellish, 1998" startWordPosition="690" endWordPosition="693"> Check”. 2.1 The sentence planner We set out to rapidly improve DIAG’s feedback mechanism. Our main goals were to to assess whether simple NLG techniques would lead to measurable improvements in the system’s output, and to conduct a systematic evaluation that would focus on language only. Thus, we did not change the tutoring strategy, or alter the interaction between student and system in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998; Reape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we use EXEMPLARS (White and Caldwell, 199</context>
<context position="18715" citStr="Reape and Mellish, 1998" startWordPosition="3031" endWordPosition="3034">are then arranged in pairs, and their difference tested for statistical significance. The BCDF is used not to obtain a single score, but to assess what the measures collectively say on the performance of the system. 4 Related work Our work touches on three issues: aggregation; evaluation of NLG systems; and work on evaluating NL interfaces for ITSs. Part of our rules implement standard types of aggregation such as simple conjunction and conjunction via shared participants (Reiter and Dale, 2000). We also introduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or </context>
</contexts>
<marker>Reape, Mellish, 1998</marker>
<rawString>Mike Reape and Chris Mellish. 1998. Just what is aggregation anyway? In Proceedings of the European Workshop on Natural Language Generation, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems. Studies in Natural Language Processing.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18591" citStr="Reiter and Dale, 2000" startWordPosition="3012" endWordPosition="3015">ines various measures to yield a cumulative score for each of the systems being evaluated. However, the cumulative scores are then arranged in pairs, and their difference tested for statistical significance. The BCDF is used not to obtain a single score, but to assess what the measures collectively say on the performance of the system. 4 Related work Our work touches on three issues: aggregation; evaluation of NLG systems; and work on evaluating NL interfaces for ITSs. Part of our rules implement standard types of aggregation such as simple conjunction and conjunction via shared participants (Reiter and Dale, 2000). We also introduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Po</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Studies in Natural Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Roma Robertson</author>
<author>A Scott Lennox</author>
<author>Liesl Osman</author>
</authors>
<title>Using a Randomised Controlled Clinical Trial to Evaluate an NLG System. In</title>
<date>2001</date>
<booktitle>ACL-2001, Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>434--441</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="19406" citStr="Reiter et al., 2001" startWordPosition="3142" endWordPosition="3145">w of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (VanLehn et al., 2000) have been evaluated, but o</context>
</contexts>
<marker>Reiter, Robertson, Lennox, Osman, 2001</marker>
<rawString>Ehud Reiter, Roma Robertson, A. Scott Lennox, and Liesl Osman. 2001. Using a Randomised Controlled Clinical Trial to Evaluate an NLG System. In ACL-2001, Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 434–441, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn P Ros´e</author>
<author>Reva Freedman</author>
<author>editors</author>
</authors>
<date>2000</date>
<booktitle>Building Dialogue Systems for Tutorial Applications (AAAI Fall Symposium). American Association for Artificial Intelligence.</booktitle>
<marker>Ros´e, Freedman, editors, 2000</marker>
<rawString>Carolyn P. Ros´e and Reva Freedman, editors. 2000. Building Dialogue Systems for Tutorial Applications (AAAI Fall Symposium). American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shapiro</author>
<author>William Rapaport</author>
</authors>
<title>The SNePS Family.</title>
<date>1992</date>
<journal>Computers and Mathematics with Applications, Special Issue on Semantic Networks in Artificial Intelligence, Part</journal>
<volume>1</volume>
<pages>23--2</pages>
<contexts>
<context position="20750" citStr="Shapiro and Rapaport, 1992" startWordPosition="3366" endWordPosition="3369"> small pilot study (Graesser et al., 2001). 5 Current and future work We are currently pursuing two lines of research. First, we added some sophistication to our sentence planner. DIAG-NLP2 is a third fully implemented version of the system (Haller et al., 2002). DIAG-NLP2 incorporates the GNOME algorithm (Kibble and Power, 2000) to generate referring expressions, including references to whole propositions such as This is caused ..., and models a few rhetorical relations such as contrast and concession. DIAG-NLP2 couples EXEMPLARS to a knowledge base built via the SNePS representation system (Shapiro and Rapaport, 1992). SNePS makes it easy to represent and reason about entire propositions, not just about objects. Second, we have conducted a constrained data collection to uncover empirical evidence for the rules we implemented in EXEMPLARS. Doing the implementation first and then looking for empirical evidence may appear backwards. As one of our goals was to rapidly improve DIAG-orig’s output and evaluate the improvement, we could not wait for the result of an empirical investigation. In this, our work follows much work on aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998), in which aggregatio</context>
</contexts>
<marker>Shapiro, Rapaport, 1992</marker>
<rawString>Stuart Shapiro and William Rapaport. 1992. The SNePS Family. Computers and Mathematics with Applications, Special Issue on Semantic Networks in Artificial Intelligence, Part 1, 23(2–5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
</authors>
<title>Segregatory coordination and ellipsis in text generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting ofthe Associationfor Computational Linguistics,</booktitle>
<pages>1220--1226</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4189" citStr="Shaw, 1998" startWordPosition="688" endWordPosition="689">l Combustion Check”. 2.1 The sentence planner We set out to rapidly improve DIAG’s feedback mechanism. Our main goals were to to assess whether simple NLG techniques would lead to measurable improvements in the system’s output, and to conduct a systematic evaluation that would focus on language only. Thus, we did not change the tutoring strategy, or alter the interaction between student and system in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998; Reape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we use EXEMPLAR</context>
<context position="21329" citStr="Shaw, 1998" startWordPosition="3461" endWordPosition="3462">tem (Shapiro and Rapaport, 1992). SNePS makes it easy to represent and reason about entire propositions, not just about objects. Second, we have conducted a constrained data collection to uncover empirical evidence for the rules we implemented in EXEMPLARS. Doing the implementation first and then looking for empirical evidence may appear backwards. As one of our goals was to rapidly improve DIAG-orig’s output and evaluate the improvement, we could not wait for the result of an empirical investigation. In this, our work follows much work on aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998), in which aggregation rules and heuristics are plausible, but are not based on any hard evidence. Even when corpus studies are used (Dalianis, 1996; Harvey and Carberry, 1998), they are not completely convincing. Aggregation rules could be posited from a corpus only if we knew the underlying representation the text had been aggregated from, which is usually not the case. The data collection we conducted was meant to address this last issue as well. To understand how a human tutor may verbalize a collection of facts, we collected 23 tutoring dialogues (for a total of 270 tutor turns) between a</context>
</contexts>
<marker>Shaw, 1998</marker>
<rawString>James Shaw. 1998. Segregatory coordination and ellipsis in text generation. In Proceedings of the 36th Annual Meeting ofthe Associationfor Computational Linguistics, pages 1220–1226, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan</author>
</authors>
<title>Nonparametric statistics for the behavioral sciences.</title>
<date>1988</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="13149" citStr="Siegel and Castellan, 1988" startWordPosition="2096" endWordPosition="2099">LP Essay score 81/100 83/100 RU recollection .72 .63 Table 2: Learning and recollection measures DIAG-orig DIAG-NLP Usefulness 4.35 4.47 Helped stay on right track 4.35 4.35 Not misleading 4.00 4.12 Conciseness 3.47 3.76 Table 3: Usability measures Although individually all but one or two measures favor DIAG-NLP, differences are not statistically significant. Indicator consultations comes closest to significance with a non-significant trend in favor of DIAG-NLP (Mann-Whitney test, U=98, p=0.11). We therefore apply the binomial cumulative distribution function (BCDF) —the one-tailed Sign Test (Siegel and Castellan, 1988) —to assess whether DIAG-NLP is better than DIAG-orig. This test measures the likelihood that DIAG-NLP could have beat DIAG-orig on m or more out of n independent measures under the null hypothesis that the two systems are equal. This test is insensitive to the magnitude of differences in each measure, noticing only which condition represents a win ((Di Eugenio et al., 2002) discusses the BCDF further). Table 4 combines the independent measures from Tables 1, 2, and 3, showing which condition was DIAG-orig DIAG-NLP Total Time p Indicator consultations p RU consultations p Parts replaced p Essa</context>
<context position="14587" citStr="Siegel and Castellan, 1988" startWordPosition="2336" endWordPosition="2339"> one measure was tied, we report two sets of probabilities assuming that the tied measure favored DIAG-orig or DIAG-NLP respectively. The probability of 9/10 (or 8/10) successes for DIAG-NLP under the null hypothesis is p = 0.01 (or 0.054), showing a significant (or marginally significant) win for DIAG-NLP. If we question whether Total Time is independent of the other measures, then p = 0.02 (or 0.09) for 8/9 (or 7/9) wins, which is at best a statistically significant and at worst a marginally significant win for DIAG-NLP. Had we followed the customary practice of discarding the tied measure (Siegel and Castellan, 1988),4 DIAG-NLP would win 8/9, p = 0.02, or 7/8, p = 0.035 (depending on the inclusion of Total Time), which are both significant. We can then conclude that the better measures for DIAG-NLP, albeit individually not statistically significant, cumulatively show that DIAG-NLP outperforms DIAG-orig. 3.2 Discussion Our work contributes to both evaluating NLG for ITSs, and evaluating NLG in general. We developed the set of metrics we collected partly based on the literature (e.g., time on task), partly because of their significance for troubleshooting (e.g., parts replaced). RU recollection, which measu</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Sidney Siegel and N. John Castellan, Jr. 1988. Nonparametric statistics for the behavioral sciences. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas M Towne</author>
</authors>
<title>Approximate reasoning techniques for intelligent diagnostic instruction.</title>
<date>1997</date>
<journal>International Journal ofArtificial Intelligence in Education.</journal>
<contexts>
<context position="1911" citStr="Towne, 1997" startWordPosition="311" endWordPosition="312">a systematic evaluation that pitted the original version of the system against the enhanced one. We show that on the whole the enhanced system outperforms the original one. Our work is also relevant to evaluating NLG systems in general, as we show how to use the binomial cumulative distribution function to assess cumulative effects. We will first discuss DIAG, the ITS we are using, and the sentence planning component we added to DIAG. We will then describe the formal evaluation we conducted. We will conclude by discussing related work and our current work. 2 Language Generation for DIAG DIAG (Towne, 1997) is a shell to build ITSs that teach students to troubleshoot complex systems such as home heating and circuitry. Authors build interactive graphical models of systems, and build lessons based on these graphical models (see Figure 1). A DIAG application presents a student with a series of troubleshooting problems of increasing difficulty. The student tests indicators and tries to infer which faulty part (RU) may cause the detected abnormal states. RU stands for replaceable unit, because the only course of action for the student to fix the problem is to replace faulty components in the graphica</context>
</contexts>
<marker>Towne, 1997</marker>
<rawString>Douglas M. Towne. 1997. Approximate reasoning techniques for intelligent diagnostic instruction. International Journal ofArtificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Trafton</author>
<author>K Wauchope</author>
<author>P Raymond</author>
<author>B Deubner</author>
<author>J Stroup</author>
<author>E Marsch</author>
</authors>
<title>How natural is natural language for intelligent tutoring systems?</title>
<date>1997</date>
<booktitle>InProceedings ofthe Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="20065" citStr="Trafton et al., 1997" startWordPosition="3254" endWordPosition="3257"> can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (VanLehn et al., 2000) have been evaluated, but only in a 5One relevant experiment is (Trafton et al., 1997), but their system does not really qualify as an ITS. very small pilot study (Graesser et al., 2001). 5 Current and future work We are currently pursuing two lines of research. First, we added some sophistication to our sentence planner. DIAG-NLP2 is a third fully implemented version of the system (Haller et al., 2002). DIAG-NLP2 incorporates the GNOME algorithm (Kibble and Power, 2000) to generate referring expressions, including references to whole propositions such as This is caused ..., and models a few rhetorical relations such as contrast and concession. DIAG-NLP2 couples EXEMPLARS to a </context>
</contexts>
<marker>Trafton, Wauchope, Raymond, Deubner, Stroup, Marsch, 1997</marker>
<rawString>J. G. Trafton, K. Wauchope, P. Raymond, B. Deubner, J. Stroup, and E. Marsch. 1997. How natural is natural language for intelligent tutoring systems? InProceedings ofthe Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>R Freedman</author>
<author>P W Jordan</author>
<author>C Murray</author>
<author>C Oran</author>
<author>M Ringenberg</author>
<author>C P Ros´e</author>
<author>K Schultze</author>
<author>R Shelby</author>
<author>D Treacy</author>
<author>A Weinstein</author>
<author>M Wintersgill</author>
</authors>
<title>Fading and deepening: The next steps for ANDES and other model-tracing tutors.</title>
<date>2000</date>
<booktitle>In Proceedings of the Intelligent Tutoring Systems Conference.</booktitle>
<marker>VanLehn, Freedman, Jordan, Murray, Oran, Ringenberg, Ros´e, Schultze, Shelby, Treacy, Weinstein, Wintersgill, 2000</marker>
<rawString>K. VanLehn, R. Freedman, P. W. Jordan, C. Murray, C. Oran, M. Ringenberg, C. P. Ros´e, K. Schultze, R. Shelby, D. Treacy, A. Weinstein, and M. Wintersgill. 2000. Fading and deepening: The next steps for ANDES and other model-tracing tutors. In Proceedings of the Intelligent Tutoring Systems Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A Framework for Evaluating Spoken Dialogue Agents.</title>
<date>1997</date>
<booktitle>In ACLEACL97, Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--280</pages>
<contexts>
<context position="17910" citStr="Walker et al., 1997" startWordPosition="2902" endWordPosition="2905">an system A is to collect a number of measures, hoping that there will be at least one statistically significant measure in favor of system B and no significant measure in favor of system A. However, reality is often murkier than this ideal result. A typical result of an evaluation may be that out of twelve measures ten favor B and two favor A, but only two show statistical significance and those two point to opposite conclusions. The BCDF is an appropriate way of assessing whether B outperforms A on the whole. Using the BCDF addresses a different type of cumulative effect than e.g. PARADISE (Walker et al., 1997). This comprehensive framework for dialogue evaluation combines various measures to yield a cumulative score for each of the systems being evaluated. However, the cumulative scores are then arranged in pairs, and their difference tested for statistical significance. The BCDF is used not to obtain a single score, but to assess what the measures collectively say on the performance of the system. 4 Related work Our work touches on three issues: aggregation; evaluation of NLG systems; and work on evaluating NL interfaces for ITSs. Part of our rules implement standard types of aggregation such as s</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997. PARADISE: A Framework for Evaluating Spoken Dialogue Agents. In ACLEACL97, Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 271–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Ted Caldwell</author>
</authors>
<title>Exemplars: A practical, extensible framework for dynamic text generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the Ninth International Workshop on Natural Language Generation,</booktitle>
<pages>266--275</pages>
<location>Niagara-on-the-Lake, Canada.</location>
<contexts>
<context position="4817" citStr="White and Caldwell, 1998" startWordPosition="794" endWordPosition="797">eape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we use EXEMPLARS (White and Caldwell, 1998), an object-oriented, rule based generator. It mixes template-style and more sophisticated types of text planning. The rules (called exemplars) are meant to capture an exemplary way of achieving a communicative goal in a given communicative context. The text planner selects rules by traversing the exemplar specialization hierarchy, and evaluating the applicability conditions associated with each exemplar. In DIAG-NLP, exemplars are of two main types, description and aggregation / layout. The four description exemplars are used when the full description of a part is required, such as whether th</context>
</contexts>
<marker>White, Caldwell, 1998</marker>
<rawString>Michael White and Ted Caldwell. 1998. Exemplars: A practical, extensible framework for dynamic text generation. In Proceedings of the Ninth International Workshop on Natural Language Generation, pages 266–275, Niagara-on-the-Lake, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Chris Mellish</author>
</authors>
<title>An empirical study on the generation of anaphora in Chinese.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<note>Special Issue on Empirical Studies in Discourse.</note>
<contexts>
<context position="19310" citStr="Yeh and Mellish, 1997" startWordPosition="3127" endWordPosition="3130">n (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system w</context>
</contexts>
<marker>Yeh, Mellish, 1997</marker>
<rawString>Ching-Long Yeh and Chris Mellish. 1997. An empirical study on the generation of anaphora in Chinese. Computational Linguistics, 23(1):169–190. Special Issue on Empirical Studies in Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Michael Young</author>
</authors>
<title>Generating Descriptions of Complex Activities.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="19358" citStr="Young, 1997" startWordPosition="3136" endWordPosition="3137">ic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (</context>
</contexts>
<marker>Young, 1997</marker>
<rawString>R. Michael Young. 1997. Generating Descriptions of Complex Activities. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>