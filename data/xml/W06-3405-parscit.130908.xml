<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.525483">
<title confidence="0.984914">
Shallow Discourse Structure for Action Item Detection
</title>
<author confidence="0.998379">
Matthew Purver, Patrick Ehlen, and John Niekrasz
</author>
<affiliation confidence="0.9692195">
Center for the Study of Language and Information
Stanford University
</affiliation>
<address confidence="0.866633">
Stanford, CA 94305
</address>
<email confidence="0.997838">
{mpurver,ehlen,niekrasz}@stanford.edu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992">
We investigated automatic action item
detection from transcripts of multi-party
meetings. Unlike previous work (Gruen-
stein et al., 2005), we use a new hierarchi-
cal annotation scheme based on the roles
utterances play in the action item assign-
ment process, and propose an approach
to automatic detection that promises im-
proved classification accuracy while en-
abling the extraction of useful information
for summarization and reporting.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998631875">
Action items are specific kinds of decisions common
in multi-party meetings, characterized by the con-
crete assignment of tasks together with certain prop-
erties such as an associated timeframe and reponsi-
ble party. Our aims are firstly to automatically de-
tect the regions of discourse which establish action
items, so their surface form can be used for a tar-
geted report or summary; and secondly, to identify
the important properties of the action items (such as
the associated tasks and deadlines) that would fos-
ter concise and informative semantically-based re-
porting (for example, adding task specifications to a
user’s to-do list). We believe both of these aims are
facilitated by taking into account the roles different
utterances play in the decision-making process – in
short, a shallow notion of discourse structure.
</bodyText>
<page confidence="0.999219">
31
</page>
<sectionHeader confidence="0.994602" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999527">
Related Work Corston-Oliver et al. (2004) at-
tempted to identify action items in e-mails, using
classifiers trained on annotations of individual sen-
tences within each e-mail. Sentences were anno-
tated with one of a set of “dialogue” act classes; one
class Task included any sentence containing items
that seemed appropriate to add to an ongoing to-
do list. They report good inter-annotator agreement
over their general tagging exercise (n &gt; 0.8), al-
though individual figures for the Task class are not
given. They then concentrated on Task sentences,
establishing a set of predictive features (in which
word n-grams emerged as “highly predictive”) and
achieved reasonable per-sentence classification per-
formance (with f-scores around 0.6).
While there are related tags for dialogue act tag-
ging schema – like DAMSL (Core and Allen, 1997),
which includes tags such as Action-Directive
and Commit, and the ICSI MRDA schema
(Shriberg et al., 2004) which includes a commit
tag – these classes are too general to allow iden-
tification of action items specifically. One compa-
rable attempt in spoken discourse took a flat ap-
proach, annotating utterances as action-item-related
or not (Gruenstein et al., 2005) over the ICSI and
ISL meeting corpora (Janin et al., 2003; Burger et
al., 2002). Their inter-annotator agreement was low
(n = .36). While this may have been partly due
to their methods, it is notable that (Core and Allen,
1997) reported even lower agreement (n = .15) on
their Commit dialogue acts. Morgan et al. (forth-
coming) then used these annotations to attempt auto-
</bodyText>
<subsectionHeader confidence="0.786019">
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31–34,
New York City, New York, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999924958333334">
matic classification, but achieved poor performance
(with f-scores around 0.3 at best).
Action Items Action items typically embody the
transfer of group responsibility to an individual.
This need not be the person who actually performs
the action (they might delegate the task to a subor-
dinate), but publicly commits to seeing that the ac-
tion is carried out; we call this person the owner of
the action item. Because this action is a social ac-
tion that is coordinated by more than one person,
its initiation is reinforced by agreement and uptake
among the owner and other participants that the ac-
tion should and will be done. And to distinguish
this action from immediate actions that occur during
the meeting and from more vague future actions that
are still in the planning stage, an action item will be
specified as expected to be carried out within a time-
frame that begins at some point after the meeting and
extends no further than the not-too-distant future. So
an action item, as a type of social action, often com-
prises four components: a task description, a time-
frame, an owner, and a round of agreement among
the owner and others. The related discourse tends to
reflect this, and we attempt to exploit this fact here.
</bodyText>
<sectionHeader confidence="0.994514" genericHeader="method">
3 Baseline Experiments
</sectionHeader>
<bodyText confidence="0.999994962962963">
We applied Gruenstein et al. (2005)’s flat annotation
schema to transcripts from a sequence of 5 short re-
lated meetings with 3 participants recorded as part
of the CALO project. Each meeting was simulated
in that its participants were given a scenario, but
was not scripted. In order to avoid entirely data-
or scenario-specific results (and also to provide an
acceptable amount of training data), we then added
a random selection of 6 ICSI and 1 ISL meetings
from Gruenstein et al. (2005)’s annotations. Like
(Corston-Oliver et al., 2004) we used support vec-
tor machines (Vapnik, 1995) via the classifier SVM-
light (Joachims, 1999). Their full set of features are
not available to us, but we experimented with com-
binations of words and n-grams and assessed classi-
fication performance via a 5-fold validation on each
of the CALO meetings. In each case, we trained
classifiers on the other 4 meetings in the CALO se-
quence, plus the fixed ICSI/ISL training selection.
Performance (per utterance, on the binary classifica-
tion problem) is shown in Table 1; overall f-score
figures are poor even on these short meetings. These
figures were obtained using words (unigrams, after
text normalization and stemming) as features – we
investigated other discriminative classifier methods,
and the use of 2- and 3-grams as features, but no
improvements were gained.
</bodyText>
<table confidence="0.9984455">
Mtg. Utts AI Utts. Precision Recall F-Score
1 191 22 0.31 0.50 0.38
2 156 27 0.36 0.33 0.35
3 196 18 0.28 0.55 0.37
4 212 15 0.20 0.60 0.30
5 198 9 0.19 0.67 0.30
</table>
<tableCaption confidence="0.998059">
Table 1: Baseline Classification Performance
</tableCaption>
<sectionHeader confidence="0.992498" genericHeader="method">
4 Hierarchical Annotations
</sectionHeader>
<bodyText confidence="0.999928806451613">
Two problems are apparent: firstly, accuracy is
lower than desired; secondly, identifying utterances
related to action items does not allow us to ac-
tually identify those action items and extract their
properties (deadline, owner etc.). But if the ut-
terances related to these properties form distinct
sub-classes which have their own distinct features,
treating them separately and combining the results
(along the lines of (Klein et al., 2002)) might al-
low better performance, while also identifying the
utterances where each property’s value is extracted.
Thus, we produced an annotation schema which
distinguishes among these four classes. The first
three correspond to the discussion and assignment
of the individual properties of the action item (task
description, timeframe and owner); the fi-
nal agreement class covers utterances which ex-
plicitly show that the action item is agreed upon.
Since the task description subclass ex-
tracts a description of the task, it must include any
utterances that specify the action to be performed,
including those that provide required antecedents for
anaphoric references. The owner subclass includes
any utterances that explicitly specify the responsible
party (e.g. “I’ll take care of that”, or “John, we’ll
leave that to you”), but not those whose function
might be taken to do so implicitly (such as agree-
ments by the responsible party). The timeframe
subclass includes any utterances that explicitly refer
to when a task may start or when it is expected to
be finished; note that this is often not specified with
</bodyText>
<page confidence="0.99645">
32
</page>
<bodyText confidence="0.999963259259259">
a date or temporal expression, but rather e.g. “by
the end of next week,” or “before the trip to Aruba”.
Finally, the agreement subclass includes any ut-
terances in which people agree that the action should
and will be done; not only acknowledgements by the
owner themselves, but also when other people ex-
press their agreement.
A single utterance may be assigned to more than
one class: “John, you need to do that by next
Monday” might count as owner and timeframe.
Likewise, there may be more than one utterance of
each class for a single action item: John’s response
“OK, I’ll do that” would also be classed as owner
(as well as agreement). While we do not require
all of these subclasses to be present for a set of ut-
terances to qualify as denoting an action item, we
expect any action item to include most of them.
We applied this annotation schema to the same
12 meetings. Initial reliability between two anno-
tators on the single ISL meeting (chosen as it pre-
sented a significantly more complex set of action
items than others in this set) was encouraging. The
best agreement was achieved on timeframe utter-
ances (κ _ .86), with owner utterances slightly
less good (between κ _ .77), and agreement and
description utterances worse but still accept-
able (κ _ .73). Further annotation is in progress.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99992853125">
We trained individual classifiers for each of the utter-
ance sub-classes, and cross-validated as before. For
agreement utterances, we used a naive n-gram
classifier similar to that of (Webb et al., 2005) for di-
alogue act detection, scoring utterances via a set of
most predictive n-grams of length 1–3 and making a
classification decision by comparing the maximum
score to a threshold (where the n-grams, their scores
and the threshold are automatically extracted from
the training data). For owner, timeframe and
task description utterances, we used SVMs
as before, using word unigrams as features (2- and
3-grams gave no improvement – probably due to the
small amount of training data). Performance var-
ied greatly by sub-class (see Table 2), with some
(e.g. agreement) achieving higher accuracy than the
baseline flat classifications, but others being worse.
As there is now significantly less training data avail-
able to each sub-class than there was for all utter-
ances grouped together in the baseline experiment,
worse performance might be expected; yet some
sub-classes perform better. The worst performing
class is owner. Examination of the data shows
that owner utterances are more likely than other
classes to be assigned to more than one category;
they may therefore have more feature overlap with
other classes, leading to less accurate classification.
Use of relevant sub-strings for training (rather than
full utterances) may help; as may part-of-speech in-
formation – while proper names may be useful fea-
tures, the name tokens themselves are sparse and
may be better substituted with a generic tag.
</bodyText>
<table confidence="0.9996964">
Class Precision Recall F-Score
description 0.23 0.41 0.29
owner 0.12 0.28 0.17
timeframe 0.19 0.38 0.26
agreement 0.48 0.44 0.40
</table>
<tableCaption confidence="0.994226">
Table 2: Sub-class Classification Performance
</tableCaption>
<bodyText confidence="0.999972083333334">
Even with poor performance for some of the sub-
classifiers, we should still be able to combine them
to get a benefit as long as their true positives cor-
relate better than their false positives (intuitively, if
they make mistakes in different places). So far we
have only conducted an initial naive experiment, in
which we combine the individual classifier decisions
in a weighted sum over a window (currently set to
5 utterances). If the sum over the window reaches
a given threshold, we hypothesize an action item,
and take the highest-confidence utterance given by
each sub-classifier in that window to provide the
corresponding property. As shown in Table 3, this
gives reasonable performance on most meetings, al-
though it does badly on meeting 5 (apparently be-
cause no explicit agreement takes place, while our
manual weights emphasized agreement).1 Most en-
couragingly, the correct examples provide some use-
ful “best” sub-class utterances, from which the rele-
vant properties could be extracted.
These results can probably be significantly im-
proved: rather than sum over the binary classifica-
tion outputs of each classifier, we can use their con-
fidence scores or posterior probabilities, and learn
</bodyText>
<footnote confidence="0.997938">
1Accuracy here is currently assessed only over correct de-
tection of an action item in a window, not correct assignment of
all sub-classes.
</footnote>
<page confidence="0.99336">
33
</page>
<table confidence="0.999547333333333">
Mtg. AIs Correct False+ False- F-Score
1 3 2 1 1 0.67
2 4 1 0 3 0.40
3 5 2 1 3 0.50
4 4 4 0 0 1.00
5 3 0 1 3 0.00
</table>
<tableCaption confidence="0.998886">
Table 3: Combined Classification Performance
</tableCaption>
<bodyText confidence="0.999877">
the combination weights to give a more robust ap-
proach. There is still a long way to go to evaluate
this approach over more data, including the accu-
racy and utility of the resulting sub-class utterance
hypotheses.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99999564516129">
So accounting for the structure of action items ap-
pears essential to detecting them in spoken dis-
course. Otherwise, classification accuracy is lim-
ited. We believe that accuracy can be improved, and
the detected utterances can be used to provide the
properties of the action item itself. An interesting
question is how and whether the structure we use
here relates to discourse structure in more general
use. If a relation exists, this would shed light on the
decision-making process we are attempting to (be-
gin to) model, and might allow us to use other (more
plentiful) annotated data.
Our future efforts focus on annotating more meet-
ings to obtain large training and testing sets. We also
wish to examine performance when working from
speech recognition hypotheses (as opposed to the
human transcripts used here), and the best way to in-
corporate multiple hypotheses (either as n-best lists
or word confusion networks). We are actively inves-
tigating alternative approaches to sub-classifier com-
bination: better performance (and a more robust and
trainable overall system) might be obtained by using
a Bayesian network, or a maximum entropy classi-
fier as used by (Klein et al., 2002). Finally, we are
developing an interface to a new large-vocabulary
version of the Gemini parser (Dowding et al., 1993)
which will allow us to use semantic parse informa-
tion as features in the individual sub-class classifiers,
and also to extract entity and event representations
from the classified utterances for automatic addition
of entries to calendars and to-do lists.
</bodyText>
<sectionHeader confidence="0.989024" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999349306122449">
S. Burger, V. MacLaren, and H. Yu. 2002. The ISL Meet-
ing Corpus: The impact of meeting type on speech
style. In Proceedings of the 6th International Confer-
ence on Spoken Language Processing (ICSLP 2002).
M. Core and J. Allen. 1997. Coding dialogues with
the DAMSL annotation scheme. In D. Traum, edi-
tor, AAAI Fall Symposium on Communicative Action
in Humans and Machines.
S. Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-
bell. 2004. Task-focused summarization of email. In
Proceedings of the Text Summarization Branches Out
ACL Workshop.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proc. 31st Annual Meeting of the Association for
Computational Linguistics.
A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meet-
ing structure annotation: Data and tools. In Proceed-
ings of the 6th SIGdial Workshop on Discourse and
Dialogue.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI Meeting Corpus.
In Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2003).
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods – Support Vector
Learning. MIT Press.
D. Klein, K. Toutanova, H. T. Ilhan, S. D. Kamvar, and
C. D. Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceedings
of the ACL Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions.
W. Morgan, S. Gupta, and P.-C. Chang. forthcoming.
Automatically detecting action items in audio meeting
recordings. Ms., under review.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI Meeting Recorder Dialog Act
Corpus. In Proceedings of the 5th SIGdial Workshop
on Discourse and Dialogue.
S. Siegel and J. N. J. Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue act
classification using intra-utterance features. In Proc.
AAAI Workshop on Spoken Language Understanding.
</reference>
<page confidence="0.999301">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904536">
<title confidence="0.999811">Shallow Discourse Structure for Action Item Detection</title>
<author confidence="0.993569">Matthew Purver</author>
<author confidence="0.993569">Patrick Ehlen</author>
<author confidence="0.993569">John</author>
<affiliation confidence="0.999087">Center for the Study of Language and Stanford University</affiliation>
<address confidence="0.999809">Stanford, CA 94305</address>
<abstract confidence="0.992155333333333">investigated automatic item transcripts of multi-party meetings. Unlike previous work (Gruenstein et al., 2005), we use a new hierarchical annotation scheme based on the roles utterances play in the action item assignment process, and propose an approach to automatic detection that promises improved classification accuracy while enabling the extraction of useful information for summarization and reporting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Burger</author>
<author>V MacLaren</author>
<author>H Yu</author>
</authors>
<title>The ISL Meeting Corpus: The impact of meeting type on speech style.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP</booktitle>
<contexts>
<context position="2805" citStr="Burger et al., 2002" startWordPosition="430" endWordPosition="433"> reasonable per-sentence classification performance (with f-scores around 0.6). While there are related tags for dialogue act tagging schema – like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al., 2004) which includes a commit tag – these classes are too general to allow identification of action items specifically. One comparable attempt in spoken discourse took a flat approach, annotating utterances as action-item-related or not (Gruenstein et al., 2005) over the ICSI and ISL meeting corpora (Janin et al., 2003; Burger et al., 2002). Their inter-annotator agreement was low (n = .36). While this may have been partly due to their methods, it is notable that (Core and Allen, 1997) reported even lower agreement (n = .15) on their Commit dialogue acts. Morgan et al. (forthcoming) then used these annotations to attempt autoProceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31–34, New York City, New York, June 2006. c�2006 Association for Computational Linguistics matic classification, but achieved poor performance (with f-scores around 0.3 at best). Action Items Action items </context>
</contexts>
<marker>Burger, MacLaren, Yu, 2002</marker>
<rawString>S. Burger, V. MacLaren, and H. Yu. 2002. The ISL Meeting Corpus: The impact of meeting type on speech style. In Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Core</author>
<author>J Allen</author>
</authors>
<title>Coding dialogues with the DAMSL annotation scheme. In</title>
<date>1997</date>
<booktitle>AAAI Fall Symposium on Communicative Action in Humans and Machines.</booktitle>
<editor>D. Traum, editor,</editor>
<contexts>
<context position="2361" citStr="Core and Allen, 1997" startWordPosition="357" endWordPosition="360"> set of “dialogue” act classes; one class Task included any sentence containing items that seemed appropriate to add to an ongoing todo list. They report good inter-annotator agreement over their general tagging exercise (n &gt; 0.8), although individual figures for the Task class are not given. They then concentrated on Task sentences, establishing a set of predictive features (in which word n-grams emerged as “highly predictive”) and achieved reasonable per-sentence classification performance (with f-scores around 0.6). While there are related tags for dialogue act tagging schema – like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al., 2004) which includes a commit tag – these classes are too general to allow identification of action items specifically. One comparable attempt in spoken discourse took a flat approach, annotating utterances as action-item-related or not (Gruenstein et al., 2005) over the ICSI and ISL meeting corpora (Janin et al., 2003; Burger et al., 2002). Their inter-annotator agreement was low (n = .36). While this may have been partly due to their methods, it is notable that (Core and Allen, 1997) reporte</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>M. Core and J. Allen. 1997. Coding dialogues with the DAMSL annotation scheme. In D. Traum, editor, AAAI Fall Symposium on Communicative Action in Humans and Machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>E Ringger</author>
<author>M Gamon</author>
<author>R Campbell</author>
</authors>
<title>Task-focused summarization of email.</title>
<date>2004</date>
<booktitle>In Proceedings of the Text Summarization Branches Out ACL Workshop.</booktitle>
<contexts>
<context position="1569" citStr="Corston-Oliver et al. (2004)" startWordPosition="232" endWordPosition="235">etect the regions of discourse which establish action items, so their surface form can be used for a targeted report or summary; and secondly, to identify the important properties of the action items (such as the associated tasks and deadlines) that would foster concise and informative semantically-based reporting (for example, adding task specifications to a user’s to-do list). We believe both of these aims are facilitated by taking into account the roles different utterances play in the decision-making process – in short, a shallow notion of discourse structure. 31 2 Background Related Work Corston-Oliver et al. (2004) attempted to identify action items in e-mails, using classifiers trained on annotations of individual sentences within each e-mail. Sentences were annotated with one of a set of “dialogue” act classes; one class Task included any sentence containing items that seemed appropriate to add to an ongoing todo list. They report good inter-annotator agreement over their general tagging exercise (n &gt; 0.8), although individual figures for the Task class are not given. They then concentrated on Task sentences, establishing a set of predictive features (in which word n-grams emerged as “highly predictiv</context>
<context position="5079" citStr="Corston-Oliver et al., 2004" startWordPosition="813" endWordPosition="816">se tends to reflect this, and we attempt to exploit this fact here. 3 Baseline Experiments We applied Gruenstein et al. (2005)’s flat annotation schema to transcripts from a sequence of 5 short related meetings with 3 participants recorded as part of the CALO project. Each meeting was simulated in that its participants were given a scenario, but was not scripted. In order to avoid entirely dataor scenario-specific results (and also to provide an acceptable amount of training data), we then added a random selection of 6 ICSI and 1 ISL meetings from Gruenstein et al. (2005)’s annotations. Like (Corston-Oliver et al., 2004) we used support vector machines (Vapnik, 1995) via the classifier SVMlight (Joachims, 1999). Their full set of features are not available to us, but we experimented with combinations of words and n-grams and assessed classification performance via a 5-fold validation on each of the CALO meetings. In each case, we trained classifiers on the other 4 meetings in the CALO sequence, plus the fixed ICSI/ISL training selection. Performance (per utterance, on the binary classification problem) is shown in Table 1; overall f-score figures are poor even on these short meetings. These figures were obtai</context>
</contexts>
<marker>Corston-Oliver, Ringger, Gamon, Campbell, 2004</marker>
<rawString>S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell. 2004. Task-focused summarization of email. In Proceedings of the Text Summarization Branches Out ACL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>J M Gawron</author>
<author>D Appelt</author>
<author>J Bear</author>
<author>L Cherny</author>
<author>R Moore</author>
<author>D Moran</author>
</authors>
<title>Gemini: A natural language system for spoken language understanding.</title>
<date>1993</date>
<booktitle>In Proc. 31st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Dowding, Gawron, Appelt, Bear, Cherny, Moore, Moran, 1993</marker>
<rawString>J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny, R. Moore, and D. Moran. 1993. Gemini: A natural language system for spoken language understanding. In Proc. 31st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruenstein</author>
<author>J Niekrasz</author>
<author>M Purver</author>
</authors>
<title>Meeting structure annotation: Data and tools.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2725" citStr="Gruenstein et al., 2005" startWordPosition="415" endWordPosition="418">dictive features (in which word n-grams emerged as “highly predictive”) and achieved reasonable per-sentence classification performance (with f-scores around 0.6). While there are related tags for dialogue act tagging schema – like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al., 2004) which includes a commit tag – these classes are too general to allow identification of action items specifically. One comparable attempt in spoken discourse took a flat approach, annotating utterances as action-item-related or not (Gruenstein et al., 2005) over the ICSI and ISL meeting corpora (Janin et al., 2003; Burger et al., 2002). Their inter-annotator agreement was low (n = .36). While this may have been partly due to their methods, it is notable that (Core and Allen, 1997) reported even lower agreement (n = .15) on their Commit dialogue acts. Morgan et al. (forthcoming) then used these annotations to attempt autoProceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31–34, New York City, New York, June 2006. c�2006 Association for Computational Linguistics matic classification, but achieved</context>
<context position="4577" citStr="Gruenstein et al. (2005)" startWordPosition="730" endWordPosition="733">immediate actions that occur during the meeting and from more vague future actions that are still in the planning stage, an action item will be specified as expected to be carried out within a timeframe that begins at some point after the meeting and extends no further than the not-too-distant future. So an action item, as a type of social action, often comprises four components: a task description, a timeframe, an owner, and a round of agreement among the owner and others. The related discourse tends to reflect this, and we attempt to exploit this fact here. 3 Baseline Experiments We applied Gruenstein et al. (2005)’s flat annotation schema to transcripts from a sequence of 5 short related meetings with 3 participants recorded as part of the CALO project. Each meeting was simulated in that its participants were given a scenario, but was not scripted. In order to avoid entirely dataor scenario-specific results (and also to provide an acceptable amount of training data), we then added a random selection of 6 ICSI and 1 ISL meetings from Gruenstein et al. (2005)’s annotations. Like (Corston-Oliver et al., 2004) we used support vector machines (Vapnik, 1995) via the classifier SVMlight (Joachims, 1999). Thei</context>
</contexts>
<marker>Gruenstein, Niekrasz, Purver, 2005</marker>
<rawString>A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meeting structure annotation: Data and tools. In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI Meeting Corpus. In</title>
<date>2003</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP</booktitle>
<contexts>
<context position="2783" citStr="Janin et al., 2003" startWordPosition="426" endWordPosition="429">ctive”) and achieved reasonable per-sentence classification performance (with f-scores around 0.6). While there are related tags for dialogue act tagging schema – like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al., 2004) which includes a commit tag – these classes are too general to allow identification of action items specifically. One comparable attempt in spoken discourse took a flat approach, annotating utterances as action-item-related or not (Gruenstein et al., 2005) over the ICSI and ISL meeting corpora (Janin et al., 2003; Burger et al., 2002). Their inter-annotator agreement was low (n = .36). While this may have been partly due to their methods, it is notable that (Core and Allen, 1997) reported even lower agreement (n = .15) on their Commit dialogue acts. Morgan et al. (forthcoming) then used these annotations to attempt autoProceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31–34, New York City, New York, June 2006. c�2006 Association for Computational Linguistics matic classification, but achieved poor performance (with f-scores around 0.3 at best). Acti</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI Meeting Corpus. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods – Support Vector Learning.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5171" citStr="Joachims, 1999" startWordPosition="830" endWordPosition="831">enstein et al. (2005)’s flat annotation schema to transcripts from a sequence of 5 short related meetings with 3 participants recorded as part of the CALO project. Each meeting was simulated in that its participants were given a scenario, but was not scripted. In order to avoid entirely dataor scenario-specific results (and also to provide an acceptable amount of training data), we then added a random selection of 6 ICSI and 1 ISL meetings from Gruenstein et al. (2005)’s annotations. Like (Corston-Oliver et al., 2004) we used support vector machines (Vapnik, 1995) via the classifier SVMlight (Joachims, 1999). Their full set of features are not available to us, but we experimented with combinations of words and n-grams and assessed classification performance via a 5-fold validation on each of the CALO meetings. In each case, we trained classifiers on the other 4 meetings in the CALO sequence, plus the fixed ICSI/ISL training selection. Performance (per utterance, on the binary classification problem) is shown in Table 1; overall f-score figures are poor even on these short meetings. These figures were obtained using words (unigrams, after text normalization and stemming) as features – we investiga</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods – Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>K Toutanova</author>
<author>H T Ilhan</author>
<author>S D Kamvar</author>
<author>C D Manning</author>
</authors>
<title>Combining heterogeneous classifiers for word-sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions.</booktitle>
<contexts>
<context position="6570" citStr="Klein et al., 2002" startWordPosition="1057" endWordPosition="1060">.50 0.38 2 156 27 0.36 0.33 0.35 3 196 18 0.28 0.55 0.37 4 212 15 0.20 0.60 0.30 5 198 9 0.19 0.67 0.30 Table 1: Baseline Classification Performance 4 Hierarchical Annotations Two problems are apparent: firstly, accuracy is lower than desired; secondly, identifying utterances related to action items does not allow us to actually identify those action items and extract their properties (deadline, owner etc.). But if the utterances related to these properties form distinct sub-classes which have their own distinct features, treating them separately and combining the results (along the lines of (Klein et al., 2002)) might allow better performance, while also identifying the utterances where each property’s value is extracted. Thus, we produced an annotation schema which distinguishes among these four classes. The first three correspond to the discussion and assignment of the individual properties of the action item (task description, timeframe and owner); the final agreement class covers utterances which explicitly show that the action item is agreed upon. Since the task description subclass extracts a description of the task, it must include any utterances that specify the action to be performed, inclu</context>
</contexts>
<marker>Klein, Toutanova, Ilhan, Kamvar, Manning, 2002</marker>
<rawString>D. Klein, K. Toutanova, H. T. Ilhan, S. D. Kamvar, and C. D. Manning. 2002. Combining heterogeneous classifiers for word-sense disambiguation. In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Automatically detecting action items in audio meeting recordings. Ms., under review.</title>
<marker>forthcoming, </marker>
<rawString>W. Morgan, S. Gupta, and P.-C. Chang. forthcoming. Automatically detecting action items in audio meeting recordings. Ms., under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The ICSI Meeting Recorder Dialog Act Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2468" citStr="Shriberg et al., 2004" startWordPosition="374" endWordPosition="377">ate to add to an ongoing todo list. They report good inter-annotator agreement over their general tagging exercise (n &gt; 0.8), although individual figures for the Task class are not given. They then concentrated on Task sentences, establishing a set of predictive features (in which word n-grams emerged as “highly predictive”) and achieved reasonable per-sentence classification performance (with f-scores around 0.6). While there are related tags for dialogue act tagging schema – like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al., 2004) which includes a commit tag – these classes are too general to allow identification of action items specifically. One comparable attempt in spoken discourse took a flat approach, annotating utterances as action-item-related or not (Gruenstein et al., 2005) over the ICSI and ISL meeting corpora (Janin et al., 2003; Burger et al., 2002). Their inter-annotator agreement was low (n = .36). While this may have been partly due to their methods, it is notable that (Core and Allen, 1997) reported even lower agreement (n = .15) on their Commit dialogue acts. Morgan et al. (forthcoming) then used these</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The ICSI Meeting Recorder Dialog Act Corpus. In Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegel</author>
<author>J N J Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<journal>McGraw-Hill. V. N. Vapnik.</journal>
<publisher>Springer.</publisher>
<marker>Siegel, Castellan, 1988</marker>
<rawString>S. Siegel and J. N. J. Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill. V. N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Webb</author>
<author>M Hepple</author>
<author>Y Wilks</author>
</authors>
<title>Dialogue act classification using intra-utterance features.</title>
<date>2005</date>
<booktitle>In Proc. AAAI Workshop on Spoken Language Understanding.</booktitle>
<contexts>
<context position="9207" citStr="Webb et al., 2005" startWordPosition="1499" endWordPosition="1502">ween two annotators on the single ISL meeting (chosen as it presented a significantly more complex set of action items than others in this set) was encouraging. The best agreement was achieved on timeframe utterances (κ _ .86), with owner utterances slightly less good (between κ _ .77), and agreement and description utterances worse but still acceptable (κ _ .73). Further annotation is in progress. 5 Experiments We trained individual classifiers for each of the utterance sub-classes, and cross-validated as before. For agreement utterances, we used a naive n-gram classifier similar to that of (Webb et al., 2005) for dialogue act detection, scoring utterances via a set of most predictive n-grams of length 1–3 and making a classification decision by comparing the maximum score to a threshold (where the n-grams, their scores and the threshold are automatically extracted from the training data). For owner, timeframe and task description utterances, we used SVMs as before, using word unigrams as features (2- and 3-grams gave no improvement – probably due to the small amount of training data). Performance varied greatly by sub-class (see Table 2), with some (e.g. agreement) achieving higher accuracy than t</context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue act classification using intra-utterance features. In Proc. AAAI Workshop on Spoken Language Understanding.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>