<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.9977335">
An Analysis of Quantitative Aspects in the Evaluation of Thematic
Segmentation Algorithms
</title>
<author confidence="0.991838">
Maria Georgescul Alexander Clark Susan Armstrong
</author>
<affiliation confidence="0.901957333333333">
ISSCO/TIM, ETI Department of Computer Science ISSCO/TIM, ETI
University of Geneva Royal Holloway University of London University of Geneva
1211 Geneva, Switzerland Egham, Surrey TW20 0EX, UK 1211 Geneva, Switzerland
</affiliation>
<email confidence="0.992571">
maria.georgescul@eti.unige.ch alexc@cs.rhul.ac.uk susan.armstrong@issco.unige.ch
</email>
<sectionHeader confidence="0.994678" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874">
We consider here the task of linear the-
matic segmentation of text documents, by
using features based on word distributions
in the text. For this task, a typical and of-
ten implicit assumption in previous stud-
ies is that a document has just one topic
and therefore many algorithms have been
tested and have shown encouraging results
on artificial data sets, generated by putting
together parts of different documents. We
show that evaluation on synthetic data is
potentially misleading and fails to give an
accurate evaluation of the performance on
real data. Moreover, we provide a criti-
cal review of existing evaluation metrics in
the literature and we propose an improved
evaluation metric.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963472727273">
The goal of thematic segmentation is to iden-
tify boundaries of topically coherent segments
in text documents. Giving a rigorous definition
of the notion of topic is difficult, but the task
of discourse/dialogue segmentation into thematic
episodes is usually described by invoking an “in-
tuitive notion of topic” (Brown and Yule, 1998).
Thematic segmentation also relates to several no-
tions such as speaker’s intention, topic flow and
cohesion.
Since it is elusive what mental representations
humans use in order to distinguish a coherent
text, different surface markers (Hirschberg and
Nakatani, 1996; Passonneau and Litman, 1997)
and external knowledge sources (Kozima and Fu-
rugori, 1994) have been exploited for the purpose
of automatic thematic segmentation. Halliday and
Hasan (1976) claim that the text meaning is re-
alised through certain language resources and they
refer to these resources by the term of cohesion.
The major classes of such text-forming resources
identified in (Halliday and Hasan, 1976) are: sub-
stitution, ellipsis, conjunction, reiteration and col-
location. In this paper, we examine one form of
lexical cohesion, namely lexical reiteration.
Following some of the most prominent dis-
course theories in literature (Grosz and Sidner,
1986; Marcu, 2000), a hierarchical representation
of the thematic episodes can be proposed. The
basis for this is the idea that topics can be re-
cursively divided into subtopics. Real texts ex-
hibit a more intricate structure, including ‘seman-
tic returns’ by which a topic is suspended at one
point and resumed later in the discourse. However,
we focus here on a reduced segmentation prob-
lem, which involves identifying non-overlapping
and non-hierarchical segments at a coarse level of
granularity.
Thematic segmentation is a valuable initial
tool in information retrieval and natural language
processing. For instance, in information ac-
cess systems, smaller and coherent passage re-
trieval is more convenient to the user than whole-
document retrieval and thematic segmentation has
been shown to improve the passage-retrieval per-
formance (Hearst and Plaunt, 1993). In cases such
as collections of transcripts there are no headers
or paragraph markers. Therefore a clear separa-
tion of the text into thematic episodes can be used
together with highlighted keywords as a kind of
‘quick read guide’ to help users to quickly navi-
gate through and understand the text. Moreover
automatic thematic segmentation has been shown
to play an important role in automatic summariza-
tion (Mani, 2001), anaphora resolution and dis-
</bodyText>
<page confidence="0.980277">
144
</page>
<note confidence="0.5951565">
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 144–151,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.994963388888889">
course/dialogue understanding.
In this paper, we concern ourselves with the task
of linear thematic segmentation and are interested
in finding out whether different segmentation sys-
tems can perform well on artificial and real data
sets without specific parameter tuning. In addi-
tion, we will refer to the implications of the choice
of a particular error metric for evaluation results.
This paper is organized as follows. Section 2
and Section 3 describe various systems and, re-
spectively, different input data selected for our
evaluation. Section 4 presents several existing
evaluation metrics and their weaknesses, as well
as a new evaluation metric that we propose. Sec-
tion 5 presents our experimental set-up and shows
comparisons between the performance of different
systems. Finally, some conclusions are drawn in
Section 6.
</bodyText>
<sectionHeader confidence="0.665265" genericHeader="introduction">
2 Comparison of Systems
</sectionHeader>
<bodyText confidence="0.999985111111111">
Combinations of different features (derived for ex-
ample from linguistic, prosodic information) have
been explored in previous studies like (Galley et
al., 2003) and (Kauchak and Chen, 2005). In
this paper, we selected for comparison three sys-
tems based merely on the lexical reiteration fea-
ture: TextTiling (Hearst, 1997), C99 (Choi, 2000)
and TextSeg (Utiyama and Isahara, 2001). In the
following, we briefly review these approaches.
</bodyText>
<subsectionHeader confidence="0.995473">
2.1 TextTiling Algorithm
</subsectionHeader>
<bodyText confidence="0.999211230769231">
The TextTiling algorithm was initially developed
by Hearst (1997) for segmentation of exposi-
tory texts into multi-paragraph thematic episodes
having a linear, non-overlapping structure (as re-
flected by the name of the algorithm). TextTiling
is widely used as a de-facto standard in the eval-
uation of alternative segmentation systems, e.g.
(Reynar, 1998; Ferret, 2002; Galley et al., 2003).
The algorithm can briefly be described by the fol-
lowing steps.
Step 1 includes stop-word removal, lemmatiza-
tion and division of the text into ‘token-sequences’
(i.e. text blocks having a fixed number of words).
Step 2 determines a score for each gap between
two consecutive token-sequences, by computing
the cosine similarity (Manning and Sch¨utze, 1999)
between the two vectors representing the frequen-
cies of the words in the two blocks.
Step 3 computes a ‘depth score’ for each token-
sequence gap, based on the local minima of the
score computed in step 2.
Step 4 consists in smoothing the scores.
Step 5 chooses from any potential boundaries
those that have the scores smaller than a certain
‘cutoff function’, based on the average and stan-
dard deviation of score distribution.
</bodyText>
<subsectionHeader confidence="0.99478">
2.2 C99 Algorithm
</subsectionHeader>
<bodyText confidence="0.999889884615385">
The C99 algorithm (Choi, 2000) makes a linear
segmentation based on a divisive clustering strat-
egy and the cosine similarity measure between any
two minimal units. More exactly, the algorithm
consists of the following steps.
Step 1: after the division of the text into min-
imal units (in our experiments, the minimal unit
is an utterance1), stop words are removed and a
stemmer is applied.
The second step consists of constructing a sim-
ilarity matrix 5mxm, where m is the number of
utterances and an element sib of the matrix corre-
sponds to the cosine similarity between the vectors
representing the frequencies of the words in the i-
th utterance and the j-th utterance.
Step 3: a ‘rank matrix’ Rm�m is computed, by
determining for each pair of utterances, the num-
ber of neighbors in 5mxm with a lower similarity
value.
In the final step, the location of thematic bound-
aries is determined by a divisive top-down cluster-
ing procedure. The criterion for division of the
current segment B into b1, ...bm subsegments is
based on the maximisation of a ‘density’ D, com-
puted for each potential repartition of boundaries
as
</bodyText>
<equation confidence="0.998773">
D = �m ,
k=1 areak
</equation>
<bodyText confidence="0.999949">
where sumk and areak refers to the sum of rank
and area of the k-th segment in B, respectively.
</bodyText>
<subsectionHeader confidence="0.999061">
2.3 TextSeg Algorithm
</subsectionHeader>
<bodyText confidence="0.998818857142857">
The TextSeg algorithm (Utiyama and Isahara,
2001) implements a probabilistic approach to de-
termine the most likely segmentation, as briefly
described below.
The segmentation task is modeled as a problem
of finding the minimum cost C(S) of a segmenta-
tion S. The segmentation cost is defined as:
</bodyText>
<equation confidence="0.547248">
C(S) ~ −logPr(WjS)Pr(S),
</equation>
<footnote confidence="0.95762225">
1Occasionally within this document we employ the term
utterance to denote either a sentence or an utterance in its
proper sense.
Ekm=1 sumk
</footnote>
<page confidence="0.997916">
145
</page>
<bodyText confidence="0.999985142857143">
where W = w1w2...wn represents the text con-
sisting of n words (after applying stop-words re-
moval and stemming) and S = S1S2...Sm is a po-
tential segmentation of W in m segments. The
probability Pr(W|S) is defined using Laplace
law, while the definition of the probability Pr(S)
is chosen in a manner inspired by information the-
ory.
A directed graph !9 is defined such that a path
in !9 corresponds to a possible segmentation of
W. Therefore, the thematic segmentation pro-
posed by the system is obtained by applying a dy-
namic programming algorithm for determining the
minimum cost path in !g.
</bodyText>
<sectionHeader confidence="0.995238" genericHeader="method">
3 Input Data
</sectionHeader>
<bodyText confidence="0.999983777777778">
When evaluating a thematic segmentation system
for an application, human annotators should pro-
vide the gold standard. The problem is that the
procedure of building such a reference corpus is
expensive. That is, the typical setting involves an
experiment with several human subjects, who are
asked to mark thematic segment boundaries based
on specific guidelines and their intuition. The
inter-annotator agreement provides the reference
segmentation. This expense can be avoided by
constructing a synthetic reference corpus by con-
catenation of segments from different documents.
Therefore, the use of artificial data for evaluation
is a general trend in many studies, e.g. (Ferret,
2002; Choi, 2000; Utiyama and Isahara, 2001).
In our experiment, we used artificial and real
data, i.e. the algorithms have been tested on the
following data sets containing English texts.
</bodyText>
<subsectionHeader confidence="0.999911">
3.1 Artificially Generated Data
</subsectionHeader>
<bodyText confidence="0.9999564">
Choi (2000) designed an artificial dataset, built by
concatenating short pieces of texts that have been
extracted from the Brown corpus. Any test sample
from this dataset consists of ten segments. Each
segment contains the first n sentences (where 3 &lt;
n &lt; 11) of a randomly selected document from
the Brown corpus. From this dataset, we randomly
chose for our evaluation 100 test samples, where
the length of a segment varied between 3 and 11
sentences.
</bodyText>
<subsectionHeader confidence="0.998936">
3.2 TDT Data
</subsectionHeader>
<bodyText confidence="0.999932545454545">
One of the commonly used data sets for topic seg-
mentation emerged from the Topic Detection and
Tracking (TDT) project, which includes the task
of story segmentation, i.e. the task of segmenting
a stream of news data into topically cohesive sto-
ries. As part of the TDT initiative several datasets
of news stories have been created. In our evalua-
tion, we used a subset of 28 documents randomly
selected from the TDT Phase 2 (TDT2) collection,
where a document contains an average of 24.67
segments.
</bodyText>
<subsectionHeader confidence="0.999853">
3.3 Meeting Transcripts
</subsectionHeader>
<bodyText confidence="0.9999481875">
The third dataset used in our evaluation contains
25 meeting transcripts from the ICSI-MR corpus
(Janin et al., 2004). The entire corpus contains
high-quality close talking microphone recordings
of multi-party dialogues. Transcriptions at word
level with utterance-level segmentations are also
available. The gold standard for thematic segmen-
tations has been kindly provided by (Galley et
al., 2003) and has been chosen by considering the
agreement between at least three human annota-
tions. Each meeting is thus divided into contigu-
ous major topic segments and contains an average
of 7.32 segments.
Note that thematic segmentation of meeting
data is a more challenging task as the thematic
transitions are subtler than those in TDT data.
</bodyText>
<sectionHeader confidence="0.993825" genericHeader="method">
4 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.9999724">
In this section, we will look in detail at the error
metrics that have been proposed in previous stud-
ies and examine their inadequacies. In addition,
we propose a new evaluation metric that we con-
sider more appropriate.
</bodyText>
<subsectionHeader confidence="0.995553">
4.1 Pk Metric
</subsectionHeader>
<bodyText confidence="0.9999396875">
(Passonneau and Litman, 1996; Beeferman et al.,
1999) underlined that the standard evaluation met-
rics of precision and recall are inadequate for the-
matic segmentation, namely by the fact that these
metrics did not account for how far away is a hy-
pothesized boundary (i.e. a boundary found by
the automatic procedure) from a reference bound-
ary (i.e. a boundary found in the reference data).
On the other hand, it is desirable that an algorithm
that places for instance a boundary just one utter-
ance away from the reference boundary to be pe-
nalized less than an algorithm that places a bound-
ary two (or more) utterances away from the ref-
erence boundary. Hence (Beeferman et al., 1999)
proposed a new metric, called PD, that allows for
a slight vagueness in where boundaries lie. More
</bodyText>
<page confidence="0.99409">
146
</page>
<bodyText confidence="0.977098">
specifically, (Beeferman et al., 1999) define PD
as follows2:
</bodyText>
<equation confidence="0.93436">
PD(ref,hyp) = E1&lt;i&lt;j≤N D(i,j)[δref(i, j) ®
δhyp(i, j)] . — —
</equation>
<bodyText confidence="0.999967277777778">
N is the number of words in the reference data.
The function δref(i, j) is evaluated to one if the
two reference corpus indices specified by its pa-
rameters i and j belong in the same segment, and
zero otherwise. Similarly, the function δhyp(i, j)
is evaluated to one, if the two indices are hypothe-
sized by the automatic procedure to belong in the
same segment, and zero otherwise. The ® opera-
tor is the XNOR function ‘both or neither’. D(i, j)
is a “distance probability distribution over the set
of possible distances between sentences chosen
randomly from the corpus”. In practice, a distri-
bution D having “all its probability mass at a fixed
distance k” (Beeferman et al., 1999) was adopted
and the metric PD was thus renamed Pk.
In the framework of the TDT initiative, (Allan
et al., 1998) give the following formal definition
of Pk and its components:
</bodyText>
<equation confidence="0.9550763">
Pk = PMiss &apos; Pseg + PFalseAlarm &apos; (1 − Pseg),
where:
PMiss = PN—k ,
PN�k
i=1 [δhyp(i,i+k)]·[1−δref(i,i+k)]
i=1 [1−δref(i,i+k)]
PN�k
i=1 [1−δhyp(i,i+k)]·[δref(i,i+k)]
PFalseAlarm = PN�k ,
i=1 δref(i,i+k)
</equation>
<bodyText confidence="0.999561933333333">
and Pseg is the a priori probability that in
the reference data a boundary occurs within an
interval of k words. Therefore Pk is calculated by
moving a window of a certain width k, where k is
usually set to half of the average number of words
per segment in the gold standard.
Pevzner and Hearst (2002) highlighted several
problems of the Pk metric. We illustrate below
what we consider the main problems of the Pk
metric, based on two examples.
Let r(i, k) be the number of boundaries be-
tween positions i and i + k in the gold standard
segmentation and h(i, k) be the number of bound-
aries between positions i and i+k in the automatic
hypothesized segmentation.
</bodyText>
<listItem confidence="0.981831">
• Example 1: If r(i, k) = 2 and h(i, k) = 1
then obviously a missing boundary should
</listItem>
<bodyText confidence="0.648565">
2Let ref be a correct segmentation and hyp be a segmen-
tation proposed by a text segmentation system. We will keep
this notations in equations introduced below.
</bodyText>
<listItem confidence="0.8995868">
be counted in Pk, i.e. PMiss should be in-
creased.
• Example 2: If r(i, k) = 1 and h(i, k) =
2 then obviously PFalseAlarm should be in-
creased.
</listItem>
<bodyText confidence="0.998534777777778">
However, considering the first example, we will
obtain δref(i, i + k) = 0, δhyp(i, i + k) = 0
and consequently PMiss is not increased. By tak-
ing the case from the second example we obtain
δref(i, i + k) = 0 and δhyp(i, i + k) = 0, involv-
ing no increase of PFalseAlarm.
In (TDT, 1998), a slightly different defini-
tion is given for the Pk metric: the definition of
miss and false alarm probabilities is replaced with:
</bodyText>
<equation confidence="0.971150083333333">
PN�k
i=1 [1−Ωhyp(i,i+k)]·[1−δref(i,i+k)]
P0 Miss = PN�k i=1[1−δref (i,i+k)]
PN�k
i=1 [1−Ωhyp(i,i+k)]·[δref (i,i+k)]
P0 FalseAlarm = PN�k
i=1 δref(i,i+k)
where:
�
1, if r(i, k) = h(i, k),
Ωhyp(i, i + k) =
0, otherwise.
</equation>
<bodyText confidence="0.9948065">
We will refer to this new definition of Pk by
P0k. Therefore, by taking the definition of
Pk0 and the first example above, we obtain
δref(i, i + k) = 0 and Ωhyp(i, i + k) = 0 and thus
P0Miss is correctly increased. However for the case
of example 2 we will obtain δref(i, i + k) = 0
and Ωhyp(i, i + k) = 0, involving no increase of
P0FalseAlarm and erroneous increase of P0Miss.
</bodyText>
<subsectionHeader confidence="0.967587">
4.2 WindowDiff metric
</subsectionHeader>
<bodyText confidence="0.99899675">
Pevzner and Hearst (2002) propose the alternative
metric called WindowDiff. By keeping our nota-
tions concerning r(i, k) and h(i, k) introduced in
the subsection 4.1, WindowDiff is defined as:
</bodyText>
<equation confidence="0.757103">
PNik[|r(i) k h(i,k)�&gt;0]WindowDiff = N .
</equation>
<bodyText confidence="0.9998965">
Similar to both Pk and P0k, WindowDiff is
also computed by moving a window of fixed size
across the test set and penalizing the algorithm
misses or erroneous algorithm boundary detec-
tions. However, unlike Pk and P0k, WindowDiff
takes into account how many boundaries fall
within the window and is penalizing in “how
many discrepancies occur between the reference
and the system results” rather than “determining
how often two units of text are incorrectly labeled
</bodyText>
<equation confidence="0.802861">
,
,
</equation>
<page confidence="0.991907">
147
</page>
<bodyText confidence="0.9989845">
as being in different segments” (Pevzner and
Hearst, 2002).
Our critique concerning WindowDiff is that
misses are less penalised than false alarms and
we argue this as follows. WindowDiff can be
rewritten as:
</bodyText>
<equation confidence="0.914457545454546">
WindowDiff = WDMiss + WDFalseAlarm,
where:
yN1k [r(i,k)&gt;h(i,k)]
WDMiss =
N—k
y��k
i=1 [r(i,k)&lt;h(i,k)]
WDFalseAlarm = N−k
Hence both misses and false alarms are weighted
by1
N−k.
</equation>
<bodyText confidence="0.985829333333333">
Note that, on the one hand, there are indeed (N-
k) equiprobable possibilities to have a false alarm
in an interval of k units. On the other hand, how-
ever, the total number of equiprobable possibil-
ities to have a miss in an interval of k units is
smaller than (N-k) since it depends on the num-
ber of reference boundaries (i.e. we can have a
miss in the interval of k units only if in that interval
the reference corpus contains at least one bound-
ary). Therefore misses, being weighted by 1
N−k,
are less penalised than false alarms.
Let Bref be the number of thematic boundaries
in the reference data. Let’s say that the refer-
ence data contains about 20% boundaries and 80%
non-boundaries from the total number of potential
boundaries. Therefore, since there are relatively
few boundaries compared with non-boundaries, a
strategy introducing no false alarms, but introduc-
ing a maximum number of misses (i.e. k · Bref
misses) can be judged as being around 80% cor-
rect by the WindowDiff measure. On the other
hand, a segmentation with no misses, but with a
maximum number of false alarms (i.e. (N − k)
false alarms) is judged as being 100% erroneous
by the WindowDiff measure. That is, misses and
false alarms are not equally penalised.
Another issue regarding WindowDiff is that it is
not clear “how does one interpret the values pro-
duced by the metric” (Pevzner and Hearst, 2002).
</bodyText>
<subsectionHeader confidence="0.96899">
4.3 Proposal for a New Metric
</subsectionHeader>
<bodyText confidence="0.999914333333333">
In order to address the inadequacies of Pk and
WindowDiff, we propose a new evaluation metric,
defined as follows:
</bodyText>
<equation confidence="0.982624333333333">
Prerror = Cmiss · Prmiss + Cfa · Prfa,
where:
Cmiss (0 ≤ Cmiss ≤ 1) is the cost of a miss, Cfa
(0 ≤ Cfa ≤ 1) is the cost of a false alarm,
y��k
i=1 [Oref h7/p(i,k)]
Prmiss = y��k
i=1 [Aref (i,k)] ,
yN1k [&apos;Fref h7/p(i,k)]
Pr fa = N−k
�
Oref (i, k)
1, if r(i,k) &gt; 0
=
0, otherwise.
</equation>
<bodyText confidence="0.999029222222222">
Prmiss could be interpreted as the probability
that the hypothesized segmentation contains less
boundaries than the reference segmentation in an
interval of k units3, conditioned by the fact that
the reference segmentation contains at least one
boundary in that interval. Analogously Prfa is
the probability that the hypothesized segmentation
contains more boundaries than the reference seg-
mentation in an interval of k units.
For certain applications where misses are more
important than false alarms or vice versa, the
Prerror can be adjusted to tackle this trade-off via
the Cfa and Cmiss parameters. In order to have
Prerror ∈ [0, 11, we suggest that Cfa and Cmiss
be chosen such that Cfa + Cmiss = 1. By choos-
2 the penalization of misses and
ing C fa—Cmiss=
false alarms is thus balanced. In consequence, a
strategy that places no boundaries at all is penal-
ized as much as a strategy proposing boundaries
everywhere (i.e. after every unit). In other words,
both such degenerate algorithms will have an error
rate Prerror of about 50%. The worst algorithm,
penalised as having an error rate Prerror of 100%
when k = 2, is the algorithm that places bound-
aries everywhere except the places where refer-
ence boundaries exist.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999908">
5.1 Test Procedure
</subsectionHeader>
<bodyText confidence="0.998575">
For the three datasets we first performed two
common preprocessing steps: common words are
eliminated using the same stop-list and remaining
words are stemmed by using Porter’s algorithm
(1980). Next, we ran the three segmenters de-
scribed in Section 2, by employing the default val-
ues for any system parameters and by letting the
</bodyText>
<equation confidence="0.8459987">
3A unit can be either a word or a sentence / an utterance.
,
.
,
�
1, if r(i, k) &gt; h(i, k)
ref hyp(i, k) = 0, otherwise
�
1, if r(i, k) &lt; h(i, k)
Pref hyp(i, k) = 0, otherwise.
</equation>
<page confidence="0.993156">
148
</page>
<bodyText confidence="0.999878333333333">
systems estimate the number of thematic bound-
aries.
We also considered the fact that C99 and
TextSeg algorithms can take into account a fixed
number of thematic boundaries. Even if the num-
ber of segments per document can vary in TDT
and meeting reference data, we consider that in a
real application it is impossible to provide to the
systems the exact number of boundaries for each
document to be segmented. Therefore, we ran C99
and TextSeg algorithms (for a second time), by
providing them only the average number of seg-
ments per document in the reference data, which
gives an estimation of the expected level of seg-
mentation granularity.
Four additional naive segmentations were also
used for evaluation, namely: no boundaries,
where the whole text is a single segment; all
boundaries, i.e. a thematic boundary is placed af-
ter each utterance; random known, i.e. the same
number of boundaries as in gold standard, distrib-
uted randomly throughout text; and random un-
known: the number of boundaries is randomly
selected and boundaries are randomly distributed
throughout text. Each of the segmentations was
evaluated with Pk, Pk0 and WindowDiff, as de-
scribed in Section 4.
</bodyText>
<subsectionHeader confidence="0.999581">
5.2 Comparative Performance of
Segmentation Systems
</subsectionHeader>
<bodyText confidence="0.999792388888889">
The results of applying each segmentation algo-
rithm to the three distinct datasets are summa-
rized in Figures 1, 2 and 3. Percent error values
are given in the figures and we used the follow-
ing abbreviations: WD to denote WindowDiff er-
ror metric; TextSeg KA to denote the TextSeg algo-
rithm (Utiyama and Isahara, 2001) when the av-
erage number of boundaries in the reference data
was provided to the algorithm; C99 KA to denote
the C99 algorithm (Choi, 2000) when the aver-
age number of boundaries in the reference data
was provided to the algorithm; N0 to denote the al-
gorithm proposing a segmentation with no bound-
aries; All to denote the algorithm proposing the de-
generate segmentation all boundaries; RK to de-
note the algorithm that generates a random known
segmentation; and RU to denote the algorithm that
generates a random unknown segmentation.
</bodyText>
<subsectionHeader confidence="0.965982">
5.2.1 Comparison of System Performance
from Artificial to Realistic Data
</subsectionHeader>
<bodyText confidence="0.999964058823529">
From the artificial data to the more realistic
data, we expect to have more noise and thus the
algorithms to constantly degrade, but as our ex-
periments show a reversal of the assessment can
appear. More exactly: as can be seen from Figure
1, both C99 and TextSeg algorithms significantly
outperformed TextTiling algorithm on the artifi-
cially created dataset, when the number of seg-
ments was determined by the systems. A com-
parison between the error rates given in Figure
1 and Figure 2 show that C99 and TextSeg have
a similar trend, by significantly decreasing their
performance on TDT data, but still giving bet-
ter results than TextTiling on TDT data. When
comparing the systems by Prerror, C99 has simi-
lar performance with TextTiling on meeting data
(see Figure 3). Moreover, when assessment is
done by using WindowDiff, Pk or P0k, both C99
and TextSeg came out worse than TextTiling on
meeting data. This demonstrates that rankings ob-
tained when evaluating on artificial data are dif-
ferent from those obtained when evaluating on re-
alistic data. An alternative interpretation can be
given by taking into account that the degenerative
no boundaries segmentation has an error rate of
only 30% by the WindowDiff, Pk and Pk0 metrics
on meeting data. That is, we could interpret that
all three systems give completely wrong segmen-
tations on meeting data (due to the fact that topic
shifts are subtler and not as abrupt as in TDT and
artificial data). Nevertheless, we tend to adopt the
first interpretation, given the weaknesses of Pk, Pk0
and WindowDiff (where misses are less penalised
than false alarms), as discussed in Section 4.
</bodyText>
<sectionHeader confidence="0.533763" genericHeader="method">
5.2.2 The Influence of the Error Metric on
Assessment
</sectionHeader>
<bodyText confidence="0.998983230769231">
By following the quantitative assessment given
by the WindowDiff metric, we observe that the
algorithm labeled N0 is three times better than
the algorithm All on meeting data (see Figure 3),
while the same algorithm N0 is considered only
two times better than All on the artificial data (see
Figure 1). This verifies the limitation of the Win-
dowDiff metric discussed in Section 4.
The four error metrics described in detail in
Section 4 have shown that the effect of knowing
the average number of boundaries on C99 is posi-
tive when testing on meeting data. However if we
want to take into account all the four error met-
</bodyText>
<page confidence="0.995163">
149
</page>
<table confidence="0.980823166666667">
100
e 80
60
E 40
20
0
TextTiling C99 TextSeg C99_KA TextSeg_K N0 All RK RU
A
Pk 34.75 11.01 7.89 10 7.15 44.12 55.5 47.71 52.51
P&apos;k 35.1 13.21 8.55 10.94 7.87 44.13 99.58 48.85 80.84
WD 35.73 13.58 9.21 11.34 8.59 43.1 99.59 48.89 80.63
Pr_error 33.33 9.1 7.71 9.34 6.87 49.87 49.79 41.61 45.01
</table>
<figureCaption confidence="0.861511">
Figure 1: Error rates of the segmentation systems on artificial data, where k = 42 and P3ey = 0.44.
</figureCaption>
<figure confidence="0.640004">
100
80
60
E
40
20
0
</figure>
<table confidence="0.763280666666667">
TextTiling C99 TextSeg C99_KA TextSeg_K N0 All RK RU
A
Pk 40.7 21.36 13.97 18.83 11.33 36.02 63.93 37.03 60.04
P&apos;k 44.92 29.5 20.37 27.69 21.4 36.04 100 45.28 89.93
WD 44.76 36.28 30.3 40.26 31.46 46.69 100 53.75 91.92
Pr_error 34.09 25.69 25.62 27.17 21.05 49.96 50 44.89 48.31
</table>
<figureCaption confidence="0.995999">
Figure 2: Error rates of the segmentation systems on TDT data, where k = 55 and P3ey = 0.3606.
</figureCaption>
<bodyText confidence="0.999848428571429">
rics, it is difficult to draw definite conclusions re-
garding the influence of knowing the average num-
ber of boundaries on TextSeg and C99 algorithms.
For example, when tested on TDT data, C99 KA
seems to work better than C99 by Pk and Pk0 met-
rics, while the WindowDiff metric gives a contra-
dictory assessment.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999985">
By comparing the performance of three systems
for thematic segmentation on different kinds of
data, we address two important issues in a quan-
titative evaluation. Strong emphasis was put on
the kind of data used for evaluation and we have
demonstrated experimentally that evaluation on
synthetic data is potentially misleading. The sec-
ond major issue addressed in this paper concerns
the choice of a valuable error metric and its side
effects on the evaluation assessment.
</bodyText>
<sectionHeader confidence="0.998309" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9962304">
This work is supported by the Interactive
Multimodal Information Management project
(http://www.im2.ch/). Many thanks to Andrei
Popescu-Belis and the anonymous reviewers for
their valuable comments. We are grateful to the
International Computer Science Institute (ICSI),
University of California for sharing the data with
us. We also wish to thank Michael Galley who
kindly provided us the thematic annotations of
ICSI data.
</bodyText>
<sectionHeader confidence="0.997864" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992636">
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
Detection and Tracking Pilot Study: Final Re-
port. In DARPA Broadcast News Transcription and
Understanding Workshop, pages 194–218, Lands-
downe, VA. Morgan Kaufmann.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation.
Machine Learning, 34(Special Issue on Natural Lan-
guage Learning):177–210.
Gillian Brown and George Yule. 1998. Discourse
Analysis. (Cambridge Textbooks in Linguistics),
Cambridge.
Freddy Choi. 2000. Advances in Domain Independent
Linear Text Segmentation. In Proceedings of the 1st
</reference>
<page confidence="0.978742">
150
</page>
<figure confidence="0.9967754375">
120
100
te 80
a
r
r 60
o
r
Er 40
20
0
TextTiling C99 TextSeg C99 KA Text A g_K N0 All RK RU
P_k 38.22 54.62 40.82 35.65 35.94 30.82 69.09 45.42 68.48
P&apos;_k 39.12 66.78 45.66 39.04 39.6 30.89 100 47.97 95.99
WD 40.82 69.41 49.27 41.98 42.48 29.31 100 49.64 95.48
Pr_error 40.17 40.27 35.45 35.83 36.61 49.8 50 50.7 53.38
</figure>
<figureCaption confidence="0.998971">
Figure 3: Error rates of the segmentation systems on meeting data, where k = 85 and P3,g = 0.3090.
</figureCaption>
<reference confidence="0.999640948051948">
Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle,
USA.
Olivier Ferret. 2002. Using Collocations for Topic
Segmentation and Link Detection. In The 19th In-
ternational Conference on Computational Linguis-
tics, Taipei, Taiwan.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multy-Party Conversation. In Annual
Meeting of the Association for Computational Lin-
guistics, pages 562–569.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, Intentions and the Structure of Discourse.
Computational Linguistics, 12:175–204.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Marti Hearst and Christian Plaunt. 1993. Subtopic
Structuring for Full-Length Document Access.
In Proceedings of the 16th Annual International
ACM/SIGIR Conference, pages 59–68, Pittsburgh,
Pennsylvania, United States.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33–64.
Julia Hirschberg and Christine Nakatani. 1996.
A Prosodic Analysis of Discourse Segments in
Direction-Giving Monologues. In Proceedings of
the 34th Annual Meeting on Association for Com-
putational Linguistics, pages 286 – 293, Santa Cruz,
California.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Macias-Guarasa, Nel-
son Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI Meeting Project: Resources and Re-
search. In ICASSP 2004 Meeting Recognition Work-
shop (NIST RT-04 Spring Recognition Evaluation),
Montreal.
David Kauchak and Francine Chen. 2005. Feature-
based segmentation of narrative documents. In Pro-
ceedings of the ACL Workshop on Feature Engi-
neering for Machine Learning in Natural Language
Processing, pages 32–39, Ann Arbor; MI; USA.
Hideki Kozima and Teiji Furugori. 1994. Segmenting
Narrative Text into Coherent Scenes. Literary and
Linguistic Computing, 9:13–19.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins Pub Co.
Chris Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press Cambridge, MA, USA.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press
Cambridge, MA, USA.
Rebecca J. Passonneau and Diane J. Litman. 1996.
Empirical Analysis of Three Dimensions of Spoken
Discourse: Segmentation, Coherence and Linguistic
Devices.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse Segmentation by Human and Automated
Means. Computational Linguistics, 23(1).
Lev Pevzner and Marti Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 16(1):19–
36.
Martin Porter. 1980. An Algorithm for Suffix Strip-
ping. Program, 14:130 – 137.
Jeffrey Reynar. 1998. Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of Penn-
sylvania.
TDT. 1998. The Topic Detection and Tracking - Phase
2 Evaluation Plan. Available from World Wide Web:
http://www.nist.gov/speech/tests/tdt/tdt98/index.htm.
Masao Utiyama and Hitoshi Isahara. 2001. A Statisti-
cal Model for Domain-Independent Text Segmenta-
tion. In ACL/EACL, pages 491–498.
</reference>
<page confidence="0.998317">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.399387">
<title confidence="0.997236">An Analysis of Quantitative Aspects in the Evaluation of Segmentation Algorithms</title>
<author confidence="0.999537">Maria Georgescul Alexander Clark Susan Armstrong</author>
<affiliation confidence="0.9993915">ISSCO/TIM, ETI Department of Computer Science ISSCO/TIM, University of Geneva Royal Holloway University of London University of</affiliation>
<address confidence="0.460709">1211 Geneva, Switzerland Egham, Surrey TW20 0EX, UK 1211 Geneva,</address>
<email confidence="0.81927">maria.georgescul@eti.unige.chalexc@cs.rhul.ac.uksusan.armstrong@issco.unige.ch</email>
<abstract confidence="0.997981833333333">We consider here the task of linear thematic segmentation of text documents, by using features based on word distributions in the text. For this task, a typical and often implicit assumption in previous studies is that a document has just one topic and therefore many algorithms have been tested and have shown encouraging results on artificial data sets, generated by putting together parts of different documents. We show that evaluation on synthetic data is potentially misleading and fails to give an accurate evaluation of the performance on real data. Moreover, we provide a critical review of existing evaluation metrics in the literature and we propose an improved evaluation metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic Detection and Tracking Pilot Study: Final Report.</title>
<date>1998</date>
<booktitle>In DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>194--218</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Landsdowne, VA.</location>
<contexts>
<context position="13277" citStr="Allan et al., 1998" startWordPosition="2125" endWordPosition="2128">he same segment, and zero otherwise. Similarly, the function δhyp(i, j) is evaluated to one, if the two indices are hypothesized by the automatic procedure to belong in the same segment, and zero otherwise. The ® operator is the XNOR function ‘both or neither’. D(i, j) is a “distance probability distribution over the set of possible distances between sentences chosen randomly from the corpus”. In practice, a distribution D having “all its probability mass at a fixed distance k” (Beeferman et al., 1999) was adopted and the metric PD was thus renamed Pk. In the framework of the TDT initiative, (Allan et al., 1998) give the following formal definition of Pk and its components: Pk = PMiss &apos; Pseg + PFalseAlarm &apos; (1 − Pseg), where: PMiss = PN—k , PN�k i=1 [δhyp(i,i+k)]·[1−δref(i,i+k)] i=1 [1−δref(i,i+k)] PN�k i=1 [1−δhyp(i,i+k)]·[δref(i,i+k)] PFalseAlarm = PN�k , i=1 δref(i,i+k) and Pseg is the a priori probability that in the reference data a boundary occurs within an interval of k words. Therefore Pk is calculated by moving a window of a certain width k, where k is usually set to half of the average number of words per segment in the gold standard. Pevzner and Hearst (2002) highlighted several problems o</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic Detection and Tracking Pilot Study: Final Report. In DARPA Broadcast News Transcription and Understanding Workshop, pages 194–218, Landsdowne, VA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical Models for Text Segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning, 34(Special Issue on Natural Language Learning):177–210.</booktitle>
<contexts>
<context position="11627" citStr="Beeferman et al., 1999" startWordPosition="1836" endWordPosition="1839">considering the agreement between at least three human annotations. Each meeting is thus divided into contiguous major topic segments and contains an average of 7.32 segments. Note that thematic segmentation of meeting data is a more challenging task as the thematic transitions are subtler than those in TDT data. 4 Evaluation Metrics In this section, we will look in detail at the error metrics that have been proposed in previous studies and examine their inadequacies. In addition, we propose a new evaluation metric that we consider more appropriate. 4.1 Pk Metric (Passonneau and Litman, 1996; Beeferman et al., 1999) underlined that the standard evaluation metrics of precision and recall are inadequate for thematic segmentation, namely by the fact that these metrics did not account for how far away is a hypothesized boundary (i.e. a boundary found by the automatic procedure) from a reference boundary (i.e. a boundary found in the reference data). On the other hand, it is desirable that an algorithm that places for instance a boundary just one utterance away from the reference boundary to be penalized less than an algorithm that places a boundary two (or more) utterances away from the reference boundary. H</context>
<context position="13165" citStr="Beeferman et al., 1999" startWordPosition="2104" endWordPosition="2107">n δref(i, j) is evaluated to one if the two reference corpus indices specified by its parameters i and j belong in the same segment, and zero otherwise. Similarly, the function δhyp(i, j) is evaluated to one, if the two indices are hypothesized by the automatic procedure to belong in the same segment, and zero otherwise. The ® operator is the XNOR function ‘both or neither’. D(i, j) is a “distance probability distribution over the set of possible distances between sentences chosen randomly from the corpus”. In practice, a distribution D having “all its probability mass at a fixed distance k” (Beeferman et al., 1999) was adopted and the metric PD was thus renamed Pk. In the framework of the TDT initiative, (Allan et al., 1998) give the following formal definition of Pk and its components: Pk = PMiss &apos; Pseg + PFalseAlarm &apos; (1 − Pseg), where: PMiss = PN—k , PN�k i=1 [δhyp(i,i+k)]·[1−δref(i,i+k)] i=1 [1−δref(i,i+k)] PN�k i=1 [1−δhyp(i,i+k)]·[δref(i,i+k)] PFalseAlarm = PN�k , i=1 δref(i,i+k) and Pseg is the a priori probability that in the reference data a boundary occurs within an interval of k words. Therefore Pk is calculated by moving a window of a certain width k, where k is usually set to half of the av</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical Models for Text Segmentation. Machine Learning, 34(Special Issue on Natural Language Learning):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gillian Brown</author>
<author>George Yule</author>
</authors>
<title>Discourse Analysis. (Cambridge Textbooks in Linguistics),</title>
<date>1998</date>
<location>Cambridge.</location>
<contexts>
<context position="1485" citStr="Brown and Yule, 1998" startWordPosition="215" endWordPosition="218">show that evaluation on synthetic data is potentially misleading and fails to give an accurate evaluation of the performance on real data. Moreover, we provide a critical review of existing evaluation metrics in the literature and we propose an improved evaluation metric. 1 Introduction The goal of thematic segmentation is to identify boundaries of topically coherent segments in text documents. Giving a rigorous definition of the notion of topic is difficult, but the task of discourse/dialogue segmentation into thematic episodes is usually described by invoking an “intuitive notion of topic” (Brown and Yule, 1998). Thematic segmentation also relates to several notions such as speaker’s intention, topic flow and cohesion. Since it is elusive what mental representations humans use in order to distinguish a coherent text, different surface markers (Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997) and external knowledge sources (Kozima and Furugori, 1994) have been exploited for the purpose of automatic thematic segmentation. Halliday and Hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion. The major cl</context>
</contexts>
<marker>Brown, Yule, 1998</marker>
<rawString>Gillian Brown and George Yule. 1998. Discourse Analysis. (Cambridge Textbooks in Linguistics), Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Choi</author>
</authors>
<title>Advances in Domain Independent Linear Text Segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Seattle, USA.</location>
<contexts>
<context position="5062" citStr="Choi, 2000" startWordPosition="764" endWordPosition="765">tion metrics and their weaknesses, as well as a new evaluation metric that we propose. Section 5 presents our experimental set-up and shows comparisons between the performance of different systems. Finally, some conclusions are drawn in Section 6. 2 Comparison of Systems Combinations of different features (derived for example from linguistic, prosodic information) have been explored in previous studies like (Galley et al., 2003) and (Kauchak and Chen, 2005). In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). In the following, we briefly review these approaches. 2.1 TextTiling Algorithm The TextTiling algorithm was initially developed by Hearst (1997) for segmentation of expository texts into multi-paragraph thematic episodes having a linear, non-overlapping structure (as reflected by the name of the algorithm). TextTiling is widely used as a de-facto standard in the evaluation of alternative segmentation systems, e.g. (Reynar, 1998; Ferret, 2002; Galley et al., 2003). The algorithm can briefly be described by the following steps. Step 1 includes stop-word </context>
<context position="6402" citStr="Choi, 2000" startWordPosition="976" endWordPosition="977">determines a score for each gap between two consecutive token-sequences, by computing the cosine similarity (Manning and Sch¨utze, 1999) between the two vectors representing the frequencies of the words in the two blocks. Step 3 computes a ‘depth score’ for each tokensequence gap, based on the local minima of the score computed in step 2. Step 4 consists in smoothing the scores. Step 5 chooses from any potential boundaries those that have the scores smaller than a certain ‘cutoff function’, based on the average and standard deviation of score distribution. 2.2 C99 Algorithm The C99 algorithm (Choi, 2000) makes a linear segmentation based on a divisive clustering strategy and the cosine similarity measure between any two minimal units. More exactly, the algorithm consists of the following steps. Step 1: after the division of the text into minimal units (in our experiments, the minimal unit is an utterance1), stop words are removed and a stemmer is applied. The second step consists of constructing a similarity matrix 5mxm, where m is the number of utterances and an element sib of the matrix corresponds to the cosine similarity between the vectors representing the frequencies of the words in the</context>
<context position="9388" citStr="Choi, 2000" startWordPosition="1474" endWordPosition="1475">uld provide the gold standard. The problem is that the procedure of building such a reference corpus is expensive. That is, the typical setting involves an experiment with several human subjects, who are asked to mark thematic segment boundaries based on specific guidelines and their intuition. The inter-annotator agreement provides the reference segmentation. This expense can be avoided by constructing a synthetic reference corpus by concatenation of segments from different documents. Therefore, the use of artificial data for evaluation is a general trend in many studies, e.g. (Ferret, 2002; Choi, 2000; Utiyama and Isahara, 2001). In our experiment, we used artificial and real data, i.e. the algorithms have been tested on the following data sets containing English texts. 3.1 Artificially Generated Data Choi (2000) designed an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus. Any test sample from this dataset consists of ten segments. Each segment contains the first n sentences (where 3 &lt; n &lt; 11) of a randomly selected document from the Brown corpus. From this dataset, we randomly chose for our evaluation 100 test samples, where </context>
<context position="21970" citStr="Choi, 2000" startWordPosition="3656" endWordPosition="3657">t text. Each of the segmentations was evaluated with Pk, Pk0 and WindowDiff, as described in Section 4. 5.2 Comparative Performance of Segmentation Systems The results of applying each segmentation algorithm to the three distinct datasets are summarized in Figures 1, 2 and 3. Percent error values are given in the figures and we used the following abbreviations: WD to denote WindowDiff error metric; TextSeg KA to denote the TextSeg algorithm (Utiyama and Isahara, 2001) when the average number of boundaries in the reference data was provided to the algorithm; C99 KA to denote the C99 algorithm (Choi, 2000) when the average number of boundaries in the reference data was provided to the algorithm; N0 to denote the algorithm proposing a segmentation with no boundaries; All to denote the algorithm proposing the degenerate segmentation all boundaries; RK to denote the algorithm that generates a random known segmentation; and RU to denote the algorithm that generates a random unknown segmentation. 5.2.1 Comparison of System Performance from Artificial to Realistic Data From the artificial data to the more realistic data, we expect to have more noise and thus the algorithms to constantly degrade, but </context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Choi. 2000. Advances in Domain Independent Linear Text Segmentation. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Using Collocations for Topic Segmentation and Link Detection.</title>
<date>2002</date>
<booktitle>In The 19th International Conference on Computational Linguistics,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5549" citStr="Ferret, 2002" startWordPosition="835" endWordPosition="836">ted for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). In the following, we briefly review these approaches. 2.1 TextTiling Algorithm The TextTiling algorithm was initially developed by Hearst (1997) for segmentation of expository texts into multi-paragraph thematic episodes having a linear, non-overlapping structure (as reflected by the name of the algorithm). TextTiling is widely used as a de-facto standard in the evaluation of alternative segmentation systems, e.g. (Reynar, 1998; Ferret, 2002; Galley et al., 2003). The algorithm can briefly be described by the following steps. Step 1 includes stop-word removal, lemmatization and division of the text into ‘token-sequences’ (i.e. text blocks having a fixed number of words). Step 2 determines a score for each gap between two consecutive token-sequences, by computing the cosine similarity (Manning and Sch¨utze, 1999) between the two vectors representing the frequencies of the words in the two blocks. Step 3 computes a ‘depth score’ for each tokensequence gap, based on the local minima of the score computed in step 2. Step 4 consists i</context>
<context position="9376" citStr="Ferret, 2002" startWordPosition="1472" endWordPosition="1473">annotators should provide the gold standard. The problem is that the procedure of building such a reference corpus is expensive. That is, the typical setting involves an experiment with several human subjects, who are asked to mark thematic segment boundaries based on specific guidelines and their intuition. The inter-annotator agreement provides the reference segmentation. This expense can be avoided by constructing a synthetic reference corpus by concatenation of segments from different documents. Therefore, the use of artificial data for evaluation is a general trend in many studies, e.g. (Ferret, 2002; Choi, 2000; Utiyama and Isahara, 2001). In our experiment, we used artificial and real data, i.e. the algorithms have been tested on the following data sets containing English texts. 3.1 Artificially Generated Data Choi (2000) designed an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus. Any test sample from this dataset consists of ten segments. Each segment contains the first n sentences (where 3 &lt; n &lt; 11) of a randomly selected document from the Brown corpus. From this dataset, we randomly chose for our evaluation 100 test sam</context>
</contexts>
<marker>Ferret, 2002</marker>
<rawString>Olivier Ferret. 2002. Using Collocations for Topic Segmentation and Link Detection. In The 19th International Conference on Computational Linguistics, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Galley</author>
<author>Kathleen McKeown</author>
<author>Eric FoslerLuissier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse Segmentation of Multy-Party Conversation.</title>
<date>2003</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="4883" citStr="Galley et al., 2003" startWordPosition="733" endWordPosition="736">er is organized as follows. Section 2 and Section 3 describe various systems and, respectively, different input data selected for our evaluation. Section 4 presents several existing evaluation metrics and their weaknesses, as well as a new evaluation metric that we propose. Section 5 presents our experimental set-up and shows comparisons between the performance of different systems. Finally, some conclusions are drawn in Section 6. 2 Comparison of Systems Combinations of different features (derived for example from linguistic, prosodic information) have been explored in previous studies like (Galley et al., 2003) and (Kauchak and Chen, 2005). In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). In the following, we briefly review these approaches. 2.1 TextTiling Algorithm The TextTiling algorithm was initially developed by Hearst (1997) for segmentation of expository texts into multi-paragraph thematic episodes having a linear, non-overlapping structure (as reflected by the name of the algorithm). TextTiling is widely used as a de-facto standard in the evaluation of </context>
<context position="10980" citStr="Galley et al., 2003" startWordPosition="1728" endWordPosition="1731">ews stories have been created. In our evaluation, we used a subset of 28 documents randomly selected from the TDT Phase 2 (TDT2) collection, where a document contains an average of 24.67 segments. 3.3 Meeting Transcripts The third dataset used in our evaluation contains 25 meeting transcripts from the ICSI-MR corpus (Janin et al., 2004). The entire corpus contains high-quality close talking microphone recordings of multi-party dialogues. Transcriptions at word level with utterance-level segmentations are also available. The gold standard for thematic segmentations has been kindly provided by (Galley et al., 2003) and has been chosen by considering the agreement between at least three human annotations. Each meeting is thus divided into contiguous major topic segments and contains an average of 7.32 segments. Note that thematic segmentation of meeting data is a more challenging task as the thematic transitions are subtler than those in TDT data. 4 Evaluation Metrics In this section, we will look in detail at the error metrics that have been proposed in previous studies and examine their inadequacies. In addition, we propose a new evaluation metric that we consider more appropriate. 4.1 Pk Metric (Passo</context>
</contexts>
<marker>Galley, McKeown, FoslerLuissier, Jing, 2003</marker>
<rawString>Michael Galley, Kathleen McKeown, Eric FoslerLuissier, and Hongyan Jing. 2003. Discourse Segmentation of Multy-Party Conversation. In Annual Meeting of the Association for Computational Linguistics, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intentions and the Structure of Discourse. Computational Linguistics,</booktitle>
<pages>12--175</pages>
<contexts>
<context position="2411" citStr="Grosz and Sidner, 1986" startWordPosition="354" endWordPosition="357">ernal knowledge sources (Kozima and Furugori, 1994) have been exploited for the purpose of automatic thematic segmentation. Halliday and Hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion. The major classes of such text-forming resources identified in (Halliday and Hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation. In this paper, we examine one form of lexical cohesion, namely lexical reiteration. Following some of the most prominent discourse theories in literature (Grosz and Sidner, 1986; Marcu, 2000), a hierarchical representation of the thematic episodes can be proposed. The basis for this is the idea that topics can be recursively divided into subtopics. Real texts exhibit a more intricate structure, including ‘semantic returns’ by which a topic is suspended at one point and resumed later in the discourse. However, we focus here on a reduced segmentation problem, which involves identifying non-overlapping and non-hierarchical segments at a coarse level of granularity. Thematic segmentation is a valuable initial tool in information retrieval and natural language processing.</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, Intentions and the Structure of Discourse. Computational Linguistics, 12:175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="1938" citStr="Halliday and Hasan (1976)" startWordPosition="281" endWordPosition="284"> is difficult, but the task of discourse/dialogue segmentation into thematic episodes is usually described by invoking an “intuitive notion of topic” (Brown and Yule, 1998). Thematic segmentation also relates to several notions such as speaker’s intention, topic flow and cohesion. Since it is elusive what mental representations humans use in order to distinguish a coherent text, different surface markers (Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997) and external knowledge sources (Kozima and Furugori, 1994) have been exploited for the purpose of automatic thematic segmentation. Halliday and Hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion. The major classes of such text-forming resources identified in (Halliday and Hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation. In this paper, we examine one form of lexical cohesion, namely lexical reiteration. Following some of the most prominent discourse theories in literature (Grosz and Sidner, 1986; Marcu, 2000), a hierarchical representation of the thematic episodes can be proposed. The basis for this is the idea that top</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
<author>Christian Plaunt</author>
</authors>
<title>Subtopic Structuring for Full-Length Document Access.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International ACM/SIGIR Conference,</booktitle>
<pages>59--68</pages>
<location>Pittsburgh, Pennsylvania, United States.</location>
<contexts>
<context position="3267" citStr="Hearst and Plaunt, 1993" startWordPosition="485" endWordPosition="488">semantic returns’ by which a topic is suspended at one point and resumed later in the discourse. However, we focus here on a reduced segmentation problem, which involves identifying non-overlapping and non-hierarchical segments at a coarse level of granularity. Thematic segmentation is a valuable initial tool in information retrieval and natural language processing. For instance, in information access systems, smaller and coherent passage retrieval is more convenient to the user than wholedocument retrieval and thematic segmentation has been shown to improve the passage-retrieval performance (Hearst and Plaunt, 1993). In cases such as collections of transcripts there are no headers or paragraph markers. Therefore a clear separation of the text into thematic episodes can be used together with highlighted keywords as a kind of ‘quick read guide’ to help users to quickly navigate through and understand the text. Moreover automatic thematic segmentation has been shown to play an important role in automatic summarization (Mani, 2001), anaphora resolution and dis144 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 144–151, Sydney, July 2006. c�2006 Association for Computational Linguisti</context>
</contexts>
<marker>Hearst, Plaunt, 1993</marker>
<rawString>Marti Hearst and Christian Plaunt. 1993. Subtopic Structuring for Full-Length Document Access. In Proceedings of the 16th Annual International ACM/SIGIR Conference, pages 59–68, Pittsburgh, Pennsylvania, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="5044" citStr="Hearst, 1997" startWordPosition="761" endWordPosition="762">eral existing evaluation metrics and their weaknesses, as well as a new evaluation metric that we propose. Section 5 presents our experimental set-up and shows comparisons between the performance of different systems. Finally, some conclusions are drawn in Section 6. 2 Comparison of Systems Combinations of different features (derived for example from linguistic, prosodic information) have been explored in previous studies like (Galley et al., 2003) and (Kauchak and Chen, 2005). In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). In the following, we briefly review these approaches. 2.1 TextTiling Algorithm The TextTiling algorithm was initially developed by Hearst (1997) for segmentation of expository texts into multi-paragraph thematic episodes having a linear, non-overlapping structure (as reflected by the name of the algorithm). TextTiling is widely used as a de-facto standard in the evaluation of alternative segmentation systems, e.g. (Reynar, 1998; Ferret, 2002; Galley et al., 2003). The algorithm can briefly be described by the following steps. Step 1 i</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti Hearst. 1997. TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Christine Nakatani</author>
</authors>
<title>A Prosodic Analysis of Discourse Segments in Direction-Giving Monologues.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>286--293</pages>
<location>Santa Cruz, California.</location>
<contexts>
<context position="1751" citStr="Hirschberg and Nakatani, 1996" startWordPosition="254" endWordPosition="257">uation metric. 1 Introduction The goal of thematic segmentation is to identify boundaries of topically coherent segments in text documents. Giving a rigorous definition of the notion of topic is difficult, but the task of discourse/dialogue segmentation into thematic episodes is usually described by invoking an “intuitive notion of topic” (Brown and Yule, 1998). Thematic segmentation also relates to several notions such as speaker’s intention, topic flow and cohesion. Since it is elusive what mental representations humans use in order to distinguish a coherent text, different surface markers (Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997) and external knowledge sources (Kozima and Furugori, 1994) have been exploited for the purpose of automatic thematic segmentation. Halliday and Hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion. The major classes of such text-forming resources identified in (Halliday and Hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation. In this paper, we examine one form of lexical cohesion, namely lexical reiteration. Following some of the most promin</context>
</contexts>
<marker>Hirschberg, Nakatani, 1996</marker>
<rawString>Julia Hirschberg and Christine Nakatani. 1996. A Prosodic Analysis of Discourse Segments in Direction-Giving Monologues. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, pages 286 – 293, Santa Cruz, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Jeremy Ang</author>
<author>Sonali Bhagat</author>
<author>Rajdip Dhillon</author>
<author>Jane Edwards</author>
<author>Javier Macias-Guarasa</author>
<author>Nelson Morgan</author>
<author>Barbara Peskin</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Chuck Wooters</author>
<author>Britta Wrede</author>
</authors>
<date>2004</date>
<booktitle>The ICSI Meeting Project: Resources and Research. In ICASSP 2004 Meeting Recognition Workshop (NIST RT-04 Spring Recognition Evaluation),</booktitle>
<location>Montreal.</location>
<contexts>
<context position="10698" citStr="Janin et al., 2004" startWordPosition="1690" endWordPosition="1693"> used data sets for topic segmentation emerged from the Topic Detection and Tracking (TDT) project, which includes the task of story segmentation, i.e. the task of segmenting a stream of news data into topically cohesive stories. As part of the TDT initiative several datasets of news stories have been created. In our evaluation, we used a subset of 28 documents randomly selected from the TDT Phase 2 (TDT2) collection, where a document contains an average of 24.67 segments. 3.3 Meeting Transcripts The third dataset used in our evaluation contains 25 meeting transcripts from the ICSI-MR corpus (Janin et al., 2004). The entire corpus contains high-quality close talking microphone recordings of multi-party dialogues. Transcriptions at word level with utterance-level segmentations are also available. The gold standard for thematic segmentations has been kindly provided by (Galley et al., 2003) and has been chosen by considering the agreement between at least three human annotations. Each meeting is thus divided into contiguous major topic segments and contains an average of 7.32 segments. Note that thematic segmentation of meeting data is a more challenging task as the thematic transitions are subtler tha</context>
</contexts>
<marker>Janin, Ang, Bhagat, Dhillon, Edwards, Macias-Guarasa, Morgan, Peskin, Shriberg, Stolcke, Wooters, Wrede, 2004</marker>
<rawString>Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon, Jane Edwards, Javier Macias-Guarasa, Nelson Morgan, Barbara Peskin, Elizabeth Shriberg, Andreas Stolcke, Chuck Wooters, and Britta Wrede. 2004. The ICSI Meeting Project: Resources and Research. In ICASSP 2004 Meeting Recognition Workshop (NIST RT-04 Spring Recognition Evaluation), Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Francine Chen</author>
</authors>
<title>Featurebased segmentation of narrative documents.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing,</booktitle>
<pages>32--39</pages>
<publisher>MI; USA.</publisher>
<location>Ann Arbor;</location>
<contexts>
<context position="4912" citStr="Kauchak and Chen, 2005" startWordPosition="738" endWordPosition="741">. Section 2 and Section 3 describe various systems and, respectively, different input data selected for our evaluation. Section 4 presents several existing evaluation metrics and their weaknesses, as well as a new evaluation metric that we propose. Section 5 presents our experimental set-up and shows comparisons between the performance of different systems. Finally, some conclusions are drawn in Section 6. 2 Comparison of Systems Combinations of different features (derived for example from linguistic, prosodic information) have been explored in previous studies like (Galley et al., 2003) and (Kauchak and Chen, 2005). In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). In the following, we briefly review these approaches. 2.1 TextTiling Algorithm The TextTiling algorithm was initially developed by Hearst (1997) for segmentation of expository texts into multi-paragraph thematic episodes having a linear, non-overlapping structure (as reflected by the name of the algorithm). TextTiling is widely used as a de-facto standard in the evaluation of alternative segmentation syst</context>
</contexts>
<marker>Kauchak, Chen, 2005</marker>
<rawString>David Kauchak and Francine Chen. 2005. Featurebased segmentation of narrative documents. In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing, pages 32–39, Ann Arbor; MI; USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
<author>Teiji Furugori</author>
</authors>
<date>1994</date>
<booktitle>Segmenting Narrative Text into Coherent Scenes. Literary and Linguistic Computing,</booktitle>
<pages>9--13</pages>
<contexts>
<context position="1840" citStr="Kozima and Furugori, 1994" startWordPosition="266" endWordPosition="270"> topically coherent segments in text documents. Giving a rigorous definition of the notion of topic is difficult, but the task of discourse/dialogue segmentation into thematic episodes is usually described by invoking an “intuitive notion of topic” (Brown and Yule, 1998). Thematic segmentation also relates to several notions such as speaker’s intention, topic flow and cohesion. Since it is elusive what mental representations humans use in order to distinguish a coherent text, different surface markers (Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997) and external knowledge sources (Kozima and Furugori, 1994) have been exploited for the purpose of automatic thematic segmentation. Halliday and Hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion. The major classes of such text-forming resources identified in (Halliday and Hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation. In this paper, we examine one form of lexical cohesion, namely lexical reiteration. Following some of the most prominent discourse theories in literature (Grosz and Sidner, 1986; Marcu, 2000), a hierarchica</context>
</contexts>
<marker>Kozima, Furugori, 1994</marker>
<rawString>Hideki Kozima and Teiji Furugori. 1994. Segmenting Narrative Text into Coherent Scenes. Literary and Linguistic Computing, 9:13–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Pub Co.</publisher>
<contexts>
<context position="3687" citStr="Mani, 2001" startWordPosition="556" endWordPosition="557">t passage retrieval is more convenient to the user than wholedocument retrieval and thematic segmentation has been shown to improve the passage-retrieval performance (Hearst and Plaunt, 1993). In cases such as collections of transcripts there are no headers or paragraph markers. Therefore a clear separation of the text into thematic episodes can be used together with highlighted keywords as a kind of ‘quick read guide’ to help users to quickly navigate through and understand the text. Moreover automatic thematic segmentation has been shown to play an important role in automatic summarization (Mani, 2001), anaphora resolution and dis144 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 144–151, Sydney, July 2006. c�2006 Association for Computational Linguistics course/dialogue understanding. In this paper, we concern ourselves with the task of linear thematic segmentation and are interested in finding out whether different segmentation systems can perform well on artificial and real data sets without specific parameter tuning. In addition, we will refer to the implications of the choice of a particular error metric for evaluation results. This paper is organized as follo</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic Summarization. John Benjamins Pub Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Chris Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="2425" citStr="Marcu, 2000" startWordPosition="358" endWordPosition="359">(Kozima and Furugori, 1994) have been exploited for the purpose of automatic thematic segmentation. Halliday and Hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion. The major classes of such text-forming resources identified in (Halliday and Hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation. In this paper, we examine one form of lexical cohesion, namely lexical reiteration. Following some of the most prominent discourse theories in literature (Grosz and Sidner, 1986; Marcu, 2000), a hierarchical representation of the thematic episodes can be proposed. The basis for this is the idea that topics can be recursively divided into subtopics. Real texts exhibit a more intricate structure, including ‘semantic returns’ by which a topic is suspended at one point and resumed later in the discourse. However, we focus here on a reduced segmentation problem, which involves identifying non-overlapping and non-hierarchical segments at a coarse level of granularity. Thematic segmentation is a valuable initial tool in information retrieval and natural language processing. For instance,</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Empirical Analysis of Three Dimensions of Spoken Discourse: Segmentation, Coherence and Linguistic Devices.</title>
<date>1996</date>
<contexts>
<context position="11602" citStr="Passonneau and Litman, 1996" startWordPosition="1832" endWordPosition="1835">2003) and has been chosen by considering the agreement between at least three human annotations. Each meeting is thus divided into contiguous major topic segments and contains an average of 7.32 segments. Note that thematic segmentation of meeting data is a more challenging task as the thematic transitions are subtler than those in TDT data. 4 Evaluation Metrics In this section, we will look in detail at the error metrics that have been proposed in previous studies and examine their inadequacies. In addition, we propose a new evaluation metric that we consider more appropriate. 4.1 Pk Metric (Passonneau and Litman, 1996; Beeferman et al., 1999) underlined that the standard evaluation metrics of precision and recall are inadequate for thematic segmentation, namely by the fact that these metrics did not account for how far away is a hypothesized boundary (i.e. a boundary found by the automatic procedure) from a reference boundary (i.e. a boundary found in the reference data). On the other hand, it is desirable that an algorithm that places for instance a boundary just one utterance away from the reference boundary to be penalized less than an algorithm that places a boundary two (or more) utterances away from </context>
</contexts>
<marker>Passonneau, Litman, 1996</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1996. Empirical Analysis of Three Dimensions of Spoken Discourse: Segmentation, Coherence and Linguistic Devices.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse Segmentation by Human and Automated Means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1781" citStr="Passonneau and Litman, 1997" startWordPosition="258" endWordPosition="261">he goal of thematic segmentation is to identify boundaries of topically coherent segments in text documents. Giving a rigorous definition of the notion of topic is difficult, but the task of discourse/dialogue segmentation into thematic episodes is usually described by invoking an “intuitive notion of topic” (Brown and Yule, 1998). Thematic segmentation also relates to several notions such as speaker’s intention, topic flow and cohesion. Since it is elusive what mental representations humans use in order to distinguish a coherent text, different surface markers (Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997) and external knowledge sources (Kozima and Furugori, 1994) have been exploited for the purpose of automatic thematic segmentation. Halliday and Hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion. The major classes of such text-forming resources identified in (Halliday and Hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation. In this paper, we examine one form of lexical cohesion, namely lexical reiteration. Following some of the most prominent discourse theories in lite</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse Segmentation by Human and Automated Means. Computational Linguistics, 23(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti Hearst</author>
</authors>
<title>A Critique and Improvement of an Evaluation Metric for Text Segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>36</pages>
<contexts>
<context position="13846" citStr="Pevzner and Hearst (2002)" startWordPosition="2222" endWordPosition="2225">he framework of the TDT initiative, (Allan et al., 1998) give the following formal definition of Pk and its components: Pk = PMiss &apos; Pseg + PFalseAlarm &apos; (1 − Pseg), where: PMiss = PN—k , PN�k i=1 [δhyp(i,i+k)]·[1−δref(i,i+k)] i=1 [1−δref(i,i+k)] PN�k i=1 [1−δhyp(i,i+k)]·[δref(i,i+k)] PFalseAlarm = PN�k , i=1 δref(i,i+k) and Pseg is the a priori probability that in the reference data a boundary occurs within an interval of k words. Therefore Pk is calculated by moving a window of a certain width k, where k is usually set to half of the average number of words per segment in the gold standard. Pevzner and Hearst (2002) highlighted several problems of the Pk metric. We illustrate below what we consider the main problems of the Pk metric, based on two examples. Let r(i, k) be the number of boundaries between positions i and i + k in the gold standard segmentation and h(i, k) be the number of boundaries between positions i and i+k in the automatic hypothesized segmentation. • Example 1: If r(i, k) = 2 and h(i, k) = 1 then obviously a missing boundary should 2Let ref be a correct segmentation and hyp be a segmentation proposed by a text segmentation system. We will keep this notations in equations introduced be</context>
<context position="15653" citStr="Pevzner and Hearst (2002)" startWordPosition="2562" endWordPosition="2565">i+k)]·[1−δref(i,i+k)] P0 Miss = PN�k i=1[1−δref (i,i+k)] PN�k i=1 [1−Ωhyp(i,i+k)]·[δref (i,i+k)] P0 FalseAlarm = PN�k i=1 δref(i,i+k) where: � 1, if r(i, k) = h(i, k), Ωhyp(i, i + k) = 0, otherwise. We will refer to this new definition of Pk by P0k. Therefore, by taking the definition of Pk0 and the first example above, we obtain δref(i, i + k) = 0 and Ωhyp(i, i + k) = 0 and thus P0Miss is correctly increased. However for the case of example 2 we will obtain δref(i, i + k) = 0 and Ωhyp(i, i + k) = 0, involving no increase of P0FalseAlarm and erroneous increase of P0Miss. 4.2 WindowDiff metric Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. By keeping our notations concerning r(i, k) and h(i, k) introduced in the subsection 4.1, WindowDiff is defined as: PNik[|r(i) k h(i,k)�&gt;0]WindowDiff = N . Similar to both Pk and P0k, WindowDiff is also computed by moving a window of fixed size across the test set and penalizing the algorithm misses or erroneous algorithm boundary detections. However, unlike Pk and P0k, WindowDiff takes into account how many boundaries fall within the window and is penalizing in “how many discrepancies occur between the reference and the system results” rather</context>
<context position="18096" citStr="Pevzner and Hearst, 2002" startWordPosition="2975" endWordPosition="2978">here are relatively few boundaries compared with non-boundaries, a strategy introducing no false alarms, but introducing a maximum number of misses (i.e. k · Bref misses) can be judged as being around 80% correct by the WindowDiff measure. On the other hand, a segmentation with no misses, but with a maximum number of false alarms (i.e. (N − k) false alarms) is judged as being 100% erroneous by the WindowDiff measure. That is, misses and false alarms are not equally penalised. Another issue regarding WindowDiff is that it is not clear “how does one interpret the values produced by the metric” (Pevzner and Hearst, 2002). 4.3 Proposal for a New Metric In order to address the inadequacies of Pk and WindowDiff, we propose a new evaluation metric, defined as follows: Prerror = Cmiss · Prmiss + Cfa · Prfa, where: Cmiss (0 ≤ Cmiss ≤ 1) is the cost of a miss, Cfa (0 ≤ Cfa ≤ 1) is the cost of a false alarm, y��k i=1 [Oref h7/p(i,k)] Prmiss = y��k i=1 [Aref (i,k)] , yN1k [&apos;Fref h7/p(i,k)] Pr fa = N−k � Oref (i, k) 1, if r(i,k) &gt; 0 = 0, otherwise. Prmiss could be interpreted as the probability that the hypothesized segmentation contains less boundaries than the reference segmentation in an interval of k units3, condit</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti Hearst. 2002. A Critique and Improvement of an Evaluation Metric for Text Segmentation. Computational Linguistics, 16(1):19– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An Algorithm for Suffix Stripping. Program,</title>
<date>1980</date>
<pages>14--130</pages>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An Algorithm for Suffix Stripping. Program, 14:130 – 137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Reynar</author>
</authors>
<title>Topic Segmentation: Algorithms and Applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5535" citStr="Reynar, 1998" startWordPosition="833" endWordPosition="834">aper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). In the following, we briefly review these approaches. 2.1 TextTiling Algorithm The TextTiling algorithm was initially developed by Hearst (1997) for segmentation of expository texts into multi-paragraph thematic episodes having a linear, non-overlapping structure (as reflected by the name of the algorithm). TextTiling is widely used as a de-facto standard in the evaluation of alternative segmentation systems, e.g. (Reynar, 1998; Ferret, 2002; Galley et al., 2003). The algorithm can briefly be described by the following steps. Step 1 includes stop-word removal, lemmatization and division of the text into ‘token-sequences’ (i.e. text blocks having a fixed number of words). Step 2 determines a score for each gap between two consecutive token-sequences, by computing the cosine similarity (Manning and Sch¨utze, 1999) between the two vectors representing the frequencies of the words in the two blocks. Step 3 computes a ‘depth score’ for each tokensequence gap, based on the local minima of the score computed in step 2. Ste</context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeffrey Reynar. 1998. Topic Segmentation: Algorithms and Applications. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TDT</author>
</authors>
<title>The Topic Detection and Tracking - Phase 2 Evaluation Plan. Available from World Wide Web: http://www.nist.gov/speech/tests/tdt/tdt98/index.htm.</title>
<date>1998</date>
<contexts>
<context position="14876" citStr="TDT, 1998" startWordPosition="2423" endWordPosition="2424"> missing boundary should 2Let ref be a correct segmentation and hyp be a segmentation proposed by a text segmentation system. We will keep this notations in equations introduced below. be counted in Pk, i.e. PMiss should be increased. • Example 2: If r(i, k) = 1 and h(i, k) = 2 then obviously PFalseAlarm should be increased. However, considering the first example, we will obtain δref(i, i + k) = 0, δhyp(i, i + k) = 0 and consequently PMiss is not increased. By taking the case from the second example we obtain δref(i, i + k) = 0 and δhyp(i, i + k) = 0, involving no increase of PFalseAlarm. In (TDT, 1998), a slightly different definition is given for the Pk metric: the definition of miss and false alarm probabilities is replaced with: PN�k i=1 [1−Ωhyp(i,i+k)]·[1−δref(i,i+k)] P0 Miss = PN�k i=1[1−δref (i,i+k)] PN�k i=1 [1−Ωhyp(i,i+k)]·[δref (i,i+k)] P0 FalseAlarm = PN�k i=1 δref(i,i+k) where: � 1, if r(i, k) = h(i, k), Ωhyp(i, i + k) = 0, otherwise. We will refer to this new definition of Pk by P0k. Therefore, by taking the definition of Pk0 and the first example above, we obtain δref(i, i + k) = 0 and Ωhyp(i, i + k) = 0 and thus P0Miss is correctly increased. However for the case of example 2 </context>
</contexts>
<marker>TDT, 1998</marker>
<rawString>TDT. 1998. The Topic Detection and Tracking - Phase 2 Evaluation Plan. Available from World Wide Web: http://www.nist.gov/speech/tests/tdt/tdt98/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A Statistical Model for Domain-Independent Text Segmentation. In</title>
<date>2001</date>
<booktitle>ACL/EACL,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="5102" citStr="Utiyama and Isahara, 2001" startWordPosition="768" endWordPosition="771">aknesses, as well as a new evaluation metric that we propose. Section 5 presents our experimental set-up and shows comparisons between the performance of different systems. Finally, some conclusions are drawn in Section 6. 2 Comparison of Systems Combinations of different features (derived for example from linguistic, prosodic information) have been explored in previous studies like (Galley et al., 2003) and (Kauchak and Chen, 2005). In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). In the following, we briefly review these approaches. 2.1 TextTiling Algorithm The TextTiling algorithm was initially developed by Hearst (1997) for segmentation of expository texts into multi-paragraph thematic episodes having a linear, non-overlapping structure (as reflected by the name of the algorithm). TextTiling is widely used as a de-facto standard in the evaluation of alternative segmentation systems, e.g. (Reynar, 1998; Ferret, 2002; Galley et al., 2003). The algorithm can briefly be described by the following steps. Step 1 includes stop-word removal, lemmatization and division of t</context>
<context position="7673" citStr="Utiyama and Isahara, 2001" startWordPosition="1195" endWordPosition="1198">: a ‘rank matrix’ Rm�m is computed, by determining for each pair of utterances, the number of neighbors in 5mxm with a lower similarity value. In the final step, the location of thematic boundaries is determined by a divisive top-down clustering procedure. The criterion for division of the current segment B into b1, ...bm subsegments is based on the maximisation of a ‘density’ D, computed for each potential repartition of boundaries as D = �m , k=1 areak where sumk and areak refers to the sum of rank and area of the k-th segment in B, respectively. 2.3 TextSeg Algorithm The TextSeg algorithm (Utiyama and Isahara, 2001) implements a probabilistic approach to determine the most likely segmentation, as briefly described below. The segmentation task is modeled as a problem of finding the minimum cost C(S) of a segmentation S. The segmentation cost is defined as: C(S) ~ −logPr(WjS)Pr(S), 1Occasionally within this document we employ the term utterance to denote either a sentence or an utterance in its proper sense. Ekm=1 sumk 145 where W = w1w2...wn represents the text consisting of n words (after applying stop-words removal and stemming) and S = S1S2...Sm is a potential segmentation of W in m segments. The proba</context>
<context position="9416" citStr="Utiyama and Isahara, 2001" startWordPosition="1476" endWordPosition="1479">the gold standard. The problem is that the procedure of building such a reference corpus is expensive. That is, the typical setting involves an experiment with several human subjects, who are asked to mark thematic segment boundaries based on specific guidelines and their intuition. The inter-annotator agreement provides the reference segmentation. This expense can be avoided by constructing a synthetic reference corpus by concatenation of segments from different documents. Therefore, the use of artificial data for evaluation is a general trend in many studies, e.g. (Ferret, 2002; Choi, 2000; Utiyama and Isahara, 2001). In our experiment, we used artificial and real data, i.e. the algorithms have been tested on the following data sets containing English texts. 3.1 Artificially Generated Data Choi (2000) designed an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus. Any test sample from this dataset consists of ten segments. Each segment contains the first n sentences (where 3 &lt; n &lt; 11) of a randomly selected document from the Brown corpus. From this dataset, we randomly chose for our evaluation 100 test samples, where the length of a segment vari</context>
<context position="21831" citStr="Utiyama and Isahara, 2001" startWordPosition="3629" endWordPosition="3632"> distributed randomly throughout text; and random unknown: the number of boundaries is randomly selected and boundaries are randomly distributed throughout text. Each of the segmentations was evaluated with Pk, Pk0 and WindowDiff, as described in Section 4. 5.2 Comparative Performance of Segmentation Systems The results of applying each segmentation algorithm to the three distinct datasets are summarized in Figures 1, 2 and 3. Percent error values are given in the figures and we used the following abbreviations: WD to denote WindowDiff error metric; TextSeg KA to denote the TextSeg algorithm (Utiyama and Isahara, 2001) when the average number of boundaries in the reference data was provided to the algorithm; C99 KA to denote the C99 algorithm (Choi, 2000) when the average number of boundaries in the reference data was provided to the algorithm; N0 to denote the algorithm proposing a segmentation with no boundaries; All to denote the algorithm proposing the degenerate segmentation all boundaries; RK to denote the algorithm that generates a random known segmentation; and RU to denote the algorithm that generates a random unknown segmentation. 5.2.1 Comparison of System Performance from Artificial to Realistic</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A Statistical Model for Domain-Independent Text Segmentation. In ACL/EACL, pages 491–498.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>