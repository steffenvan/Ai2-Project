<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<affiliation confidence="0.736416333333333">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 204-213.
Association for Computational Linguistics.
</affiliation>
<bodyText confidence="0.999693846153846">
of one particular word as input and assigns word
senses to new test instances of the same word
as output, (supervised) metonymy recognition
can take a set of labelled training instances of
different words belonging to one semantic class
as input and assign literal readings and possi-
ble metonymic patterns to new test instances
of possibly different words of the same seman-
tic class. Thus, it needs to infer from train-
ing instances like Example (3) (when labelled
as a place-for-people metonymy) that Exam-
ples (4) and (5) are also metonymic, a task which
poses no problems for most humans.
</bodyText>
<listItem confidence="0.999333666666667">
(3) &amp;quot;Bosnia&apos;s view of&amp;quot;
(4) &amp;quot;Hungary&apos;s view of&amp;quot;
(5) &amp;quot;Hungary&apos;s position on&amp;quot;
</listItem>
<bodyText confidence="0.999895269230769">
In this paper, we explore this view of
metonymy recognition as a class-based WSD
task for the semantic class of locations.&apos;
The corpus data we use is described in Sec-
tion 2. As resources reliably annotated for
metonymy do not exist (see also Section 6), we
constructed a corpus of location names anno-
tated for literal and metonymic readings.
The supervised classification algorithm we use
are decision lists as they have been successfully
used in several classification tasks (Yarowsky,
1995; Collins and Singer, 1999) (see Section 3).
In Section 4, we explore whether features tradi-
tionally used in WSD carry over to metonymy
resolution, concentrating on (i) cooccurrences;
(ii) collocations; and (iii) grammatical features.
Results are discussed in Section 5. We show that
cooccurrences are in general not appropriate for
metonymy resolution; collocations are useful but
suffer from data sparseness when used as simple
word forms; the grammatical relations subject
and object perform well but are only applicable
to a small part of the data. We then compare our
algorithm to metonymy recognition approaches
based on selectional restriction violations in Sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.982053" genericHeader="abstract">
2 Experimental Data
</sectionHeader>
<bodyText confidence="0.991012571428571">
We present a short overview of the collection
of a corpus of location names and its annota-
tion for literal and metonymic readings. A more
detailed description can be found in (Markert
and Nissim, 2002) and the annotation scheme
is downloadable from http : //www. ltg . ed. ac.
uktemalvi/mas cara/publ i cat ions . html.
</bodyText>
<subsectionHeader confidence="0.934485">
2.1 Corpus Collection
</subsectionHeader>
<bodyText confidence="0.999530470588235">
We extracted all country names from Word-
Net (Fellbaum, 1998) and the CIA factbook
(http: //www. cia. gov/cia/publications/
f actbook/). This collection of names forms our
sampling frame CountryList.
We built a corpus of text samples that con-
tains 1000 occurrences of country names, ran-
domly extracted from the British National Cor-
pus (http : //info . ox . ac .uk/bnc), henceforth
abbreviated as BNC. Any country name in
CountryList was a Possibly Metonymic Word
(PMW, henceforth) and was allowed to occur
in the samples extracted. We searched the BNC
using Gsearch (Corley et al., 2001). All samples
include a PMW surrounded by three sentences
of context. All examples introduced from now
on are from the BNC.2
</bodyText>
<subsectionHeader confidence="0.8514555">
2.2 Annotation Scheme for Location
Names
</subsectionHeader>
<bodyText confidence="0.9995745">
After excluding some undesired examples (i.e.,
noise) which our extraction method collected
(e.g. homonyms, such as &amp;quot;Greenland&amp;quot; in &amp;quot;Pro-
fessor Greenland&amp;quot;), the annotation can pro-
ceed to identify literal, metonymic, and mixed
readings. Our annotation scheme built on lists
of metonymic patterns in the literature (Lakoff
and Johnson, 1980; Fass, 1997; Stern, 1931),
but diverted from these patterns when they did
not provide full coverage or could not be distin-
guished reliably (Markert and Nissim, 2002).
The literal reading for location names com-
prises a locative (see Example (6)) and a politi-
cal entity interpretation (see Example (7)).
</bodyText>
<note confidence="0.4494825">
(6) &amp;quot;coral coast of Papua New Guinea&amp;quot;
&apos;At the moment we restrict ourselves to location
names. 2An exception is Example (12).
(7) &amp;quot;Britain&apos;s current account deficit&amp;quot;
</note>
<bodyText confidence="0.9661538">
For metonymic readings, we distinguish be-
tween general patterns (valid for all physical ob-
jects) and location-specific ones. As general pat-
terns were never encountered in our corpus, we
describe here only the latter.
</bodyText>
<listItem confidence="0.990053">
• place-for-people: a place stands for any
persons/organisations associated with it.
In Example (8), &amp;quot;San Marino&amp;quot; stands for
one of its sports teams.
(8) &amp;quot;a 29th-minute own goal from San
Marino defender&amp;quot;
</listItem>
<bodyText confidence="0.65225625">
Often, the explicit referent is underspeci-
fied, as in Example (9), where the reference
could be to the government, an organisation
or the whole population.
</bodyText>
<listItem confidence="0.9993925">
(9) &amp;quot;The ... group expressed readiness
to provide Albania with food aid&amp;quot;
</listItem>
<bodyText confidence="0.999282142857143">
We therefore adopt a hierarchical approach,
and assign a pattern (place-for-people)
at a higher level (supertype), as well as a
more specific pattern (subtype), if identifi-
able, at a lower level. This deviates from
common practice in the linguistic literature,
but has the great advantage of &apos;punishing&apos;
disagreement only at a later stage and al-
lowing fall-back options for automatic sys-
tems. We also experienced a drop in hu-
man annotation agreement from supertype
to subtype classifications (see (Markert and
Nissim, 2002)). In this paper, we evaluate
our system on supertype classification.
</bodyText>
<listItem confidence="0.999030285714286">
• place-for-event: a location name stands
for something that happened there (see Ex-
ample (2)).
• place-for-product: a place stands for a
product manufactured there (e.g., &amp;quot;Bor-
deaux&amp;quot; can refer to the wine produced
there) .
</listItem>
<bodyText confidence="0.992956">
The category othermet covers unconventional
metonymies. Since they are open-ended and
context-dependent, no specific category indi-
cating the intended semantic class can be in-
troduced. In Example (10), &amp;quot;New Jersey&amp;quot;
metonymically refers to the local typical tunes.
</bodyText>
<listItem confidence="0.99673525">
(10) &amp;quot;The thing about the record is the in-
fluences of the music. The bottom end
is very New York/New Jersey and the
top is very melodic&amp;quot;
</listItem>
<bodyText confidence="0.954684142857143">
The category othermet is only used if none of
the other categories fits.
In addition to literal and metonymic readings,
we found examples where two predicates are in-
volved, triggering a different reading each, thus
yielding a mixed reading. This often occurs with
coordinations and appositions.
</bodyText>
<listItem confidence="0.99479">
(11) &amp;quot;they arrived in Nigeria, hitherto a
leading critic of... &amp;quot;
</listItem>
<bodyText confidence="0.99871">
In Example (11), both a literal (triggered by
&amp;quot;arriving in&amp;quot;) and a place-for-people reading
(triggered by &amp;quot;leading critic&amp;quot;) are invoked. We
therefore introduced the category mixed to deal
with these cases (not treated as a category in
the literature).
</bodyText>
<subsectionHeader confidence="0.9990715">
2.3 Annotation Reliability, Distribution
and Data Preparation
</subsectionHeader>
<bodyText confidence="0.999957875">
The 1000 examples of our corpus have been inde-
pendently annotated by two computational lin-
guists, who are the authors of this paper. Repro-
ducibility of results (Krippendorff, 1980) yielded
a percentage agreement of .95 and a kappa (Car-
letta, 1996) of .88. The annotation can therefore
be considered reliable. In the corpus data used
for our classification experiments, we only in-
cluded the samples which both annotators could
agree on and which were not marked as noise.
Therefore our corpus for testing and training the
algorithm includes 925 samples. The resulting
distribution of readings is described in Table 1.
The data was further stripped of all punctua-
tion and capitalisation was removed. No stem-
ming or lemmatisation was performed.
</bodyText>
<tableCaption confidence="0.920488">
Table 1: Distribution of readings in our corpus
</tableCaption>
<equation confidence="0.746594375">
reading
literal 737 79.7
place-for-people 161 17.4
place-for-event 3 .3
place-for-product 0 .0
mixed 15 1.6
othermet 9 1.0
total 925 100.0
</equation>
<sectionHeader confidence="0.944819" genericHeader="keywords">
3 Decision lists for metonymy
resolution
</sectionHeader>
<bodyText confidence="0.994958259259259">
The distribution in the corpus shows that
metonymic readings that do not follow estab-
lished metonymic patterns (othermet) are very
rare. This seems to be the case for other kinds
of metonymies, too (Verspoor, 1997). This
strengthens our case for viewing metonymy
recognition as a classification task between
literal readings and metonymic patterns that
can be identified in advance for particular
semantic classes. We therefore explore the
usage of a classification algorithm and fea-
tures used in WSD for metonymy recognition.
The target readings for the algorithm to
distinguish are literal, place-for-people,
place-for-event, place-for-product,
othermet and mixed.
As an algorithm we use decision lists.3 The
advantage of decision lists for a first exploration
of a feature space is that their choices are easy to
follow as they make use of the most informative
feature only instead of a combination of features.
All features encountered in the training data are
ranked in the decision list (best evidence first)
according to a log-likelihood ratio calculated as
follows (Yarowsky, 1995; Martinez and Agirre,
2000):
When applying the decision list to a test ex-
</bodyText>
<footnote confidence="0.860581">
3A11 experiments reported here have also been re-
peated using a Naive Bayes classifier. The results have
not improved on decision lists.
</footnote>
<bodyText confidence="0.999836857142857">
ample, the winning reading is selected by the
feature in the test example with the highest rank
in the decision list.
We estimated probabilities via maximum like-
lihood, adopting a simple smoothing method:
0.1 is added to both the denominator and nu-
merator.
</bodyText>
<sectionHeader confidence="0.913008" genericHeader="method">
4 Exploration of feature space
</sectionHeader>
<bodyText confidence="0.988357384615384">
We investigated the following feature types. Ex-
amples are given in Table 2, together with ex-
amples of their distribution and the reading they
trigger.
Cooccurrences. They have proved useful for
WSD (Gale et al., 1993; Pedersen, 2000). We
used left and right windows of context of 8 dif-
ferent sizes: 0, 1, 2, 3, 4, 5, 10, and 25 words,
thus yielding 64 possible combinations of left
and right sizes (e.g., 13.r1 for 3 words to the left
and 1 to the right). Any content word in the
window considered was included as a feature.
Collocations. We selected 4 different collo-
cations frequently used in WSD (Ng and Lee,
1996; Martinez and Agirre, 2000). The word to
the right of the PMW, the word to the left, two
words to the left and the word to the right and
the left. The first two features consist of a sin-
gle word form, the latter two of a sequence of
two word forms. Function words were allowed
as collocations, as e.g., the presence of a prepo-
sition directly to the left of the PMW can be
indicative (see also Table 2).
Grammatical features. Following some
WSD approaches (Ng and Lee, 1996; Yarowsky,
1995) we use grammatical features, namely,
</bodyText>
<figure confidence="0.884767607142857">
Log (
)Pr (readingiI If eaturek)
Pr (reading I f eaturek)
Table 2: Examples of features and their decision list score
feature example assigned readingi Log readingi readingsa.o,
grammar
role &lt;country&gt; = subj place-for-people
role-of-verb &lt;country&gt; = subj-of-have literal
cooccurrences
content-word-in-window-14.r4
content-word-in-window-14.r4
collocations
word-to-left
word-to-right
two-words-to-left
word-to-left-and-right
states +/- 4 &lt;country&gt;
win +/- 4 &lt;country&gt;
in &lt;country&gt;
&lt;country&gt; seemed
one of &lt;country&gt;
provide &lt;country&gt; with
literal
place-for-people
literal
place-for-people
literal
place-for-people
</figure>
<table confidence="0.99790275">
4.709 11 0
3.434 3 0
7.314 150 0
3.434 3 0
4.263 7 0
3.044 2 0
0.863 57 24
3.714 4 0
</table>
<bodyText confidence="0.997800857142857">
(i) the grammatical role (role) of the PMW,
distinguishing between subjects, direct objects
and any other grammatical role (including e.g.
prepositional phrases, NP modifiers); (ii) both
the grammatical role and the stemmed form of
the corresponding verb for subjects and direct
objects (role-of-verb).
</bodyText>
<sectionHeader confidence="0.994346" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.975963615384615">
We have tested the decision list algorithm on
our annotated corpus, employing 10-fold cross-
validation. Results as reported in Table 3 are
averaged over all 10 folds. The first column de-
scribes the feature used in the experiment. Then
we report accuracy and coverage.4
number of decisions made
coverage — number of test data
number of correct decisions made
accuracy = number of decisions made
We also used a backing-off strategy to the
most frequent sense literal for the cases
where no decision could be made (increas-
ing coverage to 1) and report these results as
accuracy/coverage-backoff. As it is of particu-
lar interest to us to see how many non-literal
readings (metonymies and mixed readings) can
be correctly identified we compute precision and
recall (based on the algorithm including backing
4Please note that a test example might not be covered
because of either the absence of a feature value in the
training set or because the highest ranked feature gives
equal evidence for two different readings.
off strategy). Let A be the number of correctly
identified non-literal readings and B the number
of incorrectly identified non-literal readings.
</bodyText>
<equation confidence="0.868568">
precision =
A ± B
recall = non-literal examples in the test data
</equation>
<bodyText confidence="0.99858375">
When significance claims are made they are
based on a 10-fold cross-validated t-test, using
significance level 0.05.
The baseline used for comparison is the as-
signment of the most frequent reading literal
(see Table 1). It has a coverage of 1 as it is
applicable to all examples. Recall is 0 as no
metonymies can be recognised.
</bodyText>
<subsectionHeader confidence="0.843623">
5.1 Cooccurrences
</subsectionHeader>
<bodyText confidence="0.999921">
For all 64 window size combinations (for exam-
ple results see Table 3), the accuracy never sig-
nificantly beats the baseline. Both precision and
recall are unsatisfactory and get steadily worse
with increasing window sizes. We identified the
following reasons for such a behaviour.
Topical v. fine-grained sense distinctions.
Cooccurrences and large window sizes tradition-
ally work well for topical distinctions (Gale et
al., 1993). Metonymy, though, does often not
cross topical boundaries thus, whether a loca-
tion name is used as a literal (political) reading
or as a reading for the government often does not
change coocurrence features. This is especially
true for large window sizes.
</bodyText>
<figure confidence="0.921552">
A
A
</figure>
<tableCaption confidence="0.985445">
Table 3: Results for feature types
</tableCaption>
<table confidence="0.999756444444444">
feature accuracy/coverage accuracy/coverage-backoff precision/recall
acc COV acc-backoff cov-backoff prec rec
baseline .797 1.00 .797 1.00 n/a 0.00
cooccurrences
content-word-in-window-12.r1 .770 .443 .780 1.00 .510 .204
content-word-in-window-13.r1 .783 .588 .806 1.00 .554 .249
content-word-in-window-14.r1 .790 .686 .803 1.00 .538 .226
content-word-in-window-14.r4 .794 .959 .790 1.00 .458 .180
content-word-in-window-110.r10 .779 1.00 .779 1.00 .250 .043
collocations
word-to-left .843 .780 .809 1.00 .677 .112
word-to-right .831 .740 .810 1.00 .650 .139
two-words-to-left .858 .297 .801 1.00 .625 .053
word-to-right-and-left .870 .426 .795 1.00 .471 .087
all-collocations .819 .944 .810 1.00 .607 .185
grammar
role .837 .995 .837 1.00 .703 .344
role-of-verb+role .843 .999 .843 1.00 .750 .339
</table>
<bodyText confidence="0.99101544">
Pruning and decision list ordering. Every
content word encountered in the training set is
included in the decision list, even if it occurred
infrequently. The simple smoothing method we
used did not fully take this problem into ac-
count. Therefore, for example, a content word
wi occurring only once and with a metonymic
reading can be ranked higher than a content
word w2 occurring 10 times, 8 times with a lit-
eral reading and twice with a metonymic read-
ing. A test example containing both content
words will therefore use wi to decide in favour
of a metonymic reading, despite the weak evi-
dence. This might explain the low precision. We
therefore tested the effect of deleting all non-
informative features from the decision list, us-
ing the G2 test (Dunning, 1993) to measure in-
dependence between cooccurrence features and
readings. Using pruned decision lists yielded
some improvement in precision, but a signifi-
cant drop in coverage, given the lower number of
features used (for window 14.r1: precision=.609;
recall=.210; coverage=.098). The general ten-
dency to prefer smaller windows over larger ones
still holds.
</bodyText>
<subsectionHeader confidence="0.99618">
5.2 Collocations
</subsectionHeader>
<bodyText confidence="0.999947571428571">
The one-word collocations had in general a high
coverage as function words were included. Accu-
racy for collocations is quite good (ranking from
81.9% to 87.0%). But increasing coverage to
1.00 (coverage-backoff) causes accuracy-backoff
to drop. Recall is very low. We discuss here two
reasons why collocations do worse in metonymy
recognition than in WSD (Yarowsky, 1995; Ng
and Lee, 1996; Pedersen, 2001).
Target readings. Readings like othermet
and mixed are unsuited for a collocation-based
approach.
Sparse data. When we inspected the deci-
sion lists, we found that strong collocations are
mostly found for literal readings (e.g. spatial
prepositions to the left of the PMW), so that
a high percentage of literal examples can be
identified correctly. Some good collocations for
metonymic readings were found only once or
twice in the training data and then not again
in the test data, thus causing low recall and
accuracy-backoff. One reason for this is that
the training data for literal readings is about 5
times as big as for metonymic readings. This
is aggravated by the use of the BNC that in-
cludes a wide variety of genres using different
style, register and vocabulary.5 Often, though,
a &amp;quot;similar&amp;quot; collocation was seen (compare e.g.,
&amp;quot;view&amp;quot; and &amp;quot;position&amp;quot; in Example (4) and (5)).
Using word forms as collocations can only make
the generalisation from Example (3) to Example
(4), not the one to Example (5). Thus, we will
in the future explore semantic generalisation of
collocations by e.g., using synonym information
from Wordnet.
</bodyText>
<subsectionHeader confidence="0.993728">
5.3 Grammatical roles
</subsectionHeader>
<bodyText confidence="0.999978896551724">
Grammatical roles yield significant improve-
ments in accuracy-backoff over the baseline and
good precision. One reason is that they do not
suffer as much from sparse data and generalise
well over the whole semantic class.
Regarding the classifier based on the feature
role only, e.g., being a subject can be learned
as a good indicator for place-for-people
metonymies regardless of country name or verb.6
Recall (.344) is also promising considering that
the roles of subject and object, which give good
hints for metonymic readings, are relatively rare
(only 120 of 925 examples in our corpus were
subjects or direct objects). The classifier learns
to assign literal readings to all other instances,
whose grammatical roles are not further distin-
guished as feature values. Inclusion of more
grammatical roles might further improve recall.
Precision can be improved without sacrificing
recall by also considering the verb, if present
in the training data (classifier role-of-verb±role).
So, whereas considering the role only will lead to
assigning a place-for-people metonymy to all
subjects, this is avoided in some cases when con-
sidering the verb in addition (e.g., for being the
subject of the full verb &amp;quot;have&amp;quot;; see also Table 2).
If the grammatical role with this particular verb
has not been seen in the training data, the clas-
sifier will default to role, thus keeping coverage
</bodyText>
<footnote confidence="0.972677666666667">
5(Martinez and Agirre, 2000) also achieved better re-
sults with the use of collocations on the Wall Street Jour-
nal than on the balanced Brown Corpus.
60bviously the usefulness of grammatical roles will
also depend on the kind of metonymy prevalent in the
semantic class.
</footnote>
<bodyText confidence="0.993447444444444">
high.
Please note that the grammatical roles have
been annotated by hand as we wanted to mea-
sure the contribution of different features to
metonymy classification without encountering
error chains from e.g., parsing or tagging pro-
cesses. Therefore the results we present are an
upper bound to what can be achieved with sub-
ject/object roles.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.998108791666667">
We compared our approach and results to WSD
in Section 1 and 5, stressing word-to-word vs.
class-to-class inference.
Most traditional approaches to metonymy
recognition use violations of selectional restric-
tions (plus sometimes syntactic violations) for
recognition (Pustejovsky, 1995; Hobbs et al.,
1993; Fass, 1997; Copestake and Briscoe, 1995;
Stallard, 1993).7 Thus they furnish their algo-
rithms with (mostly hand-modelled) selectional
or grammatical restrictions. Note that selec-
tional restrictions in these approaches are nor-
mally not seen as preferences but as absolute
constraints. If and only if such an absolute con-
straint is violated a non-literal reading is pro-
posed. In those experiments in which we also
use grammatical knowledge, our system does
not have any a priori knowledge of semantic
argument-verb restrictions. Rather it refers to
previously seen training data of country names
as verb arguments and their labelled senses and
computes the likelihood of each sense using this
distribution. This is advantageous for the fol-
lowing reasons:
</bodyText>
<listItem confidence="0.682467545454545">
• There are many verbs with weak selectional
restrictions (e.g., the verb &amp;quot;seem&amp;quot;). Both
literal (see Example (12)) and metonymic
(see Example (13)) readings of a location
ocurring as subject of &amp;quot;seem&amp;quot; are therefore
possible, although one of the readings might
be more frequent given these features.
(12) &amp;quot;Hungary seemed far away.&amp;quot;
7(Markert and Hahn, 2002) enhance this with
anaphoric information.
(13) &amp;quot;Britain seemed close to interven-
</listItem>
<bodyText confidence="0.9574661">
tion.&amp;quot;
Selectional restrictions as used in most
metonymy recognition approaches there-
fore do not detect any violation. In
contrast, the training data we use sup-
plies the information that the metonymic
place-for-people reading is more fre-
quent given these grammatical features,
leading the classifier to assign the correct
reading in the majority of cases.8
</bodyText>
<listItem confidence="0.7583385">
• Our algorithm does not need to make any
assumptions about the sense of the verb.
</listItem>
<bodyText confidence="0.992504631578948">
Selectional restrictions, instead, must as-
sume that the verb is disambiguated be-
forehand as they can vary between different
verb senses (compare, e.g., the &amp;quot;confront&amp;quot;
reading and the &amp;quot;to be opposite&amp;quot; reading of
the verb &amp;quot;face&amp;quot;).
To compare our decision list algorithm role-
for-verb+role to a selectional restriction viola-
tions approach we limited our next empirical
study to the 120 examples in our data that had
the grammatical role of subjects or direct ob-
jects (SETGRAmm).
Two native speakers of English (both lin-
guists) were asked to annotate the 120 subj-
verb/obj-verb tuples in SETGRAMM for selec-
tional restriction violations. Agreement between
the two subjects was satisfactory (kappa=.70).
We then simulated two metonymy recognition
algorithms based on the annotations of subjectl
and subject2, postulating a non-literal reading
when a selectional restriction violation was an-
notated and literal otherwise and computed cor-
responding evaluation measures.
We also computed the evaluation measures for
our role-of-verb+role classifier, limited to SET-
GRAMM.
Results are summarised in Table 4. Our clas-
sifier has higher recall, but lower precision than
subject2 and subjectl . To compare the trade-off
8(Briscoe and Copestake, 1999) propose using fre-
quency information in addition to syntactic and semantic
restrictions, but use only a priori sense frequencies with-
out feature integration.
between precision and recall we computed the F-
measure for all algorithms, where our algorithm
performed best.
We also evaluate our approach more rigor-
ously than other metonymy resolution algo-
rithms Some researchers use constructed ex-
amples only (Fass, 1997; Hobbs et al., 1993;
Copestake and Briscoe, 1995; Pustejovsky, 1995;
Verspoor, 1996), and do not report any numer-
ical results. Others (Markert and Hahn, 2002;
Harabagiu, 1998; Stallard, 1993) use naturally-
occurring data that, however, seem to be anal-
ysed according to subjective intuitions of one
individual only, not assessing the reliability of
their annotation. We, instead, use a reliably
annotated corpus that we can make available
to other researchers. In addition, most pre-
vious evaluations report only recall figures for
metonymy recognition, neglecting the question
of precision and false positives as well as base-
line comparisons and accuracy. Evaluations of
metonymy interpretation (Lapata, 2001) include
more disciplined evaluations, but do not handle
metonymy recognition yet.
</bodyText>
<sectionHeader confidence="0.998837" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999523714285714">
We argued for viewing metonymy recognition
as a WSD task based on semantic classes in-
stead of individual words. This is motivated
by the regularity of most metonymic readings.
We presented a corpus reliably annotated for
metonymic and literal usage which supports this
claim. We also conducted several experiments
with a decision list algorithm to explore the use-
fulness of common WSD features for metonymy
recognition. We showed that coocurrence fea-
tures are not useful for metonymy resolution,
whereas collocation features need to be gener-
alised from word forms to semantic classes to
have wide application. Grammatical features
perform well. We also compared our grammati-
cal features to a selectional restriction based ap-
proach to recognition with promising results.
In the future, we will explore two avenues for
improvement: Firstly, we will experiment with
more sophisticated machine learning algorithms,
starting with improving on our smoothing pro-
</bodyText>
<tableCaption confidence="0.997904">
Table 4: Comparison of human subjects and decision list for grammatical roles
</tableCaption>
<table confidence="0.990820285714286">
classifier accuracy coverage precision recall F-measure
su bject 1 .625
su bject2 .708
role-of-verb±role .706
1.00 .857 .525 .651
1.00 .846 .687 .758
.992 .750 .830 .788
</table>
<bodyText confidence="0.998409461538462">
cedure, which we experienced as too simplistic
(see also (Yarowsky, 1997)). Secondly, we will
generalise the collocation features we use, incor-
porate more grammatical relations and explore
other feature types and feature combination.
Acknowledgements. Katja Markert is
funded by an Emmy Noether Fellowship of the
Deutsche Forschungsgemeinschaft (DFG) and
Malvina Nissim by ESRC Project R000239444.
We thank our colleagues Stephen Clark and
Tim O&apos;Donnell for their help with annotation
as well as two anonymous reviewers for their
comments and suggestions.
</bodyText>
<sectionHeader confidence="0.995885" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998806133928571">
Ted Briscoe and Ann Copestake. 1999. Lexical rules
in constraint-based grammar. Computational Lin-
gusitics, 25(4):487-526.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational
Linguistics, 22(2):249-254.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proc. of
the 1999 Joint SIGDAT Conference, College Park,
MD, pages 100-110.
Ann Copestake and Ted Briscoe. 1995. Semi-
productive polysemy and sense extension. Journal
of Semantics, 12:15-67.
Steffan Corley, Martin Corley, Frank Keller,
Matthew Crocker, and Shari Trewin. 2001. Find-
ing syntactic structure in unparsed corpora: The
Gsearch corpus query system. Computers and the
Humanities, 35 (2) :81-94.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19:61-74.
Dan Fass. 1997. Processing Metaphor and
Metonymy. Ablex, Stanford, CA.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge, Mass.
William Gale, Kenneth Church, and David
Yarowsky. 1993. A method for disambiguating
word senses in a large corpus. Computers and the
Humanities, 26:415-439.
Sanda Harabagiu. 1998. Deriving metonymic co-
ercions from WordNet. In Workshop of the Us-
age of WordNet in Natural Language Processing
Systems, COLING-ACL &apos;98, pages 142-148, Mon-
treal, Canada.
Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt,
and Paul Martin. 1993. Interpretation as abduc-
tion. Artificial Intelligence, 63:69-142.
Shin-ichiro Kamei and Takahiro Wakao. 1992.
Metonymy: Reassessment, survey of acceptability
and its treatment in machine translation systems.
In Proc. of the 30th Annual Meeting of the As-
sociation for Computational Linguistics; Newark,
Del., 28 June - 2 July 1992, pages 309-311.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live By. Chicago University Press, Chicago,
Ill.
Maria Lapata. 2001. A corpus-based account of reg-
ular polysemy: The case of context-sensitive ad-
jectives. In Proc. of the 2nd Meeting of the North
American Chapter of the ACL, Pittsburgh, PA.
Katja Markert and Udo Hahn. 2002. Understanding
metonymies in discourse. Artificial Intelligence,
135(1/2):145-198, February.
Katja Markert and Malvina Nissim. 2002. Towards
a corpus annotated for metonymies: the case of
location names. In Proc. of the 3&apos;d International
Conference on Language Resources and Evalua-
tion; Las Palmas, Canary Islands, 2002.
David Martinez and Eneko Agirre. 2000. One sense
per collocation and genre/topic variations. In
Proc. of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
very large corpora.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proc. of
the 34th Annual Meeting of the Association for
Computational Linguistics; Santa Cruz, Cal., 23-
28 June 1996, pages 40-47, Santa Cruz, Ca.
Geoffrey Nunberg. 1978. The Pragmatics of Refer-
ence. Ph.D. thesis, City University of New York,
New York.
Ted Pedersen. 2000. A simple approach to building
ensembles of Naive Bayesian classifiers for word
sense disambiguation. In Proc. of the 18t Confer-
ence of the North American Chapter of the ACL;
2000, pages 63-69.
Ted Pedersen. 2001. A decision tree of bigrams is an
accurate predictor of word sense. In Proc. of the
2&amp;quot; Conference of the North American Chapter of
the ACL; 2001, pages 79-86.
James Pustejovsky. 1995. The Generative Lexicon.
MIT Press, Cambridge, Mass.
David Stallard. 1993. Two kinds of metonymy. In
Proc. of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics; Columbus,
Ohio, 22-26 June 1993, pages 87-94, Columbus,
Ohio.
Gustav Stern. 1931. Meaning and Change of Mean-
ing. Goteborg: Wettergren &amp; Kerbers Forlag.
Cornelia Verspoor. 1996. Lexical limits on the in-
fluence of context. In Proc. of the 18th Annual
Conference of the Cognitive Science Society; La
Jolla, Cal., 12-15 July 1996, pages 116-120.
Cornelia Verspoor. 1997. Conventionality-governed
logical metonymy. In H. Bunt, L. Kievit,
R. Muskens, and N. Verlinden, editors, Proc.
of the 2nd International Workshop on Compu-
tational Semantics, pages 300-312, Tilburg, The
Netherlands.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proc. of the 33&apos; Annual Meeting of the Associ-
ation for Computational Linguistics; Cambridge,
Mass., 26-30 June 1995, pages 189-196.
David Yarowsky. 1997. Homograph disambiguation
in speech synthesis. In R. Sproat, J. Olive, and
J. Hirschberg, editors, Progress in Speech Synthe-
sis, pages 159-175. Springer-Verlag.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.83535675">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 204-213. Association for Computational Linguistics. particular word input and assigns word</note>
<abstract confidence="0.982078388978931">to new test instances of the word as output, (supervised) metonymy recognition can take a set of labelled training instances of different words belonging to one semantic class as input and assign literal readings and possible metonymic patterns to new test instances different words of the same semanclass. it needs to infer from training instances like Example (3) (when labelled a that Examples (4) and (5) are also metonymic, a task which poses no problems for most humans. (3) &amp;quot;Bosnia&apos;s view of&amp;quot; (4) &amp;quot;Hungary&apos;s view of&amp;quot; (5) &amp;quot;Hungary&apos;s position on&amp;quot; In this paper, we explore this view of metonymy recognition as a class-based WSD task for the semantic class of locations.&apos; The corpus data we use is described in Section 2. As resources reliably annotated for metonymy do not exist (see also Section 6), we constructed a corpus of location names annotated for literal and metonymic readings. The supervised classification algorithm we use are decision lists as they have been successfully used in several classification tasks (Yarowsky, 1995; Collins and Singer, 1999) (see Section 3). In Section 4, we explore whether features traditionally used in WSD carry over to metonymy concentrating on (i) (iii) features. Results are discussed in Section 5. We show that cooccurrences are in general not appropriate for metonymy resolution; collocations are useful but suffer from data sparseness when used as simple word forms; the grammatical relations subject and object perform well but are only applicable to a small part of the data. We then compare our algorithm to metonymy recognition approaches based on selectional restriction violations in Section 6. 2 Experimental Data We present a short overview of the collection of a corpus of location names and its annotation for literal and metonymic readings. A more detailed description can be found in (Markert and Nissim, 2002) and the annotation scheme downloadable from : //www. ltg . ed. ac. uktemalvi/mas cara/publ i cat ions . html. Corpus We extracted all country names from Word- Net (Fellbaum, 1998) and the CIA factbook (http: //www. cia. gov/cia/publications/ actbook/). collection of names forms our sampling frame CountryList. We built a corpus of text samples that conof country names, randomly extracted from the British National Cor- : //info . ox . ac .uk/bnc), abbreviated as BNC. Any country name in a Metonymic Word (PMW, henceforth) and was allowed to occur in the samples extracted. We searched the BNC et al., 2001). All samples include a PMW surrounded by three sentences of context. All examples introduced from now are from the 2.2 Annotation Scheme for Location Names After excluding some undesired examples (i.e., our extraction method collected homonyms, such as &amp;quot;Pro- Greenland&amp;quot;), annotation can proto identify metonymic, readings. Our annotation scheme built on lists of metonymic patterns in the literature (Lakoff and Johnson, 1980; Fass, 1997; Stern, 1931), but diverted from these patterns when they did provide or could not be distinguished reliably (Markert and Nissim, 2002). for location names comprises a locative (see Example (6)) and a political entity interpretation (see Example (7)). &amp;quot;coral coast of Papua New Guinea&amp;quot; &apos;At the moment we restrict ourselves to location exception is Example (12). current account deficit&amp;quot; For metonymic readings, we distinguish be- (valid for all physical oband As general patterns were never encountered in our corpus, we describe here only the latter. • place stands for any persons/organisations associated with it. Example (8), Marino&amp;quot; for one of its sports teams. &amp;quot;a own goal from Marino defender&amp;quot; Often, the explicit referent is underspecified, as in Example (9), where the reference could be to the government, an organisation or the whole population. (9) &amp;quot;The ... group expressed to provide Albania with food aid&amp;quot; therefore adopt a assign a pattern a higher level well as a specific pattern identifiable, at a lower level. This deviates from common practice in the linguistic literature, but has the great advantage of &apos;punishing&apos; disagreement only at a later stage and allowing fall-back options for automatic systems. We also experienced a drop in human annotation agreement from supertype to subtype classifications (see (Markert and Nissim, 2002)). In this paper, we evaluate our system on supertype classification. • location name stands for something that happened there (see Example (2)). • place stands for a manufactured there (e.g., &amp;quot;Borrefer to the wine produced there) . category unconventional metonymies. Since they are open-ended and context-dependent, no specific category indicating the intended semantic class can be in- In Example (10), Jersey&amp;quot; metonymically refers to the local typical tunes. &amp;quot;The thing about the record is the fluences of the music. The bottom end is very New York/New Jersey and the top is very melodic&amp;quot; category only used if none of the other categories fits. In addition to literal and metonymic readings, we found examples where two predicates are involved, triggering a different reading each, thus a This often occurs with (11) &amp;quot;they arrived in Nigeria, hitherto leading critic of... &amp;quot; In Example (11), both a literal (triggered by in&amp;quot;) and a (triggered by &amp;quot;leading critic&amp;quot;) are invoked. We introduced the category deal with these cases (not treated as a category in the literature). 2.3 Annotation Reliability, Distribution and Data Preparation The 1000 examples of our corpus have been independently annotated by two computational linguists, who are the authors of this paper. Reproducibility of results (Krippendorff, 1980) yielded percentage agreement of .95 and a (Carletta, 1996) of .88. The annotation can therefore be considered reliable. In the corpus data used for our classification experiments, we only included the samples which both annotators could agree on and which were not marked as noise. Therefore our corpus for testing and training the algorithm includes 925 samples. The resulting distribution of readings is described in Table 1. The data was further stripped of all punctuation and capitalisation was removed. No stemming or lemmatisation was performed. Table1: Distribution of readings in our corpus reading 79.7 place-for-people 161 17.4 place-for-event 3 .3 place-for-product 0 .0 mixed 15 1.6 othermet 9 1.0 925 3 Decision lists for metonymy resolution The distribution in the corpus shows that metonymic readings that do not follow estabmetonymic patterns very rare. This seems to be the case for other kinds of metonymies, too (Verspoor, 1997). This strengthens our case for viewing metonymy recognition as a classification task between literal readings and metonymic patterns that can be identified in advance for particular semantic classes. We therefore explore the usage of a classification algorithm and features used in WSD for metonymy recognition. The target readings for the algorithm to are place-for-people, place-for-event, othermet and mixed. an algorithm we use decision The advantage of decision lists for a first exploration of a feature space is that their choices are easy to follow as they make use of the most informative feature only instead of a combination of features. All features encountered in the training data are ranked in the decision list (best evidence first) according to a log-likelihood ratio calculated as follows (Yarowsky, 1995; Martinez and Agirre, 2000): applying the decision list to a test exexperiments reported here have also been repeated using a Naive Bayes classifier. The results have not improved on decision lists. ample, the winning reading is selected by the feature in the test example with the highest rank in the decision list. We estimated probabilities via maximum likelihood, adopting a simple smoothing method: 0.1 is added to both the denominator and numerator. 4 Exploration of feature space We investigated the following feature types. Examples are given in Table 2, together with examples of their distribution and the reading they trigger. have proved useful for WSD (Gale et al., 1993; Pedersen, 2000). We used left and right windows of context of 8 different sizes: 0, 1, 2, 3, 4, 5, 10, and 25 words, thus yielding 64 possible combinations of left and right sizes (e.g., 13.r1 for 3 words to the left and 1 to the right). Any content word in the window considered was included as a feature. selected 4 different collocations frequently used in WSD (Ng and Lee, 1996; Martinez and Agirre, 2000). The word to the right of the PMW, the word to the left, two words to the left and the word to the right and the left. The first two features consist of a single word form, the latter two of a sequence of two word forms. Function words were allowed as collocations, as e.g., the presence of a preposition directly to the left of the PMW can be indicative (see also Table 2). features. some WSD approaches (Ng and Lee, 1996; Yarowsky, 1995) we use grammatical features, namely, Log ( Ifeaturek) (reading If eaturek) Table 2: Examples of features and their decision list score assigned readingi Log readingi grammar role &lt;country&gt; = subj &lt;country&gt; = subj-of-have place-for-people literal role-of-verb cooccurrences content-word-in-window-14.r4 content-word-in-window-14.r4 collocations word-to-left word-to-right two-words-to-left word-to-left-and-right states +/- 4 &lt;country&gt; win +/- 4 &lt;country&gt; in &lt;country&gt; &lt;country&gt; seemed one of &lt;country&gt; provide &lt;country&gt; with literal place-for-people literal place-for-people literal place-for-people 4.709 11 0 3.434 3 0 7.314 150 0 3.434 3 0 4.263 7 0 3.044 2 0 0.863 57 24 3.714 4 0 (i) the grammatical role (role) of the PMW, distinguishing between subjects, direct objects and any other grammatical role (including e.g. prepositional phrases, NP modifiers); (ii) both the grammatical role and the stemmed form of the corresponding verb for subjects and direct objects (role-of-verb). 5 Results and Discussion We have tested the decision list algorithm on our annotated corpus, employing 10-fold crossvalidation. Results as reported in Table 3 are averaged over all 10 folds. The first column describes the feature used in the experiment. Then report accuracy and number of decisions made coverage — number of test data number of correct decisions made accuracy = number of decisions made We also used a backing-off strategy to the most frequent sense literal for the cases where no decision could be made (increasing coverage to 1) and report these results as it is of particular interest to us to see how many non-literal readings (metonymies and mixed readings) can be correctly identified we compute precision and recall (based on the algorithm including backing note that a test example might not be covered because of either the absence of a feature value in the training set or because the highest ranked feature gives equal evidence for two different readings. off strategy). Let A be the number of correctly non-literal readings and number of incorrectly identified non-literal readings. precision = ± recall = non-literal examples in the test data When significance claims are made they are based on a 10-fold cross-validated t-test, using significance level 0.05. The baseline used for comparison is the assignment of the most frequent reading literal (see Table 1). It has a coverage of 1 as it is applicable to all examples. Recall is 0 as no metonymies can be recognised. 5.1 Cooccurrences For all 64 window size combinations (for example results see Table 3), the accuracy never significantly beats the baseline. Both precision and recall are unsatisfactory and get steadily worse with increasing window sizes. We identified the following reasons for such a behaviour. Topical v. fine-grained sense distinctions. Cooccurrences and large window sizes traditionally work well for topical distinctions (Gale et al., 1993). Metonymy, though, does often not topical boundaries thus, whether a tion name is used as a literal (political) reading or as a reading for the government often does not change coocurrence features. This is especially true for large window sizes. A A Table 3: Results for feature types feature accuracy/coverage accuracy/coverage-backoff precision/recall acc COV acc-backoff cov-backoff prec rec baseline .797 1.00 .797 1.00 n/a 0.00 cooccurrences content-word-in-window-12.r1 .770 .443 .780 1.00 .510 .204 content-word-in-window-13.r1 .783 .588 .806 1.00 .554 .249 content-word-in-window-14.r1 .790 .686 .803 1.00 .538 .226 content-word-in-window-14.r4 .794 .959 .790 1.00 .458 .180 content-word-in-window-110.r10 .779 1.00 .779 1.00 .250 .043 collocations word-to-left .843 .780 .809 1.00 .677 .112 word-to-right .831 .740 .810 1.00 .650 .139 two-words-to-left .858 .297 .801 1.00 .625 .053 word-to-right-and-left .870 .426 .795 1.00 .471 .087 all-collocations .819 .944 .810 1.00 .607 .185 grammar role .837 .995 .837 1.00 .703 .344 role-of-verb+role .843 .999 .843 1.00 .750 .339 and decision list ordering. content word encountered in the training set is included in the decision list, even if it occurred infrequently. The simple smoothing method we used did not fully take this problem into account. Therefore, for example, a content word wi occurring only once and with a metonymic reading can be ranked higher than a content word w2 occurring 10 times, 8 times with a literal reading and twice with a metonymic reading. A test example containing both content will therefore use to decide in favour of a metonymic reading, despite the weak evidence. This might explain the low precision. We tested the effect of deleting all nonfrom the decision list, usthe test (Dunning, 1993) to measure incooccurrence features and readings. Using pruned decision lists yielded some improvement in precision, but a significant drop in coverage, given the lower number of used (for window recall=.210; coverage=.098). The general tendency to prefer smaller windows over larger ones still holds. 5.2 Collocations The one-word collocations had in general a high coverage as function words were included. Accuracy for collocations is quite good (ranking from 81.9% to 87.0%). But increasing coverage to to drop. Recall is very low. We discuss here two reasons why collocations do worse in metonymy recognition than in WSD (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001). readings. like unsuited for a collocation-based approach. data. we inspected the decision lists, we found that strong collocations are mostly found for literal readings (e.g. spatial prepositions to the left of the PMW), so that a high percentage of literal examples can be identified correctly. Some good collocations for metonymic readings were found only once or twice in the training data and then not again in the test data, thus causing low recall and accuracy-backoff. One reason for this is that the training data for literal readings is about 5 times as big as for metonymic readings. This is aggravated by the use of the BNC that includes a wide variety of genres using different register and Often, though, a &amp;quot;similar&amp;quot; collocation was seen (compare e.g., Example (4) and (5)). Using word forms as collocations can only make the generalisation from Example (3) to Example (4), not the one to Example (5). Thus, we will in the future explore semantic generalisation of collocations by e.g., using synonym information from Wordnet. 5.3 Grammatical roles Grammatical roles yield significant improvein the baseline and good precision. One reason is that they do not suffer as much from sparse data and generalise well over the whole semantic class. Regarding the classifier based on the feature role only, e.g., being a subject can be learned a good indicator for regardless of country name or Recall (.344) is also promising considering that the roles of subject and object, which give good hints for metonymic readings, are relatively rare (only 120 of 925 examples in our corpus were subjects or direct objects). The classifier learns to assign literal readings to all other instances, whose grammatical roles are not further distinguished as feature values. Inclusion of more grammatical roles might further improve recall. Precision can be improved without sacrificing recall by also considering the verb, if present the training data (classifier So, whereas considering the role only will lead to a to all subjects, this is avoided in some cases when considering the verb in addition (e.g., for being the of the &amp;quot;have&amp;quot;; see also Table 2). If the grammatical role with this particular verb has not been seen in the training data, the claswill default to keeping coverage and Agirre, 2000) also achieved better results with the use of collocations on the Wall Street Journal than on the balanced Brown Corpus. the usefulness of grammatical roles will also depend on the kind of metonymy prevalent in the semantic class. high. Please note that the grammatical roles have been annotated by hand as we wanted to measure the contribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes. Therefore the results we present are an upper bound to what can be achieved with subject/object roles. 6 Related Work We compared our approach and results to WSD in Section 1 and 5, stressing word-to-word vs. class-to-class inference. approaches to metonymy selectional restrictions (plus sometimes syntactic violations) for recognition (Pustejovsky, 1995; Hobbs et al., 1993; Fass, 1997; Copestake and Briscoe, 1995; Thus they furnish their algorithms with (mostly hand-modelled) selectional or grammatical restrictions. Note that selectional restrictions in these approaches are normally not seen as preferences but as absolute constraints. If and only if such an absolute constraint is violated a non-literal reading is proposed. In those experiments in which we also use grammatical knowledge, our system does have priori knowledge of semantic argument-verb restrictions. Rather it refers to previously seen training data of country names as verb arguments and their labelled senses and computes the likelihood of each sense using this distribution. This is advantageous for the following reasons: • There are many verbs with weak selectional restrictions (e.g., the verb &amp;quot;seem&amp;quot;). Both literal (see Example (12)) and metonymic (see Example (13)) readings of a location ocurring as subject of &amp;quot;seem&amp;quot; are therefore possible, although one of the readings might be more frequent given these features. seemed far away.&amp;quot; and Hahn, 2002) enhance this with anaphoric information. &amp;quot;Britain close to intervention.&amp;quot; Selectional restrictions as used in most recognition approaches therefore do not detect any violation. contrast, the training data we use supplies the information that the metonymic is more frequent given these grammatical features, leading the classifier to assign the correct in the majority of • Our algorithm does not need to make any assumptions about the sense of the verb. Selectional restrictions, instead, must assume that the verb is disambiguated beforehand as they can vary between different verb senses (compare, e.g., the &amp;quot;confront&amp;quot; reading and the &amp;quot;to be opposite&amp;quot; reading of the verb &amp;quot;face&amp;quot;). To compare our decision list algorithm rolefor-verb+role to a selectional restriction violations approach we limited our next empirical study to the 120 examples in our data that had the grammatical role of subjects or direct ob- Two native speakers of English (both linguists) were asked to annotate the 120 subjtuples in selectional restriction violations. Agreement between two subjects was satisfactory We then simulated two metonymy recognition algorithms based on the annotations of subjectl and subject2, postulating a non-literal reading when a selectional restriction violation was annotated and literal otherwise and computed corresponding evaluation measures. We also computed the evaluation measures for role-of-verb+role classifier, limited to SET- GRAMM. Results are summarised in Table 4. Our classifier has higher recall, but lower precision than subject2 and subjectl . To compare the trade-off and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy. Evaluations of 2001) include more disciplined evaluations, but do not handle metonymy recognition yet. 7 Conclusions We argued for viewing metonymy recognition as a WSD task based on semantic classes instead of individual words. This is motivated by the regularity of most metonymic readings. We presented a corpus reliably annotated for metonymic and literal usage which supports this claim. We also conducted several experiments with a decision list algorithm to explore the usefulness of common WSD features for metonymy recognition. We showed that coocurrence features are not useful for metonymy resolution, whereas collocation features need to be generalised from word forms to semantic classes to have wide application. Grammatical features perform well. We also compared our grammatical features to a selectional restriction based approach to recognition with promising results. In the future, we will explore two avenues for improvement: Firstly, we will experiment with more sophisticated machine learning algorithms, with improving on our smoothing pro- Table4: Comparisonof human subjects and decision list for roles classifier accuracy coverage precision recall F-measure su bject 1 .625 su bject2 .708 role-of-verb±role .706 1.00 .857 .525 .651 1.00 .846 .687 .758 .992 .750 .830 .788 cedure, which we experienced as too simplistic (see also (Yarowsky, 1997)). Secondly, we will generalise the collocation features we use, incorporate more grammatical relations and explore other feature types and feature combination. Markert is funded by an Emmy Noether Fellowship of the Deutsche Forschungsgemeinschaft (DFG) and Malvina Nissim by ESRC Project R000239444. We thank our colleagues Stephen Clark and Tim O&apos;Donnell for their help with annotation as well as two anonymous reviewers for their comments and suggestions. References Ted Briscoe and Ann Copestake. 1999. Lexical rules constraint-based grammar. Lin- Jean Carletta. 1996. Assessing agreement on classifitasks: The kappa statistic.</abstract>
<note confidence="0.7587939">M. Collins and Y. Singer. 1999. Unsupervised modfor named entity classification. In of the 1999 Joint SIGDAT Conference, College Park, 100-110. Ann Copestake and Ted Briscoe. 1995. Semipolysemy and sense extension. Semantics, Steffan Corley, Martin Corley, Frank Keller, Matthew Crocker, and Shari Trewin. 2001. Finding syntactic structure in unparsed corpora: The corpus query system. and the (2) :81-94. Ted Dunning. 1993. Accurate methods for the statisof surprise and coincidence. Fass. 1997. Metaphor and Stanford, CA. Fellbaum, editor. 1998. An Lexical Database. Press, Cambridge, Mass. William Gale, Kenneth Church, and David</note>
<abstract confidence="0.622600571428572">Yarowsky. 1993. A method for disambiguating senses in a large corpus. and the Sanda Harabagiu. 1998. Deriving metonymic cofrom WordNet. In of the Usage of WordNet in Natural Language Processing COLING-ACL &apos;98, 142-148, Montreal, Canada.</abstract>
<author confidence="0.975351">Jerry R Hobbs</author>
<author confidence="0.975351">Mark E Stickel</author>
<author confidence="0.975351">Douglas E Appelt</author>
<abstract confidence="0.780716166666667">and Paul Martin. 1993. Interpretation as abduc- Intelligence, Shin-ichiro Kamei and Takahiro Wakao. 1992. Metonymy: Reassessment, survey of acceptability and its treatment in machine translation systems. of the Annual Meeting of the As-</abstract>
<affiliation confidence="0.508657">sociation for Computational Linguistics; Newark,</affiliation>
<address confidence="0.691736">28 June - 2 July 1992, 309-311.</address>
<note confidence="0.951455111111111">Krippendorff. 1980. Analysis: An Into Its Methodology. Publications. Lakoff and Mark Johnson. 1980. Live By. University Press, Chicago, Ill. Maria Lapata. 2001. A corpus-based account of regular polysemy: The case of context-sensitive ad- In of the 2nd Meeting of the North Chapter of the ACL, PA. Katja Markert and Udo Hahn. 2002. Understanding in discourse. Intelligence, 135(1/2):145-198, February. Katja Markert and Malvina Nissim. 2002. Towards a corpus annotated for metonymies: the case of names. In of the International Conference on Language Resources and Evaluation; Las Palmas, Canary Islands, 2002. David Martinez and Eneko Agirre. 2000. One sense</note>
<abstract confidence="0.8107765">per collocation and genre/topic variations. In Proc. of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and very large corpora. Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word</abstract>
<note confidence="0.9124372">An exemplar-based approach. In of Annual Meeting of the Association for Computational Linguistics; Santa Cruz, Cal., 23- June 1996, 40-47, Santa Cruz, Ca. Nunberg. 1978. Pragmatics of Refer-</note>
<address confidence="0.7931375">thesis, City University of New York, New York.</address>
<note confidence="0.799377891891892">Ted Pedersen. 2000. A simple approach to building ensembles of Naive Bayesian classifiers for word disambiguation. In of the Conference of the North American Chapter of the ACL; 63-69. Ted Pedersen. 2001. A decision tree of bigrams is an predictor of word sense. In of the 2&amp;quot; Conference of the North American Chapter of ACL; 2001, Pustejovsky. 1995. Generative Lexicon. MIT Press, Cambridge, Mass. David Stallard. 1993. Two kinds of metonymy. In of the Annual Meeting of the Association for Computational Linguistics; Columbus, 22-26 June 1993, Columbus, Ohio. Stern. 1931. and Change of Mean- Wettergren Forlag. Cornelia Verspoor. 1996. Lexical limits on the inof context. In of the Annual Conference of the Cognitive Science Society; La Cal., 12-15 July 1996, Cornelia Verspoor. 1997. Conventionality-governed logical metonymy. In H. Bunt, L. Muskens, and N. Verlinden, editors, of the 2nd International Workshop on Compu- Semantics, 300-312, Tilburg, The Netherlands. David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of the 33&apos; Annual Meeting of the Association for Computational Linguistics; Cambridge, 26-30 June 1995, David Yarowsky. 1997. Homograph disambiguation in speech synthesis. In R. Sproat, J. Olive, and Hirschberg, editors, in Speech Synthe- 159-175. Springer-Verlag.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Ann Copestake</author>
</authors>
<date>1999</date>
<booktitle>Lexical rules in constraint-based grammar. Computational Lingusitics,</booktitle>
<pages>25--4</pages>
<contexts>
<context position="22150" citStr="Briscoe and Copestake, 1999" startWordPosition="3495" endWordPosition="3498">l restriction violations. Agreement between the two subjects was satisfactory (kappa=.70). We then simulated two metonymy recognition algorithms based on the annotations of subjectl and subject2, postulating a non-literal reading when a selectional restriction violation was annotated and literal otherwise and computed corresponding evaluation measures. We also computed the evaluation measures for our role-of-verb+role classifier, limited to SETGRAMM. Results are summarised in Table 4. Our classifier has higher recall, but lower precision than subject2 and subjectl . To compare the trade-off 8(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard,</context>
</contexts>
<marker>Briscoe, Copestake, 1999</marker>
<rawString>Ted Briscoe and Ann Copestake. 1999. Lexical rules in constraint-based grammar. Computational Lingusitics, 25(4):487-526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="6708" citStr="Carletta, 1996" startWordPosition="1061" endWordPosition="1063">arrived in Nigeria, hitherto a leading critic of... &amp;quot; In Example (11), both a literal (triggered by &amp;quot;arriving in&amp;quot;) and a place-for-people reading (triggered by &amp;quot;leading critic&amp;quot;) are invoked. We therefore introduced the category mixed to deal with these cases (not treated as a category in the literature). 2.3 Annotation Reliability, Distribution and Data Preparation The 1000 examples of our corpus have been independently annotated by two computational linguists, who are the authors of this paper. Reproducibility of results (Krippendorff, 1980) yielded a percentage agreement of .95 and a kappa (Carletta, 1996) of .88. The annotation can therefore be considered reliable. In the corpus data used for our classification experiments, we only included the samples which both annotators could agree on and which were not marked as noise. Therefore our corpus for testing and training the algorithm includes 925 samples. The resulting distribution of readings is described in Table 1. The data was further stripped of all punctuation and capitalisation was removed. No stemming or lemmatisation was performed. Table 1: Distribution of readings in our corpus reading literal 737 79.7 place-for-people 161 17.4 place-</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proc. of the 1999 Joint SIGDAT Conference, College Park, MD,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="1355" citStr="Collins and Singer, 1999" startWordPosition="213" endWordPosition="216">es no problems for most humans. (3) &amp;quot;Bosnia&apos;s view of&amp;quot; (4) &amp;quot;Hungary&apos;s view of&amp;quot; (5) &amp;quot;Hungary&apos;s position on&amp;quot; In this paper, we explore this view of metonymy recognition as a class-based WSD task for the semantic class of locations.&apos; The corpus data we use is described in Section 2. As resources reliably annotated for metonymy do not exist (see also Section 6), we constructed a corpus of location names annotated for literal and metonymic readings. The supervised classification algorithm we use are decision lists as they have been successfully used in several classification tasks (Yarowsky, 1995; Collins and Singer, 1999) (see Section 3). In Section 4, we explore whether features traditionally used in WSD carry over to metonymy resolution, concentrating on (i) cooccurrences; (ii) collocations; and (iii) grammatical features. Results are discussed in Section 5. We show that cooccurrences are in general not appropriate for metonymy resolution; collocations are useful but suffer from data sparseness when used as simple word forms; the grammatical relations subject and object perform well but are only applicable to a small part of the data. We then compare our algorithm to metonymy recognition approaches based on </context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proc. of the 1999 Joint SIGDAT Conference, College Park, MD, pages 100-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Ted Briscoe</author>
</authors>
<title>Semiproductive polysemy and sense extension.</title>
<date>1995</date>
<journal>Journal of Semantics,</journal>
<pages>12--15</pages>
<contexts>
<context position="19327" citStr="Copestake and Briscoe, 1995" startWordPosition="3062" endWordPosition="3065">s we wanted to measure the contribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes. Therefore the results we present are an upper bound to what can be achieved with subject/object roles. 6 Related Work We compared our approach and results to WSD in Section 1 and 5, stressing word-to-word vs. class-to-class inference. Most traditional approaches to metonymy recognition use violations of selectional restrictions (plus sometimes syntactic violations) for recognition (Pustejovsky, 1995; Hobbs et al., 1993; Fass, 1997; Copestake and Briscoe, 1995; Stallard, 1993).7 Thus they furnish their algorithms with (mostly hand-modelled) selectional or grammatical restrictions. Note that selectional restrictions in these approaches are normally not seen as preferences but as absolute constraints. If and only if such an absolute constraint is violated a non-literal reading is proposed. In those experiments in which we also use grammatical knowledge, our system does not have any a priori knowledge of semantic argument-verb restrictions. Rather it refers to previously seen training data of country names as verb arguments and their labelled senses a</context>
<context position="22613" citStr="Copestake and Briscoe, 1995" startWordPosition="3566" endWordPosition="3569">re summarised in Table 4. Our classifier has higher recall, but lower precision than subject2 and subjectl . To compare the trade-off 8(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accu</context>
</contexts>
<marker>Copestake, Briscoe, 1995</marker>
<rawString>Ann Copestake and Ted Briscoe. 1995. Semiproductive polysemy and sense extension. Journal of Semantics, 12:15-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffan Corley</author>
<author>Martin Corley</author>
<author>Frank Keller</author>
<author>Matthew Crocker</author>
<author>Shari Trewin</author>
</authors>
<title>Finding syntactic structure in unparsed corpora: The Gsearch corpus query system.</title>
<date>2001</date>
<journal>Computers and the Humanities,</journal>
<volume>35</volume>
<issue>2</issue>
<pages>81--94</pages>
<contexts>
<context position="2952" citStr="Corley et al., 2001" startWordPosition="470" endWordPosition="473">ons . html. 2.1 Corpus Collection We extracted all country names from WordNet (Fellbaum, 1998) and the CIA factbook (http: //www. cia. gov/cia/publications/ f actbook/). This collection of names forms our sampling frame CountryList. We built a corpus of text samples that contains 1000 occurrences of country names, randomly extracted from the British National Corpus (http : //info . ox . ac .uk/bnc), henceforth abbreviated as BNC. Any country name in CountryList was a Possibly Metonymic Word (PMW, henceforth) and was allowed to occur in the samples extracted. We searched the BNC using Gsearch (Corley et al., 2001). All samples include a PMW surrounded by three sentences of context. All examples introduced from now on are from the BNC.2 2.2 Annotation Scheme for Location Names After excluding some undesired examples (i.e., noise) which our extraction method collected (e.g. homonyms, such as &amp;quot;Greenland&amp;quot; in &amp;quot;Professor Greenland&amp;quot;), the annotation can proceed to identify literal, metonymic, and mixed readings. Our annotation scheme built on lists of metonymic patterns in the literature (Lakoff and Johnson, 1980; Fass, 1997; Stern, 1931), but diverted from these patterns when they did not provide full covera</context>
</contexts>
<marker>Corley, Corley, Keller, Crocker, Trewin, 2001</marker>
<rawString>Steffan Corley, Martin Corley, Frank Keller, Matthew Crocker, and Shari Trewin. 2001. Finding syntactic structure in unparsed corpora: The Gsearch corpus query system. Computers and the Humanities, 35 (2) :81-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--61</pages>
<contexts>
<context position="15040" citStr="Dunning, 1993" startWordPosition="2388" endWordPosition="2389">ntly. The simple smoothing method we used did not fully take this problem into account. Therefore, for example, a content word wi occurring only once and with a metonymic reading can be ranked higher than a content word w2 occurring 10 times, 8 times with a literal reading and twice with a metonymic reading. A test example containing both content words will therefore use wi to decide in favour of a metonymic reading, despite the weak evidence. This might explain the low precision. We therefore tested the effect of deleting all noninformative features from the decision list, using the G2 test (Dunning, 1993) to measure independence between cooccurrence features and readings. Using pruned decision lists yielded some improvement in precision, but a significant drop in coverage, given the lower number of features used (for window 14.r1: precision=.609; recall=.210; coverage=.098). The general tendency to prefer smaller windows over larger ones still holds. 5.2 Collocations The one-word collocations had in general a high coverage as function words were included. Accuracy for collocations is quite good (ranking from 81.9% to 87.0%). But increasing coverage to 1.00 (coverage-backoff) causes accuracy-ba</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19:61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<date>1997</date>
<booktitle>Processing Metaphor and Metonymy.</booktitle>
<location>Ablex, Stanford, CA.</location>
<contexts>
<context position="3466" citStr="Fass, 1997" startWordPosition="550" endWordPosition="551">lowed to occur in the samples extracted. We searched the BNC using Gsearch (Corley et al., 2001). All samples include a PMW surrounded by three sentences of context. All examples introduced from now on are from the BNC.2 2.2 Annotation Scheme for Location Names After excluding some undesired examples (i.e., noise) which our extraction method collected (e.g. homonyms, such as &amp;quot;Greenland&amp;quot; in &amp;quot;Professor Greenland&amp;quot;), the annotation can proceed to identify literal, metonymic, and mixed readings. Our annotation scheme built on lists of metonymic patterns in the literature (Lakoff and Johnson, 1980; Fass, 1997; Stern, 1931), but diverted from these patterns when they did not provide full coverage or could not be distinguished reliably (Markert and Nissim, 2002). The literal reading for location names comprises a locative (see Example (6)) and a political entity interpretation (see Example (7)). (6) &amp;quot;coral coast of Papua New Guinea&amp;quot; &apos;At the moment we restrict ourselves to location names. 2An exception is Example (12). (7) &amp;quot;Britain&apos;s current account deficit&amp;quot; For metonymic readings, we distinguish between general patterns (valid for all physical objects) and location-specific ones. As general patterns</context>
<context position="19298" citStr="Fass, 1997" startWordPosition="3060" endWordPosition="3061">ed by hand as we wanted to measure the contribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes. Therefore the results we present are an upper bound to what can be achieved with subject/object roles. 6 Related Work We compared our approach and results to WSD in Section 1 and 5, stressing word-to-word vs. class-to-class inference. Most traditional approaches to metonymy recognition use violations of selectional restrictions (plus sometimes syntactic violations) for recognition (Pustejovsky, 1995; Hobbs et al., 1993; Fass, 1997; Copestake and Briscoe, 1995; Stallard, 1993).7 Thus they furnish their algorithms with (mostly hand-modelled) selectional or grammatical restrictions. Note that selectional restrictions in these approaches are normally not seen as preferences but as absolute constraints. If and only if such an absolute constraint is violated a non-literal reading is proposed. In those experiments in which we also use grammatical knowledge, our system does not have any a priori knowledge of semantic argument-verb restrictions. Rather it refers to previously seen training data of country names as verb argument</context>
<context position="22564" citStr="Fass, 1997" startWordPosition="3560" endWordPosition="3561">, limited to SETGRAMM. Results are summarised in Table 4. Our classifier has higher recall, but lower precision than subject2 and subjectl . To compare the trade-off 8(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false p</context>
</contexts>
<marker>Fass, 1997</marker>
<rawString>Dan Fass. 1997. Processing Metaphor and Metonymy. Ablex, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1993</date>
<pages>26--415</pages>
<contexts>
<context position="9233" citStr="Gale et al., 1993" startWordPosition="1461" endWordPosition="1464">ed here have also been repeated using a Naive Bayes classifier. The results have not improved on decision lists. ample, the winning reading is selected by the feature in the test example with the highest rank in the decision list. We estimated probabilities via maximum likelihood, adopting a simple smoothing method: 0.1 is added to both the denominator and numerator. 4 Exploration of feature space We investigated the following feature types. Examples are given in Table 2, together with examples of their distribution and the reading they trigger. Cooccurrences. They have proved useful for WSD (Gale et al., 1993; Pedersen, 2000). We used left and right windows of context of 8 different sizes: 0, 1, 2, 3, 4, 5, 10, and 25 words, thus yielding 64 possible combinations of left and right sizes (e.g., 13.r1 for 3 words to the left and 1 to the right). Any content word in the window considered was included as a feature. Collocations. We selected 4 different collocations frequently used in WSD (Ng and Lee, 1996; Martinez and Agirre, 2000). The word to the right of the PMW, the word to the left, two words to the left and the word to the right and the left. The first two features consist of a single word form</context>
<context position="13182" citStr="Gale et al., 1993" startWordPosition="2100" endWordPosition="2103">assignment of the most frequent reading literal (see Table 1). It has a coverage of 1 as it is applicable to all examples. Recall is 0 as no metonymies can be recognised. 5.1 Cooccurrences For all 64 window size combinations (for example results see Table 3), the accuracy never significantly beats the baseline. Both precision and recall are unsatisfactory and get steadily worse with increasing window sizes. We identified the following reasons for such a behaviour. Topical v. fine-grained sense distinctions. Cooccurrences and large window sizes traditionally work well for topical distinctions (Gale et al., 1993). Metonymy, though, does often not cross topical boundaries thus, whether a location name is used as a literal (political) reading or as a reading for the government often does not change coocurrence features. This is especially true for large window sizes. A A Table 3: Results for feature types feature accuracy/coverage accuracy/coverage-backoff precision/recall acc COV acc-backoff cov-backoff prec rec baseline .797 1.00 .797 1.00 n/a 0.00 cooccurrences content-word-in-window-12.r1 .770 .443 .780 1.00 .510 .204 content-word-in-window-13.r1 .783 .588 .806 1.00 .554 .249 content-word-in-window-</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1993</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1993. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
</authors>
<title>Deriving metonymic coercions from WordNet.</title>
<date>1998</date>
<booktitle>In Workshop of the Usage of WordNet in Natural Language Processing Systems, COLING-ACL &apos;98,</booktitle>
<pages>142--148</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="22739" citStr="Harabagiu, 1998" startWordPosition="3587" endWordPosition="3588">iscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy. Evaluations of metonymy interpretation (Lapata, 2001) include more disciplined evaluations, but do not handle metonymy r</context>
</contexts>
<marker>Harabagiu, 1998</marker>
<rawString>Sanda Harabagiu. 1998. Deriving metonymic coercions from WordNet. In Workshop of the Usage of WordNet in Natural Language Processing Systems, COLING-ACL &apos;98, pages 142-148, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark E Stickel</author>
<author>Douglas E Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--69</pages>
<contexts>
<context position="19286" citStr="Hobbs et al., 1993" startWordPosition="3056" endWordPosition="3059">es have been annotated by hand as we wanted to measure the contribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes. Therefore the results we present are an upper bound to what can be achieved with subject/object roles. 6 Related Work We compared our approach and results to WSD in Section 1 and 5, stressing word-to-word vs. class-to-class inference. Most traditional approaches to metonymy recognition use violations of selectional restrictions (plus sometimes syntactic violations) for recognition (Pustejovsky, 1995; Hobbs et al., 1993; Fass, 1997; Copestake and Briscoe, 1995; Stallard, 1993).7 Thus they furnish their algorithms with (mostly hand-modelled) selectional or grammatical restrictions. Note that selectional restrictions in these approaches are normally not seen as preferences but as absolute constraints. If and only if such an absolute constraint is violated a non-literal reading is proposed. In those experiments in which we also use grammatical knowledge, our system does not have any a priori knowledge of semantic argument-verb restrictions. Rather it refers to previously seen training data of country names as v</context>
<context position="22584" citStr="Hobbs et al., 1993" startWordPosition="3562" endWordPosition="3565"> SETGRAMM. Results are summarised in Table 4. Our classifier has higher recall, but lower precision than subject2 and subjectl . To compare the trade-off 8(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as </context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and Paul Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63:69-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shin-ichiro Kamei</author>
<author>Takahiro Wakao</author>
</authors>
<title>Metonymy: Reassessment, survey of acceptability and its treatment in machine translation systems.</title>
<date>1992</date>
<booktitle>In Proc. of the 30th Annual Meeting of the Association for Computational Linguistics;</booktitle>
<volume>2</volume>
<pages>309--311</pages>
<location>Newark, Del.,</location>
<marker>Kamei, Wakao, 1992</marker>
<rawString>Shin-ichiro Kamei and Takahiro Wakao. 1992. Metonymy: Reassessment, survey of acceptability and its treatment in machine translation systems. In Proc. of the 30th Annual Meeting of the Association for Computational Linguistics; Newark, Del., 28 June - 2 July 1992, pages 309-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology.</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="6641" citStr="Krippendorff, 1980" startWordPosition="1050" endWordPosition="1051">ding. This often occurs with coordinations and appositions. (11) &amp;quot;they arrived in Nigeria, hitherto a leading critic of... &amp;quot; In Example (11), both a literal (triggered by &amp;quot;arriving in&amp;quot;) and a place-for-people reading (triggered by &amp;quot;leading critic&amp;quot;) are invoked. We therefore introduced the category mixed to deal with these cases (not treated as a category in the literature). 2.3 Annotation Reliability, Distribution and Data Preparation The 1000 examples of our corpus have been independently annotated by two computational linguists, who are the authors of this paper. Reproducibility of results (Krippendorff, 1980) yielded a percentage agreement of .95 and a kappa (Carletta, 1996) of .88. The annotation can therefore be considered reliable. In the corpus data used for our classification experiments, we only included the samples which both annotators could agree on and which were not marked as noise. Therefore our corpus for testing and training the algorithm includes 925 samples. The resulting distribution of readings is described in Table 1. The data was further stripped of all punctuation and capitalisation was removed. No stemming or lemmatisation was performed. Table 1: Distribution of readings in o</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago, Ill.</location>
<contexts>
<context position="3454" citStr="Lakoff and Johnson, 1980" startWordPosition="546" endWordPosition="549">MW, henceforth) and was allowed to occur in the samples extracted. We searched the BNC using Gsearch (Corley et al., 2001). All samples include a PMW surrounded by three sentences of context. All examples introduced from now on are from the BNC.2 2.2 Annotation Scheme for Location Names After excluding some undesired examples (i.e., noise) which our extraction method collected (e.g. homonyms, such as &amp;quot;Greenland&amp;quot; in &amp;quot;Professor Greenland&amp;quot;), the annotation can proceed to identify literal, metonymic, and mixed readings. Our annotation scheme built on lists of metonymic patterns in the literature (Lakoff and Johnson, 1980; Fass, 1997; Stern, 1931), but diverted from these patterns when they did not provide full coverage or could not be distinguished reliably (Markert and Nissim, 2002). The literal reading for location names comprises a locative (see Example (6)) and a political entity interpretation (see Example (7)). (6) &amp;quot;coral coast of Papua New Guinea&amp;quot; &apos;At the moment we restrict ourselves to location names. 2An exception is Example (12). (7) &amp;quot;Britain&apos;s current account deficit&amp;quot; For metonymic readings, we distinguish between general patterns (valid for all physical objects) and location-specific ones. As gene</context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>George Lakoff and Mark Johnson. 1980. Metaphors We Live By. Chicago University Press, Chicago, Ill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>A corpus-based account of regular polysemy: The case of context-sensitive adjectives.</title>
<date>2001</date>
<booktitle>In Proc. of the 2nd Meeting of the North American Chapter of the ACL,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="23272" citStr="Lapata, 2001" startWordPosition="3665" endWordPosition="3666">ot report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy. Evaluations of metonymy interpretation (Lapata, 2001) include more disciplined evaluations, but do not handle metonymy recognition yet. 7 Conclusions We argued for viewing metonymy recognition as a WSD task based on semantic classes instead of individual words. This is motivated by the regularity of most metonymic readings. We presented a corpus reliably annotated for metonymic and literal usage which supports this claim. We also conducted several experiments with a decision list algorithm to explore the usefulness of common WSD features for metonymy recognition. We showed that coocurrence features are not useful for metonymy resolution, whereas</context>
</contexts>
<marker>Lapata, 2001</marker>
<rawString>Maria Lapata. 2001. A corpus-based account of regular polysemy: The case of context-sensitive adjectives. In Proc. of the 2nd Meeting of the North American Chapter of the ACL, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Udo Hahn</author>
</authors>
<title>Understanding metonymies in discourse.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<pages>135--1</pages>
<contexts>
<context position="20397" citStr="Markert and Hahn, 2002" startWordPosition="3228" endWordPosition="3231"> of semantic argument-verb restrictions. Rather it refers to previously seen training data of country names as verb arguments and their labelled senses and computes the likelihood of each sense using this distribution. This is advantageous for the following reasons: • There are many verbs with weak selectional restrictions (e.g., the verb &amp;quot;seem&amp;quot;). Both literal (see Example (12)) and metonymic (see Example (13)) readings of a location ocurring as subject of &amp;quot;seem&amp;quot; are therefore possible, although one of the readings might be more frequent given these features. (12) &amp;quot;Hungary seemed far away.&amp;quot; 7(Markert and Hahn, 2002) enhance this with anaphoric information. (13) &amp;quot;Britain seemed close to intervention.&amp;quot; Selectional restrictions as used in most metonymy recognition approaches therefore do not detect any violation. In contrast, the training data we use supplies the information that the metonymic place-for-people reading is more frequent given these grammatical features, leading the classifier to assign the correct reading in the majority of cases.8 • Our algorithm does not need to make any assumptions about the sense of the verb. Selectional restrictions, instead, must assume that the verb is disambiguated be</context>
<context position="22722" citStr="Markert and Hahn, 2002" startWordPosition="3583" endWordPosition="3586">mpare the trade-off 8(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy. Evaluations of metonymy interpretation (Lapata, 2001) include more disciplined evaluations, but do not </context>
</contexts>
<marker>Markert, Hahn, 2002</marker>
<rawString>Katja Markert and Udo Hahn. 2002. Understanding metonymies in discourse. Artificial Intelligence, 135(1/2):145-198, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Towards a corpus annotated for metonymies: the case of location names.</title>
<date>2002</date>
<booktitle>In Proc. of the 3&apos;d International Conference on Language Resources and Evaluation; Las</booktitle>
<location>Palmas, Canary Islands,</location>
<contexts>
<context position="2225" citStr="Markert and Nissim, 2002" startWordPosition="350" endWordPosition="353">e show that cooccurrences are in general not appropriate for metonymy resolution; collocations are useful but suffer from data sparseness when used as simple word forms; the grammatical relations subject and object perform well but are only applicable to a small part of the data. We then compare our algorithm to metonymy recognition approaches based on selectional restriction violations in Section 6. 2 Experimental Data We present a short overview of the collection of a corpus of location names and its annotation for literal and metonymic readings. A more detailed description can be found in (Markert and Nissim, 2002) and the annotation scheme is downloadable from http : //www. ltg . ed. ac. uktemalvi/mas cara/publ i cat ions . html. 2.1 Corpus Collection We extracted all country names from WordNet (Fellbaum, 1998) and the CIA factbook (http: //www. cia. gov/cia/publications/ f actbook/). This collection of names forms our sampling frame CountryList. We built a corpus of text samples that contains 1000 occurrences of country names, randomly extracted from the British National Corpus (http : //info . ox . ac .uk/bnc), henceforth abbreviated as BNC. Any country name in CountryList was a Possibly Metonymic Wo</context>
<context position="3620" citStr="Markert and Nissim, 2002" startWordPosition="573" endWordPosition="576">hree sentences of context. All examples introduced from now on are from the BNC.2 2.2 Annotation Scheme for Location Names After excluding some undesired examples (i.e., noise) which our extraction method collected (e.g. homonyms, such as &amp;quot;Greenland&amp;quot; in &amp;quot;Professor Greenland&amp;quot;), the annotation can proceed to identify literal, metonymic, and mixed readings. Our annotation scheme built on lists of metonymic patterns in the literature (Lakoff and Johnson, 1980; Fass, 1997; Stern, 1931), but diverted from these patterns when they did not provide full coverage or could not be distinguished reliably (Markert and Nissim, 2002). The literal reading for location names comprises a locative (see Example (6)) and a political entity interpretation (see Example (7)). (6) &amp;quot;coral coast of Papua New Guinea&amp;quot; &apos;At the moment we restrict ourselves to location names. 2An exception is Example (12). (7) &amp;quot;Britain&apos;s current account deficit&amp;quot; For metonymic readings, we distinguish between general patterns (valid for all physical objects) and location-specific ones. As general patterns were never encountered in our corpus, we describe here only the latter. • place-for-people: a place stands for any persons/organisations associated with </context>
<context position="5095" citStr="Markert and Nissim, 2002" startWordPosition="806" endWordPosition="809">ion or the whole population. (9) &amp;quot;The ... group expressed readiness to provide Albania with food aid&amp;quot; We therefore adopt a hierarchical approach, and assign a pattern (place-for-people) at a higher level (supertype), as well as a more specific pattern (subtype), if identifiable, at a lower level. This deviates from common practice in the linguistic literature, but has the great advantage of &apos;punishing&apos; disagreement only at a later stage and allowing fall-back options for automatic systems. We also experienced a drop in human annotation agreement from supertype to subtype classifications (see (Markert and Nissim, 2002)). In this paper, we evaluate our system on supertype classification. • place-for-event: a location name stands for something that happened there (see Example (2)). • place-for-product: a place stands for a product manufactured there (e.g., &amp;quot;Bordeaux&amp;quot; can refer to the wine produced there) . The category othermet covers unconventional metonymies. Since they are open-ended and context-dependent, no specific category indicating the intended semantic class can be introduced. In Example (10), &amp;quot;New Jersey&amp;quot; metonymically refers to the local typical tunes. (10) &amp;quot;The thing about the record is the influ</context>
</contexts>
<marker>Markert, Nissim, 2002</marker>
<rawString>Katja Markert and Malvina Nissim. 2002. Towards a corpus annotated for metonymies: the case of location names. In Proc. of the 3&apos;d International Conference on Language Resources and Evaluation; Las Palmas, Canary Islands, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Eneko Agirre</author>
</authors>
<title>One sense per collocation and genre/topic variations.</title>
<date>2000</date>
<booktitle>In Proc. of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and very large corpora.</booktitle>
<contexts>
<context position="8547" citStr="Martinez and Agirre, 2000" startWordPosition="1346" endWordPosition="1349">sed in WSD for metonymy recognition. The target readings for the algorithm to distinguish are literal, place-for-people, place-for-event, place-for-product, othermet and mixed. As an algorithm we use decision lists.3 The advantage of decision lists for a first exploration of a feature space is that their choices are easy to follow as they make use of the most informative feature only instead of a combination of features. All features encountered in the training data are ranked in the decision list (best evidence first) according to a log-likelihood ratio calculated as follows (Yarowsky, 1995; Martinez and Agirre, 2000): When applying the decision list to a test ex3A11 experiments reported here have also been repeated using a Naive Bayes classifier. The results have not improved on decision lists. ample, the winning reading is selected by the feature in the test example with the highest rank in the decision list. We estimated probabilities via maximum likelihood, adopting a simple smoothing method: 0.1 is added to both the denominator and numerator. 4 Exploration of feature space We investigated the following feature types. Examples are given in Table 2, together with examples of their distribution and the r</context>
<context position="18384" citStr="Martinez and Agirre, 2000" startWordPosition="2915" endWordPosition="2918">f more grammatical roles might further improve recall. Precision can be improved without sacrificing recall by also considering the verb, if present in the training data (classifier role-of-verb±role). So, whereas considering the role only will lead to assigning a place-for-people metonymy to all subjects, this is avoided in some cases when considering the verb in addition (e.g., for being the subject of the full verb &amp;quot;have&amp;quot;; see also Table 2). If the grammatical role with this particular verb has not been seen in the training data, the classifier will default to role, thus keeping coverage 5(Martinez and Agirre, 2000) also achieved better results with the use of collocations on the Wall Street Journal than on the balanced Brown Corpus. 60bviously the usefulness of grammatical roles will also depend on the kind of metonymy prevalent in the semantic class. high. Please note that the grammatical roles have been annotated by hand as we wanted to measure the contribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes. Therefore the results we present are an upper bound to what can be achieved with subject/object roles. 6 Related Work We</context>
</contexts>
<marker>Martinez, Agirre, 2000</marker>
<rawString>David Martinez and Eneko Agirre. 2000. One sense per collocation and genre/topic variations. In Proc. of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and very large corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics;</booktitle>
<pages>40--47</pages>
<location>Santa Cruz, Cal.,</location>
<contexts>
<context position="9633" citStr="Ng and Lee, 1996" startWordPosition="1536" endWordPosition="1539">We investigated the following feature types. Examples are given in Table 2, together with examples of their distribution and the reading they trigger. Cooccurrences. They have proved useful for WSD (Gale et al., 1993; Pedersen, 2000). We used left and right windows of context of 8 different sizes: 0, 1, 2, 3, 4, 5, 10, and 25 words, thus yielding 64 possible combinations of left and right sizes (e.g., 13.r1 for 3 words to the left and 1 to the right). Any content word in the window considered was included as a feature. Collocations. We selected 4 different collocations frequently used in WSD (Ng and Lee, 1996; Martinez and Agirre, 2000). The word to the right of the PMW, the word to the left, two words to the left and the word to the right and the left. The first two features consist of a single word form, the latter two of a sequence of two word forms. Function words were allowed as collocations, as e.g., the presence of a preposition directly to the left of the PMW can be indicative (see also Table 2). Grammatical features. Following some WSD approaches (Ng and Lee, 1996; Yarowsky, 1995) we use grammatical features, namely, Log ( )Pr (readingiI If eaturek) Pr (reading I f eaturek) Table 2: Examp</context>
<context position="15798" citStr="Ng and Lee, 1996" startWordPosition="2500" endWordPosition="2503">significant drop in coverage, given the lower number of features used (for window 14.r1: precision=.609; recall=.210; coverage=.098). The general tendency to prefer smaller windows over larger ones still holds. 5.2 Collocations The one-word collocations had in general a high coverage as function words were included. Accuracy for collocations is quite good (ranking from 81.9% to 87.0%). But increasing coverage to 1.00 (coverage-backoff) causes accuracy-backoff to drop. Recall is very low. We discuss here two reasons why collocations do worse in metonymy recognition than in WSD (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001). Target readings. Readings like othermet and mixed are unsuited for a collocation-based approach. Sparse data. When we inspected the decision lists, we found that strong collocations are mostly found for literal readings (e.g. spatial prepositions to the left of the PMW), so that a high percentage of literal examples can be identified correctly. Some good collocations for metonymic readings were found only once or twice in the training data and then not again in the test data, thus causing low recall and accuracy-backoff. One reason for this is that the training data for lite</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics; Santa Cruz, Cal., 23-28 June 1996, pages 40-47, Santa Cruz, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>The Pragmatics of Reference.</title>
<date>1978</date>
<tech>Ph.D. thesis,</tech>
<institution>City University of New</institution>
<location>York, New York.</location>
<marker>Nunberg, 1978</marker>
<rawString>Geoffrey Nunberg. 1978. The Pragmatics of Reference. Ph.D. thesis, City University of New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of Naive Bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proc. of the 18t Conference of the North American Chapter of the ACL;</booktitle>
<pages>63--69</pages>
<contexts>
<context position="9250" citStr="Pedersen, 2000" startWordPosition="1465" endWordPosition="1466">een repeated using a Naive Bayes classifier. The results have not improved on decision lists. ample, the winning reading is selected by the feature in the test example with the highest rank in the decision list. We estimated probabilities via maximum likelihood, adopting a simple smoothing method: 0.1 is added to both the denominator and numerator. 4 Exploration of feature space We investigated the following feature types. Examples are given in Table 2, together with examples of their distribution and the reading they trigger. Cooccurrences. They have proved useful for WSD (Gale et al., 1993; Pedersen, 2000). We used left and right windows of context of 8 different sizes: 0, 1, 2, 3, 4, 5, 10, and 25 words, thus yielding 64 possible combinations of left and right sizes (e.g., 13.r1 for 3 words to the left and 1 to the right). Any content word in the window considered was included as a feature. Collocations. We selected 4 different collocations frequently used in WSD (Ng and Lee, 1996; Martinez and Agirre, 2000). The word to the right of the PMW, the word to the left, two words to the left and the word to the right and the left. The first two features consist of a single word form, the latter two </context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Ted Pedersen. 2000. A simple approach to building ensembles of Naive Bayesian classifiers for word sense disambiguation. In Proc. of the 18t Conference of the North American Chapter of the ACL; 2000, pages 63-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A decision tree of bigrams is an accurate predictor of word sense.</title>
<date>2001</date>
<booktitle>In Proc. of the 2&amp;quot; Conference of the North American Chapter of the ACL;</booktitle>
<pages>79--86</pages>
<contexts>
<context position="15815" citStr="Pedersen, 2001" startWordPosition="2504" endWordPosition="2505">n coverage, given the lower number of features used (for window 14.r1: precision=.609; recall=.210; coverage=.098). The general tendency to prefer smaller windows over larger ones still holds. 5.2 Collocations The one-word collocations had in general a high coverage as function words were included. Accuracy for collocations is quite good (ranking from 81.9% to 87.0%). But increasing coverage to 1.00 (coverage-backoff) causes accuracy-backoff to drop. Recall is very low. We discuss here two reasons why collocations do worse in metonymy recognition than in WSD (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001). Target readings. Readings like othermet and mixed are unsuited for a collocation-based approach. Sparse data. When we inspected the decision lists, we found that strong collocations are mostly found for literal readings (e.g. spatial prepositions to the left of the PMW), so that a high percentage of literal examples can be identified correctly. Some good collocations for metonymic readings were found only once or twice in the training data and then not again in the test data, thus causing low recall and accuracy-backoff. One reason for this is that the training data for literal readings is a</context>
</contexts>
<marker>Pedersen, 2001</marker>
<rawString>Ted Pedersen. 2001. A decision tree of bigrams is an accurate predictor of word sense. In Proc. of the 2&amp;quot; Conference of the North American Chapter of the ACL; 2001, pages 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="19266" citStr="Pustejovsky, 1995" startWordPosition="3054" endWordPosition="3055">the grammatical roles have been annotated by hand as we wanted to measure the contribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes. Therefore the results we present are an upper bound to what can be achieved with subject/object roles. 6 Related Work We compared our approach and results to WSD in Section 1 and 5, stressing word-to-word vs. class-to-class inference. Most traditional approaches to metonymy recognition use violations of selectional restrictions (plus sometimes syntactic violations) for recognition (Pustejovsky, 1995; Hobbs et al., 1993; Fass, 1997; Copestake and Briscoe, 1995; Stallard, 1993).7 Thus they furnish their algorithms with (mostly hand-modelled) selectional or grammatical restrictions. Note that selectional restrictions in these approaches are normally not seen as preferences but as absolute constraints. If and only if such an absolute constraint is violated a non-literal reading is proposed. In those experiments in which we also use grammatical knowledge, our system does not have any a priori knowledge of semantic argument-verb restrictions. Rather it refers to previously seen training data o</context>
<context position="22632" citStr="Pustejovsky, 1995" startWordPosition="3570" endWordPosition="3571"> classifier has higher recall, but lower precision than subject2 and subjectl . To compare the trade-off 8(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy. Evaluations o</context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>James Pustejovsky. 1995. The Generative Lexicon. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Stallard</author>
</authors>
<title>Two kinds of metonymy.</title>
<date>1993</date>
<booktitle>In Proc. of the 31st Annual Meeting of the Association for Computational Linguistics;</booktitle>
<pages>87--94</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="19344" citStr="Stallard, 1993" startWordPosition="3066" endWordPosition="3067">ntribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes. Therefore the results we present are an upper bound to what can be achieved with subject/object roles. 6 Related Work We compared our approach and results to WSD in Section 1 and 5, stressing word-to-word vs. class-to-class inference. Most traditional approaches to metonymy recognition use violations of selectional restrictions (plus sometimes syntactic violations) for recognition (Pustejovsky, 1995; Hobbs et al., 1993; Fass, 1997; Copestake and Briscoe, 1995; Stallard, 1993).7 Thus they furnish their algorithms with (mostly hand-modelled) selectional or grammatical restrictions. Note that selectional restrictions in these approaches are normally not seen as preferences but as absolute constraints. If and only if such an absolute constraint is violated a non-literal reading is proposed. In those experiments in which we also use grammatical knowledge, our system does not have any a priori knowledge of semantic argument-verb restrictions. Rather it refers to previously seen training data of country names as verb arguments and their labelled senses and computes the l</context>
<context position="22756" citStr="Stallard, 1993" startWordPosition="3589" endWordPosition="3590">ke, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy. Evaluations of metonymy interpretation (Lapata, 2001) include more disciplined evaluations, but do not handle metonymy recognition yet. 7</context>
</contexts>
<marker>Stallard, 1993</marker>
<rawString>David Stallard. 1993. Two kinds of metonymy. In Proc. of the 31st Annual Meeting of the Association for Computational Linguistics; Columbus, Ohio, 22-26 June 1993, pages 87-94, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gustav Stern</author>
</authors>
<title>Meaning and Change of Meaning.</title>
<date>1931</date>
<journal>Goteborg: Wettergren &amp; Kerbers Forlag.</journal>
<contexts>
<context position="3480" citStr="Stern, 1931" startWordPosition="552" endWordPosition="553">ur in the samples extracted. We searched the BNC using Gsearch (Corley et al., 2001). All samples include a PMW surrounded by three sentences of context. All examples introduced from now on are from the BNC.2 2.2 Annotation Scheme for Location Names After excluding some undesired examples (i.e., noise) which our extraction method collected (e.g. homonyms, such as &amp;quot;Greenland&amp;quot; in &amp;quot;Professor Greenland&amp;quot;), the annotation can proceed to identify literal, metonymic, and mixed readings. Our annotation scheme built on lists of metonymic patterns in the literature (Lakoff and Johnson, 1980; Fass, 1997; Stern, 1931), but diverted from these patterns when they did not provide full coverage or could not be distinguished reliably (Markert and Nissim, 2002). The literal reading for location names comprises a locative (see Example (6)) and a political entity interpretation (see Example (7)). (6) &amp;quot;coral coast of Papua New Guinea&amp;quot; &apos;At the moment we restrict ourselves to location names. 2An exception is Example (12). (7) &amp;quot;Britain&apos;s current account deficit&amp;quot; For metonymic readings, we distinguish between general patterns (valid for all physical objects) and location-specific ones. As general patterns were never en</context>
</contexts>
<marker>Stern, 1931</marker>
<rawString>Gustav Stern. 1931. Meaning and Change of Meaning. Goteborg: Wettergren &amp; Kerbers Forlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelia Verspoor</author>
</authors>
<title>Lexical limits on the influence of context.</title>
<date>1996</date>
<booktitle>In Proc. of the 18th Annual Conference of the Cognitive Science Society; La Jolla, Cal.,</booktitle>
<pages>12--15</pages>
<contexts>
<context position="22649" citStr="Verspoor, 1996" startWordPosition="3572" endWordPosition="3573">her recall, but lower precision than subject2 and subjectl . To compare the trade-off 8(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration. between precision and recall we computed the Fmeasure for all algorithms, where our algorithm performed best. We also evaluate our approach more rigorously than other metonymy resolution algorithms Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results. Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturallyoccurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation. We, instead, use a reliably annotated corpus that we can make available to other researchers. In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy. Evaluations of metonymy interp</context>
</contexts>
<marker>Verspoor, 1996</marker>
<rawString>Cornelia Verspoor. 1996. Lexical limits on the influence of context. In Proc. of the 18th Annual Conference of the Cognitive Science Society; La Jolla, Cal., 12-15 July 1996, pages 116-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelia Verspoor</author>
</authors>
<title>Conventionality-governed logical metonymy. In</title>
<date>1997</date>
<booktitle>Proc. of the 2nd International Workshop on Computational Semantics,</booktitle>
<pages>300--312</pages>
<editor>H. Bunt, L. Kievit, R. Muskens, and N. Verlinden, editors,</editor>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="7645" citStr="Verspoor, 1997" startWordPosition="1212" endWordPosition="1213">stribution of readings is described in Table 1. The data was further stripped of all punctuation and capitalisation was removed. No stemming or lemmatisation was performed. Table 1: Distribution of readings in our corpus reading literal 737 79.7 place-for-people 161 17.4 place-for-event 3 .3 place-for-product 0 .0 mixed 15 1.6 othermet 9 1.0 total 925 100.0 3 Decision lists for metonymy resolution The distribution in the corpus shows that metonymic readings that do not follow established metonymic patterns (othermet) are very rare. This seems to be the case for other kinds of metonymies, too (Verspoor, 1997). This strengthens our case for viewing metonymy recognition as a classification task between literal readings and metonymic patterns that can be identified in advance for particular semantic classes. We therefore explore the usage of a classification algorithm and features used in WSD for metonymy recognition. The target readings for the algorithm to distinguish are literal, place-for-people, place-for-event, place-for-product, othermet and mixed. As an algorithm we use decision lists.3 The advantage of decision lists for a first exploration of a feature space is that their choices are easy t</context>
</contexts>
<marker>Verspoor, 1997</marker>
<rawString>Cornelia Verspoor. 1997. Conventionality-governed logical metonymy. In H. Bunt, L. Kievit, R. Muskens, and N. Verlinden, editors, Proc. of the 2nd International Workshop on Computational Semantics, pages 300-312, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. of the 33&apos; Annual Meeting of the Association for Computational Linguistics;</booktitle>
<pages>189--196</pages>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="1328" citStr="Yarowsky, 1995" startWordPosition="211" endWordPosition="212">a task which poses no problems for most humans. (3) &amp;quot;Bosnia&apos;s view of&amp;quot; (4) &amp;quot;Hungary&apos;s view of&amp;quot; (5) &amp;quot;Hungary&apos;s position on&amp;quot; In this paper, we explore this view of metonymy recognition as a class-based WSD task for the semantic class of locations.&apos; The corpus data we use is described in Section 2. As resources reliably annotated for metonymy do not exist (see also Section 6), we constructed a corpus of location names annotated for literal and metonymic readings. The supervised classification algorithm we use are decision lists as they have been successfully used in several classification tasks (Yarowsky, 1995; Collins and Singer, 1999) (see Section 3). In Section 4, we explore whether features traditionally used in WSD carry over to metonymy resolution, concentrating on (i) cooccurrences; (ii) collocations; and (iii) grammatical features. Results are discussed in Section 5. We show that cooccurrences are in general not appropriate for metonymy resolution; collocations are useful but suffer from data sparseness when used as simple word forms; the grammatical relations subject and object perform well but are only applicable to a small part of the data. We then compare our algorithm to metonymy recog</context>
<context position="8519" citStr="Yarowsky, 1995" startWordPosition="1344" endWordPosition="1345">m and features used in WSD for metonymy recognition. The target readings for the algorithm to distinguish are literal, place-for-people, place-for-event, place-for-product, othermet and mixed. As an algorithm we use decision lists.3 The advantage of decision lists for a first exploration of a feature space is that their choices are easy to follow as they make use of the most informative feature only instead of a combination of features. All features encountered in the training data are ranked in the decision list (best evidence first) according to a log-likelihood ratio calculated as follows (Yarowsky, 1995; Martinez and Agirre, 2000): When applying the decision list to a test ex3A11 experiments reported here have also been repeated using a Naive Bayes classifier. The results have not improved on decision lists. ample, the winning reading is selected by the feature in the test example with the highest rank in the decision list. We estimated probabilities via maximum likelihood, adopting a simple smoothing method: 0.1 is added to both the denominator and numerator. 4 Exploration of feature space We investigated the following feature types. Examples are given in Table 2, together with examples of </context>
<context position="10123" citStr="Yarowsky, 1995" startWordPosition="1630" endWordPosition="1631">idered was included as a feature. Collocations. We selected 4 different collocations frequently used in WSD (Ng and Lee, 1996; Martinez and Agirre, 2000). The word to the right of the PMW, the word to the left, two words to the left and the word to the right and the left. The first two features consist of a single word form, the latter two of a sequence of two word forms. Function words were allowed as collocations, as e.g., the presence of a preposition directly to the left of the PMW can be indicative (see also Table 2). Grammatical features. Following some WSD approaches (Ng and Lee, 1996; Yarowsky, 1995) we use grammatical features, namely, Log ( )Pr (readingiI If eaturek) Pr (reading I f eaturek) Table 2: Examples of features and their decision list score feature example assigned readingi Log readingi readingsa.o, grammar role &lt;country&gt; = subj place-for-people role-of-verb &lt;country&gt; = subj-of-have literal cooccurrences content-word-in-window-14.r4 content-word-in-window-14.r4 collocations word-to-left word-to-right two-words-to-left word-to-left-and-right states +/- 4 &lt;country&gt; win +/- 4 &lt;country&gt; in &lt;country&gt; &lt;country&gt; seemed one of &lt;country&gt; provide &lt;country&gt; with literal place-for-people </context>
<context position="15780" citStr="Yarowsky, 1995" startWordPosition="2498" endWordPosition="2499">recision, but a significant drop in coverage, given the lower number of features used (for window 14.r1: precision=.609; recall=.210; coverage=.098). The general tendency to prefer smaller windows over larger ones still holds. 5.2 Collocations The one-word collocations had in general a high coverage as function words were included. Accuracy for collocations is quite good (ranking from 81.9% to 87.0%). But increasing coverage to 1.00 (coverage-backoff) causes accuracy-backoff to drop. Recall is very low. We discuss here two reasons why collocations do worse in metonymy recognition than in WSD (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001). Target readings. Readings like othermet and mixed are unsuited for a collocation-based approach. Sparse data. When we inspected the decision lists, we found that strong collocations are mostly found for literal readings (e.g. spatial prepositions to the left of the PMW), so that a high percentage of literal examples can be identified correctly. Some good collocations for metonymic readings were found only once or twice in the training data and then not again in the test data, thus causing low recall and accuracy-backoff. One reason for this is that the trai</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of the 33&apos; Annual Meeting of the Association for Computational Linguistics; Cambridge, Mass., 26-30 June 1995, pages 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Homograph disambiguation in speech synthesis. In</title>
<date>1997</date>
<booktitle>Progress in Speech Synthesis,</booktitle>
<pages>159--175</pages>
<editor>R. Sproat, J. Olive, and J. Hirschberg, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>Yarowsky, 1997</marker>
<rawString>David Yarowsky. 1997. Homograph disambiguation in speech synthesis. In R. Sproat, J. Olive, and J. Hirschberg, editors, Progress in Speech Synthesis, pages 159-175. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>