<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044602">
<title confidence="0.9901665">
Sensible: L2 Translation Assistance by Emulating the Manual
Post-Editing Process
</title>
<author confidence="0.911181">
Liling Tan, Anne-Kathrin Schumann, Jose M.M. Martinez and Francis Bond1
</author>
<affiliation confidence="0.6972625">
Universit¨at des Saarland / Campus, Saarbr¨ucken, Germany
Nanyang Technological University1 / 14 Nanyang Drive, Singapore
</affiliation>
<email confidence="0.949172">
alvations@gmail.com, anne.schumann@mx.uni-saarland.de,
j.martinez@mx.uni-saarland.de, bond@ieee.org
</email>
<sectionHeader confidence="0.993783" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900888888889">
This paper describes the Post-Editor Z sys-
tem submitted to the L2 writing assis-
tant task in SemEval-2014. The aim of
task is to build a translation assistance
system to translate untranslated sentence
fragments. This is not unlike the task
of post-editing where human translators
improve machine-generated translations.
Post-Editor Z emulates the manual pro-
cess of post-editing by (i) crawling and ex-
tracting parallel sentences that contain the
untranslated fragments from a Web-based
translation memory, (ii) extracting the pos-
sible translations of the fragments indexed
by the translation memory and (iii) apply-
ing simple cosine-based sentence similar-
ity to rank possible translations for the un-
translated fragment.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.92868935483871">
In this paper, we present a collaborative submis-
sion between Saarland University and Nanyang
Technological University to the L2 Translation As-
sistant task in SemEval-2014. Our team name
is Sensible and the participating system is Post-
Editor Z (PEZ).
The L2 Translation Assistant task concerns the
translation of an untranslated fragment from a par-
tially translated sentence. For instance, given a
sentence, “Ich konnte B¨arbel noch on the border
in einen letzten S-Bahn-Zug nach Westberlin set-
zen.”, the aim is to provide an appropriate transla-
tion for the underline phrase, i.e. an der Grenze.
The aim of the task is not unlike the task of
post-editing where human translators correct er-
rors provided by machine-generated translations.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The main difference is that in the context of post-
editing the source text is provided. A transla-
tion workflow that incorporates post-editing be-
gins with a source sentence, e.g. “I could still
sit on the border in the very last tram to West
Berlin.” and the human translator is provided with
a machine-generated translation with untranslated
fragments such as the previous example and some-
times “fixing” the translation would simply re-
quire substituting the appropriate translation for
the untranslated fragment.
</bodyText>
<sectionHeader confidence="0.989145" genericHeader="introduction">
2 Related Tasks and Previous
Approaches
</sectionHeader>
<bodyText confidence="0.995702259259259">
The L2 writing assistant task lies between the
lines of machine translation and crosslingual word
sense disambiguation (CLWSD) or crosslingual
lexical substitution (CLS) (Lefever and Hoste,
2013; Mihalcea et al. 2010).
While CLWSD systems resolve the correct
semantics of the translation by providing the
correct lemma in the target language, CLS at-
tempts to provide also the correct form of the
translation with the right morphology. Machine
translation tasks focus on producing translations
of whole sentences/documents while crosslingual
word sense disambiguation targets a single lexical
item.
Previously, CLWSD systems have tried distri-
butional semantics and string matching methods
(Tan and Bond, 2013), unsupervised clustering of
word alignment vectors (Apidianaki, 2013) and
supervised classification-based approaches trained
on local context features for a window of three
words containing the focus word (van Gompel,
2010; van Gompel and van den Bosch, 2013; Rud-
nick et al., 2013). Interestingly, Carpuat (2013)
approached the CLWSD task with a Statistical MT
system .
Short of concatenating outputs of CLWSD /
CLS outputs and dealing with a reordering issue
</bodyText>
<page confidence="0.966735">
541
</page>
<note confidence="0.730353">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541–545,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9998262">
and responding to the task organizers’ call to avoid
implementing a full machine translation system
to tackle the task, we designed PEZ as an Auto-
matic Post-Editor (APE) that attempts to resolve
untranslated fragments.
</bodyText>
<sectionHeader confidence="0.995497" genericHeader="method">
3 Automatic Post-Editors
</sectionHeader>
<bodyText confidence="0.999920666666667">
APEs target various types of MT errors from de-
terminer selection (Knight and Chander, 1994) to
grammatical agreement (Mareˇcek et al., 2011).
Untranslated fragments from machine translations
are the result of out-of-vocabulary (OOV) words.
Previous approaches to the handling of un-
translated fragments include using a pivot lan-
guage to translate the OOV word(s) into a third
language and then back into to the source lan-
guage, thereby extracting paraphrases to OOV
(Callison-burch and Osborne, 2006), combining
sub-lexical/constituent translations of the OOV
word(s) to generate the translation (Huang et al.,
2011) or finding paraphrases of the OOV words
that have available translations (Marton et al.,
2009; Razmara et al., 2013). 1
However the simplest approach to handle un-
translated fragments is to increase the size of par-
allel data. The web is vast and infinite, a human
translator would consult the web when encounter-
ing a word that he/she cannot translate easily. The
most human-like approach to post-editing a for-
eign untranslated fragment is to do a search on
the web or a translation memory and choose the
most appropriate translation of the fragment from
the search result given the context of the machine
translated sentence.
</bodyText>
<sectionHeader confidence="0.992451" genericHeader="method">
4 Motivation
</sectionHeader>
<bodyText confidence="0.93375852173913">
When post-editing an untranslated fragment, a hu-
man translator would (i) first query a translation
memory or parallel corpus for the untranslated
fragment in the source language, (ii) then attempt
to understand the various context that the fragment
can occur in and (iii) finally he/she would sur-
mise appropriate translations for the untranslated
fragment based on semantic and grammatical con-
straints of the chosen translations.
1in MT, evaluation is normally performed using automatic
metrics based on automatic evaluation metrics that compares
scores based on string/word similarity between the machine-
generated translation and a reference output, simply remov-
ing OOV would have improved the metric “scores” of the
system (Habash, 2008; Tan and Pal, 2014).
The PEZ system was designed to emulate the
manual post-editing process by (i) first crawling
a web-based translation memory, (ii) then extract-
ing parallel sentences that contain the untranslated
fragments and the corresponding translations of
the fragments indexed by the translation memory
and (iii) finally ranking them based on cosine sim-
ilarity of the context words.
</bodyText>
<sectionHeader confidence="0.984253" genericHeader="method">
5 System Description
</sectionHeader>
<bodyText confidence="0.999884">
The PEZ system consists of three components,
viz (i) a Web Translation Memory (WebTM)
crawler, (ii) the XLING reranker and (iii) a longest
ngram/string match module.
</bodyText>
<subsectionHeader confidence="0.987986">
5.1 WebTM Crawler
</subsectionHeader>
<bodyText confidence="0.999871">
Given the query fragment and the context sen-
tence, “Die Frau kehrte alone nach Lima zur¨uck”,
the crawler queries www.bab.la and returns
sentences containing the untranslated fragment
with various possible tranlsations, e.g:
</bodyText>
<listItem confidence="0.992818333333333">
• isoliert : Darum sollten wir den Kaffee nicht
isoliert betrachten.
• alleine : Die Kommission kann nun aber f¨ur
ihr Verhalten nicht alleine die Folgen tragen.
• Allein : Allein in der Europischen Union
sind.
</listItem>
<bodyText confidence="0.999942555555556">
The retrieval mechanism is based on the
fact that the target translations of the queried
word/phrase are bolded on a web-based TM and
thus they can be easily extracted by manipulating
the text between &lt;bold&gt;...&lt;/bold&gt; tags. Al-
though the indexed translations were easy to ex-
tract, there were few instances where the transla-
tions were embedded betweeen the bold tags on
the web-based TM.
</bodyText>
<subsectionHeader confidence="0.995858">
5.2 XLING Reranker
</subsectionHeader>
<bodyText confidence="0.999893454545455">
XLING is a light-weight cosine-based sentence
similarity script used in the previous CLWSD
shared task in SemEval-2013 (Tan and Bond,
2013). Given the sentences from the WebTM
crawler, the reranker first removes all stopwords
from the sentences and then ranks the sentences
based on the number of overlapping stems.
In situations where there are no overlapping
content words from the sentences, XLING falls
back on the most common translation of the un-
translated fragment.
</bodyText>
<page confidence="0.976331">
542
</page>
<table confidence="0.9989956">
acc en-de rec acc en-es rec acc fr-en rec acc nl-en rec
wac wac wac wac
WebTM 0.160 0.184 0.647 0.145 0.175 0.470 0.055 0.067 0.210 0.092 0.099 0.214
XLING 0.152 0.178 0.647 0.141 0.171 0.470 0.055 0.067 0.210 0.088 0.095 0.214
PEZ 0.162 0.233 0.878 0.239 0.351 0.819 0.081 0.116 0.321 0.115 0.152 0.335
</table>
<tableCaption confidence="0.997661">
Table 1: Results for Best Evaluation of the System Runs.
</tableCaption>
<subsectionHeader confidence="0.998872">
5.3 Longest Ngram/String Matches
</subsectionHeader>
<bodyText confidence="0.999285214285714">
Due to the low coverage of the indexed trans-
lations on the web TM, it is necessary to ex-
tract more candidate translations. Assuming lit-
tle knowledge about the target language, human
translator would find parallel sentences containing
the untranslated fragment and resort to finding re-
peating phrases that occurs among the target lan-
guage sentences.
For instance, when we query the phrase history
book from the context “Von ihr habe ich mehr gel-
ernt als aus manchem history book.”, the longest
ngram/string matches module retrieves several tar-
get language sentences without any indexed trans-
lation:
</bodyText>
<listItem confidence="0.995789285714286">
• Ich weise darauf hin oder nehme an, dass
dies in den Geschichtsb¨uchern auch so
erw¨ahnt wird.
• Wenn die Geschichtsb¨ucher geschrieben wer-
den wird unser Zeitalter, denke ich, wegen
drei Dingen erinnert werden.
• Ich bin sicher, Pr¨asident Mugabe hat sich
nun einen Platz in den Geschichtsb¨uchern
gesichert, wenn auch aus den falschen
Gr¨unden.
• In den Geschichtsb¨uchern wird f¨ur jeden
einzelnen Tag der letzten mehr als 227 Jahre
an Gewalttaten oder Tragdien auf dem eu-
rop¨aischen Kontinent erinnert.
</listItem>
<bodyText confidence="0.999725">
By simply spotting the repeating word/string
from the target language sentences it is pos-
sible to guess that the possible candidates
for “history book” are Geschichtsb¨ucher or
Geschichtsb¨uchern. Computationally, this can
be achieved by looking for the longest matching
ngrams or the longest matching string across the
target language sentences fetched by the WebTM
crawler.
</bodyText>
<subsectionHeader confidence="0.976493">
5.4 System Runs
</subsectionHeader>
<bodyText confidence="0.996804">
We submitted three system runs to the L2 writing
assistant task in Semeval-2014.
</bodyText>
<listItem confidence="0.913233444444444">
1. WebTM: a baseline configuration which out-
puts the most frequent indexed translation of
the untranslated fragment from the Web TM.
2. XLING: reranks the WebTM outputs based
on cosine similarity.
3. PEZ: similar to the XLING but when the
WebTM fetches no output, the system looks
for longest common substring and reranks the
outputs based on cosine similarity.
</listItem>
<sectionHeader confidence="0.997291" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.998342777777778">
The evaluation of the task is based on three met-
rics, viz. absolute accuracy (acc), word-based ac-
curacy (wac) and recall (rec).
Absolute accuracy measures the number of
fragments that match the gold translation of the
untranslated fragments. Word-based accuracy as-
signs a score according to the longest consecutive
matching substring between output fragment and
reference fragment; it is computed as such:
</bodyText>
<equation confidence="0.9598385">
wac - |longestmatch(output,reference)|
max(|output|,|reference|)
</equation>
<bodyText confidence="0.997689666666667">
Recall accounts for the number of fragments for
which output was given (regardless of whether it
was correct).
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999947357142857">
Table 1 presents the results for the best evalua-
tion scores of the PEZ system runs for the En-
glish to German (en-de), English to Spanish (en-
es), French to English (fr-en) and Dutch to English
(nl-en) evaluations. Figure 1 presents the word ac-
curacy of the system runs for both best and out-of-
five (oof) evaluation2.
The results show that using the longest
ngram/string improves the recall and subsequently
the accuracy and word accuracy of the system.
However, this is not true when guessing untrans-
lated fragments from L1 English to L2. This is
due to the low recall of the system when search-
ing for the untranslated fragment in French and
</bodyText>
<footnote confidence="0.970828">
2Please refer to http://goo.gl/y9f5Na for results
of other competing systems
</footnote>
<page confidence="0.997507">
543
</page>
<figureCaption confidence="0.989366">
Figure 1: Word Accuracy of System Runs (best on
the left, oof on the right).
</figureCaption>
<bodyText confidence="0.9821165">
Dutch, where the English words/phases indexed in
the TM is much larger than other languages.
</bodyText>
<sectionHeader confidence="0.997238" genericHeader="evaluation">
8 Error Analysis
</sectionHeader>
<bodyText confidence="0.9999194">
We manually inspected the English-German out-
puts from the PEZ system and identified several
particularities of the outputs that account for the
low performance of the system for this language
pair.
</bodyText>
<subsectionHeader confidence="0.999061">
8.1 Weird Expressions in the TM
</subsectionHeader>
<bodyText confidence="0.999571777777778">
When attempting to translate Nevertheless in the
context of “Nevertheless hat sich die neue Bun-
desrepublik Deutschland unter amerikanischem
Druck an der militrischen Einmischung auf dem
Balkan beteiligt.” where the gold translation is
Trotzdem or Nichtsdestotrotz. The PEZ system re-
trieves the following sentence pairs that contains a
rarely used expression nichtsdestoweniger from a
literally translated sentence pair in the TM:
</bodyText>
<listItem confidence="0.9938375">
• EN: But nevertheless it is a fact that nobody
can really recognize their views in the report.
• DE: Aber nichtsdestoweniger kann sich nie-
mand so recht in dem Bericht wiederfinden.
</listItem>
<bodyText confidence="0.998657625">
Another example of weird expression is when
translating “husband” in the context of “In der
Silvesternacht sind mein husband und ich auf die
Bahnhofstraße gegangen.”. PEZ provided a lesser
use yet valid translation Gemahl instead of the
gold translation Mann. In this case, it is also a
matter of register where in a more formal register
one will use Gemahl instead of Mann.
</bodyText>
<subsectionHeader confidence="0.9983825">
8.2 Missing / Additional Words from
Matches
</subsectionHeader>
<bodyText confidence="0.999650857142857">
When extracting candidate translations from the
TM index or longest ngram/string, there are sev-
eral matches where the PEZ system outputs a par-
tial phrase or phrases with additional tokens that
cause the disparity between the absolute accuracy
and word accuracy. An instance of missing words
is as follows:
</bodyText>
<listItem confidence="0.992092941176471">
• Input: Eine genetische Veranlagung
plays a decisive role.
• PEZ: Eine genetische Ueranlagung
eine entscheidende rolle.
• Gold: Eine genetische Ueranlagung
spielt (dabei) eine entscheidende rolle.
For the addition of superfluous words is as fol-
lows:
• Input: Ger¨ate wie Handys sind
not permitted wenn sie nicht unterrichtlichen
Belangen dienen.
• PEZ: Ger¨ate wie Handys sind es verboten,
wenn sie nicht unterrichtlichen Belangen
dienen.
• Gold: Ger¨ate wie Handys sind verboten
wenn sie nicht unterrichtlichen Belangen di-
enen.
</listItem>
<subsectionHeader confidence="0.996709">
8.3 Case Sensitivity
</subsectionHeader>
<bodyText confidence="0.999952">
For the English-German evaluation, there are sev-
eral instances where the PEZ system produces the
correct translation of the phrase but in lower cases
and this resulted in poorer accuracy. This is unique
to German target language and possibly contribut-
ing to the lower scores as compared to the English-
Spanish evaluation.
</bodyText>
<sectionHeader confidence="0.996143" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.9999347">
In this paper, we presented the PEZ automatic
post-editor system in the L2 writing assistant task
in SemEval-2014. The PEZ post-editing system is
a resource lean approach to provide translation for
untranslated fragments based on no prior training
data and simple string manipulations from a web-
based translation memory.
The PEZ system attempts to emulate the pro-
cess of a human translator post-editing out-
of-vocabulary words from a machine-generated
</bodyText>
<page confidence="0.992945">
544
</page>
<bodyText confidence="0.999888833333333">
translation. The best configuration of the PEZ sys-
tem involves a simple string search for the longest
common ngram/string from the target language
sentences without having word/phrasal alignment
and also avoiding the need to handle word reorder-
ing for multi-token untranslated fragments.
</bodyText>
<sectionHeader confidence="0.990278" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999584">
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement n◦ 317471.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863714285714">
Marianna Apidianaki. 2013. Limsi : Cross-lingual
word sense disambiguation using translation sense
clustering. In Second Joint Conference on Lexi-
cal and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
178–182, Atlanta, Georgia, USA, June.
Chris Callison-burch and Miles Osborne. 2006. Im-
proved statistical machine translation using para-
phrases. In In Proceedings of HLT/NAACL-2006,
pages 17–24.
Marine Carpuat. 2013. Nrc: A machine translation ap-
proach to cross-lingual word sense disambiguation
(semeval-2013 task 10). In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 188–192, Atlanta, Georgia, USA, June.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In ACL, pages 57–
60.
Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang,
Shih-Ting Huang, and Jason S. Chang. 2011. Using
sublexical translations to handle the oov problem in
machine translation. ACM Trans. Asian Lang. Inf.
Process., 10(3):16.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In AAAI, pages 779–784.
David Mareˇcek, Rudolf Rosa, Petra Galuˇsˇc´akov´a, and
Ondˇrej Bojar. 2011. Two-step translation with
grammatical post-processing. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
WMT ’11, pages 426–432, Stroudsburg, PA, USA.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
EMNLP, pages 381–390.
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1105–
1115, Sofia, Bulgaria, August.
Alex Rudnick, Can Liu, and Michael Gasser. 2013.
Hltdi: Cl-wsd using markov random fields for
semeval-2013 task 10. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 171–177, Atlanta, Georgia, USA, June.
Liling Tan and Francis Bond. 2013. Xling: Match-
ing query sentences to a parallel corpus using topic
models for wsd. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
167–170, Atlanta, Georgia, USA, June.
Liling Tan and Santanu Pal. 2014. Manawi: Using
multi-word expressions and named entities to im-
prove machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
Baltimore, USA, August.
Maarten van Gompel and Antal van den Bosch. 2013.
Wsd2: Parameter optimisation for memory-based
cross-lingual word-sense disambiguation. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 183–187, Atlanta, Geor-
gia, USA, June.
Maarten van Gompel. 2010. Uvt-wsd1: A cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, SemEval ’10, pages 238–241,
Stroudsburg, PA, USA.
</reference>
<page confidence="0.998532">
545
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.234434">
<title confidence="0.9620105">L2 Translation Assistance by Emulating the Post-Editing Process</title>
<author confidence="0.569221">Anne-Kathrin Schumann Tan</author>
<author confidence="0.569221">Jose M M Martinez</author>
<author confidence="0.569221">Francis</author>
<affiliation confidence="0.668331">Universit¨at des Saarland / Campus, Saarbr¨ucken,</affiliation>
<address confidence="0.536224">Technological / 14 Nanyang Drive,</address>
<email confidence="0.8536045">alvations@gmail.com,j.martinez@mx.uni-saarland.de,bond@ieee.org</email>
<abstract confidence="0.99076852631579">This paper describes the Post-Editor Z system submitted to the L2 writing assistant task in SemEval-2014. The aim of task is to build a translation assistance system to translate untranslated sentence fragments. This is not unlike the task of post-editing where human translators improve machine-generated translations. Post-Editor Z emulates the manual process of post-editing by (i) crawling and extracting parallel sentences that contain the untranslated fragments from a Web-based translation memory, (ii) extracting the possible translations of the fragments indexed by the translation memory and (iii) applying simple cosine-based sentence similarity to rank possible translations for the untranslated fragment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marianna Apidianaki</author>
</authors>
<title>Limsi : Cross-lingual word sense disambiguation using translation sense clustering.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>178--182</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3396" citStr="Apidianaki, 2013" startWordPosition="487" endWordPosition="488">on (CLS) (Lefever and Hoste, 2013; Mihalcea et al. 2010). While CLWSD systems resolve the correct semantics of the translation by providing the correct lemma in the target language, CLS attempts to provide also the correct form of the translation with the right morphology. Machine translation tasks focus on producing translations of whole sentences/documents while crosslingual word sense disambiguation targets a single lexical item. Previously, CLWSD systems have tried distributional semantics and string matching methods (Tan and Bond, 2013), unsupervised clustering of word alignment vectors (Apidianaki, 2013) and supervised classification-based approaches trained on local context features for a window of three words containing the focus word (van Gompel, 2010; van Gompel and van den Bosch, 2013; Rudnick et al., 2013). Interestingly, Carpuat (2013) approached the CLWSD task with a Statistical MT system . Short of concatenating outputs of CLWSD / CLS outputs and dealing with a reordering issue 541 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541–545, Dublin, Ireland, August 23-24, 2014. and responding to the task organizers’ call to avoid implementing a </context>
</contexts>
<marker>Apidianaki, 2013</marker>
<rawString>Marianna Apidianaki. 2013. Limsi : Cross-lingual word sense disambiguation using translation sense clustering. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 178–182, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-burch</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases. In</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL-2006,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="4671" citStr="Callison-burch and Osborne, 2006" startWordPosition="679" endWordPosition="682">the task, we designed PEZ as an Automatic Post-Editor (APE) that attempts to resolve untranslated fragments. 3 Automatic Post-Editors APEs target various types of MT errors from determiner selection (Knight and Chander, 1994) to grammatical agreement (Mareˇcek et al., 2011). Untranslated fragments from machine translations are the result of out-of-vocabulary (OOV) words. Previous approaches to the handling of untranslated fragments include using a pivot language to translate the OOV word(s) into a third language and then back into to the source language, thereby extracting paraphrases to OOV (Callison-burch and Osborne, 2006), combining sub-lexical/constituent translations of the OOV word(s) to generate the translation (Huang et al., 2011) or finding paraphrases of the OOV words that have available translations (Marton et al., 2009; Razmara et al., 2013). 1 However the simplest approach to handle untranslated fragments is to increase the size of parallel data. The web is vast and infinite, a human translator would consult the web when encountering a word that he/she cannot translate easily. The most human-like approach to post-editing a foreign untranslated fragment is to do a search on the web or a translation me</context>
</contexts>
<marker>Callison-burch, Osborne, 2006</marker>
<rawString>Chris Callison-burch and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In In Proceedings of HLT/NAACL-2006, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>Nrc: A machine translation approach to cross-lingual word sense disambiguation (semeval-2013 task 10).</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>188--192</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3639" citStr="Carpuat (2013)" startWordPosition="524" endWordPosition="525"> with the right morphology. Machine translation tasks focus on producing translations of whole sentences/documents while crosslingual word sense disambiguation targets a single lexical item. Previously, CLWSD systems have tried distributional semantics and string matching methods (Tan and Bond, 2013), unsupervised clustering of word alignment vectors (Apidianaki, 2013) and supervised classification-based approaches trained on local context features for a window of three words containing the focus word (van Gompel, 2010; van Gompel and van den Bosch, 2013; Rudnick et al., 2013). Interestingly, Carpuat (2013) approached the CLWSD task with a Statistical MT system . Short of concatenating outputs of CLWSD / CLS outputs and dealing with a reordering issue 541 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541–545, Dublin, Ireland, August 23-24, 2014. and responding to the task organizers’ call to avoid implementing a full machine translation system to tackle the task, we designed PEZ as an Automatic Post-Editor (APE) that attempts to resolve untranslated fragments. 3 Automatic Post-Editors APEs target various types of MT errors from determiner selection (K</context>
</contexts>
<marker>Carpuat, 2013</marker>
<rawString>Marine Carpuat. 2013. Nrc: A machine translation approach to cross-lingual word sense disambiguation (semeval-2013 task 10). In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 188–192, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four techniques for online handling of out-of-vocabulary words in arabic-english statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="6164" citStr="Habash, 2008" startWordPosition="913" endWordPosition="914">or the untranslated fragment in the source language, (ii) then attempt to understand the various context that the fragment can occur in and (iii) finally he/she would surmise appropriate translations for the untranslated fragment based on semantic and grammatical constraints of the chosen translations. 1in MT, evaluation is normally performed using automatic metrics based on automatic evaluation metrics that compares scores based on string/word similarity between the machinegenerated translation and a reference output, simply removing OOV would have improved the metric “scores” of the system (Habash, 2008; Tan and Pal, 2014). The PEZ system was designed to emulate the manual post-editing process by (i) first crawling a web-based translation memory, (ii) then extracting parallel sentences that contain the untranslated fragments and the corresponding translations of the fragments indexed by the translation memory and (iii) finally ranking them based on cosine similarity of the context words. 5 System Description The PEZ system consists of three components, viz (i) a Web Translation Memory (WebTM) crawler, (ii) the XLING reranker and (iii) a longest ngram/string match module. 5.1 WebTM Crawler Gi</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in arabic-english statistical machine translation. In ACL, pages 57– 60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-Chi Huang</author>
<author>Ho-Ching Yen</author>
<author>Ping-Che Yang</author>
<author>Shih-Ting Huang</author>
<author>Jason S Chang</author>
</authors>
<title>Using sublexical translations to handle the oov problem in machine translation.</title>
<date>2011</date>
<journal>ACM Trans. Asian Lang. Inf. Process.,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="4787" citStr="Huang et al., 2011" startWordPosition="694" endWordPosition="697">ors APEs target various types of MT errors from determiner selection (Knight and Chander, 1994) to grammatical agreement (Mareˇcek et al., 2011). Untranslated fragments from machine translations are the result of out-of-vocabulary (OOV) words. Previous approaches to the handling of untranslated fragments include using a pivot language to translate the OOV word(s) into a third language and then back into to the source language, thereby extracting paraphrases to OOV (Callison-burch and Osborne, 2006), combining sub-lexical/constituent translations of the OOV word(s) to generate the translation (Huang et al., 2011) or finding paraphrases of the OOV words that have available translations (Marton et al., 2009; Razmara et al., 2013). 1 However the simplest approach to handle untranslated fragments is to increase the size of parallel data. The web is vast and infinite, a human translator would consult the web when encountering a word that he/she cannot translate easily. The most human-like approach to post-editing a foreign untranslated fragment is to do a search on the web or a translation memory and choose the most appropriate translation of the fragment from the search result given the context of the mac</context>
</contexts>
<marker>Huang, Yen, Yang, Huang, Chang, 2011</marker>
<rawString>Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang, Shih-Ting Huang, and Jason S. Chang. 2011. Using sublexical translations to handle the oov problem in machine translation. ACM Trans. Asian Lang. Inf. Process., 10(3):16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In AAAI,</booktitle>
<pages>779--784</pages>
<contexts>
<context position="4263" citStr="Knight and Chander, 1994" startWordPosition="618" endWordPosition="621">) approached the CLWSD task with a Statistical MT system . Short of concatenating outputs of CLWSD / CLS outputs and dealing with a reordering issue 541 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541–545, Dublin, Ireland, August 23-24, 2014. and responding to the task organizers’ call to avoid implementing a full machine translation system to tackle the task, we designed PEZ as an Automatic Post-Editor (APE) that attempts to resolve untranslated fragments. 3 Automatic Post-Editors APEs target various types of MT errors from determiner selection (Knight and Chander, 1994) to grammatical agreement (Mareˇcek et al., 2011). Untranslated fragments from machine translations are the result of out-of-vocabulary (OOV) words. Previous approaches to the handling of untranslated fragments include using a pivot language to translate the OOV word(s) into a third language and then back into to the source language, thereby extracting paraphrases to OOV (Callison-burch and Osborne, 2006), combining sub-lexical/constituent translations of the OOV word(s) to generate the translation (Huang et al., 2011) or finding paraphrases of the OOV words that have available translations (M</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In AAAI, pages 779–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mareˇcek</author>
<author>Rudolf Rosa</author>
<author>Petra Galuˇsˇc´akov´a</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Two-step translation with grammatical post-processing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>426--432</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>Mareˇcek, Rosa, Galuˇsˇc´akov´a, Bojar, 2011</marker>
<rawString>David Mareˇcek, Rudolf Rosa, Petra Galuˇsˇc´akov´a, and Ondˇrej Bojar. 2011. Two-step translation with grammatical post-processing. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 426–432, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved statistical machine translation using monolingually-derived paraphrases.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>381--390</pages>
<contexts>
<context position="4881" citStr="Marton et al., 2009" startWordPosition="709" endWordPosition="712">) to grammatical agreement (Mareˇcek et al., 2011). Untranslated fragments from machine translations are the result of out-of-vocabulary (OOV) words. Previous approaches to the handling of untranslated fragments include using a pivot language to translate the OOV word(s) into a third language and then back into to the source language, thereby extracting paraphrases to OOV (Callison-burch and Osborne, 2006), combining sub-lexical/constituent translations of the OOV word(s) to generate the translation (Huang et al., 2011) or finding paraphrases of the OOV words that have available translations (Marton et al., 2009; Razmara et al., 2013). 1 However the simplest approach to handle untranslated fragments is to increase the size of parallel data. The web is vast and infinite, a human translator would consult the web when encountering a word that he/she cannot translate easily. The most human-like approach to post-editing a foreign untranslated fragment is to do a search on the web or a translation memory and choose the most appropriate translation of the fragment from the search result given the context of the machine translated sentence. 4 Motivation When post-editing an untranslated fragment, a human tra</context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved statistical machine translation using monolingually-derived paraphrases. In EMNLP, pages 381–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Maryam Siahbani</author>
<author>Reza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1105--1115</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4904" citStr="Razmara et al., 2013" startWordPosition="713" endWordPosition="716">ement (Mareˇcek et al., 2011). Untranslated fragments from machine translations are the result of out-of-vocabulary (OOV) words. Previous approaches to the handling of untranslated fragments include using a pivot language to translate the OOV word(s) into a third language and then back into to the source language, thereby extracting paraphrases to OOV (Callison-burch and Osborne, 2006), combining sub-lexical/constituent translations of the OOV word(s) to generate the translation (Huang et al., 2011) or finding paraphrases of the OOV words that have available translations (Marton et al., 2009; Razmara et al., 2013). 1 However the simplest approach to handle untranslated fragments is to increase the size of parallel data. The web is vast and infinite, a human translator would consult the web when encountering a word that he/she cannot translate easily. The most human-like approach to post-editing a foreign untranslated fragment is to do a search on the web or a translation memory and choose the most appropriate translation of the fragment from the search result given the context of the machine translated sentence. 4 Motivation When post-editing an untranslated fragment, a human translator would (i) first</context>
</contexts>
<marker>Razmara, Siahbani, Haffari, Sarkar, 2013</marker>
<rawString>Majid Razmara, Maryam Siahbani, Reza Haffari, and Anoop Sarkar. 2013. Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105– 1115, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Rudnick</author>
<author>Can Liu</author>
<author>Michael Gasser</author>
</authors>
<title>Hltdi: Cl-wsd using markov random fields for semeval-2013 task 10.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>171--177</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3608" citStr="Rudnick et al., 2013" startWordPosition="518" endWordPosition="522">so the correct form of the translation with the right morphology. Machine translation tasks focus on producing translations of whole sentences/documents while crosslingual word sense disambiguation targets a single lexical item. Previously, CLWSD systems have tried distributional semantics and string matching methods (Tan and Bond, 2013), unsupervised clustering of word alignment vectors (Apidianaki, 2013) and supervised classification-based approaches trained on local context features for a window of three words containing the focus word (van Gompel, 2010; van Gompel and van den Bosch, 2013; Rudnick et al., 2013). Interestingly, Carpuat (2013) approached the CLWSD task with a Statistical MT system . Short of concatenating outputs of CLWSD / CLS outputs and dealing with a reordering issue 541 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541–545, Dublin, Ireland, August 23-24, 2014. and responding to the task organizers’ call to avoid implementing a full machine translation system to tackle the task, we designed PEZ as an Automatic Post-Editor (APE) that attempts to resolve untranslated fragments. 3 Automatic Post-Editors APEs target various types of MT erro</context>
</contexts>
<marker>Rudnick, Liu, Gasser, 2013</marker>
<rawString>Alex Rudnick, Can Liu, and Michael Gasser. 2013. Hltdi: Cl-wsd using markov random fields for semeval-2013 task 10. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 171–177, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Francis Bond</author>
</authors>
<title>Xling: Matching query sentences to a parallel corpus using topic models for wsd.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>167--170</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3326" citStr="Tan and Bond, 2013" startWordPosition="477" endWordPosition="480">ual word sense disambiguation (CLWSD) or crosslingual lexical substitution (CLS) (Lefever and Hoste, 2013; Mihalcea et al. 2010). While CLWSD systems resolve the correct semantics of the translation by providing the correct lemma in the target language, CLS attempts to provide also the correct form of the translation with the right morphology. Machine translation tasks focus on producing translations of whole sentences/documents while crosslingual word sense disambiguation targets a single lexical item. Previously, CLWSD systems have tried distributional semantics and string matching methods (Tan and Bond, 2013), unsupervised clustering of word alignment vectors (Apidianaki, 2013) and supervised classification-based approaches trained on local context features for a window of three words containing the focus word (van Gompel, 2010; van Gompel and van den Bosch, 2013; Rudnick et al., 2013). Interestingly, Carpuat (2013) approached the CLWSD task with a Statistical MT system . Short of concatenating outputs of CLWSD / CLS outputs and dealing with a reordering issue 541 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541–545, Dublin, Ireland, August 23-24, 2014</context>
<context position="7742" citStr="Tan and Bond, 2013" startWordPosition="1159" endWordPosition="1162">lleine die Folgen tragen. • Allein : Allein in der Europischen Union sind. The retrieval mechanism is based on the fact that the target translations of the queried word/phrase are bolded on a web-based TM and thus they can be easily extracted by manipulating the text between &lt;bold&gt;...&lt;/bold&gt; tags. Although the indexed translations were easy to extract, there were few instances where the translations were embedded betweeen the bold tags on the web-based TM. 5.2 XLING Reranker XLING is a light-weight cosine-based sentence similarity script used in the previous CLWSD shared task in SemEval-2013 (Tan and Bond, 2013). Given the sentences from the WebTM crawler, the reranker first removes all stopwords from the sentences and then ranks the sentences based on the number of overlapping stems. In situations where there are no overlapping content words from the sentences, XLING falls back on the most common translation of the untranslated fragment. 542 acc en-de rec acc en-es rec acc fr-en rec acc nl-en rec wac wac wac wac WebTM 0.160 0.184 0.647 0.145 0.175 0.470 0.055 0.067 0.210 0.092 0.099 0.214 XLING 0.152 0.178 0.647 0.141 0.171 0.470 0.055 0.067 0.210 0.088 0.095 0.214 PEZ 0.162 0.233 0.878 0.239 0.351 </context>
</contexts>
<marker>Tan, Bond, 2013</marker>
<rawString>Liling Tan and Francis Bond. 2013. Xling: Matching query sentences to a parallel corpus using topic models for wsd. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 167–170, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Santanu Pal</author>
</authors>
<title>Manawi: Using multi-word expressions and named entities to improve machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, USA,</location>
<contexts>
<context position="6184" citStr="Tan and Pal, 2014" startWordPosition="915" endWordPosition="918">lated fragment in the source language, (ii) then attempt to understand the various context that the fragment can occur in and (iii) finally he/she would surmise appropriate translations for the untranslated fragment based on semantic and grammatical constraints of the chosen translations. 1in MT, evaluation is normally performed using automatic metrics based on automatic evaluation metrics that compares scores based on string/word similarity between the machinegenerated translation and a reference output, simply removing OOV would have improved the metric “scores” of the system (Habash, 2008; Tan and Pal, 2014). The PEZ system was designed to emulate the manual post-editing process by (i) first crawling a web-based translation memory, (ii) then extracting parallel sentences that contain the untranslated fragments and the corresponding translations of the fragments indexed by the translation memory and (iii) finally ranking them based on cosine similarity of the context words. 5 System Description The PEZ system consists of three components, viz (i) a Web Translation Memory (WebTM) crawler, (ii) the XLING reranker and (iii) a longest ngram/string match module. 5.1 WebTM Crawler Given the query fragme</context>
</contexts>
<marker>Tan, Pal, 2014</marker>
<rawString>Liling Tan and Santanu Pal. 2014. Manawi: Using multi-word expressions and named entities to improve machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarten van Gompel</author>
<author>Antal van den Bosch</author>
</authors>
<title>Wsd2: Parameter optimisation for memory-based cross-lingual word-sense disambiguation.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>183--187</pages>
<location>Atlanta, Georgia, USA,</location>
<marker>van Gompel, van den Bosch, 2013</marker>
<rawString>Maarten van Gompel and Antal van den Bosch. 2013. Wsd2: Parameter optimisation for memory-based cross-lingual word-sense disambiguation. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 183–187, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarten van Gompel</author>
</authors>
<title>Uvt-wsd1: A crosslingual word sense disambiguation system.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>238--241</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>van Gompel, 2010</marker>
<rawString>Maarten van Gompel. 2010. Uvt-wsd1: A crosslingual word sense disambiguation system. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 238–241, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>