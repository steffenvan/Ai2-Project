<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.098090">
<title confidence="0.9997885">
A Simple Method for Resolution of Definite Reference
in a Shared Visual Context
</title>
<author confidence="0.993497">
Alexander Siebert David Schlangen
</author>
<affiliation confidence="0.9078895">
Berlin-Brandenburgische Department of Linguistics
Akademie der Wissenschaften University of Potsdam, Germany
</affiliation>
<email confidence="0.994966">
siebert@bbaw.de das@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.995546" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999310071428572">
We present a method for resolving definite ex-
ophoric reference to visually shared objects
that is based on a) an automatically learned,
simple mapping of words to visual features
(“visual word semantics”), b) an automat-
ically learned, semantically-motivated utter-
ance segmentation (“visual grammar”), and c)
a procedure that, given an utterance, uses b)
to combine a) to yield a resolution. We evalu-
ated the method both on a pre-recorded corpus
and in an online setting, where it performed
with 81% (chance: 14%) and 66% accuracy,
respectively. This is comparable to results re-
ported in related work on simpler settings.
</bodyText>
<sectionHeader confidence="0.975924" genericHeader="keywords">
1 The Task
</sectionHeader>
<bodyText confidence="0.984887307692308">
The method described in this paper is a module of
a dialogue system that acts as a collaborator of a
human player in the task of manipulating visually
present puzzle objects. An example scene is shown
in Figure 1 (the indices a and b are added here for
illustrative purposes). Given utterances like those in
(1), the task of the module is to identify the likely
referents (here, a and b, respectively).1
(1) a.Take the piece in the middle on the left side.
b.Take the piece in the middle.
More formally, the task can be characterised as fol-
lows: possibly starting with an a priori assump-
tion about likely referents (e.g., from knowledge of
</bodyText>
<footnote confidence="0.995632333333333">
1Our system is implemented for German input; for ease of
description we use examples from our corpus translated into
English here.
</footnote>
<page confidence="0.9937">
84
</page>
<figureCaption confidence="0.999902">
Figure 1: Example Scene
</figureCaption>
<bodyText confidence="0.99556225">
discourse salience), the module uses the evidence
present in the utterance (words, syntax) and in the
visual scene (visual features) to derive at a new as-
sumption about likely referents. If we call such an
assumption a confidence function c that assigns to
each object in the domain O, a number between 0
and 1; i.e., c : O —* R, then reference resolution is a
function r that takes a triple of an initial confidence
function c, an utterance u, and a visual scene repre-
sentation v to yield an updated confidence function
c′. Formally: r:CxUxV—*C.
In the following, we describe the resources
needed to set up the module, its subcomponents, and
the evaluation we performed. We close by relating
the proposed method to prior work and discussing
future extensions.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="introduction">
2 Resources
</sectionHeader>
<subsectionHeader confidence="0.996787">
2.1 Corpus
</subsectionHeader>
<bodyText confidence="0.98781575">
As our method is based on automatically learned
models, a corpus is required. Our intended use case
is similar to the setting described in (Schlangen and
Fern´andez, 2007), but with the addition of a shared
visual context. We collected 300 scene descriptions
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 84–87,
Columbus, June 2008. c�2008 Association for Computational Linguistics
(of scenes containing between 1 and 12 distinct,
monochrome shapes, randomly placed and rotated
on a rectangular area) using the two-part methodol-
ogy of (Siebert et al., 2007) that yields recordings
and quality assessments (here: attempts to follow
other subjects’ instructions). We also later recorded
an additional 300 scene descriptions by a single
speaker, to further increase our data base.
After transcription of the recordings (239 min-
utes of audio material), we discarded roughly 6%
of the instructions because they could not be fol-
lowed by the evaluators, and a further 4% because
the complexity of the descriptions was outside the
scope of what we wanted to model. The remaining
instructions were then automatically cleaned from
dysfluencies, morphologically lemmatised and POS
tagged, and annotated as described below.
</bodyText>
<subsectionHeader confidence="0.990766">
2.2 Computer Vision
</subsectionHeader>
<bodyText confidence="0.99999065">
The other required resource is a visual perception
algorithm. We use it to compute a feature repre-
sentation of every visual scene as presented in the
data collection:2 First, each object is represented by
a number of object features such as size / length /
height of the bounding box, center of gravity, num-
ber of edges. Second, topological features note for
each object the distance to certain points on the
board (edges, center, etc.) and to other objects.
(For details on the computation of such features see
for example (Regier and Carlson, 2001).) Lastly,
we also compute groupings of objects by clustering
along columns and rows or both (see Figure 2 for an
illustration). For each group, we compute two sets
of topological features, one for the objects within
the group (e.g., distance to the center of the group),
and one for the configuration of groups (distance of
group to other objects). This set of features was se-
lected to be representative of typical basic visual fea-
tures.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="method">
3 Components
</sectionHeader>
<subsectionHeader confidence="0.999907">
3.1 Visual Grammar
</subsectionHeader>
<bodyText confidence="0.987683571428571">
The ‘visual grammar’ segments utterances accord-
ing to functional aspects on two levels. The first
2At the moment, the input to the algorithm is a symbolic
representation of the scene (which object is where); the features
are designed to also be derivable from digital images instead,
using standard computer vision techniques (Shapiro and Stock-
man, 2001); this is future work, however.
</bodyText>
<figureCaption confidence="0.991879">
Figure 2: Scene with Horizontal Group Detection
</figureCaption>
<bodyText confidence="0.994599625">
describes the macro-structure of a spatial expres-
sion, i.e., the division into target (the denoted ob-
ject; T) and optional landmarks (other objects; LM)
and their relation to the target (R; see example in Ta-
ble 2). The second level annotates the spatial-lexical
function of each word, e.g., whether the word de-
notes a piece or a configuration of pieces (Table 1).
A fully ‘parsed’ example is shown in Table 2.
</bodyText>
<figure confidence="0.7162456">
the cross from the second column from left at the top
l r d n p g r d r d r
(a) - Annotation of spatial lexical functions
T R LM LM LM LM T
(b) - Segmentation of visual spatial parts
</figure>
<tableCaption confidence="0.956646">
Table 2: Example Annotation / ‘Parse’
</tableCaption>
<bodyText confidence="0.999761375">
Given the requirement for robustness, we decided
against a hand-written grammar for deriving such
annotations; the moderate size of our corpus on
the other hand made for example Markov model-
based approaches difficult to apply. We hence chose
transformation-based learning to create this (shal-
low) segmentation grammar, converting the seg-
mentation task into a tagging task (as is done in
</bodyText>
<table confidence="0.9986316">
Name Description Examples
l lexical reference T,piece,cross
d r topological direction top left Corner
d s topological distance outer left
d n numeric second column
p g group (perceptually active) from the left column
g s synthetic group the three pieces on the left
f landmark field N in the Middle
r prepositional relation in the middle
grad grading function exactly right
</table>
<tableCaption confidence="0.997958">
Table 1: Visual Lexical Functions of Words
</tableCaption>
<page confidence="0.999366">
85
</page>
<bodyText confidence="0.999908444444444">
(Ramshaw and Marcus, 1995), inter alia). In our ap-
proach, each token that is to be tagged is itself repre-
sented in three different forms or layers: lemmatised
word, as POS-tag, and by its spatial-functional tag
(as in Table 1; added by simple look-up). All these
layers can be accessed in the learned rules. Apart
from this, the module is a straightforward imple-
mentation of (Ramshaw and Marcus, 1995), which
in turn adapts (Brill, 1993) for syntactic chunking.
</bodyText>
<subsectionHeader confidence="0.998822">
3.2 Visual Word Semantics
</subsectionHeader>
<bodyText confidence="0.99999574">
To learn the visual semantics of words we imple-
mented a simple technique for grounding words in
perceptions. Roughly, the idea is to extract from
all instances in which a word was used in the train-
ing corpus and all associated scenes a prototypical
visual meaning representation by identifying those
features whose values best predict the appropriate-
ness of the word given a scene. (This is similar in
spirit to the approach used in (Roy, 2002).)
As material for learning, we only used the sim-
ple expressions (target only, no landmark) in the
corpus, to ensure that all words used were in some
way ‘about’ the target. The algorithm iterates over
all pairs of utterance and scene and saves for each
lemma all visual information. This creates for each
lemma a matrix of feature values with as many rows
as there were occurrences of the lemma. The values
in each column (that is, for each feature) are then
normalised to the interval [-1, 1] and the standard
deviation is recorded.
The next tasks then are a) to compute one sin-
gle representative value for each feature, but only
b) for those features that carry semantic weight for
the given word (i.e., to compute a dimensionality re-
duction). E.g., for the lemma ’left’, we want the fea-
ture x distance to center to be part of the semantic
model, but not y distance to center.
One option for a) is to simply take the average
value as representative for a feature (for a given
word). While this works for some words, it causes
problems for others which imply a maximisation
and not a prototypisation. E.g., the lemma left is
best represented by maximal values of the feature
x distance to center, not by the average of all val-
ues for all occurrences of left (this will yield some-
thing like leftish). Perhaps surprisingly, representa-
tion through the majority value, i.e., choosing the
most frequent value as representative for a feature
(for a given word), performed better, and is hence
the method we chose.
For b), dimensionality reduction, we again chose
a very simple approach (much simpler than for ex-
ample (Roy, 2002)): features are filtered out as ir-
relevant for a given lemma features if their variance
is above a certain threshold. To give an example,
for the lemma left the distribution of values of the
feature x distance to center varies with a Q of 0.05,
that of y distance to center with a Q of 0.41. We
empirically determined the setting of the threshold
such that it excluded the latter.3
</bodyText>
<subsectionHeader confidence="0.962939">
3.3 Combination
</subsectionHeader>
<figureCaption confidence="0.996462">
Figure 3: Steps of the Algorithm for Example Utterance
</figureCaption>
<bodyText confidence="0.9998742">
The combination algorithm works through the
segmented utterance and combines visual word se-
mantics to yield a reference hypothesis. Figure 3
illustrates this process for the example from Table 2.
On detecting a landmark segment (Step 1), the res-
olution algorithm ‘activates’ the appropriate group;
which one this is is determined by the p g item in
the landmark segment. (Here: column). The group
is then treated as a single object, and (Step 2) the
semantics of topological terms (d r or d s) in the
landmark segment is applied to it (more on this in
a second). For our example, this yields a ranking
of all columns with respect to their ‘left-ness’. The
ordinal ‘second’ finally simply picks out the second
element on this list–the second group w.r.t. the prop-
erty of leftness (Step 3). The expressions in the tar-
get segment are now only applied to the members
of the group that was selected in this way; i.e., the
semantic models of ‘top’ and ‘cross’ are now only
applied to the objects in that column (Steps 4 to 6).
</bodyText>
<footnote confidence="0.976407666666667">
3With more data and hence the possibility to set aside a de-
velopment set, one could and should of course set such a thresh-
old automatically.
</footnote>
<page confidence="0.998038">
86
</page>
<bodyText confidence="0.9999339">
Semantic word models are applied through a sim-
ple calculation of distance between values (of se-
mantic model and actual scene): the closer, the bet-
ter the match of word to scene. (Modulo selectivity
of a feature; for a feature that occurred for all lem-
mata with a high specificity (small Q), good matches
are expected to be closer to the prototype value than
for features with a high variability.)
This method encodes parts of the utterance se-
mantics procedurally, namely the way how certain
phrases (here grouped under the label landmark) se-
mantically modify other phrases (here grouped un-
der the label target). This encoding makes the al-
gorithm perhaps harder to understand than seman-
tic composition rules tied to syntactic rules, but it
also affords a level of abstraction over specific syn-
tactic rules: our very general concepts of landmark
and target cover various ways of modification (e.g.
through PPs or relative clauses), adding to the ro-
bustness of the approach.
</bodyText>
<sectionHeader confidence="0.999418" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999645038461538">
With an f-score of 0.985 (10-fold cross validation),
the transformation-based learning of the segmen-
tation performs quite well, roughly at the level
of state-of-the-art POS-taggers (albeit with a much
smaller tag inventory). Also evaluated via cross-
validation on the corpus, the resolution component
as a whole performs with an accuracy of 80.67%
(using frequency-based word-semantic features; it
drops to 66.95% for average-based). There were on
average 7 objects in each scene in the corpus; i.e.
the baseline of getting the reference right by chance
is 14%. Our system significantly improves over this
baseline.
We also evaluated the system in a more realis-
tic application situation. We asked subjects to refer
to certain pieces in presented scenes (via typed ut-
terances); here, the system reached a success-rate
of 66% (7 subjects, 100 scene / utterance pairs).
While this is considerably lower than the corpus-
based evaluation, it is still on a par with related
systems using more complicated resolution methods
(Roy, 2002; Gorniak and Roy, 2004). We also think
these results represent the lower end of the perfor-
mance range that can be expected in practical use,
as in an interactive dialogue system users have time
to adapt to the capabilities of the system.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999994466666667">
We have presented a method for resolving defi-
nite, exophoric reference to objects that are visu-
ally co-present to user and system. The method
combines automatically acquired models (a ‘visual
word semantics’, a simple, but effective mapping be-
tween visual features and words; and a ‘visual gram-
mar’, a semantically motivated segmentation of ut-
terances) and hard-coded knowledge (combination
procedure). To us, this combines the strengths of
two approaches: statistical, where robustness and
wide coverage is required, hard-coding, where few,
but complex patterns are concerned.
We are currently integrating the module into a
working dialogue system; in future work we will in-
vestigate the use of digital images as input format.
</bodyText>
<sectionHeader confidence="0.998151" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.636177">
This work was supported by DFG through an Emmy
Noether Programm Grant to the second author.
</bodyText>
<sectionHeader confidence="0.998696" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846461538461">
Eric Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania.
Peter Gorniak and Deb Roy. 2004. Grounded semantic
composition for visual scenes. In Journal of Artifical
Intelligence Research.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Pro-
ceedings of the Third Workshop on Very Large Cor-
pora, pages 82–94.
Terry Regier and Laura A. Carlson. 2001. Grounding
spatial language in perception: An empirical and com-
putational investigation. In Journal of Experimental
Psychology, volume 130, pages 273–298.
Deb Roy. 2002. Learning words and syntax for a visual
description task. Computer Speech and Language,
16(3).
David Schlangen and Raquel Fern´andez. 2007. Beyond
repair: Testing the limits of the conversational repair
system. In Proceedings of SIGdial 2007, pages 51–
54, Antwerp, Belgium, September.
Linda G. Shapiro and George C. Stockman. 2001. Com-
puter Vision. Prentice Hall, New Jersey, USA.
Alexander Siebert, David Schlangen, and Raquel
Fern´andez. 2007. An implemented method for dis-
tributed collection and assessment of speech data. In
Proceedings of SIGdial 2007, Antwerp, Belgium.
</reference>
<page confidence="0.999475">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.118902">
<title confidence="0.998513">A Simple Method for Resolution of Definite Reference in a Shared Visual Context</title>
<author confidence="0.999897">Alexander Siebert David Schlangen</author>
<affiliation confidence="0.9907">Berlin-Brandenburgische Department of Linguistics</affiliation>
<address confidence="0.927696">Akademie der Wissenschaften University of Potsdam, Germany</address>
<email confidence="0.975436">siebert@bbaw.dedas@ling.uni-potsdam.de</email>
<abstract confidence="0.998544896551724">We present a method for resolving definite exophoric reference to visually shared objects that is based on a) an automatically learned, simple mapping of words to visual features (“visual word semantics”), b) an automatically learned, semantically-motivated utterance segmentation (“visual grammar”), and c) a procedure that, given an utterance, uses b) to combine a) to yield a resolution. We evaluated the method both on a pre-recorded corpus and in an online setting, where it performed with 81% (chance: 14%) and 66% accuracy, respectively. This is comparable to results reported in related work on simpler settings. 1 The Task The method described in this paper is a module of a dialogue system that acts as a collaborator of a human player in the task of manipulating visually present puzzle objects. An example scene is shown Figure 1 (the indices added here for illustrative purposes). Given utterances like those in (1), the task of the module is to identify the likely (here, (1) a.Take the piece in the middle on the left side. b.Take the piece in the middle. More formally, the task can be characterised as folpossibly starting with an priori assumption about likely referents (e.g., from knowledge of system is implemented for German input; for ease of description we use examples from our corpus translated into English here. 84 Figure 1: Example Scene discourse salience), the module uses the evidence present in the utterance (words, syntax) and in the visual scene (visual features) to derive at a new assumption about likely referents. If we call such an a function assigns to object in the domain a number between 0 1; i.e., —* then resolution a takes a triple of an initial confidence an utterance and a visual scene repreyield an updated confidence function Formally: In the following, we describe the resources needed to set up the module, its subcomponents, and the evaluation we performed. We close by relating the proposed method to prior work and discussing future extensions. 2 Resources 2.1 Corpus As our method is based on automatically learned models, a corpus is required. Our intended use case is similar to the setting described in (Schlangen and Fern´andez, 2007), but with the addition of a shared visual context. We collected 300 scene descriptions of the 9th SIGdial Workshop on Discourse and pages 84–87, June 2008. Association for Computational Linguistics (of scenes containing between 1 and 12 distinct, monochrome shapes, randomly placed and rotated on a rectangular area) using the two-part methodology of (Siebert et al., 2007) that yields recordings and quality assessments (here: attempts to follow other subjects’ instructions). We also later recorded an additional 300 scene descriptions by a single speaker, to further increase our data base. After transcription of the recordings (239 minutes of audio material), we discarded roughly 6% of the instructions because they could not be followed by the evaluators, and a further 4% because the complexity of the descriptions was outside the scope of what we wanted to model. The remaining instructions were then automatically cleaned from dysfluencies, morphologically lemmatised and POS tagged, and annotated as described below. 2.2 Computer Vision The other required resource is a visual perception algorithm. We use it to compute a feature representation of every visual scene as presented in the First, each object is represented by number of features as size / length / height of the bounding box, center of gravity, numof edges. Second, features for each object the distance to certain points on the board (edges, center, etc.) and to other objects. (For details on the computation of such features see for example (Regier and Carlson, 2001).) Lastly, we also compute groupings of objects by clustering along columns and rows or both (see Figure 2 for an illustration). For each group, we compute two sets one for the objects within the group (e.g., distance to the center of the group), and one for the configuration of groups (distance of group to other objects). This set of features was selected to be representative of typical basic visual features. 3 Components 3.1 Visual Grammar The ‘visual grammar’ segments utterances according to functional aspects on two levels. The first the moment, the input to the algorithm is a symbolic representation of the scene (which object is where); the features are designed to also be derivable from digital images instead, using standard computer vision techniques (Shapiro and Stockman, 2001); this is future work, however. Figure 2: Scene with Horizontal Group Detection describes the macro-structure of a spatial expresi.e., the division into denoted ob- T) and optional (other objects; LM) their the target (R; see example in Table 2). The second level annotates the spatial-lexical function of each word, e.g., whether the word denotes a piece or a configuration of pieces (Table 1). A fully ‘parsed’ example is shown in Table 2. cross fromthe column fromleft at the top l r d n p g r d r d r of spatial lexical functions T R LM LM LM LM T of visual spatial parts Table 2: Example Annotation / ‘Parse’ Given the requirement for robustness, we decided against a hand-written grammar for deriving such annotations; the moderate size of our corpus on the other hand made for example Markov modelbased approaches difficult to apply. We hence chose transformation-based learning to create this (shallow) segmentation grammar, converting the segmentation task into a tagging task (as is done in Name Description Examples l lexical reference d r topological direction left d s topological distance d n numeric p g group (perceptually active) the left g s synthetic group three the left f landmark field N the r prepositional relation middle grad grading function Table 1: Visual Lexical Functions of Words 85 and Marcus, 1995), In our approach, each token that is to be tagged is itself represented in three different forms or layers: lemmatised word, as POS-tag, and by its spatial-functional tag (as in Table 1; added by simple look-up). All these layers can be accessed in the learned rules. Apart from this, the module is a straightforward implementation of (Ramshaw and Marcus, 1995), which in turn adapts (Brill, 1993) for syntactic chunking. 3.2 Visual Word Semantics To learn the visual semantics of words we implemented a simple technique for grounding words in perceptions. Roughly, the idea is to extract from all instances in which a word was used in the training corpus and all associated scenes a prototypical visual meaning representation by identifying those features whose values best predict the appropriateness of the word given a scene. (This is similar in spirit to the approach used in (Roy, 2002).) As material for learning, we only used the simple expressions (target only, no landmark) in the corpus, to ensure that all words used were in some way ‘about’ the target. The algorithm iterates over all pairs of utterance and scene and saves for each lemma all visual information. This creates for each lemma a matrix of feature values with as many rows as there were occurrences of the lemma. The values in each column (that is, for each feature) are then normalised to the interval [-1, 1] and the standard deviation is recorded. The next tasks then are a) to compute one single representative value for each feature, but only b) for those features that carry semantic weight for the given word (i.e., to compute a dimensionality reduction). E.g., for the lemma ’left’, we want the feadistance to center be part of the semantic but not distance to One option for a) is to simply take the average value as representative for a feature (for a given word). While this works for some words, it causes problems for others which imply a maximisation not a prototypisation. E.g., the lemma represented by of the feature distance to not by the average of all valfor all occurrences of will yield somelike Perhaps surprisingly, representation through the majority value, i.e., choosing the most frequent value as representative for a feature (for a given word), performed better, and is hence the method we chose. For b), dimensionality reduction, we again chose a very simple approach (much simpler than for example (Roy, 2002)): features are filtered out as irrelevant for a given lemma features if their variance is above a certain threshold. To give an example, the lemma distribution of values of the distance to center with a of distance to center a We empirically determined the setting of the threshold that it excluded the 3.3 Combination Figure 3: Steps of the Algorithm for Example Utterance The combination algorithm works through the segmented utterance and combines visual word semantics to yield a reference hypothesis. Figure 3 illustrates this process for the example from Table 2. On detecting a landmark segment (Step 1), the resolution algorithm ‘activates’ the appropriate group; one this is is determined by the g in landmark segment. (Here: The group is then treated as a single object, and (Step 2) the semantics of topological terms (d r or d s) in the landmark segment is applied to it (more on this in a second). For our example, this yields a ranking of all columns with respect to their ‘left-ness’. The ordinal ‘second’ finally simply picks out the second element on this list–the second group w.r.t. the property of leftness (Step 3). The expressions in the target segment are now only applied to the members of the group that was selected in this way; i.e., the semantic models of ‘top’ and ‘cross’ are now only applied to the objects in that column (Steps 4 to 6). more data and hence the possibility to set aside a development set, one could and should of course set such a threshold automatically. 86 Semantic word models are applied through a simple calculation of distance between values (of semantic model and actual scene): the closer, the better the match of word to scene. (Modulo selectivity of a feature; for a feature that occurred for all lemwith a high specificity (small good matches are expected to be closer to the prototype value than for features with a high variability.) This method encodes parts of the utterance semantics procedurally, namely the way how certain (here grouped under the label semantically modify other phrases (here grouped unthe label This encoding makes the algorithm perhaps harder to understand than semantic composition rules tied to syntactic rules, but it also affords a level of abstraction over specific synrules: our very general concepts of various ways of modification (e.g. through PPs or relative clauses), adding to the robustness of the approach. 4 Evaluation With an f-score of 0.985 (10-fold cross validation), the transformation-based learning of the segmentation performs quite well, roughly at the level of state-of-the-art POS-taggers (albeit with a much smaller tag inventory). Also evaluated via crossvalidation on the corpus, the resolution component as a whole performs with an accuracy of 80.67% (using frequency-based word-semantic features; it drops to 66.95% for average-based). There were on average 7 objects in each scene in the corpus; i.e. the baseline of getting the reference right by chance is 14%. Our system significantly improves over this baseline. We also evaluated the system in a more realistic application situation. We asked subjects to refer to certain pieces in presented scenes (via typed utterances); here, the system reached a success-rate of 66% (7 subjects, 100 scene / utterance pairs). While this is considerably lower than the corpusbased evaluation, it is still on a par with related systems using more complicated resolution methods (Roy, 2002; Gorniak and Roy, 2004). We also think these results represent the lower end of the performance range that can be expected in practical use, as in an interactive dialogue system users have time to adapt to the capabilities of the system. 5 Conclusions We have presented a method for resolving definite, exophoric reference to objects that are visually co-present to user and system. The method combines automatically acquired models (a ‘visual word semantics’, a simple, but effective mapping between visual features and words; and a ‘visual grammar’, a semantically motivated segmentation of utterances) and hard-coded knowledge (combination procedure). To us, this combines the strengths of two approaches: statistical, where robustness and wide coverage is required, hard-coding, where few, but complex patterns are concerned. We are currently integrating the module into a working dialogue system; in future work we will investigate the use of digital images as input format.</abstract>
<note confidence="0.775279966666667">Acknowledgements This work was supported by DFG through an Emmy Noether Programm Grant to the second author. References Brill. 1993. Corpus-Based Approach to Language Ph.D. thesis, University of Pennsylvania. Peter Gorniak and Deb Roy. 2004. Grounded semantic for visual scenes. In of Artifical Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpages 82–94. Terry Regier and Laura A. Carlson. 2001. Grounding spatial language in perception: An empirical and cominvestigation. In of Experimental volume 130, pages 273–298. Deb Roy. 2002. Learning words and syntax for a visual task. Speech and 16(3). David Schlangen and Raquel Fern´andez. 2007. Beyond repair: Testing the limits of the conversational repair In of SIGdial pages 51– 54, Antwerp, Belgium, September. G. Shapiro and George C. Stockman. 2001. Com- Prentice Hall, New Jersey, USA. Alexander Siebert, David Schlangen, and Raquel Fern´andez. 2007. An implemented method for distributed collection and assessment of speech data. In of SIGdial Antwerp, Belgium. 87</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7075" citStr="Brill, 1993" startWordPosition="1165" endWordPosition="1166">e pieces on the left f landmark field N in the Middle r prepositional relation in the middle grad grading function exactly right Table 1: Visual Lexical Functions of Words 85 (Ramshaw and Marcus, 1995), inter alia). In our approach, each token that is to be tagged is itself represented in three different forms or layers: lemmatised word, as POS-tag, and by its spatial-functional tag (as in Table 1; added by simple look-up). All these layers can be accessed in the learned rules. Apart from this, the module is a straightforward implementation of (Ramshaw and Marcus, 1995), which in turn adapts (Brill, 1993) for syntactic chunking. 3.2 Visual Word Semantics To learn the visual semantics of words we implemented a simple technique for grounding words in perceptions. Roughly, the idea is to extract from all instances in which a word was used in the training corpus and all associated scenes a prototypical visual meaning representation by identifying those features whose values best predict the appropriateness of the word given a scene. (This is similar in spirit to the approach used in (Roy, 2002).) As material for learning, we only used the simple expressions (target only, no landmark) in the corpus</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Grounded semantic composition for visual scenes.</title>
<date>2004</date>
<journal>In Journal of Artifical Intelligence Research.</journal>
<contexts>
<context position="12847" citStr="Gorniak and Roy, 2004" startWordPosition="2150" endWordPosition="2153">ed). There were on average 7 objects in each scene in the corpus; i.e. the baseline of getting the reference right by chance is 14%. Our system significantly improves over this baseline. We also evaluated the system in a more realistic application situation. We asked subjects to refer to certain pieces in presented scenes (via typed utterances); here, the system reached a success-rate of 66% (7 subjects, 100 scene / utterance pairs). While this is considerably lower than the corpusbased evaluation, it is still on a par with related systems using more complicated resolution methods (Roy, 2002; Gorniak and Roy, 2004). We also think these results represent the lower end of the performance range that can be expected in practical use, as in an interactive dialogue system users have time to adapt to the capabilities of the system. 5 Conclusions We have presented a method for resolving definite, exophoric reference to objects that are visually co-present to user and system. The method combines automatically acquired models (a ‘visual word semantics’, a simple, but effective mapping between visual features and words; and a ‘visual grammar’, a semantically motivated segmentation of utterances) and hard-coded kno</context>
</contexts>
<marker>Gorniak, Roy, 2004</marker>
<rawString>Peter Gorniak and Deb Roy. 2004. Grounded semantic composition for visual scenes. In Journal of Artifical Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="6664" citStr="Ramshaw and Marcus, 1995" startWordPosition="1092" endWordPosition="1095">ifficult to apply. We hence chose transformation-based learning to create this (shallow) segmentation grammar, converting the segmentation task into a tagging task (as is done in Name Description Examples l lexical reference T,piece,cross d r topological direction top left Corner d s topological distance outer left d n numeric second column p g group (perceptually active) from the left column g s synthetic group the three pieces on the left f landmark field N in the Middle r prepositional relation in the middle grad grading function exactly right Table 1: Visual Lexical Functions of Words 85 (Ramshaw and Marcus, 1995), inter alia). In our approach, each token that is to be tagged is itself represented in three different forms or layers: lemmatised word, as POS-tag, and by its spatial-functional tag (as in Table 1; added by simple look-up). All these layers can be accessed in the learned rules. Apart from this, the module is a straightforward implementation of (Ramshaw and Marcus, 1995), which in turn adapts (Brill, 1993) for syntactic chunking. 3.2 Visual Word Semantics To learn the visual semantics of words we implemented a simple technique for grounding words in perceptions. Roughly, the idea is to extra</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, pages 82–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
<author>Laura A Carlson</author>
</authors>
<title>Grounding spatial language in perception: An empirical and computational investigation.</title>
<date>2001</date>
<journal>In Journal of Experimental Psychology,</journal>
<volume>130</volume>
<pages>273--298</pages>
<contexts>
<context position="4295" citStr="Regier and Carlson, 2001" startWordPosition="690" endWordPosition="693">ised and POS tagged, and annotated as described below. 2.2 Computer Vision The other required resource is a visual perception algorithm. We use it to compute a feature representation of every visual scene as presented in the data collection:2 First, each object is represented by a number of object features such as size / length / height of the bounding box, center of gravity, number of edges. Second, topological features note for each object the distance to certain points on the board (edges, center, etc.) and to other objects. (For details on the computation of such features see for example (Regier and Carlson, 2001).) Lastly, we also compute groupings of objects by clustering along columns and rows or both (see Figure 2 for an illustration). For each group, we compute two sets of topological features, one for the objects within the group (e.g., distance to the center of the group), and one for the configuration of groups (distance of group to other objects). This set of features was selected to be representative of typical basic visual features. 3 Components 3.1 Visual Grammar The ‘visual grammar’ segments utterances according to functional aspects on two levels. The first 2At the moment, the input to th</context>
</contexts>
<marker>Regier, Carlson, 2001</marker>
<rawString>Terry Regier and Laura A. Carlson. 2001. Grounding spatial language in perception: An empirical and computational investigation. In Journal of Experimental Psychology, volume 130, pages 273–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb Roy</author>
</authors>
<title>Learning words and syntax for a visual description task.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="7570" citStr="Roy, 2002" startWordPosition="1248" endWordPosition="1249">s, the module is a straightforward implementation of (Ramshaw and Marcus, 1995), which in turn adapts (Brill, 1993) for syntactic chunking. 3.2 Visual Word Semantics To learn the visual semantics of words we implemented a simple technique for grounding words in perceptions. Roughly, the idea is to extract from all instances in which a word was used in the training corpus and all associated scenes a prototypical visual meaning representation by identifying those features whose values best predict the appropriateness of the word given a scene. (This is similar in spirit to the approach used in (Roy, 2002).) As material for learning, we only used the simple expressions (target only, no landmark) in the corpus, to ensure that all words used were in some way ‘about’ the target. The algorithm iterates over all pairs of utterance and scene and saves for each lemma all visual information. This creates for each lemma a matrix of feature values with as many rows as there were occurrences of the lemma. The values in each column (that is, for each feature) are then normalised to the interval [-1, 1] and the standard deviation is recorded. The next tasks then are a) to compute one single representative v</context>
<context position="9194" citStr="Roy, 2002" startWordPosition="1533" endWordPosition="1534">rds, it causes problems for others which imply a maximisation and not a prototypisation. E.g., the lemma left is best represented by maximal values of the feature x distance to center, not by the average of all values for all occurrences of left (this will yield something like leftish). Perhaps surprisingly, representation through the majority value, i.e., choosing the most frequent value as representative for a feature (for a given word), performed better, and is hence the method we chose. For b), dimensionality reduction, we again chose a very simple approach (much simpler than for example (Roy, 2002)): features are filtered out as irrelevant for a given lemma features if their variance is above a certain threshold. To give an example, for the lemma left the distribution of values of the feature x distance to center varies with a Q of 0.05, that of y distance to center with a Q of 0.41. We empirically determined the setting of the threshold such that it excluded the latter.3 3.3 Combination Figure 3: Steps of the Algorithm for Example Utterance The combination algorithm works through the segmented utterance and combines visual word semantics to yield a reference hypothesis. Figure 3 illust</context>
<context position="12823" citStr="Roy, 2002" startWordPosition="2148" endWordPosition="2149">average-based). There were on average 7 objects in each scene in the corpus; i.e. the baseline of getting the reference right by chance is 14%. Our system significantly improves over this baseline. We also evaluated the system in a more realistic application situation. We asked subjects to refer to certain pieces in presented scenes (via typed utterances); here, the system reached a success-rate of 66% (7 subjects, 100 scene / utterance pairs). While this is considerably lower than the corpusbased evaluation, it is still on a par with related systems using more complicated resolution methods (Roy, 2002; Gorniak and Roy, 2004). We also think these results represent the lower end of the performance range that can be expected in practical use, as in an interactive dialogue system users have time to adapt to the capabilities of the system. 5 Conclusions We have presented a method for resolving definite, exophoric reference to objects that are visually co-present to user and system. The method combines automatically acquired models (a ‘visual word semantics’, a simple, but effective mapping between visual features and words; and a ‘visual grammar’, a semantically motivated segmentation of uttera</context>
</contexts>
<marker>Roy, 2002</marker>
<rawString>Deb Roy. 2002. Learning words and syntax for a visual description task. Computer Speech and Language, 16(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Raquel Fern´andez</author>
</authors>
<title>Beyond repair: Testing the limits of the conversational repair system.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGdial 2007,</booktitle>
<pages>51--54</pages>
<location>Antwerp, Belgium,</location>
<marker>Schlangen, Fern´andez, 2007</marker>
<rawString>David Schlangen and Raquel Fern´andez. 2007. Beyond repair: Testing the limits of the conversational repair system. In Proceedings of SIGdial 2007, pages 51– 54, Antwerp, Belgium, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linda G Shapiro</author>
<author>George C Stockman</author>
</authors>
<title>Computer Vision.</title>
<date>2001</date>
<publisher>Prentice Hall,</publisher>
<location>New Jersey, USA.</location>
<contexts>
<context position="5120" citStr="Shapiro and Stockman, 2001" startWordPosition="825" endWordPosition="829">the objects within the group (e.g., distance to the center of the group), and one for the configuration of groups (distance of group to other objects). This set of features was selected to be representative of typical basic visual features. 3 Components 3.1 Visual Grammar The ‘visual grammar’ segments utterances according to functional aspects on two levels. The first 2At the moment, the input to the algorithm is a symbolic representation of the scene (which object is where); the features are designed to also be derivable from digital images instead, using standard computer vision techniques (Shapiro and Stockman, 2001); this is future work, however. Figure 2: Scene with Horizontal Group Detection describes the macro-structure of a spatial expression, i.e., the division into target (the denoted object; T) and optional landmarks (other objects; LM) and their relation to the target (R; see example in Table 2). The second level annotates the spatial-lexical function of each word, e.g., whether the word denotes a piece or a configuration of pieces (Table 1). A fully ‘parsed’ example is shown in Table 2. the cross from the second column from left at the top l r d n p g r d r d r (a) - Annotation of spatial lexica</context>
</contexts>
<marker>Shapiro, Stockman, 2001</marker>
<rawString>Linda G. Shapiro and George C. Stockman. 2001. Computer Vision. Prentice Hall, New Jersey, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Siebert</author>
<author>David Schlangen</author>
<author>Raquel Fern´andez</author>
</authors>
<title>An implemented method for distributed collection and assessment of speech data.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGdial 2007,</booktitle>
<location>Antwerp, Belgium.</location>
<marker>Siebert, Schlangen, Fern´andez, 2007</marker>
<rawString>Alexander Siebert, David Schlangen, and Raquel Fern´andez. 2007. An implemented method for distributed collection and assessment of speech data. In Proceedings of SIGdial 2007, Antwerp, Belgium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>