<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.876709">
Adaptive Parser-Centric Text Normalization
</title>
<author confidence="0.978177">
Congle Zhang∗
</author>
<affiliation confidence="0.998923">
Dept of Computer Science and Engineering
University of Washington, Seattle, WA 98195, USA
</affiliation>
<email confidence="0.983029">
clzhang@cs.washington.edu
</email>
<author confidence="0.659568">
Tyler Baldwin Howard Ho Benny Kimelfeld Yunyao Li
</author>
<affiliation confidence="0.608569">
IBM Research - Almaden
</affiliation>
<address confidence="0.996593">
650 Harry Road, San Jose, CA 95120, USA
</address>
<email confidence="0.998825">
Itbaldwi,ctho,kimelfeld,yunyaolil@us.ibm.com
</email>
<sectionHeader confidence="0.994779" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999798214285714">
Text normalization is an important first
step towards enabling many Natural Lan-
guage Processing (NLP) tasks over infor-
mal text. While many of these tasks, such
as parsing, perform the best over fully
grammatically correct text, most existing
text normalization approaches narrowly
define the task in the word-to-word sense;
that is, the task is seen as that of mapping
all out-of-vocabulary non-standard words
to their in-vocabulary standard forms. In
this paper, we take a parser-centric view
of normalization that aims to convert raw
informal text into grammatically correct
text. To understand the real effect of nor-
malization on the parser, we tie normal-
ization performance directly to parser per-
formance. Additionally, we design a cus-
tomizable framework to address the often
overlooked concept of domain adaptabil-
ity, and illustrate that the system allows for
transfer to new domains with a minimal
amount of data and effort. Our experimen-
tal study over datasets from three domains
demonstrates that our approach outper-
forms not only the state-of-the-art word-
to-word normalization techniques, but also
manual word-to-word annotations.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997871224489796">
Text normalization is the task of transforming in-
formal writing into its standard form in the lan-
guage. It is an important processing step for a
wide range of Natural Language Processing (NLP)
tasks such as text-to-speech synthesis, speech
recognition, information extraction, parsing, and
machine translation (Sproat et al., 2001).
∗This work was conducted at IBM.
The use of normalization in these applications
poses multiple challenges. First, as it is most often
conceptualized, normalization is seen as the task
of mapping all out-of-vocabulary non-standard
word tokens to their in-vocabulary standard forms.
However, the scope of the task can also be seen as
much wider, encompassing whatever actions are
required to convert the raw text into a fully gram-
matical sentence. This broader definition of the
normalization task may include modifying punc-
tuation and capitalization, and adding, removing,
or reordering words. Second, as with other NLP
techniques, normalization approaches are often fo-
cused on one primary domain of interest (e.g.,
Twitter data). Because the style of informal writ-
ing may be different in different data sources,
tailoring an approach towards a particular data
source can improve performance in the desired do-
main. However, this is often done at the cost of
adaptability.
This work introduces a customizable normal-
ization approach designed with domain transfer in
mind. In short, customization is done by provid-
ing the normalizer with replacement generators,
which we define in Section 3. We show that the
introduction of a small set of domain-specific gen-
erators and training data allows our model to out-
perform a set of competitive baselines, including
state-of-the-art word-to-word normalization. Ad-
ditionally, the flexibility of the model also allows it
to attempt to produce fully grammatical sentences,
something not typically handled by word-to-word
normalization approaches.
Another potential problem with state-of-the-art
normalization is the lack of appropriate evaluation
metrics. The normalization task is most frequently
motivated by pointing to the need for clean text
for downstream processing applications, such as
syntactic parsing. However, most studies of nor-
malization give little insight into whether and to
what degree the normalization process improves
</bodyText>
<page confidence="0.969232">
1159
</page>
<note confidence="0.913726">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159–1168,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999807833333333">
the performance of the downstream application.
For instance, it is unclear how performance mea-
sured by the typical normalization evaluation met-
rics of word error rate and BLEU score (Pap-
ineni et al., 2002) translates into performance on
a parsing task, where a well placed punctuation
mark may provide more substantial improvements
than changing a non-standard word form. To ad-
dress this problem, this work introduces an eval-
uation metric that ties normalization performance
directly to the performance of a downstream de-
pendency parser.
The rest of this paper is organized as follows.
In Section 2 we discuss previous approaches to
the normalization problem. Section 3 presents
our normalization framework, including the actual
normalization and learning procedures. Our in-
stantiation of this model is presented in Section 4.
In Section 5 we introduce the parser driven eval-
uation metric, and present experimental results of
our model with respect to several baselines in three
different domains. Finally, we discuss our exper-
imental study in Section 6 and conclude in Sec-
tion 7.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999946138461538">
Sproat et al. (2001) took the first major look at
the normalization problem, citing the need for nor-
malized text for downstream applications. Unlike
later works that would primarily focus on specific
noisy data sets, their work is notable for attempt-
ing to develop normalization as a general process
that could be applied to different domains. The re-
cent rise of heavily informal writing styles such as
Twitter and SMS messages set off a new round of
interest in the normalization problem.
Research on SMS and Twitter normalization has
been roughly categorized as drawing inspiration
from three other areas of NLP (Kobus et al., 2008):
machine translation, spell checking, and automatic
speech recognition. The statistical machine trans-
lation (SMT) metaphor was the first proposed to
handle the text normalization problem (Aw et al.,
2006). In this mindset, normalizing SMS can be
seen as a translation task from a source language
(informal) to a target language (formal), which can
be undertaken with typical noisy channel based
models. Work by Choudhury et al. (2007) adopted
the spell checking metaphor, casting the problem
in terms of character-level, rather than word-level,
edits. They proposed an HMM based model that
takes into account both grapheme and phoneme
information. Kobus et al. (2008) undertook a
hybrid approach that pulls inspiration from both
the machine translation and speech recognition
metaphors.
Many other approaches have been examined,
most of which are at least partially reliant on
the above three metaphors. Cook and Steven-
son (2009) perform an unsupervised method,
again based on the noisy channel model. Pen-
nell and Liu (2011) developed a CRF tagger for
deletion-based abbreviation on tweets. Xue et
al. (2011) incorporated orthographic, phonetic,
contextual, and acronym expansion factors to nor-
malize words in both Twitter and SMS. Liu et
al. (2011) modeled the generation process from
dictionary words to non-standard tokens under an
unsupervised sequence labeling framework. Han
and Baldwin (2011) use a classifier to detect ill-
formed words, and then generate correction can-
didates based on morphophonemic similarity. Re-
cent work has looked at the construction of nor-
malization dictionaries (Han et al., 2012) and on
improving coverage by integrating different hu-
man perspectives (Liu et al., 2012).
Although it is almost universally used as a mo-
tivating factor, most normalization work does not
directly focus on improving downstream appli-
cations. While a few notable exceptions high-
light the need for normalization as part of text-
to-speech systems (Beaufort et al., 2010; Pennell
and Liu, 2010), these works do not give any di-
rect insight into how much the normalization pro-
cess actually improves the performance of these
systems. To our knowledge, the work presented
here is the first to clearly link the output of a nor-
malization system to the output of the downstream
application. Similarly, our work is the first to pri-
oritize domain adaptation during the new wave of
text message normalization.
</bodyText>
<sectionHeader confidence="0.988979" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.99991225">
In this section we introduce our normalization
framework, which draws inspiration from our pre-
vious work on spelling correction for search (Bao
et al., 2011).
</bodyText>
<subsectionHeader confidence="0.999365">
3.1 Replacement Generators
</subsectionHeader>
<bodyText confidence="0.560938666666667">
Our input the original, unnormalized text, repre-
sented as a sequence x = x1, x2, ... , xn of tokens
xi. In this section we will use the following se-
</bodyText>
<page confidence="0.987869">
1160
</page>
<bodyText confidence="0.995091">
quence as our running example:
</bodyText>
<equation confidence="0.94842">
x = Ay1 woudent2 of3 see4&apos;em5
</equation>
<bodyText confidence="0.997889833333333">
where space replaces comma for readability, and
each token is subscripted by its position. Given the
input x, we apply a series of replacement genera-
tors, where a replacement generator is a function
that takes x as input and produces a collection of
replacements. Here, a replacement is a statement
of the form “replace tokens xi, ... , xj−1 with s.”
More precisely, a replacement is a triple (i, j, s),
where 1 G i G j G n + 1 and s is a sequence of
tokens. Note that in the case where i = j, the se-
quence s should be inserted right before xi; and in
the special case where s is empty, we simply delete
xi, ... , xj−1. For instance, in our running exam-
ple the replacement (2, 3, would not) replaces
x2 = woudent with would not; (1, 2, Ay) re-
places x1 with itself (hence, does not change x);
(1, 2, E) (where E is the empty sequence) deletes
x1; (6, 6, .) inserts a period at the end of the se-
quence.
The provided replacement generators can be ei-
ther generic (cross domain) or domain-specific, al-
lowing for domain customization. In Section 4,
we discuss the replacement generators used in our
empirical study.
</bodyText>
<subsectionHeader confidence="0.999711">
3.2 Normalization Graph
</subsectionHeader>
<bodyText confidence="0.998959">
Given the input x and the set of replacements pro-
duced by our generators, we associate a unique
Boolean variable Xr with each replacement r. As
expected, Xr being true means that the replace-
ment r takes place in producing the output se-
quence.
Next, we introduce dependencies among vari-
ables. We first discuss the syntactic consistency
of truth assignments. Let r1 = (i1, j1, s1) and
r2 = (i2, j2, s2) be two replacements. We say
that r1 and r2 are locally consistent if the inter-
vals [i1, j1) and [i2, j2) are disjoint. Moreover,
we do not allow two insertions to take place at
the same position; therefore, we exclude [i1, j1)
and [i2, j2) from the definition of local consistency
when i1 = j1 = i2 = j2. If r1 and r2 are locally
consistent and j1 = i2, then we say that r2 is a
consistent follower of r1.
A truth assignment α to our variables Xr is
sound if every two replacements r and r&apos; with
α(Xr) = α(Xr,) = true are locally consis-
tent. We say that α is complete if every token
of x is captured by at least one replacement r
with α(Xr) = true. Finally, we say that α
is legal if it is sound and complete. The out-
put (normalized sequence) defined by a legal as-
signment α is, naturally, the concatenation (from
left to right) of the strings s in the replacements
r = (i, j, s) with α(Xr) = true. In Fig-
ure 1, for example, if the nodes with a grey
shade are the ones associated with true vari-
ables under α, then the output defined by α is
T would not have seen them.
Our variables carry two types of interdependen-
cies. The first is that of syntactic consistency: the
entire assignment is required to be legal. The sec-
ond captures correlation among replacements. For
instance, if we replace of with have in our run-
ning example, then the next see token is more
likely to be replaced with seen. In this work,
dependencies of the second type are restricted to
pairs of variables, where each pair corresponds to
a replacement and a consistent follower thereof.
The above dependencies can be modeled over a
standard undirected graph using Conditional Ran-
dom Fields (Lafferty et al., 2001). However, the
graph would be complex: in order to model lo-
cal consistency, there should be edges between ev-
ery two nodes that violate local consistency. Such
a model renders inference and learning infeasi-
ble. Therefore, we propose a clearer model by a
directed graph, as illustrated in Figure 1 (where
nodes are represented by replacements r instead
of the variables Xr, for readability). To incorpo-
rate correlation among replacements, we introduce
an edge from Xr to Xr, whenever r&apos; is a consis-
tent follower of r. Moreover, we introduce two
dummy nodes, start and end, with an edge from
start to each variable that corresponds to a prefix
of the input sequence x, and an edge from each
variable that corresponds to a suffix of x to end.
The principal advantage of modeling the depen-
dencies in such a directed graph is that now, the le-
gal assignments are in one-to-one correspondence
with the paths from start to end; this is a straight-
forward observation that we do not prove here.
We appeal to the log-linear model formulation
to define the probability of an assignment. The
conditional probability of an assignment α, given
an input sequence x and the weight vector O =
(01, ... , 0k) for our features, is defined as p(α �
</bodyText>
<page confidence="0.992824">
1161
</page>
<figureCaption confidence="0.990803">
Figure 1: Example of a normalization graph; the
nodes are replacements generated by the replace-
ment generators, and every path from start to end
implies a legal assignment
</figureCaption>
<equation confidence="0.977471333333333">
x, O) = 0 if α is not legal, and otherwise,
1 p(α  |x, O) = Z(x) fj exp( θjφj( ))
X,Y,x
</equation>
<bodyText confidence="0.996750666666667">
Here, Z(x) is the partition function, X -+ Y E α
refers to an edge X -+ Y with α(X) = true and
α(Y ) = true, and φ1(X, Y, x), ... , φk(X, Y, x)
are real valued feature functions that are weighted
by θ1, ... , θk (the model’s parameters), respec-
tively.
</bodyText>
<subsectionHeader confidence="0.934336">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.995777043478261">
When performing inference, we wish to select
the output sequence with the highest probability,
given the input sequence x and the weight vector
O (i.e., MAP inference). Specifically, we want an
assignment α* = arg maxα p(α  |x, O).
While exact inference is computationally hard
on general graph models, in our model it boils
down to finding the longest path in a weighted
and acyclic directed graph. Indeed, our directed
graph (illustrated in Figure 1) is acyclic. We as-
sign the real value Ej θjφj(X,Y,x) to the edge
X -+ Y , as the weight. As stated in Section 3.2,
a legal assignment α corresponds to a path from
start to end; moreover, the sum of the weights on
that path is equal to log p(α  |x, O) + log Z(x).
In particular, a longer path corresponds to an as-
signment with greater probability. Therefore, we
can solve the MAP inference within our model by
finding the weighted longest path in the directed
acyclic graph. The algorithm in Figure 2 summa-
rizes the inference procedure to normalize the in-
put sequence x.
Input:
</bodyText>
<listItem confidence="0.910320388888889">
1. A sequence x to normalize;
2. A weight vector O = (θ1, ... , θk).
Generate replacements: Apply all replace-
ment generators to get a set of replacements r,
each r is a triple (i, j, s).
Build a normalization graph:
1. For each replacement r, create a node Xr.
2. For each r&apos; and r, create an edge Xr to
Xr, if r&apos; is a consistent follower of r.
3. Create two dummy nodes start and end,
and create edges from start to all prefix
nodes and end to all suffix nodes.
4. For each edge X -+ Y , compute the fea-
tures φj (((X, Y, x), and weight the edge by
Ej θjφj(X,Y,x).
MAP Inference: Find a weighted longest path
P from start to end, and return α*, where
α*(Xr) = true iff Xr E P.
</listItem>
<figureCaption confidence="0.987284">
Figure 2: Normalization algorithm
</figureCaption>
<subsectionHeader confidence="0.986097">
3.4 Learning
</subsectionHeader>
<bodyText confidence="0.928334888888889">
Our labeled data consists of pairs (xi, ygold i),
where xi is an input sequence (to normalize) and
yi is a (manually) normalized sequence. We
gold
obtain a truth assignment αgold ifrom each ygold
i
by selecting an assignment α that minimizes the
edit distance between ygold iand the normalized
text implied by α:
</bodyText>
<equation confidence="0.946935333333333">
αgold i= arg minDIST(y(α),ygold
i ) (1)
α
</equation>
<bodyText confidence="0.998938">
Here, y(α) denotes the normalized text implied by
α, and DIST is a token-level edit distance. We
apply a simple dynamic-programming algorithm
to compute αgold i. Finally, the items in our training
data are the pairs (xi, αgold
</bodyText>
<equation confidence="0.480513">
i ).
</equation>
<bodyText confidence="0.9813425">
Learning over similar models is commonly
done via maximum likelihood estimation:
</bodyText>
<equation confidence="0.9408075">
L(O) = log fj p(αi = αgold i |xi, O)
i
</equation>
<bodyText confidence="0.633134">
Taking the partial derivative gives the following:
</bodyText>
<equation confidence="0.9774265">
� �
gold
Φj (αi , xi) − Ep(αi Xi,Θ)Φj(αi, xi)
i
</equation>
<bodyText confidence="0.992378">
where Φj(α, x) = EX-+Y φj(X, Y, x), that is,
the sum of values for the jth feature along the
</bodyText>
<table confidence="0.95395025">
h1, 2, Ayi
h3, 4, ofi
h4, 5, seeni
h1, 2, Ii
start
h2, 4, would not havei
h2, 3, wouldi
h4, 6, see himi
end
h5, 6, themi
h6, 6, .i
X-+Y Eα j
</table>
<page confidence="0.830885">
1162
</page>
<figure confidence="0.684047357142857">
Input:
1. A set {(xi, ygold
i )}n i=1 of sequences and
their gold normalization;
2. Number T of iterations.
Initialization: Initialize each θj as zero, and
ld
obtain each αioaccording to (1).
Repeat T times:
1. Infer each α∗i from xi using the current O;
2. θj ← θj+Ei(Φj(αgold
i , xi)−Φj(α∗i ,xi))
for all j = 1, ... , k.
Output: O = (θ1, ... , θk)
</figure>
<figureCaption confidence="0.999719">
Figure 3: Learning algorithm
</figureCaption>
<bodyText confidence="0.99097725">
path defined by α, and Ep(αi|xi,Θ)Φj(αi, xi) is the
expected value of that sum (over all legal assign-
ments αi), assuming the current weight vector.
How to efficiently compute
Ep(αi|xi,Θ)Φj(αi,xi) in our model is un-
clear; naively, it requires enumerating all legal
assignments. We instead opt to use a more
tractable perceptron-style algorithm (Collins,
2002). Instead of computing the expectation,
we simply compute Φj(α∗i , xi), where α∗i is the
assignment with the highest probability, generated
using the current weight vector. The result is then:
� Φj(αgold
i , xi) − Φj(α∗ i , xi)
Our learning applies the following two steps it-
eratively. (1) Generate the most probable sequence
within the current weights. (2) Update the weights
by comparing the path generated in the previous
step to the gold standard path. The algorithm in
Figure 3 summarizes the procedure.
</bodyText>
<sectionHeader confidence="0.999405" genericHeader="method">
4 Instantiation
</sectionHeader>
<bodyText confidence="0.99983425">
In this section, we discuss our instantiation of the
model presented in the previous section. In partic-
ular, we describe our replacement generators and
features.
</bodyText>
<subsectionHeader confidence="0.997359">
4.1 Replacement Generators
</subsectionHeader>
<bodyText confidence="0.999993333333333">
One advantage of our proposed model is that
the reliance on replacement generators allows for
strong flexibility. Each generator can be seen as a
black box, allowing replacements that are created
heuristically, statistically, or by external tools to be
incorporated within the same framework.
</bodyText>
<table confidence="0.999111545454546">
Generator From To
leave intact good good
edit distance bac back
lowercase NEED need
capitalize it It
Google spell disspaear disappear
contraction wouldn’t would not
slang language ima I am going to
insert punctuation e .
duplicated punctuation !? !
delete filler lmao e
</table>
<tableCaption confidence="0.999732">
Table 1: Example replacement generators
</tableCaption>
<bodyText confidence="0.999672222222222">
To build a set of generic replacement generators
suitable for normalizing a variety of data types, we
collected a set of about 400 Twitter posts as devel-
opment data. Using that data, a series of gener-
ators were created; a sample of them are shown
in Table 1. As shown in the table, these gener-
ators cover a variety of normalization behavior,
from changing non-standard word forms to insert-
ing and deleting tokens.
</bodyText>
<subsectionHeader confidence="0.803084">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.973249322580645">
Although the proposed framework supports real
valued features, all features in our system are bi-
nary. In total, we used 70 features. Our feature set
pulls information from several different sources:
N-gram: Our n-gram features indicate the fre-
quency of the phrases induced by an edge. These
features are turned into binary ones by bucketing
their log values. For example, on the edge from
(1, 2, T) to (2, 3, would) such a feature will indi-
cate whether the frequency of T would is over
a threshold. We use the Corpus of Contemporary
English (Davies, 2008 ) to produce our n-gram in-
formation.
Part-of-speech: Part-of-speech information
can be used to produce features that encourage
certain behavior, such as avoiding the deletion of
noun phrases. We generate part-of-speech infor-
mation over the original raw text using a Twit-
ter part-of-speech tagger (Ritter et al., 2011). Of
course, the part-of-speech information obtained
this way is likely to be noisy, and we expect our
learning algorithm to take that into account.
Positional: Information from positions is used
primarily to handle capitalization and punctuation
insertion, for example, by incorporating features
for capitalized words after stop punctuation or the
insertion of stop punctuation at the end of the sen-
tence.
Lineage: Finally, we include binary features
�
i
</bodyText>
<page confidence="0.854277">
1163
</page>
<bodyText confidence="0.957137">
that indicate which generator spawned the replace-
ment.
</bodyText>
<sectionHeader confidence="0.993776" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999969833333333">
In this section, we present an empirical study of
our framework. The study is done over datasets
from three different domains. The goal is to eval-
uate the framework in two aspects: (1) usefulness
for downstream applications (specifically depen-
dency parsing), and (2) domain adaptability.
</bodyText>
<subsectionHeader confidence="0.987133">
5.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.956635880952381">
A few different metrics have been used to evaluate
normalizer performance, including word error rate
and BLEU score. While each metric has its pros
and cons, they all rely on word-to-word matching
and treat each word equally. In this work, we aim
to evaluate the performance of a normalizer based
on how it affects the performance of downstream
applications. We find that the conventional metrics
are not directly applicable, for several reasons. To
begin with, the assumption that words have equal
weights is unlikely to hold. Additionally, these
metrics tend to ignore other important non-word
information such as punctuation or capitalization.
They also cannot take into account other aspects
that may have an impact on downstream perfor-
mance, such as the word reordering as seen in the
example in Figure 4. Therefore, we propose a new
evaluation metric that directly equates normaliza-
tion performance with the performance of a com-
mon downstream application—dependency pars-
ing.
To realize our desired metric, we apply the fol-
lowing procedure. First, we produce gold standard
normalized data by manually normalizing sen-
tences to their full grammatically correct form. In
addition to the word-to-word mapping performed
in typical normalization gold standard generation,
this annotation procedure includes all actions nec-
essary to make the sentence grammatical, such as
word reordering, modifying capitalization, and re-
moving emoticons. We then run an off-the-shelf
dependency parser on the gold standard normal-
ized data to produce our gold standard parses. Al-
though the parser could still produce mistakes on
the grammatical sentences, we feel that this pro-
vides a realistic benchmark for comparison, as it
represents an upper bound on the possible perfor-
mance of the parser, and avoids an expensive sec-
ond round of manual annotation.
Test Gold SVO
I kinda wanna get I kind of want to
ipad NEW get a new iPad.
</bodyText>
<equation confidence="0.930299166666667">
verb(get) verb(want) precisionv = 1
verb(get) recallv = 21
1
subj(getj) subj(want,I) precision.. = 3
subj(get,wanna) subj(getj) recallso = 1
obj(get,NEW) obj(get,iPad) 3
</equation>
<figureCaption confidence="0.893518333333333">
Figure 4: The subjects, verbs, and objects identi-
fied on example test/gold text, and corresponding
metric scores
</figureCaption>
<bodyText confidence="0.9996824">
To compare the parses produced over automati-
cally normalized data to the gold standard, we look
at the subjects, verbs, and objects (SVO) identi-
fied in each parse. The metric shown in Equa-
tions (2) and (3) below is based on the identified
subjects and objects in those parses. Note that SO
denotes the set of identified subjects and objects
whereas SOgold denotes the set of subjects and
objects identified when parsing the gold-standard
normalization.
</bodyText>
<equation confidence="0.9904728">
|SO ∩ SOgold|
precisionso = (2)
|SO|
recallso = |SO ∩ SOgold |(3)
|SOgold|
</equation>
<bodyText confidence="0.99999625">
We similarly define precisionv and recallv, where
we compare the set V of identified verbs to Vgold
of those found in the gold-standard normalization.
An example is shown in Figure 4.
</bodyText>
<subsectionHeader confidence="0.835656">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999985625">
To establish the extensibility of our normaliza-
tion system, we present results in three different
domains: Twitter posts, Short Message Service
(SMS) messages, and call-center logs. For Twitter
and SMS messages, we used established datasets
to compare with previous work. As no estab-
lished call-center log dataset exists, we collected
our own. In each case, we ran the proposed system
with two different configurations: one using only
the generic replacement generators presented in
Section 4 (denoted as generic), and one that adds
additional domain-specific generators for the cor-
responding domain (denoted as domain-specific).
All runs use ten-fold cross validation for training
and evaluation. The Stanford parser1 (Marneffe
et al., 2006) was used to produce all dependency
</bodyText>
<footnote confidence="0.990089">
1Version 2.0.4, http://nlp.stanford.edu/
software/lex-parser.shtml
</footnote>
<page confidence="0.995922">
1164
</page>
<bodyText confidence="0.962648">
parses. We compare our system to the following
baseline solutions:
</bodyText>
<listItem confidence="0.623052">
w/oN: No normalization is performed.
Google: Output of the Google spell checker.
</listItem>
<bodyText confidence="0.9778875">
w2wN: The output of the word-to-word normal-
ization of Han and Baldwin (2011). Not available
for call-center data.
Gw2wN: The manual gold standard word-to-
word normalizations of previous work (Choud-
hury et al., 2007; Han and Baldwin, 2011). Not
available for call-center data.
Our results use the metrics of Section 5.1.
</bodyText>
<sectionHeader confidence="0.372125" genericHeader="method">
5.2.1 Twitter
</sectionHeader>
<bodyText confidence="0.99988496875">
To evaluate the performance on Twitter data, we
use the dataset of randomly sampled tweets pro-
duced by (Han and Baldwin, 2011). Because the
gold standard used in this work only provided
word mappings for out-of-vocabulary words and
did not enforce grammaticality, we reannotated the
gold standard data2. Their original gold standard
annotations were kept as a baseline.
To produce Twitter-specific generators, we ex-
amined the Twitter development data collected for
generic generator production (Section 4). These
generators focused on the Twitter-specific notions
of hashtags (#), ats (@), and retweets (RT). For
each case, we implemented generators that al-
lowed for either the initial symbol or the entire to-
ken to be deleted (e.g., @Hertz to Hertz, @Hertz
to E).
The results are given in Table 2. As shown,
the domain-specific generators yielded perfor-
mance significantly above the generic ones and all
baselines. Even without domain-specific genera-
tors, our system outperformed the word-to-word
normalization approaches. Most notably, both
the generic and domain-specific systems outper-
formed the gold standard word-to-word normal-
izations. These results validate the hypothesis that
simple word-to-word normalization is insufficient
if the goal of normalization is to improve depen-
dency parsing; even if a system could produce
perfect word-to-word normalization, it would pro-
duce lower quality parses than those produced by
our approach.
</bodyText>
<footnote confidence="0.997925333333333">
2Our results and the reannotations of the Twitter and SMS
data are available at https://www.cs.washington.
edu/node/9091/
</footnote>
<table confidence="0.99970575">
System Verb Subject-Object
Pre Rec F1 Pre Rec F1
w/oN 83.7 68.1 75.1 31.7 38.6 34.8
Google 88.9 78.8 83.5 36.1 46.3 40.6
w2wN 87.5 81.5 84.4 44.5 58.9 50.7
Gw2w 89.8 83.8 86.7 46.9 61.0 53.0
generic 91.7 88.9 90.3 53.6 70.2 60.8
domain specific 95.3 88.7 91.9 72.5 76.3 74.4
</table>
<tableCaption confidence="0.999055">
Table 2: Performance on Twitter dataset
</tableCaption>
<subsectionHeader confidence="0.517059">
5.2.2 SMS
</subsectionHeader>
<bodyText confidence="0.999932775">
To evaluate the performance on SMS data, we use
the Treasure My Text data collected by Choud-
hury et al. (2007). As with the Twitter data, the
word-to-word normalizations were reannotated to
enforce grammaticality. As a replacement genera-
tor for SMS-specific substitutions, we used a map-
ping dictionary of SMS abbreviations.3 No further
SMS-specific development data was needed.
Table 3 gives the results on the SMS data. The
SMS dataset proved to be more difficult than the
Twitter dataset, with the overall performance of
every system being lower. While this drop of per-
formance may be a reflection of the difference in
data styles between SMS and Twitter, it is also
likely a product of the collection methodology.
The collection methodology of the Treasure My
Text dataset dictated that every message must have
at least one mistake, which may have resulted in a
dataset that was noisier than average.
Nonetheless, the trends on SMS data mirror
those on Twitter data, with the domain-specific
generators achieving the greatest overall perfor-
mance. However, while the generic setting still
manages to outperform most baselines, it did not
outperform the gold word-to-word normalization.
In fact, the gold word-to-word normalization was
much more competitive on this data, outperform-
ing even the domain-specific system on verbs
alone. This should not be seen as surprising, as
word-to-word normalization is most likely to be
beneficial for cases like this where the proportion
of non-standard tokens is high.
It should be noted that the SMS dataset as avail-
able has had all punctuation removed. While this
may be appropriate for word-to-word normaliza-
tion, this preprocessing may have an effect on the
parse of the sentence. As our system has the abil-
ity to add punctuation but our baseline systems do
not, this has the potential to artificially inflate our
results. To ensure a fair comparison, we manually
</bodyText>
<footnote confidence="0.960357">
3http://www.netlingo.com/acronyms.php
</footnote>
<page confidence="0.951162">
1165
</page>
<table confidence="0.99981175">
System Verb Subject-Object
Rec Pre F1 Rec Pre F1
w/oN 76.4 48.1 59.0 19.5 21.5 20.4
Google 85.1 61.6 71.5 22.4 26.2 24.1
w2wN 78.5 61.5 68.9 29.9 36.0 32.6
Gw2wN 87.6 76.6 81.8 38.0 50.6 43.4
generic 86.5 77.4 81.7 35.5 47.7 40.7
domain specific 88.1 75.0 81.0 41.0 49.5 44.8
</table>
<tableCaption confidence="0.997673">
Table 3: Performance on SMS dataset
</tableCaption>
<table confidence="0.999854833333333">
System Verb Subject-Object
Pre Rec F1 Pre Rec F1
w/oN 98.5 97.1 97.8 69.2 66.1 67.6
Google 99.2 97.9 98.5 70.5 67.3 68.8
generic 98.9 97.4 98.1 71.3 67.9 69.6
domain specific 99.2 97.4 98.3 87.9 83.1 85.4
</table>
<tableCaption confidence="0.999795">
Table 4: Performance on call-center dataset
</tableCaption>
<bodyText confidence="0.9998055">
added punctuation to a randomly selected small
subset of the SMS data and reran each system.
This experiment suggested that, in contrast to the
hypothesis, adding punctuation actually improved
the results of the proposed system more substan-
tially than that of the baseline systems.
</bodyText>
<subsectionHeader confidence="0.827652">
5.2.3 Call-Center
</subsectionHeader>
<bodyText confidence="0.999968565217391">
Although Twitter and SMS data are unmistakably
different, there are many similarities between the
two, such as the frequent use of shorthand word
forms that omit letters. The examination of call-
center logs allows us to examine the ability of our
system to perform normalization in more disparate
domains. Our call-center data consists of text-
based responses to questions about a user’s expe-
rience with a call-center (e.g., their overall satis-
faction with the service). We use call-center logs
from a major company, and collect about 150 re-
sponses for use in our evaluation. We collected
an additional small set of data to develop our call-
center-specific generators.
Results on the call-center dataset are in Table 4.
As shown, the raw call-center data was compar-
atively clean, resulting in higher baseline perfor-
mance than in other domains. Unlike on previ-
ous datasets, the use of generic mappings only
provided a small improvement over the baseline.
However, the use of domain-specific generators
once again led to significantly increased perfor-
mance on subjects and objects.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998444113207548">
The results presented in the previous section sug-
gest that domain transfer using the proposed nor-
malization framework is possible with only a
small amount of effort. The relatively modest
set of additional replacement generators included
in each data set allowed the domain-specific ap-
proaches to significantly outperform the generic
approach. In the call-center case, performance im-
provements could be seen by referencing a very
small amount of development data. In the SMS
case, the presence of a domain-specific dictionary
allowed for performance improvements without
the need for any development data at all. It is
likely, though not established, that employing fur-
ther development data would result in further per-
formance improvements. We leave further investi-
gation to future work.
The results in Section 5.2 establish a point that
has often been assumed but, to the best of our
knowledge, has never been explicitly shown: per-
forming normalization is indeed beneficial to de-
pendency parsing on informal text. The parse of
the normalized text was substantially better than
the parse of the original raw text in all domains,
with absolute performance increases ranging from
about 18-25% on subjects and objects. Further-
more, the results suggest that, as hypothesized,
preparing an informal text for a parsing task re-
quires more than simple word-to-word normaliza-
tion. The proposed approach significantly outper-
forms the state-of-the-art word-to-word normal-
ization approach. Perhaps most interestingly, the
proposed approach performs on par with, and in
several cases superior to, gold standard word-to-
word annotations. This result gives strong evi-
dence for the conclusion that parser-targeted nor-
malization requires a broader understanding of the
scope of the normalization task.
While the work presented here gives promis-
ing results, there are still many behaviors found
in informal text that prove challenging. One
such example is the word reordering seen in Fig-
ure 4. Although word reordering could be incor-
porated into the model as a combination of a dele-
tion and an insertion, the model as currently de-
vised cannot easily link these two replacements
to one another. Additionally, instances of re-
ordering proved hard to detect in practice. As
such, no reordering-based replacement generators
were implemented in the presented system. An-
other case that proved difficult was the insertion
of missing tokens. For instance, the informal
sentence “Day 3 still don’t freaking
</bodyText>
<page confidence="0.97809">
1166
</page>
<bodyText confidence="0.9998615">
feel good!:(” could be formally rendered
as “It is day 3 and I still do not
feel good!”. Attempts to address missing to-
kens in the model resulted in frequent false pos-
itives. Similarly, punctuation insertion proved to
be challenging, often requiring a deep analysis
of the sentence. For example, contrast the sen-
tence “I’m watching a movie I don’t
know its name.” which would benefit from
inserted punctuation, with “I’m watching a
movie I don’t know.”, which would not.
We feel that the work presented here provides a
foundation for future work to more closely exam-
ine these challenges.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999703125">
This work presents a framework for normalization
with an eye towards domain adaptation. The pro-
posed framework builds a statistical model over a
series of replacement generators. By doing so, it
allows a designer to quickly adapt a generic model
to a new domain with the inclusion of a small set of
domain-specific generators. Tests over three dif-
ferent domains suggest that, using this model, only
a small amount of domain-specific data is neces-
sary to tailor an approach towards a new domain.
Additionally, this work introduces a parser-
centric view of normalization, in which the per-
formance of the normalizer is directly tied to the
performance of a downstream dependency parser.
This evaluation metric allows for a deeper under-
standing of how certain normalization actions im-
pact the output of the parser. Using this met-
ric, this work established that, when dependency
parsing is the goal, typical word-to-word normal-
ization approaches are insufficient. By taking a
broader look at the normalization task, the ap-
proach presented here is able to outperform not
only state-of-the-art word-to-word normalization
approaches but also manual word-to-word annota-
tions.
Although the work presented here established
that more than word-to-word normalization was
necessary to produce parser-ready normalizations,
it remains unclear which specific normalization
tasks are most critical to parser performance. We
leave this interesting area of examination to future
work.
</bodyText>
<sectionHeader confidence="0.997136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974">
We thank the anonymous reviewers of ACL for
helpful comments and suggestions. We also thank
Ioana R. Stanoi for her comments on a prelim-
inary version of this work, Daniel S. Weld for
his support, and Alan Ritter, Monojit Choudhury,
Bo Han, and Fei Liu for sharing their tools and
data. The first author is partially supported by the
DARPA Machine Reading Program under AFRL
prime contract numbers FA8750-09-C-0181 and
FA8750-09-C-0179. Any opinions, findings, con-
clusions, or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA, AFRL, or the
US government. This work is a part of IBM’s Sys-
temT project (Chiticariu et al., 2010).
</bodyText>
<sectionHeader confidence="0.999203" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999431454545454">
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normal-
ization. In ACL, pages 33–40.
Zhuowei Bao, Benny Kimelfeld, and Yunyao Li. 2011.
A graph approach to spelling correction in domain-
centric search. In ACL, pages 905–914.
Richard Beaufort, Sophie Roekhaut, Louise-Am´elie
Cougnon, and C´edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing sms messages. In ACL, pages 770–779.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128–137.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157–174.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1–8.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC, pages 71–78.
Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In ACL, pages 368–378.
</reference>
<page confidence="0.876761">
1167
</page>
<reference confidence="0.999632813953488">
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In EMNLP-CoNLL, pages 421–432.
Catherine Kobus, Franc¸ois Yvon, and G´eraldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In COLING, pages 441–
448.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282–289.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor
supervision. In ACL, pages 71–76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. In ACL, pages 1035–1044.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449–454.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311–
318.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP, pages
4842–4845.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP, pages 974–982.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in Tweets: An ex-
perimental study. In EMNLP, pages 1524–1534.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech &amp; Language, 15(3):287–
333.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison.
2011. Normalizing microtext. In Analyzing Micro-
text, volume WS-11-05 of AAAI Workshops.
</reference>
<page confidence="0.995232">
1168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900851">
<title confidence="0.967523">Adaptive Parser-Centric Text Normalization</title>
<affiliation confidence="0.9708615">Dept of Computer Science and University of Washington, Seattle, WA 98195,</affiliation>
<email confidence="0.999158">clzhang@cs.washington.edu</email>
<author confidence="0.999178">Tyler Baldwin Howard Ho Benny Kimelfeld Yunyao</author>
<affiliation confidence="0.9998">IBM Research -</affiliation>
<address confidence="0.999801">650 Harry Road, San Jose, CA 95120,</address>
<abstract confidence="0.999662310344828">Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly the task in the that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normalization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for sms text normalization.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="5932" citStr="Aw et al., 2006" startWordPosition="892" endWordPosition="895">work is notable for attempting to develop normalization as a general process that could be applied to different domains. The recent rise of heavily informal writing styles such as Twitter and SMS messages set off a new round of interest in the normalization problem. Research on SMS and Twitter normalization has been roughly categorized as drawing inspiration from three other areas of NLP (Kobus et al., 2008): machine translation, spell checking, and automatic speech recognition. The statistical machine translation (SMT) metaphor was the first proposed to handle the text normalization problem (Aw et al., 2006). In this mindset, normalizing SMS can be seen as a translation task from a source language (informal) to a target language (formal), which can be undertaken with typical noisy channel based models. Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approa</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for sms text normalization. In ACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuowei Bao</author>
<author>Benny Kimelfeld</author>
<author>Yunyao Li</author>
</authors>
<title>A graph approach to spelling correction in domaincentric search.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>905--914</pages>
<contexts>
<context position="8297" citStr="Bao et al., 2011" startWordPosition="1267" endWordPosition="1270">(Beaufort et al., 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. To our knowledge, the work presented here is the first to clearly link the output of a normalization system to the output of the downstream application. Similarly, our work is the first to prioritize domain adaptation during the new wave of text message normalization. 3 Model In this section we introduce our normalization framework, which draws inspiration from our previous work on spelling correction for search (Bao et al., 2011). 3.1 Replacement Generators Our input the original, unnormalized text, represented as a sequence x = x1, x2, ... , xn of tokens xi. In this section we will use the following se1160 quence as our running example: x = Ay1 woudent2 of3 see4&apos;em5 where space replaces comma for readability, and each token is subscripted by its position. Given the input x, we apply a series of replacement generators, where a replacement generator is a function that takes x as input and produces a collection of replacements. Here, a replacement is a statement of the form “replace tokens xi, ... , xj−1 with s.” More p</context>
</contexts>
<marker>Bao, Kimelfeld, Li, 2011</marker>
<rawString>Zhuowei Bao, Benny Kimelfeld, and Yunyao Li. 2011. A graph approach to spelling correction in domaincentric search. In ACL, pages 905–914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beaufort</author>
</authors>
<title>Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>770--779</pages>
<marker>Beaufort, 2010</marker>
<rawString>Richard Beaufort, Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon. 2010. A hybrid rule/model-based finite-state framework for normalizing sms messages. In ACL, pages 770–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Chiticariu</author>
<author>Rajasekar Krishnamurthy</author>
<author>Yunyao Li</author>
<author>Sriram Raghavan</author>
<author>Frederick Reiss</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>SystemT: An algebraic approach to declarative information extraction.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>128--137</pages>
<marker>Chiticariu, Krishnamurthy, Li, Raghavan, Reiss, Vaithyanathan, 2010</marker>
<rawString>Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Sriram Raghavan, Frederick Reiss, and Shivakumar Vaithyanathan. 2010. SystemT: An algebraic approach to declarative information extraction. In ACL, pages 128–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>IJDAR,</journal>
<pages>10--3</pages>
<contexts>
<context position="6162" citStr="Choudhury et al. (2007)" startWordPosition="930" endWordPosition="933">d of interest in the normalization problem. Research on SMS and Twitter normalization has been roughly categorized as drawing inspiration from three other areas of NLP (Kobus et al., 2008): machine translation, spell checking, and automatic speech recognition. The statistical machine translation (SMT) metaphor was the first proposed to handle the text normalization problem (Aw et al., 2006). In this mindset, normalizing SMS can be seen as a translation task from a source language (informal) to a target language (formal), which can be undertaken with typical noisy channel based models. Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed</context>
<context position="24634" citStr="Choudhury et al., 2007" startWordPosition="4093" endWordPosition="4097">ing domain (denoted as domain-specific). All runs use ten-fold cross validation for training and evaluation. The Stanford parser1 (Marneffe et al., 2006) was used to produce all dependency 1Version 2.0.4, http://nlp.stanford.edu/ software/lex-parser.shtml 1164 parses. We compare our system to the following baseline solutions: w/oN: No normalization is performed. Google: Output of the Google spell checker. w2wN: The output of the word-to-word normalization of Han and Baldwin (2011). Not available for call-center data. Gw2wN: The manual gold standard word-toword normalizations of previous work (Choudhury et al., 2007; Han and Baldwin, 2011). Not available for call-center data. Our results use the metrics of Section 5.1. 5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011). Because the gold standard used in this work only provided word mappings for out-of-vocabulary words and did not enforce grammaticality, we reannotated the gold standard data2. Their original gold standard annotations were kept as a baseline. To produce Twitter-specific generators, we examined the Twitter development data collected for generic generat</context>
<context position="26752" citStr="Choudhury et al. (2007)" startWordPosition="4424" endWordPosition="4428">produce lower quality parses than those produced by our approach. 2Our results and the reannotations of the Twitter and SMS data are available at https://www.cs.washington. edu/node/9091/ System Verb Subject-Object Pre Rec F1 Pre Rec F1 w/oN 83.7 68.1 75.1 31.7 38.6 34.8 Google 88.9 78.8 83.5 36.1 46.3 40.6 w2wN 87.5 81.5 84.4 44.5 58.9 50.7 Gw2w 89.8 83.8 86.7 46.9 61.0 53.0 generic 91.7 88.9 90.3 53.6 70.2 60.8 domain specific 95.3 88.7 91.9 72.5 76.3 74.4 Table 2: Performance on Twitter dataset 5.2.2 SMS To evaluate the performance on SMS data, we use the Treasure My Text data collected by Choudhury et al. (2007). As with the Twitter data, the word-to-word normalizations were reannotated to enforce grammaticality. As a replacement generator for SMS-specific substitutions, we used a mapping dictionary of SMS abbreviations.3 No further SMS-specific development data was needed. Table 3 gives the results on the SMS data. The SMS dataset proved to be more difficult than the Twitter dataset, with the overall performance of every system being lower. While this drop of performance may be a reflection of the difference in data styles between SMS and Twitter, it is also likely a product of the collection method</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. IJDAR, 10(3-4):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="17049" citStr="Collins, 2002" startWordPosition="2897" endWordPosition="2898"> Initialize each θj as zero, and ld obtain each αioaccording to (1). Repeat T times: 1. Infer each α∗i from xi using the current O; 2. θj ← θj+Ei(Φj(αgold i , xi)−Φj(α∗i ,xi)) for all j = 1, ... , k. Output: O = (θ1, ... , θk) Figure 3: Learning algorithm path defined by α, and Ep(αi|xi,Θ)Φj(αi, xi) is the expected value of that sum (over all legal assignments αi), assuming the current weight vector. How to efficiently compute Ep(αi|xi,Θ)Φj(αi,xi) in our model is unclear; naively, it requires enumerating all legal assignments. We instead opt to use a more tractable perceptron-style algorithm (Collins, 2002). Instead of computing the expectation, we simply compute Φj(α∗i , xi), where α∗i is the assignment with the highest probability, generated using the current weight vector. The result is then: � Φj(αgold i , xi) − Φj(α∗ i , xi) Our learning applies the following two steps iteratively. (1) Generate the most probable sequence within the current weights. (2) Update the weights by comparing the path generated in the previous step to the gold standard path. The algorithm in Figure 3 summarizes the procedure. 4 Instantiation In this section, we discuss our instantiation of the model presented in the</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In CALC,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="6657" citStr="Cook and Stevenson (2009)" startWordPosition="1004" endWordPosition="1008">l) to a target language (formal), which can be undertaken with typical noisy channel based models. Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent wo</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In CALC, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davies</author>
</authors>
<title>The corpus of contemporary american english: 450 million words,</title>
<date>2008</date>
<note>Avialable online at: http://corpus. byu.edu/coca/.</note>
<contexts>
<context position="19347" citStr="Davies, 2008" startWordPosition="3276" endWordPosition="3277">word forms to inserting and deleting tokens. 4.2 Features Although the proposed framework supports real valued features, all features in our system are binary. In total, we used 70 features. Our feature set pulls information from several different sources: N-gram: Our n-gram features indicate the frequency of the phrases induced by an edge. These features are turned into binary ones by bucketing their log values. For example, on the edge from (1, 2, T) to (2, 3, would) such a feature will indicate whether the frequency of T would is over a threshold. We use the Corpus of Contemporary English (Davies, 2008 ) to produce our n-gram information. Part-of-speech: Part-of-speech information can be used to produce features that encourage certain behavior, such as avoiding the deletion of noun phrases. We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al., 2011). Of course, the part-of-speech information obtained this way is likely to be noisy, and we expect our learning algorithm to take that into account. Positional: Information from positions is used primarily to handle capitalization and punctuation insertion, for example, by incorpor</context>
</contexts>
<marker>Davies, 2008</marker>
<rawString>Mark Davies. 2008-. The corpus of contemporary american english: 450 million words, 1990-present. Avialable online at: http://corpus. byu.edu/coca/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter. In</title>
<date>2011</date>
<booktitle>ACL,</booktitle>
<pages>368--378</pages>
<contexts>
<context position="7127" citStr="Han and Baldwin (2011)" startWordPosition="1075" endWordPosition="1078">etaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al., 2010; Pennell and Liu, 2010),</context>
<context position="24497" citStr="Han and Baldwin (2011)" startWordPosition="4073" endWordPosition="4076">cement generators presented in Section 4 (denoted as generic), and one that adds additional domain-specific generators for the corresponding domain (denoted as domain-specific). All runs use ten-fold cross validation for training and evaluation. The Stanford parser1 (Marneffe et al., 2006) was used to produce all dependency 1Version 2.0.4, http://nlp.stanford.edu/ software/lex-parser.shtml 1164 parses. We compare our system to the following baseline solutions: w/oN: No normalization is performed. Google: Output of the Google spell checker. w2wN: The output of the word-to-word normalization of Han and Baldwin (2011). Not available for call-center data. Gw2wN: The manual gold standard word-toword normalizations of previous work (Choudhury et al., 2007; Han and Baldwin, 2011). Not available for call-center data. Our results use the metrics of Section 5.1. 5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011). Because the gold standard used in this work only provided word mappings for out-of-vocabulary words and did not enforce grammaticality, we reannotated the gold standard data2. Their original gold standard annotation</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In ACL, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs. In EMNLP-CoNLL,</title>
<date>2012</date>
<pages>421--432</pages>
<contexts>
<context position="7339" citStr="Han et al., 2012" startWordPosition="1109" endWordPosition="1112"> model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al., 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. To our knowledge, the work presented here is the first to clearly link the o</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In EMNLP-CoNLL, pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Franc¸ois Yvon</author>
<author>G´eraldine Damnati</author>
</authors>
<title>Normalizing SMS: are two metaphors better than one?</title>
<date>2008</date>
<booktitle>In COLING,</booktitle>
<pages>441--448</pages>
<contexts>
<context position="5727" citStr="Kobus et al., 2008" startWordPosition="863" endWordPosition="866">2001) took the first major look at the normalization problem, citing the need for normalized text for downstream applications. Unlike later works that would primarily focus on specific noisy data sets, their work is notable for attempting to develop normalization as a general process that could be applied to different domains. The recent rise of heavily informal writing styles such as Twitter and SMS messages set off a new round of interest in the normalization problem. Research on SMS and Twitter normalization has been roughly categorized as drawing inspiration from three other areas of NLP (Kobus et al., 2008): machine translation, spell checking, and automatic speech recognition. The statistical machine translation (SMT) metaphor was the first proposed to handle the text normalization problem (Aw et al., 2006). In this mindset, normalizing SMS can be seen as a translation task from a source language (informal) to a target language (formal), which can be undertaken with typical noisy channel based models. Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes int</context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Franc¸ois Yvon, and G´eraldine Damnati. 2008. Normalizing SMS: are two metaphors better than one? In COLING, pages 441– 448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="11759" citStr="Lafferty et al., 2001" startWordPosition="1916" endWordPosition="1919"> them. Our variables carry two types of interdependencies. The first is that of syntactic consistency: the entire assignment is required to be legal. The second captures correlation among replacements. For instance, if we replace of with have in our running example, then the next see token is more likely to be replaced with seen. In this work, dependencies of the second type are restricted to pairs of variables, where each pair corresponds to a replacement and a consistent follower thereof. The above dependencies can be modeled over a standard undirected graph using Conditional Random Fields (Lafferty et al., 2001). However, the graph would be complex: in order to model local consistency, there should be edges between every two nodes that violate local consistency. Such a model renders inference and learning infeasible. Therefore, we propose a clearer model by a directed graph, as illustrated in Figure 1 (where nodes are represented by replacements r instead of the variables Xr, for readability). To incorporate correlation among replacements, we introduce an edge from Xr to Xr, whenever r&apos; is a consistent follower of r. Moreover, we introduce two dummy nodes, start and end, with an edge from start to ea</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Bingqing Wang</author>
<author>Yang Liu</author>
</authors>
<title>Insertion, deletion, or substitution? normalizing text messages without pre-categorization nor supervision.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>71--76</pages>
<contexts>
<context position="6977" citStr="Liu et al. (2011)" startWordPosition="1055" endWordPosition="1058">me information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications</context>
</contexts>
<marker>Liu, Weng, Wang, Liu, 2011</marker>
<rawString>Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011. Insertion, deletion, or substitution? normalizing text messages without pre-categorization nor supervision. In ACL, pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A broad-coverage normalization system for social media language. In</title>
<date>2012</date>
<booktitle>ACL,</booktitle>
<pages>1035--1044</pages>
<contexts>
<context position="7428" citStr="Liu et al., 2012" startWordPosition="1123" endWordPosition="1126">tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al., 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. To our knowledge, the work presented here is the first to clearly link the output of a normalization system to the output of the downstream application. Similarly, o</context>
</contexts>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-coverage normalization system for social media language. In ACL, pages 1035–1044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06,</booktitle>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC-06, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4202" citStr="Papineni et al., 2002" startWordPosition="617" endWordPosition="621">ng to the need for clean text for downstream processing applications, such as syntactic parsing. However, most studies of normalization give little insight into whether and to what degree the normalization process improves 1159 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159–1168, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics the performance of the downstream application. For instance, it is unclear how performance measured by the typical normalization evaluation metrics of word error rate and BLEU score (Papineni et al., 2002) translates into performance on a parsing task, where a well placed punctuation mark may provide more substantial improvements than changing a non-standard word form. To address this problem, this work introduces an evaluation metric that ties normalization performance directly to the performance of a downstream dependency parser. The rest of this paper is organized as follows. In Section 2 we discuss previous approaches to the normalization problem. Section 3 presents our normalization framework, including the actual normalization and learning procedures. Our instantiation of this model is pr</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL, pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>Normalization of text messages for text-to-speech.</title>
<date>2010</date>
<booktitle>In ICASSP,</booktitle>
<pages>4842--4845</pages>
<contexts>
<context position="7726" citStr="Pennell and Liu, 2010" startWordPosition="1171" endWordPosition="1174"> Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al., 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. To our knowledge, the work presented here is the first to clearly link the output of a normalization system to the output of the downstream application. Similarly, our work is the first to prioritize domain adaptation during the new wave of text message normalization. 3 Model In this section we introduce our normalization framework, which draws inspiration from our previous work on spelling correction for search (Bao et al., 2011). 3.1 Replacement Generators </context>
</contexts>
<marker>Pennell, Liu, 2010</marker>
<rawString>Deana Pennell and Yang Liu. 2010. Normalization of text messages for text-to-speech. In ICASSP, pages 4842–4845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>A character-level machine translation approach for normalization of SMS abbreviations.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>974--982</pages>
<contexts>
<context position="6752" citStr="Pennell and Liu (2011)" startWordPosition="1020" endWordPosition="1024"> Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on impro</context>
</contexts>
<marker>Pennell, Liu, 2011</marker>
<rawString>Deana Pennell and Yang Liu. 2011. A character-level machine translation approach for normalization of SMS abbreviations. In IJCNLP, pages 974–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in Tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1524--1534</pages>
<contexts>
<context position="19665" citStr="Ritter et al., 2011" startWordPosition="3322" endWordPosition="3325"> the phrases induced by an edge. These features are turned into binary ones by bucketing their log values. For example, on the edge from (1, 2, T) to (2, 3, would) such a feature will indicate whether the frequency of T would is over a threshold. We use the Corpus of Contemporary English (Davies, 2008 ) to produce our n-gram information. Part-of-speech: Part-of-speech information can be used to produce features that encourage certain behavior, such as avoiding the deletion of noun phrases. We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al., 2011). Of course, the part-of-speech information obtained this way is likely to be noisy, and we expect our learning algorithm to take that into account. Positional: Information from positions is used primarily to handle capitalization and punctuation insertion, for example, by incorporating features for capitalized words after stop punctuation or the insertion of stop punctuation at the end of the sentence. Lineage: Finally, we include binary features � i 1163 that indicate which generator spawned the replacement. 5 Evaluation In this section, we present an empirical study of our framework. The st</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in Tweets: An experimental study. In EMNLP, pages 1524–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan W Black</author>
<author>Stanley F Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>333</pages>
<contexts>
<context position="1826" citStr="Sproat et al., 2001" startWordPosition="264" endWordPosition="267">ransfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations. 1 Introduction Text normalization is the task of transforming informal writing into its standard form in the language. It is an important processing step for a wide range of Natural Language Processing (NLP) tasks such as text-to-speech synthesis, speech recognition, information extraction, parsing, and machine translation (Sproat et al., 2001). ∗This work was conducted at IBM. The use of normalization in these applications poses multiple challenges. First, as it is most often conceptualized, normalization is seen as the task of mapping all out-of-vocabulary non-standard word tokens to their in-vocabulary standard forms. However, the scope of the task can also be seen as much wider, encompassing whatever actions are required to convert the raw text into a fully grammatical sentence. This broader definition of the normalization task may include modifying punctuation and capitalization, and adding, removing, or reordering words. Secon</context>
<context position="5113" citStr="Sproat et al. (2001)" startWordPosition="762" endWordPosition="765">rmance of a downstream dependency parser. The rest of this paper is organized as follows. In Section 2 we discuss previous approaches to the normalization problem. Section 3 presents our normalization framework, including the actual normalization and learning procedures. Our instantiation of this model is presented in Section 4. In Section 5 we introduce the parser driven evaluation metric, and present experimental results of our model with respect to several baselines in three different domains. Finally, we discuss our experimental study in Section 6 and conclude in Section 7. 2 Related Work Sproat et al. (2001) took the first major look at the normalization problem, citing the need for normalized text for downstream applications. Unlike later works that would primarily focus on specific noisy data sets, their work is notable for attempting to develop normalization as a general process that could be applied to different domains. The recent rise of heavily informal writing styles such as Twitter and SMS messages set off a new round of interest in the normalization problem. Research on SMS and Twitter normalization has been roughly categorized as drawing inspiration from three other areas of NLP (Kobus</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan W. Black, Stanley F. Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of non-standard words. Computer Speech &amp; Language, 15(3):287– 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenzhen Xue</author>
<author>Dawei Yin</author>
<author>Brian D Davison</author>
</authors>
<title>Normalizing microtext. In Analyzing Microtext, volume WS-11-05 of AAAI Workshops.</title>
<date>2011</date>
<contexts>
<context position="6836" citStr="Xue et al. (2011)" startWordPosition="1034" endWordPosition="1037"> in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Althou</context>
</contexts>
<marker>Xue, Yin, Davison, 2011</marker>
<rawString>Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011. Normalizing microtext. In Analyzing Microtext, volume WS-11-05 of AAAI Workshops.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>