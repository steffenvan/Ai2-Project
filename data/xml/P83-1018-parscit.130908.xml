<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<title confidence="0.891170285714286">
SYNTACI IC CONSTRAINTS AND EFFICIENT PAItSAIIIIATY
Robert C. Berwick
Room 820, Ml&apos;! Artificial Intelligence Laboratory
545 Technology Square, Cambridge, MA 02139
Amy S. Weinberg
Deparuncnt of Linguistics, MIT
Cambridge, MA 02139
</title>
<sectionHeader confidence="0.654302" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.983242316455696">
A central goal of linguistic theory is to explain why natural
languages are the way they are. It has often been supposed that
computational considerations ought to play a role in this
characterization, but rigorous arguments along these lines have been
difficult to come by. In this paper we show how a key &amp;quot;axiom&apos;&apos; of
certain theories of grammar, Subjacency, can be explained by
appealing to general restrictions on on-line parsing plus natural
constraints on the rule-writing vocabulary of grammars. The
explanation avoids the problems with Marcus&apos; [19801 attempt to
account for the same constraint. The argument is robust with respect
to machine implementation, and thus avoids the problems that often
arise when making detailed claims about parsing efficiency. It has the
added virtue of unifying in the functional domain of parsing certain
grammatically disparate phenomena, as well as making a strong claim
about the way in which the grammar is actually embedded into an
on-line sentence processor.
I INTRODUCTION
In its short history, computational linguistics has been driven by
two distinct but interrelated goals. On the one hand, it has aimed at
computational explanations of distinctively human linguistic behavior
-- that is, accounts of why natural languages are the way they are
viewed from the perspective of computation. On the other hand, it has
accumulated a stock of engineering methods for building machines to
deal with natural (and artificial) languages. Sometimes a single body
of research has combined both goals. This was true of the work of
Marcus [19801, for example. But all too often the goals have remained
opposed -- even to the extent that current transformational theory has
been disparaged as hopelessly &amp;quot;intractable&amp;quot; and no help at all in
constructing working parsers.
This paper shows that modern transformational grammar (the
&amp;quot;Government-Binding&amp;quot; or &amp;quot;GB&amp;quot; theory as described in Chomsky
[1981]) can contribute to both aims of computational linguistics. We
show that by combining simple assumptions about efficient parsability
along with some assumptions about just how grammatical theory is to
be &amp;quot;embedded&amp;quot; in a model of language processing, one can actually
ex flain sonic key constraints of natural languages, such as Subjacency.
(rhe argument is different from that used in Marcus [19801.) In fact,
almost the entire pattern of constraints taken as &amp;quot;axioms&amp;quot; by the GB
theory can be accounted for. Second, contrary to what has sometimes
been supposed, by exploiting these constraints we can show that a
GB-based theory is particularly compatible wiih efficient parsing
designs. in particular, with extended 1.1((k,t) parsers (of the Sort
described by Marcus [19801). We can extend the 1.1((k.t) design to
accommodate such phenomena as antecedent-PRO. and pronominal
binding. rightward fnovement, gapping, and VP deletion.
A. Functional Explanations of! .ocality Principles
Let us consider how to explain locality constraints in natural
languages. First of all, what exactly do we mean by a &amp;quot;locality
constraint&amp;quot;? The paradigm case is that of Subiacency: the distance
between a displaced constituent and its &amp;quot;underlying&amp;quot; canonical
argument position cannot be too large, where the distance is gauged (in
English) in terms of the number of the number of S(entence) or NP
phrase boundaries. For example, in sentence (la) below, John (the
so-called &amp;quot;antecedent&amp;quot;) is just one S-boundary away from its
presumably &amp;quot;underlying&amp;quot; argument position (denoted &amp;quot;x&amp;quot;, the
&amp;quot;trace&amp;quot;)) as the Subject of the embedded clause, and the sentence is
fine:
(1a) John seems Is x to like ice cream].
However, all we have to do is to make the link between John and x
extend over two S&apos;s, and the sentence is ill-formed:
(lb) John seems [s it is certain (s x to like ice cream
This restriction entails a &amp;quot;successive cyclic&amp;quot; analysis of
transformational rules (see Chomsky [19731). In order to derive a
sentence like lie) below without violating the Subjacency condition,
we must move the NP from its canonical argument position through
the empty Subject position in the next higher S and then to its surface
slot:
(1c) John seems [el to be certain x to get the ice cream.
Since the intermediate subject position is filled in (lb) there is no licit
derivation for this sentence.
More precisely, we can state the Subjacency constraint as follows:
No rule of grammar can involve X and Y in a configuration like the
following,
where a and /.3 are bounding nodes (in English, S or NP phrases).
Why should natural languages be designed this way and not some
other way? Why, that is, should a constraint like Subjacency exist at
all? Our general result is that under a certain set of assumptions about
grammars and their relationship to human sentence processing one can
actually expect the following pattern of syntactic locality constraints:
</bodyText>
<footnote confidence="0.520773">
(1) The antecedent-trace relationship must
obey Subjacency, but other &amp;quot;binding&amp;quot;
realtionships (e.g.. NP--Pro) need not obey
Subjacency.
</footnote>
<page confidence="0.996448">
119
</page>
<listItem confidence="0.9987118">
(2) Gapping constructions must be subject
to a bounding condition resembling
Subjacency, but VP deletion need not be.
(3) Rightward movement must be strictly
bounded.
</listItem>
<figureCaption confidence="0.890071909090909">
To the extent that this predicted pattern of constraints is actually
observed -- as it is in English and other languages -- we obtain a
genuine functional explanation of these constraints and support for the
assumptions themselves. The argument is different from Marcus&apos;
because it accounts for syntactic locality constraints (like Subjacency)
as the joint effect of a particular theory of grammar, a theory of how
that grammar is used in parsing, a criterion for efficient parsability,
and a theory of of how the parser is built. In contrast, Marcus
attempted to argue that Subjacency could be derived from just the
(independently justified) operating principles of a particular kind of
parser.
</figureCaption>
<bodyText confidence="0.832620551020408">
B. Assumption
The assumptions we make are the following:
(1) The grammar includes a level of
annotated surface structure indicating how
constituents have been displaced from their
canonical predicate argument positions.
Further, sentence analysis is divided into
two stages, along the lines indicated by the
theory of Government and Binding: the
first stage is a purely syntactic analysis that
rebuilds annotated surface structure; the
second stage carries out the interpretation
of variables, binds them to operators, all
making use of the &amp;quot;referential indices&amp;quot; of
NPs.
(2) To be &amp;quot;visible&amp;quot; at a stage of analysis a
linguistic representation must be written in
the vocabulary of that level. For example,
to be affected by syntactic operations, a
representation must be expressed in a
syntactic vocabulary (in, the usual sense): to
be interpreted by operations at the second
stage, the NPs in a representation must
possess referential indices. (This
assumption is not needed to derive the
Subjacency constraint, but may be used to
account for another &amp;quot;axiom&amp;quot; of current
grammatical theory, the so-called
&amp;quot;constituent command&amp;quot; constraint on
antecedents and the variables that they
bind.) This &amp;quot;visibility&amp;quot; assumption is a
rather natural one.
(3) The rule-writing vocabulary of the
grammar cannot make use of arithmetic
predicates such as &amp;quot;one&amp;quot;, &amp;quot;two&amp;quot; or &amp;quot;three&amp;quot;,
but only such predicates as &amp;quot;adjacent&amp;quot;.
Further, quantificational statements are not
allowed in rules. These two assumptions
arc also rather standard. It has often been
noted that grammars &amp;quot;do not count&amp;quot; -- that
grammatical predicates are structurally
based. There is no rule of grammar that
takes the just the fourth constituent of a
sentence and moves it, for example. In
contrast, many different kinds of rules of
grammar make reference to adjacent
constituents. (This is a feature found in
morphological, phonological, and syntactic
rules.)
</bodyText>
<listItem confidence="0.513657">
(4) Parsing is not done via a method that
carries along (a representation) of all
</listItem>
<bodyText confidence="0.973947392857143">
possible derivations in parallel. In
particular, an Earley-type algorithm is ruled
out. To the extent that multiple options
about derivations are not pursued, the parse
is &amp;quot;deterministic.&amp;quot;
(5) The left-context of the parse (as defined
in Aho and Ullman [19721) is literally
represented, rather than generatively
represented (as, e.g., a regular set). In
particular, just the symbols used by the
grammar (S, NP, VP...) are part of the
left-context vocabulary, and not &amp;quot;complex&amp;quot;
symbols serving as proxies for the set of
left-context strings.&apos; In effect, we make the
(quite strong) assumption that the sentence
processor adopts a direct, transparent
embedding of the grammar.
Other theories or parsing methods do not meet these constraints
and fail to explain the existence of locality constraints with respect to
this particular set of assumptions. 2 For example. as we show, there is
no reason to expect a constraint like Subjacency in the Generalized
Phrase Structure Grammars (GPSGs) of Gazdar 119811, because there
is no inherent barrier to easily processing a sentence where an
antecedent and a trace are unboundedly far from each other.
Similarly, if a parsing method like Farley&apos;s algorithm were actually
used by people. thon Subjacency remains a mystery on the functional
grounds of efficient parsability. (It could still be explained on other
functional grounds, e.g., that of learnability.)
</bodyText>
<sectionHeader confidence="0.699026" genericHeader="method">
II PARSING AND LOCALITY PRINCIPLES
</sectionHeader>
<bodyText confidence="0.8438742">
To begin the actual argument then, assume that on-line sentence
processing is done by something like a deterministic parser.3
Sentences like (2) cause trouble for such a parser:
(2) What i do you think that John told Mary...that tie
would like to eat e.
</bodyText>
<footnote confidence="0.592408285714286">
1. Recall that the successive lines of a left- or right-most derivation in a context-free
grammar constitute a regular language. as shown m. e.g.. DeRemer 119691.
2. Plainly. one is free to imagine some other set of assumptions that would do the job.
3. If one assumes a backtracking parser. then the argument can also be made to go
thmugh hut only by a.ssulliing, that backtracking is very costly Since this sort of parser
clearly s-uhiumes the 11(11(1-type machines under the right consirual of &apos;cost&amp;quot;, we make
thc stronger assumption of 112(k)-ness.
</footnote>
<page confidence="0.992939">
120
</page>
<bodyText confidence="0.999634882352941">
The problem is that on recognizing the verb eat the parser must decide
whether to expand the parse with a trace (the transitive reading) or
with no postverbal clement (the intransitive reading). The ambiguity
cannot be locally resolved since eat takes both readings. It can only be
resolved by checking to see whether there is an actual antecedent.
Further, observe that this is indeed a parsing decision: the machine
MUSE make some decision about how to to build a portion of the parse
tree. Finally, given non-parallelism, the parser is not allowed to pursue
both paths at once: it must decide now how to build the parse tree (by
inserting an empty NP trace or not).
Therefore, assuming that the correct decision is to be made on-line
(or that retractions of incorrect decisions are costly) there must be an
actual parsing rule that expands a category as transitive ill there is an
immediate postverbal NP in the string (no movement) or if an actual
antecedent is present. However, the phonologically overt antecedent
can be unboundedly far away from the gap. Therefore, it would seem
that the relevant parsing rule would have to refer to a potentially
unbounded left context. Such a rule cannot be stated in the finite
control table of an 1.12(k) parser. Therefore we must find some finite
way of expressing the domain over which the antecedent must be
searched.
There are two ways of accomplishing this. First, one could express
all possible left-contexts as some regular set, and then carry this
representation along in the finite control table of the I.12(k) machine.
This is always possible in the case of a cuntext-free grammar, and in
fact is the &amp;quot;standard&amp;quot; approach.4 However, in the case of (e.g.) wiz
movement. this demands a generative encoding of the associated finite
state automaton, via the use of complex ygnbois like &apos;&apos;S/wit&apos;&apos;
t denoting the &amp;quot;state&amp;quot; that a wit has been encountered) and rules to pass
long this non-literal representation of the state of the parse. his
approach works, since we can pass along this state encoding through
the VP (via the complex non-terminal symbol VP/wh) and finally into
the embedded S. This complex non-terminal is then used to trigger an
expansion of ear into its transitive form. In fact, this is precisely the
solution method advocated by Gazdar. We see then that if one adopts
a non-terminal encoding scheme there should be no problem in
parsing any single long-distance gap-filler relationship. That is, there
is no need for a constraint like Subjacency.5
Second, the problem of unbounded left-context is directly avoided
if the search space is limited to some literally finite left context. But
this is just what the Subjacency constraint does: it limits where an
antecedent NP could be to an immediately adjacent S or S. This
constraint has a simple interpretation in an actual parser (like that built
by Marcus [191101). The IF-THEN pattern-action rules that make up
the Marcus parser&apos;s finite control &amp;quot;transition table&amp;quot; must be finite in
order to be stored inside a machine. The rule actions themselves are
literally finite. If the rule patterns must be literally stored (e.g., the
pattern fs [s...fs must be stored as an actual arbitrarily long string of S
nodes, rather than as the regular set S+), then these patterns must be
literally finite. That is, parsing patterns &apos;oust refer to literally bounded
right and left context (in terms of phrasal nodes).6 Note further that
</bodyText>
<footnote confidence="0.938745142857143">
4 iroiiomne the approach of DeRemer 119691, one builds a finite state automaton that
rccognues exactly i he set of left-context strings that can arise during the course of a
right-mosI deroation, the so-called chumetertalc finite slate automaton.
5 !lamb the same holds for a &amp;quot;hold cell&amp;quot; approach to computing filler-gap
1C13,11011Shipi.
6. Actually Men, this kind of device falls silo the category of bounded conies t parsing,
as defined by I loyd [19641.
</footnote>
<bodyText confidence="0.985249368421053">
this constraint depends on We sheer representability of the parser&apos;s
rule system in a finite machine, rather than on any details of
implementation. Therefore it will hold invariantly with respect to
machine design -- no [natter kind of machine we build, if we assume a
literal representation of left-contexts, then some kind of finiteness
constraint is required. The robustness of this result contrasts with the
usual problems in applying &amp;quot;efficiency&amp;quot; results to explain gram &apos;&apos;&amp;quot;&amp;quot;:cal
constraints. These often fail because it is difficult to consider all
possible implementations simultaneously. However, if the argument is
invariant with respect to machine desing, this problem is avoided.
Given literal left-contexts and no (or costly) backtracking, the
argument so far motivates wine bounding condition for ambiguous
sentences like these. However, to get the full range of cases these
functional facts must interact with properties of the rule writing system
as defined by the grammar. We will derive the fact that the bounding
condition must be alajacency (as opposed to tri- or quad-jacency) by
appeal to the fact that grammatical constraints and rules are stated in a
vocabulary which is non-countutg. Arithmetic predicates are
forbidden. But this means that since only the preditate &amp;quot;adiaecnt&amp;quot; is
permitted, any literal bounding reyriction !Oust be expressed in terms
of adjacent domains: hence Subjacency. (Note Mac &amp;quot;adjacent&amp;quot; is also
an arithmetic predicate.) l:urther, Subjacency must appiyto ill traces
(not just traces of mubiguously transitive/intransitive verbs) bccause
restriction to just the ambiguous cases would involve using existential
quantification. Quantificational predicates are barred in the rule
writing vocabulary of natural grammars.7
Next we extend the approach to NP movement and Gapping.
Gapping is particularly interesting because it is difficult to explain
why this construction (unlike other deletion rules) is bounded. That is,
why is (3) but not (4) grammatical:
(3) John will hit Frank and Bill will My&apos;) George.
*(4)John will hit Frank and I don&apos;t believe Bill will
[elvpGeorge.
The problem with gapping constructions is that the attachment of
phonologically identical complements is governed by the verb that the
complement follows. Extraction tests show that in (5) the phrase after
,llary attaches to V while in (6) it attaches to V- (See Hornstein and
Weinberg [19811 for details.)
</bodyText>
<listItem confidence="0.9474725">
(5) John will run after Mary.
(6) John will arrive after Mary.
</listItem>
<bodyText confidence="0.992280615384615">
In gapping structures, however, die verb of the gapped constituent s
not present in the string. Therefore, correct attachment of the
complement can only be guaranteed by accessing die antecedent in the
previous clause. If this is true however, then the bounding argument
for Subjacency applies to this case as well: given deterministic parsing
of gapping done correctly, and a literal representation of left-context,
then gapping must be context-bounded. Note that this is a particularly
7 Of course, there is a another natural predicate that would produce a finite bound on
rule context: if Ni&apos; and trace had to be in the -ame S domain Presumably, this is also an
option that could get realoed in some natural grammars: the result* languages would
not have overt movement outside of an S. Note that the natural predicates simply give
the range of poNsi hie natural gransays. noi those actually found.
The elimination &amp;quantification predicaments supportable on grounds of acquisition.
</bodyText>
<page confidence="0.994115">
121
</page>
<bodyText confidence="0.9993675">
interesting example because it shows how grammatically dissimilar
operations like w/-movement and gapping can &amp;quot;fall together&amp;quot; in the
functional domain of parsing.
Chomsky, Noam [19811 Lectures on Government and Binding, Eons
Publications,
NP-trace and gapping constructions contrast with
antecedent/(pro)nominal binding, lexical anaphor relationships, and
VP deletion. These last three do not obey Subjaccncy. For example, a
Noun Phrase can bc unboundedly far from a (phonologically empty)
PRO, even in terms of
Johni thought it was certain [PROi feeding himself]
would be easy.
Note though that in these cases the expansion of the syntactic tree does
not depend on the presence or absence of an antecedent.
(Pro)nominals and lexical anaphors are phonologically realized in the
string and can unambiguously tell the parser how to expand the tree.
(After the tree is fully expanded the parser may search back to see
whether the clement is bound to an antecedent, but this is not a
parsing decision.) VP deletion sites are also always locally detectable
from the simple fact that every sentence requires a VP. The same
argument applies to PRO. PRO is locally detectable as the only
phonologically unrealized element that can appear in an ungoverned
context. and the predicate &amp;quot;ungoverned&amp;quot; is local.8 In short, there is no
parsing decision that hinges on establishing the PRO-antecedent, VP
deletion-antecedent, or lexical anaphor-antecedent relationship. But
then, we should not expect bounding principles to apply in these cases,
and, in fact, we do not find these elements subject to bounding. Once
again then, apparently diverse grammaucal phenomena behave alike
within a functional realm.
To summarize, we can explain why Subjacency applies to exactly
those elements that the grammar stipulates it MUSE apply to. We do
this using both facts about the functional design of a parsing system
and properties of the formal rule writing vocabulary. To the extent
that the array of assumptions about the grammar and parser actually
explain this observed constraint on human linguistic behavior, we
obtain a powerful argument that certain kinds .of grammatical
representations and parsing designs are actually implicated in human
sentence processing.
</bodyText>
<sectionHeader confidence="0.853789" genericHeader="method">
Ill ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.9990092">
This report describes work done at the Artificial Intelligence
Laboratory of the Massachusetts Institute of Icchnology. Support for
the Laboratory&apos;s artificial intelligence research is provided in part by
the Advanced Research Projects Agency of the Department of Defense
under Office of Naval Research Contract N000 l4-80-C-0505.
</bodyText>
<sectionHeader confidence="0.838322" genericHeader="method">
IV REFERENCES
</sectionHeader>
<reference confidence="0.997081842105263">
Aho, Alfred and Ullman, Jeffrey 119721 The Theory of Parsing
Translation, and Compiling, vol. 1., Prentice-Flail.
Chomsky. Noam [19731 &amp;quot;Conditions on Transformations,&amp;quot;in S.
Anderson &amp; P Kiparsky, eds. d Festschrift for Morris Halle. Holt,
Rinehart and Winston.
R Since a Is ungovet nett iff a governed is false, and a plowed is a bounded predicate.
iciii icstri,ted to onighly it sunlit: maximal projection (at worst an S).
DeRemer, Frederick [19691 Practical Transhaws for LR(k) languages,
PhD dissertation, MD Department of Electrical Engineering and
Computer Science.
Floyd, Robert [19641 &amp;quot;Bounded-context syntactic analysis,&amp;quot;
Communications of the Association for Computing A.lachinery, 7, pp.
62-66.
Gazdar, Gerald [19811 &amp;quot;Unbounded dependencies and coordinate
structure,&amp;quot; Linguistic Inquiry, 12:2 155-184.
Hornstein. Norbert and Weinberg, Amy (19811 &amp;quot;Preposition stranding
and case theory,&amp;quot; Linguistic Inquiry, 12:1.
Marcus, Mitchell [19801 A Theory of Syntactic Recognition for Natural
Language, MIT Press
</reference>
<page confidence="0.997454">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941172">
<title confidence="0.99206">SYNTACI IC CONSTRAINTS AND EFFICIENT PAItSAIIIIATY</title>
<author confidence="0.999863">Robert C Berwick</author>
<affiliation confidence="0.967951">Room 820, Ml&apos;! Artificial Intelligence Laboratory</affiliation>
<address confidence="0.999741">545 Technology Square, Cambridge, MA 02139</address>
<author confidence="0.999928">Amy S Weinberg</author>
<affiliation confidence="0.997657">Deparuncnt of Linguistics, MIT</affiliation>
<address confidence="0.999975">Cambridge, MA 02139</address>
<abstract confidence="0.998962823529412">A central goal of linguistic theory is to explain why natural languages are the way they are. It has often been supposed that computational considerations ought to play a role in this characterization, but rigorous arguments along these lines have been difficult to come by. In this paper we show how a key &amp;quot;axiom&apos;&apos; of certain theories of grammar, Subjacency, can be explained by appealing to general restrictions on on-line parsing plus natural constraints on the rule-writing vocabulary of grammars. The explanation avoids the problems with Marcus&apos; [19801 attempt to account for the same constraint. The argument is robust with respect to machine implementation, and thus avoids the problems that often arise when making detailed claims about parsing efficiency. It has the added virtue of unifying in the functional domain of parsing certain grammatically disparate phenomena, as well as making a strong claim about the way in which the grammar is actually embedded into an on-line sentence processor.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alfred Aho</author>
<author>Jeffrey Ullman</author>
</authors>
<title>119721 The Theory of Parsing Translation,</title>
<journal>and Compiling,</journal>
<volume>1</volume>
<publisher>Prentice-Flail.</publisher>
<marker>Aho, Ullman, </marker>
<rawString>Aho, Alfred and Ullman, Jeffrey 119721 The Theory of Parsing Translation, and Compiling, vol. 1., Prentice-Flail.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chomsky</author>
</authors>
<title>Noam [19731 &amp;quot;Conditions on Transformations,&amp;quot;in</title>
<booktitle>d Festschrift for Morris Halle.</booktitle>
<editor>S. Anderson &amp; P Kiparsky, eds.</editor>
<location>Holt, Rinehart and Winston.</location>
<marker>Chomsky, </marker>
<rawString>Chomsky. Noam [19731 &amp;quot;Conditions on Transformations,&amp;quot;in S. Anderson &amp; P Kiparsky, eds. d Festschrift for Morris Halle. Holt, Rinehart and Winston.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Since</author>
</authors>
<title>a Is ungovet nett iff a governed is false, and a plowed is a bounded predicate. iciii icstri,ted to onighly it sunlit: maximal projection (at worst an S).</title>
<marker>Since, </marker>
<rawString>R Since a Is ungovet nett iff a governed is false, and a plowed is a bounded predicate. iciii icstri,ted to onighly it sunlit: maximal projection (at worst an S).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Frederick DeRemer</author>
</authors>
<title>19691 Practical Transhaws for LR(k) languages,</title>
<institution>MD Department of Electrical Engineering and Computer Science.</institution>
<note>PhD dissertation,</note>
<marker>DeRemer, </marker>
<rawString>DeRemer, Frederick [19691 Practical Transhaws for LR(k) languages, PhD dissertation, MD Department of Electrical Engineering and Computer Science.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Floyd</author>
</authors>
<title>19641 &amp;quot;Bounded-context syntactic analysis,&amp;quot;</title>
<journal>Communications of the Association for Computing A.lachinery,</journal>
<volume>7</volume>
<pages>62--66</pages>
<marker>Floyd, </marker>
<rawString>Floyd, Robert [19641 &amp;quot;Bounded-context syntactic analysis,&amp;quot; Communications of the Association for Computing A.lachinery, 7, pp. 62-66.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>19811 &amp;quot;Unbounded dependencies and coordinate structure,&amp;quot;</title>
<journal>Linguistic Inquiry,</journal>
<volume>12</volume>
<pages>155--184</pages>
<marker>Gazdar, </marker>
<rawString>Gazdar, Gerald [19811 &amp;quot;Unbounded dependencies and coordinate structure,&amp;quot; Linguistic Inquiry, 12:2 155-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert</author>
<author>Weinberg</author>
</authors>
<title>Preposition stranding and case theory,&amp;quot; Linguistic Inquiry,</title>
<date>1981</date>
<pages>12--1</pages>
<marker>Norbert, Weinberg, 1981</marker>
<rawString>Hornstein. Norbert and Weinberg, Amy (19811 &amp;quot;Preposition stranding and case theory,&amp;quot; Linguistic Inquiry, 12:1.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>19801 A Theory of Syntactic Recognition for Natural Language,</title>
<publisher>MIT Press</publisher>
<marker>Marcus, </marker>
<rawString>Marcus, Mitchell [19801 A Theory of Syntactic Recognition for Natural Language, MIT Press</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>