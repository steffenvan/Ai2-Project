<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000159">
<title confidence="0.945036">
Rule Filtering by Pattern for Efficient Hierarchical Translation
</title>
<author confidence="0.8332055">
Gonzalo Iglesias* Adri`a de Gispert‡
Eduardo R. Banga* William Byrne‡
</author>
<affiliation confidence="0.949247">
* University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
</affiliation>
<email confidence="0.926123">
{giglesia,erbanga}@gts.tsc.uvigo.es
</email>
<affiliation confidence="0.813038">
‡ University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
</affiliation>
<email confidence="0.995246">
{ad465,wjb31}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.993788" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857153846154">
We describe refinements to hierarchical
translation search procedures intended to
reduce both search errors and memory us-
age through modifications to hypothesis
expansion in cube pruning and reductions
in the size of the rule sets used in transla-
tion. Rules are put into syntactic classes
based on the number of non-terminals and
the pattern, and various filtering strate-
gies are then applied to assess the impact
on translation speed and quality. Results
are reported on the 2008 NIST Arabic-to-
English evaluation task.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923818181818">
Hierarchical phrase-based translation (Chiang,
2005) has emerged as one of the dominant cur-
rent approaches to statistical machine translation.
Hiero translation systems incorporate many of
the strengths of phrase-based translation systems,
such as feature-based translation and strong tar-
get language models, while also allowing flexi-
ble translation and movement based on hierarchi-
cal rules extracted from aligned parallel text. The
approach has been widely adopted and reported to
be competitive with other large-scale data driven
approaches, e.g. (Zollmann et al., 2008).
Large-scale hierarchical SMT involves auto-
matic rule extraction from aligned parallel text,
model parameter estimation, and the use of cube
pruning k-best list generation in hierarchical trans-
lation. The number of hierarchical rules extracted
far exceeds the number of phrase translations typ-
ically found in aligned text. While this may lead
to improved translation quality, there is also the
risk of lengthened translation times and increased
memory usage, along with possible search errors
due to the pruning procedures needed in search.
We describe several techniques to reduce mem-
ory usage and search errors in hierarchical trans-
lation. Memory usage can be reduced in cube
pruning (Chiang, 2007) through smart memoiza-
tion, and spreading neighborhood exploration can
be used to reduce search errors. However, search
errors can still remain even when implementing
simple phrase-based translation. We describe a
‘shallow’ search through hierarchical rules which
greatly speeds translation without any effect on
quality. We then describe techniques to analyze
and reduce the set of hierarchical rules. We do
this based on the structural properties of rules and
develop strategies to identify and remove redun-
dant or harmful rules. We identify groupings of
rules based on non-terminals and their patterns and
assess the impact on translation quality and com-
putational requirements for each given rule group.
We find that with appropriate filtering strategies
rule sets can be greatly reduced in size without im-
pact on translation performance.
</bodyText>
<sectionHeader confidence="0.757255" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.99176625">
The search and rule pruning techniques described
in the following sections add to a growing lit-
erature of refinements to the hierarchical phrase-
based SMT systems originally described by Chi-
ang (2005; 2007). Subsequent work has addressed
improvements and extensions to the search proce-
dure itself, the extraction of the hierarchical rules
needed for translation, and has also reported con-
trastive experiments with other SMT architectures.
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning
to improve translation speed. Venugopal et al.
(2007) introduce a Hiero variant with relaxed con-
straints for hypothesis recombination during pars-
ing; speed and results are comparable to those of
cube pruning, as described by Chiang (2007). Li
and Khudanpur (2008) report significant improve-
ments in translation speed by taking unseen n-
grams into account within cube pruning to mini-
mize language model requests. Dyer et al. (2008)
</bodyText>
<note confidence="0.922893">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997314">
380
</page>
<bodyText confidence="0.993942225">
extend the translation of source sentences to trans-
lation of input lattices following Chappelier et al.
(1999).
Extensions to Hiero Blunsom et al. (2008)
discuss procedures to combine discriminative la-
tent models with hierarchical SMT. The Syntax-
Augmented Machine Translation system (Zoll-
mann and Venugopal, 2006) incorporates target
language syntactic constituents in addition to the
synchronous grammars used in translation. Shen
at al. (2008) make use of target dependency trees
and a target dependency language model during
decoding. Marton and Resnik (2008) exploit shal-
low correspondences of hierarchical rules with
source syntactic constituents extracted from par-
allel text, an approach also investigated by Chiang
(2005). Zhang and Gildea (2006) propose bina-
rization for synchronous grammars as a means to
control search complexity arising from more com-
plex, syntactic, hierarchical rules sets.
Hierarchical rule extraction Zhang et al. (2008)
describe a linear algorithm, a modified version of
shift-reduce, to extract phrase pairs organized into
a tree from which hierarchical rules can be directly
extracted. Lopez (2007) extracts rules on-the-fly
from the training bitext during decoding, search-
ing efficiently for rule patterns using suffix arrays.
Analysis and Contrastive Experiments Zollman
et al. (2008) compare phrase-based, hierarchical
and syntax-augmented decoders for translation of
Arabic, Chinese, and Urdu into English, and they
find that attempts to expedite translation by simple
schemes which discard rules also degrade transla-
tion performance. Lopez (2008) explores whether
lexical reordering or the phrase discontiguity in-
herent in hierarchical rules explains improvements
over phrase-based systems. Hierarchical transla-
tion has also been used to great effect in combina-
tion with other translation architectures (e.g. (Sim
et al., 2007; Rosti et al., 2007)).
</bodyText>
<subsectionHeader confidence="0.527017">
1.2 Outline
</subsectionHeader>
<bodyText confidence="0.9989682">
The paper proceeds as follows. Section 2 de-
scribes memoization and spreading neighborhood
exploration in cube pruning intended to reduce
memory usage and search errors, respectively. A
detailed comparison with a simple phrase-based
system is presented. Section 3 describes pattern-
based rule filtering and various procedures to se-
lect rule sets for use in translation with an aim
to improving translation quality while minimizing
rule set size. Finally, Section 4 concludes.
</bodyText>
<sectionHeader confidence="0.43082" genericHeader="method">
2 Two Refinements in Cube Pruning
</sectionHeader>
<bodyText confidence="0.999935625">
Chiang (2007) introduced cube pruning to apply
language models in pruning during the generation
of k-best translation hypotheses via the application
of hierarchical rules in the CYK algorithm. In the
implementation of Hiero described here, there is
the parser itself, for which we use a variant of the
CYK algorithm closely related to CYK+ (Chap-
pelier and Rajman, 1998); it employs hypothesis
recombination, without pruning, while maintain-
ing back pointers. Before k-best list generation
with cube pruning, we apply a smart memoiza-
tion procedure intended to reduce memory con-
sumption during k-best list expansion. Within the
cube pruning algorithm we use spreading neigh-
borhood exploration to improve robustness in the
face of search errors.
</bodyText>
<subsectionHeader confidence="0.981016">
2.1 Smart Memoization
</subsectionHeader>
<bodyText confidence="0.9999341">
Each cell in the chart built by the CYK algorithm
contains all possible derivations of a span of the
source sentence being translated. After the parsing
stage is completed, it is possible to make a very ef-
ficient sweep through the backpointers of the CYK
grid to count how many times each cell will be ac-
cessed by the k-best generation algorithm. When
k-best list generation is running, the number of
times each cell is visited is logged so that, as each
cell is visited for the last time, the k-best list as-
sociated with each cell is deleted. This continues
until the one k-best list remaining at the top of the
chart spans the entire sentence. Memory reduc-
tions are substantial for longer sentences: for the
longest sentence in the tuning set described later
(105 words in length), smart memoization reduces
memory usage during the cube pruning stage from
2.1GB to 0.7GB. For average length sentences of
approx. 30 words, memory reductions of 30% are
typical.
</bodyText>
<subsectionHeader confidence="0.999788">
2.2 Spreading Neighborhood Exploration
</subsectionHeader>
<bodyText confidence="0.999929">
In generation of a k-best list of translations for
a source sentence span, every derivation is trans-
formed into a cube containing the possible trans-
lations arising from that derivation, along with
their translation and language model scores (Chi-
ang, 2007). These derivations may contain non-
terminals which must be expanded based on hy-
potheses generated by lower cells, which them-
</bodyText>
<page confidence="0.983484">
381
</page>
<table confidence="0.7818076">
HIERO MJ1 HIERO HIERO SHALLOW
X → (V2V1,V1V2) X → (y,α) X → (ys,αs)
X → (V ,V ) y, α E ({X} U T)+ X → (V ,V )
V → (s,t) V → (s,t)
s, t E T+ s, t E T+; ys, αs E ({V } U T)+
</table>
<tableCaption confidence="0.987208">
Table 1: Hierarchical grammars (not including glue rules). T is the set of terminals.
</tableCaption>
<bodyText confidence="0.998924966666667">
selves may contain non-terminals. For efficiency
each cube maintains a queue of hypotheses, called
here the frontier queue, ranked by translation and
language model score; it is from these frontier
queues that hypotheses are removed to create the
k-best list for each cell. When a hypothesis is ex-
tracted from a frontier queue, that queue is updated
by searching through the neighborhood of the ex-
tracted item to find novel hypotheses to add; if no
novel hypotheses are found, that queue necessar-
ily shrinks. This shrinkage can lead to search er-
rors. We therefore require that, when a hypothe-
sis is removed, new candidates must be added by
exploring a neighborhood which spreads from the
last extracted hypothesis. Each axis of the cube
is searched (here, to a depth of 20) until a novel
hypothesis is found. In this way, up to three new
candidates are added for each entry extracted from
a frontier queue.
Chiang (2007) describes an initialization pro-
cedure in which these frontier queues are seeded
with a single candidate per axis; we initialize each
frontier queue to a depth of bN-1+1, where Nnt is
the number of non-terminals in the derivation and
b is a search parameter set throughout to 10. By
starting with deep frontier queues and by forcing
them to grow during search we attempt to avoid
search errors by ensuring that the universe of items
within the frontier queues does not decrease as the
k-best lists are filled.
</bodyText>
<subsectionHeader confidence="0.9983475">
2.3 A Study of Hiero Search Errors in
Phrase-Based Translation
</subsectionHeader>
<bodyText confidence="0.9999262">
Experiments reported in this paper are based
on the NIST MT08 Arabic-to-English transla-
tion task. Alignments are generated over all al-
lowed parallel data, (∼150M words per language).
Features extracted from the alignments and used
in translation are in common use: target lan-
guage model, source-to-target and target-to-source
phrase translation models, word and rule penalties,
number of usages of the glue rule, source-to-target
and target-to-source lexical models, and three rule
</bodyText>
<figureCaption confidence="0.662102">
Figure 1: Spreading neighborhood exploration
</figureCaption>
<bodyText confidence="0.996320818181818">
within a cube, just before and after extraction
of the item C. Grey squares represent the fron-
tier queue; black squares are candidates already
extracted. Chiang (2007) would only consider
adding items X to the frontier queue, so the queue
would shrink. Spreading neighborhood explo-
ration adds candidates S to the frontier queue.
count features inspired by Bender et al. (2007).
MET (Och, 2003) iterative parameter estimation
under IBM BLEU is performed on the develop-
ment set. The English language used model is a
4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. In addition to the
MT08 set itself, we use a development set mt02-
05-tune formed from the odd numbered sentences
of the NIST MT02 through MT05 evaluation sets;
the even numbered sentences form the validation
set mt02-05-test. The mt02-05-tune set has 2,075
sentences.
We first compare the cube pruning decoder to
the TTM (Kumar et al., 2006), a phrase-based
SMT system implemented with Weighted Finite-
State Tansducers (Allauzen et al., 2007). The sys-
tem implements either a monotone phrase order
translation, or an MJ1 (maximum phrase jump of
1) reordering model (Kumar and Byrne, 2005).
Relative to the complex movement and translation
allowed by Hiero and other models, MJ1 is clearly
inferior (Dreyer et al., 2007); MJ1 was developed
with efficiency in mind so as to run with a mini-
mum of search errors in translation and to be eas-
ily and exactly realized via WFSTs. Even for the
</bodyText>
<page confidence="0.991846">
382
</page>
<bodyText confidence="0.996780130434783">
large models used in an evaluation task, the TTM
system is reported to run largely without pruning
(Blackwood et al., 2008).
The Hiero decoder can easily be made to
implement MJ1 reordering by allowing only a
restricted set of reordering rules in addition to
the usual glue rule, as shown in left-hand column
of Table 1, where T is the set of terminals.
Constraining Hiero in this way makes it possible
to compare its performance to the exact WFST
TTM implementation and to identify any search
errors made by Hiero.
Table 2 shows the lowercased IBM BLEU
scores obtained by the systems for mt02-05-tune
with monotone and reordered search, and with
MET-optimised parameters for MJ1 reordering.
For Hiero, an N-best list depth of 10,000 is used
throughout. In the monotone case, all phrase-
based systems perform similarly although Hiero
does make search errors. For simple MJ1 re-
ordering, the basic Hiero search procedure makes
many search errors and these lead to degradations
in BLEU. Spreading neighborhood expansion re-
duces the search errors and improves BLEU score
significantly but search errors remain a problem.
Search errors are even more apparent after MET.
This is not surprising, given that mt02-05-tune is
the set over which MET is run: MET drives up the
likelihood of good hypotheses at the expense of
poor hypotheses, but search errors often increase
due to the expanded dynamic range of the hypoth-
esis scores.
Our aim in these experiments was to demon-
strate that spreading neighborhood exploration can
aid in avoiding search errors. We emphasize that
we are not proposing that Hiero should be used to
implement reordering models such as MJ1 which
were created for completely different search pro-
cedures (e.g. WFST composition). However these
experiments do suggest that search errors may be
an issue, particularly as the search space grows
to include the complex long-range movement al-
lowed by the hierarchical rules. We next study
various filtering procedures to reduce hierarchi-
cal rule sets to find a balance between translation
speed, memory usage, and performance.
</bodyText>
<sectionHeader confidence="0.919689" genericHeader="method">
3 Rule Filtering by Pattern
</sectionHeader>
<bodyText confidence="0.7823935">
Hierarchical rules X → (γ,α) are composed of
sequences of terminals and non-terminals, which
</bodyText>
<table confidence="0.999008">
Monotone MJ1 MJ1+MET
BLEU SE BLEU SE BLEU SE
a 44.7 - 47.2 - 49.1 -
b 44.5 342 46.7 555 48.4 822
c 44.7 77 47.1 191 48.9 360
</table>
<tableCaption confidence="0.992255">
Table 2: Phrase-based TTM and Hiero perfor-
</tableCaption>
<bodyText confidence="0.984933860465116">
mance on mt02-05-tune for TTM (a), Hiero (b),
Hiero with spreading neighborhood exploration
(c). SE is the number of Hiero hypotheses with
search errors.
we call elements. In the source, a maximum of
two non-adjacent non-terminals is allowed (Chi-
ang, 2007). Leaving aside rules without non-
terminals (i.e. phrase pairs as used in phrase-
based translation), rules can be classed by their
number of non-terminals, N,,,t, and their number
of elements, Ne. There are 5 possible classes:
N,,,t.Ne= 1.2,1.3, 2.3, 2.4, 2.5.
During rule extraction we search each class sep-
arately to control memory usage. Furthermore, we
extract from alignments only those rules which are
relevant to our given test set; for computation of
backward translation probabilities we log general
counts of target-side rules but discard unneeded
rules. Even with this restriction, our initial ruleset
for mt02-05-tune exceeds 175M rules, of which
only 0.62M are simple phrase pairs.
The question is whether all these rules are
needed for translation. If the rule set can be re-
duced without reducing translation quality, both
memory efficiency and translation speed can be
increased. Previously published approaches to re-
ducing the rule set include: enforcing a mini-
mum span of two words per non-terminal (Lopez,
2008), which would reduce our set to 115M rules;
or a minimum count (mincount) threshold (Zoll-
mann et al., 2008), which would reduce our set
to 78M (mincount=2) or 57M (mincount=3) rules.
Shen et al. (2008) describe the result of filter-
ing rules by insisting that target-side rules are
well-formed dependency trees. This reduces their
rule set from 140M to 26M rules. This filtering
leads to a degradation in translation performance
(see Table 2 of Shen et al. (2008)), which they
counter by adding a dependency LM in translation.
As another reference point, Chiang (2007) reports
Chinese-to-English translation experiments based
on 5.5M rules.
Zollmann et al. (2008) report that filtering rules
</bodyText>
<page confidence="0.996557">
383
</page>
<bodyText confidence="0.999938875">
en masse leads to degradation in translation per-
formance. Rather than apply a coarse filtering,
such as a mincount for all rules, we follow a more
syntactic approach and further classify our rules
according to their pattern and apply different fil-
ters to each pattern depending on its value in trans-
lation. The premise is that some patterns are more
important than others.
</bodyText>
<subsectionHeader confidence="0.939897">
3.1 Rule Patterns
</subsectionHeader>
<table confidence="0.999775842105263">
Class Rule Pattern Types
Nnt.Ne (source , target)
(wX1 , wX1) 1185028
1.2 (wX1 , wX1w) 153130
(wX1 , X1w) 97889
1.3 (wX1w , wX1w) 32903522
(wX1w , wX1) 989540
2.3 (X1wX2 , X1wX2) 1554656
(X2wX1 , X1wX2) 39163
(wX1wX2 , wX1wX2) 26901823
(X1wX2w , X1wX2w) 26053969
2.4 (wX1wX2 , wX1wX2w) 2534510
(wX2wX1 , wX1wX2) 349176
(X2wX1w , X1wX2w) 259459
(wX1wX2w , wX1wX2w) 61704299
(wX1wX2w , wX1X2w) 3149516
2.5 (wX1wX2w , X1wX2w) 2330797
(wX2wX1w , wX1wX2w) 275810
(wX2wX1w , wX1X2w) 205801
</table>
<tableCaption confidence="0.974798">
Table 3: Hierarchical rule patterns classed by
</tableCaption>
<bodyText confidence="0.999173296296296">
number of non-terminals, Nnt, number of ele-
ments Ne, source and target patterns, and types in
the rule set extracted for mt02-05-tune.
Given a rule set, we define source patterns and
target patterns by replacing every sequence of
non-terminals by a single symbol ‘w’ (indicating
word, i.e. terminal string, w E T+). Each hierar-
chical rule has a unique source and target pattern
which together define the rule pattern.
By ignoring the identity and the number of ad-
jacent terminals, the rule pattern represents a nat-
ural generalization of any rule, capturing its struc-
ture and the type of reordering it encodes. In to-
tal, there are 66 possible rule patterns. Table 3
presents a few examples extracted for mt02-05-
tune, showing that some patterns are much more
diverse than others. For example, patterns with
two non-terminals (Nnt=2) are richer than pat-
terns with Nnt=1, as they cover many more dis-
tinct rules. Additionally, patterns with two non-
terminals which also have a monotonic relation-
ship between source and target non-terminals are
much more diverse than their reordered counter-
parts.
Some examples of extracted rules and their cor-
responding pattern follow, where Arabic is shown
in Buckwalter encoding.
</bodyText>
<equation confidence="0.945032666666667">
Pattern (wX1 , wX1w) :
(w+ qAl X1 , the X1said)
Pattern (wX1w , wX1) :
(fy X1kAnwn Al&gt;wl , on december X1)
Pattern (wX1wX2 , wX1wX2w) :
(Hl X1lAzmp X2 , a X1solution to the X2crisis)
</equation>
<subsectionHeader confidence="0.999436">
3.2 Building an Initial Rule Set
</subsectionHeader>
<bodyText confidence="0.999979903225807">
We describe a greedy approach to building a rule
set in which rules belonging to a pattern are added
to the rule set guided by the improvements they
yield on mt02-05-tune relative to the monotone
Hiero system described in the previous section.
We find that certain patterns seem not to con-
tribute to any improvement. This is particularly
significant as these patterns often encompass large
numbers of rules, as with patterns with match-
ing source and target patterns. For instance, we
found no improvement when adding the pattern
(X1w,X1w), of which there were 1.2M instances
(Table 3). Since concatenation is already possible
under the general glue rule, rules with this pattern
are redundant. By contrast, the much less frequent
reordered counterpart, i.e. the (wX1,X1w) pat-
tern (0.01M instances), provides substantial gains.
The situation is analogous for rules with two non-
terminals (Nnt=2).
Based on exploratory analyses (not reported
here, for space) an initial rule set was built by
excluding patterns reported in Table 4. In to-
tal, 171.5M rules are excluded, for a remaining
set of 4.2M rules, 3.5M of which are hierarchi-
cal. We acknowledge that adding rules in this way,
by greedy search, is less than ideal and inevitably
raises questions with respect to generality and re-
peatability. However in our experience this is a
robust approach, mainly because the initial trans-
lation system runs very fast; it is possible to run
many exploratory experiments in a short time.
</bodyText>
<page confidence="0.996846">
384
</page>
<table confidence="0.953920363636364">
Excluded Rules Types
a (X1w,X1w) , (wX1,wX1) 2332604
b (X1wX2,*) 2121594
c 52955792
(X1wX2w,X1wX2w) ,
(wX1 wX2,wX1 wX2 )
d (wX1wX2w,*) 69437146
e N,,,t.N,= 1.3 w mincount=5 32394578
f N,,,t.N,= 2.3 w mincount=5 166969
g N,,,t.N,= 2.4 w mincount=10 11465410
h N,,,t.N,= 2.5 w mincount=5 688804
</table>
<tableCaption confidence="0.99983">
Table 4: Rules excluded from the initial rule set.
</tableCaption>
<subsectionHeader confidence="0.896649">
3.3 Shallow versus Fully Hierarchical
Translation
</subsectionHeader>
<bodyText confidence="0.999845578947368">
In measuring the effectiveness of rules in transla-
tion, we also investigate whether a ‘fully hierarchi-
cal’ search is needed or whether a shallow search
is also effective. In constrast to full Hiero, in the
shallow search, only phrases are allowed to be sub-
stituted into non-terminals. The rules used in each
case can be expressed as shown in the 2nd and 3rd
columns of Table 1. Shallow search can be con-
sidered (loosely) to be a form of rule filtering.
As can be seen in Table 5 there is no impact on
BLEU, while translation speed increases by a fac-
tor of 7. Of course, these results are specific to this
Arabic-to-English translation task, and need not
be expected to carry over to other language pairs,
such as Chinese-to-English translation. However,
the impact of this search simplification is easy to
measure, and the gains can be significant enough,
that it may be worth investigation even for lan-
guages with complex long distance movement.
</bodyText>
<table confidence="0.99806675">
mt02-05- -tune -test
System Time BLEU BLEU
HIERO 14.0 52.1 51.5
HIERO - shallow 2.0 52.1 51.4
</table>
<tableCaption confidence="0.841002">
Table 5: Translation performance and time (in sec-
onds per word) for full vs. shallow Hiero.
</tableCaption>
<listItem confidence="0.852157692307692">
• Number of translations (NT). We keep the
NT most frequent α, i.e. each -y is allowed to
have at most NT rules.
• Number of reordered translations (NRT).
We keep the NRT most frequent α with
monotonic non-terminals and the NRT most
frequent α with reordered non-terminals.
• Count percentage (CP). We keep the most
frequent α until their aggregated number of
counts reaches a certain percentage CP of the
total counts of X --+ (-y,*). Some -y’s are al-
lowed to have more α’s than others, depend-
ing on their count distribution.
</listItem>
<bodyText confidence="0.998954">
Results applying these filters with various
thresholds are given in Table 6, including num-
ber of rules and decoding time. As shown, all
filters achieve at least a 50% speed-up in decod-
ing time by discarding 15% to 25% of the base-
line rules. Remarkably, performance is unaffected
when applying the simple NT and NRT filters
with a threshold of 20 translations. Finally, the
CM filter behaves slightly worse for thresholds of
90% for the same decoding time. For this reason,
we select NRT=20 as our general filter.
</bodyText>
<table confidence="0.999016545454545">
mt02-05- -tune -test
Filter Time Rules BLEU BLEU
baseline 2.0 4.20 52.1 51.4
NT=10 0.8 3.25 52.0 51.3
NT=15 0.8 3.43 52.0 51.3
NT=20 0.8 3.56 52.1 51.4
NRT=10 0.9 3.29 52.0 51.3
NRT=15 1.0 3.48 52.0 51.4
NRT=20 1.0 3.59 52.1 51.4
CP=50 0.7 2.56 51.4 50.9
CP=90 1.0 3.60 52.0 51.3
</table>
<tableCaption confidence="0.969205">
Table 6: Impact of general rule filters on transla-
</tableCaption>
<bodyText confidence="0.7248765">
tion (IBM BLEU), time (in seconds per word) and
number of rules (in millions).
</bodyText>
<subsectionHeader confidence="0.967588">
3.4 Individual Rule Filters
</subsectionHeader>
<bodyText confidence="0.9997936">
We now filter rules individually (not by class) ac-
cording to their number of translations. For each
fixed -y V T+ (i.e. with at least 1 non-terminal),
we define the following filters over rules X --+
(-y,α):
</bodyText>
<subsectionHeader confidence="0.988686">
3.5 Pattern-based Rule Filters
</subsectionHeader>
<bodyText confidence="0.999951333333333">
In this section we first reconsider whether reintro-
ducing the monotonic rules (originally excluded as
described in rows ’b’, ’c’, ’d’ in Table 4) affects
performance. Results are given in the upper rows
of Table 7. For all classes, we find that reintroduc-
ing these rules increases the total number of rules
</bodyText>
<page confidence="0.996392">
385
</page>
<table confidence="0.999369153846154">
mt02-05- -tune -test
Nnt.Ne Filter Time Rules BLEU BLEU
baseline NRT=20 1.0 3.59 52.1 51.4
2.3 +monotone 1.1 4.08 51.5 51.1
2.4 +monotone 2.0 11.52 51.6 51.0
2.5 +monotone 1.8 6.66 51.7 51.2
1.3 mincount=3 1.0 5.61 52.1 51.3
2.3 mincount=1 1.2 3.70 52.1 51.4
2.4 mincount=5 1.8 4.62 52.0 51.3
2.4 mincount=15 1.0 3.37 52.0 51.4
2.5 mincount=1 1.1 4.27 52.2 51.5
1.2 mincount=5 1.0 3.51 51.8 51.3
1.2 mincount=10 1.0 3.50 51.7 51.2
</table>
<tableCaption confidence="0.999705">
Table 7: Effect of pattern-based rule filters. Time in seconds per word. Rules in millions.
</tableCaption>
<bodyText confidence="0.999944304347826">
substantially, despite the NRT=20 filter, but leads
to degradation in translation performance.
We next reconsider the mincount threshold val-
ues for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi-
nally described in Table 4 (rows ’e’ to ’h’). Results
under various mincount cutoffs for each class are
given in Table 7 (middle five rows). For classes
2.3 and 2.5, the mincount cutoff can be reduced
to 1 (i.e. all rules are kept) with slight translation
improvements. In contrast, reducing the cutoff for
classes 1.3 and 2.4 to 3 and 5, respectively, adds
many more rules with no increase in performance.
We also find that increasing the cutoff to 15 for
class 2.4 yields the same results with a smaller rule
set. Finally, we consider further filtering applied to
class 1.2 with mincount 5 and 10 (final two rows
in Table 7). The number of rules is largely un-
changed, but translation performance drops con-
sistently as more rules are removed.
Based on these experiments, we conclude that it
is better to apply separate mincount thresholds to
the classes to obtain optimal performance with a
minimum size rule set.
</bodyText>
<subsectionHeader confidence="0.996579">
3.6 Large Language Models and Evaluation
</subsectionHeader>
<bodyText confidence="0.99992075">
Finally, in this section we report results of our
shallow hierarchical system with the 2.5 min-
count=1 configuration from Table 7, after includ-
ing the following N-best list rescoring steps.
</bodyText>
<listItem confidence="0.948164">
• Large-LM rescoring. We build sentence-
</listItem>
<bodyText confidence="0.810985166666667">
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ∼4.7B words of English newswire text,
and apply them to rescore each 10000-best
list.
• Minimum Bayes Risk (MBR). We then rescore
the first 1000-best hypotheses with MBR,
taking the negative sentence level BLEU
score as the loss function to minimise (Ku-
mar and Byrne, 2004).
Table 8 shows results for mt02-05-tune, mt02-
05-test, the NIST subsets from the MT06 evalu-
ation (mt06-nist-nw for newswire data and mt06-
nist-ng for newsgroup) and mt08, as measured by
lowercased IBM BLEU and TER (Snover et al.,
2006). Mixed case NIST BLEU for this system on
mt08 is 42.5. This is directly comparable to offi-
cial MT08 evaluation results1.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999964923076923">
This paper focuses on efficient large-scale hierar-
chical translation while maintaining good trans-
lation quality. Smart memoization and spreading
neighborhood exploration during cube pruning are
described and shown to reduce memory consump-
tion and Hiero search errors using a simple phrase-
based system as a contrast.
We then define a general classification of hi-
erarchical rules, based on their number of non-
terminals, elements and their patterns, for refined
extraction and filtering.
For a large-scale Arabic-to-English task, we
show that shallow hierarchical decoding is as good
</bodyText>
<footnote confidence="0.985109">
1Full MT08 results are available at
http://www.nist.gov/speech/tests/mt/2008/. It is worth
noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
</footnote>
<page confidence="0.990466">
386
</page>
<table confidence="0.996288">
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08
HIERO+MET 52.2 / 41.6 51.5 / 42.2 48.4 / 43.6 35.3 / 53.2 42.5 / 48.6
+rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1
</table>
<tableCaption confidence="0.9892735">
Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language mod-
els and MBR decoding.
</tableCaption>
<bodyText confidence="0.999948260869565">
as fully hierarchical search and that decoding time
is dramatically decreased. In addition, we describe
individual rule filters based on the distribution of
translations with further time reductions at no cost
in translation scores. This is in direct contrast
to recent reported results in which other filtering
strategies lead to degraded performance (Shen et
al., 2008; Zollmann et al., 2008).
We find that certain patterns are of much greater
value in translation than others and that separate
minimum count filters should be applied accord-
ingly. Some patterns were found to be redundant
or harmful, in particular those with two monotonic
non-terminals. Moreover, we show that the value
of a pattern is not directly related to the number of
rules it encompasses, which can lead to discarding
large numbers of rules as well as to dramatic speed
improvements.
Although reported experiments are only for
Arabic-to-English translation, we believe the ap-
proach will prove to be general. Pattern relevance
will vary for other language pairs, but we expect
filtering strategies to be equally worth pursuing.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999265">
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government re-
search grant BES-2007-15956 (project TEC2006-
13694-C03-03).
</bodyText>
<sectionHeader confidence="0.998614" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999744105263158">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11–23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396–401.
Graeme Blackwood, Adri`a de Gispert, Jamie Brunning,
and William Byrne. 2008. Large-scale statistical
machine translation with weighted finite state trans-
ducers. In Proceedings ofFSMNLP, pages 27–35.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200–208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858–867.
Jean-C´edric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133–137.
Jean-C´edric Chappelier, Martin Rajman, Ram´on
Arag¨u´es, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95–104.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings ofACL, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT
using efficient BLEU oracle computation. In Pro-
ceedings of SSST, NAACL-HLT 2007 / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings ofACL-HLT, pages 1012–1020.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings ofACL, pages 144–151.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169–
176.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of HLT-EMNLP, pages
161–168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35–75.
</reference>
<page confidence="0.983624">
387
</page>
<reference confidence="0.98693901754386">
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the ACL-HLT Second Workshop
on Syntax and Structure in Statistical Translation,
pages 10–18.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings ofEMNLP-
CONLL, pages 976–985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
pages 505–512.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT, pages 1003–
1011.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160–167.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of HLT-
NAACL, pages 228–235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-HLT, pages 577–585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation
system combination. In Proceedings of ICASSP,
volume 4, pages 105–108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings ofAMTA, pages 223–231.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings ofHLT-NAACL, pages 500–507.
Hao Zhang and Daniel Gildea. 2006. Synchronous
binarization for machine translation. In Proceedings
ofHLT-NAACL, pages 256–263.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING, pages 1081–1088.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of NAACL Workshop on Statistical
Machine Translation, pages 138–141.
Andreas Zollmann, Ashish Venugopal, Franz Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of COLING, pages
1145–1152.
</reference>
<page confidence="0.998247">
388
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.174239">
<title confidence="0.999212">Rule Filtering by Pattern for Efficient Hierarchical Translation</title>
<author confidence="0.9315635">Adri`a de_R William</author>
<address confidence="0.452772">of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain</address>
<email confidence="0.750985">giglesia@gts.tsc.uvigo.es</email>
<email confidence="0.750985">erbanga@gts.tsc.uvigo.es</email>
<address confidence="0.544002">of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.</address>
<abstract confidence="0.992762357142857">We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-to- English evaluation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of CIAA,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="12091" citStr="Allauzen et al., 2007" startWordPosition="1888" endWordPosition="1891">opment set. The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted FiniteState Tansducers (Allauzen et al., 2007). The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the 382 large models used in an evaluation task, the TTM system is reported to run largely without pruning (Blackwood et al., 2008). The Hiero decoder can easily </context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Evgeny Matusov</author>
<author>Stefan Hahn</author>
<author>Sasa Hasan</author>
<author>Shahram Khadivi</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Arabic-to-English spoken language translation system.</title>
<date>2007</date>
<booktitle>In Proceedings of ASRU,</booktitle>
<pages>396--401</pages>
<contexts>
<context position="11380" citStr="Bender et al. (2007)" startWordPosition="1773" endWordPosition="1776">del, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the frontier queue; black squares are candidates already extracted. Chiang (2007) would only consider adding items X to the frontier queue, so the queue would shrink. Spreading neighborhood exploration adds candidates S to the frontier queue. count features inspired by Bender et al. (2007). MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set. The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et</context>
</contexts>
<marker>Bender, Matusov, Hahn, Hasan, Khadivi, Ney, 2007</marker>
<rawString>Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa Hasan, Shahram Khadivi, and Hermann Ney. 2007. The RWTH Arabic-to-English spoken language translation system. In Proceedings of ASRU, pages 396–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adri`a de Gispert</author>
<author>Jamie Brunning</author>
<author>William Byrne</author>
</authors>
<title>Large-scale statistical machine translation with weighted finite state transducers.</title>
<date>2008</date>
<booktitle>In Proceedings ofFSMNLP,</booktitle>
<pages>27--35</pages>
<marker>Blackwood, de Gispert, Brunning, Byrne, 2008</marker>
<rawString>Graeme Blackwood, Adri`a de Gispert, Jamie Brunning, and William Byrne. 2008. Large-scale statistical machine translation with weighted finite state transducers. In Proceedings ofFSMNLP, pages 27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>200--208</pages>
<contexts>
<context position="4322" citStr="Blunsom et al. (2008)" startWordPosition="641" endWordPosition="644">arsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006)</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL-HLT, pages 200–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="26224" citStr="Brants et al., 2007" startWordPosition="4253" endWordPosition="4256">le 7). The number of rules is largely unchanged, but translation performance drops consistently as more rules are removed. Based on these experiments, we conclude that it is better to apply separate mincount thresholds to the classes to obtain optimal performance with a minimum size rule set. 3.6 Large Language Models and Evaluation Finally, in this section we report results of our shallow hierarchical system with the 2.5 mincount=1 configuration from Table 7, after including the following N-best list rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore each 10000-best list. • Minimum Bayes Risk (MBR). We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise (Kumar and Byrne, 2004). Table 8 shows results for mt02-05-tune, mt02- 05-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06- nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006). Mixed case NIST BLEU for this system on mt08 i</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLP-ACL, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A generalized CYK algorithm for parsing stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proceedings of TAPD,</booktitle>
<pages>133--137</pages>
<contexts>
<context position="6947" citStr="Chappelier and Rajman, 1998" startWordPosition="1025" endWordPosition="1029">ed. Section 3 describes patternbased rule filtering and various procedures to select rule sets for use in translation with an aim to improving translation quality while minimizing rule set size. Finally, Section 4 concludes. 2 Two Refinements in Cube Pruning Chiang (2007) introduced cube pruning to apply language models in pruning during the generation of k-best translation hypotheses via the application of hierarchical rules in the CYK algorithm. In the implementation of Hiero described here, there is the parser itself, for which we use a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998); it employs hypothesis recombination, without pruning, while maintaining back pointers. Before k-best list generation with cube pruning, we apply a smart memoization procedure intended to reduce memory consumption during k-best list expansion. Within the cube pruning algorithm we use spreading neighborhood exploration to improve robustness in the face of search errors. 2.1 Smart Memoization Each cell in the chart built by the CYK algorithm contains all possible derivations of a span of the source sentence being translated. After the parsing stage is completed, it is possible to make a very ef</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-C´edric Chappelier and Martin Rajman. 1998. A generalized CYK algorithm for parsing stochastic CFG. In Proceedings of TAPD, pages 133–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
<author>Ram´on Arag¨u´es</author>
<author>Antoine Rozenknop</author>
</authors>
<title>Lattice parsing for speech recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of TALN,</booktitle>
<pages>95--104</pages>
<marker>Chappelier, Rajman, Arag¨u´es, Rozenknop, 1999</marker>
<rawString>Jean-C´edric Chappelier, Martin Rajman, Ram´on Arag¨u´es, and Antoine Rozenknop. 1999. Lattice parsing for speech recognition. In Proceedings of TALN, pages 95–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="949" citStr="Chiang, 2005" startWordPosition="131" endWordPosition="132">1}@eng.cam.ac.uk Abstract We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-toEnglish evaluation task. 1 Introduction Hierarchical phrase-based translation (Chiang, 2005) has emerged as one of the dominant current approaches to statistical machine translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (Zollmann et al., 2008). Large-scale hierarchical SMT involves automatic rule extraction from aligned p</context>
<context position="3228" citStr="Chiang (2005" startWordPosition="475" endWordPosition="477">es of rules and develop strategies to identify and remove redundant or harmful rules. We identify groupings of rules based on non-terminals and their patterns and assess the impact on translation quality and computational requirements for each given rule group. We find that with appropriate filtering strategies rule sets can be greatly reduced in size without impact on translation performance. 1.1 Related Work The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) repor</context>
<context position="4897" citStr="Chiang (2005)" startWordPosition="724" endWordPosition="725">ensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings ofACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2170" citStr="Chiang, 2007" startWordPosition="314" endWordPosition="315">text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical translation. The number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text. While this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search. We describe several techniques to reduce memory usage and search errors in hierarchical translation. Memory usage can be reduced in cube pruning (Chiang, 2007) through smart memoization, and spreading neighborhood exploration can be used to reduce search errors. However, search errors can still remain even when implementing simple phrase-based translation. We describe a ‘shallow’ search through hierarchical rules which greatly speeds translation without any effect on quality. We then describe techniques to analyze and reduce the set of hierarchical rules. We do this based on the structural properties of rules and develop strategies to identify and remove redundant or harmful rules. We identify groupings of rules based on non-terminals and their patt</context>
<context position="3516" citStr="Chiang (2007)" startWordPosition="517" endWordPosition="518">iate filtering strategies rule sets can be greatly reduced in size without impact on translation performance. 1.1 Related Work The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, 30 March – 3 April 2009. </context>
<context position="6591" citStr="Chiang (2007)" startWordPosition="971" endWordPosition="972"> other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)). 1.2 Outline The paper proceeds as follows. Section 2 describes memoization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively. A detailed comparison with a simple phrase-based system is presented. Section 3 describes patternbased rule filtering and various procedures to select rule sets for use in translation with an aim to improving translation quality while minimizing rule set size. Finally, Section 4 concludes. 2 Two Refinements in Cube Pruning Chiang (2007) introduced cube pruning to apply language models in pruning during the generation of k-best translation hypotheses via the application of hierarchical rules in the CYK algorithm. In the implementation of Hiero described here, there is the parser itself, for which we use a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998); it employs hypothesis recombination, without pruning, while maintaining back pointers. Before k-best list generation with cube pruning, we apply a smart memoization procedure intended to reduce memory consumption during k-best list expansion.</context>
<context position="8598" citStr="Chiang, 2007" startWordPosition="1297" endWordPosition="1299">e. Memory reductions are substantial for longer sentences: for the longest sentence in the tuning set described later (105 words in length), smart memoization reduces memory usage during the cube pruning stage from 2.1GB to 0.7GB. For average length sentences of approx. 30 words, memory reductions of 30% are typical. 2.2 Spreading Neighborhood Exploration In generation of a k-best list of translations for a source sentence span, every derivation is transformed into a cube containing the possible translations arising from that derivation, along with their translation and language model scores (Chiang, 2007). These derivations may contain nonterminals which must be expanded based on hypotheses generated by lower cells, which them381 HIERO MJ1 HIERO HIERO SHALLOW X → (V2V1,V1V2) X → (y,α) X → (ys,αs) X → (V ,V ) y, α E ({X} U T)+ X → (V ,V ) V → (s,t) V → (s,t) s, t E T+ s, t E T+; ys, αs E ({V } U T)+ Table 1: Hierarchical grammars (not including glue rules). T is the set of terminals. selves may contain non-terminals. For efficiency each cube maintains a queue of hypotheses, called here the frontier queue, ranked by translation and language model score; it is from these frontier queues that hypo</context>
<context position="9904" citStr="Chiang (2007)" startWordPosition="1541" endWordPosition="1542">om a frontier queue, that queue is updated by searching through the neighborhood of the extracted item to find novel hypotheses to add; if no novel hypotheses are found, that queue necessarily shrinks. This shrinkage can lead to search errors. We therefore require that, when a hypothesis is removed, new candidates must be added by exploring a neighborhood which spreads from the last extracted hypothesis. Each axis of the cube is searched (here, to a depth of 20) until a novel hypothesis is found. In this way, up to three new candidates are added for each entry extracted from a frontier queue. Chiang (2007) describes an initialization procedure in which these frontier queues are seeded with a single candidate per axis; we initialize each frontier queue to a depth of bN-1+1, where Nnt is the number of non-terminals in the derivation and b is a search parameter set throughout to 10. By starting with deep frontier queues and by forcing them to grow during search we attempt to avoid search errors by ensuring that the universe of items within the frontier queues does not decrease as the k-best lists are filled. 2.3 A Study of Hiero Search Errors in Phrase-Based Translation Experiments reported in thi</context>
<context position="11171" citStr="Chiang (2007)" startWordPosition="1741" endWordPosition="1742">ranslation task. Alignments are generated over all allowed parallel data, (∼150M words per language). Features extracted from the alignments and used in translation are in common use: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the frontier queue; black squares are candidates already extracted. Chiang (2007) would only consider adding items X to the frontier queue, so the queue would shrink. Spreading neighborhood exploration adds candidates S to the frontier queue. count features inspired by Bender et al. (2007). MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set. The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the N</context>
<context position="15163" citStr="Chiang, 2007" startWordPosition="2404" endWordPosition="2406"> a balance between translation speed, memory usage, and performance. 3 Rule Filtering by Pattern Hierarchical rules X → (γ,α) are composed of sequences of terminals and non-terminals, which Monotone MJ1 MJ1+MET BLEU SE BLEU SE BLEU SE a 44.7 - 47.2 - 49.1 - b 44.5 342 46.7 555 48.4 822 c 44.7 77 47.1 191 48.9 360 Table 2: Phrase-based TTM and Hiero performance on mt02-05-tune for TTM (a), Hiero (b), Hiero with spreading neighborhood exploration (c). SE is the number of Hiero hypotheses with search errors. we call elements. In the source, a maximum of two non-adjacent non-terminals is allowed (Chiang, 2007). Leaving aside rules without nonterminals (i.e. phrase pairs as used in phrasebased translation), rules can be classed by their number of non-terminals, N,,,t, and their number of elements, Ne. There are 5 possible classes: N,,,t.Ne= 1.2,1.3, 2.3, 2.4, 2.5. During rule extraction we search each class separately to control memory usage. Furthermore, we extract from alignments only those rules which are relevant to our given test set; for computation of backward translation probabilities we log general counts of target-side rules but discard unneeded rules. Even with this restriction, our initi</context>
<context position="16757" citStr="Chiang (2007)" startWordPosition="2660" endWordPosition="2661">an of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules. Shen et al. (2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees. This reduces their rule set from 140M to 26M rules. This filtering leads to a degradation in translation performance (see Table 2 of Shen et al. (2008)), which they counter by adding a dependency LM in translation. As another reference point, Chiang (2007) reports Chinese-to-English translation experiments based on 5.5M rules. Zollmann et al. (2008) report that filtering rules 383 en masse leads to degradation in translation performance. Rather than apply a coarse filtering, such as a mincount for all rules, we follow a more syntactic approach and further classify our rules according to their pattern and apply different filters to each pattern depending on its value in translation. The premise is that some patterns are more important than others. 3.1 Rule Patterns Class Rule Pattern Types Nnt.Ne (source , target) (wX1 , wX1) 1185028 1.2 (wX1 , </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Keith Hall</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Comparing reordering constraints for SMT using efficient BLEU oracle computation.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="12367" citStr="Dreyer et al., 2007" startWordPosition="1933" endWordPosition="1936">bered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted FiniteState Tansducers (Allauzen et al., 2007). The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the 382 large models used in an evaluation task, the TTM system is reported to run largely without pruning (Blackwood et al., 2008). The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual glue rule, as shown in left-hand column of Table 1, where T is the set of terminals. Constraining Hiero in this way makes it possible to compare its performance </context>
</contexts>
<marker>Dreyer, Hall, Khudanpur, 2007</marker>
<rawString>Markus Dreyer, Keith Hall, and Sanjeev Khudanpur. 2007. Comparing reordering constraints for SMT using efficient BLEU oracle computation. In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="3988" citStr="Dyer et al. (2008)" startWordPosition="589" endWordPosition="592">eded for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in trans</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings ofACL-HLT, pages 1012–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="3516" citStr="Huang and Chiang (2007)" startWordPosition="515" endWordPosition="518">th appropriate filtering strategies rule sets can be greatly reduced in size without impact on translation performance. 1.1 Related Work The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, 30 March – 3 April 2009. </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings ofACL, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="26538" citStr="Kumar and Byrne, 2004" startWordPosition="4303" endWordPosition="4307"> Models and Evaluation Finally, in this section we report results of our shallow hierarchical system with the 2.5 mincount=1 configuration from Table 7, after including the following N-best list rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore each 10000-best list. • Minimum Bayes Risk (MBR). We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise (Kumar and Byrne, 2004). Table 8 shows results for mt02-05-tune, mt02- 05-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06- nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006). Mixed case NIST BLEU for this system on mt08 is 42.5. This is directly comparable to official MT08 evaluation results1. 4 Conclusions This paper focuses on efficient large-scale hierarchical translation while maintaining good translation quality. Smart memoization and spreading neighborhood exploration during cube pruning are described and shown to reduce me</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of HLT-NAACL, pages 169– 176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="12236" citStr="Kumar and Byrne, 2005" startWordPosition="1912" endWordPosition="1915">he English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted FiniteState Tansducers (Allauzen et al., 2007). The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the 382 large models used in an evaluation task, the TTM system is reported to run largely without pruning (Blackwood et al., 2008). The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual glue rule, as shown in left-ha</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of HLT-EMNLP, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer translation template model for statistical machine translation.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="11991" citStr="Kumar et al., 2006" startWordPosition="1874" endWordPosition="1877">. (2007). MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set. The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted FiniteState Tansducers (Allauzen et al., 2007). The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the 382 large models used in an evaluation task, the TTM syste</context>
</contexts>
<marker>Kumar, Deng, Byrne, 2006</marker>
<rawString>Shankar Kumar, Yonggang Deng, and William Byrne. 2006. A weighted finite state transducer translation template model for statistical machine translation. Natural Language Engineering, 12(1):35–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-HLT Second Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="3822" citStr="Li and Khudanpur (2008)" startWordPosition="562" endWordPosition="565">lly described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugment</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In Proceedings of the ACL-HLT Second Workshop on Syntax and Structure in Statistical Translation, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLPCONLL,</booktitle>
<pages>976--985</pages>
<contexts>
<context position="5301" citStr="Lopez (2007)" startWordPosition="784" endWordPosition="785"> during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hi</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proceedings ofEMNLPCONLL, pages 976–985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="5754" citStr="Lopez (2008)" startWordPosition="846" endWordPosition="847">orithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)). 1.2 Outline The paper proceeds as follows. Section 2 describes memoization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively. A detailed comparison with a simple phrase-based system is presented. Section 3 describes patternbase</context>
<context position="16190" citStr="Lopez, 2008" startWordPosition="2566" endWordPosition="2567">to our given test set; for computation of backward translation probabilities we log general counts of target-side rules but discard unneeded rules. Even with this restriction, our initial ruleset for mt02-05-tune exceeds 175M rules, of which only 0.62M are simple phrase pairs. The question is whether all these rules are needed for translation. If the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can be increased. Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules. Shen et al. (2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees. This reduces their rule set from 140M to 26M rules. This filtering leads to a degradation in translation performance (see Table 2 of Shen et al. (2008)), which they counter by adding a dependency LM in translation. As another reference point, Chiang (2007) reports Chinese-to-English trans</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-scale translation models via pattern matching. In Proceedings of COLING, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="4731" citStr="Marton and Resnik (2008)" startWordPosition="699" endWordPosition="702">il 2009. c�2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly fro</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-HLT, pages 1003– 1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11397" citStr="Och, 2003" startWordPosition="1778" endWordPosition="1779">arget-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the frontier queue; black squares are candidates already extracted. Chiang (2007) would only consider adding items X to the frontier queue, so the queue would shrink. Spreading neighborhood exploration adds candidates S to the frontier queue. count features inspired by Bender et al. (2007). MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set. The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a ph</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="6055" citStr="Rosti et al., 2007" startWordPosition="888" endWordPosition="891">sis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)). 1.2 Outline The paper proceeds as follows. Section 2 describes memoization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively. A detailed comparison with a simple phrase-based system is presented. Section 3 describes patternbased rule filtering and various procedures to select rule sets for use in translation with an aim to improving translation quality while minimizing rule set size. Finally, Section 4 concludes. 2 Two Refinements in Cube Pruning Chiang (2007) introduced cube pruning to apply language models in pruning dur</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie Dorr. 2007. Combining outputs from multiple machine translation systems. In Proceedings of HLTNAACL, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="16391" citStr="Shen et al. (2008)" startWordPosition="2599" endWordPosition="2602"> for mt02-05-tune exceeds 175M rules, of which only 0.62M are simple phrase pairs. The question is whether all these rules are needed for translation. If the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can be increased. Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules. Shen et al. (2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees. This reduces their rule set from 140M to 26M rules. This filtering leads to a degradation in translation performance (see Table 2 of Shen et al. (2008)), which they counter by adding a dependency LM in translation. As another reference point, Chiang (2007) reports Chinese-to-English translation experiments based on 5.5M rules. Zollmann et al. (2008) report that filtering rules 383 en masse leads to degradation in translation performance. Rather than apply a coarse filtering, such as a </context>
<context position="28406" citStr="Shen et al., 2008" startWordPosition="4595" endWordPosition="4598">/ 41.6 51.5 / 42.2 48.4 / 43.6 35.3 / 53.2 42.5 / 48.6 +rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1 Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language models and MBR decoding. as fully hierarchical search and that decoding time is dramatically decreased. In addition, we describe individual rule filters based on the distribution of translations with further time reductions at no cost in translation scores. This is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance (Shen et al., 2008; Zollmann et al., 2008). We find that certain patterns are of much greater value in translation than others and that separate minimum count filters should be applied accordingly. Some patterns were found to be redundant or harmful, in particular those with two monotonic non-terminals. Moreover, we show that the value of a pattern is not directly related to the number of rules it encompasses, which can lead to discarding large numbers of rules as well as to dramatic speed improvements. Although reported experiments are only for Arabic-to-English translation, we believe the approach will prove </context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-HLT, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William Byrne</author>
<author>Mark Gales</author>
<author>Hichem Sahbi</author>
<author>Phil Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<volume>4</volume>
<pages>105--108</pages>
<contexts>
<context position="6034" citStr="Sim et al., 2007" startWordPosition="884" endWordPosition="887">ffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)). 1.2 Outline The paper proceeds as follows. Section 2 describes memoization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively. A detailed comparison with a simple phrase-based system is presented. Section 3 describes patternbased rule filtering and various procedures to select rule sets for use in translation with an aim to improving translation quality while minimizing rule set size. Finally, Section 4 concludes. 2 Two Refinements in Cube Pruning Chiang (2007) introduced cube pruning to apply language </context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William Byrne, Mark Gales, Hichem Sahbi, and Phil Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proceedings of ICASSP, volume 4, pages 105–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="26776" citStr="Snover et al., 2006" startWordPosition="4343" endWordPosition="4346">ld sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore each 10000-best list. • Minimum Bayes Risk (MBR). We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise (Kumar and Byrne, 2004). Table 8 shows results for mt02-05-tune, mt02- 05-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06- nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006). Mixed case NIST BLEU for this system on mt08 is 42.5. This is directly comparable to official MT08 evaluation results1. 4 Conclusions This paper focuses on efficient large-scale hierarchical translation while maintaining good translation quality. Smart memoization and spreading neighborhood exploration during cube pruning are described and shown to reduce memory consumption and Hiero search errors using a simple phrasebased system as a contrast. We then define a general classification of hierarchical rules, based on their number of nonterminals, elements and their patterns, for refined extra</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Vogel Stephan</author>
</authors>
<title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
<date>2007</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>500--507</pages>
<contexts>
<context position="3612" citStr="Venugopal et al. (2007)" startWordPosition="529" endWordPosition="532">ranslation performance. 1.1 Related Work The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 380 extend the translation of source sentences </context>
</contexts>
<marker>Venugopal, Zollmann, Stephan, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, and Vogel Stephan. 2007. An efficient two-pass approach to synchronous-CFG driven statistical MT. In Proceedings ofHLT-NAACL, pages 500–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="4922" citStr="Zhang and Gildea (2006)" startWordPosition="726" endWordPosition="729">o Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchic</context>
</contexts>
<marker>Zhang, Gildea, 2006</marker>
<rawString>Hao Zhang and Daniel Gildea. 2006. Synchronous binarization for machine translation. In Proceedings ofHLT-NAACL, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from wordlevel alignments in linear time.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="5120" citStr="Zhang et al. (2008)" startWordPosition="755" endWordPosition="758">t language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade tra</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammar rules from wordlevel alignments in linear time. In Proceedings of COLING, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="4482" citStr="Zollmann and Venugopal, 2006" startWordPosition="662" endWordPosition="666">s in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchic</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of NAACL Workshop on Statistical Machine Translation, pages 138–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1145--1152</pages>
<contexts>
<context position="1469" citStr="Zollmann et al., 2008" startWordPosition="205" endWordPosition="208"> Arabic-toEnglish evaluation task. 1 Introduction Hierarchical phrase-based translation (Chiang, 2005) has emerged as one of the dominant current approaches to statistical machine translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (Zollmann et al., 2008). Large-scale hierarchical SMT involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical translation. The number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text. While this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search. We describe several techniques to reduce memory usage and </context>
<context position="16297" citStr="Zollmann et al., 2008" startWordPosition="2582" endWordPosition="2586">s of target-side rules but discard unneeded rules. Even with this restriction, our initial ruleset for mt02-05-tune exceeds 175M rules, of which only 0.62M are simple phrase pairs. The question is whether all these rules are needed for translation. If the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can be increased. Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules. Shen et al. (2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees. This reduces their rule set from 140M to 26M rules. This filtering leads to a degradation in translation performance (see Table 2 of Shen et al. (2008)), which they counter by adding a dependency LM in translation. As another reference point, Chiang (2007) reports Chinese-to-English translation experiments based on 5.5M rules. Zollmann et al. (2008) report that filtering rules 383 en masse lea</context>
<context position="28430" citStr="Zollmann et al., 2008" startWordPosition="4599" endWordPosition="4602">48.4 / 43.6 35.3 / 53.2 42.5 / 48.6 +rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1 Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language models and MBR decoding. as fully hierarchical search and that decoding time is dramatically decreased. In addition, we describe individual rule filters based on the distribution of translations with further time reductions at no cost in translation scores. This is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance (Shen et al., 2008; Zollmann et al., 2008). We find that certain patterns are of much greater value in translation than others and that separate minimum count filters should be applied accordingly. Some patterns were found to be redundant or harmful, in particular those with two monotonic non-terminals. Moreover, we show that the value of a pattern is not directly related to the number of rules it encompasses, which can lead to discarding large numbers of rules as well as to dramatic speed improvements. Although reported experiments are only for Arabic-to-English translation, we believe the approach will prove to be general. Pattern r</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz Och, and Jay Ponte. 2008. A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT. In Proceedings of COLING, pages 1145–1152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>