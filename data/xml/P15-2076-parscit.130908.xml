<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001640">
<title confidence="0.698837">
Non-distributional Word Vector Representations
</title>
<author confidence="0.929385">
Manaal Faruqui and Chris Dyer
</author>
<affiliation confidence="0.816486666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.998478">
{mfaruqui, cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999733294117647">
Data-driven representation learning for
words is a technique of central importance
in NLP. While indisputably useful as a
source of features in downstream tasks,
such vectors tend to consist of uninter-
pretable components whose relationship to
the categories of traditional lexical seman-
tic theories is tenuous at best. We present
a method for constructing interpretable
word vectors from hand-crafted linguis-
tic resources like WordNet, FrameNet etc.
These vectors are binary (i.e, contain only
0 and 1) and are 99.9% sparse. We analyze
their performance on state-of-the-art eval-
uation methods for distributional models
of word vectors and find they are competi-
tive to standard distributional approaches.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999855152542373">
Distributed representations of words have been
shown to benefit a diverse set of NLP tasks in-
cluding syntactic parsing (Lazaridou et al., 2013;
Bansal et al., 2014), named entity recognition
(Guo et al., 2014) and sentiment analysis (Socher
et al., 2013). Additionally, because they can be
induced directly from unannotated corpora, they
are likewise available in domains and languages
where traditional linguistic resources do not ex-
haust. Intrinsic evaluations on various tasks are
helping refine vector learning methods to discover
representations that captures many facts about lex-
ical semantics (Turney, 2001; Turney and Pantel,
2010).
Yet induced word vectors do not look anything
like the representations described in most lexi-
cal semantic theories, which focus on identifying
classes of words (Levin, 1993; Baker et al., 1998;
Schuler, 2005; Miller, 1995). Though expensive
to construct, conceptualizing word meanings sym-
bolically is important for theoretical understand-
ing and interpretability is desired in computational
models.
Our contribution to this discussion is a new
technique that constructs task-independent word
vector representations using linguistic knowledge
derived from pre-constructed linguistic resources
like WordNet (Miller, 1995), FrameNet (Baker et
al., 1998), Penn Treebank (Marcus et al., 1993)
etc. In such word vectors every dimension is a lin-
guistic feature and 1/0 indicates the presence or
absence of that feature in a word, thus the vec-
tor representations are binary while being highly
sparse (≈ 99.9%). Since these vectors do not en-
code any word cooccurrence information, they are
non-distributional. An additional benefit of con-
structing such vectors is that they are fully inter-
pretable i.e, every dimension of these vectors maps
to a linguistic feature unlike distributional word
vectors where the vector dimensions have no in-
terpretability.
Of course, engineering feature vectors from lin-
guistic resources is established practice in many
applications of discriminative learning; e.g., pars-
ing (McDonald and Pereira, 2006; Nivre, 2008)
or part of speech tagging (Ratnaparkhi, 1996;
Collins, 2002). However, despite a certain com-
mon inventories of features that re-appear across
many tasks, feature engineering tends to be seen
as a task-specific problem, and engineered feature
vectors are not typically evaluated independently
of the tasks they are designed for. We evaluate the
quality of our linguistic vectors on a number of
tasks that have been proposed for evaluating dis-
tributional word vectors. We show that linguistic
word vectors are comparable to current state-of-
the-art distributional word vectors trained on bil-
lions of words as evaluated on a battery of seman-
tic and syntactic evaluation benchmarks.1
</bodyText>
<footnote confidence="0.9923315">
1Our vectors can be downloaded at: https://
github.com/mfaruqui/non-distributional
</footnote>
<page confidence="0.752791">
464
</page>
<note confidence="0.279877">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 464–469,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.9999238">
Lexicon Vocabulary Features
WordNet 10,794 92,117
Supersense 71,836 54
FrameNet 9,462 4,221
Emotion 6,468 10
Connotation 76,134 12
Color 14,182 12
Part of Speech 35,606 20
Syn. &amp; Ant. 35,693 75,972
Union 119,257 172,418
</table>
<tableCaption confidence="0.9841075">
Table 1: Sizes of vocabualry and features induced
from different linguistic resources.
</tableCaption>
<sectionHeader confidence="0.968994" genericHeader="method">
2 Linguistic Word Vectors
</sectionHeader>
<bodyText confidence="0.999226228571429">
We construct linguistic word vectors by extracting
word level information from linguistic resources.
Table 1 shows the size of vocabulary and number
of features induced from every lexicon. We now
describe various linguistic resources that we use
for constructing linguistic word vectors.
WordNet. WordNet (Miller, 1995) is an En-
glish lexical database that groups words into sets
of synonyms called synsets and records a num-
ber of relations among these synsets or their
members. For a word we look up its synset
for all possible part of speech (POS) tags that
it can assume. For example, film will have
SYNSET.FILM.V.01 and SYNSET.FILM.N.01 as
features as it can be both a verb and a noun. In ad-
dition to synsets, we include the hyponym (for ex.
HYPO.COLLAGEFILM.N.01), hypernym (for ex.
HYPER:SHEET.N.06) and holonym synset of the
word as features. We also collect antonyms and
pertainyms of all the words in a synset and include
those as features in the linguistic vector.
Supsersenses. WordNet partitions nouns and
verbs into semantic field categories known as
supsersenses (Ciaramita and Altun, 2006; Nas-
tase, 2008). For example, lioness evokes the su-
persense SS.NOUN.ANIMAL. These supersenses
were further extended to adjectives (Tsvetkov et
al., 2014).2 We use these supsersense tags for
nouns, verbs and adjectives as features in the lin-
guistic word vectors.
FrameNet. FrameNet (Baker et al., 1998; Fill-
more et al., 2003) is a rich linguistic resource that
contains information about lexical and predicate-
argument semantics in English. Frames can be
realized on the surface by many different word
</bodyText>
<footnote confidence="0.6562645">
2http://www.cs.cmu.edu/˜ytsvetko/
adj-supersenses.tar.gz
</footnote>
<bodyText confidence="0.999193571428572">
types, which suggests that the word types evok-
ing the same frame should be semantically related.
For every word, we use the frame it evokes along
with the roles of the evoked frame as its features.
Since, information in FrameNet is part of speech
(POS) disambiguated, we couple these feature
with the corresponding POS tag of the word. For
example, since appreciate is a verb, it will have
the following features: VERB.FRAME.REGARD,
VERB.FRAME.ROLE.EVALUEE etc.
Emotion &amp; Sentiment. Mohammad and Turney
(2013) constructed two different lexicons that as-
sociate words to sentiment polarity and to emo-
tions resp. using crowdsourcing. The polar-
ity is either positive or negative but there are
eight different kinds of emotions like anger, an-
ticipation, joy etc. Every word in the lexicon is
associated with these properties. For example,
cannibal evokes POL.NEG, EMO.DISGUST and
EMO.FEAR. We use these properties as features
in linguistic vectors.
Connotation. Feng et al. (2013) construct a lex-
icon that contains information about connotation
of words that are seemingly objective but often
allude nuanced sentiment. They assign positive,
negative and neutral connotations to these words.
This lexicon differs from Mohammad and Tur-
ney (2013) in that it has a more subtle shade of
sentiment and it extends to many more words.
For example, delay has a negative connotation
CON.NOUN.NEG, floral has a positive connota-
tion CON.ADJ.POS and outline has a neutral con-
notation CON.VERB.NEUT.
Color. Most languages have expressions involv-
ing color, for example green with envy and grey
with uncertainly are phrases used in English. The
word-color associtation lexicon produced by Mo-
hammad (2011) using crowdsourcing lists the col-
ors that a word evokes in English. We use every
color in this lexicon as a feature in the vector. For
example, COLOR.RED is a feature evoked by the
word blood.
Part of Speech Tags. The Penn Treebank (Mar-
cus et al., 1993) annotates naturally occurring text
for linguistic structure. It contains syntactic parse
trees and POS tags for every word in the corpus.
We collect all the possible POS tags that a word is
annotated with and use it as features in the linguis-
tic vectors. For example, love has PTB.NOUN,
</bodyText>
<page confidence="0.997234">
465
</page>
<table confidence="0.858756666666667">
Word POL.POS COLOR.PINK SS.NOUN.FEELING PTB.VERB ANTO.FAIR · · · CON.NOUN.POS
love 1 1 1 1 0 1
hate 0 0 1 1 0 0
ugly 0 0 0 0 1 0
beauty 1 1 0 0 0 1
refundable 0 0 0 0 0 1
</table>
<tableCaption confidence="0.873482">
Table 2: Some linguistic word vectors. 1 indicates presence and 0 indicates absence of a linguistic
feature.
</tableCaption>
<bodyText confidence="0.976903033333333">
PTB.VERB as features.
Synonymy &amp; Antonymy. We use Roget’s the-
saurus (Roget, 1852) to collect sets of synony-
mous words.3 For every word, its synonymous
word is used as a feature in the linguistic vec-
tor. For example, adoration and affair have
a feature SYNO.LOVE, admissible has a fea-
ture SYNO.ACCEPTABLE. The synonym lexi-
con contains 25,338 words after removal of mul-
tiword phrases. In a similar manner, we also
use antonymy relations between words as fea-
tures in the word vector. The antonymous words
for a given word were collected from Ordway
(1913).4 An example would be of impartial-
ity, which has features ANTO.FAVORITISM and
ANTO.INJUSTICE. The antonym lexicon has
10,355 words. These features are different from
those induced from WordNet as the former en-
code word-word relations whereas the latter en-
code word-synset relations.
After collecting features from the various lin-
guistic resources described above we obtain lin-
guistic word vectors of length 172,418 dimen-
sions. These vectors are 99.9% sparse i.e, each
vector on an average contains only 34 non-zero
features out of 172,418 total features. On average
a linguistic feature (vector dimension) is active for
15 word types. The linguistic word vectors con-
tain 119,257 unique word types. Table 2 shows
linguistic vectors for some of the words.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998608">
We first briefly describe the evaluation tasks and
then present results.
</bodyText>
<subsectionHeader confidence="0.995077">
3.1 Evaluation Tasks
</subsectionHeader>
<bodyText confidence="0.999312333333333">
Word Similarity. We evaluate our word repre-
sentations on three different benchmarks to mea-
sure word similarity. The first one is the widely
</bodyText>
<footnote confidence="0.996137">
3http://www.gutenberg.org/ebooks/10681
4https://archive.org/details/
synonymsantonyms00ordwiala
</footnote>
<bodyText confidence="0.9997625">
used WS-353 dataset (Finkelstein et al., 2001),
which contains 353 pairs of English words that
have been assigned similarity ratings by humans.
The second is the RG-65 dataset (Rubenstein and
Goodenough, 1965) of 65 words pairs. The third
dataset is SimLex (Hill et al., 2014) which has
been constructed to overcome the shortcomings
of WS-353 and contains 999 pairs of adjectives,
nouns and verbs. Word similarity is computed
using cosine similarity between two words and
Spearman’s rank correlation is reported between
the rankings produced by vector model against the
human rankings.
Sentiment Analysis. Socher et al. (2013) cre-
ated a treebank containing sentences annotated
with fine-grained sentiment labels on phrases and
sentences from movie review excerpts. The
coarse-grained treebank of positive and negative
classes has been split into training, development,
and test datasets containing 6,920, 872, and 1,821
sentences, respectively. We use average of the
word vectors of a given sentence as features in
an E2-regularized logistic regression for classifica-
tion. The classifier is tuned on the dev set and ac-
curacy is reported on the test set.
NP-Bracketing. Lazaridou et al. (2013) con-
structed a dataset from the Penn TreeBank (Mar-
cus et al., 1993) of noun phrases (NP) of length
three words, where the first can be an adjective or
a noun and the other two are nouns. The task is to
predict the correct bracketing in the parse tree for
a given noun phrase. For example, local (phone
company) and (blood pressure) medicine exhibit
left and right bracketing respectively. We append
the word vectors of the three words in the NP in or-
der and use them as features in an E2-regularized
logistic regression classifier. The dataset contains
2,227 noun phrases split into 10 folds. The clas-
sifier is tuned on the first fold and cross-validation
accuracy is reported on the remaining nine folds.
</bodyText>
<page confidence="0.999116">
466
</page>
<table confidence="0.999788">
Vector Length (D) Params. Corpus Size WS-353 RG-65 SimLex Senti NP
Skip-Gram 300 D x N 300 billion 65.6 72.8 43.6 81.5 80.1
Glove 300 D x N 6 billion 60.5 76.6 36.9 77.7 77.9
LSA 300 D x N 1 billion 67.3 77.0 49.6 81.1 79.7
Ling Sparse 172,418 – – 44.6 77.8 56.6 79.4 83.3
Ling Dense 300 D x N – 45.4 67.0 57.8 75.4 76.2
Skip-Gram ⊕ Ling Sparse 172,718 – – 67.1 80.5 55.5 82.4 82.8
</table>
<tableCaption confidence="0.9922375">
Table 3: Performance of different type of word vectors on evaluation tasks reported by Spearman’s
correlation (first 3 columns) and Accuracy (last 2 columns). Bold shows the best performance for a task.
</tableCaption>
<subsectionHeader confidence="0.999044">
3.2 Linguistic Vs. Distributional Vectors
</subsectionHeader>
<bodyText confidence="0.999978666666667">
In order to make our linguistic vectors comparable
to publicly available distributional word vectors,
we perform singular value decompostion (SVD)
on the linguistic matrix to obtain word vectors of
lower dimensionality. If L E {0, 1IN×D is the lin-
guistic matrix with N word types and D linguistic
features, then we can obtain U E RN×K from the
SVD of L as follows: L = UEVT, with K being
the desired length of the lower dimensional space.
We compare both sparse and dense linguistic
vectors to three widely used distributional word
vector models. The first two are the pre-trained
Skip-Gram (Mikolov et al., 2013)5 and Glove
(Pennington et al., 2014)6 word vectors each of
length 300, trained on 300 billion and 6 billion
words respectively. We used latent semantic anal-
ysis (LSA) to obtain word vectors from the SVD
decomposition of a word-word cooccurrence ma-
trix (Turney and Pantel, 2010). These were trained
on 1 billion words of Wikipedia with vector length
300 and context window of 5 words.
</bodyText>
<subsectionHeader confidence="0.845183">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.997640571428571">
Table 3 shows the performance of different word
vector types on the evaluation tasks. It can be seen
that although Skip-Gram, Glove &amp; LSA perform
better than linguistic vectors on WS-353, the lin-
guistic vectors outperform them by a huge mar-
gin on SimLex. Linguistic vectors also perform
better at RG-65. On sentiment analysis, linguis-
tic vectors are competitive with Skip-Gram vec-
tors and on the NP-bracketing task they outper-
form all distributional vectors with a statistically
significant margin (p &lt; 0.05, McNemar’s test Di-
etterich (1998)). We append the sparse linguis-
tic vectors to Skip-Gram vectors and evaluate the
resultant vectors as shown in the bottom row of
</bodyText>
<tableCaption confidence="0.626763">
Table 3. The combined vector outperforms Skip-
</tableCaption>
<footnote confidence="0.992173333333333">
5https://code.google.com/p/word2vec
6http://www-nlp.stanford.edu/projects/
glove/
</footnote>
<bodyText confidence="0.986751666666667">
Gram on all tasks, showing that linguistic vectors
contain useful information orthogonal to distribu-
tional information.
It is evident from the results that linguistic vec-
tors are either competitive or better to state-of-the-
art distributional vector models. Sparse linguis-
tic word vectors are high dimensional but they are
also sparse, which makes them computationally
easy to work with.
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999901935483871">
Linguistic resources like WordNet have found ex-
tensive applications in lexical semantics, for ex-
ample, for word sense disambiguation, word simi-
larity etc. (Resnik, 1995; Agirre et al., 2009). Re-
cently there has been interest in using linguistic
resources to enrich word vector representations.
In these approaches, relational information among
words obtained from WordNet, Freebase etc. is
used as a constraint to encourage words with sim-
ilar properties in lexical ontologies to have simi-
lar word vectors (Xu et al., 2014; Yu and Dredze,
2014; Bian et al., 2014; Fried and Duh, 2014;
Faruqui et al., 2015a). Distributional represen-
tations have also been shown to improve by us-
ing experiential data in addition to distributional
context (Andrews et al., 2009). We have shown
that simple vector concatenation can likewise be
used to improve representations (further confirm-
ing the established finding that lexical resources
and cooccurrence information provide somewhat
orthogonal information), but it is certain that more
careful combination strategies can be used.
Although distributional word vector dimensions
cannot, in general, be identified with linguistic
properties, it has been shown that some vector
construction strategies yield dimensions that are
relatively more interpretable (Murphy et al., 2012;
Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et
al., 2015b). However, such analysis is difficult
to generalize across models of representation. In
constrast to distributional word vectors, linguistic
</bodyText>
<page confidence="0.998411">
467
</page>
<bodyText confidence="0.999824333333333">
word vectors have interpretable dimensions as ev-
ery dimension is a linguistic property.
Linguistic word vectors require no training as
there are no parameters to be optimized, meaning
they are computationally economical. While good
quality linguistic word vectors may only be ob-
tained for languages with rich linguistic resources,
such resources do exist in many languages and
should not be disregarded.
</bodyText>
<sectionHeader confidence="0.995011" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999678">
We have presented a novel method of constructing
word vector representations solely using linguistic
knowledge from pre-existing linguistic resources.
These non-distributional, linguistic word vectors
are competitive to the current models of distribu-
tional word vectors as evaluated on a battery of
tasks. Linguistic vectors are fully interpretable
as every dimension is a linguistic feature and are
highly sparse, so they are computationally easy to
work with.
</bodyText>
<sectionHeader confidence="0.786893" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99988">
We thank Nathan Schneider for giving comments
on an earlier draft of this paper and the anonymous
reviewers for their feedback.
</bodyText>
<sectionHeader confidence="0.968754" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982440592105263">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In Proc. of
NAACL.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological review, 116(3):463.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proc. of
ACL.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proc. of ACL.
Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Proc. of MLKDD.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learn-
ing algorithms. Neural Computation.
Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2015a.
Retrofitting word vectors to semantic lexicons. In
Proc. of NAACL.
Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris
Dyer, and Noah A. Smith. 2015b. Sparse overcom-
plete word vector representations. In Proc. ofACL.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proc. of
ACL.
Charles Fillmore, Christopher Johnson, and Miriam
Petruck. 2003. Lexicographic relevance: select-
ing information from corpus evidence. International
Journal of Lexicography.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW.
Daniel Fried and Kevin Duh. 2014. Incorporating both
distributional and relational semantics in word rep-
resentations. arXiv preprint arXiv:1412.4369.
Alona Fyshe, Partha P. Talukdar, Brian Murphy, and
Tom M. Mitchell. 2014. Interpretable semantic vec-
tors from a joint model of brain- and text- based
meaning. In Proc. of ACL.
Alona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian
Murphy, and Tom M. Mitchell. 2015. A composi-
tional and interpretable semantic space. In Proc. of
NAACL.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proc. of EMNLP.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. CoRR, abs/1408.3456.
Angeliki Lazaridou, Eva Maria Vecchi, and Marco
Baroni. 2013. Fish transporters and miracle
homes: How compositional distributional semantics
can help NP parsing. In Proc. of EMNLP.
Beth Levin. 1993. English verb classes and alterna-
tions : a preliminary investigation. University of
Chicago Press.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
</reference>
<page confidence="0.997068">
468
</page>
<reference confidence="0.993231608695652">
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
George A Miller. 1995. Wordnet: a lexical database
for english. Communications of the ACM.
Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
Computational Intelligence, 29(3):436–465.
Saif Mohammad. 2011. Colourful language: Mea-
suring word-colour associations. In Proc. of the
Workshop on Cognitive Modeling and Computa-
tional Linguistics.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Learning effective and interpretable seman-
tic models using non-negative sparse embedding. In
Proc. of COLING.
Vivi Nastase. 2008. Unsupervised all-words word
sense disambiguation with grammatical dependen-
cies. In Proc. of IJCNLP.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Edith Bertha Ordway. 1913. Synonyms and Antonyms:
An Alphabetical List of Words in Common Use,
Grouped with Others of Similar and Opposite Mean-
ing. Sully and Kleinteich.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proc. of EMNLP.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. of
EMNLP.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Proc.
of IJCAI.
P. M. Roget. 1852. Roget’s Thesaurus of English
words and phrases. Available from Project Gutem-
berg.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10).
Karin Kipper Schuler. 2005. Verbnet: A Broad-
coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. of EMNLP.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting english adjective senses with super-
senses. In Proc. of LREC.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. JAIR, pages 141–188.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proc. of ECML.
Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proc. of CIKM.
Mo Yu and Mark Dredze. 2014. Improving lexical
embeddings with semantic knowledge. In Proc. of
ACL.
</reference>
<page confidence="0.999625">
469
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.415967">
<title confidence="0.682662333333333">Non-distributional Word Vector Representations Faruqui Language Technologies</title>
<affiliation confidence="0.976672">Carnegie Mellon</affiliation>
<address confidence="0.997912">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.994318944444444">Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations. Psychological review,</title>
<date>2009</date>
<pages>116--3</pages>
<contexts>
<context position="15688" citStr="Andrews et al., 2009" startWordPosition="2462" endWordPosition="2465">d similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological review, 116(3):463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1753" citStr="Baker et al., 1998" startWordPosition="251" endWordPosition="254">entiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and interpretability is desired in computational models. Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al., 1993) etc. In such word vectors every dimension is a linguistic feature and 1/0 indicates the presence or absen</context>
<context position="5777" citStr="Baker et al., 1998" startWordPosition="855" endWordPosition="858">. HYPER:SHEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.NOUN.ANIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word 2http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz types, which suggests that the word types evoking the same frame should be semantically related. For every word, we use the frame it evokes along with the roles of the evoked frame as its features. Since, information in FrameNet is part of speech (POS) disambiguated, we couple these feature with the corresponding POS tag of the word. For exa</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1084" citStr="Bansal et al., 2014" startWordPosition="152" endWordPosition="155">aditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches. 1 Introduction Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which f</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Knowledge-powered deep learning for word embedding.</title>
<date>2014</date>
<booktitle>In Proc. of MLKDD.</booktitle>
<contexts>
<context position="15491" citStr="Bian et al., 2014" startWordPosition="2431" endWordPosition="2434">es them computationally easy to work with. 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic</context>
</contexts>
<marker>Bian, Gao, Liu, 2014</marker>
<rawString>Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-powered deep learning for word embedding. In Proc. of MLKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5476" citStr="Ciaramita and Altun, 2006" startWordPosition="809" endWordPosition="812"> For a word we look up its synset for all possible part of speech (POS) tags that it can assume. For example, film will have SYNSET.FILM.V.01 and SYNSET.FILM.N.01 as features as it can be both a verb and a noun. In addition to synsets, we include the hyponym (for ex. HYPO.COLLAGEFILM.N.01), hypernym (for ex. HYPER:SHEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.NOUN.ANIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word 2http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz types, which suggests that the word types </context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3059" citStr="Collins, 2002" startWordPosition="443" endWordPosition="444">sparse (≈ 99.9%). Since these vectors do not encode any word cooccurrence information, they are non-distributional. An additional benefit of constructing such vectors is that they are fully interpretable i.e, every dimension of these vectors maps to a linguistic feature unlike distributional word vectors where the vector dimensions have no interpretability. Of course, engineering feature vectors from linguistic resources is established practice in many applications of discriminative learning; e.g., parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002). However, despite a certain common inventories of features that re-appear across many tasks, feature engineering tends to be seen as a task-specific problem, and engineered feature vectors are not typically evaluated independently of the tasks they are designed for. We evaluate the quality of our linguistic vectors on a number of tasks that have been proposed for evaluating distributional word vectors. We show that linguistic word vectors are comparable to current state-ofthe-art distributional word vectors trained on billions of words as evaluated on a battery of semantic and syntactic evalu</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation.</journal>
<contexts>
<context position="14274" citStr="Dietterich (1998)" startWordPosition="2251" endWordPosition="2253">Wikipedia with vector length 300 and context window of 5 words. 3.3 Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove &amp; LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge margin on SimLex. Linguistic vectors also perform better at RG-65. On sentiment analysis, linguistic vectors are competitive with Skip-Gram vectors and on the NP-bracketing task they outperform all distributional vectors with a statistically significant margin (p &lt; 0.05, McNemar’s test Dietterich (1998)). We append the sparse linguistic vectors to Skip-Gram vectors and evaluate the resultant vectors as shown in the bottom row of Table 3. The combined vector outperforms Skip5https://code.google.com/p/word2vec 6http://www-nlp.stanford.edu/projects/ glove/ Gram on all tasks, showing that linguistic vectors contain useful information orthogonal to distributional information. It is evident from the results that linguistic vectors are either competitive or better to state-of-theart distributional vector models. Sparse linguistic word vectors are high dimensional but they are also sparse, which mak</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Jesse Dodge</author>
<author>Sujay K Jauhar</author>
<author>Chris Dyer</author>
<author>Eduard Hovy</author>
<author>Noah A Smith</author>
</authors>
<title>Retrofitting word vectors to semantic lexicons.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="15534" citStr="Faruqui et al., 2015" startWordPosition="2439" endWordPosition="2442">h. 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some ve</context>
</contexts>
<marker>Faruqui, Dodge, Jauhar, Dyer, Hovy, Smith, 2015</marker>
<rawString>Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015a. Retrofitting word vectors to semantic lexicons. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Yulia Tsvetkov</author>
<author>Dani Yogatama</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Sparse overcomplete word vector representations.</title>
<date>2015</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="15534" citStr="Faruqui et al., 2015" startWordPosition="2439" endWordPosition="2442">h. 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some ve</context>
</contexts>
<marker>Faruqui, Tsvetkov, Yogatama, Dyer, Smith, 2015</marker>
<rawString>Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A. Smith. 2015b. Sparse overcomplete word vector representations. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Jun Seok Kang</author>
<author>Polina Kuznetsova</author>
<author>Yejin Choi</author>
</authors>
<title>Connotation lexicon: A dash of sentiment beneath the surface meaning.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7009" citStr="Feng et al. (2013)" startWordPosition="1042" endWordPosition="1045">preciate is a verb, it will have the following features: VERB.FRAME.REGARD, VERB.FRAME.ROLE.EVALUEE etc. Emotion &amp; Sentiment. Mohammad and Turney (2013) constructed two different lexicons that associate words to sentiment polarity and to emotions resp. using crowdsourcing. The polarity is either positive or negative but there are eight different kinds of emotions like anger, anticipation, joy etc. Every word in the lexicon is associated with these properties. For example, cannibal evokes POL.NEG, EMO.DISGUST and EMO.FEAR. We use these properties as features in linguistic vectors. Connotation. Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation CON.NOUN.NEG, floral has a positive connotation CON.ADJ.POS and outline has a neutral connotation CON.VERB.NEUT. Color. Most languages have expressions involving color, for example green with envy and grey w</context>
</contexts>
<marker>Feng, Kang, Kuznetsova, Choi, 2013</marker>
<rawString>Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment beneath the surface meaning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
<author>Christopher Johnson</author>
<author>Miriam Petruck</author>
</authors>
<title>Lexicographic relevance: selecting information from corpus evidence.</title>
<date>2003</date>
<journal>International Journal of Lexicography.</journal>
<contexts>
<context position="5801" citStr="Fillmore et al., 2003" startWordPosition="859" endWordPosition="863">and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.NOUN.ANIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word 2http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz types, which suggests that the word types evoking the same frame should be semantically related. For every word, we use the frame it evokes along with the roles of the evoked frame as its features. Since, information in FrameNet is part of speech (POS) disambiguated, we couple these feature with the corresponding POS tag of the word. For example, since appreciate i</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles Fillmore, Christopher Johnson, and Miriam Petruck. 2003. Lexicographic relevance: selecting information from corpus evidence. International Journal of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="10240" citStr="Finkelstein et al., 2001" startWordPosition="1573" endWordPosition="1576">ures out of 172,418 total features. On average a linguistic feature (vector dimension) is active for 15 word types. The linguistic word vectors contain 119,257 unique word types. Table 2 shows linguistic vectors for some of the words. 3 Experiments We first briefly describe the evaluation tasks and then present results. 3.1 Evaluation Tasks Word Similarity. We evaluate our word representations on three different benchmarks to measure word similarity. The first one is the widely 3http://www.gutenberg.org/ebooks/10681 4https://archive.org/details/ synonymsantonyms00ordwiala used WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the RG-65 dataset (Rubenstein and Goodenough, 1965) of 65 words pairs. The third dataset is SimLex (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and Spearman’s rank correlation is reported between the rankings produced by vector model against the human rankings. Sentiment Analysis. Socher et al. (2013) created a treebank </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Fried</author>
<author>Kevin Duh</author>
</authors>
<title>Incorporating both distributional and relational semantics in word representations. arXiv preprint arXiv:1412.4369.</title>
<date>2014</date>
<contexts>
<context position="15512" citStr="Fried and Duh, 2014" startWordPosition="2435" endWordPosition="2438">ally easy to work with. 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has b</context>
</contexts>
<marker>Fried, Duh, 2014</marker>
<rawString>Daniel Fried and Kevin Duh. 2014. Incorporating both distributional and relational semantics in word representations. arXiv preprint arXiv:1412.4369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alona Fyshe</author>
<author>Partha P Talukdar</author>
<author>Brian Murphy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Interpretable semantic vectors from a joint model of brain- and text- based meaning.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="16259" citStr="Fyshe et al., 2014" startWordPosition="2541" endWordPosition="2544">o distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distributional word vectors, linguistic 467 word vectors have interpretable dimensions as every dimension is a linguistic property. Linguistic word vectors require no training as there are no parameters to be optimized, meaning they are computationally economical. While good quality linguistic word vectors may only be obtained for languages with rich linguistic resources, such resources do exist in many languages and should not be disregarded. 5 Concl</context>
</contexts>
<marker>Fyshe, Talukdar, Murphy, Mitchell, 2014</marker>
<rawString>Alona Fyshe, Partha P. Talukdar, Brian Murphy, and Tom M. Mitchell. 2014. Interpretable semantic vectors from a joint model of brain- and text- based meaning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alona Fyshe</author>
<author>Leila Wehbe</author>
<author>Partha P Talukdar</author>
<author>Brian Murphy</author>
<author>Tom M Mitchell</author>
</authors>
<title>A compositional and interpretable semantic space.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="16279" citStr="Fyshe et al., 2015" startWordPosition="2545" endWordPosition="2548">text (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distributional word vectors, linguistic 467 word vectors have interpretable dimensions as every dimension is a linguistic property. Linguistic word vectors require no training as there are no parameters to be optimized, meaning they are computationally economical. While good quality linguistic word vectors may only be obtained for languages with rich linguistic resources, such resources do exist in many languages and should not be disregarded. 5 Conclusion We have presen</context>
</contexts>
<marker>Fyshe, Wehbe, Talukdar, Murphy, Mitchell, 2015</marker>
<rawString>Alona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian Murphy, and Tom M. Mitchell. 2015. A compositional and interpretable semantic space. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1129" citStr="Guo et al., 2014" startWordPosition="159" endWordPosition="162">t best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches. 1 Introduction Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, </context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="10470" citStr="Hill et al., 2014" startWordPosition="1611" endWordPosition="1614">Experiments We first briefly describe the evaluation tasks and then present results. 3.1 Evaluation Tasks Word Similarity. We evaluate our word representations on three different benchmarks to measure word similarity. The first one is the widely 3http://www.gutenberg.org/ebooks/10681 4https://archive.org/details/ synonymsantonyms00ordwiala used WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the RG-65 dataset (Rubenstein and Goodenough, 1965) of 65 words pairs. The third dataset is SimLex (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and Spearman’s rank correlation is reported between the rankings produced by vector model against the human rankings. Sentiment Analysis. Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and tes</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Eva Maria Vecchi</author>
<author>Marco Baroni</author>
</authors>
<title>Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1062" citStr="Lazaridou et al., 2013" startWordPosition="148" endWordPosition="151"> to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches. 1 Introduction Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical sema</context>
<context position="11386" citStr="Lazaridou et al. (2013)" startWordPosition="1749" endWordPosition="1752"> human rankings. Sentiment Analysis. Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as features in an E2-regularized logistic regression for classification. The classifier is tuned on the dev set and accuracy is reported on the test set. NP-Bracketing. Lazaridou et al. (2013) constructed a dataset from the Penn TreeBank (Marcus et al., 1993) of noun phrases (NP) of length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit left and right bracketing respectively. We append the word vectors of the three words in the NP in order and use them as features in an E2-regularized logistic regression classifier. The dataset contains 2,227 noun phrases split into 10 folds. The cla</context>
</contexts>
<marker>Lazaridou, Vecchi, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Eva Maria Vecchi, and Marco Baroni. 2013. Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations : a preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1733" citStr="Levin, 1993" startWordPosition="249" endWordPosition="250">, 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and interpretability is desired in computational models. Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al., 1993) etc. In such word vectors every dimension is a linguistic feature and 1/0 indicates t</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations : a preliminary investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="2247" citStr="Marcus et al., 1993" startWordPosition="315" endWordPosition="318">ions described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and interpretability is desired in computational models. Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al., 1993) etc. In such word vectors every dimension is a linguistic feature and 1/0 indicates the presence or absence of that feature in a word, thus the vector representations are binary while being highly sparse (≈ 99.9%). Since these vectors do not encode any word cooccurrence information, they are non-distributional. An additional benefit of constructing such vectors is that they are fully interpretable i.e, every dimension of these vectors maps to a linguistic feature unlike distributional word vectors where the vector dimensions have no interpretability. Of course, engineering feature vectors fro</context>
<context position="7971" citStr="Marcus et al., 1993" startWordPosition="1199" endWordPosition="1203">rds. For example, delay has a negative connotation CON.NOUN.NEG, floral has a positive connotation CON.ADJ.POS and outline has a neutral connotation CON.VERB.NEUT. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, COLOR.RED is a feature evoked by the word blood. Part of Speech Tags. The Penn Treebank (Marcus et al., 1993) annotates naturally occurring text for linguistic structure. It contains syntactic parse trees and POS tags for every word in the corpus. We collect all the possible POS tags that a word is annotated with and use it as features in the linguistic vectors. For example, love has PTB.NOUN, 465 Word POL.POS COLOR.PINK SS.NOUN.FEELING PTB.VERB ANTO.FAIR · · · CON.NOUN.POS love 1 1 1 1 0 1 hate 0 0 1 1 0 0 ugly 0 0 0 0 1 0 beauty 1 1 0 0 0 1 refundable 0 0 0 0 0 1 Table 2: Some linguistic word vectors. 1 indicates presence and 0 indicates absence of a linguistic feature. PTB.VERB as features. Synony</context>
<context position="11453" citStr="Marcus et al., 1993" startWordPosition="1761" endWordPosition="1765">eebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as features in an E2-regularized logistic regression for classification. The classifier is tuned on the dev set and accuracy is reported on the test set. NP-Bracketing. Lazaridou et al. (2013) constructed a dataset from the Penn TreeBank (Marcus et al., 1993) of noun phrases (NP) of length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit left and right bracketing respectively. We append the word vectors of the three words in the NP in order and use them as features in an E2-regularized logistic regression classifier. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validation accuracy is</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="2984" citStr="McDonald and Pereira, 2006" startWordPosition="430" endWordPosition="433"> that feature in a word, thus the vector representations are binary while being highly sparse (≈ 99.9%). Since these vectors do not encode any word cooccurrence information, they are non-distributional. An additional benefit of constructing such vectors is that they are fully interpretable i.e, every dimension of these vectors maps to a linguistic feature unlike distributional word vectors where the vector dimensions have no interpretability. Of course, engineering feature vectors from linguistic resources is established practice in many applications of discriminative learning; e.g., parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002). However, despite a certain common inventories of features that re-appear across many tasks, feature engineering tends to be seen as a task-specific problem, and engineered feature vectors are not typically evaluated independently of the tasks they are designed for. We evaluate the quality of our linguistic vectors on a number of tasks that have been proposed for evaluating distributional word vectors. We show that linguistic word vectors are comparable to current state-ofthe-art distributional word vectors trained on </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="13336" citStr="Mikolov et al., 2013" startWordPosition="2096" endWordPosition="2099">Vectors In order to make our linguistic vectors comparable to publicly available distributional word vectors, we perform singular value decompostion (SVD) on the linguistic matrix to obtain word vectors of lower dimensionality. If L E {0, 1IN×D is the linguistic matrix with N word types and D linguistic features, then we can obtain U E RN×K from the SVD of L as follows: L = UEVT, with K being the desired length of the lower dimensional space. We compare both sparse and dense linguistic vectors to three widely used distributional word vector models. The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively. We used latent semantic analysis (LSA) to obtain word vectors from the SVD decomposition of a word-word cooccurrence matrix (Turney and Pantel, 2010). These were trained on 1 billion words of Wikipedia with vector length 300 and context window of 5 words. 3.3 Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove &amp; LSA perform better than linguistic vectors on WS-353, the linguistic </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM.</journal>
<contexts>
<context position="1783" citStr="Miller, 1995" startWordPosition="257" endWordPosition="258">013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and interpretability is desired in computational models. Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al., 1993) etc. In such word vectors every dimension is a linguistic feature and 1/0 indicates the presence or absence of that feature in a word, </context>
<context position="4693" citStr="Miller, 1995" startWordPosition="680" endWordPosition="681">794 92,117 Supersense 71,836 54 FrameNet 9,462 4,221 Emotion 6,468 10 Connotation 76,134 12 Color 14,182 12 Part of Speech 35,606 20 Syn. &amp; Ant. 35,693 75,972 Union 119,257 172,418 Table 1: Sizes of vocabualry and features induced from different linguistic resources. 2 Linguistic Word Vectors We construct linguistic word vectors by extracting word level information from linguistic resources. Table 1 shows the size of vocabulary and number of features induced from every lexicon. We now describe various linguistic resources that we use for constructing linguistic word vectors. WordNet. WordNet (Miller, 1995) is an English lexical database that groups words into sets of synonyms called synsets and records a number of relations among these synsets or their members. For a word we look up its synset for all possible part of speech (POS) tags that it can assume. For example, film will have SYNSET.FILM.V.01 and SYNSET.FILM.N.01 as features as it can be both a verb and a noun. In addition to synsets, we include the hyponym (for ex. HYPO.COLLAGEFILM.N.01), hypernym (for ex. HYPER:SHEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset a</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word-emotion association lexicon.</title>
<date>2013</date>
<journal>Computational Intelligence,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="6543" citStr="Mohammad and Turney (2013)" startWordPosition="970" endWordPosition="973">rames can be realized on the surface by many different word 2http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz types, which suggests that the word types evoking the same frame should be semantically related. For every word, we use the frame it evokes along with the roles of the evoked frame as its features. Since, information in FrameNet is part of speech (POS) disambiguated, we couple these feature with the corresponding POS tag of the word. For example, since appreciate is a verb, it will have the following features: VERB.FRAME.REGARD, VERB.FRAME.ROLE.EVALUEE etc. Emotion &amp; Sentiment. Mohammad and Turney (2013) constructed two different lexicons that associate words to sentiment polarity and to emotions resp. using crowdsourcing. The polarity is either positive or negative but there are eight different kinds of emotions like anger, anticipation, joy etc. Every word in the lexicon is associated with these properties. For example, cannibal evokes POL.NEG, EMO.DISGUST and EMO.FEAR. We use these properties as features in linguistic vectors. Connotation. Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentim</context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2013. Crowdsourcing a word-emotion association lexicon. Computational Intelligence, 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>Colourful language: Measuring word-colour associations.</title>
<date>2011</date>
<booktitle>In Proc. of the Workshop on Cognitive Modeling and Computational Linguistics.</booktitle>
<contexts>
<context position="7717" citStr="Mohammad (2011)" startWordPosition="1153" endWordPosition="1155">objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation CON.NOUN.NEG, floral has a positive connotation CON.ADJ.POS and outline has a neutral connotation CON.VERB.NEUT. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, COLOR.RED is a feature evoked by the word blood. Part of Speech Tags. The Penn Treebank (Marcus et al., 1993) annotates naturally occurring text for linguistic structure. It contains syntactic parse trees and POS tags for every word in the corpus. We collect all the possible POS tags that a word is annotated with and use it as features in the linguistic vectors. For example, love has PTB.NOUN, 465 Word POL.POS COLOR.PINK SS.NOUN.FEELING PTB.VERB ANTO.</context>
</contexts>
<marker>Mohammad, 2011</marker>
<rawString>Saif Mohammad. 2011. Colourful language: Measuring word-colour associations. In Proc. of the Workshop on Cognitive Modeling and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning effective and interpretable semantic models using non-negative sparse embedding.</title>
<date>2012</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="16239" citStr="Murphy et al., 2012" startWordPosition="2537" endWordPosition="2540">al data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distributional word vectors, linguistic 467 word vectors have interpretable dimensions as every dimension is a linguistic property. Linguistic word vectors require no training as there are no parameters to be optimized, meaning they are computationally economical. While good quality linguistic word vectors may only be obtained for languages with rich linguistic resources, such resources do exist in many languages and should not be </context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012. Learning effective and interpretable semantic models using non-negative sparse embedding. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Unsupervised all-words word sense disambiguation with grammatical dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of IJCNLP.</booktitle>
<contexts>
<context position="5492" citStr="Nastase, 2008" startWordPosition="813" endWordPosition="815">synset for all possible part of speech (POS) tags that it can assume. For example, film will have SYNSET.FILM.V.01 and SYNSET.FILM.N.01 as features as it can be both a verb and a noun. In addition to synsets, we include the hyponym (for ex. HYPO.COLLAGEFILM.N.01), hypernym (for ex. HYPER:SHEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.NOUN.ANIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word 2http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz types, which suggests that the word types evoking the same</context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Vivi Nastase. 2008. Unsupervised all-words word sense disambiguation with grammatical dependencies. In Proc. of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="2998" citStr="Nivre, 2008" startWordPosition="434" endWordPosition="435">s the vector representations are binary while being highly sparse (≈ 99.9%). Since these vectors do not encode any word cooccurrence information, they are non-distributional. An additional benefit of constructing such vectors is that they are fully interpretable i.e, every dimension of these vectors maps to a linguistic feature unlike distributional word vectors where the vector dimensions have no interpretability. Of course, engineering feature vectors from linguistic resources is established practice in many applications of discriminative learning; e.g., parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002). However, despite a certain common inventories of features that re-appear across many tasks, feature engineering tends to be seen as a task-specific problem, and engineered feature vectors are not typically evaluated independently of the tasks they are designed for. We evaluate the quality of our linguistic vectors on a number of tasks that have been proposed for evaluating distributional word vectors. We show that linguistic word vectors are comparable to current state-ofthe-art distributional word vectors trained on billions of wo</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edith Bertha Ordway</author>
</authors>
<title>Synonyms and Antonyms: An Alphabetical List of Words in Common Use, Grouped with Others of Similar and Opposite Meaning. Sully and Kleinteich.</title>
<date>1913</date>
<contexts>
<context position="9095" citStr="Ordway (1913)" startWordPosition="1408" endWordPosition="1409">es presence and 0 indicates absence of a linguistic feature. PTB.VERB as features. Synonymy &amp; Antonymy. We use Roget’s thesaurus (Roget, 1852) to collect sets of synonymous words.3 For every word, its synonymous word is used as a feature in the linguistic vector. For example, adoration and affair have a feature SYNO.LOVE, admissible has a feature SYNO.ACCEPTABLE. The synonym lexicon contains 25,338 words after removal of multiword phrases. In a similar manner, we also use antonymy relations between words as features in the word vector. The antonymous words for a given word were collected from Ordway (1913).4 An example would be of impartiality, which has features ANTO.FAVORITISM and ANTO.INJUSTICE. The antonym lexicon has 10,355 words. These features are different from those induced from WordNet as the former encode word-word relations whereas the latter encode word-synset relations. After collecting features from the various linguistic resources described above we obtain linguistic word vectors of length 172,418 dimensions. These vectors are 99.9% sparse i.e, each vector on an average contains only 34 non-zero features out of 172,418 total features. On average a linguistic feature (vector dime</context>
</contexts>
<marker>Ordway, 1913</marker>
<rawString>Edith Bertha Ordway. 1913. Synonyms and Antonyms: An Alphabetical List of Words in Common Use, Grouped with Others of Similar and Opposite Meaning. Sully and Kleinteich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="13373" citStr="Pennington et al., 2014" startWordPosition="2102" endWordPosition="2105">istic vectors comparable to publicly available distributional word vectors, we perform singular value decompostion (SVD) on the linguistic matrix to obtain word vectors of lower dimensionality. If L E {0, 1IN×D is the linguistic matrix with N word types and D linguistic features, then we can obtain U E RN×K from the SVD of L as follows: L = UEVT, with K being the desired length of the lower dimensional space. We compare both sparse and dense linguistic vectors to three widely used distributional word vector models. The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively. We used latent semantic analysis (LSA) to obtain word vectors from the SVD decomposition of a word-word cooccurrence matrix (Turney and Pantel, 2010). These were trained on 1 billion words of Wikipedia with vector length 300 and context window of 5 words. 3.3 Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove &amp; LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge mar</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3043" citStr="Ratnaparkhi, 1996" startWordPosition="441" endWordPosition="442">while being highly sparse (≈ 99.9%). Since these vectors do not encode any word cooccurrence information, they are non-distributional. An additional benefit of constructing such vectors is that they are fully interpretable i.e, every dimension of these vectors maps to a linguistic feature unlike distributional word vectors where the vector dimensions have no interpretability. Of course, engineering feature vectors from linguistic resources is established practice in many applications of discriminative learning; e.g., parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002). However, despite a certain common inventories of features that re-appear across many tasks, feature engineering tends to be seen as a task-specific problem, and engineered feature vectors are not typically evaluated independently of the tasks they are designed for. We evaluate the quality of our linguistic vectors on a number of tasks that have been proposed for evaluating distributional word vectors. We show that linguistic word vectors are comparable to current state-ofthe-art distributional word vectors trained on billions of words as evaluated on a battery of semantic and</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="15098" citStr="Resnik, 1995" startWordPosition="2369" endWordPosition="2370">ttp://www-nlp.stanford.edu/projects/ glove/ Gram on all tasks, showing that linguistic vectors contain useful information orthogonal to distributional information. It is evident from the results that linguistic vectors are either competitive or better to state-of-theart distributional vector models. Sparse linguistic word vectors are high dimensional but they are also sparse, which makes them computationally easy to work with. 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="false">
<title>Roget’s Thesaurus of English words and phrases. Available from Project Gutemberg.</title>
<marker></marker>
<rawString>P. M. Roget. 1852. Roget’s Thesaurus of English words and phrases. Available from Project Gutemberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="10403" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1598" endWordPosition="1601">257 unique word types. Table 2 shows linguistic vectors for some of the words. 3 Experiments We first briefly describe the evaluation tasks and then present results. 3.1 Evaluation Tasks Word Similarity. We evaluate our word representations on three different benchmarks to measure word similarity. The first one is the widely 3http://www.gutenberg.org/ebooks/10681 4https://archive.org/details/ synonymsantonyms00ordwiala used WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the RG-65 dataset (Rubenstein and Goodenough, 1965) of 65 words pairs. The third dataset is SimLex (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and Spearman’s rank correlation is reported between the rankings produced by vector model against the human rankings. Sentiment Analysis. Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and </context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>Verbnet: A Broadcoverage, Comprehensive Verb Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1768" citStr="Schuler, 2005" startWordPosition="255" endWordPosition="256">ocher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and interpretability is desired in computational models. Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al., 1993) etc. In such word vectors every dimension is a linguistic feature and 1/0 indicates the presence or absence of that feat</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>Karin Kipper Schuler. 2005. Verbnet: A Broadcoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1174" citStr="Socher et al., 2013" startWordPosition="166" endWordPosition="169">ng interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches. 1 Introduction Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Mill</context>
<context position="10820" citStr="Socher et al. (2013)" startWordPosition="1663" endWordPosition="1666">S-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the RG-65 dataset (Rubenstein and Goodenough, 1965) of 65 words pairs. The third dataset is SimLex (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and Spearman’s rank correlation is reported between the rankings produced by vector model against the human rankings. Sentiment Analysis. Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as features in an E2-regularized logistic regression for classification. The classifier is tuned on the dev set and accuracy is reported on the test set. NP-Bracketing. Lazaridou et al. (2013) constructed a dataset from the Pe</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Nathan Schneider</author>
<author>Dirk Hovy</author>
<author>Archna Bhatia</author>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Augmenting english adjective senses with supersenses.</title>
<date>2014</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="5630" citStr="Tsvetkov et al., 2014" startWordPosition="831" endWordPosition="834">.N.01 as features as it can be both a verb and a noun. In addition to synsets, we include the hyponym (for ex. HYPO.COLLAGEFILM.N.01), hypernym (for ex. HYPER:SHEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.NOUN.ANIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word 2http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz types, which suggests that the word types evoking the same frame should be semantically related. For every word, we use the frame it evokes along with the roles of the evoked frame as its features</context>
</contexts>
<marker>Tsvetkov, Schneider, Hovy, Bhatia, Faruqui, Dyer, 2014</marker>
<rawString>Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna Bhatia, Manaal Faruqui, and Chris Dyer. 2014. Augmenting english adjective senses with supersenses. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>JAIR,</journal>
<pages>141--188</pages>
<contexts>
<context position="1559" citStr="Turney and Pantel, 2010" startWordPosition="220" endWordPosition="223">resentations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and interpretability is desired in computational models. Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like Wo</context>
<context position="13614" citStr="Turney and Pantel, 2010" startWordPosition="2142" endWordPosition="2145">N word types and D linguistic features, then we can obtain U E RN×K from the SVD of L as follows: L = UEVT, with K being the desired length of the lower dimensional space. We compare both sparse and dense linguistic vectors to three widely used distributional word vector models. The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively. We used latent semantic analysis (LSA) to obtain word vectors from the SVD decomposition of a word-word cooccurrence matrix (Turney and Pantel, 2010). These were trained on 1 billion words of Wikipedia with vector length 300 and context window of 5 words. 3.3 Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove &amp; LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge margin on SimLex. Linguistic vectors also perform better at RG-65. On sentiment analysis, linguistic vectors are competitive with Skip-Gram vectors and on the NP-bracketing task they outperform all distributional vectors with a statistically si</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. JAIR, pages 141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Proc. of ECML.</booktitle>
<contexts>
<context position="1533" citStr="Turney, 2001" startWordPosition="218" endWordPosition="219">istributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and interpretability is desired in computational models. Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed li</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Proc. of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Xu</author>
<author>Yalong Bai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Gang Wang</author>
<author>Xiaoguang Liu</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Rcnet: A general framework for incorporating knowledge into word representations.</title>
<date>2014</date>
<booktitle>In Proc. of CIKM.</booktitle>
<contexts>
<context position="15451" citStr="Xu et al., 2014" startWordPosition="2423" endWordPosition="2426">al but they are also sparse, which makes them computationally easy to work with. 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, i</context>
</contexts>
<marker>Xu, Bai, Bian, Gao, Wang, Liu, Liu, 2014</marker>
<rawString>Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rcnet: A general framework for incorporating knowledge into word representations. In Proc. of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="15472" citStr="Yu and Dredze, 2014" startWordPosition="2427" endWordPosition="2430">lso sparse, which makes them computationally easy to work with. 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identif</context>
</contexts>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>