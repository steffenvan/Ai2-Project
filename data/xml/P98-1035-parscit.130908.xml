<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.950196">
Exploiting Syntactic Structure for Language Modeling
</title>
<note confidence="0.69638475">
Ciprian Chelba and Frederick Jelinek
Center for Language and Speech Processing
The Johns Hopkins University, Barton Hall 320
3400 N. Charles St., Baltimore, MD-21218, USA
</note>
<email confidence="0.884371">
{chelbajelinek}Ojhu.edu
</email>
<sectionHeader confidence="0.996194" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999757416666667">
The paper presents a language model that devel-
ops syntactic structure and uses it to extract mean-
ingful information from the word history, thus en-
abling the use of long distance dependencies. The
model assigns probability to every joint sequence
of words-binary-parse-structure with headword an-
notation and operates in a left-to-right manner —
therefore usable for automatic speech recognition.
The model, its probabilistic parameterization, and a
set of experiments meant to evaluate its predictive
power are presented; an improvement over standard
trigram modeling is achieved.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.915372">
The main goal of the present work is to develop a lan-
guage model that uses syntactic structure to model
long-distance dependencies. During the summer96
DoD Workshop a similar attempt was made by the
dependency modeling group. The model we present
is closely related to the one investigated in (Chelba
et al., 1997), however different in a few important
aspects:
• our model operates in a left-to-right manner, al-
lowing the decoding of word lattices, as opposed to
the one referred to previously, where only whole sen-
tences could be processed, thus reducing its applica-
bility to n-best list re-scoring; the syntactic structure
is developed as a model component;
• our model is a factored version of the one
in (Chelba et al., 1997), thus enabling the calculation
of the joint probability of words and parse structure;
this was not possible in the previous case due to the
huge computational complexity of the model.
Our model develops syntactic structure incremen-
tally while traversing the sentence from left to right.
This is the main difference between our approach
and other approaches to statistical natural language
parsing. Our parsing strategy is similar to the in-
cremental syntax ones proposed relatively recently
in the linguistic community (Philips, 1996). The
probabilistic model, its parameterization and a few
experiments that are meant to evaluate its potential
for speech recognition are presented.
</bodyText>
<figure confidence="0.665752">
//&apos;\ // cents_NP
the_DT contract_NN odc•I &amp;quot;Itt: with_IN a_DT loss_NN of_IN 7_CD %, after
</figure>
<figureCaption confidence="0.998842">
Figure 1: Partial parse
</figureCaption>
<sectionHeader confidence="0.917303" genericHeader="method">
2 The Basic Idea and Terminology
</sectionHeader>
<bodyText confidence="0.999828566666667">
Consider predicting the word after in the sentence:
the contract ended with a loss of 7 cents
after trading as low as 89 cents.
A 3-gram approach would predict after from
(7, cents) whereas it is intuitively clear that the
strongest predictor would be ended which is outside
the reach of even 7-grams. Our assumption is that
what enables humans to make a good prediction of
after is the syntactic structure in the past. The
linguistically correct partial parse of the word his-
tory when predicting after is shown in Figure 1.
The word ended is called the headword of the con-
stituent (ended (with (...))) and ended is an ex-
posed headword when predicting after — topmost
headword in the largest constituent that contains it.
The syntactic structure in the past filters out irrel-
evant words and points to the important ones, thus
enabling the use of long distance information when
predicting the next word.
Our model will attempt to build the syntactic
structure incrementally while traversing the sen-
tence left-to-right. The model will assign a probabil-
ity P(W,T) to every sentence W with every possible
POStag assignment, binary branching parse, non-
terminal label and headword annotation for every
constituent of T.
Let W be a sentence of length n words to which
we have prepended &lt;s&gt; and appended &lt;/s&gt; so
that wo =&lt;s&gt; and w,,±1 =&lt;/s&gt;. Let Wk be the
word k-prefix wo wk of the sentence and Wk Tk
</bodyText>
<figure confidence="0.7743575">
VP
ith_PP
oss_NP
of_PP
225
T_(-m)
h_( -m) (&lt;s&gt;. SB) h_
h_.■ -2 )
h_1.1
h_O
(&lt;s&gt; SB)
(w—P, LP) O&apos;,—(P*1).1—(ps.1)) (w_k. 1._k) k..&apos;I I
</figure>
<figureCaption confidence="0.78827">
Figure 2: A word-parse k-prefix Figure 4: Before an adjoin operation
</figureCaption>
<equation confidence="0.4912388">
h&apos;_0 = (kJ ).word, NTIabel)
TA1114.1)&lt;-&lt;s&gt;
&lt;s&gt;
h&apos;_(-1)= h_(-2)
(&lt;s&gt;, SB) (w_I• I—I) (w_n, t_n) (&lt;/s&gt;, SE)
</equation>
<figureCaption confidence="0.998779">
Figure 3: Complete parse
</figureCaption>
<bodyText confidence="0.9984194">
the word-parse k-prefix. To stress this point, a
word-parse k-prefix contains — for a given parse
— only those binary subtrees whose span is com-
pletely included in the word k-prefix, excluding
wo =&lt;s&gt;. Single words along with their POStag
can be regarded as root-only trees. Figure 2 shows
a word-parse k-prefix; h_O h_{-m} are the ex-
posed heads, each head being a pair(headword, non-
terminal label), or (word, POStag) in the case of a
root-only tree. A complete parse — Figure 3 — is
any binary parse of the
(wi ti) • • • (Wn, to) (&lt;Is&gt; , SE) sequence with the
restriction that (&lt;/s&gt;, TOP&apos;) is the only allowed
head. Note that ((wi, ti) .(w, t)) needn&apos;t be a
constituent, but for the parses where it is, there is
no restriction on which of its words is the headword
or what is the non-terminal label that accompanies
the headword.
The model will operate by means of three mod-
ules:
</bodyText>
<listItem confidence="0.9957343">
• WORD-PREDICTOR predicts the next word
wk+1 given the word-parse k-prefix and then passes
control to the TAGGER;
• TAGGER predicts the POStag of the next word
tk+1 given the word-parse k-prefix and the newly
predicted word and then passes control to the
PARSER;
• PARSER grows the already existing binary
branching structure by repeatedly generating the
transitions:
</listItem>
<bodyText confidence="0.954075333333333">
(unary, NTlabel), (adjoin-left, NTlabel) or
(adj oin-right , NTlabel) until it passes control
to the PREDICTOR by taking a null transition.
NTlabel is the non-terminal label assigned to the
newly built constituent and {left , right} specifies
where the new headword is inherited from.
The operations performed by the PARSER are
illustrated in Figures 4-6 and they ensure that all
possible binary branching parses with all possible
</bodyText>
<figureCaption confidence="0.998355">
Figure 5: Result of adjoin-left under NTlabel
</figureCaption>
<bodyText confidence="0.9988038">
headword and non-terminal label assignments for
the w1 wk word sequence can be generated. The
following algorithm formalizes the above description
of the sequential generation of a sentence with a
complete parse.
</bodyText>
<equation confidence="0.999786333333333">
Transition t; // a PARSER transition
predict (&lt;s&gt;, SB);
do{
//WORD-PREDICTOR and TAGGER
predict (next_word, POStag);
//PARSER
do{
if(h_f-11.word != &lt;s&gt;){
if(h_O.word == &lt;/s&gt;)
t = (adjoin-right, TOP&apos;);
else{
if(h_O.tag == NTlabel)
t = [(adjoin-{left,right}, NTlabel),
null];
else
t = [(unary, NTlabel),
(adjoin-{left,right}, NTlabel),
null];
1
else{
if(h_O.tag == NTlabel)
t = null;
else
t = [(unary, NTlabel), null];
}while(t != null) //done PARSER
}while(!(h_O.word==&lt;/s&gt; &amp;&amp; h_{-1}.word==&lt;s&gt;))
t = (adjoin-right, TOP) ; //adjoin &lt;s&gt;_SB; DONE;
</equation>
<bodyText confidence="0.99738575">
The unary transition is allowed only when the
most recent exposed head is a leaf of the tree —
a regular word along with its POStag — hence it
can be taken at most once at a given position in the
</bodyText>
<page confidence="0.992556">
226
</page>
<figure confidence="0.992471666666667">
h&apos;_( -I }=h_f -2 } h_O= (h_O.word, NTlabel)
1-_( -m51 }&lt;-&lt;s&gt;
&lt;5&gt;
</figure>
<figureCaption confidence="0.999984">
Figure 6: Result of adjoin-right under NTlabel
</figureCaption>
<bodyText confidence="0.999642125">
input word string. The second subtree in Figure 2
provides an example of a unary transition followed
by a null transition.
It is easy to see that any given word sequence
with a possible parse and headword annotation is
generated by a unique sequence of model actions.
This will prove very useful in initializing our model
parameters from a treebank — see section 3.5.
</bodyText>
<sectionHeader confidence="0.999208" genericHeader="method">
3 Probabilistic Model
</sectionHeader>
<bodyText confidence="0.9996915">
The probability P(W,T) of a word sequence W and
a complete parse T can be broken into:
</bodyText>
<equation confidence="0.997404666666667">
P(W,T)
fl Lti [ p(wk/wk_iTk-i)-P (t /W
_ k , k - 1 Tk -1 Wk )
N,,
11 P(pl: IWk-171-1,Wk,tk)Pt • • • Pic-i)j(1)
i=1
</equation>
<bodyText confidence="0.690872">
where:
</bodyText>
<listItem confidence="0.999843125">
• Wk-171--1 is the word-parse (k - 1)-prefix
• wk is the word predicted by WORD-PREDICTOR
• tk is the tag assigned to wk by the TAGGER
• Nk - 1 is the number of operations the PARSER
executes before passing control to the WORD-
PREDICTOR (the Nk-th operation at position k is
the null transition); Nk is a function of T
• pi denotes the i-th PARSER operation carried out
</listItem>
<equation confidence="0.962581692307692">
at position k in the word string;
E (unary, NTlabel),
(adjoin-left, NTlabel),
(adjoin-right, NTlabel), null},
E { (adjoin-left, NTlabel),
(adjoin-right, NTlabel)},1&lt; &lt; Nk
k
pi - _
null, i = Nk
Our model is based on three probabilities:
P(wk/Wk-iTk-i) (2)
P(tk I Wk,Wk-lTk-1) (3)
Fqp! Iwk,tk7Wk-171-1)Pj • • •71:-1) (4)
</equation>
<bodyText confidence="0.999827777777778">
As can be seen, (wk,tk,Wk--171-1, Pt • • • 14 1) is one
of the Nk word-parse k-prefixes WkTk at position k
in the sentence, i = I, Nk.
To ensure a proper probabilistic model (1) we
have to make sure that (2), (3) and (4) are well de-
fined conditional probabilities and that the model
halts with probability one. Consequently, certain
PARSER and WORD-PREDICTOR probabilities
must be given specific values:
</bodyText>
<listItem confidence="0.994634285714286">
• P(nu111WkTk) = 1, if h_{-1}.word = &lt;s&gt; and
h_{0} (&lt;/s&gt;, TOP&apos;) — that is, before predicting
&lt;/s&gt; — ensures that (&lt;s&gt;, SB) is adjoined in the
last step of the parsing process;
• P((adjoin-right, TOP)/WkTk) = 1, if
h_O = (&lt;/s&gt;, TOP&apos;) and h_{-1} .word = &lt;s&gt;
and
</listItem>
<bodyText confidence="0.94624975">
P((adjoin-right, TOP&apos;)/WkTk) = 1, if
h_O = (&lt;/s&gt;, TOP&apos;) and h_{-1}. word 0 &lt;s&gt;
ensure that the parse generated by our model is con-
sistent with the definition of a complete parse;
</bodyText>
<listItem confidence="0.9866356">
• P( (unary , NTlabel)/WkTk) = 0, if h_O.tag
POStag ensures correct treatment of unary produc-
tions;
• 3€ &gt; 0, VWk-iTk-1, P(Wk=&lt;/ S&gt; IWk-iTk-i) &gt;
ensures that the model halts with probability one.
</listItem>
<bodyText confidence="0.999378">
The word-predictor model (2) predicts the next
word based on the preceding 2 exposed heads, thus
making the following equivalence classification:
</bodyText>
<equation confidence="0.705573">
P(Wk /Wk - 1 Tk -1 ) = P (Wk h -1 )
</equation>
<bodyText confidence="0.9997426">
After experimenting with several equivalence clas-
sifications of the word-parse prefix for the tagger
model, the conditioning part of model (3) was re-
duced to using the word to be tagged and the tags
of the two most recent exposed heads:
</bodyText>
<equation confidence="0.947771">
P(tk I Wk,Wk-lTk-1) = P(tkiWk,ho.tag,h_i.tag)
</equation>
<bodyText confidence="0.999933571428572">
Model (4) assigns probability to different parses of
the word k-prefix by chaining the elementary oper-
ations described above. The workings of the parser
module are similar to those of Spatter (Jelinek et al.,
1994). The equivalence classification of the WkTk
word-parse we used for the parser model (4) was the
same as the one used in (Collins, 1996):
</bodyText>
<equation confidence="0.670604">
P(Pik IWkTk) = P(1)1: 1110,11-1)
</equation>
<bodyText confidence="0.9998795">
It is worth noting that if the binary branching
structure developed by the parser were always right-
branching and we mapped the POStag and non-
terminal label vocabularies to a single type then our
model would be equivalent to a trigram language
model.
</bodyText>
<subsectionHeader confidence="0.999642">
3.1 Modeling Tools
</subsectionHeader>
<bodyText confidence="0.999331875">
All model components — WORD-PREDICTOR,
TAGGER, PARSER — are conditional probabilis-
tic models of the type P(y/xi, x2, ,x,2) where
y,x1,x2,...,xn, belong to a mixed bag of words,
POStags, non-terminal labels and parser operations
(y only). For simplicity, the modeling method we
chose was deleted interpolation among relative fre-
quency estimates of different orders f„(.) using a
</bodyText>
<page confidence="0.959157">
227
</page>
<bodyText confidence="0.586166">
recursive mixing scheme:
</bodyText>
<equation confidence="0.99747">
P(Y/xi • • , xn) =
A(xi, • • • , Xn) • P(Y IX1, • • Xn-i)
(1 — A(Xi, Xn)) • fn(YIX1,•••,x.), (5)
f–i(y) = uniform(vocabulary(y)) (6)
</equation>
<bodyText confidence="0.999421142857143">
As can be seen, the context mixing scheme dis-
cards items in the context in right-to-left order. The
A coefficients are tied based on the range of the
count C(xi, , xn). The approach is a standard
one which doesn&apos;t require an extensive description
given the literature available on it (Jelinek and Mer-
cer, 1980).
</bodyText>
<subsectionHeader confidence="0.999872">
3.2 Search Strategy
</subsectionHeader>
<bodyText confidence="0.999919307692308">
Since the number of parses for a given word prefix
Wk grows exponentially with k,I{Tk}1 0(2k), the
state space of our model is huge even for relatively
short sentences so we had to use a search strategy
that prunes it. Our choice was a synchronous multi-
stack search algorithm which is very similar to a
beam search.
Each stack contains hypotheses — partial parses
— that have been constructed by the same number of
predictor and the same number of parser operations.
The hypotheses in each stack are ranked according
to the ln(P(W, T)) score, highest on top. The width
of the search is controlled by two parameters:
</bodyText>
<listItem confidence="0.934190142857143">
• the maximum stack depth — the maximum num-
ber of hypotheses the stack can contain at any given
state;
• log-probability threshold — the difference between
the log-probability score of the top-most hypothesis
and the bottom-most hypothesis at any given state
of the stack cannot be larger than a given threshold.
</listItem>
<figureCaption confidence="0.7323045">
Figure 7 shows schematically the operations asso-
ciated with the scanning of a new word wk+1. The
</figureCaption>
<bodyText confidence="0.996817125">
above pruning strategy proved to be insufficient so
we chose to also discard all hypotheses whose score
is more than the log-probability threshold below the
score of the topmost hypothesis. This additional
pruning step is performed after all hypotheses in
stage k&apos; have been extended with the null parser
transition and thus prepared for scanning a new
word.
</bodyText>
<subsectionHeader confidence="0.998207">
3.3 Word Level Perplexity
</subsectionHeader>
<bodyText confidence="0.9991555">
The conditional perplexity calculated by assigning
to a whole sentence the probability:
</bodyText>
<equation confidence="0.973875">
P(WIT*)= 11Ponk±i/wkm, (7)
k=0
</equation>
<bodyText confidence="0.99974175">
where T* = argmaxTP(W,T), is not valid because
it is not causal: when predicting wk+i we use T*
which was determined by looking at the entire sen-
tence. To be able to compare the perplexity of our
</bodyText>
<figureCaption confidence="0.996671">
Figure 7: One search extension cycle
</figureCaption>
<bodyText confidence="0.995617833333333">
model with that resulting from the standard tri-
gram approach, we need to factor in the entropy of
guessing the correct parse I&apos;: before predicting wk+i,
based solely on the word prefix Wk.
The probability assignment for the word at posi-
tion k + 1 in the input sentence is made using:
</bodyText>
<equation confidence="0.980947">
P(wk+i/wk) =
ETkESk P(Wk+1 /WkTk) •
p(wk,To =P(wkTk)/ E P(Wil)
Tkesk
</equation>
<bodyText confidence="0.999847">
which ensures a proper probability over strings W*,
where Sk is the set of all parses present in our stacks
at the current stage k.
Another possibility for evaluating the word level
perplexity of our model is to approximate the prob-
ability of a whole sentence:
</bodyText>
<equation confidence="0.97378">
P(W) = E P(W,T(k)) (10)
k=1
</equation>
<bodyText confidence="0.999944666666667">
where T(k) is one of the &amp;quot;N-best&amp;quot; — in the sense
defined by our search — parses for W. This is a
deficient probability assignment, however useful for
justifying the model parameter re-estimation.
The two estimates (8) and (10) are both consistent
in the sense that if the sums are carried over all
</bodyText>
<page confidence="0.987621">
228
</page>
<figure confidence="0.990699071428572">
(k&apos;) \ ss (k+1)
- -
0 parser op&amp;quot; - - 0 parser op
)1(+1 predict._ A :+1 predict.
_
p parser op
)k+1 predict.
p+1 parser
)k+1 predict.
P_k parser
)k+1 predict.
P_k+1 parse 13_k+ 1 parser
_ _ _ _
+I predict. I predict.
\ word predictor
and tagger \ null parser transitions
1,
parser adjoin/unary transitions
(k)
0 parser o
k predict.
p parser op
k predict.
p+1 parser
k predict.
P_k parser
k predict.
p parser op
— ■- - - ?Ak+ I predict.
1
1 : &apos; &apos; p+1 parser
i
- - - - 1-, - - ?-ic+ I predict.
&amp;quot;i
1
1
1
1
P_k parser
_ _ ir _ _11
Ak+ I predict.
1
</figure>
<bodyText confidence="0.995572">
possible parses we get the correct value for the word
level perplexity of our model.
</bodyText>
<subsectionHeader confidence="0.940356">
3.4 Parameter Re-estimation
</subsectionHeader>
<bodyText confidence="0.9984135625">
The major problem we face when trying to reesti-
mate the model parameters is the huge state space of
the model and the fact that dynamic programming
techniques similar to those used in HMM parame-
ter re-estimation cannot be used with our model.
Our solution is inspired by an HMM re-estimation
technique that works on pruned — N-best — trel-
lises(Byrne et al., 1998).
Let (W, T(k)), k = 1 . . . N be the set of hypothe-
ses that survived our pruning strategy until the end
of the parsing process for sentence W. Each of
them was produced by a sequence of model actions,
chained together as described in section 2; let us call
the sequence of model actions that produced a given
(W, T) the derivation(W,T).
Let an elementary event in the derivation(W,T)
</bodyText>
<listItem confidence="0.837674">
be (e&apos;),4&apos;)) where:
• 1 is the index of the current model action;
• mi is the model component — WORD-
PREDICTOR, TAGGER, PARSER — that takes
action number 1 in the derivation(W,T);
• y (ml)is the action taken at position 1 in the deriva-
tion:
if mi = WORD-PREDICTOR, then yr&apos;) is a word;
if mi = TAGGER, then yrni) is a POStag;
if mi = PARSER, then e&apos;) is a parser-action;
• 14(m1) is the context in which the above action was
taken:
if mi = WORD-PREDICTOR or PARSER, then
(&apos;) = (ho.tag, ho.word, h_i .tag , h_i.word);
if mi = TAGGER, then
</listItem>
<bodyText confidence="0.998650454545454">
= (word-to-tag, ho.tag , h_i.tag).
The probability associated with each model ac-
tion is determined as described in section 3.1, based
on counts C(m)(y(m), x(m)), one set for each model
component.
Assuming that the deleted interpolation coeffi-
cients and the count ranges used for tying them stay
fixed, these counts are the only parameters to be
re-estimated in an eventual re-estimation procedure;
indeed, once a set of counts C(m)(y(m), x(m)) is spec-
ified for a given model m, we can easily calculate:
</bodyText>
<listItem confidence="0.995473">
• the relative frequency estimates
</listItem>
<equation confidence="0.696989">
:0TO( (m)/ (in) \
</equation>
<bodyText confidence="0.69739">
in ky /Xn ) for all context orders
n = 0. . .maximum-order (mode/ (m));
</bodyText>
<listItem confidence="0.989459333333333">
• the count C(m)(44)) used for determining the
A(47)) value to be used with the order-n context
(n)
</listItem>
<equation confidence="0.538129">
X n •
</equation>
<bodyText confidence="0.99419766101695">
This is all we need for calculating the probability of
an elementary event and then the probability of an
entire derivation.
One training iteration of the re-estimation proce-
dure we propose is described by the following algo-
rithm:
N-best parse development data; // counts.Ei
// prepare counts.E(i+1)
for each model component c{
gather_counts development model_c;
}
In the parsing stage we retain for each &amp;quot;N-best&amp;quot; hy-
pothesis (W, T(k)), k = 1 . . . N, only the quantity
0(w77-0)) = p(w, T(0) / N-,N1 &apos; p k/w7 T(k))
and its derivation(W,T(k)). We then scan all
the derivations in the &amp;quot;development set&amp;quot; and, for
each occurrence of the elementary event (y(m) , x(m))
in derivation(W,T(k)) we accumulate the value
0(W, T(k)) in the C(m)(y(m), x(m)) counter to be
used in the next iteration.
The intuition behind this procedure is that
0(W, T(k)) is an approximation to the P(T(k)/W)
probability which places all its mass on the parses
that survived the parsing process; the above proce-
dure simply accumulates the expected values of the
counts Om) (y(m) , x(m)) under the 0(W, T(k)) con-
ditional distribution. As explained previously, the
Om) (y(m) , x(m)) counts are the parameters defining
our model, making our procedure similar to a rigor-
ous EM approach (Dempster et al., 1977).
A particular — and very interesting — case is that
of events which had count zero but get a non-zero
count in the next iteration, caused by the &amp;quot;N-best&amp;quot;
nature of the re-estimation process. Consider a given
sentence in our &amp;quot;development&amp;quot; set. The &amp;quot;N-best&amp;quot;
derivations for this sentence are trajectories through
the state space of our model. They will change
from one iteration to the other due to the smooth-
ing involved in the probability estimation and the
change of the parameters — event counts — defin-
ing our model, thus allowing new events to appear
and discarding others through purging low probabil-
ity events from the stacks. The higher the number
of trajectories per sentence, the more dynamic this
change is expected to be.
The results we obtained are presented in the ex-
periments section. All the perplexity evaluations
were done using the left-to-right formula (8) (L2R-
PPL) for which the perplexity on the &amp;quot;development
set&amp;quot; is not guaranteed to decrease from one itera-
tion to another. However, we believe that our re-
estimation method should not increase the approxi-
mation to perplexity based on (10) (SUM-PPL) —
again, on the &amp;quot;development set&amp;quot;; we rely on the con-
sistency property outlined at the end of section 3.3
to correlate the desired decrease in L2R-PPL with
that in SUM-PPL. No claim can be made about
the change in either L2R-PPL or SUM-PPL on test
data.
</bodyText>
<page confidence="0.995361">
229
</page>
<figure confidence="0.9406705">
A z.
Y_k Y_n Y_ I
</figure>
<figureCaption confidence="0.999662">
Figure 8: Binarization schemes
</figureCaption>
<subsectionHeader confidence="0.933005">
3.5 Initial Parameters
</subsectionHeader>
<bodyText confidence="0.97241425">
Each model component — WORD-PREDICTOR,
TAGGER, PARSER — is trained initially from a
set of parsed sentences, after each parse tree (W, T)
undergoes:
</bodyText>
<listItem confidence="0.9831061">
• headword percolation and binarization — see sec-
tion 4;
• decomposition into its derivation(W,T).
Then, separately for each m model component, we:
• gather joint counts C(m)(y(m),x(m)) from the
derivations that make up the &amp;quot;development data&amp;quot;
using 0(W, T) = 1;
• estimate the deleted interpolation coefficients on
joint counts gathered from &amp;quot;check data&amp;quot; using the
EM algorithm.
</listItem>
<bodyText confidence="0.999855666666667">
These are the initial parameters used with the re-
estimation procedure described in the previous sec-
tion.
</bodyText>
<sectionHeader confidence="0.994313" genericHeader="method">
4 Headword Percolation and
Binarization
</sectionHeader>
<bodyText confidence="0.999929041666667">
In order to get initial statistics for our model com-
ponents we needed to binarize the UPenn Tree-
bank (Marcus et al., 1995) parse trees and perco-
late headwords. The procedure we used was to first
percolate headwords using a context-free (CF) rule-
based approach and then binarize the parses by us-
ing a rule-based approach again.
The headword of a phrase is the word that best
represents the phrase, all the other words in the
phrase being modifiers of the headword. Statisti-
cally speaking, we were satisfied with the output
of an enhanced version of the procedure described
in (Collins, 1996) — also known under the name
&amp;quot;Magerman Sz Black Headword Percolation Rules&amp;quot;.
Once the position of the headword within a con-
stituent — equivalent with a CF production of the
type Z -4 Y1... Y,2 , where Z, Yr, are non-
terminal labels or POStags (only for Yi) — is iden-
tified to be k, we binarize the constituent as follows:
depending on the Z identity, a fixed rule is used
to decide which of the two binarization schemes in
Figure 8 to apply. The intermediate nodes created
by the above binarization schemes receive the non-
terminal label Z&apos;.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999862285714286">
Due to the low speed of the parser — 200 wds/min
for stack depth 10 and log-probability threshold
6.91 nats (1/1000) — we could carry out the re-
estimation technique described in section 3.4 on only
1 Mwds of training data. For convenience we chose
to work on the UPenn Treebank corpus. The vocab-
ulary sizes were:
</bodyText>
<listItem confidence="0.9337862">
• word vocabulary: 10k, open — all words outside
the vocabulary are mapped to the &lt;unk&gt; token;
• POS tag vocabulary: 40, closed;
• non-terminal tag vocabulary: 52, closed;
• parser operation vocabulary: 107, closed;
</listItem>
<bodyText confidence="0.9917794375">
The training data was split into &amp;quot;development&amp;quot; set
— 929,564wds (sections 00-20) — and &amp;quot;check set&amp;quot;
— 73,760wds (sections 21-22); the test set size was
82,430wds (sections 23-24). The &amp;quot;check&amp;quot; set has
been used for estimating the interpolation weights
and tuning the search parameters; the &amp;quot;develop-
ment&amp;quot; set has been used for gathering/estimating
counts; the test set has been used strictly for evalu-
ating model performance.
Table 1 shows the results of the re-estimation tech-
nique presented in section 3.4. We achieved a reduc-
tion in test-data perplexity bringing an improvement
over a deleted interpolation trigram model whose
perplexity was 167.14 on the same training-test data;
the reduction is statistically significant according to
a sign test.
</bodyText>
<table confidence="0.9153574">
E0 24.70 167.47
El 22.34 160.76
E2 21.69 158.97
E3 21.26 158.28
3-gram 21.20 167.14
</table>
<tableCaption confidence="0.994655">
Table 1: Parameter re-estimation results
</tableCaption>
<bodyText confidence="0.820658">
Simple linear interpolation between our model and
the trigram model:
</bodyText>
<equation confidence="0.926085">
Q(wk+i/Wk) =
A P(wk+i/wk-i,wk) + (1 + A) &apos; P(wk+iiWk)
</equation>
<bodyText confidence="0.8123746">
yielded a further improvement in PPL, as shown in
Table 2. The interpolation weight was estimated on
check data to be A = 0.36.
An overall relative reduction of 11% over the trigram
model has been achieved.
</bodyText>
<sectionHeader confidence="0.996181" genericHeader="conclusions">
6 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.9983615">
The large difference between the perplexity of our
model calculated on the &amp;quot;development&amp;quot; set — used
</bodyText>
<figure confidence="0.933823857142857">
iteration
number
DEV set
L2R-PPL
TEST set
L2R-PPL
Y_k Y_n
</figure>
<page confidence="0.973348">
230
</page>
<table confidence="0.7078312">
iteration TEST set TEST set
number L2R-PPL 3-gram interpolated PPL
E0 167.47 152.25
E3 158.28 148.90
3-gram 167.14 167.14
</table>
<tableCaption confidence="0.999674">
Table 2: Interpolation with trigram results
</tableCaption>
<bodyText confidence="0.999934727272727">
for model parameter estimation — and &amp;quot;test&amp;quot; set —
unseen data — shows that the initial point we choose
for the parameter values has already captured a lot
of information from the training data. The same
problem is encountered in standard n-gram language
modeling; however, our approach has more flexibility
in dealing with it due to the possibility of reestimat-
ing the model parameters.
We believe that the above experiments show the
potential of our approach for improved language
models. Our future plans include:
</bodyText>
<listItem confidence="0.760598666666667">
• experiment with other parameterizations than the
two most recent exposed heads in the word predictor
model and parser;
• estimate a separate word predictor for left-to-
right language modeling. Note that the correspond-
ing model predictor was obtained via re-estimation
aimed at increasing the probability of the &amp;quot;N-best&amp;quot;
parses of the entire sentence;
• reduce vocabulary of parser operations; extreme
case: no non-terminal labels/POS tags, word only
model; this will increase the speed of the parser
thus rendering it usable on larger amounts of train-
ing data and allowing the use of deeper stacks —
resulting in more &amp;quot;N-best&amp;quot; derivations per sentence
during re-estimation;
• relax — flatten — the initial statistics in the re-
estimation of model parameters; this would allow the
model parameters to converge to a different point
that might yield a lower word-level perplexity;
• evaluate model performance on n-best sentences
output by an automatic speech recognizer.
</listItem>
<sectionHeader confidence="0.99889" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.997348">
This research has been funded by the NSF
IRI-19618874 grant (STIMULATE).
The authors would like to thank Sanjeev Khu-
danpur for his insightful suggestions. Also to Harry
Printz, Eric Ristad, Andreas Stolcke, Dekai Wu and
all the other members of the dependency model-
ing group at the summer96 DoD Workshop for use-
ful comments on the model, programming support
and an extremely creative environment. Also thanks
to Eric Brill, Sanjeev Khudanpur, David Yarowsky,
Radu Florian, Lidia Mangu and Jun Wu for useful
input during the meetings of the people working on
our STIMULATE grant.
</bodyText>
<sectionHeader confidence="0.998327" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999772555555556">
W. Byrne, A. Gunawardana, and S. Khudanpur.
1998. Information geometry and EM variants.
Technical Report CLSP Research Note 17, De-
partment of Electical and Computer Engineering,
The Johns Hopkins University, Baltimore, MD.
C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu-
danpur, L. Mangu, H. Printz, E. S. Ristad,
R. Rosenfeld, A. Stolcke, and D. Wu. 1997. Struc-
ture and performance of a dependency language
model. In Proceedings of Eurospeech, volume 5,
pages 2775-2778. Rhodes, Greece.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 184-
191. Santa Cruz, CA.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. In Journal of the Royal Statistical
Society, volume 39 of B, pages 1-38.
Frederick Jelinek and Robert Mercer. 1980. Inter-
polated estimation of markov source parameters
from sparse data. In E. Gelsema and L. Kanal, ed-
itors, Pattern Recognition in Practice, pages 381-
397.
F. Jelinek, J. Lafferty, D. M. Magerman, R. Mercer,
A. Ratnaparkhi, and S. Roukos. 1994. Decision
tree parsing using a hidden derivational model.
In ARPA, editor, Proceedings of the Human Lan-
guage Technology Workshop, pages 272-277.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1995. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Lin-
guistics, 19(2):313-330.
Colin Philips. 1996. Order and Structure. Ph.D.
thesis, MIT. Distributed by MITWPL.
</reference>
<page confidence="0.997929">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966336">
<title confidence="0.999988">Exploiting Syntactic Structure for Language Modeling</title>
<author confidence="0.998746">Chelba Jelinek</author>
<affiliation confidence="0.998527">Center for Language and Speech Processing</affiliation>
<address confidence="0.9970455">The Johns Hopkins University, Barton Hall 320 3400 N. Charles St., Baltimore, MD-21218, USA</address>
<email confidence="0.999789">{chelbajelinek}Ojhu.edu</email>
<abstract confidence="0.997984615384615">The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Byrne</author>
<author>A Gunawardana</author>
<author>S Khudanpur</author>
</authors>
<title>Information geometry and EM variants.</title>
<date>1998</date>
<tech>Technical Report CLSP Research Note 17,</tech>
<institution>Department of Electical and Computer Engineering, The Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<contexts>
<context position="14977" citStr="Byrne et al., 1998" startWordPosition="2552" endWordPosition="2556">parser op — ■- - - ?Ak+ I predict. 1 1 : &apos; &apos; p+1 parser i - - - - 1-, - - ?-ic+ I predict. &amp;quot;i 1 1 1 1 P_k parser _ _ ir _ _11 Ak+ I predict. 1 possible parses we get the correct value for the word level perplexity of our model. 3.4 Parameter Re-estimation The major problem we face when trying to reestimate the model parameters is the huge state space of the model and the fact that dynamic programming techniques similar to those used in HMM parameter re-estimation cannot be used with our model. Our solution is inspired by an HMM re-estimation technique that works on pruned — N-best — trellises(Byrne et al., 1998). Let (W, T(k)), k = 1 . . . N be the set of hypotheses that survived our pruning strategy until the end of the parsing process for sentence W. Each of them was produced by a sequence of model actions, chained together as described in section 2; let us call the sequence of model actions that produced a given (W, T) the derivation(W,T). Let an elementary event in the derivation(W,T) be (e&apos;),4&apos;)) where: • 1 is the index of the current model action; • mi is the model component — WORDPREDICTOR, TAGGER, PARSER — that takes action number 1 in the derivation(W,T); • y (ml)is the action taken at posit</context>
</contexts>
<marker>Byrne, Gunawardana, Khudanpur, 1998</marker>
<rawString>W. Byrne, A. Gunawardana, and S. Khudanpur. 1998. Information geometry and EM variants. Technical Report CLSP Research Note 17, Department of Electical and Computer Engineering, The Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>D Engle</author>
<author>F Jelinek</author>
<author>V Jimenez</author>
<author>S Khudanpur</author>
<author>L Mangu</author>
<author>H Printz</author>
<author>E S Ristad</author>
<author>R Rosenfeld</author>
<author>A Stolcke</author>
<author>D Wu</author>
</authors>
<title>Structure and performance of a dependency language model.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<volume>5</volume>
<pages>2775--2778</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="1166" citStr="Chelba et al., 1997" startWordPosition="169" endWordPosition="172">rd annotation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. 1 Introduction The main goal of the present work is to develop a language model that uses syntactic structure to model long-distance dependencies. During the summer96 DoD Workshop a similar attempt was made by the dependency modeling group. The model we present is closely related to the one investigated in (Chelba et al., 1997), however different in a few important aspects: • our model operates in a left-to-right manner, allowing the decoding of word lattices, as opposed to the one referred to previously, where only whole sentences could be processed, thus reducing its applicability to n-best list re-scoring; the syntactic structure is developed as a model component; • our model is a factored version of the one in (Chelba et al., 1997), thus enabling the calculation of the joint probability of words and parse structure; this was not possible in the previous case due to the huge computational complexity of the model.</context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, Ristad, Rosenfeld, Stolcke, Wu, 1997</marker>
<rawString>C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudanpur, L. Mangu, H. Printz, E. S. Ristad, R. Rosenfeld, A. Stolcke, and D. Wu. 1997. Structure and performance of a dependency language model. In Proceedings of Eurospeech, volume 5, pages 2775-2778. Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="10075" citStr="Collins, 1996" startWordPosition="1685" endWordPosition="1686"> with several equivalence classifications of the word-parse prefix for the tagger model, the conditioning part of model (3) was reduced to using the word to be tagged and the tags of the two most recent exposed heads: P(tk I Wk,Wk-lTk-1) = P(tkiWk,ho.tag,h_i.tag) Model (4) assigns probability to different parses of the word k-prefix by chaining the elementary operations described above. The workings of the parser module are similar to those of Spatter (Jelinek et al., 1994). The equivalence classification of the WkTk word-parse we used for the parser model (4) was the same as the one used in (Collins, 1996): P(Pik IWkTk) = P(1)1: 1110,11-1) It is worth noting that if the binary branching structure developed by the parser were always rightbranching and we mapped the POStag and nonterminal label vocabularies to a single type then our model would be equivalent to a trigram language model. 3.1 Modeling Tools All model components — WORD-PREDICTOR, TAGGER, PARSER — are conditional probabilistic models of the type P(y/xi, x2, ,x,2) where y,x1,x2,...,xn, belong to a mixed bag of words, POStags, non-terminal labels and parser operations (y only). For simplicity, the modeling method we chose was deleted i</context>
<context position="20622" citStr="Collins, 1996" startWordPosition="3529" endWordPosition="3530">lation and Binarization In order to get initial statistics for our model components we needed to binarize the UPenn Treebank (Marcus et al., 1995) parse trees and percolate headwords. The procedure we used was to first percolate headwords using a context-free (CF) rulebased approach and then binarize the parses by using a rule-based approach again. The headword of a phrase is the word that best represents the phrase, all the other words in the phrase being modifiers of the headword. Statistically speaking, we were satisfied with the output of an enhanced version of the procedure described in (Collins, 1996) — also known under the name &amp;quot;Magerman Sz Black Headword Percolation Rules&amp;quot;. Once the position of the headword within a constituent — equivalent with a CF production of the type Z -4 Y1... Y,2 , where Z, Yr, are nonterminal labels or POStags (only for Yi) — is identified to be k, we binarize the constituent as follows: depending on the Z identity, a fixed rule is used to decide which of the two binarization schemes in Figure 8 to apply. The intermediate nodes created by the above binarization schemes receive the nonterminal label Z&apos;. 5 Experiments Due to the low speed of the parser — 200 wds/m</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 184-191. Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>In Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="17907" citStr="Dempster et al., 1977" startWordPosition="3071" endWordPosition="3074">m) , x(m)) in derivation(W,T(k)) we accumulate the value 0(W, T(k)) in the C(m)(y(m), x(m)) counter to be used in the next iteration. The intuition behind this procedure is that 0(W, T(k)) is an approximation to the P(T(k)/W) probability which places all its mass on the parses that survived the parsing process; the above procedure simply accumulates the expected values of the counts Om) (y(m) , x(m)) under the 0(W, T(k)) conditional distribution. As explained previously, the Om) (y(m) , x(m)) counts are the parameters defining our model, making our procedure similar to a rigorous EM approach (Dempster et al., 1977). A particular — and very interesting — case is that of events which had count zero but get a non-zero count in the next iteration, caused by the &amp;quot;N-best&amp;quot; nature of the re-estimation process. Consider a given sentence in our &amp;quot;development&amp;quot; set. The &amp;quot;N-best&amp;quot; derivations for this sentence are trajectories through the state space of our model. They will change from one iteration to the other due to the smoothing involved in the probability estimation and the change of the parameters — event counts — defining our model, thus allowing new events to appear and discarding others through purging low pr</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. In Journal of the Royal Statistical Society, volume 39 of B, pages 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<editor>In E. Gelsema and L. Kanal, editors,</editor>
<contexts>
<context position="11229" citStr="Jelinek and Mercer, 1980" startWordPosition="1878" endWordPosition="1882">ns (y only). For simplicity, the modeling method we chose was deleted interpolation among relative frequency estimates of different orders f„(.) using a 227 recursive mixing scheme: P(Y/xi • • , xn) = A(xi, • • • , Xn) • P(Y IX1, • • Xn-i) (1 — A(Xi, Xn)) • fn(YIX1,•••,x.), (5) f–i(y) = uniform(vocabulary(y)) (6) As can be seen, the context mixing scheme discards items in the context in right-to-left order. The A coefficients are tied based on the range of the count C(xi, , xn). The approach is a standard one which doesn&apos;t require an extensive description given the literature available on it (Jelinek and Mercer, 1980). 3.2 Search Strategy Since the number of parses for a given word prefix Wk grows exponentially with k,I{Tk}1 0(2k), the state space of our model is huge even for relatively short sentences so we had to use a search strategy that prunes it. Our choice was a synchronous multistack search algorithm which is very similar to a beam search. Each stack contains hypotheses — partial parses — that have been constructed by the same number of predictor and the same number of parser operations. The hypotheses in each stack are ranked according to the ln(P(W, T)) score, highest on top. The width of the se</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In E. Gelsema and L. Kanal, editors, Pattern Recognition in Practice, pages 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>D M Magerman</author>
<author>R Mercer</author>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
</authors>
<title>Decision tree parsing using a hidden derivational model.</title>
<date>1994</date>
<booktitle>Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>272--277</pages>
<editor>In ARPA, editor,</editor>
<contexts>
<context position="9939" citStr="Jelinek et al., 1994" startWordPosition="1659" endWordPosition="1662">on the preceding 2 exposed heads, thus making the following equivalence classification: P(Wk /Wk - 1 Tk -1 ) = P (Wk h -1 ) After experimenting with several equivalence classifications of the word-parse prefix for the tagger model, the conditioning part of model (3) was reduced to using the word to be tagged and the tags of the two most recent exposed heads: P(tk I Wk,Wk-lTk-1) = P(tkiWk,ho.tag,h_i.tag) Model (4) assigns probability to different parses of the word k-prefix by chaining the elementary operations described above. The workings of the parser module are similar to those of Spatter (Jelinek et al., 1994). The equivalence classification of the WkTk word-parse we used for the parser model (4) was the same as the one used in (Collins, 1996): P(Pik IWkTk) = P(1)1: 1110,11-1) It is worth noting that if the binary branching structure developed by the parser were always rightbranching and we mapped the POStag and nonterminal label vocabularies to a single type then our model would be equivalent to a trigram language model. 3.1 Modeling Tools All model components — WORD-PREDICTOR, TAGGER, PARSER — are conditional probabilistic models of the type P(y/xi, x2, ,x,2) where y,x1,x2,...,xn, belong to a mix</context>
</contexts>
<marker>Jelinek, Lafferty, Magerman, Mercer, Ratnaparkhi, Roukos, 1994</marker>
<rawString>F. Jelinek, J. Lafferty, D. M. Magerman, R. Mercer, A. Ratnaparkhi, and S. Roukos. 1994. Decision tree parsing using a hidden derivational model. In ARPA, editor, Proceedings of the Human Language Technology Workshop, pages 272-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="20154" citStr="Marcus et al., 1995" startWordPosition="3448" endWordPosition="3451"> binarization — see section 4; • decomposition into its derivation(W,T). Then, separately for each m model component, we: • gather joint counts C(m)(y(m),x(m)) from the derivations that make up the &amp;quot;development data&amp;quot; using 0(W, T) = 1; • estimate the deleted interpolation coefficients on joint counts gathered from &amp;quot;check data&amp;quot; using the EM algorithm. These are the initial parameters used with the reestimation procedure described in the previous section. 4 Headword Percolation and Binarization In order to get initial statistics for our model components we needed to binarize the UPenn Treebank (Marcus et al., 1995) parse trees and percolate headwords. The procedure we used was to first percolate headwords using a context-free (CF) rulebased approach and then binarize the parses by using a rule-based approach again. The headword of a phrase is the word that best represents the phrase, all the other words in the phrase being modifiers of the headword. Statistically speaking, we were satisfied with the output of an enhanced version of the procedure described in (Collins, 1996) — also known under the name &amp;quot;Magerman Sz Black Headword Percolation Rules&amp;quot;. Once the position of the headword within a constituent </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1995</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1995. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Philips</author>
</authors>
<date>1996</date>
<booktitle>Order and Structure. Ph.D. thesis, MIT. Distributed by MITWPL.</booktitle>
<contexts>
<context position="2116" citStr="Philips, 1996" startWordPosition="323" endWordPosition="324">component; • our model is a factored version of the one in (Chelba et al., 1997), thus enabling the calculation of the joint probability of words and parse structure; this was not possible in the previous case due to the huge computational complexity of the model. Our model develops syntactic structure incrementally while traversing the sentence from left to right. This is the main difference between our approach and other approaches to statistical natural language parsing. Our parsing strategy is similar to the incremental syntax ones proposed relatively recently in the linguistic community (Philips, 1996). The probabilistic model, its parameterization and a few experiments that are meant to evaluate its potential for speech recognition are presented. //&apos;\ // cents_NP the_DT contract_NN odc•I &amp;quot;Itt: with_IN a_DT loss_NN of_IN 7_CD %, after Figure 1: Partial parse 2 The Basic Idea and Terminology Consider predicting the word after in the sentence: the contract ended with a loss of 7 cents after trading as low as 89 cents. A 3-gram approach would predict after from (7, cents) whereas it is intuitively clear that the strongest predictor would be ended which is outside the reach of even 7-grams. Our</context>
</contexts>
<marker>Philips, 1996</marker>
<rawString>Colin Philips. 1996. Order and Structure. Ph.D. thesis, MIT. Distributed by MITWPL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>