<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000131">
<title confidence="0.962079">
Statistical Ranking in Tactical Generation
</title>
<author confidence="0.994647">
Erik Velldal
</author>
<affiliation confidence="0.999036">
University of Oslo (Norway)
</affiliation>
<email confidence="0.980062">
erik.velldal@ifi.uio.no
</email>
<sectionHeader confidence="0.997234" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999058533333333">
In this paper we describe and evaluate
several statistical models for the task of
realization ranking, i.e. the problem of
discriminating between competing surface
realizations generated for a given input se-
mantics. Three models (and several vari-
ants) are trained and tested: ann-gram
language model, a discriminative maxi-
mum entropy model using structural in-
formation (and incorporating the language
model as a separate feature), and finally an
SVM ranker trained on the same feature
set. The resulting hybrid tactical generator
is part of a larger, semantic transfer MT
system.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879807692308">
This paper describes the application of several dif-
ferent statistical models for the task of realiza-
tion ranking in tactical generation, i.e. the problem
of choosing among multiple paraphrases that are
generated for a given meaning representation. The
specific realization component we use is the open-
source chart generator of the Linguistic Knowl-
edge Builder (LKB; Carroll, Copestake, Flickinger,
&amp; Poznanski, 1999; Carroll &amp; Oepen, 2005).
Given a meaning representation in the form of
Minimal Recursion Semantics (MRS; Copestake,
Flickinger, Malouf, Riehemann, &amp; Sag, 1995),
the generator outputs English realizations in ac-
cordance with the HPSG LinGO English Resource
Grammar (ERG; Flickinger, 2002).
As an example of generator output, a sub-set
of alternate realizations that are produced for a
single input MRS is shown in Figure 1. For the
two data sets considered in this paper, the aver-
age number of realizations produced by the gen-
erator is 85.7 and 102.2 (the maximum numbers
are 4176 and 3408, respectively). Thus, there is
immediate demand for a principled way of choos-
ing a single output among the generated candi-
dates. For this task we train and test three differ-
ent statistical models: ann-gram language model,
</bodyText>
<note confidence="0.526335333333333">
Stephan Oepen
University of Oslo (Norway)
and CSLI Stanford (CA)
</note>
<email confidence="0.884313">
oe@csli.stanford.edu
</email>
<bodyText confidence="0.999910558823529">
a maximum entropy model (MaxEnt) and a (lin-
ear) support vector machine (SVM). These are
all models that have proved popular within the
NLP community, but it is usually only the first
of these three that has been applied to the task
of ranking in sentence generation. The latter two
models that we present here go beyond the sur-
face information used by then-gram model, and
are trained on a symmetric treebank with features
defined over the full HPSG analyses of compet-
ing realizations. Furthermore, such discriminative
models are suitable for ‘on-line’ use within our
generator—adopting the technique of selective un-
packing from a packed forest (Carroll &amp; Oepen,
2005)—which means our hybrid realizer obviates
the need for exhaustive enumeration of candidate
outputs. The present results extend our earlier
work (Velldal, Oepen, &amp; Flickinger, 2004)—and
the related work of Nakanishi, Miyao, &amp; Tsu-
jii (2005)—to an enlarged data set, more feature
types, and additional learners.
The rest of this paper is structured as follows.
Section 2 first gives a general summary of the var-
ious statistical models we will be considering, as
well as the measures used for evaluating them. We
then go on to define the task we are aiming to solve
in terms of treebank data and feature types in Sec-
tion 3. By looking at different variants of the Max-
Ent model we review some results for the relative
contribution of individual features and the impact
of frequency cutoffs for feature selection. Keeping
these parameters constant then, Section 4 provides
an array of empirical results on the relative perfor-
mance of the various approaches.
</bodyText>
<sectionHeader confidence="0.997101" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999727">
In this section we briefly review the different types
of statistical models that we use for ranking the
output of the generator. We start by describing
the language model, and then go on to review the
framework for discriminative MaxEnt models and
SVM rankers. In the following we will use s and
r to denote semantic inputs and generated realiza-
tions respectively.
</bodyText>
<page confidence="0.940898">
517
</page>
<note confidence="0.995555777777778">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 517–525,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Remember that dogs must be on a leash.
Remember dogs must be on a leash.
On a leash, remember that dogs must be.
On a leash, remember dogs must be.
A leash, remember that dogs must be on.
A leash, remember dogs must be on.
Dogs, remember must be on a leash.
</note>
<tableCaption confidence="0.992787">
Table 1: A small example set of generator out-
</tableCaption>
<bodyText confidence="0.995014875">
puts using the ERG. Where the input semantics is
no specified for aspects of information structure
(e.g. requesting foregrounding of a specific entity),
paraphrases include all grammatically legitimate
topicalizations. Other choices involve, for exam-
ple, the optionality of complementizers and rela-
tive pronouns, permutation of (intersective) modi-
fiers, and lexical and orthographic alternations.
</bodyText>
<subsectionHeader confidence="0.971242">
2.1 Language Models
</subsectionHeader>
<bodyText confidence="0.993568933333334">
The use ofn-gram language models is the most
common approach to statistical selection in gen-
eration (Langkilde &amp; Knight, 1998; and White
(2004); inter alios). In order to better assert the
relative performance of the discriminative mod-
els and the structural features we present below,
we also apply a trigram model to the ranking
problem. Using the freely available CMU SLM
Toolkit (Clarkson &amp; Rosenfeld, 1997), we trained
a trigram model on an unannotated version of
the British National Corpus (BNC), containing
roughly 100 million words (using Witten-Bell dis-
counting and back-off). Given such a model pn,
the score of a realization ri with surface form
wki1=(wi1,...,wik)is then computed as
</bodyText>
<equation confidence="0.997576333333333">
k
(1)F(s,ri)= pn(wi,jIwi,j—n,...,wi,j-1)
j=1
</equation>
<bodyText confidence="0.996884">
Given the scoring functionF, the best realiza-
tion is selected according to the following decision
function:
</bodyText>
<equation confidence="0.960884">
(2) r=argmax F(s,r&apos;)
r&apos;EY(s)
</equation>
<bodyText confidence="0.997380888888889">
Although in this case scoring is not conditioned
on the input semantics at all, we still include it to
make the function formulation more general as we
will be reusing it later.
Note that, as the realizations in our symmet-
ric treebank also include punctuation marks, these
are also treated as separate tokens by the language
model (in addition to pseudo-tokens marking sen-
tence boundaries).
</bodyText>
<subsectionHeader confidence="0.999258">
2.2 Maximum Entropy Models
</subsectionHeader>
<bodyText confidence="0.999513566666667">
Maximum entropy modeling provides a very flex-
ible framework that has been widely used for a
range of tasks in NLP, including parse selection
(e.g. Johnson, Geman, Canon, Chi, &amp; Riezler,
1999; Malouf &amp; Noord, 2004) and reranking for
machine translation (e.g. Och et al., 2004). A
model is specified by a set of real-valued feature
functions that describe properties of the data, and
an associated set of learned weights that determine
the contribution of each feature.
Let us first introduce some notation before we
go on. Let Y(si) = {r1, ... , rn,} be the set of re-
alizations licensed by the grammar for a semantic
representation si. Now, let our (positive) training
data be given asXp={x1,...,xN}where each
xiis a pair(si,rj)for whichrjEY(si)andrj
is annotated in the treebank as being a correct re-
alization ofsi. Note that we might have several
different members ofY(si)that pair up withsi
in Xp. In our set-up, this is the case where multi-
ple HPSG derivations for the same input semantics
project identical surface strings.
Given a set of d features (as further described
in Section 3.2), each pair of semantic input s and
hypothesized realization r is mapped to a feature
vector -4�(s, r) E ltd. The goal is then to find a
vector of weights w E ltd that optimize the like-
lihood of the training data. A conditional MaxEnt
model of the probability of a realization r given
the semanticss, is defined as
</bodyText>
<equation confidence="0.973672">
(3) pw(rIs)= Zw(s)
eF„(s,r)
</equation>
<bodyText confidence="0.998605666666667">
where the functionFwis simply the sum of the
products of all feature values and feature weights,
given by
</bodyText>
<equation confidence="0.920572666666667">
d
(4)Fw(s,r)= wi-I�i(s,r)=w•-4�(s,r)
i=1
</equation>
<bodyText confidence="0.737326">
The normalization termZwis defined as
</bodyText>
<equation confidence="0.921665">
(5) Zw(s)=� eF„(s,r)
r&apos;EY(s)
</equation>
<bodyText confidence="0.999251">
When we want to find the best realization for a
given input semantics according to a modelpw, it
is sufficient to compute the score function as in
Equation (4) and then use the decision function
previously given in Equation (2) above. When it
</bodyText>
<page confidence="0.995089">
518
</page>
<bodyText confidence="0.990935666666667">
comes to estimating1 the parametersw, the pro-
cedure seeks to maximize the (log of) a penalized
likelihood function as in
</bodyText>
<equation confidence="0.869120166666667">
log L(w) — Ei=1 wi
lowing Joachims (2002), the goal is to minimize
(7) V(w;0=2w&apos;w+C1:�i;j;k
1
(6) w=argmax d 2 subject to the following constraints,
w 2o-2
</equation>
<bodyText confidence="0.9999253125">
where L(w) is the ‘conditionalized’ likelihood of
the training data Xp (Johnson et al., 1999), com-
puted as L(w) = HNi=1pw(rilsi). The second
term of the likelihood function in Equation (6) is
a penalty term that is commonly used for reducing
the tendency of log-linear models to over-fit, es-
pecially when training on sparse data using many
features (Chen &amp; Rosenfeld, 1999; Johnson et al.,
1999; Malouf &amp; Noord, 2004). More specifically
it defines a zero-mean Gaussian prior on the fea-
ture weights which effectively leads to less ex-
treme values. After empirically tuning the prior
on our ‘Jotunheimen’ treebank (training and test-
ing by 10-fold cross-validation), we ended up us-
ingQ2=0:003for the MaxEnt models applied in
this paper.
</bodyText>
<subsectionHeader confidence="0.998704">
2.3 SVM Rankers
</subsectionHeader>
<bodyText confidence="0.999173814814815">
In this section we briefly formulate the optimiza-
tion problem in terms of support vector machines.
Our starting point is the SVM approach introduced
in Joachims (2002) for learning ranking functions
for information retrieval. In our case the aim is to
learn a ranking function from a set of preference
relations on sentences generated for a given input
semantics.
In contrast to the MaxEnt approach, the SVM
approach has a geometric rather than probabilistic
view on the problem. Similarly to the the MaxEnt
set-up, the SVM learner will try to learn a linear
scoring function as defined in Equation (4) above.
However, instead of maximizing the probability
of the preferred or positive realizations, we try to
maximize their value forFwdirectly.
Recall our definition of the set of positive train-
ing examples in Section 2.2. Let us here analo-
gously defineXn={x1;:::;xQ}to be the neg-
ative counterpart, so that for a given pair x =
(si; rj) E Xn, we have that rj E Y(si) but rj is
not annotated as a preferred realization of si. Fol-
1We use the TADM open-source package (Malouf, 2002)
for training the models, using its limited-memory variable
metric as the optimization method and experimentally deter-
mine the optimal convergence threshold and variance of the
prior.
</bodyText>
<equation confidence="0.812945166666667">
( 8) Vijk s.t. (sk; ri) E Xp A (sk; rj) E Xn :
w &apos; -4�(sk; ri) &gt;— w (sk; rj) + 1 — �i;j;k
(9 ) Vijk : �i;j;k &gt;— 0
Joachims (2002) shows that the preference con-
straints in Equation (8) can be rewritten as
(10)w&apos;(�(sk;ri)—&apos;D(sk;rj))&gt;—1—�i;j;k
</equation>
<bodyText confidence="0.99041012">
so that the optimization problem is equivalent to
training a classifier on the pairwise difference vec-
tors-(sk;ri)—&apos;(sk;rj). The (non-negative)
slack variables �i;j;k are commonly used in SVMs
to make it possible to approximate a solution by
allowing some error in cases where a separating
hyperplane can not be found. The trade-off be-
tween maximizing the margin size and minimizing
the training error is governed by the constant C.
Using the SVMlightpackage by Joachims (1999),
we empirically specifiedC=0:005for the model
described in this paper. Note that, for the ex-
periments reported here, we will only be mak-
ing binary distinctions of preferred/non-preferred
realizations, although the approach presented in
(Joachims, 2002) is formulated for the more gen-
eral case of learning ordinal ranking functions.
Finally, given a linear SVM, we score and se-
lect realizations in the same way as we did with
the MaxEnt model, according to Equations (4) and
(2). Note, however, that it is also possible to use
non-linear kernel functions with this set-up, since
the ranking function can be represented as a linear
combination of the feature vectors as in
(11)w&apos;4�(s;ri)=1:aj;k-4�(sj;rk)-4�(s;ri)
</bodyText>
<subsectionHeader confidence="0.991155">
2.4 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9999669">
The models presented in this paper are evaluated
with respect to two simple metrics: exact match
accuracy and word accuracy. The exact match
measure simply counts the number of times that
the model assigns the highest score to a string that
exactly matches a corresponding ‘gold’ or refer-
ence sentence (i.e. a sentence that is marked as
preferred in the treebank). This score is discounted
appropriately in the case of ties between preferred
and non-preferred candidates.
</bodyText>
<page confidence="0.990117">
519
</page>
<bodyText confidence="0.999957954545455">
if several realizations are given the top rank by
the model. We also include the exact match accu-
racy for the five best candidates according to the
models (see then-best columns of Table 6).
The simple measure of exact match accuracy of-
fers a very intuitive and transparent view on model
performance. However, it is also in some respects
too harsh as an evaluation measure in our setting
since there will often be more than just one of
the candidate realizations that provides a reason-
able rendering of the input semantics. We there-
fore also include WA as similarity-based evalua-
tion metric. This measure is based on the Lev-
ensthein distance between a candidate string and
a reference, also known as edit distance. This is
given by the minimum number of deletions, sub-
stitutions and insertions of words that are required
to transform one string into another. If we letd,s
andirepresent the number of necessary deletions,
substitutions and insertions respectively, and letl
be the length of the reference, then WA is defined
as
</bodyText>
<equation confidence="0.587124">
(12) WA=1- d+s+i
l
</equation>
<bodyText confidence="0.9999814">
The scores produced by similarity measures such
as WA are often difficult to interpret, but at least
they provide an alternative view on the relative
performance of the different models that we want
to compare. We could also have used several
other similarity measures here, such as the BLEU
score which is a well-established evaluation metric
within MT, but in our experience the various string
similarity measures usually agree on the relative
ranking of the different models.
</bodyText>
<sectionHeader confidence="0.839245" genericHeader="method">
3 Data Sets and Features
</sectionHeader>
<bodyText confidence="0.999735">
The following sections summarize the data sets
and the feature types used in the experiments.
</bodyText>
<subsectionHeader confidence="0.999312">
3.1 Symmetric Treebanks
</subsectionHeader>
<bodyText confidence="0.999269090909091">
Conditional parse selection models are standardly
trained on a treebank consisting of strings paired
with their optimal analyses. For our discriminative
realization ranking experiments we require train-
ing corpora that provide the inverse relation. By
assuming that the preferences captured in a stan-
dard treebank can constitute a bidirectional rela-
tion, Velldal et al. (2004) propose a notion of sym-
metric treebanks as the combination of (a) a set of
pairings of surface forms and associated seman-
tics; combined with (b) the sets of alternative anal-
</bodyText>
<table confidence="0.97841075">
Jotunheimen
Bin Items Words Trees Gold Chance
100&lt;n 396 21.7 367.4 20.7 0.083
50&lt;n&lt;100 246 18.5 73.7 11.5 0.160
10&lt;n&lt;50 831 14.8 24.2 6.3 0.277
5&lt;n&lt;10 426 10.1 7.0 3.0 0.436
1&lt;n&lt;5 291 11.2 3.3 1.6 0.486
Total 2190 15.1 85.7 8.2 0.287
Rondane
Bin Items Words Trees Gold Chance
100&lt;n 107 21.8 498.4 17.8 0.060
50&lt;n&lt;100 63 19.1 72.9 12.0 0.162
10&lt;n&lt;50 244 15.2 23.4 4.9 0.234
5&lt;n&lt;10 119 11.9 7.2 2.7 0.377
1&lt;n&lt;5 101 9.3 3.21 1.5 0.476
Total 634 15.1 102.2 6.8 0.263
</table>
<tableCaption confidence="0.992814">
Table 2: Some core metrics for the symmetric tree-
</tableCaption>
<bodyText confidence="0.9903508125">
banks Jotunheimen’ (top) and Rondane’ (bot-
tom). The former data set was used for devel-
opment and cross-validation testing, the latter for
cross-genre held-out testing. The data items are
aggregated relative to their number of realizations.
The columns are, from left to right, the subdivi-
sion of the data according to the number of real-
izations, total number of items scored (excluding
items with only one realization and ones where
all realizations are marked as preferred), aver-
age string length, average number of realizations,
and average number of references. The rightmost
column shows a random choice baseline, i.e. the
probability of selecting the preferred realization
by chance.
yses for each surface form, and (c) sets of alter-
nate realizations of each semantic form. Using
the semantics of the preferred analyses in an ex-
isting treebank as input to the generator, we can
produce all equivalent paraphrases of the original
string. Furthermore, assuming that the original
surface form is an optimal verbalization of the cor-
responding semantics, we can automatically label
the preferred realization(s) by matching the yields
of the generated trees against the original strings
in the ‘source’ treebank. The result is what we
call a generation treebank, which taken together
with the original parse-oriented pairings constitute
a full symmetrical treebank.
We have successfully applied this technique to
the tourism segments of the LinGO Redwoods
treebank, which in turn is built atop the ERG.2
</bodyText>
<footnote confidence="0.995423">
2See ‘http://www,delph-in,net/erg/’ for fur-
</footnote>
<page confidence="0.99184">
520
</page>
<bodyText confidence="0.9987705">
Table 2 summarizes the two resulting data sets,
which are both comprised of instructional texts
on tourist activities, the application domain of the
background MT system.
</bodyText>
<subsectionHeader confidence="0.998324">
3.2 Feature Templates
</subsectionHeader>
<bodyText confidence="0.999810481481482">
For the purpose of parse selection, Toutanova,
Manning, Shieber, Flickinger, &amp; Oepen (2002)
and Toutanova &amp; Manning (2002) train a dis-
criminative log-linear model on the Redwoods
parse treebank, using features defined over deriva-
tion trees with non-terminals representing the con-
struction types and lexical types of the HPSG
grammar (see Figure 1). The basic feature set
of our MaxEnt realization ranker is defined in the
same way (corresponding to the PCFG-S model of
Toutanova &amp; Manning, 2002), each feature captur-
ing a sub-tree from the derivation limited to depth
one. Table 3 shows example features in our Max-
Ent and SVM models, where the feature template
# 1 corresponds to local derivation sub-trees. To
reduce the effects of data sparseness, feature type
#2 in Table 3 provides a back-off to derivation
sub-trees, where the sequence of daughters is re-
duced, in turn, to just one of the daughters. Con-
versely, to facilitate sampling of larger contexts
than just sub-trees of depth one, feature template
# 1 allows optional grandparenting, including the
upward chain of dominating nodes in some fea-
tures. In our experiments, we found that grandpar-
enting of up to three dominating nodes gave the
best balance of enlarged context vs. data sparse-
ness.
</bodyText>
<figure confidence="0.849701">
subjh
third sg fin verb
v unerg le
barks
</figure>
<figureCaption confidence="0.986171">
Figure 1: Sample HPSG derivation tree for the
</figureCaption>
<bodyText confidence="0.917965">
sentence the dog barks. Phrasal nodes are la-
beled with identifiers of grammar rules, and (pre-
terminal) lexical nodes with class names for types
of lexical entries.
In addition to these dominance-oriented fea-
tures taken from the derivation trees of each re-
alization, our models also include more surface-
ther information and download pointers.
</bodyText>
<table confidence="0.7541571875">
Id Sample Features
1 (0 subjh hspec third sg fin verb)
1 (1 A subjh hspec third sg fin verb)
1 (0 hspec det the le sing noun)
1 (1 subjh hspec det the le sing noun)
1 (2 A subjh hspec det the le sing noun)
2 (0 subjh third sg fin verb)
2 (0 subjh hspce)
2 (1 subjh hspec det the le)
2 (1 subjh hspec sing noun)
3 (1 n intr le dog)
3 (2 det the le n intr le dog)
3 (3 a det the le n intr le dog)
4 (1 n intr le)
4 (2 det the le n intr le)
4 (3 a det the le n intr le)
</table>
<tableCaption confidence="0.959916">
Table 3: Examples of structural features extracted
</tableCaption>
<bodyText confidence="0.952511952380952">
from the derivation tree in Figure 1. The first col-
umn identifies the feature template corresponding
to each example; in the examples, the first integer
value is a parameter to feature templates, i.e. the
depth of grandparenting (types 1 and 2) orn-gram
size (types 3 and 4). The special symbolsAanda
denote the root of the tree and left periphery of the
yield, respectively.
oriented features, viz.n-grams of lexical types
with or without lexicalization. Feature type # 3 in
Table 3 definesn-grams of variable size, where
(in a loose analogy to part of speech tagging) se-
quences of lexical types capture syntactic cate-
gory assignments. Feature templates # 3 and # 4
only differ with regard to lexicalization, as the for-
mer includes the surface token associated with the
rightmost element of eachn-gram. Unless other-
wise noted, we used a maximumn-gram size of
three in the experiments reported here, again due
to its empirically determined best overall perfor-
mance.
The number of instantiated features produced
by the feature templates easily grows quite large.
For the ‘Jotunheimen’ data the total number of dis-
tinct feature instantiations is 312,650. For the ex-
periments in this paper we implemented a simple
frequency based cutoff by removing features that
are observed as relevant less thanctimes. We here
follow the approach of Malouf &amp; Noord (2004)
where relevance of a feature is simply defined as
taking on a different value for any two competing
candidates for the same input. A feature is only
included in training if it is relevant for more than
citems in the training data. Table 4 shows the ef-
fect on the accuracy of the MaxEnt model when
varying the cutoff. We see that a model can be
hspec
dog
det the le
the
sing noun
n intr le
</bodyText>
<page confidence="0.979209">
521
</page>
<table confidence="0.999000846153846">
Cutoff Features Accuracy
—312,650 71.18
1 264,455 71.18
2 112,051 70.03
3 66,069 70.28
4 46,139 69.30
5 35,295 67.93
10 16,036 65.36
20 7,563 63.05
50 2,605 59.10
100 889 54.21
200 261 50.11
500 34 34.70
</table>
<tableCaption confidence="0.662685363636364">
Table 4: The effects of frequency-based feature se-
lection with respect to model size and accuracy.
model configuration
basic model of (Velldal et al., 2004)
basic plus partial daughter sequence
basic plus grandparenting
basic plus lexical type trigrams
basic plus all of the above
basic plus language model
basic plus all of the above
Table 5: Performance summaries of best-
</tableCaption>
<bodyText confidence="0.996292777777778">
performing realization rankers using various fea-
ture configurations, when compared to the set-up
of Velldal et al. (2004). These scores where com-
puted using a relevance cutoff of 3 and optimizing
the variance of the prior for individual configura-
tions.
compacted quite aggressively without sacrificing
much in performance. For all models presented
below we use a cutoff ofc=3.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999866653846154">
In this section we present contrastive results for
the models defined in Section 2 above, evaluated
against the exact match accuracy and word accu-
racy as described in Section 2.4.
As can be seen in Table 6, both the MaxEnt
and SVM learner does a much better job than
then-gram model at identifying the correct refer-
ence strings. The two discriminative models per-
form very similarly, however, although the Max-
Ent model often seems to do slightly better.
When working with a cross-validation set-up
the difference between the learners can conve-
niently be tested using an approach such as the
cross-validated pairedt-test described by Diet-
terich (1998). We also tried this approach using
the Wilcoxon Matched-Pairs Signed-Ranks test as
a non-parametric alternative without the assump-
tion of normality of differences made in thet-test.
However, none of the two tests found that the dif-
ferences between the MaxEnt model and the SVM
model were significant fora=0.05(using two-
sided tests).
Note that, due to memory constraints, we only
included a random sample of maximum 50 non-
preferred realizations per item in the training data
used for the SVM ranker. Even so, the SVM
trained on the full ‘Jotunheimen’ data had a to-
tal of 66,621 example vectors in its training data,
which spawned a total of 639,301 preference con-
straints with respect to the optimization problem
of Equations 8 and 10. We did not try to maxi-
mize performance on the development data by re-
peatedly training with different random samples,
but this might be one way to improve the results.
Although we were only able to present results
using linear kernels for the SVM ranker in this pa-
per, preliminary experiments using a polynomial
kernel seem to give promising results. Due to
memory constraints and long convergence times,
we were only able to train such a model on half
of the ‘Jotunheimen’ data. However, when testing
on the remaining half, it achieved an exact match
accuracy of71.03%. This is comparable to the
performance achieved by the linear SVM through
full 10-fold training and testing. Moreover, there
is reason to believe that these results will improve
once we manage to train on the full data set.
In order to assess the effect of increasing the
size of the training set, Figure 3 presents learning
curves for two MaxEnt configurations, viz. the ba-
sic configurational model and the one including all
features but the language model. Each data point
</bodyText>
<figureCaption confidence="0.982243333333333">
Figure 2: Exact match accuracy scores for the dif-
ferent models. Data items are binned with respect
to the number of distinct realizations.
</figureCaption>
<figure confidence="0.993163681818182">
90
80
70
60
50
40
30
20
10
0
Baseline
BNC LM
SVM
MaxEnt
match WA
63.09 0.904
64.64 0.910
67.54 0.923
68.61 0.921
70.28 0.927
67.96 0.912
72.28 0.928
</figure>
<page confidence="0.976268">
522
</page>
<table confidence="0.9988604">
Model Jotunheimen Rondane
accuracy n-best WA accuracy n-best WA
BNC LM 53.24 78.81 0.882 54.19 77.19 0.891
SVM 71.11 84.69 0.922 63.64 83.12 0.906
MaxEnt 72.28 84.59 0.927 64.28 83.60 0.903
</table>
<tableCaption confidence="0.69431175">
Table 6: Performance of the different learners. The results on the ‘Jotunheimen’ treebank for the discrim-
inative models are averages from 10-fold cross-validation. A model trained on the entire ‘Jotunheimen’
data was used when testing on ‘Rondane’. Note that the training accuracy of the SVM learner on the
‘Jotunheimen’ training set is 91.69%, while it’s 92.99% for the MaxEnt model.
</tableCaption>
<figure confidence="0.97447">
10 20 30 40 50 60 70 80 90 100
training data (%)
</figure>
<figureCaption confidence="0.999812">
Figure 3: Learning curves for two MaxEnt model
</figureCaption>
<bodyText confidence="0.999935378787879">
configurations (trained without cutoffs). Al-
though there appears to be a saturation effect in
model performance with increasing amounts of
‘Jotunheimen’ training data, for the richer config-
uration (using all features but the language model)
further enlarging the training data still seems at-
tractive.
corresponds to average exact match performance
for 10-fold cross-validation on ‘Jotunheimen’, but
restricting the amount of training data presented to
the learner to between 10 and 100 per cent of the
total. At 60 per cent training data, the two mod-
els already perform at60.6%and68.4%accuracy,
and the learning curves are starting to flatten out.
Somewhat remarkably, the richer model including
partial daughter back-off, grandparenting, and lex-
ical type trigrams already outperforms the baseline
model by a clear margin with just a small fraction
of the training data, so the MaxEnt learner appears
to make effective use of the greatly enlarged fea-
ture space.
When testing against the ‘Rondane’ held-
out set and comparing to performance on the
‘Jotunheimen’ cross-validation set, we see that the
performance of both the MaxEnt model and the
SVM degrades quite a bit. Of course, some drop
in performance is to be expected as the estimation
parameters had been tuned to this development set.
Furthermore, as can be seen from Table 2, the
baseline is also slightly lower for the ‘Rondane’
test set as the average number of realizations is
higher. Also, while basically from the same do-
main, the two text collections differ noticeably
in style: ‘Jotunheimen’ is based on edited, high-
quality guide books; ‘Rondane’ has been gathered
from a variety of web sites. Note, however, that
the performance of the BNCn-gram model seems
to be more stable across the different data sets.
In any case we see that, for our realization rank-
ing task, the use of discriminative models in com-
bination with structural features extracted from
treebanks, clearly outperforms the surface ori-
ented, generativen-gram model. This is in spite of
the relatively modest size of the treebanked train-
ing data available to the discriminative models. On
the ‘Rondane’ test set the reduction in error rate
for the combined MaxEnt model relative to then-
gram LM, is22.03%. The error reduction for the
SVM over the LM on ‘Rondane’ is 20.63%.
Another factor that is likely to be important for
the differences in performance is the fact that the
treebank data is better tuned to the domain of ap-
plication or the test data. The n-gram language
model, on the other hand, was only trained on
the general-domain BNC data. Note, however,
that when testing on ‘Rondane’, we also tried to
combine this general-domain model with an ad-
ditional in-domain model trained only on the text
that formed the basis of the ‘Jotunheimen’ tree-
bank, a total of 5024 sentences. The optimal
weights for linearly combining these two models
were calculated using the interpolation tool in the
CMU toolkit (using the expectation maximization
(EM) algorithm, minimizing the perplexity on a
held out data set of 330 sentences). However,
when applied to the ‘Rondane’ test set, this in-
</bodyText>
<figure confidence="0.994867">
accuracy (%)
75
70
65
60
55
50
Basic
All
</figure>
<page confidence="0.991587">
523
</page>
<table confidence="0.99860875">
model error ties correct
BNC LM 253 68 313
MaxEnt (sans LM) 222 63 349
MaxEnt (combined) 225 3 404
</table>
<tableCaption confidence="0.992283">
Table 7: Exact match error counts for three mod-
</tableCaption>
<bodyText confidence="0.995382926829268">
els, viz. the BNC LM only, the MaxEnt model by
itself (using all feature types except the LM prob-
ability), and the combined MaxEnt model. The
intermediate column corresponds to ties or partial
errors, i.e. the number of items for which multiple
candidates were ranked at the top, of which some
were actually preferred and some not. Primarily
this latter error type is reduced by including the
LM feature in the MaxEnt universe.
terpolated model failed to improve on the results
achieved by just using the larger general-domain
model alone. This is probably due to the small
amount of domain specific data that we presently
have available for training.
Another observation about ourn-gram experi-
ments that is worth a mention is that we found that
ranking realizations according to non-normalized
log probabilities directly resulted in much bet-
ter accuracy than using a length normalized score
such as the geometric mean.
Finally, Table 7 breaks down per-item exact
match errors for three distinct ranking configura-
tions, viz. the BNC LM only, the structural Max-
Ent model only, and the combined MaxEnt model,
which includes the LM probability as an addi-
tional feature; all numbers are for application to
the held-out ‘Rondane’ test set. Further contrast-
ing the first two of these, the BNC LM yields 129
unique errors, in the sense that the structural Max-
Ent makes the correct predictions on these items,
contrasted to 98 unique errors in the structural
MaxEnt model. When compared to the only 124
errors made equally by both rankers, we conclude
that the different approaches have partially com-
plementary strengths and weaknesses. This ob-
servation is confirmed in the relatively substan-
tial improvement in ranking performance of the
combined model on the ‘Rondane’ test: The ex-
act match accuracies of then-gram model, the ba-
sic MaxEnt model and the combined model are
54.19%,59.43%and64.28%, respectively.
</bodyText>
<sectionHeader confidence="0.995765" genericHeader="conclusions">
5 Summary and Outlook
</sectionHeader>
<bodyText confidence="0.999823404761905">
Applying three alternate statistical models to the
realization ranking task, we found that discrimi-
native models with access to structural informa-
tion substantially outperform the traditional lan-
guage model approach. Using comparatively
small amounts of annotated training data, we were
able to boost ranking performance from around
54%to more than72%, albeit for a limited, rea-
sonably coherent domain and genre. The incre-
mental addition of feature templates into the Max-
Ent model suggests a trend of diminishing return,
most likely due to increasing overlap in the portion
of the problem space captured across templates,
and possibly reflecting limitations in the amount
of training data. The comparison of the Max-
Ent and SVM rankers suggest comparable perfor-
mance on our task, not showing statistically signif-
icant differences. Nevertheless, in terms of scala-
bility when using large data sets, it seems clear
that the MaxEnt framework is a more practical and
manageable alternative, both in terms of training
time and memory requirements.
As further work we would like to try to train an
SVM that takes full advantage of the ranking po-
tential of the set-up described in (Joachims, 2002).
Instead of just making binary (right/wrong) dis-
tinctions, we could grade the realizations in the
training data according to their WA scores toward
the references and try to learn a similar ranking.
So far we have only been able to do preliminary
experiments with this set-up on a small sub-set of
the data. When evaluated with the accuracy mea-
sures used in this paper the results were not as
good as those obtained when training with only
two ranks, however this might very well look dif-
ferent if we evaluate the full rankings (e.g. number
of swapped pairs) instead of just focusing on the
top ranked candidates. Note that it is also possible
to use such graded training data with the MaxEnt
models, by letting the probabilities of the empiri-
cal distribution be based on similarity scores such
as WA instead of frequencies.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99924275">
The work reported here is part of the Norwe-
gian LOGON project on precision MT, and we
are grateful to numerous colleagues; please see
‘http://www.emmtee.net’ for background.
Furthermore, we warmly acknowledge the sup-
port and productive criticism provided by Dan
Flickinger (the ERG developer), Francis Bond,
John Carrol, and three anonymous reviewers.
</bodyText>
<page confidence="0.997117">
524
</page>
<sectionHeader confidence="0.998347" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999256846153846">
Carroll, J., Copestake, A., Flickinger, D., &amp; Poz-
nanski, V. (1999). An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings
of the 7th European Workshop on Natural Lan-
guage Generation. Toulouse.
Carroll, J., &amp; Oepen, S. (2005). High-efficiency re-
alization for a wide-coverage unification gram-
mar. In R. Dale &amp; K. fai Wong (Eds.), Proceed-
ings of the 2nd International Joint Conference
on Natural Language Processing (Vol. 3651,
pp. 165 –176). Jeju, Korea: Springer.
Chen, S. F., &amp; Rosenfeld, R. (1999). A Gaussian
prior for smoothing maximum entropy mod-
els (Tech. Rep.). Carnegie Mellon University.
(Technical Report CMUCS-CS-99-108)
Clarkson, P., &amp; Rosenfeld, R. (1997). Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings ofESCA Eurospeech.
Copestake, A., Flickinger, D., Malouf, R., Riehe-
mann, S., &amp; Sag, I. (1995). Translation using
minimal recursion semantics. In Proceedings
of the Sixth International Conference on The-
oretical and Methodological Issues in Machine
Translation. Leuven, Belgium.
Dietterich, T. G. (1998). Approximate statisti-
cal test for comparing supervised classifica-
tion learning algorithms. Neural Computation,
10(7), 1895–1923.
Flickinger, D. (2002). On building a more effi-
cient grammar by exploiting types. In S. Oepen,
D. Flickinger, J. Tsujii, , &amp; H. Uszkoreit (Eds.),
Collaborative language engineering: A case
study in efficient grammar-based processing
(pp. 1–17). CSLI Press.
Joachims, T. (1999). Making large-scale svm
learning practical. In B. Sch¨olkopf, C. Burges,
&amp; A. Smola (Eds.), Advances in kernel methods
- support vector learning. MIT-Press.
Joachims, T. (2002). Optimizing search engines
using clickthrough data. In Proceedings of the
ACM conference on knowledge discovery and
data mining (KDD). ACM.
Johnson, M., Geman, S., Canon, S., Chi, Z., &amp;
Riezler, S. (1999). Estimators for stochastic
‘unification-based’ grammars. In Proceedings
of the 37th Meeting of the Association for Com-
putational Linguistics (pp. 535–541). College
Park, MD.
Langkilde, I., &amp; Knight, K. (1998). The practical
value of n-grams in generation. In International
natural language generation workshop.
Malouf, R. (2002). A comparison of algorithms for
maximum entropy parameter estimation.In Pro-
ceedings of the 6th Conference on Natural Lan-
guage Learning (pp. 49–55). Taipei, Taiwan.
Malouf, R., &amp; Noord, G. van. (2004). Wide cov-
erage parsing with stochastic attribute value
grammars. In Proceedings of the IJCNLP work-
shop Beyond Shallow Analysis. Hainan, China.
Nakanishi, H., Miyao, Y., &amp; Tsujii, J. (2005).
Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings
of the 9th International Workshop on Pars-
ing Technologies (pp. 93–102). Vancouver,
Canada: Association for Computational Lin-
guistics.
Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L.,
Smith, D., Eng, K., Jain, V., Jin, Z., &amp; Radev, D.
(2004). A smorgasbord of features for statistical
machine translation. In Proceedings of the 5th
Conference of the North American Chapter of
the ACL. Boston.
Toutanova, K., &amp; Manning, C. D. (2002). Feature
selection for a rich HPSG grammar using deci-
sion trees. In Proceedings of the 6th Conference
on Natural Language Learning. Taipei, Taiwan.
Toutanova, K., Manning, C. D., Shieber, S. M.,
Flickinger, D., &amp; Oepen, S. (2002). Parse dis-
ambiguation for a rich hpsg grammar. In First
workshop on treebanks and linguistic theories.
Sozopol, Bulgaria.
Velldal, E., Oepen, S., &amp; Flickinger, D. (2004).
Paraphrasing treebanks for stochastic realiza-
tion ranking. In Proceedings of the 3rd work-
shop on Treebanks and Linguistic Theories.
T¨ubingen, Germany.
White, M. (2004). Reining in CCG chart realiza-
tion. In Proceedings of the 3rd International
Conference on Natural Language Generation.
Hampshire, UK.
</reference>
<page confidence="0.998466">
525
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967665">
<title confidence="0.999772">Statistical Ranking in Tactical Generation</title>
<author confidence="0.999868">Erik Velldal</author>
<affiliation confidence="0.999227">University of Oslo (Norway)</affiliation>
<email confidence="0.993051">erik.velldal@ifi.uio.no</email>
<abstract confidence="0.9984544375">In this paper we describe and evaluate several statistical models for the task of i.e. the problem of discriminating between competing surface realizations generated for a given input semantics. Three models (and several variare trained and tested: language model, a discriminative maximum entropy model using structural information (and incorporating the language model as a separate feature), and finally an SVM ranker trained on the same feature set. The resulting hybrid tactical generator is part of a larger, semantic transfer MT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>V Poznanski</author>
</authors>
<title>An efficient chart generator for (semi-)lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation.</booktitle>
<location>Toulouse.</location>
<contexts>
<context position="1127" citStr="Carroll, Copestake, Flickinger, &amp; Poznanski, 1999" startWordPosition="160" endWordPosition="165">rmation (and incorporating the language model as a separate feature), and finally an SVM ranker trained on the same feature set. The resulting hybrid tactical generator is part of a larger, semantic transfer MT system. 1 Introduction This paper describes the application of several different statistical models for the task of realization ranking in tactical generation, i.e. the problem of choosing among multiple paraphrases that are generated for a given meaning representation. The specific realization component we use is the opensource chart generator of the Linguistic Knowledge Builder (LKB; Carroll, Copestake, Flickinger, &amp; Poznanski, 1999; Carroll &amp; Oepen, 2005). Given a meaning representation in the form of Minimal Recursion Semantics (MRS; Copestake, Flickinger, Malouf, Riehemann, &amp; Sag, 1995), the generator outputs English realizations in accordance with the HPSG LinGO English Resource Grammar (ERG; Flickinger, 2002). As an example of generator output, a sub-set of alternate realizations that are produced for a single input MRS is shown in Figure 1. For the two data sets considered in this paper, the average number of realizations produced by the generator is 85.7 and 102.2 (the maximum numbers are 4176 and 3408, respective</context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>Carroll, J., Copestake, A., Flickinger, D., &amp; Poznanski, V. (1999). An efficient chart generator for (semi-)lexicalist grammars. In Proceedings of the 7th European Workshop on Natural Language Generation. Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>S Oepen</author>
</authors>
<title>High-efficiency realization for a wide-coverage unification grammar. In</title>
<date>2005</date>
<booktitle>Proceedings of the 2nd International Joint Conference on Natural Language Processing</booktitle>
<volume>3651</volume>
<pages>165--176</pages>
<publisher>Jeju, Korea: Springer.</publisher>
<contexts>
<context position="1151" citStr="Carroll &amp; Oepen, 2005" startWordPosition="166" endWordPosition="169">separate feature), and finally an SVM ranker trained on the same feature set. The resulting hybrid tactical generator is part of a larger, semantic transfer MT system. 1 Introduction This paper describes the application of several different statistical models for the task of realization ranking in tactical generation, i.e. the problem of choosing among multiple paraphrases that are generated for a given meaning representation. The specific realization component we use is the opensource chart generator of the Linguistic Knowledge Builder (LKB; Carroll, Copestake, Flickinger, &amp; Poznanski, 1999; Carroll &amp; Oepen, 2005). Given a meaning representation in the form of Minimal Recursion Semantics (MRS; Copestake, Flickinger, Malouf, Riehemann, &amp; Sag, 1995), the generator outputs English realizations in accordance with the HPSG LinGO English Resource Grammar (ERG; Flickinger, 2002). As an example of generator output, a sub-set of alternate realizations that are produced for a single input MRS is shown in Figure 1. For the two data sets considered in this paper, the average number of realizations produced by the generator is 85.7 and 102.2 (the maximum numbers are 4176 and 3408, respectively). Thus, there is imme</context>
<context position="2692" citStr="Carroll &amp; Oepen, 2005" startWordPosition="415" endWordPosition="418">) support vector machine (SVM). These are all models that have proved popular within the NLP community, but it is usually only the first of these three that has been applied to the task of ranking in sentence generation. The latter two models that we present here go beyond the surface information used by then-gram model, and are trained on a symmetric treebank with features defined over the full HPSG analyses of competing realizations. Furthermore, such discriminative models are suitable for ‘on-line’ use within our generator—adopting the technique of selective unpacking from a packed forest (Carroll &amp; Oepen, 2005)—which means our hybrid realizer obviates the need for exhaustive enumeration of candidate outputs. The present results extend our earlier work (Velldal, Oepen, &amp; Flickinger, 2004)—and the related work of Nakanishi, Miyao, &amp; Tsujii (2005)—to an enlarged data set, more feature types, and additional learners. The rest of this paper is structured as follows. Section 2 first gives a general summary of the various statistical models we will be considering, as well as the measures used for evaluating them. We then go on to define the task we are aiming to solve in terms of treebank data and feature </context>
</contexts>
<marker>Carroll, Oepen, 2005</marker>
<rawString>Carroll, J., &amp; Oepen, S. (2005). High-efficiency realization for a wide-coverage unification grammar. In R. Dale &amp; K. fai Wong (Eds.), Proceedings of the 2nd International Joint Conference on Natural Language Processing (Vol. 3651, pp. 165 –176). Jeju, Korea: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models</title>
<date>1999</date>
<tech>(Technical Report CMUCS-CS-99-108)</tech>
<institution>(Tech. Rep.). Carnegie Mellon University.</institution>
<contexts>
<context position="8765" citStr="Chen &amp; Rosenfeld, 1999" startWordPosition="1410" endWordPosition="1413">rametersw, the procedure seeks to maximize the (log of) a penalized likelihood function as in log L(w) — Ei=1 wi lowing Joachims (2002), the goal is to minimize (7) V(w;0=2w&apos;w+C1:�i;j;k 1 (6) w=argmax d 2 subject to the following constraints, w 2o-2 where L(w) is the ‘conditionalized’ likelihood of the training data Xp (Johnson et al., 1999), computed as L(w) = HNi=1pw(rilsi). The second term of the likelihood function in Equation (6) is a penalty term that is commonly used for reducing the tendency of log-linear models to over-fit, especially when training on sparse data using many features (Chen &amp; Rosenfeld, 1999; Johnson et al., 1999; Malouf &amp; Noord, 2004). More specifically it defines a zero-mean Gaussian prior on the feature weights which effectively leads to less extreme values. After empirically tuning the prior on our ‘Jotunheimen’ treebank (training and testing by 10-fold cross-validation), we ended up usingQ2=0:003for the MaxEnt models applied in this paper. 2.3 SVM Rankers In this section we briefly formulate the optimization problem in terms of support vector machines. Our starting point is the SVM approach introduced in Joachims (2002) for learning ranking functions for information retrieva</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Chen, S. F., &amp; Rosenfeld, R. (1999). A Gaussian prior for smoothing maximum entropy models (Tech. Rep.). Carnegie Mellon University. (Technical Report CMUCS-CS-99-108)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge Toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings ofESCA Eurospeech.</booktitle>
<contexts>
<context position="5336" citStr="Clarkson &amp; Rosenfeld, 1997" startWordPosition="844" endWordPosition="847">e topicalizations. Other choices involve, for example, the optionality of complementizers and relative pronouns, permutation of (intersective) modifiers, and lexical and orthographic alternations. 2.1 Language Models The use ofn-gram language models is the most common approach to statistical selection in generation (Langkilde &amp; Knight, 1998; and White (2004); inter alios). In order to better assert the relative performance of the discriminative models and the structural features we present below, we also apply a trigram model to the ranking problem. Using the freely available CMU SLM Toolkit (Clarkson &amp; Rosenfeld, 1997), we trained a trigram model on an unannotated version of the British National Corpus (BNC), containing roughly 100 million words (using Witten-Bell discounting and back-off). Given such a model pn, the score of a realization ri with surface form wki1=(wi1,...,wik)is then computed as k (1)F(s,ri)= pn(wi,jIwi,j—n,...,wi,j-1) j=1 Given the scoring functionF, the best realization is selected according to the following decision function: (2) r=argmax F(s,r&apos;) r&apos;EY(s) Although in this case scoring is not conditioned on the input semantics at all, we still include it to make the function formulation </context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, P., &amp; Rosenfeld, R. (1997). Statistical language modeling using the CMU-Cambridge Toolkit. In Proceedings ofESCA Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>R Malouf</author>
<author>S Riehemann</author>
<author>I Sag</author>
</authors>
<title>Translation using minimal recursion semantics.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<location>Leuven, Belgium.</location>
<contexts>
<context position="1286" citStr="Copestake, Flickinger, Malouf, Riehemann, &amp; Sag, 1995" startWordPosition="182" endWordPosition="188">cal generator is part of a larger, semantic transfer MT system. 1 Introduction This paper describes the application of several different statistical models for the task of realization ranking in tactical generation, i.e. the problem of choosing among multiple paraphrases that are generated for a given meaning representation. The specific realization component we use is the opensource chart generator of the Linguistic Knowledge Builder (LKB; Carroll, Copestake, Flickinger, &amp; Poznanski, 1999; Carroll &amp; Oepen, 2005). Given a meaning representation in the form of Minimal Recursion Semantics (MRS; Copestake, Flickinger, Malouf, Riehemann, &amp; Sag, 1995), the generator outputs English realizations in accordance with the HPSG LinGO English Resource Grammar (ERG; Flickinger, 2002). As an example of generator output, a sub-set of alternate realizations that are produced for a single input MRS is shown in Figure 1. For the two data sets considered in this paper, the average number of realizations produced by the generator is 85.7 and 102.2 (the maximum numbers are 4176 and 3408, respectively). Thus, there is immediate demand for a principled way of choosing a single output among the generated candidates. For this task we train and test three dif</context>
</contexts>
<marker>Copestake, Flickinger, Malouf, Riehemann, Sag, 1995</marker>
<rawString>Copestake, A., Flickinger, D., Malouf, R., Riehemann, S., &amp; Sag, I. (1995). Translation using minimal recursion semantics. In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation. Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Approximate statistical test for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<issue>7</issue>
<pages>1895--1923</pages>
<contexts>
<context position="22373" citStr="Dietterich (1998)" startWordPosition="3683" endWordPosition="3685">astive results for the models defined in Section 2 above, evaluated against the exact match accuracy and word accuracy as described in Section 2.4. As can be seen in Table 6, both the MaxEnt and SVM learner does a much better job than then-gram model at identifying the correct reference strings. The two discriminative models perform very similarly, however, although the MaxEnt model often seems to do slightly better. When working with a cross-validation set-up the difference between the learners can conveniently be tested using an approach such as the cross-validated pairedt-test described by Dietterich (1998). We also tried this approach using the Wilcoxon Matched-Pairs Signed-Ranks test as a non-parametric alternative without the assumption of normality of differences made in thet-test. However, none of the two tests found that the differences between the MaxEnt model and the SVM model were significant fora=0.05(using twosided tests). Note that, due to memory constraints, we only included a random sample of maximum 50 nonpreferred realizations per item in the training data used for the SVM ranker. Even so, the SVM trained on the full ‘Jotunheimen’ data had a total of 66,621 example vectors in its</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Dietterich, T. G. (1998). Approximate statistical test for comparing supervised classification learning algorithms. Neural Computation, 10(7), 1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types. In</title>
<date>2002</date>
<pages>1--17</pages>
<publisher>CSLI Press.</publisher>
<contexts>
<context position="1414" citStr="Flickinger, 2002" startWordPosition="205" endWordPosition="206">task of realization ranking in tactical generation, i.e. the problem of choosing among multiple paraphrases that are generated for a given meaning representation. The specific realization component we use is the opensource chart generator of the Linguistic Knowledge Builder (LKB; Carroll, Copestake, Flickinger, &amp; Poznanski, 1999; Carroll &amp; Oepen, 2005). Given a meaning representation in the form of Minimal Recursion Semantics (MRS; Copestake, Flickinger, Malouf, Riehemann, &amp; Sag, 1995), the generator outputs English realizations in accordance with the HPSG LinGO English Resource Grammar (ERG; Flickinger, 2002). As an example of generator output, a sub-set of alternate realizations that are produced for a single input MRS is shown in Figure 1. For the two data sets considered in this paper, the average number of realizations produced by the generator is 85.7 and 102.2 (the maximum numbers are 4176 and 3408, respectively). Thus, there is immediate demand for a principled way of choosing a single output among the generated candidates. For this task we train and test three different statistical models: ann-gram language model, Stephan Oepen University of Oslo (Norway) and CSLI Stanford (CA) oe@csli.sta</context>
</contexts>
<marker>Flickinger, 2002</marker>
<rawString>Flickinger, D. (2002). On building a more efficient grammar by exploiting types. In S. Oepen, D. Flickinger, J. Tsujii, , &amp; H. Uszkoreit (Eds.), Collaborative language engineering: A case study in efficient grammar-based processing (pp. 1–17). CSLI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale svm learning practical. In</title>
<date>1999</date>
<tech>MIT-Press.</tech>
<contexts>
<context position="11125" citStr="Joachims (1999)" startWordPosition="1804" endWordPosition="1805">i;j;k &gt;— 0 Joachims (2002) shows that the preference constraints in Equation (8) can be rewritten as (10)w&apos;(�(sk;ri)—&apos;D(sk;rj))&gt;—1—�i;j;k so that the optimization problem is equivalent to training a classifier on the pairwise difference vectors-(sk;ri)—&apos;(sk;rj). The (non-negative) slack variables �i;j;k are commonly used in SVMs to make it possible to approximate a solution by allowing some error in cases where a separating hyperplane can not be found. The trade-off between maximizing the margin size and minimizing the training error is governed by the constant C. Using the SVMlightpackage by Joachims (1999), we empirically specifiedC=0:005for the model described in this paper. Note that, for the experiments reported here, we will only be making binary distinctions of preferred/non-preferred realizations, although the approach presented in (Joachims, 2002) is formulated for the more general case of learning ordinal ranking functions. Finally, given a linear SVM, we score and select realizations in the same way as we did with the MaxEnt model, according to Equations (4) and (2). Note, however, that it is also possible to use non-linear kernel functions with this set-up, since the ranking function </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. (1999). Making large-scale svm learning practical. In B. Sch¨olkopf, C. Burges, &amp; A. Smola (Eds.), Advances in kernel methods - support vector learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACM conference on knowledge discovery and data mining (KDD).</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="8278" citStr="Joachims (2002)" startWordPosition="1331" endWordPosition="1332">e functionFwis simply the sum of the products of all feature values and feature weights, given by d (4)Fw(s,r)= wi-I�i(s,r)=w•-4�(s,r) i=1 The normalization termZwis defined as (5) Zw(s)=� eF„(s,r) r&apos;EY(s) When we want to find the best realization for a given input semantics according to a modelpw, it is sufficient to compute the score function as in Equation (4) and then use the decision function previously given in Equation (2) above. When it 518 comes to estimating1 the parametersw, the procedure seeks to maximize the (log of) a penalized likelihood function as in log L(w) — Ei=1 wi lowing Joachims (2002), the goal is to minimize (7) V(w;0=2w&apos;w+C1:�i;j;k 1 (6) w=argmax d 2 subject to the following constraints, w 2o-2 where L(w) is the ‘conditionalized’ likelihood of the training data Xp (Johnson et al., 1999), computed as L(w) = HNi=1pw(rilsi). The second term of the likelihood function in Equation (6) is a penalty term that is commonly used for reducing the tendency of log-linear models to over-fit, especially when training on sparse data using many features (Chen &amp; Rosenfeld, 1999; Johnson et al., 1999; Malouf &amp; Noord, 2004). More specifically it defines a zero-mean Gaussian prior on the fea</context>
<context position="10536" citStr="Joachims (2002)" startWordPosition="1716" endWordPosition="1717">of positive training examples in Section 2.2. Let us here analogously defineXn={x1;:::;xQ}to be the negative counterpart, so that for a given pair x = (si; rj) E Xn, we have that rj E Y(si) but rj is not annotated as a preferred realization of si. Fol1We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. ( 8) Vijk s.t. (sk; ri) E Xp A (sk; rj) E Xn : w &apos; -4�(sk; ri) &gt;— w (sk; rj) + 1 — �i;j;k (9 ) Vijk : �i;j;k &gt;— 0 Joachims (2002) shows that the preference constraints in Equation (8) can be rewritten as (10)w&apos;(�(sk;ri)—&apos;D(sk;rj))&gt;—1—�i;j;k so that the optimization problem is equivalent to training a classifier on the pairwise difference vectors-(sk;ri)—&apos;(sk;rj). The (non-negative) slack variables �i;j;k are commonly used in SVMs to make it possible to approximate a solution by allowing some error in cases where a separating hyperplane can not be found. The trade-off between maximizing the margin size and minimizing the training error is governed by the constant C. Using the SVMlightpackage by Joachims (1999), we empiri</context>
<context position="31536" citStr="Joachims, 2002" startWordPosition="5198" endWordPosition="5199"> of the problem space captured across templates, and possibly reflecting limitations in the amount of training data. The comparison of the MaxEnt and SVM rankers suggest comparable performance on our task, not showing statistically significant differences. Nevertheless, in terms of scalability when using large data sets, it seems clear that the MaxEnt framework is a more practical and manageable alternative, both in terms of training time and memory requirements. As further work we would like to try to train an SVM that takes full advantage of the ranking potential of the set-up described in (Joachims, 2002). Instead of just making binary (right/wrong) distinctions, we could grade the realizations in the training data according to their WA scores toward the references and try to learn a similar ranking. So far we have only been able to do preliminary experiments with this set-up on a small sub-set of the data. When evaluated with the accuracy measures used in this paper the results were not as good as those obtained when training with only two ranks, however this might very well look different if we evaluate the full rankings (e.g. number of swapped pairs) instead of just focusing on the top rank</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims, T. (2002). Optimizing search engines using clickthrough data. In Proceedings of the ACM conference on knowledge discovery and data mining (KDD). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for stochastic ‘unification-based’ grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Meeting of the Association for Computational Linguistics</booktitle>
<pages>535--541</pages>
<location>College Park, MD.</location>
<contexts>
<context position="6411" citStr="Johnson, Geman, Canon, Chi, &amp; Riezler, 1999" startWordPosition="1010" endWordPosition="1016">tion: (2) r=argmax F(s,r&apos;) r&apos;EY(s) Although in this case scoring is not conditioned on the input semantics at all, we still include it to make the function formulation more general as we will be reusing it later. Note that, as the realizations in our symmetric treebank also include punctuation marks, these are also treated as separate tokens by the language model (in addition to pseudo-tokens marking sentence boundaries). 2.2 Maximum Entropy Models Maximum entropy modeling provides a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection (e.g. Johnson, Geman, Canon, Chi, &amp; Riezler, 1999; Malouf &amp; Noord, 2004) and reranking for machine translation (e.g. Och et al., 2004). A model is specified by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Let us first introduce some notation before we go on. Let Y(si) = {r1, ... , rn,} be the set of realizations licensed by the grammar for a semantic representation si. Now, let our (positive) training data be given asXp={x1,...,xN}where each xiis a pair(si,rj)for whichrjEY(si)andrj is annotated in the treebank as being a </context>
<context position="8486" citStr="Johnson et al., 1999" startWordPosition="1363" endWordPosition="1366">(s) When we want to find the best realization for a given input semantics according to a modelpw, it is sufficient to compute the score function as in Equation (4) and then use the decision function previously given in Equation (2) above. When it 518 comes to estimating1 the parametersw, the procedure seeks to maximize the (log of) a penalized likelihood function as in log L(w) — Ei=1 wi lowing Joachims (2002), the goal is to minimize (7) V(w;0=2w&apos;w+C1:�i;j;k 1 (6) w=argmax d 2 subject to the following constraints, w 2o-2 where L(w) is the ‘conditionalized’ likelihood of the training data Xp (Johnson et al., 1999), computed as L(w) = HNi=1pw(rilsi). The second term of the likelihood function in Equation (6) is a penalty term that is commonly used for reducing the tendency of log-linear models to over-fit, especially when training on sparse data using many features (Chen &amp; Rosenfeld, 1999; Johnson et al., 1999; Malouf &amp; Noord, 2004). More specifically it defines a zero-mean Gaussian prior on the feature weights which effectively leads to less extreme values. After empirically tuning the prior on our ‘Jotunheimen’ treebank (training and testing by 10-fold cross-validation), we ended up usingQ2=0:003for t</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, M., Geman, S., Canon, S., Chi, Z., &amp; Riezler, S. (1999). Estimators for stochastic ‘unification-based’ grammars. In Proceedings of the 37th Meeting of the Association for Computational Linguistics (pp. 535–541). College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>In International natural language generation workshop.</booktitle>
<contexts>
<context position="5051" citStr="Langkilde &amp; Knight, 1998" startWordPosition="798" endWordPosition="801">. Dogs, remember must be on a leash. Table 1: A small example set of generator outputs using the ERG. Where the input semantics is no specified for aspects of information structure (e.g. requesting foregrounding of a specific entity), paraphrases include all grammatically legitimate topicalizations. Other choices involve, for example, the optionality of complementizers and relative pronouns, permutation of (intersective) modifiers, and lexical and orthographic alternations. 2.1 Language Models The use ofn-gram language models is the most common approach to statistical selection in generation (Langkilde &amp; Knight, 1998; and White (2004); inter alios). In order to better assert the relative performance of the discriminative models and the structural features we present below, we also apply a trigram model to the ranking problem. Using the freely available CMU SLM Toolkit (Clarkson &amp; Rosenfeld, 1997), we trained a trigram model on an unannotated version of the British National Corpus (BNC), containing roughly 100 million words (using Witten-Bell discounting and back-off). Given such a model pn, the score of a realization ri with surface form wki1=(wi1,...,wik)is then computed as k (1)F(s,ri)= pn(wi,jIwi,j—n,.</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, I., &amp; Knight, K. (1998). The practical value of n-grams in generation. In International natural language generation workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.In</title>
<date>2002</date>
<booktitle>Proceedings of the 6th Conference on Natural Language Learning</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="10223" citStr="Malouf, 2002" startWordPosition="1655" endWordPosition="1656">he problem. Similarly to the the MaxEnt set-up, the SVM learner will try to learn a linear scoring function as defined in Equation (4) above. However, instead of maximizing the probability of the preferred or positive realizations, we try to maximize their value forFwdirectly. Recall our definition of the set of positive training examples in Section 2.2. Let us here analogously defineXn={x1;:::;xQ}to be the negative counterpart, so that for a given pair x = (si; rj) E Xn, we have that rj E Y(si) but rj is not annotated as a preferred realization of si. Fol1We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. ( 8) Vijk s.t. (sk; ri) E Xp A (sk; rj) E Xn : w &apos; -4�(sk; ri) &gt;— w (sk; rj) + 1 — �i;j;k (9 ) Vijk : �i;j;k &gt;— 0 Joachims (2002) shows that the preference constraints in Equation (8) can be rewritten as (10)w&apos;(�(sk;ri)—&apos;D(sk;rj))&gt;—1—�i;j;k so that the optimization problem is equivalent to training a classifier on the pairwise difference vectors-(sk;ri)—&apos;(sk;rj). The (non-negative) slack variables �i;j;k are comm</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Malouf, R. (2002). A comparison of algorithms for maximum entropy parameter estimation.In Proceedings of the 6th Conference on Natural Language Learning (pp. 49–55). Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>G van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP workshop Beyond Shallow Analysis.</booktitle>
<location>Hainan, China.</location>
<contexts>
<context position="6434" citStr="Malouf &amp; Noord, 2004" startWordPosition="1017" endWordPosition="1020">n this case scoring is not conditioned on the input semantics at all, we still include it to make the function formulation more general as we will be reusing it later. Note that, as the realizations in our symmetric treebank also include punctuation marks, these are also treated as separate tokens by the language model (in addition to pseudo-tokens marking sentence boundaries). 2.2 Maximum Entropy Models Maximum entropy modeling provides a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection (e.g. Johnson, Geman, Canon, Chi, &amp; Riezler, 1999; Malouf &amp; Noord, 2004) and reranking for machine translation (e.g. Och et al., 2004). A model is specified by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Let us first introduce some notation before we go on. Let Y(si) = {r1, ... , rn,} be the set of realizations licensed by the grammar for a semantic representation si. Now, let our (positive) training data be given asXp={x1,...,xN}where each xiis a pair(si,rj)for whichrjEY(si)andrj is annotated in the treebank as being a correct realization ofs</context>
<context position="8810" citStr="Malouf &amp; Noord, 2004" startWordPosition="1418" endWordPosition="1421"> (log of) a penalized likelihood function as in log L(w) — Ei=1 wi lowing Joachims (2002), the goal is to minimize (7) V(w;0=2w&apos;w+C1:�i;j;k 1 (6) w=argmax d 2 subject to the following constraints, w 2o-2 where L(w) is the ‘conditionalized’ likelihood of the training data Xp (Johnson et al., 1999), computed as L(w) = HNi=1pw(rilsi). The second term of the likelihood function in Equation (6) is a penalty term that is commonly used for reducing the tendency of log-linear models to over-fit, especially when training on sparse data using many features (Chen &amp; Rosenfeld, 1999; Johnson et al., 1999; Malouf &amp; Noord, 2004). More specifically it defines a zero-mean Gaussian prior on the feature weights which effectively leads to less extreme values. After empirically tuning the prior on our ‘Jotunheimen’ treebank (training and testing by 10-fold cross-validation), we ended up usingQ2=0:003for the MaxEnt models applied in this paper. 2.3 SVM Rankers In this section we briefly formulate the optimization problem in terms of support vector machines. Our starting point is the SVM approach introduced in Joachims (2002) for learning ranking functions for information retrieval. In our case the aim is to learn a ranking </context>
<context position="20366" citStr="Malouf &amp; Noord (2004)" startWordPosition="3339" endWordPosition="3342">the surface token associated with the rightmost element of eachn-gram. Unless otherwise noted, we used a maximumn-gram size of three in the experiments reported here, again due to its empirically determined best overall performance. The number of instantiated features produced by the feature templates easily grows quite large. For the ‘Jotunheimen’ data the total number of distinct feature instantiations is 312,650. For the experiments in this paper we implemented a simple frequency based cutoff by removing features that are observed as relevant less thanctimes. We here follow the approach of Malouf &amp; Noord (2004) where relevance of a feature is simply defined as taking on a different value for any two competing candidates for the same input. A feature is only included in training if it is relevant for more than citems in the training data. Table 4 shows the effect on the accuracy of the MaxEnt model when varying the cutoff. We see that a model can be hspec dog det the le the sing noun n intr le 521 Cutoff Features Accuracy —312,650 71.18 1 264,455 71.18 2 112,051 70.03 3 66,069 70.28 4 46,139 69.30 5 35,295 67.93 10 16,036 65.36 20 7,563 63.05 50 2,605 59.10 100 889 54.21 200 261 50.11 500 34 34.70 Ta</context>
</contexts>
<marker>Malouf, Noord, 2004</marker>
<rawString>Malouf, R., &amp; Noord, G. van. (2004). Wide coverage parsing with stochastic attribute value grammars. In Proceedings of the IJCNLP workshop Beyond Shallow Analysis. Hainan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nakanishi</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies</booktitle>
<pages>93--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, Canada:</location>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Nakanishi, H., Miyao, Y., &amp; Tsujii, J. (2005). Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings of the 9th International Workshop on Parsing Technologies (pp. 93–102). Vancouver, Canada: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th Conference of the North American Chapter of the ACL.</booktitle>
<location>Boston.</location>
<contexts>
<context position="6496" citStr="Och et al., 2004" startWordPosition="1027" endWordPosition="1030">ll, we still include it to make the function formulation more general as we will be reusing it later. Note that, as the realizations in our symmetric treebank also include punctuation marks, these are also treated as separate tokens by the language model (in addition to pseudo-tokens marking sentence boundaries). 2.2 Maximum Entropy Models Maximum entropy modeling provides a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection (e.g. Johnson, Geman, Canon, Chi, &amp; Riezler, 1999; Malouf &amp; Noord, 2004) and reranking for machine translation (e.g. Och et al., 2004). A model is specified by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Let us first introduce some notation before we go on. Let Y(si) = {r1, ... , rn,} be the set of realizations licensed by the grammar for a semantic representation si. Now, let our (positive) training data be given asXp={x1,...,xN}where each xiis a pair(si,rj)for whichrjEY(si)andrj is annotated in the treebank as being a correct realization ofsi. Note that we might have several different members ofY(si)th</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A., Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D., Eng, K., Jain, V., Jin, Z., &amp; Radev, D. (2004). A smorgasbord of features for statistical machine translation. In Proceedings of the 5th Conference of the North American Chapter of the ACL. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Feature selection for a rich HPSG grammar using decision trees.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="16932" citStr="Toutanova &amp; Manning (2002)" startWordPosition="2735" endWordPosition="2738">a generation treebank, which taken together with the original parse-oriented pairings constitute a full symmetrical treebank. We have successfully applied this technique to the tourism segments of the LinGO Redwoods treebank, which in turn is built atop the ERG.2 2See ‘http://www,delph-in,net/erg/’ for fur520 Table 2 summarizes the two resulting data sets, which are both comprised of instructional texts on tourist activities, the application domain of the background MT system. 3.2 Feature Templates For the purpose of parse selection, Toutanova, Manning, Shieber, Flickinger, &amp; Oepen (2002) and Toutanova &amp; Manning (2002) train a discriminative log-linear model on the Redwoods parse treebank, using features defined over derivation trees with non-terminals representing the construction types and lexical types of the HPSG grammar (see Figure 1). The basic feature set of our MaxEnt realization ranker is defined in the same way (corresponding to the PCFG-S model of Toutanova &amp; Manning, 2002), each feature capturing a sub-tree from the derivation limited to depth one. Table 3 shows example features in our MaxEnt and SVM models, where the feature template # 1 corresponds to local derivation sub-trees. To reduce the </context>
</contexts>
<marker>Toutanova, Manning, 2002</marker>
<rawString>Toutanova, K., &amp; Manning, C. D. (2002). Feature selection for a rich HPSG grammar using decision trees. In Proceedings of the 6th Conference on Natural Language Learning. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
<author>S M Shieber</author>
<author>D Flickinger</author>
<author>S Oepen</author>
</authors>
<title>Parse disambiguation for a rich hpsg grammar.</title>
<date>2002</date>
<booktitle>In First workshop on treebanks and linguistic theories. Sozopol,</booktitle>
<marker>Toutanova, Manning, Shieber, Flickinger, Oepen, 2002</marker>
<rawString>Toutanova, K., Manning, C. D., Shieber, S. M., Flickinger, D., &amp; Oepen, S. (2002). Parse disambiguation for a rich hpsg grammar. In First workshop on treebanks and linguistic theories. Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Velldal</author>
<author>S Oepen</author>
<author>D Flickinger</author>
</authors>
<title>Paraphrasing treebanks for stochastic realization ranking.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd workshop on Treebanks and Linguistic Theories.</booktitle>
<location>T¨ubingen, Germany.</location>
<contexts>
<context position="2871" citStr="Velldal, Oepen, &amp; Flickinger, 2004" startWordPosition="439" endWordPosition="443">pplied to the task of ranking in sentence generation. The latter two models that we present here go beyond the surface information used by then-gram model, and are trained on a symmetric treebank with features defined over the full HPSG analyses of competing realizations. Furthermore, such discriminative models are suitable for ‘on-line’ use within our generator—adopting the technique of selective unpacking from a packed forest (Carroll &amp; Oepen, 2005)—which means our hybrid realizer obviates the need for exhaustive enumeration of candidate outputs. The present results extend our earlier work (Velldal, Oepen, &amp; Flickinger, 2004)—and the related work of Nakanishi, Miyao, &amp; Tsujii (2005)—to an enlarged data set, more feature types, and additional learners. The rest of this paper is structured as follows. Section 2 first gives a general summary of the various statistical models we will be considering, as well as the measures used for evaluating them. We then go on to define the task we are aiming to solve in terms of treebank data and feature types in Section 3. By looking at different variants of the MaxEnt model we review some results for the relative contribution of individual features and the impact of frequency cu</context>
<context position="14381" citStr="Velldal et al. (2004)" startWordPosition="2323" endWordPosition="2326"> the various string similarity measures usually agree on the relative ranking of the different models. 3 Data Sets and Features The following sections summarize the data sets and the feature types used in the experiments. 3.1 Symmetric Treebanks Conditional parse selection models are standardly trained on a treebank consisting of strings paired with their optimal analyses. For our discriminative realization ranking experiments we require training corpora that provide the inverse relation. By assuming that the preferences captured in a standard treebank can constitute a bidirectional relation, Velldal et al. (2004) propose a notion of symmetric treebanks as the combination of (a) a set of pairings of surface forms and associated semantics; combined with (b) the sets of alternative analJotunheimen Bin Items Words Trees Gold Chance 100&lt;n 396 21.7 367.4 20.7 0.083 50&lt;n&lt;100 246 18.5 73.7 11.5 0.160 10&lt;n&lt;50 831 14.8 24.2 6.3 0.277 5&lt;n&lt;10 426 10.1 7.0 3.0 0.436 1&lt;n&lt;5 291 11.2 3.3 1.6 0.486 Total 2190 15.1 85.7 8.2 0.287 Rondane Bin Items Words Trees Gold Chance 100&lt;n 107 21.8 498.4 17.8 0.060 50&lt;n&lt;100 63 19.1 72.9 12.0 0.162 10&lt;n&lt;50 244 15.2 23.4 4.9 0.234 5&lt;n&lt;10 119 11.9 7.2 2.7 0.377 1&lt;n&lt;5 101 9.3 3.21 1.5 </context>
<context position="21120" citStr="Velldal et al., 2004" startWordPosition="3480" endWordPosition="3483"> feature is only included in training if it is relevant for more than citems in the training data. Table 4 shows the effect on the accuracy of the MaxEnt model when varying the cutoff. We see that a model can be hspec dog det the le the sing noun n intr le 521 Cutoff Features Accuracy —312,650 71.18 1 264,455 71.18 2 112,051 70.03 3 66,069 70.28 4 46,139 69.30 5 35,295 67.93 10 16,036 65.36 20 7,563 63.05 50 2,605 59.10 100 889 54.21 200 261 50.11 500 34 34.70 Table 4: The effects of frequency-based feature selection with respect to model size and accuracy. model configuration basic model of (Velldal et al., 2004) basic plus partial daughter sequence basic plus grandparenting basic plus lexical type trigrams basic plus all of the above basic plus language model basic plus all of the above Table 5: Performance summaries of bestperforming realization rankers using various feature configurations, when compared to the set-up of Velldal et al. (2004). These scores where computed using a relevance cutoff of 3 and optimizing the variance of the prior for individual configurations. compacted quite aggressively without sacrificing much in performance. For all models presented below we use a cutoff ofc=3. 4 Resu</context>
</contexts>
<marker>Velldal, Oepen, Flickinger, 2004</marker>
<rawString>Velldal, E., Oepen, S., &amp; Flickinger, D. (2004). Paraphrasing treebanks for stochastic realization ranking. In Proceedings of the 3rd workshop on Treebanks and Linguistic Theories. T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Conference on Natural Language Generation.</booktitle>
<publisher>Hampshire, UK.</publisher>
<contexts>
<context position="5069" citStr="White (2004)" startWordPosition="803" endWordPosition="804">leash. Table 1: A small example set of generator outputs using the ERG. Where the input semantics is no specified for aspects of information structure (e.g. requesting foregrounding of a specific entity), paraphrases include all grammatically legitimate topicalizations. Other choices involve, for example, the optionality of complementizers and relative pronouns, permutation of (intersective) modifiers, and lexical and orthographic alternations. 2.1 Language Models The use ofn-gram language models is the most common approach to statistical selection in generation (Langkilde &amp; Knight, 1998; and White (2004); inter alios). In order to better assert the relative performance of the discriminative models and the structural features we present below, we also apply a trigram model to the ranking problem. Using the freely available CMU SLM Toolkit (Clarkson &amp; Rosenfeld, 1997), we trained a trigram model on an unannotated version of the British National Corpus (BNC), containing roughly 100 million words (using Witten-Bell discounting and back-off). Given such a model pn, the score of a realization ri with surface form wki1=(wi1,...,wik)is then computed as k (1)F(s,ri)= pn(wi,jIwi,j—n,...,wi,j-1) j=1 Giv</context>
</contexts>
<marker>White, 2004</marker>
<rawString>White, M. (2004). Reining in CCG chart realization. In Proceedings of the 3rd International Conference on Natural Language Generation. Hampshire, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>