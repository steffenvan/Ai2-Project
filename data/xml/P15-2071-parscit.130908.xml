<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009104">
<title confidence="0.991585">
Automatic Discrimination between Cognates and Borrowings
</title>
<author confidence="0.9937">
Alina Maria Ciobanu, Liviu P. Dinu
</author>
<affiliation confidence="0.979327">
Faculty of Mathematics and Computer Science, University of Bucharest
Center for Computational Linguistics, University of Bucharest
</affiliation>
<email confidence="0.982008">
alina.ciobanu@my.fmi.unibuc.ro,ldinu@fmi.unibuc.ro
</email>
<sectionHeader confidence="0.993556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998425">
Identifying the type of relationship be-
tween words provides a deeper insight into
the history of a language and allows a bet-
ter characterization of language related-
ness. In this paper, we propose a com-
putational approach for discriminating be-
tween cognates and borrowings. We show
that orthographic features have discrimi-
native power and we analyze the underly-
ing linguistic factors that prove relevant in
the classification task. To our knowledge,
this is the first attempt of this kind.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999629">
Natural languages are living eco-systems. They
are subject to continuous change due, in part, to
the natural phenomena of language contact and
borrowing (Campbell, 1998). According to Hall
(1960), there is no such thing as a “pure language”
– a language “without any borrowing from a for-
eign language”. Although admittedly regarded
as relevant factors in the history of a language
(McMahon et al., 2005), borrowings bias the ge-
netic classification of the languages, characteriz-
ing them as being closer than they actually are
(Minett and Wang, 2003). Thus, the need for
discriminating between cognates and borrowings
emerges. Heggarty (2012) acknowledges the ne-
cessity and difficulty of the task, emphasizing the
role of the “computerized approaches”.
In this paper we address the task of automati-
cally distinguishing between borrowings and cog-
nates: given a pair of words, the task is to de-
termine whether one is a historical descendant of
the other, or whether they both share a common
ancestor. A borrowing (also called loanword), is
defined by Campbell (1998) as a “lexical item (a
word) which has been ‘borrowed’ from another
language, a word which originally was not part of
the vocabulary of the recipient language but was
adopted from some other language and made part
of the borrowing language’s vocabulary”. The no-
tion of cognate is much more relaxed, and vari-
ous NLP tasks and applications use different def-
initions of the cognate pairs. In some situations,
cognates and borrowings are considered together,
and are referred to as historically connected words
(Kessler, 2001) or denoted by the term correlates
(Heggarty, 2012; McMahon et al., 2005). In some
tasks, such as statistical machine translation (Kon-
drak et al., 2003) and sentence alignment, or when
studying the similarity or intelligibility of the lan-
guages, cognates are seen as words that have sim-
ilar spelling and meaning, their etymology being
completely disregarded. However, in problems
of language classification, distinguishing cognates
from borrowings is essential. Here, we account
for the etymology of the words, and we adopt the
following definition: two words form a cognate
pair if they share a common ancestor and have
the same meaning. In other words, they derive di-
rectly from the same word, have a similar meaning
and, due to various (possibly language-specific)
changes across time, their forms might differ.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9997695">
In a natural way, one of the most investigated
problems in historical linguistics is to determine
whether similar words are related or not (Kondrak,
2002). Investigating pairs of related words is very
useful not only in historical and comparative lin-
guistics, but also in the study of language relat-
edness (Ng et al., 2010), phylogenetic inference
(Atkinson et al., 2005) and in identifying how and
to what extent languages changed over time or in-
fluenced each other.
Most studies in this area focus on automatically
identifying pairs of cognates. For measuring the
orthographic or phonetic proximity of the cog-
nate candidates, string similarity metrics (Inkpen
</bodyText>
<page confidence="0.946505">
431
</page>
<bodyText confidence="0.9484212">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
et al., 2005; Hall and Klein, 2010) and algo-
rithms for string alignment (Delmestri and Cris-
tianini, 2010) have been applied, both in cognate
detection (Koehn and Knight, 2000; Mulloni and
Pekar, 2006; Navlea and Todirascu, 2011) and in
cognate production (Beinborn et al., 2013; Mul-
loni, 2007). Minett and Wang (2003) focus on
identifying borrowings within a family of genet-
ically related languages and propose, to this end,
a distance-based and a character-based technique.
Minett and Wang (2005) address the problem of
identifying language contact, building on the idea
that borrowings bias the lexical similarities among
genetically related languages.
According to the regularity principle, the dis-
tinction between cognates and borrowings benefits
from the regular sound changes that generate reg-
ular phoneme correspondences in cognates (Kon-
drak, 2002). In turn, sound correspondences are
represented, to a certain extent, by alphabetic char-
acter correspondences (Delmestri and Cristianini,
2010).
</bodyText>
<sectionHeader confidence="0.976422" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999865416666667">
In light of this, we investigate whether cognates
can be automatically distinguished from borrow-
ings based on their orthography. More specifically,
our task is as follows: given a pair of words in two
different languages (x, y), we want to determine
whether x and y are cognates or if y is borrowed
from x (in other words, x is the etymon of y).
Our starting point is a methodology that has
previously proven successful in discriminating be-
tween related and unrelated words (Ciobanu and
Dinu, 2014b). Briefly, the method comprises the
following steps:
</bodyText>
<listItem confidence="0.973627166666667">
1) Aligning the pairs of related words using a
string alignment algorithm;
2) Extracting orthographic features from the
aligned words;
3) Training a binary classifier to discriminate
between the two types of relationship.
</listItem>
<bodyText confidence="0.998742333333333">
To align the pairs of related words, we em-
ploy the Needleman-Wunsch global alignment al-
gorithm (Needleman and Wunsch, 1970), which is
equivalent to the weighted edit distance algorithm.
We consider words as input sequences and we use
a very simple substitution matrix1, which assigns
</bodyText>
<footnote confidence="0.994135">
1In our future work, we intend to also experiment with
more informed language-specific substitution matrices.
</footnote>
<table confidence="0.997199333333333">
Lang. Cognates Borrowings
len1 len2 edit len1 len2 edit
IT-RO 7.95 8.78 0.26 7.58 8.41 0.29
ES-RO 7.91 8.33 0.26 5.78 6.14 0.52
PT-RO 7.99 8.35 0.28 5.35 5.42 0.52
TR-RO 7.35 6.88 0.31 6.49 6.09 0.44
</table>
<tableCaption confidence="0.913126">
Table 2: Statistics for the dataset of related words.
</tableCaption>
<bodyText confidence="0.951430391304348">
Given a pair of languages (L1, L2), the len1 and
len2 columns represent the average word length of
the words in L1 and L2. The edit column rep-
resents the average normalized edit distance be-
tween the words. The values are computed only
on the training data, to keep the test data unseen.
equal scores to all substitutions, disregarding dia-
critics (e.g., we ensure that e and e` are matched).
As features, we use characters n-grams extracted
from the alignment2. We mark word boundaries
with $ symbols. For example, the Romanian word
funct¸ie (meaning function) and its Spanish cognate
pair funci´on are aligned as follows:
$ f u n c t¸ i e - $
$ f u n c - i o´ n $
The features for n = 2 are:
$f&gt;-$f, fu&gt;-fu, un&gt;-un, nc&gt;-nc, ct¸&gt;-c-,
t¸i&gt;--i, ie&gt;-i´o, e-&gt;-´on, -$&gt;-n$.
For the prediction task, we experiment with
two models, Naive Bayes and Support Vector Ma-
chines. We extend the method by introducing ad-
ditional linguistic features and we conduct an anal-
ysis on their predictive power.
</bodyText>
<sectionHeader confidence="0.996133" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999915">
In this section we present and analyze the experi-
ments we run for discriminating between cognates
and borrowings.
</bodyText>
<subsectionHeader confidence="0.99309">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.991568909090909">
Our experiments revolve around Romanian, a Ro-
mance language belonging to the Italic branch
of the Indo-European language family. It is sur-
rounded by Slavic languages and its relationship
with the big Romance kernel was difficult. Its ge-
ographic position, at the North of the Balkans, put
2While the original methodology proposed features ex-
tracted around mismatches in the alignment, we now compare
two approaches: 1) features extracted around mismatches,
and 2) features extracted from the entire alignment. The latter
approach leads to better results, as measured on the test set.
</bodyText>
<page confidence="0.99333">
432
</page>
<figure confidence="0.811535">
Lang. Borrowings Cognates
IT-RO baletto → balet (ballet) vittoria - victorie (victory) T victoria (LAT)
PT-RO selva → selv˘a(selva) instinto - instinct (instinct) T instinctus (LAT)
ES-RO machete → macet˘a (machete) castillo - castel (castle) T castellum (LAT)
TR-RO t¨ut¨un → tutun (tobacco) aranjman - aranjament (arrangement) T arrangement (FR)
</figure>
<tableCaption confidence="0.985386">
Table 1: Examples of borrowings and cognates. For cognates we also report the common ancestor.
</tableCaption>
<bodyText confidence="0.999570888888889">
it in contact not only with the Balkan area, but also
with the vast majority of Slavic languages. Polit-
ical and administrative relationships with the Ot-
toman Empire, Greece (the Phanariot domination)
and the Habsburg Empire exposed Romanian to
a wide variety of linguistic influences. We apply
our method on four pairs of languages extracted
from the dataset proposed by Ciobanu and Dinu
(2014c):
</bodyText>
<listItem confidence="0.99998475">
• Italian - Romanian (IT-RO);
• Portuguese - Romanian (PT-RO);
• Spanish -Romanian (ES-RO);
• Turkish -Romanian (TR-RO).
</listItem>
<bodyText confidence="0.999943166666667">
For the first three pairs of languages, which
are formed of sister languages3, most cognate
pairs have a Latin common ancestor, while for the
fourth pair, formed of languages belonging to dif-
ferent families (Romance and Turkic), most of the
cognate pairs have a common French etymology,
and date back to the end of the 19th century, when
both Romanian and Turkish borrowed massively
from French. In Table 1 we provide examples of
borrowings and cognates.
The dataset contains borrowings4 and cognates
that share a common ancestor. The words (and in-
formation about their origins) were extracted from
electronic dictionaries and their relationships were
determined based on their etymology. We use a
stratified dataset of 2,600 pairs of related words
for each pair of languages. In Table 2 we provide
an initial analysis of our dataset. We report statis-
tics regarding the length of the words and the edit
distance between them. The difference in length
between the related words shows what operations
to expect when aligning the words. Romanian
words are almost in all situations shorter, in av-
erage, than their pairs. For TR-RO len1 is higher
</bodyText>
<footnote confidence="0.9706786">
3Sister languages are “languages which are related to one
another by virtue of having descended from the same com-
mon ancestor (proto-language)” (Campbell, 1998).
4Romanian is always the recipient language in our dataset
(i.e., the language that borrowed the words).
</footnote>
<bodyText confidence="0.9891441">
than len2, so we expect more deletions for this pair
of languages. The edit columns show how much
words vary from one language to another based on
their relationship (cognates or borrowings). For
IT-RO both distances are small (0.26 and 0.29), as
opposed to the other languages, where there is a
more significant difference between the two (e.g.,
0.26 and 0.52 for ES-RO). The small difference
for IT-RO might make the discrimination between
the two classes more difficult.
</bodyText>
<subsectionHeader confidence="0.956409">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.9999648">
Given the initial analysis presented above, we
hypothesize that the distance between the words
might be indicative of the type of relationship
between them. Previous studies (Inkpen et al.,
2005; Gomes and Lopes, 2011) show that related
and non-related words can be distinguished based
on the distance between them, but a finer-grained
task, such as determining the type of relationship
between the words, is probably more subtle. We
compare our method with two baselines:
</bodyText>
<listItem confidence="0.972049">
• A baseline which assigns a label based on the
normalized edit distance between the words:
given a test instance pair words - word2, we
subtract the average normalized edit distance
between words and word2 from the aver-
age normalized edit distance of the cognate
pairs and from the average normalized edit
distance between the borrowings and their et-
ymons (computed on the training set; see Ta-
ble 2), and assign the label which yields a
smaller difference (in absolute value). In case
of equality, the label is chosen randomly.
• A decision tree classifier, following the strat-
</listItem>
<bodyText confidence="0.910421">
egy proposed by Inkpen et al. (2005): we
use the normalized edit distance as single fea-
ture, and we fit a decision tree classifier with
the maximum tree depth set to 1. We per-
form 3-fold cross-validation in order to se-
lect the best threshold for discriminating be-
tween borrowings and cognates. Using the
</bodyText>
<page confidence="0.997517">
433
</page>
<bodyText confidence="0.999709666666667">
best threshold selected for each language, we
further assign one of the two classes to the
pairs of words in our test set.
</bodyText>
<subsectionHeader confidence="0.99758">
4.3 Task Setup
</subsectionHeader>
<bodyText confidence="0.9998372">
We experiment with Naive Bayes and Support
Vector Machines (SVMs) to learn orthographic
changes. We put our system together using the
Weka5 workbench (Hall et al., 2009). For SVM,
we employ the radial basis function kernel (RBF)
and we use the wrapper provided by Weka for
LibSVM (Chang and Lin, 2011). For each lan-
guage pair, we split the dataset in two stratified
subsets, for training and testing, with a 3:1 ra-
tio. We experiment with different values for the
n-gram size (n E {1, 2, 3}) and we perform grid
search and 3-fold cross validation over the train-
ing set in order to optimize hyperparameters c and
-y for SVM. We search over {1, 2, ...,10} for c and
over {10−
</bodyText>
<subsectionHeader confidence="0.918775">
4.4 Results Analysis
</subsectionHeader>
<bodyText confidence="0.999972384615385">
Table 3 and Table 4 show the results of our ex-
periment. The two baselines produce comparable
results. For all pairs of languages, our method sig-
nificantly improves over the baselines (99% con-
fidence level)6 with values between 7% and 29%
for the F1 score, suggesting that the n-grams ex-
tracted from the alignment of the words are bet-
ter indicators of the type of relationship than the
edit distance between them. The best results are
obtained for TR-RO, with an F1 score of 92.1,
followed closely by PT-RO with 90.1 and ES-RO
with 85.5. These results show that, for these pairs
of languages, the orthographic cues are different
with regard to the relationship between the words.
For IT-RO we obtain the lowest F1 score, 69.0.
In this experiment, we know beforehand that
there is a relationship between the words, and our
aim is to identify the type of relationship. How-
ever, in many situations this kind of a-priori in-
formation is not available. In a real scenario, we
would have either to add an intermediary clas-
sifier for discriminating between related and un-
related words, or to discriminate between three
classes: cognates, borrowings, and unrelated. We
augment our dataset with unrelated words (deter-
mined based on their etymology), building a strat-
</bodyText>
<footnote confidence="0.98740675">
5www.cs.waikato.ac.nz/ml/weka
6All the statistical significance tests reported in this paper
are performed on 1,000 iterations of paired bootstrap resam-
pling (Koehn, 2004).
</footnote>
<table confidence="0.999543666666667">
Lang. Baseline #1 Baseline #2
P R Fl P R Fl
IT-RO 50.7 50.7 50.7 64.4 54.5 45.0
PT-RO 79.3 79.0 79.2 80.1 80.0 80.0
ES-RO 78.6 78.4 78.5 78.6 78.5 78.4
TR-RO 61.1 61.0 61.1 62.5 59.8 57.6
</table>
<tableCaption confidence="0.669533666666667">
Table 3: Weighted average precision (P), recall
(R) and F1 score (F1) for automatic discrimina-
tion between cognates and borrowings.
</tableCaption>
<bodyText confidence="0.9978435">
ified dataset annotated with three classes, and we
repeat the previous experiment. The performance
decreases7, but the results are still significantly
better than chance (99% confidence level).
</bodyText>
<subsectionHeader confidence="0.986266">
4.5 Linguistic Factors
</subsectionHeader>
<bodyText confidence="0.999950481481481">
To gain insight into the factors with high predictive
power, we perform several further experiments.
Part of speech. We investigate whether adding
knowledge about the part of speech of the
words leads to performance improvements.
Verbs, nouns, adverbs and adjectives have
language-specific endings, thus we assume that
part of speech might be useful when learning
orthographic patterns. We obtain POS tags from
the DexOnline8 machine-readable dictionary.
We employ the POS feature as an additional
categorical feature for the learning algorithm. It
turns out that, except for PT-RO (F1 score 92.3),
the additional POS feature does not improve the
performance of our method.
Syllabication. We analyze whether the system
benefits from using the syllabified form of the
words as input to the alignment algorithm. We
are interested to see if marking the boundaries be-
tween the syllables improves the alignment (and,
thus, the feature extraction). We obtain the syl-
labication for the words in our dataset from the
RoSyllabiDict dictionary (Barbu, 2008) for Roma-
nian words and several available Perl modules9 for
the other languages. For PT-RO and ES-RO the
F1 score increases by about 1%, reaching a value
of 93.4 for the former and 86.7 for the latter.
</bodyText>
<footnote confidence="0.927604714285714">
7Weighted average Fl score on the test set for SVM:
IT-RO 63.8, PT-RO 77.6, ES-RO 74.0, TR-RO 86.1.
8www.dexonline.ro
9Lingua::ID::Hyphenate modules where ID ∈ {IT, PT,
ES, TR}, available on the Comprehensive Perl Archive Net-
work: www.cpan.org.
2,10−1,100,101,102} for-y.
</footnote>
<page confidence="0.934704">
434
</page>
<table confidence="0.999703">
Lang. Naive Bayes SVM
P R F1 n P R F1 n c -y
IT-RO 68.6 68.2 68.3 3 69.2 69.1 69.0 3 10 0.10
PT-RO 92.6 91.7 92.1 3 90.1 90.0 90.0 3 3 0.10
ES-RO 85.3 84.5 84.9 3 85.7 85.5 85.5 2 2 0.10
TR-RO 89.7 89.4 89.5 3 90.3 90.2 90.1 3 6 0.01
</table>
<tableCaption confidence="0.919176">
Table 4: Weighted average precision (P), recall (R), F1 score (F1) and optimal n-gram size for automatic
discrimination between cognates and borrowings. For SVM we also report the optimal values for c and -y.
</tableCaption>
<bodyText confidence="0.999848027027027">
Consonants. We examine the performance of
our system when trained and tested only on the
aligned consonant skeletons of the words (i.e., a
version of the words where vowels are discarded).
According to Ashby and Maidment (2005), conso-
nants change at a slower pace than vowels across
time; while the former are regarded as reference
points, the latter are believed to carry less infor-
mation useful for identifying the words (Gooskens
et al., 2008). The performance of the system
decreases when vowels are removed (95% confi-
dence level). We also train and test the decision
tree classifier on this version of the dataset, and
its performance is lower in this case as well (95%
confidence level), indicating that, for our task, the
information carried by the vowels is helpful.
Stems. We repeat the first experiment using
stems as input, instead of lemmas. What we seek
to understand is whether the aligned affixes are in-
dicative of the type of relationship between the
words. We use the Snowball Stemmer10 and we
find that the performance decreases when stems
are used instead of lemmas. Performing a x2 fea-
ture ranking on the features extracted from mis-
matches in the alignment of the related words re-
veals further insight into this matter: for all pairs
of languages, at least one feature containing the $
character (indicating the beginning or the end of a
word) is ranked among the 10 most relevant fea-
tures, and over 50 are ranked among the 500 most
relevant features. This suggests that prefixes and
suffixes (usually removed by the stemmer) vary
with the type of relationship between the words.
Diacritics. We explore whether removing dia-
critics influences the performance of the system.
Many words have undergone transformations by
the augmentation of language-specific diacritics
</bodyText>
<footnote confidence="0.753451">
10http://snowball.tartarus.org
</footnote>
<bodyText confidence="0.9996971">
when entering a new language (Ciobanu and Dinu,
2014a). For this reason, we expect diacritics to
play a role in the classification task. We observe
that, when diacritics are removed, the F1 score
on the test set is lower in almost all situations.
Analyzing the ranking of the features extracted
from mismatches in the alignment provides even
stronger evidence in this direction: for all pairs of
languages, more than a fifth of the top 500 features
contain diacritics.
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99997885">
In this paper, we propose a computational method
for discriminating between cognates and borrow-
ings based on their orthography. Our results show
that it is possible to identify the type of rela-
tionship with fairly good performance (over 85.0
F1 score) for 3 out of the 4 pairs of languages we
investigate. Our predictive analysis shows that the
orthographic cues are different for cognates and
borrowings, and that underlying linguistic factors
captured by our model, such as affixes and diacrit-
ics, are indicative of the type of relationship be-
tween the words. Other insights, such as the syl-
labication or the part of speech of the words, are
shown to have little or no predictive power. We
intend to further account for finer-grained char-
acteristics of the words and to extend our exper-
iments to more languages. The method we pro-
pose is language-independent, but we believe that
incorporating language-specific knowledge might
improve the system’s performance.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9994204">
We thank the anonymous reviewers for their help-
ful and constructive comments. The contribu-
tion of the authors to this paper is equal. Liviu
P. Dinu was supported by UEFISCDI, PNII-ID-
PCE-2011-3-0959.
</bodyText>
<page confidence="0.999014">
435
</page>
<sectionHeader confidence="0.990072" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999297411214953">
Michael Ashby and John Maidment. 2005. Introduc-
ing Phonetic Science. Cambridge University Press.
Quentin D. Atkinson, Russell D. Gray, Geoff K.
Nicholls, and David J. Welch. 2005. From Words
to Dates: Water into Wine, Mathemagic or Phylo-
genetic Inference? Transactions of the Philological
Society, 103(2):193–219.
Ana-Maria Barbu. 2008. Romanian Lexical Data
Bases: Inflected and Syllabic Forms Dictionaries. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation, LREC 2008,
pages 1937–1941.
Lisa Beinborn, Torsten Zesch, and Iryna Gurevych.
2013. Cognate Production using Character-based
Machine Translation. In Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing, IJCNLP 2013, pages 883–891.
Lyle Campbell. 1998. Historical Linguistics. An Intro-
duction. MIT Press.
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2(3):27:1–27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.
Alina Maria Ciobanu and Liviu P. Dinu. 2014a. An
Etymological Approach to Cross-Language Ortho-
graphic Similarity. Application on Romanian. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2014, pages 1047–1058.
Alina Maria Ciobanu and Liviu P. Dinu. 2014b. Au-
tomatic Detection of Cognates Using Orthographic
Alignment. In Proceedings of the 52stAnnual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2014, pages 99–105.
Alina Maria Ciobanu and Liviu P. Dinu. 2014c. Build-
ing a Dataset of Multilingual Cognates for the Ro-
manian Lexicon. In Proceedings of the 9th Interna-
tional Conference on Language Resources and Eval-
uation, LREC 2014.
Antonella Delmestri and Nello Cristianini. 2010.
String Similarity Measures and PAM-like Matrices
for Cognate Identification. Bucharest Working Pa-
pers in Linguistics, 12(2):71–82.
Lu´ıs Gomes and Jos´e Gabriel Pereira Lopes. 2011.
Measuring Spelling Similarity for Cognate Identifi-
cation. In Proceedings of the 15th Portugese Con-
ference on Progress in Artificial Intelligence, EPIA
2011, pages 624–633. Software available at http:
//research.variancia.com/spsim.
Charlotte Gooskens, Wilbert Heeringa, and Karin Bei-
jering. 2008. Phonetic and Lexical Predictors of
Intelligibility. International Journal of Humanities
and Arts Computing, 2(1-2):63–81.
David Hall and Dan Klein. 2010. Finding Cognate
Groups Using Phylogenies. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2010, pages 1030–1039.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10–18.
Robert Anderson Hall. 1960. Linguistics and Your
Language. Doubleday New York.
Paul Heggarty. 2012. Beyond Lexicostatistics: How
to Get More out of ”Word List” Comparisons. In
Quantitative Approaches to Linguistic Diversity:
Commemorating the Centenary of the Birth of Mor-
ris Swadesh, pages 113–137. Benjamins.
Diana Inkpen, Oana Frunza, and Grzegorz Kondrak.
2005. Automatic Identification of Cognates and
False Friends in French and English. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, RANLP
2005, pages 251–257.
Brett Kessler. 2001. The Significance of Word Lists.
Stanford: CSLI Publications.
Philipp Koehn and Kevin Knight. 2000. Estimat-
ing Word Translation Probabilities from Unrelated
Monolingual Corpora Using the EM Algorithm. In
Proceedings of the 17th National Conference on Ar-
tificial Intelligence and 12th Conference on Inno-
vative Applications of Artificial Intelligence, pages
711–715.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2004, pages 388–
395.
Grzegorz Kondrak, Daniel Marcu, and Keven Knight.
2003. Cognates Can Improve Statistical Translation
Models. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, HLT-NAACL 2003, pages 46–48.
Grzegorz Kondrak. 2002. Algorithms For Language
Reconstruction. Ph.D. thesis, University of Toronto.
April McMahon, Paul Heggarty, Robert McMahon,
and Natalia Slaska. 2005. Swadesh Sublists
and the Benefits of Borrowing: an Andean Case
Study. Transactions of the Philological Society,
103(2):147–170.
James W. Minett and William S.-Y. Wang. 2003.
On Detecting Borrowing: Distance-based and
Character-based Approaches. Diachronica,
20(2):289–331.
</reference>
<page confidence="0.990448">
436
</page>
<reference confidence="0.998202466666667">
James W. Minett and William S.-Y. Wang. 2005. Ver-
tical and Horizontal Transmission in Language Evo-
lution. Transactions of the Philological Society,
103(2):121–146.
Andrea Mulloni and Viktor Pekar. 2006. Automatic
Detection of Orthographic Cues for Cognate Recog-
nition. In In Proceedings of the 5th International
Conference on Language Resources and Evaluation,
LREC 2006, pages 2387–2390.
Andrea Mulloni. 2007. Automatic Prediction of Cog-
nate Orthography Using Support Vector Machines.
In Proceedings of the 45th Annual Meeting of the
ACL: Student Research Workshop, ACL 2007, pages
25–30.
Mirabela Navlea and Amalia Todirascu. 2011. Using
Cognates in a French-Romanian Lexical Alignment
System: A Comparative Study. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing, RANLP 2011, pages
247–253.
Saul B. Needleman and Christian D. Wunsch. 1970. A
General Method Applicable to the Search for Sim-
ilarities in the Amino Acid Sequence of Two Pro-
teins. Journal of Molecular Biology, 48(3):443 –
453.
Ee-Lee Ng, Beatrice Chin, Alvin W. Yeo, and Bali
Ranaivo-Malanc¸on. 2010. Identification of Closely-
Related Indigenous Languages: An Orthographic
Approach. International Journal ofAsian Language
Processing, 20(2):43–62.
</reference>
<page confidence="0.998623">
437
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.703111">
<title confidence="0.998393">Automatic Discrimination between Cognates and Borrowings</title>
<author confidence="0.94047">Alina Maria Ciobanu</author>
<author confidence="0.94047">P Liviu</author>
<affiliation confidence="0.9485855">Faculty of Mathematics and Computer Science, University of Center for Computational Linguistics, University of</affiliation>
<email confidence="0.797219">alina.ciobanu@my.fmi.unibuc.ro,ldinu@fmi.unibuc.ro</email>
<abstract confidence="0.996782153846154">Identifying the type of relationship between words provides a deeper insight into the history of a language and allows a better characterization of language relatedness. In this paper, we propose a computational approach for discriminating between cognates and borrowings. We show that orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classification task. To our knowledge, this is the first attempt of this kind.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Ashby</author>
<author>John Maidment</author>
</authors>
<title>Introducing Phonetic Science.</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="17511" citStr="Ashby and Maidment (2005)" startWordPosition="2834" endWordPosition="2837">IT-RO 68.6 68.2 68.3 3 69.2 69.1 69.0 3 10 0.10 PT-RO 92.6 91.7 92.1 3 90.1 90.0 90.0 3 3 0.10 ES-RO 85.3 84.5 84.9 3 85.7 85.5 85.5 2 2 0.10 TR-RO 89.7 89.4 89.5 3 90.3 90.2 90.1 3 6 0.01 Table 4: Weighted average precision (P), recall (R), F1 score (F1) and optimal n-gram size for automatic discrimination between cognates and borrowings. For SVM we also report the optimal values for c and -y. Consonants. We examine the performance of our system when trained and tested only on the aligned consonant skeletons of the words (i.e., a version of the words where vowels are discarded). According to Ashby and Maidment (2005), consonants change at a slower pace than vowels across time; while the former are regarded as reference points, the latter are believed to carry less information useful for identifying the words (Gooskens et al., 2008). The performance of the system decreases when vowels are removed (95% confidence level). We also train and test the decision tree classifier on this version of the dataset, and its performance is lower in this case as well (95% confidence level), indicating that, for our task, the information carried by the vowels is helpful. Stems. We repeat the first experiment using stems as</context>
</contexts>
<marker>Ashby, Maidment, 2005</marker>
<rawString>Michael Ashby and John Maidment. 2005. Introducing Phonetic Science. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quentin D Atkinson</author>
<author>Russell D Gray</author>
<author>Geoff K Nicholls</author>
<author>David J Welch</author>
</authors>
<title>From Words to Dates: Water into Wine, Mathemagic or Phylogenetic Inference?</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="3572" citStr="Atkinson et al., 2005" startWordPosition="553" endWordPosition="556"> if they share a common ancestor and have the same meaning. In other words, they derive directly from the same word, have a similar meaning and, due to various (possibly language-specific) changes across time, their forms might differ. 2 Related Work In a natural way, one of the most investigated problems in historical linguistics is to determine whether similar words are related or not (Kondrak, 2002). Investigating pairs of related words is very useful not only in historical and comparative linguistics, but also in the study of language relatedness (Ng et al., 2010), phylogenetic inference (Atkinson et al., 2005) and in identifying how and to what extent languages changed over time or influenced each other. Most studies in this area focus on automatically identifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein,</context>
</contexts>
<marker>Atkinson, Gray, Nicholls, Welch, 2005</marker>
<rawString>Quentin D. Atkinson, Russell D. Gray, Geoff K. Nicholls, and David J. Welch. 2005. From Words to Dates: Water into Wine, Mathemagic or Phylogenetic Inference? Transactions of the Philological Society, 103(2):193–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Barbu</author>
</authors>
<title>Romanian Lexical Data Bases: Inflected and Syllabic Forms Dictionaries.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC</booktitle>
<pages>1937--1941</pages>
<contexts>
<context position="16363" citStr="Barbu, 2008" startWordPosition="2630" endWordPosition="2631">eadable dictionary. We employ the POS feature as an additional categorical feature for the learning algorithm. It turns out that, except for PT-RO (F1 score 92.3), the additional POS feature does not improve the performance of our method. Syllabication. We analyze whether the system benefits from using the syllabified form of the words as input to the alignment algorithm. We are interested to see if marking the boundaries between the syllables improves the alignment (and, thus, the feature extraction). We obtain the syllabication for the words in our dataset from the RoSyllabiDict dictionary (Barbu, 2008) for Romanian words and several available Perl modules9 for the other languages. For PT-RO and ES-RO the F1 score increases by about 1%, reaching a value of 93.4 for the former and 86.7 for the latter. 7Weighted average Fl score on the test set for SVM: IT-RO 63.8, PT-RO 77.6, ES-RO 74.0, TR-RO 86.1. 8www.dexonline.ro 9Lingua::ID::Hyphenate modules where ID ∈ {IT, PT, ES, TR}, available on the Comprehensive Perl Archive Network: www.cpan.org. 2,10−1,100,101,102} for-y. 434 Lang. Naive Bayes SVM P R F1 n P R F1 n c -y IT-RO 68.6 68.2 68.3 3 69.2 69.1 69.0 3 10 0.10 PT-RO 92.6 91.7 92.1 3 90.1 9</context>
</contexts>
<marker>Barbu, 2008</marker>
<rawString>Ana-Maria Barbu. 2008. Romanian Lexical Data Bases: Inflected and Syllabic Forms Dictionaries. In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2008, pages 1937–1941.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Beinborn</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Cognate Production using Character-based Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing, IJCNLP</booktitle>
<pages>883--891</pages>
<contexts>
<context position="4420" citStr="Beinborn et al., 2013" startWordPosition="680" endWordPosition="683">ty of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences in cognates (Kondrak, 2002). In turn, sound corre</context>
</contexts>
<marker>Beinborn, Zesch, Gurevych, 2013</marker>
<rawString>Lisa Beinborn, Torsten Zesch, and Iryna Gurevych. 2013. Cognate Production using Character-based Machine Translation. In Proceedings of the 6th International Joint Conference on Natural Language Processing, IJCNLP 2013, pages 883–891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyle Campbell</author>
</authors>
<title>Historical Linguistics. An Introduction.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="956" citStr="Campbell, 1998" startWordPosition="134" endWordPosition="135">s a deeper insight into the history of a language and allows a better characterization of language relatedness. In this paper, we propose a computational approach for discriminating between cognates and borrowings. We show that orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classification task. To our knowledge, this is the first attempt of this kind. 1 Introduction Natural languages are living eco-systems. They are subject to continuous change due, in part, to the natural phenomena of language contact and borrowing (Campbell, 1998). According to Hall (1960), there is no such thing as a “pure language” – a language “without any borrowing from a foreign language”. Although admittedly regarded as relevant factors in the history of a language (McMahon et al., 2005), borrowings bias the genetic classification of the languages, characterizing them as being closer than they actually are (Minett and Wang, 2003). Thus, the need for discriminating between cognates and borrowings emerges. Heggarty (2012) acknowledges the necessity and difficulty of the task, emphasizing the role of the “computerized approaches”. In this paper we a</context>
<context position="10566" citStr="Campbell, 1998" startWordPosition="1672" endWordPosition="1673">. We use a stratified dataset of 2,600 pairs of related words for each pair of languages. In Table 2 we provide an initial analysis of our dataset. We report statistics regarding the length of the words and the edit distance between them. The difference in length between the related words shows what operations to expect when aligning the words. Romanian words are almost in all situations shorter, in average, than their pairs. For TR-RO len1 is higher 3Sister languages are “languages which are related to one another by virtue of having descended from the same common ancestor (proto-language)” (Campbell, 1998). 4Romanian is always the recipient language in our dataset (i.e., the language that borrowed the words). than len2, so we expect more deletions for this pair of languages. The edit columns show how much words vary from one language to another based on their relationship (cognates or borrowings). For IT-RO both distances are small (0.26 and 0.29), as opposed to the other languages, where there is a more significant difference between the two (e.g., 0.26 and 0.52 for ES-RO). The small difference for IT-RO might make the discrimination between the two classes more difficult. 4.2 Baselines Given </context>
</contexts>
<marker>Campbell, 1998</marker>
<rawString>Lyle Campbell. 1998. Historical Linguistics. An Introduction. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<note>Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="12958" citStr="Chang and Lin, 2011" startWordPosition="2070" endWordPosition="2073">ier with the maximum tree depth set to 1. We perform 3-fold cross-validation in order to select the best threshold for discriminating between borrowings and cognates. Using the 433 best threshold selected for each language, we further assign one of the two classes to the pairs of words in our test set. 4.3 Task Setup We experiment with Naive Bayes and Support Vector Machines (SVMs) to learn orthographic changes. We put our system together using the Weka5 workbench (Hall et al., 2009). For SVM, we employ the radial basis function kernel (RBF) and we use the wrapper provided by Weka for LibSVM (Chang and Lin, 2011). For each language pair, we split the dataset in two stratified subsets, for training and testing, with a 3:1 ratio. We experiment with different values for the n-gram size (n E {1, 2, 3}) and we perform grid search and 3-fold cross validation over the training set in order to optimize hyperparameters c and -y for SVM. We search over {1, 2, ...,10} for c and over {10− 4.4 Results Analysis Table 3 and Table 4 show the results of our experiment. The two baselines produce comparable results. For all pairs of languages, our method significantly improves over the baselines (99% confidence level)6 </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Maria Ciobanu</author>
<author>Liviu P Dinu</author>
</authors>
<title>An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>1047--1058</pages>
<contexts>
<context position="5656" citStr="Ciobanu and Dinu, 2014" startWordPosition="872" endWordPosition="875"> represented, to a certain extent, by alphabetic character correspondences (Delmestri and Cristianini, 2010). 3 Our Approach In light of this, we investigate whether cognates can be automatically distinguished from borrowings based on their orthography. More specifically, our task is as follows: given a pair of words in two different languages (x, y), we want to determine whether x and y are cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating between related and unrelated words (Ciobanu and Dinu, 2014b). Briefly, the method comprises the following steps: 1) Aligning the pairs of related words using a string alignment algorithm; 2) Extracting orthographic features from the aligned words; 3) Training a binary classifier to discriminate between the two types of relationship. To align the pairs of related words, we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), which is equivalent to the weighted edit distance algorithm. We consider words as input sequences and we use a very simple substitution matrix1, which assigns 1In our future work, we intend to also </context>
<context position="9139" citStr="Ciobanu and Dinu (2014" startWordPosition="1440" endWordPosition="1443">) castillo - castel (castle) T castellum (LAT) TR-RO t¨ut¨un → tutun (tobacco) aranjman - aranjament (arrangement) T arrangement (FR) Table 1: Examples of borrowings and cognates. For cognates we also report the common ancestor. it in contact not only with the Balkan area, but also with the vast majority of Slavic languages. Political and administrative relationships with the Ottoman Empire, Greece (the Phanariot domination) and the Habsburg Empire exposed Romanian to a wide variety of linguistic influences. We apply our method on four pairs of languages extracted from the dataset proposed by Ciobanu and Dinu (2014c): • Italian - Romanian (IT-RO); • Portuguese - Romanian (PT-RO); • Spanish -Romanian (ES-RO); • Turkish -Romanian (TR-RO). For the first three pairs of languages, which are formed of sister languages3, most cognate pairs have a Latin common ancestor, while for the fourth pair, formed of languages belonging to different families (Romance and Turkic), most of the cognate pairs have a common French etymology, and date back to the end of the 19th century, when both Romanian and Turkish borrowed massively from French. In Table 1 we provide examples of borrowings and cognates. The dataset contains</context>
<context position="19154" citStr="Ciobanu and Dinu, 2014" startWordPosition="3102" endWordPosition="3105">pairs of languages, at least one feature containing the $ character (indicating the beginning or the end of a word) is ranked among the 10 most relevant features, and over 50 are ranked among the 500 most relevant features. This suggests that prefixes and suffixes (usually removed by the stemmer) vary with the type of relationship between the words. Diacritics. We explore whether removing diacritics influences the performance of the system. Many words have undergone transformations by the augmentation of language-specific diacritics 10http://snowball.tartarus.org when entering a new language (Ciobanu and Dinu, 2014a). For this reason, we expect diacritics to play a role in the classification task. We observe that, when diacritics are removed, the F1 score on the test set is lower in almost all situations. Analyzing the ranking of the features extracted from mismatches in the alignment provides even stronger evidence in this direction: for all pairs of languages, more than a fifth of the top 500 features contain diacritics. 5 Conclusions In this paper, we propose a computational method for discriminating between cognates and borrowings based on their orthography. Our results show that it is possible to i</context>
</contexts>
<marker>Ciobanu, Dinu, 2014</marker>
<rawString>Alina Maria Ciobanu and Liviu P. Dinu. 2014a. An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, pages 1047–1058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Maria Ciobanu</author>
<author>Liviu P Dinu</author>
</authors>
<title>Automatic Detection of Cognates Using Orthographic Alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52stAnnual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<pages>99--105</pages>
<contexts>
<context position="5656" citStr="Ciobanu and Dinu, 2014" startWordPosition="872" endWordPosition="875"> represented, to a certain extent, by alphabetic character correspondences (Delmestri and Cristianini, 2010). 3 Our Approach In light of this, we investigate whether cognates can be automatically distinguished from borrowings based on their orthography. More specifically, our task is as follows: given a pair of words in two different languages (x, y), we want to determine whether x and y are cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating between related and unrelated words (Ciobanu and Dinu, 2014b). Briefly, the method comprises the following steps: 1) Aligning the pairs of related words using a string alignment algorithm; 2) Extracting orthographic features from the aligned words; 3) Training a binary classifier to discriminate between the two types of relationship. To align the pairs of related words, we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), which is equivalent to the weighted edit distance algorithm. We consider words as input sequences and we use a very simple substitution matrix1, which assigns 1In our future work, we intend to also </context>
<context position="9139" citStr="Ciobanu and Dinu (2014" startWordPosition="1440" endWordPosition="1443">) castillo - castel (castle) T castellum (LAT) TR-RO t¨ut¨un → tutun (tobacco) aranjman - aranjament (arrangement) T arrangement (FR) Table 1: Examples of borrowings and cognates. For cognates we also report the common ancestor. it in contact not only with the Balkan area, but also with the vast majority of Slavic languages. Political and administrative relationships with the Ottoman Empire, Greece (the Phanariot domination) and the Habsburg Empire exposed Romanian to a wide variety of linguistic influences. We apply our method on four pairs of languages extracted from the dataset proposed by Ciobanu and Dinu (2014c): • Italian - Romanian (IT-RO); • Portuguese - Romanian (PT-RO); • Spanish -Romanian (ES-RO); • Turkish -Romanian (TR-RO). For the first three pairs of languages, which are formed of sister languages3, most cognate pairs have a Latin common ancestor, while for the fourth pair, formed of languages belonging to different families (Romance and Turkic), most of the cognate pairs have a common French etymology, and date back to the end of the 19th century, when both Romanian and Turkish borrowed massively from French. In Table 1 we provide examples of borrowings and cognates. The dataset contains</context>
<context position="19154" citStr="Ciobanu and Dinu, 2014" startWordPosition="3102" endWordPosition="3105">pairs of languages, at least one feature containing the $ character (indicating the beginning or the end of a word) is ranked among the 10 most relevant features, and over 50 are ranked among the 500 most relevant features. This suggests that prefixes and suffixes (usually removed by the stemmer) vary with the type of relationship between the words. Diacritics. We explore whether removing diacritics influences the performance of the system. Many words have undergone transformations by the augmentation of language-specific diacritics 10http://snowball.tartarus.org when entering a new language (Ciobanu and Dinu, 2014a). For this reason, we expect diacritics to play a role in the classification task. We observe that, when diacritics are removed, the F1 score on the test set is lower in almost all situations. Analyzing the ranking of the features extracted from mismatches in the alignment provides even stronger evidence in this direction: for all pairs of languages, more than a fifth of the top 500 features contain diacritics. 5 Conclusions In this paper, we propose a computational method for discriminating between cognates and borrowings based on their orthography. Our results show that it is possible to i</context>
</contexts>
<marker>Ciobanu, Dinu, 2014</marker>
<rawString>Alina Maria Ciobanu and Liviu P. Dinu. 2014b. Automatic Detection of Cognates Using Orthographic Alignment. In Proceedings of the 52stAnnual Meeting of the Association for Computational Linguistics, ACL 2014, pages 99–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Maria Ciobanu</author>
<author>Liviu P Dinu</author>
</authors>
<title>Building a Dataset of Multilingual Cognates for the Romanian Lexicon.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC</booktitle>
<contexts>
<context position="5656" citStr="Ciobanu and Dinu, 2014" startWordPosition="872" endWordPosition="875"> represented, to a certain extent, by alphabetic character correspondences (Delmestri and Cristianini, 2010). 3 Our Approach In light of this, we investigate whether cognates can be automatically distinguished from borrowings based on their orthography. More specifically, our task is as follows: given a pair of words in two different languages (x, y), we want to determine whether x and y are cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating between related and unrelated words (Ciobanu and Dinu, 2014b). Briefly, the method comprises the following steps: 1) Aligning the pairs of related words using a string alignment algorithm; 2) Extracting orthographic features from the aligned words; 3) Training a binary classifier to discriminate between the two types of relationship. To align the pairs of related words, we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), which is equivalent to the weighted edit distance algorithm. We consider words as input sequences and we use a very simple substitution matrix1, which assigns 1In our future work, we intend to also </context>
<context position="9139" citStr="Ciobanu and Dinu (2014" startWordPosition="1440" endWordPosition="1443">) castillo - castel (castle) T castellum (LAT) TR-RO t¨ut¨un → tutun (tobacco) aranjman - aranjament (arrangement) T arrangement (FR) Table 1: Examples of borrowings and cognates. For cognates we also report the common ancestor. it in contact not only with the Balkan area, but also with the vast majority of Slavic languages. Political and administrative relationships with the Ottoman Empire, Greece (the Phanariot domination) and the Habsburg Empire exposed Romanian to a wide variety of linguistic influences. We apply our method on four pairs of languages extracted from the dataset proposed by Ciobanu and Dinu (2014c): • Italian - Romanian (IT-RO); • Portuguese - Romanian (PT-RO); • Spanish -Romanian (ES-RO); • Turkish -Romanian (TR-RO). For the first three pairs of languages, which are formed of sister languages3, most cognate pairs have a Latin common ancestor, while for the fourth pair, formed of languages belonging to different families (Romance and Turkic), most of the cognate pairs have a common French etymology, and date back to the end of the 19th century, when both Romanian and Turkish borrowed massively from French. In Table 1 we provide examples of borrowings and cognates. The dataset contains</context>
<context position="19154" citStr="Ciobanu and Dinu, 2014" startWordPosition="3102" endWordPosition="3105">pairs of languages, at least one feature containing the $ character (indicating the beginning or the end of a word) is ranked among the 10 most relevant features, and over 50 are ranked among the 500 most relevant features. This suggests that prefixes and suffixes (usually removed by the stemmer) vary with the type of relationship between the words. Diacritics. We explore whether removing diacritics influences the performance of the system. Many words have undergone transformations by the augmentation of language-specific diacritics 10http://snowball.tartarus.org when entering a new language (Ciobanu and Dinu, 2014a). For this reason, we expect diacritics to play a role in the classification task. We observe that, when diacritics are removed, the F1 score on the test set is lower in almost all situations. Analyzing the ranking of the features extracted from mismatches in the alignment provides even stronger evidence in this direction: for all pairs of languages, more than a fifth of the top 500 features contain diacritics. 5 Conclusions In this paper, we propose a computational method for discriminating between cognates and borrowings based on their orthography. Our results show that it is possible to i</context>
</contexts>
<marker>Ciobanu, Dinu, 2014</marker>
<rawString>Alina Maria Ciobanu and Liviu P. Dinu. 2014c. Building a Dataset of Multilingual Cognates for the Romanian Lexicon. In Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonella Delmestri</author>
<author>Nello Cristianini</author>
</authors>
<title>String Similarity Measures and PAM-like Matrices for Cognate Identification. Bucharest Working Papers in Linguistics,</title>
<date>2010</date>
<pages>12--2</pages>
<contexts>
<context position="4248" citStr="Delmestri and Cristianini, 2010" startWordPosition="652" endWordPosition="656">anguages changed over time or influenced each other. Most studies in this area focus on automatically identifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinc</context>
</contexts>
<marker>Delmestri, Cristianini, 2010</marker>
<rawString>Antonella Delmestri and Nello Cristianini. 2010. String Similarity Measures and PAM-like Matrices for Cognate Identification. Bucharest Working Papers in Linguistics, 12(2):71–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu´ıs Gomes</author>
<author>Jos´e Gabriel Pereira Lopes</author>
</authors>
<title>Measuring Spelling Similarity for Cognate Identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Portugese Conference on Progress in Artificial Intelligence, EPIA 2011,</booktitle>
<pages>624--633</pages>
<note>Software available at http: //research.variancia.com/spsim.</note>
<contexts>
<context position="11378" citStr="Gomes and Lopes, 2011" startWordPosition="1800" endWordPosition="1803">show how much words vary from one language to another based on their relationship (cognates or borrowings). For IT-RO both distances are small (0.26 and 0.29), as opposed to the other languages, where there is a more significant difference between the two (e.g., 0.26 and 0.52 for ES-RO). The small difference for IT-RO might make the discrimination between the two classes more difficult. 4.2 Baselines Given the initial analysis presented above, we hypothesize that the distance between the words might be indicative of the type of relationship between them. Previous studies (Inkpen et al., 2005; Gomes and Lopes, 2011) show that related and non-related words can be distinguished based on the distance between them, but a finer-grained task, such as determining the type of relationship between the words, is probably more subtle. We compare our method with two baselines: • A baseline which assigns a label based on the normalized edit distance between the words: given a test instance pair words - word2, we subtract the average normalized edit distance between words and word2 from the average normalized edit distance of the cognate pairs and from the average normalized edit distance between the borrowings and th</context>
</contexts>
<marker>Gomes, Lopes, 2011</marker>
<rawString>Lu´ıs Gomes and Jos´e Gabriel Pereira Lopes. 2011. Measuring Spelling Similarity for Cognate Identification. In Proceedings of the 15th Portugese Conference on Progress in Artificial Intelligence, EPIA 2011, pages 624–633. Software available at http: //research.variancia.com/spsim.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotte Gooskens</author>
<author>Wilbert Heeringa</author>
<author>Karin Beijering</author>
</authors>
<date>2008</date>
<booktitle>Phonetic and Lexical Predictors of Intelligibility. International Journal of Humanities and Arts Computing,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="17730" citStr="Gooskens et al., 2008" startWordPosition="2871" endWordPosition="2874">on (P), recall (R), F1 score (F1) and optimal n-gram size for automatic discrimination between cognates and borrowings. For SVM we also report the optimal values for c and -y. Consonants. We examine the performance of our system when trained and tested only on the aligned consonant skeletons of the words (i.e., a version of the words where vowels are discarded). According to Ashby and Maidment (2005), consonants change at a slower pace than vowels across time; while the former are regarded as reference points, the latter are believed to carry less information useful for identifying the words (Gooskens et al., 2008). The performance of the system decreases when vowels are removed (95% confidence level). We also train and test the decision tree classifier on this version of the dataset, and its performance is lower in this case as well (95% confidence level), indicating that, for our task, the information carried by the vowels is helpful. Stems. We repeat the first experiment using stems as input, instead of lemmas. What we seek to understand is whether the aligned affixes are indicative of the type of relationship between the words. We use the Snowball Stemmer10 and we find that the performance decreases</context>
</contexts>
<marker>Gooskens, Heeringa, Beijering, 2008</marker>
<rawString>Charlotte Gooskens, Wilbert Heeringa, and Karin Beijering. 2008. Phonetic and Lexical Predictors of Intelligibility. International Journal of Humanities and Arts Computing, 2(1-2):63–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Finding Cognate Groups Using Phylogenies.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<pages>1030--1039</pages>
<contexts>
<context position="4178" citStr="Hall and Klein, 2010" startWordPosition="642" endWordPosition="645">n et al., 2005) and in identifying how and to what extent languages changed over time or influenced each other. Most studies in this area focus on automatically identifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically</context>
</contexts>
<marker>Hall, Klein, 2010</marker>
<rawString>David Hall and Dan Klein. 2010. Finding Cognate Groups Using Phylogenies. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010, pages 1030–1039.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12826" citStr="Hall et al., 2009" startWordPosition="2046" endWordPosition="2049">rategy proposed by Inkpen et al. (2005): we use the normalized edit distance as single feature, and we fit a decision tree classifier with the maximum tree depth set to 1. We perform 3-fold cross-validation in order to select the best threshold for discriminating between borrowings and cognates. Using the 433 best threshold selected for each language, we further assign one of the two classes to the pairs of words in our test set. 4.3 Task Setup We experiment with Naive Bayes and Support Vector Machines (SVMs) to learn orthographic changes. We put our system together using the Weka5 workbench (Hall et al., 2009). For SVM, we employ the radial basis function kernel (RBF) and we use the wrapper provided by Weka for LibSVM (Chang and Lin, 2011). For each language pair, we split the dataset in two stratified subsets, for training and testing, with a 3:1 ratio. We experiment with different values for the n-gram size (n E {1, 2, 3}) and we perform grid search and 3-fold cross validation over the training set in order to optimize hyperparameters c and -y for SVM. We search over {1, 2, ...,10} for c and over {10− 4.4 Results Analysis Table 3 and Table 4 show the results of our experiment. The two baselines p</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Anderson Hall</author>
</authors>
<title>Linguistics and Your Language.</title>
<date>1960</date>
<publisher>Doubleday</publisher>
<location>New York.</location>
<contexts>
<context position="982" citStr="Hall (1960)" startWordPosition="138" endWordPosition="139">story of a language and allows a better characterization of language relatedness. In this paper, we propose a computational approach for discriminating between cognates and borrowings. We show that orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classification task. To our knowledge, this is the first attempt of this kind. 1 Introduction Natural languages are living eco-systems. They are subject to continuous change due, in part, to the natural phenomena of language contact and borrowing (Campbell, 1998). According to Hall (1960), there is no such thing as a “pure language” – a language “without any borrowing from a foreign language”. Although admittedly regarded as relevant factors in the history of a language (McMahon et al., 2005), borrowings bias the genetic classification of the languages, characterizing them as being closer than they actually are (Minett and Wang, 2003). Thus, the need for discriminating between cognates and borrowings emerges. Heggarty (2012) acknowledges the necessity and difficulty of the task, emphasizing the role of the “computerized approaches”. In this paper we address the task of automat</context>
</contexts>
<marker>Hall, 1960</marker>
<rawString>Robert Anderson Hall. 1960. Linguistics and Your Language. Doubleday New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Heggarty</author>
</authors>
<title>Beyond Lexicostatistics: How to Get More out of ”Word List” Comparisons.</title>
<date>2012</date>
<booktitle>In Quantitative Approaches to Linguistic Diversity: Commemorating the Centenary of the Birth of Morris Swadesh,</booktitle>
<pages>113--137</pages>
<publisher>Benjamins.</publisher>
<contexts>
<context position="1427" citStr="Heggarty (2012)" startWordPosition="209" endWordPosition="210">ing eco-systems. They are subject to continuous change due, in part, to the natural phenomena of language contact and borrowing (Campbell, 1998). According to Hall (1960), there is no such thing as a “pure language” – a language “without any borrowing from a foreign language”. Although admittedly regarded as relevant factors in the history of a language (McMahon et al., 2005), borrowings bias the genetic classification of the languages, characterizing them as being closer than they actually are (Minett and Wang, 2003). Thus, the need for discriminating between cognates and borrowings emerges. Heggarty (2012) acknowledges the necessity and difficulty of the task, emphasizing the role of the “computerized approaches”. In this paper we address the task of automatically distinguishing between borrowings and cognates: given a pair of words, the task is to determine whether one is a historical descendant of the other, or whether they both share a common ancestor. A borrowing (also called loanword), is defined by Campbell (1998) as a “lexical item (a word) which has been ‘borrowed’ from another language, a word which originally was not part of the vocabulary of the recipient language but was adopted fro</context>
</contexts>
<marker>Heggarty, 2012</marker>
<rawString>Paul Heggarty. 2012. Beyond Lexicostatistics: How to Get More out of ”Word List” Comparisons. In Quantitative Approaches to Linguistic Diversity: Commemorating the Centenary of the Birth of Morris Swadesh, pages 113–137. Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Inkpen</author>
<author>Oana Frunza</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Automatic Identification of Cognates and False Friends in French and English.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP</booktitle>
<pages>251--257</pages>
<contexts>
<context position="11354" citStr="Inkpen et al., 2005" startWordPosition="1796" endWordPosition="1799">es. The edit columns show how much words vary from one language to another based on their relationship (cognates or borrowings). For IT-RO both distances are small (0.26 and 0.29), as opposed to the other languages, where there is a more significant difference between the two (e.g., 0.26 and 0.52 for ES-RO). The small difference for IT-RO might make the discrimination between the two classes more difficult. 4.2 Baselines Given the initial analysis presented above, we hypothesize that the distance between the words might be indicative of the type of relationship between them. Previous studies (Inkpen et al., 2005; Gomes and Lopes, 2011) show that related and non-related words can be distinguished based on the distance between them, but a finer-grained task, such as determining the type of relationship between the words, is probably more subtle. We compare our method with two baselines: • A baseline which assigns a label based on the normalized edit distance between the words: given a test instance pair words - word2, we subtract the average normalized edit distance between words and word2 from the average normalized edit distance of the cognate pairs and from the average normalized edit distance betwe</context>
</contexts>
<marker>Inkpen, Frunza, Kondrak, 2005</marker>
<rawString>Diana Inkpen, Oana Frunza, and Grzegorz Kondrak. 2005. Automatic Identification of Cognates and False Friends in French and English. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP 2005, pages 251–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
</authors>
<title>The Significance of Word Lists.</title>
<date>2001</date>
<publisher>CSLI Publications.</publisher>
<location>Stanford:</location>
<contexts>
<context position="2371" citStr="Kessler, 2001" startWordPosition="364" endWordPosition="365">h share a common ancestor. A borrowing (also called loanword), is defined by Campbell (1998) as a “lexical item (a word) which has been ‘borrowed’ from another language, a word which originally was not part of the vocabulary of the recipient language but was adopted from some other language and made part of the borrowing language’s vocabulary”. The notion of cognate is much more relaxed, and various NLP tasks and applications use different definitions of the cognate pairs. In some situations, cognates and borrowings are considered together, and are referred to as historically connected words (Kessler, 2001) or denoted by the term correlates (Heggarty, 2012; McMahon et al., 2005). In some tasks, such as statistical machine translation (Kondrak et al., 2003) and sentence alignment, or when studying the similarity or intelligibility of the languages, cognates are seen as words that have similar spelling and meaning, their etymology being completely disregarded. However, in problems of language classification, distinguishing cognates from borrowings is essential. Here, we account for the etymology of the words, and we adopt the following definition: two words form a cognate pair if they share a comm</context>
</contexts>
<marker>Kessler, 2001</marker>
<rawString>Brett Kessler. 2001. The Significance of Word Lists. Stanford: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Estimating Word Translation Probabilities from Unrelated Monolingual Corpora Using the EM Algorithm.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>711--715</pages>
<contexts>
<context position="4317" citStr="Koehn and Knight, 2000" startWordPosition="664" endWordPosition="667">focus on automatically identifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound </context>
</contexts>
<marker>Koehn, Knight, 2000</marker>
<rawString>Philipp Koehn and Kevin Knight. 2000. Estimating Word Translation Probabilities from Unrelated Monolingual Corpora Using the EM Algorithm. In Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on Innovative Applications of Artificial Intelligence, pages 711–715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP 2004,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="14777" citStr="Koehn, 2004" startWordPosition="2380" endWordPosition="2381">nd our aim is to identify the type of relationship. However, in many situations this kind of a-priori information is not available. In a real scenario, we would have either to add an intermediary classifier for discriminating between related and unrelated words, or to discriminate between three classes: cognates, borrowings, and unrelated. We augment our dataset with unrelated words (determined based on their etymology), building a strat5www.cs.waikato.ac.nz/ml/weka 6All the statistical significance tests reported in this paper are performed on 1,000 iterations of paired bootstrap resampling (Koehn, 2004). Lang. Baseline #1 Baseline #2 P R Fl P R Fl IT-RO 50.7 50.7 50.7 64.4 54.5 45.0 PT-RO 79.3 79.0 79.2 80.1 80.0 80.0 ES-RO 78.6 78.4 78.5 78.6 78.5 78.4 TR-RO 61.1 61.0 61.1 62.5 59.8 57.6 Table 3: Weighted average precision (P), recall (R) and F1 score (F1) for automatic discrimination between cognates and borrowings. ified dataset annotated with three classes, and we repeat the previous experiment. The performance decreases7, but the results are still significantly better than chance (99% confidence level). 4.5 Linguistic Factors To gain insight into the factors with high predictive power, </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP 2004, pages 388– 395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Daniel Marcu</author>
<author>Keven Knight</author>
</authors>
<title>Cognates Can Improve Statistical Translation Models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, HLT-NAACL</booktitle>
<pages>46--48</pages>
<contexts>
<context position="2523" citStr="Kondrak et al., 2003" startWordPosition="386" endWordPosition="390">’ from another language, a word which originally was not part of the vocabulary of the recipient language but was adopted from some other language and made part of the borrowing language’s vocabulary”. The notion of cognate is much more relaxed, and various NLP tasks and applications use different definitions of the cognate pairs. In some situations, cognates and borrowings are considered together, and are referred to as historically connected words (Kessler, 2001) or denoted by the term correlates (Heggarty, 2012; McMahon et al., 2005). In some tasks, such as statistical machine translation (Kondrak et al., 2003) and sentence alignment, or when studying the similarity or intelligibility of the languages, cognates are seen as words that have similar spelling and meaning, their etymology being completely disregarded. However, in problems of language classification, distinguishing cognates from borrowings is essential. Here, we account for the etymology of the words, and we adopt the following definition: two words form a cognate pair if they share a common ancestor and have the same meaning. In other words, they derive directly from the same word, have a similar meaning and, due to various (possibly lan</context>
</contexts>
<marker>Kondrak, Marcu, Knight, 2003</marker>
<rawString>Grzegorz Kondrak, Daniel Marcu, and Keven Knight. 2003. Cognates Can Improve Statistical Translation Models. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, HLT-NAACL 2003, pages 46–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Algorithms For Language Reconstruction.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="3355" citStr="Kondrak, 2002" startWordPosition="520" endWordPosition="521"> problems of language classification, distinguishing cognates from borrowings is essential. Here, we account for the etymology of the words, and we adopt the following definition: two words form a cognate pair if they share a common ancestor and have the same meaning. In other words, they derive directly from the same word, have a similar meaning and, due to various (possibly language-specific) changes across time, their forms might differ. 2 Related Work In a natural way, one of the most investigated problems in historical linguistics is to determine whether similar words are related or not (Kondrak, 2002). Investigating pairs of related words is very useful not only in historical and comparative linguistics, but also in the study of language relatedness (Ng et al., 2010), phylogenetic inference (Atkinson et al., 2005) and in identifying how and to what extent languages changed over time or influenced each other. Most studies in this area focus on automatically identifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics </context>
<context position="4998" citStr="Kondrak, 2002" startWordPosition="765" endWordPosition="767">te production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences in cognates (Kondrak, 2002). In turn, sound correspondences are represented, to a certain extent, by alphabetic character correspondences (Delmestri and Cristianini, 2010). 3 Our Approach In light of this, we investigate whether cognates can be automatically distinguished from borrowings based on their orthography. More specifically, our task is as follows: given a pair of words in two different languages (x, y), we want to determine whether x and y are cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating b</context>
</contexts>
<marker>Kondrak, 2002</marker>
<rawString>Grzegorz Kondrak. 2002. Algorithms For Language Reconstruction. Ph.D. thesis, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>April McMahon</author>
<author>Paul Heggarty</author>
<author>Robert McMahon</author>
<author>Natalia Slaska</author>
</authors>
<title>Swadesh Sublists and the Benefits of Borrowing: an Andean Case Study.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="1190" citStr="McMahon et al., 2005" startWordPosition="172" endWordPosition="175">orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classification task. To our knowledge, this is the first attempt of this kind. 1 Introduction Natural languages are living eco-systems. They are subject to continuous change due, in part, to the natural phenomena of language contact and borrowing (Campbell, 1998). According to Hall (1960), there is no such thing as a “pure language” – a language “without any borrowing from a foreign language”. Although admittedly regarded as relevant factors in the history of a language (McMahon et al., 2005), borrowings bias the genetic classification of the languages, characterizing them as being closer than they actually are (Minett and Wang, 2003). Thus, the need for discriminating between cognates and borrowings emerges. Heggarty (2012) acknowledges the necessity and difficulty of the task, emphasizing the role of the “computerized approaches”. In this paper we address the task of automatically distinguishing between borrowings and cognates: given a pair of words, the task is to determine whether one is a historical descendant of the other, or whether they both share a common ancestor. A borr</context>
<context position="2444" citStr="McMahon et al., 2005" startWordPosition="374" endWordPosition="377">defined by Campbell (1998) as a “lexical item (a word) which has been ‘borrowed’ from another language, a word which originally was not part of the vocabulary of the recipient language but was adopted from some other language and made part of the borrowing language’s vocabulary”. The notion of cognate is much more relaxed, and various NLP tasks and applications use different definitions of the cognate pairs. In some situations, cognates and borrowings are considered together, and are referred to as historically connected words (Kessler, 2001) or denoted by the term correlates (Heggarty, 2012; McMahon et al., 2005). In some tasks, such as statistical machine translation (Kondrak et al., 2003) and sentence alignment, or when studying the similarity or intelligibility of the languages, cognates are seen as words that have similar spelling and meaning, their etymology being completely disregarded. However, in problems of language classification, distinguishing cognates from borrowings is essential. Here, we account for the etymology of the words, and we adopt the following definition: two words form a cognate pair if they share a common ancestor and have the same meaning. In other words, they derive direct</context>
</contexts>
<marker>McMahon, Heggarty, McMahon, Slaska, 2005</marker>
<rawString>April McMahon, Paul Heggarty, Robert McMahon, and Natalia Slaska. 2005. Swadesh Sublists and the Benefits of Borrowing: an Andean Case Study. Transactions of the Philological Society, 103(2):147–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Minett</author>
<author>William S-Y Wang</author>
</authors>
<title>On Detecting Borrowing: Distance-based and Character-based Approaches.</title>
<date>2003</date>
<journal>Diachronica,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1335" citStr="Minett and Wang, 2003" startWordPosition="195" endWordPosition="198"> To our knowledge, this is the first attempt of this kind. 1 Introduction Natural languages are living eco-systems. They are subject to continuous change due, in part, to the natural phenomena of language contact and borrowing (Campbell, 1998). According to Hall (1960), there is no such thing as a “pure language” – a language “without any borrowing from a foreign language”. Although admittedly regarded as relevant factors in the history of a language (McMahon et al., 2005), borrowings bias the genetic classification of the languages, characterizing them as being closer than they actually are (Minett and Wang, 2003). Thus, the need for discriminating between cognates and borrowings emerges. Heggarty (2012) acknowledges the necessity and difficulty of the task, emphasizing the role of the “computerized approaches”. In this paper we address the task of automatically distinguishing between borrowings and cognates: given a pair of words, the task is to determine whether one is a historical descendant of the other, or whether they both share a common ancestor. A borrowing (also called loanword), is defined by Campbell (1998) as a “lexical item (a word) which has been ‘borrowed’ from another language, a word w</context>
<context position="4460" citStr="Minett and Wang (2003)" startWordPosition="687" endWordPosition="690">milarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences in cognates (Kondrak, 2002). In turn, sound correspondences are represented, to a certain</context>
</contexts>
<marker>Minett, Wang, 2003</marker>
<rawString>James W. Minett and William S.-Y. Wang. 2003. On Detecting Borrowing: Distance-based and Character-based Approaches. Diachronica, 20(2):289–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Minett</author>
<author>William S-Y Wang</author>
</authors>
<title>Vertical and Horizontal Transmission in Language Evolution.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="4640" citStr="Minett and Wang (2005)" startWordPosition="714" endWordPosition="717">Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences in cognates (Kondrak, 2002). In turn, sound correspondences are represented, to a certain extent, by alphabetic character correspondences (Delmestri and Cristianini, 2010). 3 Our Approach In light of this, we investigate whether cognates can be automatically distinguis</context>
</contexts>
<marker>Minett, Wang, 2005</marker>
<rawString>James W. Minett and William S.-Y. Wang. 2005. Vertical and Horizontal Transmission in Language Evolution. Transactions of the Philological Society, 103(2):121–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Mulloni</author>
<author>Viktor Pekar</author>
</authors>
<title>Automatic Detection of Orthographic Cues for Cognate Recognition. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation, LREC</booktitle>
<pages>2387--2390</pages>
<contexts>
<context position="4342" citStr="Mulloni and Pekar, 2006" startWordPosition="668" endWordPosition="671">dentifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate reg</context>
</contexts>
<marker>Mulloni, Pekar, 2006</marker>
<rawString>Andrea Mulloni and Viktor Pekar. 2006. Automatic Detection of Orthographic Cues for Cognate Recognition. In In Proceedings of the 5th International Conference on Language Resources and Evaluation, LREC 2006, pages 2387–2390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Mulloni</author>
</authors>
<title>Automatic Prediction of Cognate Orthography Using Support Vector Machines.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL: Student Research Workshop, ACL</booktitle>
<pages>25--30</pages>
<contexts>
<context position="4436" citStr="Mulloni, 2007" startWordPosition="684" endWordPosition="686">dates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences in cognates (Kondrak, 2002). In turn, sound correspondences are r</context>
</contexts>
<marker>Mulloni, 2007</marker>
<rawString>Andrea Mulloni. 2007. Automatic Prediction of Cognate Orthography Using Support Vector Machines. In Proceedings of the 45th Annual Meeting of the ACL: Student Research Workshop, ACL 2007, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirabela Navlea</author>
<author>Amalia Todirascu</author>
</authors>
<title>Using Cognates in a French-Romanian Lexical Alignment System: A Comparative Study.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP</booktitle>
<pages>247--253</pages>
<contexts>
<context position="4371" citStr="Navlea and Todirascu, 2011" startWordPosition="672" endWordPosition="675">tes. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences </context>
</contexts>
<marker>Navlea, Todirascu, 2011</marker>
<rawString>Mirabela Navlea and Amalia Todirascu. 2011. Using Cognates in a French-Romanian Lexical Alignment System: A Comparative Study. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP 2011, pages 247–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul B Needleman</author>
<author>Christian D Wunsch</author>
</authors>
<title>A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins.</title>
<date>1970</date>
<journal>Journal of Molecular Biology,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>453</pages>
<contexts>
<context position="6056" citStr="Needleman and Wunsch, 1970" startWordPosition="932" endWordPosition="935">cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating between related and unrelated words (Ciobanu and Dinu, 2014b). Briefly, the method comprises the following steps: 1) Aligning the pairs of related words using a string alignment algorithm; 2) Extracting orthographic features from the aligned words; 3) Training a binary classifier to discriminate between the two types of relationship. To align the pairs of related words, we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), which is equivalent to the weighted edit distance algorithm. We consider words as input sequences and we use a very simple substitution matrix1, which assigns 1In our future work, we intend to also experiment with more informed language-specific substitution matrices. Lang. Cognates Borrowings len1 len2 edit len1 len2 edit IT-RO 7.95 8.78 0.26 7.58 8.41 0.29 ES-RO 7.91 8.33 0.26 5.78 6.14 0.52 PT-RO 7.99 8.35 0.28 5.35 5.42 0.52 TR-RO 7.35 6.88 0.31 6.49 6.09 0.44 Table 2: Statistics for the dataset of related words. Given a pair of languages (L1, L2), the len1 and len2 columns represent the</context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>Saul B. Needleman and Christian D. Wunsch. 1970. A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins. Journal of Molecular Biology, 48(3):443 – 453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ee-Lee Ng</author>
<author>Beatrice Chin</author>
<author>Alvin W Yeo</author>
<author>Bali Ranaivo-Malanc¸on</author>
</authors>
<title>Identification of CloselyRelated Indigenous Languages: An Orthographic Approach.</title>
<date>2010</date>
<journal>International Journal ofAsian Language Processing,</journal>
<volume>20</volume>
<issue>2</issue>
<marker>Ng, Chin, Yeo, Ranaivo-Malanc¸on, 2010</marker>
<rawString>Ee-Lee Ng, Beatrice Chin, Alvin W. Yeo, and Bali Ranaivo-Malanc¸on. 2010. Identification of CloselyRelated Indigenous Languages: An Orthographic Approach. International Journal ofAsian Language Processing, 20(2):43–62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>