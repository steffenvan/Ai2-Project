<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995891">
Example-based machine translation using DP-matching between
word sequences
</title>
<author confidence="0.822658">
Eiichiro Sumita
</author>
<affiliation confidence="0.531203">
ATR Spoken Language Translation Research Laboratories
</affiliation>
<address confidence="0.928368666666667">
2-2 Hikaridai, Seika, Soraku,
Kyoto 619-0288,
Japan
</address>
<email confidence="0.998241">
sumita@slt.atr.co.jp
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999618833333333">
We propose a new approach under the
example-based machine translation
paradigm. First, the proposed approach
retrieves the most similar example by
carrying out DP-matching of the input
sentence and example sentences while
measuring the semantic distance of the
words. Second, the approach adjusts
the gap between the input and the most
similar example by using a bilingual
dictionary. We show the results of a
computational experiment.
</bodyText>
<sectionHeader confidence="0.998842" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9806375">
Knowledge acquisition from corpora is viable for
machine translation. The background is as follows:
</bodyText>
<listItem confidence="0.998829111111111">
• Demands have been increasing for machine
translation systems to handle a wider range of
languages and domains.
• MT requires bulk knowledge consisting of rules
and dictionaries.
• Building knowledge consumes considerable
time and money.
• Bilingual/multilingual translations have become
widely available.
</listItem>
<bodyText confidence="0.713752">
There are two approaches in corpus-based
translation:
</bodyText>
<listItem confidence="0.9926843125">
1. Statistical Machine Translation (SMT): SMT
learns models for translation from corpora and
dictionaries and searches for the best translation
according to the models in run-time (Brown et
al., 1990; Knight, 1997; Ney et al., 2000).
2. Example-Based Machine Translation (EBMT):
EBMT uses the corpus directly. EBMT retrieves
the translation examples that are best matched
to an input expression and adjusts the examples
to obtain the translation (Nagao, 1981; Sadler
1989; Sato and Nagao, 1990; Sumita and Iida,
1991; Kitano, 1993; Furuse et al., 1994;
Watanabe and Maruyama, 1994; Cranias et al.,
1994; Jones, 1996; Veale and Way, 1997; Carl,
1999, Andriamanankasina et al., 1999; Brown,
2000).
</listItem>
<figure confidence="0.958441666666667">
(1) Sentence-
Aligned
Bilingual
Corpus
Input
sentence
(2) Bilingual
Dictionary
Retrieval + Adjustment
(3) Thesauri
Target
sentence
</figure>
<figureCaption confidence="0.999823">
Figure 1 Configuration
</figureCaption>
<bodyText confidence="0.9972622">
This paper pursues EBMT and proposes a
new approach by using the distance between
word sequences. The following sections show
the algorithm, experimental results, and
implications and prospects.
</bodyText>
<sectionHeader confidence="0.8786915" genericHeader="introduction">
2 The proposed method
2.1 Configuration
</sectionHeader>
<bodyText confidence="0.999893714285714">
As shown in Figure 1, our resources are (1) a
bilingual corpus, in which sentences are aligned
beforehand; (2) a bilingual dictionary, which is used
for word alignment and translation; and (3) thesauri
of both languages, which are used for aiding word
alignment and incorporating the semantic distance
between words into the word sequence distance.
</bodyText>
<subsectionHeader confidence="0.979153">
2.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.995074">
The translation process consists of four steps:1
</bodyText>
<listItem confidence="0.6984705">
I. Retrieve the most similar translation pair;
II. Generate translation patterns;
III. Select the best translation pattern;
IV. Substitute target words for source words.
</listItem>
<bodyText confidence="0.999511">
Here, we illustrate the algorithm using translation
from Japanese to English step by step.
</bodyText>
<subsectionHeader confidence="0.916041">
2.2.1 Retrieval - Step I
</subsectionHeader>
<bodyText confidence="0.999915384615385">
This step scans the source parts of all example
sentences in the bilingual corpus. By measuring the
distance (dist shown below) between the word
sequences of the input and example sentences, it
retrieves the examples with the minimum distance,
provided the distance is smaller than the given
threshold. Otherwise, the whole translation fails with
no output.
According to equation (1), dist is calculated as
follows: The counts of the Insertion (I), Deletion (D),
and Substitution (S) operations are summed up and
the total is normalized by the sum of the length of the
source and example sequences.
</bodyText>
<footnote confidence="0.3954135">
1 Step I corresponds to Retrieval in Figure 1 and steps II,
III, and IV correspond to Adjustment.
</footnote>
<bodyText confidence="0.99726">
Substitution (S) considers the semantic distance
between two substituted words and is called
SEMDIST. SEMDIST is defined as the division of K
(the level of the least common abstraction in the
thesaurus of two words) by N (the height of the
thesaurus) according to equation (2) (Sumita and Iida,
1991). It ranges from 0 to 1.
Let’s observe the following two sentences,2 (1-j)
the input and (2-j) the source sentence of the
translation example, where the hatched parts
represent the differences between the two sentences.
</bodyText>
<equation confidence="0.966482">
(1-j) iro/ga/ki/ni/iri/masen
[color/SUB/favor/OBJ/enter/POLITE-NOT]
{I do not care for the color.}
(2-j) dezain/ga/ki/ni/iri/masen
[design/SUB/favor/OBJ/enter/ POLITE-NOT]
{I do not care for the design.}
</equation>
<bodyText confidence="0.999828888888889">
Because “iro” and “dezain” are completely dissimilar
in the thesauri used in the experiment, SEMDIST is 1,
and therefore, the dist between them is (0+0+2*1) /
(6+6) = 0.167. The dist is calculated efficiently by a
standard dynamic programming technique (Cormen
1989).
This step is an application of the so-called DP-
matching, which is often used in speech recognition
research.
</bodyText>
<subsubsectionHeader confidence="0.594684">
2.2.2 Pattern Generation - Step II
</subsubsectionHeader>
<bodyText confidence="0.855547166666667">
First, the step stores the hatched parts of the input
sentence in memory for the following translation.
Second, the step aligns the hatched parts of source
sentence (2-j) to corresponding target sentence (2-e)
of the translation example by using lexical resources.
3 We do not align non-hatched parts word by word.
We assume that non-hatched parts correspond
together as a whole. This keeps most parts of the
example unchanged in order to avoid mixing errors
or unnaturalness in the translation.
(2-j) dezain/ga/ki/ni/iri/masen
(2-e) I do not like the design.
</bodyText>
<footnote confidence="0.819977363636364">
2 A Japanese sentence has no word boundary marker such
as the blank character in English so we put « / » between
Japanese words. The brackets show the English literal
translation word by word and the braces show the sentence
translation in English.
3 We do not consider on the alignment mechanism in this
proposal. We have a free hand in selecting an appropriate
alignment method out of a spectrum (Manning and Hinrich,
1999) ranging from statistical to lexical types. In the
experiment, we rely on a bilingual dictionary and thesauri
in both languages.
</footnote>
<equation confidence="0.985076615384615">
F-1 I ❑D ❑21
Linput ❑Lexample
SEMDIST
SEMDIST
(
2
)
(
1
)
dist
K
N
</equation>
<bodyText confidence="0.9997145">
We obtain the following translation pattern,
where the variable X is used to connect source
(2-j-p) and target (2-e-p) and store instance (1-j-
b) in the input sentence.
</bodyText>
<equation confidence="0.973506">
(2-j-p) X/ga/ki/ni/iri/masen
(2-e-p) I do not like the X
(1-j-b) X = “iro”
</equation>
<subsubsectionHeader confidence="0.818907">
2.2.3 Pattern Selection - Step III
</subsubsectionHeader>
<bodyText confidence="0.9982772">
We may retrieve more than one example, and,
moreover, translation patterns can differ. We have to
select the most suitable one from among these
translation patterns. We use a heuristic rule for this
purpose.
</bodyText>
<listItem confidence="0.998948428571429">
1. Maximize the frequency of the translation
pattern.
2. If this cannot be determined, maximize the sum
of the frequency of words in the generated
translation patterns.
3. If this cannot be determined, select one
randomly as a last resort.
</listItem>
<subsubsectionHeader confidence="0.653572">
2.2.4 Word Substitution - Step IV
</subsubsectionHeader>
<bodyText confidence="0.9997032">
This step is straightforward. By translating the source
word of the variable using the bilingual dictionary,
and instantiating the variable within the target part of
the selected translation pattern by target word (1-e-b),
we finally get target sentence (1-e).
</bodyText>
<sectionHeader confidence="0.998221" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.9997194">
To see whether this rough approach works or not, we
conducted a computational experiment using a large-
scale bilingual corpus. In this section, we show the
experimental conditions, performance, and error
analysis.
</bodyText>
<tableCaption confidence="0.628973">
Table 1 Corpus Statistics
</tableCaption>
<sectionHeader confidence="0.49187" genericHeader="method">
Sentences 204,108
</sectionHeader>
<subsectionHeader confidence="0.992347">
Bilingual Corpus
</subsectionHeader>
<bodyText confidence="0.999993888888889">
We built a collection of Japanese sentences and their
English translations, which are usually found in
phrasebooks for foreign tourists. Because the
translations were made sentence by sentence, the
corpus was sentence-aligned by birth. We lemmatized
and POS-tagged both the Japanese and English
sentences using our morphological analysis programs.
The total sentence count was about 200 K. 4 The
statistics are summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.954032">
Test set
</subsectionHeader>
<bodyText confidence="0.99995975">
A quality evaluation was done for 500 sentences
selected randomly from the above-mentioned corpus
and the remaining sentences were used as translation
examples for the experiment.
</bodyText>
<subsectionHeader confidence="0.937536">
Bilingual Dictionary
</subsectionHeader>
<bodyText confidence="0.999859666666667">
We also used a bilingual dictionary previously
developed for another MT system in the travel
domain (Sumita et al.1999).
</bodyText>
<subsectionHeader confidence="0.7558">
Thesaurus
</subsectionHeader>
<bodyText confidence="0.999893666666667">
We used thesauri whose hierarchies are based on the
Kadokawa Ruigo-shin-jiten (Ohno 1984) for distance
calculation and word alignment.
</bodyText>
<subsectionHeader confidence="0.946694">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999972666666667">
Here, we show coverage and accuracy results as
evidence that our proposed machine translation
system works.
</bodyText>
<subsectionHeader confidence="0.904039">
3.2.1 Coverage
</subsectionHeader>
<bodyText confidence="0.999981">
Our approach does not produce any translation when
there is no example whose dist is within the given
threshold, which was 1/3 in the experiment.
</bodyText>
<tableCaption confidence="0.957089">
Table 2 Coverage and Sentence Length
</tableCaption>
<table confidence="0.9946666">
% Average length
Exactly 46.4 5.6
Approximately 42.8 7.7
No Output 10.8 11.0
Total 100.0 7.0
</table>
<equation confidence="0.978631125">
(1-e-b) X = “color”
(1-e) I do not like the color.
(J) 8.3
(E) 6.1
(J) 1,689,449
(E) 1,235,747
(J) 19,640
(E) 15,374
</equation>
<bodyText confidence="0.99829225">
Our approach covers about 90% of 500
randomly selected sentences. As shown in Table 2,
one half of 90% is matched exactly and the other half
is matched approximately (dist &lt; 1/3).
</bodyText>
<figure confidence="0.551309333333333">
Sentence Length
Words
Vocabulary
</figure>
<bodyText confidence="0.91941125">
4 We call a sequence of sentences uttered by a single
speaker an utterance. Our corpus is in fact aligned utterance
by utterance. Strictly speaking, ‘sentence’ in this paper
should be replaced by ‘utterance.’
</bodyText>
<subsectionHeader confidence="0.882955">
3.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999978066666667">
The characteristics of no output sentences
are clearly explained by the average length. Our
approach is not good with longer sentences because
our algorithm has no explicit step of decomposing an
input sentence into sub-sentences and because the
longer the sentence, the smaller the possibility that
there exists a similar sentence in the example
database.
We assume that a coverage of 90% is
important because this means that if 200 K sentences
were input into the system, the system would produce
a translation 90% of the time. In other words, the
system would help the user 90% of the time to
communicate with foreign people (assuming the user
to be in a foreign country).
</bodyText>
<sectionHeader confidence="0.723854" genericHeader="method">
3.2.2 Accuracy
</sectionHeader>
<subsectionHeader confidence="0.992077">
Quality Ranking
</subsectionHeader>
<bodyText confidence="0.9948">
Each translation is graded into one of four ranks5
(described below) by a bilingual human translator
who is a native speaker of the target language,
American English:
</bodyText>
<listItem confidence="0.992636333333333">
(A) Perfect: no problems in either information or
grammar; (B) Fair: easy-to-understand with some
unimportant information missing or flawed grammar;
(C) Acceptable: broken but understandable with
effort; (D) Nonsense: important information has been
translated incorrectly.
</listItem>
<tableCaption confidence="0.963369">
Table 3 Translation Accuracy
</tableCaption>
<figure confidence="0.863573833333333">
Rank %
Good A 41.4
B 25.2
C 11.8
Bad D 10.8
F(No output) 10.8
</figure>
<sectionHeader confidence="0.78153" genericHeader="method">
Result
</sectionHeader>
<bodyText confidence="0.99960225">
As shown in Table 3, our proposal achieved a high
accuracy of about 80% (A, B, C ranks in total). The
remaining 20% are divided into ranks D and F (No
output).
</bodyText>
<subsectionHeader confidence="0.888386">
Long Sentence Problem
</subsectionHeader>
<bodyText confidence="0.9915155">
Figure 2 shows6 that the accuracy clearly decreases
as the dist increases. This implies two points: (1)
dist can indicate the quality of the produced
translation, in contrast with the fact that MT systems
usually do not provide any confidence factor on their
results. The user is safe if he/she confines
himself/herself to using translations with a small dist
value; (2) The current algorithm has a problem in
handling distant examples, which usually relate to the
long sentence problem.
</bodyText>
<sectionHeader confidence="0.3819" genericHeader="method">
3.2.3 Error Analysis
</sectionHeader>
<bodyText confidence="0.9999098">
As shown in the previous two subsections, the most
dominant problem is in dealing with relatively longer
sentences. We point out here that even for shorter
sentences there are problems, although they are less
frequent, as follows:
</bodyText>
<listItem confidence="0.774198">
• Idioms or collocations
</listItem>
<bodyText confidence="0.998749857142857">
Even when the dist between the two sentences is
small, i.e., they are quite similar in the source
language, the meanings of the sentences can vary and
the translation can be different in the target language.
This case is not so frequent, but is possible by idioms
or collocations as exemplified in the following
sample.
</bodyText>
<figure confidence="0.998782666666667">
EXACT A
0.1&gt; d &gt;=0 B
0.2&gt; d &gt;=0.1 C
0.3&gt; d &gt;=0.2 D
1/3&gt; d &gt;=0.3 F
0 50 100 150 200 250
</figure>
<figureCaption confidence="0.985264">
Figure 2 Accuracy by Distance
</figureCaption>
<bodyText confidence="0.5594">
5 This ranking was developed for evaluation in spoken
language translation. For more details, see (Sumita et al.,
1999).
</bodyText>
<page confidence="0.906586">
6 The horizontal axis indicates the number of sentences.
</page>
<figure confidence="0.994481875">
Proposed MT
A commercial MT
A
B
C
D
F
0% 20% 40% 60% 80% 100%
</figure>
<figureCaption confidence="0.908729">
Figure 3 Comparison of Proposed EBMT and a Commercial MT
</figureCaption>
<figure confidence="0.699048285714286">
1. kata/o/tsume/teitadake/masu/ka
[shoulders/OBJ/shorten-or-sit-
closely/REQUEST/POLITE/QUESTION]
{Could you tighten the shoulders up?}
2. seki/o/tsume/teitadake/masu/ka
[seat/OBJ/shorten-or-close-
up/REQUEST/POLITE/QUESTION]
</figure>
<figureCaption confidence="0.388489">
{Could you move over a little?}
</figureCaption>
<bodyText confidence="0.9994368">
The replaceability between “kata” and “seki” does
not hold for these two similar sentences. To avoid
this problem, a feedback mechanism of erroneous
translations built into the system is one possible
solution.
</bodyText>
<listItem confidence="0.957561">
• Noise in data
</listItem>
<bodyText confidence="0.999766583333333">
The proposed approach accepts the translation
example blindly. If the translation is wrong or
inappropriate, the output is directly made defective.
The next two translations show contextual
inappropriateness. The source parts of the two
examples are exactly the same, but the target part of
the first example is neutral and that of the second
example is specific, i.e., valid only in a special
situation. Preventing this requires cleaning the
example database, preferably by machine, or
collecting sufficiently large-scale data to suppress the
influence of noisy examples.
</bodyText>
<listItem confidence="0.99281275">
1. hai/ari/masu = Yes, we do.
[yes/exist/POLITE]
2. hai/ari/masu = Yes, we have a shuttle bus.
[yes/exist/POLITE]
</listItem>
<sectionHeader confidence="0.998196" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999951">
Here, we explain the implications of the experimental
results and discuss the future extension.
</bodyText>
<subsectionHeader confidence="0.981448">
4.1 Limitations
</subsectionHeader>
<bodyText confidence="0.999887">
Our proposal has the limitations listed below, but we
would like to note that we have obtained high
coverage and accuracy for the phrasebook task.
</bodyText>
<listItem confidence="0.834230125">
• Database limitation: If a nearest neighbor
within the threshold does not exist in the
example database, we cannot perform
translation. One positive note is that we were
able to build the necessary example database for
the phrasebook task, which is not a toy.
• Context limitation: We cannot translate context-
dependent words, because contexts are often
hard to embed in an example database. For
example, Japanese ‘konnichiwa’ corresponds to
‘Good morning’ or ‘Good afternoon’ in English
depending on the time of utterance. It is in
general difficult to embed such kinds of
situational information into the example
database.
• Implementation limitation: We have no method
</listItem>
<bodyText confidence="0.635955833333333">
for dividing an input into chunks (such as
clauses) at present, so long sentences cannot be
dealt with. In addition, no investigation has
been made on robustness with respect to
recognition errors yet. However, DP-matching
is expected to be effective.
</bodyText>
<subsectionHeader confidence="0.995374">
4.2 Generality vs. Quality
</subsectionHeader>
<bodyText confidence="0.99975625">
There is no commercial system that can translate
phrasebook sentences at this level of accuracy.
Figure 3 shows a comparison of our proposal and a
commercial machine translation system that accepts
travel conversations. Table 4 shows sample
translations by the above two systems. The upper
translation was produced by our proposed system and
the lower translation was produced by the
commercial system.
The reason behind the performance difference
for this task is that the commercial one was built as a
general-purpose system and phrasebook sentences
are not easy to translate into high-quality results by
using such a general-purpose architecture.
However, we must admit that general-purpose
architectures are effective and we do not mean to
</bodyText>
<tableCaption confidence="0.977027">
Table 4 Sample translations by two MTs
</tableCaption>
<table confidence="0.981447588235294">
pan/o/motto/itadake/masu/ka Could I have some more bread?
[bread/OBJ/more/get/POLITE/QUESTION]
Is it possible to have bread more?
suteeki/no/yaki/kagen/wa How would like your steak?
[steak/of/grill/degree/TOPIC]
The roasting addition and subtraction of the steak.
Sentaku/ki/no/tsukai/kata/o/oshie/tekure/masen/ka Will you show me how to use the washing machine?
[washing/machine/of/use/way/OBJ/teach/REQUEST/PO
LITE/NEGATION/QUESTION]
Don’t I let me know a way of using a washing machine.
eigo/wa/tokui/dehaari/masen I’m not good at English.
[English/TOPIC/strong/be/ POLITE/NEGATION]
English isn’t good.
korera/no/en/wo/doru/ni/ryougae/shi/tai/n/desu/ga I’d like to change yen into dollars.
[these/of/yen/OBJ/dollar/IND-
OBJ/exchange/do/want/PARTICLE/be/but]
But made to want to change these yen into dollars.
</table>
<bodyText confidence="0.9943734">
criticize them by using this comparison. It is
reasonable to suggest that general-purpose
architectures are not the most suitable option for
achieving high-quality translations in restricted
domains.
</bodyText>
<subsectionHeader confidence="0.9994835">
4.3 Development Cost and Its Reduction in
the Future
</subsectionHeader>
<bodyText confidence="0.998797333333333">
We do not need grammars and transfer rules but we
do need a bilingual corpus and lexical resources.
The adaptability of our approach to
multilingual translation is promising. Because we
have already succeeded in J-to-E, one of the most
difficult translation pairs, we have little concern
about other pairs. If we can create an n lingual corpus,
we can make n(n-1) MT systems.
To enable such a dream within a shorter
timeframe, we have to reduce the necessary resources
such as bilingual dictionaries and thesauri by
automating the construction of lexical knowledge.
We are aiming at such additional cost
reduction.
We also want to eliminate restrictions, e.g.,
sentence-aligned and morphologically tagged
example database. By doing so, the applicability of
our approach can be increased. This is another
important challenge.
A further challenging goal is to establish
technology enabling the use of a small-scale corpus.
</bodyText>
<subsectionHeader confidence="0.982563">
4.4 Related Research
</subsectionHeader>
<bodyText confidence="0.99994">
Here, we would like to compare our proposal and
related research in four points: level of knowledge,
application of dynamic programming, the use of
thesauri, and the task.
</bodyText>
<subsectionHeader confidence="0.953021">
Knowledge of EBMT
</subsectionHeader>
<bodyText confidence="0.9999698">
Many EBMT studies (Sato and Nagao, 1990; Sato,
1991; Furuse et al., 1994; Sadler, 1989) assume the
existence of a bank of aligned bilingual trees or a set
of translation patterns. However, building such
knowledge is done by humans and is very expensive.
Methods for automating knowledge building are still
being developed. In contrast, our proposal does not
rely on such a high-level analysis of the corpus and
requires only word-level knowledge, i.e.,
morphological tags and dictionaries.
</bodyText>
<subsectionHeader confidence="0.928611">
Dynamic programming
</subsectionHeader>
<bodyText confidence="0.999937666666667">
Dynamic programming has been used within the
EBMT paradigm (1) for technical term translation
(Sato, 1993), and (2) for translation support (Cranias
et al., 1994).
Sato translates technical terms, which are
usually compound nouns, while we translate
sentences. He uses a corpus in which translation units
of a pair of technical terms are aligned, while we do
not require the alignment of translation units. He
defines the matching score and we define the
distance between word sequences, which are
different. However, both are computed by a standard
dynamic programming technique.
Based on surface structures and content
words, Cranias defined a similarity score between
texts and introduced the idea of clustering the
translation memory to speed up the retrieval of
similar translation examples. The score is again
computed by a standard dynamic programming
technique, but Cranias provides not a translation but
only a retrieval.
</bodyText>
<subsectionHeader confidence="0.493035">
Thesaurus
</subsectionHeader>
<bodyText confidence="0.999980375">
Brown (2000) uses equivalence classes to
successfully improve the coverage of EBMT. He
proposed a method of automatically generating
equivalence classes using clustering techniques,
while we use hand-coded thesauri (in the experiment).
Such automation is very attractive, and the author is
planning to follow in Brown’s line, in spite of a fear
that low frequent words will not be dealt with
effectively by clustering techniques. Brown uses a
hard condition, i.e., whether a word is included in an
equivalence class or not, while we provide the
relative distance between two words. It is unknown
which method is better for EBMT. We do not plan on
sticking with the current implementation using hand-
coded thesauri, as we realize that further research on
these open problems is indispensable.
</bodyText>
<subsectionHeader confidence="0.869078">
Phrasebook task
</subsectionHeader>
<bodyText confidence="0.999905230769231">
The phrasebook task was first advocated for the task
of speech translation by (Stentiford and Steer, 1987).
They pointed out that when communicating within a
limited domain such as international telephone
communications, it is nearly possible to specify all of
the required message concepts. They used a keyword-
based approach to access concepts to overcome
speech recognition errors. On the other hand, we use
DP-matching techniques for this end.7 The scalability
of the keyword-based approach has raised questions
because enlarging a corpus directly increases the
chances of conflict in identifying the concepts to be
conveyed.
</bodyText>
<sectionHeader confidence="0.988905" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999965090909091">
We proposed a new approach using DP-matching for
retrieving examples within EBMT and demonstrated
its coverage and accuracy through a computational
experiment for a restricted domain, i.e., a phrasebook
task for foreign tourists.
There is much room for our translation
method to improve: (1) decomposing input sentences
will improve the coverage, and (2) indexing or
clustering the example database will drastically
improve the efficiency of the current naïve
implementation.
</bodyText>
<sectionHeader confidence="0.969686" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99886675">
First, the author thanks anonymous reviewers for
their useful comments. The author’s heartfelt thanks
go to Kadokawa-Shoten for providing the Ruigo-
Shin-Jiten. Thanks also go to Dr. Seiichi
</bodyText>
<footnote confidence="0.575335">
7 We believe our approach is robust against speech
recognition errors, but we have not yet applied it to speech
recognition results.
</footnote>
<bodyText confidence="0.986650666666667">
YAMAMOTO, President and Mr. Satoshi SHIRAI,
Department Head, for providing the author with the
chance to pursue this research.
</bodyText>
<sectionHeader confidence="0.990096" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915864197531">
Andriamanankasina, T., Araki, K. and Tochinai, T.
1999. Example-Based Machine Translation of Part-
Of-Speech Tagged Sentences by Recursive Division.
Proceedings of MT SUMMIT VII. Singapore.
Brown, P. F., Cocke, J., Della Pietra, S. A., Della
Pietra, V. J., Jelinek, F., Lafferty, J., Mercer, R. L.,
and Roossin, P. S. 1990. A Statistical Approach to
Machine Translation. Computational Linguistics
16(2).
Brown, R. D. 2000. Automated Generalization of
Translation Examples. In Proceedings of the
Eighteenth International Conference on
Computational Linguistics (COLING-2000), pp. 125-
131. Saarbrücken, Germany, August 2000.
Carl, M. 1999. Inducing Translation Templates for
Example-Based Machine Translation, Proc. Of MT-
Summit VII.
Cormen, H. T., Leiserson, C. E. and Rivest, L. R.
1989. Introduction to Algorithms, MIT Press, p. 1028.
Cranias, L., Papageorgiou, H. and Piperidis, S. 1994.
A Matching Technique in Example-Based Machine
Translation. Institute for Language and Speech
Processing, Greece. Paper presented to Computation
and Language.
Furuse, O., Sumita, E. and Iida, H. 1994. Transfer-
Driven Machine Translation Utilizing Empirical
Knowledge. Transactions of IPSJ, Vol. 35, No. 3, pp.
414-425 (in Japanese).
Jones, D. 1996. Analogical Natural Language
Processing. UCL Press. London, 155p.
Kitano, H. 1993. A Comprehensive and Practical
Model of Memory-Based Machine Translation. Proc.
of IJCAI-93. pp. 1276-1282.
Knight, K. 1997. Automating Knowledge Acquisition
for Machine Translation, AI Magazine, 18/4.
Manning, D., C. and Hinrich, S. (1999) Chapter 5 of
Foundations of statistical natural language
processing, MIT Press, p. 680.
Nagao, M. 1981. A Framework of a Mechanical
Translation between Japanese and English by
Analogy Principle, in Artificial and Human
Intelligence, A. Elithorn and R. Banerji (eds.) North-
Holland, pp. 173-180, 1984.
Ney, H., Och, F. J. and Vogel, S. 2000. Statistical
Translation of Spoken Dialogues in the Vermobil
System, Proc. Of MSC2000, pp. 69-74.
Ohno, S. and Hamanishi, M. 1984. Ruigo-Shin-Jiten,
Kadokawa, p. 932 (in Japanese).
Sadler, V. Working with Analogical Semantics, 1989).
Foris Publications, p. 256.
Sato, S. and Nagao, M. 1990. Toward Memory-based
Translation. In the proceedings of the International
Conference on Computational Linguistics, COLING-
90, Helsinki, Finland, August 1990.
Sato, S. 1991. MBT2: A Method for Combining
Fragments of Examples in Example-Based
Translation. JJSAI, Vol. 6, No. 6, pp. 861-871 (in
Japanese).
Sato, S. 1993. Example-Based Translation of
Technical Terms. Proc. of TMI-93, pp. 58-68,.
Stentiford, F. M. W. and Steer, M. G. 1987. A Speech
Driven Language Translation System, Proc. of
European Conference on Speech Technology, Vol. 2,
pp. 418-421.
Sumita, E. and Iida, H. 1991. Experiments and
Prospects of Example-Based Machine Translation.
Proc. of ACL-91, pp. 185-192.
Sumita, E., Yamada, S., Yamamoto, K., Paul, M.,
Kashioka, H., Ishikawa, K. and Shirai, S. 1999.
Solutions to Problems Inherent in Spoken-language
Translation: The ATR-MATRIX Approach, Proc. of
7th MT Summit, pp. 229-235.
Veale, T. and Way, A. 1997. Gaijin: A Template-
Driven Bootstrapping Approach to Example-Based
Machine Translation, in the Proceedings of
NeMNLP&apos;97, New Methods in Natural Language
Processing, Sofia, Bulgaria.
Watanabe, H. and Maruyama, H. 1994. A
Transfer System Using Example-Based Approach.
IEICE Transactions on Information and Systems, Vol.
E77-D, No. 2, pp. 247-257.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.237823">
<title confidence="0.991503">Example-based machine translation using DP-matching word sequences</title>
<author confidence="0.92208">Eiichiro</author>
<affiliation confidence="0.962734">ATR Spoken Language Translation Research</affiliation>
<address confidence="0.808691">Hikaridai, Seika,</address>
<email confidence="0.860535">Kyoto</email>
<affiliation confidence="0.356421">Japan</affiliation>
<email confidence="0.957431">sumita@slt.atr.co.jp</email>
<abstract confidence="0.998262615384615">We propose a new approach under the example-based machine translation paradigm. First, the proposed approach retrieves the most similar example by carrying out DP-matching of the input sentence and example sentences while measuring the semantic distance of the words. Second, the approach adjusts the gap between the input and the most similar example by using a bilingual dictionary. We show the results of a computational experiment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Andriamanankasina</author>
<author>K Araki</author>
<author>T Tochinai</author>
</authors>
<title>Example-Based Machine Translation of PartOf-Speech Tagged Sentences by Recursive Division.</title>
<date>1999</date>
<booktitle>Proceedings of MT SUMMIT VII.</booktitle>
<contexts>
<context position="1819" citStr="Andriamanankasina et al., 1999" startWordPosition="258" endWordPosition="261">ns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionary, which is used for word alignment and translation; and (3) thes</context>
</contexts>
<marker>Andriamanankasina, Araki, Tochinai, 1999</marker>
<rawString>Andriamanankasina, T., Araki, K. and Tochinai, T. 1999. Example-Based Machine Translation of PartOf-Speech Tagged Sentences by Recursive Division. Proceedings of MT SUMMIT VII. Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>V J</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1338" citStr="Brown et al., 1990" startWordPosition="183" endWordPosition="186">le for machine translation. The background is as follows: • Demands have been increasing for machine translation systems to handle a wider range of languages and domains. • MT requires bulk knowledge consisting of rules and dictionaries. • Building knowledge consumes considerable time and money. • Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3)</context>
</contexts>
<marker>Brown, Cocke, Pietra, A, Pietra, J, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J., Mercer, R. L., and Roossin, P. S. 1990. A Statistical Approach to Machine Translation. Computational Linguistics 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Brown</author>
</authors>
<title>Automated Generalization of Translation Examples.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING-2000),</booktitle>
<pages>125--131</pages>
<location>Saarbrücken, Germany,</location>
<contexts>
<context position="1833" citStr="Brown, 2000" startWordPosition="262" endWordPosition="263">orpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionary, which is used for word alignment and translation; and (3) thesauri of both l</context>
<context position="18887" citStr="Brown (2000)" startWordPosition="2910" endWordPosition="2911">gned, while we do not require the alignment of translation units. He defines the matching score and we define the distance between word sequences, which are different. However, both are computed by a standard dynamic programming technique. Based on surface structures and content words, Cranias defined a similarity score between texts and introduced the idea of clustering the translation memory to speed up the retrieval of similar translation examples. The score is again computed by a standard dynamic programming technique, but Cranias provides not a translation but only a retrieval. Thesaurus Brown (2000) uses equivalence classes to successfully improve the coverage of EBMT. He proposed a method of automatically generating equivalence classes using clustering techniques, while we use hand-coded thesauri (in the experiment). Such automation is very attractive, and the author is planning to follow in Brown’s line, in spite of a fear that low frequent words will not be dealt with effectively by clustering techniques. Brown uses a hard condition, i.e., whether a word is included in an equivalence class or not, while we provide the relative distance between two words. It is unknown which method is </context>
</contexts>
<marker>Brown, 2000</marker>
<rawString>Brown, R. D. 2000. Automated Generalization of Translation Examples. In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING-2000), pp. 125-131. Saarbrücken, Germany, August 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carl</author>
</authors>
<title>Inducing Translation Templates for Example-Based Machine Translation,</title>
<date>1999</date>
<booktitle>Proc. Of MTSummit VII.</booktitle>
<contexts>
<context position="1787" citStr="Carl, 1999" startWordPosition="256" endWordPosition="257">T): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionary, which is used for word alignme</context>
</contexts>
<marker>Carl, 1999</marker>
<rawString>Carl, M. 1999. Inducing Translation Templates for Example-Based Machine Translation, Proc. Of MTSummit VII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Cormen</author>
<author>C E Leiserson</author>
<author>L R Rivest</author>
</authors>
<title>Introduction to Algorithms,</title>
<date>1989</date>
<pages>1028</pages>
<publisher>MIT Press,</publisher>
<marker>Cormen, Leiserson, Rivest, 1989</marker>
<rawString>Cormen, H. T., Leiserson, C. E. and Rivest, L. R. 1989. Introduction to Algorithms, MIT Press, p. 1028.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cranias</author>
<author>H Papageorgiou</author>
<author>S Piperidis</author>
</authors>
<title>A Matching Technique in Example-Based Machine Translation. Institute for Language and Speech Processing, Greece. Paper presented to Computation and Language.</title>
<date>1994</date>
<contexts>
<context position="1741" citStr="Cranias et al., 1994" startWordPosition="246" endWordPosition="249">ased translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a biling</context>
<context position="18096" citStr="Cranias et al., 1994" startWordPosition="2787" endWordPosition="2790">; Furuse et al., 1994; Sadler, 1989) assume the existence of a bank of aligned bilingual trees or a set of translation patterns. However, building such knowledge is done by humans and is very expensive. Methods for automating knowledge building are still being developed. In contrast, our proposal does not rely on such a high-level analysis of the corpus and requires only word-level knowledge, i.e., morphological tags and dictionaries. Dynamic programming Dynamic programming has been used within the EBMT paradigm (1) for technical term translation (Sato, 1993), and (2) for translation support (Cranias et al., 1994). Sato translates technical terms, which are usually compound nouns, while we translate sentences. He uses a corpus in which translation units of a pair of technical terms are aligned, while we do not require the alignment of translation units. He defines the matching score and we define the distance between word sequences, which are different. However, both are computed by a standard dynamic programming technique. Based on surface structures and content words, Cranias defined a similarity score between texts and introduced the idea of clustering the translation memory to speed up the retrieva</context>
</contexts>
<marker>Cranias, Papageorgiou, Piperidis, 1994</marker>
<rawString>Cranias, L., Papageorgiou, H. and Piperidis, S. 1994. A Matching Technique in Example-Based Machine Translation. Institute for Language and Speech Processing, Greece. Paper presented to Computation and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Furuse</author>
<author>E Sumita</author>
<author>H Iida</author>
</authors>
<title>TransferDriven Machine Translation Utilizing Empirical Knowledge.</title>
<date>1994</date>
<journal>Transactions of IPSJ,</journal>
<volume>35</volume>
<pages>414--425</pages>
<note>(in Japanese).</note>
<contexts>
<context position="1690" citStr="Furuse et al., 1994" startWordPosition="238" endWordPosition="241">ly available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in w</context>
<context position="17496" citStr="Furuse et al., 1994" startWordPosition="2696" endWordPosition="2699">ch additional cost reduction. We also want to eliminate restrictions, e.g., sentence-aligned and morphologically tagged example database. By doing so, the applicability of our approach can be increased. This is another important challenge. A further challenging goal is to establish technology enabling the use of a small-scale corpus. 4.4 Related Research Here, we would like to compare our proposal and related research in four points: level of knowledge, application of dynamic programming, the use of thesauri, and the task. Knowledge of EBMT Many EBMT studies (Sato and Nagao, 1990; Sato, 1991; Furuse et al., 1994; Sadler, 1989) assume the existence of a bank of aligned bilingual trees or a set of translation patterns. However, building such knowledge is done by humans and is very expensive. Methods for automating knowledge building are still being developed. In contrast, our proposal does not rely on such a high-level analysis of the corpus and requires only word-level knowledge, i.e., morphological tags and dictionaries. Dynamic programming Dynamic programming has been used within the EBMT paradigm (1) for technical term translation (Sato, 1993), and (2) for translation support (Cranias et al., 1994)</context>
</contexts>
<marker>Furuse, Sumita, Iida, 1994</marker>
<rawString>Furuse, O., Sumita, E. and Iida, H. 1994. TransferDriven Machine Translation Utilizing Empirical Knowledge. Transactions of IPSJ, Vol. 35, No. 3, pp. 414-425 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jones</author>
</authors>
<title>Analogical Natural Language Processing.</title>
<date>1996</date>
<publisher>UCL Press.</publisher>
<location>London,</location>
<contexts>
<context position="1754" citStr="Jones, 1996" startWordPosition="250" endWordPosition="251">tatistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionar</context>
</contexts>
<marker>Jones, 1996</marker>
<rawString>Jones, D. 1996. Analogical Natural Language Processing. UCL Press. London, 155p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kitano</author>
</authors>
<title>A Comprehensive and Practical Model of Memory-Based Machine Translation.</title>
<date>1993</date>
<booktitle>Proc. of IJCAI-93.</booktitle>
<pages>1276--1282</pages>
<contexts>
<context position="1669" citStr="Kitano, 1993" startWordPosition="236" endWordPosition="237">ve become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a b</context>
</contexts>
<marker>Kitano, 1993</marker>
<rawString>Kitano, H. 1993. A Comprehensive and Practical Model of Memory-Based Machine Translation. Proc. of IJCAI-93. pp. 1276-1282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Automating Knowledge Acquisition for Machine Translation,</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<contexts>
<context position="1352" citStr="Knight, 1997" startWordPosition="187" endWordPosition="188">lation. The background is as follows: • Demands have been increasing for machine translation systems to handle a wider range of languages and domains. • MT requires bulk knowledge consisting of rules and dictionaries. • Building knowledge consumes considerable time and money. • Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Targ</context>
</contexts>
<marker>Knight, 1997</marker>
<rawString>Knight, K. 1997. Automating Knowledge Acquisition for Machine Translation, AI Magazine, 18/4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Manning</author>
<author>C</author>
<author>S Hinrich</author>
</authors>
<title>Chapter 5 of Foundations of statistical natural language processing,</title>
<date>1999</date>
<pages>680</pages>
<publisher>MIT Press,</publisher>
<marker>Manning, C, Hinrich, 1999</marker>
<rawString>Manning, D., C. and Hinrich, S. (1999) Chapter 5 of Foundations of statistical natural language processing, MIT Press, p. 680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>A Framework of a Mechanical Translation between Japanese and English by Analogy Principle,</title>
<date>1981</date>
<booktitle>in Artificial and Human Intelligence,</booktitle>
<pages>173--180</pages>
<editor>A. Elithorn and R. Banerji (eds.) NorthHolland,</editor>
<contexts>
<context position="1597" citStr="Nagao, 1981" startWordPosition="224" endWordPosition="225">s considerable time and money. • Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed </context>
</contexts>
<marker>Nagao, 1981</marker>
<rawString>Nagao, M. 1981. A Framework of a Mechanical Translation between Japanese and English by Analogy Principle, in Artificial and Human Intelligence, A. Elithorn and R. Banerji (eds.) NorthHolland, pp. 173-180, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>F J Och</author>
<author>S Vogel</author>
</authors>
<date>2000</date>
<booktitle>Statistical Translation of Spoken Dialogues in the Vermobil System, Proc. Of MSC2000,</booktitle>
<pages>69--74</pages>
<contexts>
<context position="1371" citStr="Ney et al., 2000" startWordPosition="189" endWordPosition="192">ckground is as follows: • Demands have been increasing for machine translation systems to handle a wider range of languages and domains. • MT requires bulk knowledge consisting of rules and dictionaries. • Building knowledge consumes considerable time and money. • Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure </context>
</contexts>
<marker>Ney, Och, Vogel, 2000</marker>
<rawString>Ney, H., Och, F. J. and Vogel, S. 2000. Statistical Translation of Spoken Dialogues in the Vermobil System, Proc. Of MSC2000, pp. 69-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ohno</author>
<author>M Hamanishi</author>
</authors>
<date>1984</date>
<note>Ruigo-Shin-Jiten, Kadokawa, p. 932 (in Japanese).</note>
<marker>Ohno, Hamanishi, 1984</marker>
<rawString>Ohno, S. and Hamanishi, M. 1984. Ruigo-Shin-Jiten, Kadokawa, p. 932 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
</authors>
<title>Working with Analogical Semantics,</title>
<date>1989</date>
<pages>256</pages>
<publisher>Foris Publications,</publisher>
<contexts>
<context position="1610" citStr="Sadler 1989" startWordPosition="226" endWordPosition="227">e time and money. • Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Co</context>
<context position="17511" citStr="Sadler, 1989" startWordPosition="2700" endWordPosition="2701">duction. We also want to eliminate restrictions, e.g., sentence-aligned and morphologically tagged example database. By doing so, the applicability of our approach can be increased. This is another important challenge. A further challenging goal is to establish technology enabling the use of a small-scale corpus. 4.4 Related Research Here, we would like to compare our proposal and related research in four points: level of knowledge, application of dynamic programming, the use of thesauri, and the task. Knowledge of EBMT Many EBMT studies (Sato and Nagao, 1990; Sato, 1991; Furuse et al., 1994; Sadler, 1989) assume the existence of a bank of aligned bilingual trees or a set of translation patterns. However, building such knowledge is done by humans and is very expensive. Methods for automating knowledge building are still being developed. In contrast, our proposal does not rely on such a high-level analysis of the corpus and requires only word-level knowledge, i.e., morphological tags and dictionaries. Dynamic programming Dynamic programming has been used within the EBMT paradigm (1) for technical term translation (Sato, 1993), and (2) for translation support (Cranias et al., 1994). Sato translat</context>
</contexts>
<marker>Sadler, 1989</marker>
<rawString>Sadler, V. Working with Analogical Semantics, 1989). Foris Publications, p. 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
<author>M Nagao</author>
</authors>
<title>Toward Memory-based Translation.</title>
<date>1990</date>
<booktitle>In the proceedings of the International Conference on Computational Linguistics, COLING90,</booktitle>
<location>Helsinki, Finland,</location>
<contexts>
<context position="1632" citStr="Sato and Nagao, 1990" startWordPosition="228" endWordPosition="231">ney. • Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown i</context>
<context position="17463" citStr="Sato and Nagao, 1990" startWordPosition="2690" endWordPosition="2693">cal knowledge. We are aiming at such additional cost reduction. We also want to eliminate restrictions, e.g., sentence-aligned and morphologically tagged example database. By doing so, the applicability of our approach can be increased. This is another important challenge. A further challenging goal is to establish technology enabling the use of a small-scale corpus. 4.4 Related Research Here, we would like to compare our proposal and related research in four points: level of knowledge, application of dynamic programming, the use of thesauri, and the task. Knowledge of EBMT Many EBMT studies (Sato and Nagao, 1990; Sato, 1991; Furuse et al., 1994; Sadler, 1989) assume the existence of a bank of aligned bilingual trees or a set of translation patterns. However, building such knowledge is done by humans and is very expensive. Methods for automating knowledge building are still being developed. In contrast, our proposal does not rely on such a high-level analysis of the corpus and requires only word-level knowledge, i.e., morphological tags and dictionaries. Dynamic programming Dynamic programming has been used within the EBMT paradigm (1) for technical term translation (Sato, 1993), and (2) for translati</context>
</contexts>
<marker>Sato, Nagao, 1990</marker>
<rawString>Sato, S. and Nagao, M. 1990. Toward Memory-based Translation. In the proceedings of the International Conference on Computational Linguistics, COLING90, Helsinki, Finland, August 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
</authors>
<title>MBT2: A Method for Combining Fragments of Examples in Example-Based Translation.</title>
<date>1991</date>
<journal>JJSAI,</journal>
<volume>6</volume>
<pages>861--871</pages>
<note>(in Japanese).</note>
<contexts>
<context position="17475" citStr="Sato, 1991" startWordPosition="2694" endWordPosition="2695">aiming at such additional cost reduction. We also want to eliminate restrictions, e.g., sentence-aligned and morphologically tagged example database. By doing so, the applicability of our approach can be increased. This is another important challenge. A further challenging goal is to establish technology enabling the use of a small-scale corpus. 4.4 Related Research Here, we would like to compare our proposal and related research in four points: level of knowledge, application of dynamic programming, the use of thesauri, and the task. Knowledge of EBMT Many EBMT studies (Sato and Nagao, 1990; Sato, 1991; Furuse et al., 1994; Sadler, 1989) assume the existence of a bank of aligned bilingual trees or a set of translation patterns. However, building such knowledge is done by humans and is very expensive. Methods for automating knowledge building are still being developed. In contrast, our proposal does not rely on such a high-level analysis of the corpus and requires only word-level knowledge, i.e., morphological tags and dictionaries. Dynamic programming Dynamic programming has been used within the EBMT paradigm (1) for technical term translation (Sato, 1993), and (2) for translation support (</context>
</contexts>
<marker>Sato, 1991</marker>
<rawString>Sato, S. 1991. MBT2: A Method for Combining Fragments of Examples in Example-Based Translation. JJSAI, Vol. 6, No. 6, pp. 861-871 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
</authors>
<title>Example-Based Translation of Technical Terms.</title>
<date>1993</date>
<booktitle>Proc. of TMI-93,</booktitle>
<pages>58--68</pages>
<contexts>
<context position="18040" citStr="Sato, 1993" startWordPosition="2780" endWordPosition="2781">EBMT studies (Sato and Nagao, 1990; Sato, 1991; Furuse et al., 1994; Sadler, 1989) assume the existence of a bank of aligned bilingual trees or a set of translation patterns. However, building such knowledge is done by humans and is very expensive. Methods for automating knowledge building are still being developed. In contrast, our proposal does not rely on such a high-level analysis of the corpus and requires only word-level knowledge, i.e., morphological tags and dictionaries. Dynamic programming Dynamic programming has been used within the EBMT paradigm (1) for technical term translation (Sato, 1993), and (2) for translation support (Cranias et al., 1994). Sato translates technical terms, which are usually compound nouns, while we translate sentences. He uses a corpus in which translation units of a pair of technical terms are aligned, while we do not require the alignment of translation units. He defines the matching score and we define the distance between word sequences, which are different. However, both are computed by a standard dynamic programming technique. Based on surface structures and content words, Cranias defined a similarity score between texts and introduced the idea of cl</context>
</contexts>
<marker>Sato, 1993</marker>
<rawString>Sato, S. 1993. Example-Based Translation of Technical Terms. Proc. of TMI-93, pp. 58-68,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M W Stentiford</author>
<author>M G Steer</author>
</authors>
<title>A Speech Driven Language Translation System,</title>
<date>1987</date>
<booktitle>Proc. of European Conference on Speech Technology,</booktitle>
<volume>2</volume>
<pages>418--421</pages>
<contexts>
<context position="19788" citStr="Stentiford and Steer, 1987" startWordPosition="3051" endWordPosition="3054"> is planning to follow in Brown’s line, in spite of a fear that low frequent words will not be dealt with effectively by clustering techniques. Brown uses a hard condition, i.e., whether a word is included in an equivalence class or not, while we provide the relative distance between two words. It is unknown which method is better for EBMT. We do not plan on sticking with the current implementation using handcoded thesauri, as we realize that further research on these open problems is indispensable. Phrasebook task The phrasebook task was first advocated for the task of speech translation by (Stentiford and Steer, 1987). They pointed out that when communicating within a limited domain such as international telephone communications, it is nearly possible to specify all of the required message concepts. They used a keywordbased approach to access concepts to overcome speech recognition errors. On the other hand, we use DP-matching techniques for this end.7 The scalability of the keyword-based approach has raised questions because enlarging a corpus directly increases the chances of conflict in identifying the concepts to be conveyed. 5 Concluding Remarks We proposed a new approach using DP-matching for retriev</context>
</contexts>
<marker>Stentiford, Steer, 1987</marker>
<rawString>Stentiford, F. M. W. and Steer, M. G. 1987. A Speech Driven Language Translation System, Proc. of European Conference on Speech Technology, Vol. 2, pp. 418-421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>H Iida</author>
</authors>
<title>Experiments and Prospects of Example-Based Machine Translation.</title>
<date>1991</date>
<booktitle>Proc. of ACL-91,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="1655" citStr="Sumita and Iida, 1991" startWordPosition="232" endWordPosition="235">lingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resourc</context>
<context position="3917" citStr="Sumita and Iida, 1991" startWordPosition="586" endWordPosition="589">tion (1), dist is calculated as follows: The counts of the Insertion (I), Deletion (D), and Substitution (S) operations are summed up and the total is normalized by the sum of the length of the source and example sequences. 1 Step I corresponds to Retrieval in Figure 1 and steps II, III, and IV correspond to Adjustment. Substitution (S) considers the semantic distance between two substituted words and is called SEMDIST. SEMDIST is defined as the division of K (the level of the least common abstraction in the thesaurus of two words) by N (the height of the thesaurus) according to equation (2) (Sumita and Iida, 1991). It ranges from 0 to 1. Let’s observe the following two sentences,2 (1-j) the input and (2-j) the source sentence of the translation example, where the hatched parts represent the differences between the two sentences. (1-j) iro/ga/ki/ni/iri/masen [color/SUB/favor/OBJ/enter/POLITE-NOT] {I do not care for the color.} (2-j) dezain/ga/ki/ni/iri/masen [design/SUB/favor/OBJ/enter/ POLITE-NOT] {I do not care for the design.} Because “iro” and “dezain” are completely dissimilar in the thesauri used in the experiment, SEMDIST is 1, and therefore, the dist between them is (0+0+2*1) / (6+6) = 0.167. Th</context>
</contexts>
<marker>Sumita, Iida, 1991</marker>
<rawString>Sumita, E. and Iida, H. 1991. Experiments and Prospects of Example-Based Machine Translation. Proc. of ACL-91, pp. 185-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>S Yamada</author>
<author>K Yamamoto</author>
<author>M Paul</author>
<author>H Kashioka</author>
<author>K Ishikawa</author>
<author>S Shirai</author>
</authors>
<title>Solutions to Problems Inherent in Spoken-language Translation: The ATR-MATRIX Approach,</title>
<date>1999</date>
<booktitle>Proc. of 7th MT Summit,</booktitle>
<pages>229--235</pages>
<contexts>
<context position="11878" citStr="Sumita et al., 1999" startWordPosition="1875" endWordPosition="1878">gh they are less frequent, as follows: • Idioms or collocations Even when the dist between the two sentences is small, i.e., they are quite similar in the source language, the meanings of the sentences can vary and the translation can be different in the target language. This case is not so frequent, but is possible by idioms or collocations as exemplified in the following sample. EXACT A 0.1&gt; d &gt;=0 B 0.2&gt; d &gt;=0.1 C 0.3&gt; d &gt;=0.2 D 1/3&gt; d &gt;=0.3 F 0 50 100 150 200 250 Figure 2 Accuracy by Distance 5 This ranking was developed for evaluation in spoken language translation. For more details, see (Sumita et al., 1999). 6 The horizontal axis indicates the number of sentences. Proposed MT A commercial MT A B C D F 0% 20% 40% 60% 80% 100% Figure 3 Comparison of Proposed EBMT and a Commercial MT 1. kata/o/tsume/teitadake/masu/ka [shoulders/OBJ/shorten-or-sitclosely/REQUEST/POLITE/QUESTION] {Could you tighten the shoulders up?} 2. seki/o/tsume/teitadake/masu/ka [seat/OBJ/shorten-or-closeup/REQUEST/POLITE/QUESTION] {Could you move over a little?} The replaceability between “kata” and “seki” does not hold for these two similar sentences. To avoid this problem, a feedback mechanism of erroneous translations built </context>
</contexts>
<marker>Sumita, Yamada, Yamamoto, Paul, Kashioka, Ishikawa, Shirai, 1999</marker>
<rawString>Sumita, E., Yamada, S., Yamamoto, K., Paul, M., Kashioka, H., Ishikawa, K. and Shirai, S. 1999. Solutions to Problems Inherent in Spoken-language Translation: The ATR-MATRIX Approach, Proc. of 7th MT Summit, pp. 229-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>A Way</author>
</authors>
<title>Gaijin: A TemplateDriven Bootstrapping Approach to Example-Based Machine Translation,</title>
<date>1997</date>
<booktitle>in the Proceedings of NeMNLP&apos;97, New Methods in Natural Language Processing,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1775" citStr="Veale and Way, 1997" startWordPosition="252" endWordPosition="255">chine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionary, which is used for </context>
</contexts>
<marker>Veale, Way, 1997</marker>
<rawString>Veale, T. and Way, A. 1997. Gaijin: A TemplateDriven Bootstrapping Approach to Example-Based Machine Translation, in the Proceedings of NeMNLP&apos;97, New Methods in Natural Language Processing, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Watanabe</author>
<author>H Maruyama</author>
</authors>
<title>A Transfer System Using Example-Based Approach.</title>
<date>1994</date>
<journal>IEICE Transactions on Information and Systems,</journal>
<volume>77</volume>
<pages>247--257</pages>
<contexts>
<context position="1719" citStr="Watanabe and Maruyama, 1994" startWordPosition="242" endWordPosition="245">re two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). (1) SentenceAligned Bilingual Corpus Input sentence (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 The proposed method 2.1 Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned be</context>
</contexts>
<marker>Watanabe, Maruyama, 1994</marker>
<rawString>Watanabe, H. and Maruyama, H. 1994. A Transfer System Using Example-Based Approach. IEICE Transactions on Information and Systems, Vol. E77-D, No. 2, pp. 247-257.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>