<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000233">
<title confidence="0.879801">
Knowledge Graph Embedding via Dynamic Mapping Matrix
</title>
<author confidence="0.955821">
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu and Jun Zhao
</author>
<affiliation confidence="0.9484165">
National Laboratory of Pattern Recognition (NLPR)
Institute of Automation Chinese Academy of Sciences, Beijing, 100190, China
</affiliation>
<email confidence="0.992185">
{guoliang.ji,shizhu.he,lhxu,kliu,jzhao}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.99464" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930461538462">
Knowledge graphs are useful resources for
numerous AI applications, but they are far
from completeness. Previous work such as
TransE, TransH and TransR/CTransR re-
gard a relation as translation from head en-
tity to tail entity and the CTransR achieves
state-of-the-art performance. In this pa-
per, we propose a more fine-grained model
named TransD, which is an improvement
of TransR/CTransR. In TransD, we use
two vectors to represent a named sym-
bol object (entity and relation). The first
one represents the meaning of a(n) entity
(relation), the other one is used to con-
struct mapping matrix dynamically. Com-
pared with TransR/CTransR, TransD not
only considers the diversity of relations,
but also entities. TransD has less param-
eters and has no matrix-vector multipli-
cation operations, which makes it can be
applied on large scale graphs. In Experi-
ments, we evaluate our model on two typ-
ical tasks including triplets classification
and link prediction. Evaluation results
show that our approach outperforms state-
of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971245283019">
Knowledge Graphs such as WordNet (Miller
1995), Freebase (Bollacker et al. 2008) and Yago
(Suchanek et al. 2007) have been playing a piv-
otal role in many AI applications, such as relation
extraction(RE), question answering(Q&amp;A), etc.
They usually contain huge amounts of structured
data as the form of triplets (head entity, relation,
tail entity)(denoted as (h, r, t)), where relation
models the relationship between the two entities.
As most knowledge graphs have been built either
collaboratively or (partly) automatically, they of-
ten suffer from incompleteness. Knowledge graph
completion is to predict relations between entities
based on existing triplets in a knowledge graph. In
the past decade, much work based on symbol and
logic has been done for knowledge graph comple-
tion, but they are neither tractable nor enough con-
vergence for large scale knowledge graphs. Re-
cently, a powerful approach for this task is to en-
code every element (entities and relations) of a
knowledge graph into a low-dimensional embed-
ding vector space. These methods do reasoning
over knowledge graphs through algebraic opera-
tions (see section ”Related Work”).
Among these methods, TransE (Bordes et al.
2013) is simple and effective, and also achieves
state-of-the-art prediction performance. It learns
low-dimensional embeddings for every entity and
relation in knowledge graphs. These vector em-
beddings are denoted by the same letter in bold-
face. The basic idea is that every relation is re-
garded as translation in the embedding space. For
a golden triplet (h, r, t), the embedding h is close
to the embedding t by adding the embedding r,
that is h + r ≈ t. TransE is suitable for 1-to-1
relations, but has flaws when dealing with 1-to-
N, N-to-1 and N-to-N relations. TransH (Wang
et al. 2014) is proposed to solve these issues.
TransH regards a relation as a translating oper-
ation on a relation-specific hyperplane, which is
characterized by a norm vector wr and a trans-
lation vector dr. The embeddings h and t are
first projected to the hyperplane of relation r to
obtain vectors h1 = h − wr hwr and t1 =
t − wr twr, and then h1 + dr ≈ t1. Both
in TransE and TransH, the embeddings of enti-
ties and relations are in the same space. How-
ever, entities and relations are different types ob-
jects, it is insufficient to model them in the same
space. TransR/CTransR (Lin et al. 2015) set a
mapping matrix Mr and a vector r for every re-
lation r. In TransR, h and t are projected to the
aspects that relation r focuses on through the ma-
</bodyText>
<page confidence="0.968192">
687
</page>
<note confidence="0.98884">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 687–696,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.989401">
Figure 1: Simple illustration of TransD. Each
</figureCaption>
<bodyText confidence="0.992059081967213">
shape represents an entity pair appearing in a
triplet of relation r. Mrh and Mrt are mapping
matrices of h and t, respectively. hip, tip(i =
1, 2, 3), and rp are projection vectors. hi⊥ and
ti⊥(i = 1, 2, 3) are projected vectors of entities.
The projected vectors satisfy hi⊥ + r ≈ ti⊥(i =
1, 2, 3).
trix Mr and then Mrh + r ≈ Mrt. CTransR is
an extension of TransR by clustering diverse head-
tail entity pairs into groups and learning distinct
relation vectors for each group. TransR/CTransR
has significant improvements compared with pre-
vious state-of-the-art models. However, it also has
several flaws: (1) For a typical relation r, all en-
tities share the same mapping matrix Mr. How-
ever, the entities linked by a relation always con-
tains various types and attributes. For example, in
triplet (friedrich burklein, nationality, germany),
friedrich burklein and germany are typical differ-
ent types of entities. These entities should be pro-
jected in different ways; (2) The projection oper-
ation is an interactive process between an entity
and a relation, it is unreasonable that the map-
ping matrices are determined only by relations;
and (3) Matrix-vector multiplication makes it has
large amount of calculation, and when relation
number is large, it also has much more param-
eters than TransE and TransH. As the complex-
ity, TransR/CTransR is difficult to apply on large-
scale knowledge graphs.
In this paper, we propose a novel method named
TransD to model knowledge graphs. Figure 1
shows the basic idea of TransD. In TransD, we de-
fine two vectors for each entity and relation. The
first vector represents the meaning of an entity or
a relation, the other one (called projection vector)
represents the way that how to project a entity em-
bedding into a relation vector space and it will
be used to construct mapping matrices. There-
fore, every entity-relation pair has an unique map-
ping matrix. In addition, TransD has no matrix-
by-vector operations which can be replaced by
vectors operations. We evaluate TransD with the
task of triplets classification and link prediction.
The experimental results show that our method has
significant improvements compared with previous
models.
Our contributions in this paper are: (1)We pro-
pose a novel model TransD, which constructs a
dynamic mapping matrix for each entity-relation
pair by considering the diversity of entities and re-
lations simultaneously. It provides a flexible style
to project entity representations to relation vec-
tor space; (2) Compared with TransR/CTransR,
TransD has fewer parameters and has no matrix-
vector multiplication. It is easy to be applied
on large-scale knowledge graphs like TransE and
TransH; and (3) In experiments, our approach
outperforms previous models including TransE,
TransH and TransR/CTransR in link prediction
and triplets classification tasks.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999976363636364">
Before proceeding, we define our mathematical
notations. We denote a triplet by (h, r, t) and their
column vectors by bold lower case letters h, r, t;
matrices by bold upper case letters, such as M;
tensors by bold upper case letters with a hat, such
as M. Score function is represented by fr(h, t).
For a golden triplet (h, r, t) that corresponds to a
true fact in real world, it always get a relatively
higher score, and lower for an negative triplet.
Other notations will be described in the appropri-
ate sections.
</bodyText>
<subsectionHeader confidence="0.999184">
2.1 TransE, TransH and TransR/CTransR
</subsectionHeader>
<bodyText confidence="0.9945366">
As mentioned in Introduction section, TransE
(Bordes et al. 2013) regards the relation r as trans-
lation from h to t for a golden triplet (h, r, t).
Hence, (h+r) is close to (t) and the score function
is
</bodyText>
<equation confidence="0.999647">
fr(h,t) = −kh + r − tk22 (1)
</equation>
<bodyText confidence="0.962128777777778">
TransE is only suitable for 1-to-1 relations, there
remain flaws for 1-to-N, N-to-1 and N-to-N rela-
tions.
To solve these problems, TransH (Wang et al.
2014) proposes an improved model named trans-
lation on a hyperplane. On hyperplanes of differ-
ent relations, a given entity has different represen-
tations. Similar to TransE, TransH has the score
function as follows:
</bodyText>
<equation confidence="0.820147">
fr(h,t) = −kh⊥ + r − t⊥k22 (2)
Entity Space —non apace
</equation>
<page confidence="0.996707">
688
</page>
<note confidence="0.975649230769231">
Model #Parameters # Operations (Time complexity)
Unstructured (Bordes et al. 2012; 2014) O(Nem) O(Nt)
SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt)
SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt)
SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt)
LFM (Jenatton et al. 2012; Sutskever et al. 2009) O(Nem + Nrn2)(m = n) O((m2 + m)Nt)
SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt)
NTN (Socher et al. 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2 + m)s + 2mk + k)Nt)
TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt)
TransH (Wang et al. 2014) O(Nem + 2Nrn)(m = n) O(2mNt)
TransR (Lin et al. 2015) O(Nem + Nr(m + 1)n) O(2mnNt)
CTransR (Lin et al. 2015) O(Nem + Nr(m + d)n) O(2mnNt)
TransD (this paper) O(2Nem + 2Nrn) O(2nNt)
</note>
<tableCaption confidence="0.983858">
Table 1: Complexity (the number of parameters and the number of multiplication operations in an epoch)
</tableCaption>
<bodyText confidence="0.991053142857143">
of several embedding models. Ne and Nr represent the number of entities and relations, respectively.
Nt represents the number of triplets in a knowledge graph. m is the dimension of entity embedding
space and n is the dimension of relation embedding space. d denotes the average number of clusters of a
relation. k is the number of hidden nodes of a neural network and s is the number of slice of a tensor.
In order to ensure that h1 and t1 are on the hy-
perplane of r, TransH restricts I IwrI I = 1.
Both TransE and TransH assume that entities
and relations are in the same vector space. But
relations and entities are different types of ob-
jects, they should not be in the same vector space.
TransR/CTransR (Lin et al. 2015) is proposed
based on the idea. TransR set a mapping matrix
Mr for each relation r to map entity embedding
into relation vector space. Its score function is:
</bodyText>
<equation confidence="0.799869">
fr(h, t) = −I IMrh + r − MrtI I22. (3)
</equation>
<bodyText confidence="0.99992">
where Mr E Rm×n, h, t E Rn and r E Rm.
CTransR is an extension of TransR. As head-tail
entity pairs present various patterns in different re-
lations, CTransR clusters diverse head-tail entity
pairs into groups and sets a relation vector for each
group.
</bodyText>
<subsectionHeader confidence="0.992108">
2.2 Other Models
</subsectionHeader>
<bodyText confidence="0.999878666666667">
Unstructured. Unstructured model (Bordes et al.
2012; 2014) ignores relations, only models entities
as embeddings. The score function is
</bodyText>
<equation confidence="0.998297">
fr(h,t) = −I Ih − tI I22. (4)
</equation>
<bodyText confidence="0.999962833333333">
It’s a simple case of TransE. Obviously, Unstruc-
tured model can not distinguish different relations.
Structured Embedding (SE). SE model (Bordes
et al. 2011) sets two separate matrices Mrh and
Mrt to project head and tail entities for each rela-
tion. Its score function is defined as follows:
</bodyText>
<equation confidence="0.987292">
fr(h,t) = −I IMrhh − MrttI I1 (5)
</equation>
<bodyText confidence="0.998889777777778">
Semantic Matching Energy (SME). SME model
(Bordes et al. 2012; 2014) encodes each named
symbolic object (entities and relations) as a vector.
Its score function is a neural network that captures
correlations between entities and relations via ma-
trix operations. Parameters of the neural network
are shared by all relations. SME defines two se-
mantic matching energy functions for optimiza-
tion, a linear form
</bodyText>
<equation confidence="0.801896666666667">
gη = Mη1eη + Mη2r + bη (6)
and a bilinear form
gη = (Mη1eη) ® (Mη2r) + bη (7)
</equation>
<bodyText confidence="0.999701">
where q = {left, right}, eleft = h, eright = t
and ® is the Hadamard product. The score func-
tion is
</bodyText>
<equation confidence="0.505332">
fr(h, t) = gleftTgright (8)
</equation>
<bodyText confidence="0.983986">
In (Bordes et al.2014), matrices of the bilinear
form are replaced by tensors.
Latent Factor Model (LFM). LFM model (Je-
natton et al. 2012; Sutskever et al. 2009) en-
codes each entity into a vector and sets a ma-
trix for every relation. It defines a score function
fr(h, t) = hTMrt, which incorporates the inter-
action of the two entity vectors in a simple and
effecitve way.
Single Layer Model (SLM). SLM model is de-
signed as a baseline of Neural Tensor Network
(Socher et al. 2013). The model constructs a non-
linear neural network to represent the score func-
tion defined as follows.
fr(h, t) = uTr f(Mr1h + Mr2t + br) (9)
where Mr1, Mr2 and br are parameters indexed
by relation r, f() is tanh operation.
</bodyText>
<page confidence="0.995745">
689
</page>
<bodyText confidence="0.99966625">
Neural Tensor Network (NTN). NTN model
(Socher et al. 2013) extends SLM model by con-
sidering the second-order correlations into nonlin-
ear neural networks. The score function is
fr(h, t) = uTr f (hT�Wrt + Mr L t J + br) (10)
where Wr represents a 3-way tensor, Mr denotes
the weight matrix, br is the bias and f() is tanh
operation. NTN is the most expressive model so
far, but it has so many parameters that it is difficult
to scale up to large knowledge graphs.
Table 1 lists the complexity of all the above
models. The complexity (especially for time) of
TransD is much less than TransR/CTransR and is
similar to TransE and TransH. Therefore, TransD
is effective and train faster than TransR/CTransR.
Beyond these embedding models, there is other re-
lated work of modeling multi-relational data, such
as matrix factorization, recommendations, etc. In
experiments, we refer to the results of RESCAL
presented in (Lin et al. 2015) and compare with it.
</bodyText>
<sectionHeader confidence="0.989298" genericHeader="method">
3 Our Method
</sectionHeader>
<bodyText confidence="0.999979444444444">
We first define notations. Triplets are represented
as (hi, ri, ti)(i = 1, 2, ... , nt), where hi denotes
a head entity, ti denotes a tail entity and ri de-
notes a relation. Their embeddings are denoted by
hi, ri, ti(i = 1, 2,... , nt). We use A to represent
golden triplets set, and use A, to denote negative
triplets set. Entities set and relations set are de-
noted by E and R, respectively. We use Imxn to
denote the identity matrix of size m x n.
</bodyText>
<subsectionHeader confidence="0.999498">
3.1 Multiple Types of Entities and Relations
</subsectionHeader>
<bodyText confidence="0.9982165">
Considering the diversity of relations, CTransR
segments triplets of a specific relation r into
several groups and learns a vector representa-
tion for each group. However, entities also
have various types. Figure 2 shows several
kinds of head and tail entities of relation loca-
tion.location.partially containedby in FB15k. In
both TransH and TransR/CTransR, all types of en-
tities share the same mapping vectors/matrices.
However, different types of entities have differ-
ent attributes and functions, it is insufficient to let
them share the same transform parameters of a re-
lation. And for a given relation, similar entities
should have similar mapping matrices and other-
wise for dissimilar entities. Furthermore, the map-
ping process is a transaction between entities and
relations that both have various types. Therefore,
we propose a more fine-grained model TransD,
which considers different types of both entities
and relations, to encode knowledge graphs into
embedding vectors via dynamic mapping matrices
produced by projection vectors.
</bodyText>
<figureCaption confidence="0.773031">
Figure 2: Multiple types of entities of relation lo-
cation.location.partially containedby.
</figureCaption>
<subsectionHeader confidence="0.993292">
3.2 TransD
</subsectionHeader>
<bodyText confidence="0.99992">
Model In TransD, each named symbol object (en-
tities and relations) is represented by two vectors.
The first one captures the meaning of entity (rela-
tion), the other one is used to construct mapping
matrices. For example, given a triplet (h, r, t),
its vectors are h, hp, r, rp, t, tp, where subscript
p marks the projection vectors, h, hp, t, tp E Rn
and r, rp E Rm. For each triplet (h, r, t), we
set two mapping matrices Mrh, Mrt E Rmxn to
project entities from entity space to relation space.
They are defined as follows:
</bodyText>
<equation confidence="0.999884">
Mrh = rphTp + Imxn (11)
Mrt = rptTp + Imxn (12)
</equation>
<bodyText confidence="0.999875555555556">
Therefore, the mapping matrices are determined
by both entities and relations, and this kind of
operation makes the two projection vectors inter-
act sufficiently because each element of them can
meet every entry comes from another vector. As
we initialize each mapping matrix with an identity
matrix, we add the Imxn to Mrh and Mrh. With
the mapping matrices, we define the projected vec-
tors as follows:
</bodyText>
<equation confidence="0.948615">
h1 = Mrhh, t1 = Mrtt (13)
</equation>
<page confidence="0.801473">
690
</page>
<bodyText confidence="0.851475">
Then the score function is
</bodyText>
<equation confidence="0.9937658">
fr(h,t) = −kh1 + r − t1k2 (14)
2
In experiments, we enforce constrains as khk2 ≤
1,ktk2 ≤ 1,krk2 ≤ 1,kh1k2 ≤ 1 and kt1k2 ≤
1.
</equation>
<bodyText confidence="0.9993515">
Training Objective We assume that there are
nt triplets in training set and denote the ith triplet
by (hi, ri, ti)(i = 1, 2, ... , nt). Each triplet has a
label yi to indicate the triplet is positive (yi = 1)
or negative (yi = 0). Then the golden and neg-
ative triplets are denoted by A = {(hj, rj, tj) |
yj = 1} and A0 = {(hj, rj, tj)  |yj = 0}, respec-
tively. Before training, one important trouble is
that knowledge graphs only encode positive train-
ing triplets, they do not contain negative examples.
Therefore, we obtain A from knowledge graphs
and generate A0 as follows: A0 = {(hl, rk, tk) |
hl =6 hk ∧ yk = 1} ∪ {(hk, rk, tl)  |tl =6 tk ∧ yk =
1}. We also use two strategies “unif” and “bern”
described in (Wang et al. 2014) to replace the head
or tail entity.
Let us use ξ and ξ0 to denote a golden triplet
and a corresponding negative triplet, respectively.
Then we define the following margin-based rank-
ing loss as the objective for training:
</bodyText>
<equation confidence="0.995109">
XL = X [γ + fr(ξ0) − fr(ξ)]+ (15)
ξEΔ ξ0EΔ0
</equation>
<bodyText confidence="0.999990875">
where [x]+ °= max (0, x), and γ is the margin sep-
arating golden triplets and negative triplets. The
process of minimizing the above objective is car-
ried out with stochastic gradient descent (SGD).
In order to speed up the convergence and avoid
overfitting, we initiate the entity and relation em-
beddings with the results of TransE and initiate all
the transfer matrices with identity matrices.
</bodyText>
<subsectionHeader confidence="0.9794235">
3.3 Connections with TransE, TransH and
TransR/CTransR
</subsectionHeader>
<bodyText confidence="0.999451833333333">
TransE is a special case of TransD when the di-
mension of vectors satisfies m = n and all projec-
tion vectors are set zero.
TransH is related to TransD when we set m =
n. Under the setting, projected vectors of entities
can be rewritten as follows:
</bodyText>
<equation confidence="0.9999575">
h1 = Mrhh = h + hTp hrp (16)
t1 = Mrtt = t + tTp trp (17)
</equation>
<bodyText confidence="0.997799266666667">
Hence, when m = n, the difference between
TransD and TransH is that projection vectors are
determinded only by relations in TransH, but
TransD’s projection vectors are determinded by
both entities and relations.
As to TransR/CTransR, TransD is an improve-
ment of it. TransR/CTransR directly defines a
mapping matrix for each relation, TransD con-
sturcts two mapping matrices dynamically for
each triplet by setting a projection vector for each
entity and relation. In addition, TransD has no
matrix-vector multiplication operation which can
be replaced by vector operations. Without loss of
generality, we assume m ≥ n, the projected vec-
tors can be computed as follows:
</bodyText>
<equation confidence="0.999982">
h1 = Mrhh = hTp hrp + [ hT, 0T T (18)
t1 = Mrtt = tTp trp + [ tT, 0T ]T (19)
</equation>
<bodyText confidence="0.999743666666667">
Therefore, TransD has less calculation than
TransR/CTransR, which makes it train faster and
can be applied on large-scale knowledge graphs.
</bodyText>
<sectionHeader confidence="0.997483" genericHeader="method">
4 Experiments and Results Analysis
</sectionHeader>
<bodyText confidence="0.999982">
We evaluate our apporach on two tasks: triplets
classification and link prediction. Then we show
the experiments results and some analysis of them.
</bodyText>
<subsectionHeader confidence="0.992739">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9689659375">
Triplets classification and link prediction are im-
plemented on two popular knowledge graphs:
WordNet (Miller 1995) and Freebase (Bollacker
et al. 2008). WordNet is a large lexical knowledge
graph. Entities in WordNet are synonyms which
express distinct concepts. Relations in WordNet
are conceptual-semantic and lexical relations. In
this paper, we use two subsets of WordNet: WN11
(Socher et al. 2013) and WN18 (Bordes et al.
2014). Freebase is a large collaborative knowl-
edge base consists of a large number of the world
facts, such as triplets (anthony asquith, location,
london) and (nobuko otowa, profession, actor).
We also use two subsets of Freebase: FB15k (Bor-
des et al. 2014) and FB13 (Socher et al. 2013).
Table 2 lists statistics of the 4 datasets.
</bodyText>
<table confidence="0.9986114">
Dataset #Rel #Ent #Train #Valid #Test
WN11 11 38,696 112,581 2,609 10,544
WN18 18 40,943 141,442 5,000 5,000
FB13 13 75,043 316,232 5908 23,733
FB15k 1,345 14,951 483,142 50,000 59,071
</table>
<tableCaption confidence="0.999219">
Table 2: Datesets used in the experiments.
</tableCaption>
<page confidence="0.997574">
691
</page>
<subsectionHeader confidence="0.901174">
4.2 Triplets Classification
</subsectionHeader>
<bodyText confidence="0.99985672">
Triplets classification aims to judge whether a
given triplet (h, r, t) is correct or not, which is a
binary classification task. Previous work (Socher
et al. 2013; Wang et al. 2014; Lin et al. 2015)
had explored this task. In this paper ,we use three
datasets WN11, FB13 and FB15k to evaluate our
approach. The test sets of WN11 and FB13 pro-
vided by (Socher et al. 2013) contain golden and
negative triplets. As to FB15k, its test set only
contains correct triplets, which requires us to con-
struct negative triplets. In this parper, we construct
negative triplets following the same setting used
for FB13 (Socher et al. 2013).
For triplets classification, we set a threshold 6r
for each relation r. 6r is obtained by maximizing
the classification accuracies on the valid set. For a
given triplet (h, r, t), if its score is larger than 6r,
it will be classified as positive, otherwise negative.
We compare our model with several previous
embedding models presented in Related Work sec-
tion. As we construct negative triplets for FB15k
by ourselves, we use the codes of TransE, TransH
and TransR/CTransR provied by (Lin et al. 2015)
to evaluate the datasets instead of reporting the re-
sults of (Wang et al.2014; Lin et al. 2015) directly.
In this experiment, we optimize the objective
with ADADELTA SGD (Zeiler 2012). We select
the margin -y among 11, 2, 5, 101, the dimen-
sion of entity vectors m and the dimension of re-
lation vectors n among 120, 50, 80, 1001, and
the mini-batch size B among 1100, 200, 1000,
48001. The best configuration obtained by valid
set are:-y = 1, m, n = 100, B = 1000 and tak-
ing L2 as dissimilarity on WN11; -y = 1, m, n =
100, B = 200 and taking L2 as dissimilarity on
FB13; -y = 2, m, n = 100, B = 4800 and tak-
ing L1 as dissimilarity on FB15k. For all the
three datasets, We traverse to training for 1000
rounds. As described in Related Work section,
TransD trains much faster than TransR (On our
PC, TransR needs 70 seconds and TransD merely
spends 24 seconds a round on FB15k).
Table 3 shows the evaluation results of triplets
classification. On WN11, we found that there are
570 entities appearing in valid and test sets but
not appearing in train set, we call them ”NULL
Entity”. In valid and test sets, there are 1680
(6.4%) triplets containing ”NULL Entity”. In
NTN(+E), these entity embeddings can be ob-
tained by word embedding. In TransD, how-
</bodyText>
<table confidence="0.9998983125">
Data sets WN11 FB13 FB15K
SE 53.0 75.2 -
SME(bilinear) 70.0 63.7 -
SLM 69.9 85.3 -
LFM 73.8 84.3 -
NTN 70.4 87.1 68.2
NTN(+E) 86.2 90.0 -
TransE(unif) 75.9 70.9 77.3
TransE(bern) 75.9 81.5 79.8
TransH(unif) 77.7 76.5 74.2
TransH(bern) 78.8 83.3 79.9
TransR(unif) 85.5 74.7 81.1
TransR(bern) 85.9 82.5 82.1
CTransR(bern) 85.7 - 84.3
TransD(unif) 85.6 85.9 86.4
TransD(bern) 86.4 89.1 88.0
</table>
<tableCaption confidence="0.894321">
Table 3: Experimental results of Triplets Classifi-
cation(%). “+E” means that the results are com-
bined with word embedding.
</tableCaption>
<bodyText confidence="0.996078470588235">
ever, they are only initialized randomly. There-
fore, it is not fair for TransD, but we also achieve
the accuracy 86.4% which is higher than that of
NTN(+E) (86.2%). From Table 3, we can con-
clude that: (1) On WN11, TransD outperforms any
other previous models including TransE, TransH
and TransR/CTransR, especially NTN(+E); (2)
On FB13, the classification accuracy of TransD
achieves 89.1%, which is significantly higher than
that of TransE, TransH and TransR/CTransR and
is near to the performance of NTN(+E) (90.0%);
and (3) Under most circumstances, the ”bern”
sampling method works better than ”unif”.
Figure 3 shows the prediction accuracy of dif-
ferent relations. On the three datasets, different
relations have different prediction accuracy: some
are higher and the others are lower. Here we fo-
cus on the relations which have lower accuracy.
On WN11, the relation similar to obtains accuracy
51%, which is near to random prediction accuracy.
In the view of intuition, similar to can be inferred
from other information. However, the number of
entity pairs linked by relation similar to is only
1672, which accounts for 1.5% in all train data,
and prediction of the relation needs much infor-
mation about entities. Therefore, the insufficient
of train data is the main cause. On FB13, the
accuracies of relations cuase of death and gender
are lower than that of other relations because they
are difficult to infer from other imformation, espe-
cially cuase of death. Relation gender may be in-
ferred from a person’s name (Socher et al. 2013),
but we learn a vector for each name, not for the
words included in the names, which makes the
</bodyText>
<page confidence="0.985334">
692
</page>
<figure confidence="0.998418571428572">
100
95
90
85
80
75
70
65
60
55
50
45
50 60 70 80 90 100
subordinate_instance_of
synset_domain_topic
member_meronym
member_holonym
domain_regio
domain_topic
has_instance
similar_to
has_part
type_of
part_of
50 60 70 80 90 100
WN11
unif
bern
cause_of_death
profession
nationality
institution
ethnicity
religion
gender
50 60 70 80 90 100
FB13
unif
bern
l���r����%7 af ���if�
FB15K
Accuracy(%) Accuracy(%) Accuracy(%) of &amp;quot;bern&amp;quot;
</figure>
<figureCaption confidence="0.723344">
Figure 3: Classification accuracies of different relations on the three datasets. For FB15k, each triangle
represent a relation, in which the red triangles represent the relations whose accuracies of “bern” or
“unif” are lower than 50% and the blacks are higher than 50%. The red line represents the function
y = x. We can see that the most relations are in the lower part of the red line.
</figureCaption>
<bodyText confidence="0.999839428571429">
names information useless for gender. On FB15k,
accuracies of some relations are lower than 50%,
for which some are lack of train data and some are
difficult to infer. Hence, the ability of reasoning
new facts based on knowledge graphs is under a
certain limitation, and a complementary approach
is to extract facts from plain texts.
</bodyText>
<subsectionHeader confidence="0.999511">
4.3 Link Prediction
</subsectionHeader>
<bodyText confidence="0.999876065573771">
Link prediction is to predict the missing h or t for
a golden triplet (h, r, t). In this task, we remove
the head or tail entity and then replace it with all
the entities in dictionary in turn for each triplet in
test set. We first compute scores of those corrupted
triplets and then rank them by descending order;
the rank of the correct entity is finally stored. The
task emphasizes the rank of the correct entity in-
stead of only finding the best one entity. Simi-
lar to (Bordes et al. 2013), we report two mea-
sures as our evaluation metrics: the average rank
of all correct entites (Mean Rank) and the propor-
tion of correct entities ranked in top 10 (Hits@10).
A lower Mean Rank and a higher Hits@10 should
be achieved by a good embedding model. We call
the evaluation setting ”Raw’. Noting the fact that
a corrupted triplet may also exist in knowledge
graphs, the corrupted triplet should be regard as
a correct triplet. Hence, we should remove the
corrupted triplets included in train, valid and test
sets before ranking. We call this evaluation setting
”Filter”. In this paper, we will report evaluation
results of the two settings .
In this task, we use two datasets: WN18 and
FB15k. As all the data sets are the same, we
refer to their experimental results in this paper.
On WN18, we also use ADADELTA SGD (Zeiler
2012) for optimization. We select the margin -y
among 10.1, 0.5, 1, 21, the dimension of entity
vectors m and the dimension of relation vectors n
among 120, 50, 80, 1001, and the mini-batch size
B among 1100, 200, 1000, 14001. The best con-
figuration obtained by valid set are:-y = 1, m, n =
50, B = 200 and taking L2 as dissimilarity. For
both the two datasets, We traverse to training for
1000 rounds.
Experimental results on both WN18 and FB15k
are shown in Table 4. From Table 4, we can
conclude that: (1) TransD outperforms other
baseline embedding models (TransE, TransH and
TransR/CTransR), especially on sparse dataset,
i.e., FB15k; (2) Compared with CTransR, TransD
is a more fine-grained model which considers the
multiple types of entities and relations simultane-
ously, and it achieves a better performance. It in-
dicates that TransD handles complicated internal
correlations of entities and relations in knowledge
graphs better than CTransR; (3) The “bern” sam-
pling trick can reduce false negative labels than
“unif”.
For the comparison of Hits@10 of different
kinds of relations, Table 5 shows the detailed
results by mapping properties of relations1 on
FB15k. From Table 5, we can see that TransD
outperforms TransE, TransH and TransR/CTransR
significantly in both “unif” and “bern” settings.
TransD achieves better performance than CTransR
in all types of relations (1-to-1, 1-to-N, N-to-1 and
N-to-N). For N-to-N relations in predicting both
head and tail, our approach improves the Hits@10
by almost 7.4% than CTransR. In particular, for
</bodyText>
<footnote confidence="0.53274">
1Mapping properties of relations follows the same rules in
(Bordes et al. 2013)
</footnote>
<page confidence="0.995931">
693
</page>
<note confidence="0.9583886875">
Data sets WN18 FB15K
Metric Mean Rank Hits@10 Mean Rank Hits@10
Raw Filt Raw Filt Raw Filt Raw Filt
Unstructured (Bordes et al. 2012) 315 304 35.3 38.2 1,074 979 4.5 6.3
RESCAL (Nickle, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 828 683 28.4 44.1
SE (Bordes et al. 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8
SME (linear) (Bordes et al.2012) 545 533 65.1 74.1 274 154 30.7 40.8
SME (Bilinear) (Bordes et al. 2012) 526 509 54.7 61.3 284 158 31.3 41.3
LFM (Jenatton et al. 2012) 469 456 71.4 81.6 283 164 26.0 33.1
TransE (Bordes et al. 2013) 263 251 75.4 89.2 243 125 34.9 47.1
TransH (unif) (Wang et al. 2014) 318 303 75.4 86.7 211 84 42.5 58.5
TransH (bern) (Wang et al. 2014) 401 388 73.0 82.3 212 87 45.7 64.4
TransR (unif) (Lin et al. 2015) 232 219 78.3 91.7 226 78 43.8 65.5
TransR (bern) (Lin et al. 2015) 238 225 79.8 92.0 198 77 48.2 68.7
CTransR (unif) (Lin et al. 2015) 243 230 78.9 92.3 233 82 44.0 66.3
CTransR (bern) (Lin et al. 2015) 231 218 79.4 92.3 199 75 48.4 70.2
</note>
<table confidence="0.7193585">
TransD (unif) 242 229 79.2 92.5 211 67 49.4 74.2
TransD (bern) 224 212 79.6 92.2 194 91 53.4 77.3
</table>
<tableCaption confidence="0.993699">
Table 4: Experimental results on link prediction.
</tableCaption>
<table confidence="0.9310425">
Tasks Prediction Head (Hits@10) Prediction Tail (Hits@10)
Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N
</table>
<note confidence="0.983854636363636">
Unstructured (Bordes et al. 2012) 34.5 2.5 6.1 6.6 34.3 4.2 1.9 6.6
SE (Bordes et al. 2011) 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
SME (linear) (Bordes et al.2012) 35.1 53.7 19.0 40.3 32.7 14.9 61.6 43.3
SME (Bilinear) (Bordes et al. 2012) 30.9 69.6 19.9 38.6 28.2 13.1 76.0 41.8
TransE (Bordes et al. 2013) 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransH (unif) (Wang et al. 2014) 66.7 81.7 30.2 57.4 63.7 30.1 83.2 60.8
TransH (bern) (Wang et al. 2014) 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
TransR (unif) (Lin et al. 2015) 76.9 77.9 38.1 66.9 76.2 38.4 76.2 69.1
TransR (bern) (Lin et al. 2015) 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
CTransR (unif) (Lin et al. 2015) 78.6 77.8 36.4 68.0 77.4 37.8 78.0 70.3
CTransR (bern) (Lin et al. 2015) 81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8
</note>
<table confidence="0.996896">
TransD (unif) 80.7 85.8 47.1 75.6 80.0 54.5 80.7 77.9
TransD (bern) 86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2
</table>
<tableCaption confidence="0.999605">
Table 5: Experimental results on FB15K by mapping properities of relations (%).
</tableCaption>
<bodyText confidence="0.998917875">
N-to-1 relations (predicting head) and 1-to-N rela-
tions (predicting tail), TransD improves the accu-
racy by 9.0% and 14.7% compared with previous
state-of-the-art results, respectively. Therefore,
the diversity of entities and relations in knowl-
edge grahps is an important factor and the dynamic
mapping matrix is suitable for modeling knowl-
edge graphs.
</bodyText>
<sectionHeader confidence="0.920028" genericHeader="method">
5 Properties of Projection Vectors
</sectionHeader>
<bodyText confidence="0.999991944444445">
As mentioned in Section ”Introduction”, TransD
is based on the motivation that each mapping ma-
trix is determined by entity-relation pair dynam-
ically. These mapping matrices are constructed
with projection vectors of entities and relations.
Here, we analysis the properties of projection vec-
tors. We seek the similar objects (entities and rela-
tions) for a given object (entities and relations) by
projection vectors. As WN18 has the most enti-
ties (40,943 entities which contains various types
of words. FB13 also has many entities, but the
most are person’s names) and FB15k has the most
relations (1,345 relations), we show the similarity
of projection vectors on them. Table 6 and 7 show
that the same category objects have similar projec-
tion vectors. The similarity of projection vectors
of different types of entities and relations indicates
the rationality of our method.
</bodyText>
<sectionHeader confidence="0.998087" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999902833333333">
We introduced a model TransD that embed knowl-
edge graphs into continues vector space for their
completion. TransD has less complexity and more
flexibility than TransR/CTransR. When learning
embeddings of named symbol objects (entities or
relations), TransD considers the diversity of them
both. Extensive experiments show that TransD
outperforms TrasnE, TransH and TransR/CTransR
on two tasks including triplets classification and
link prediction.
As shown in Triplets Classification section, not
all new facts can be deduced from the exist-
</bodyText>
<page confidence="0.996052">
694
</page>
<table confidence="0.998683285714286">
Datesets WN18
Entities and Definitions upset VB 4 cause to overturn from an upright or srbija NN 1 a historical region in central and
normal position northern Yugoslavia
sway VB 4 cause to move back and forth montenegro NN 1 a former country bordering on the
Adriatic Sea
shift VB 2 change place or direction constantina NN 1 a Romanian resort city on the Black
Sea
Similar Entities and flap VB 3 move with a thrashing motion lappland NN 1 a region in northmost Europe inhab-
Definitions ited by Lapps
fluctuate VB 1 cause to fluctuate or move in a wave- plattensee NN 1 a large shallow lake in western Hun-
like pattern gary
leaner NN 1 (horseshoes) the throw of a horse- brasov NN 1 a city in central Romania in the
shoe so as to lean against (but not en- foothills of the Transylvanian Alps
circle) the stake
</table>
<tableCaption confidence="0.9883635">
Table 6: Entity projection vectors similarity (in descending order) computed on WN18. The similarity
scores are computed with cosine function.
</tableCaption>
<table confidence="0.999385153846154">
Datesets FB15k
Relation /location/statistical region/rent50 2./measurement unit/dated money value/currency
Similar relations /location/statistical region/rent50 3./measurement unit/dated money value/currency
/location/statistical region/rent50 1./measurement unit/dated money value/currency
/location/statistical region/rent50 4./measurement unit/dated money value/currency
/location/statistical region/rent50 0./measurement unit/dated money value/currency
/location/statistical region/gdp nominal./measurement unit/dated money value/currency
Relation /sports/sports team/roster./soccer/football roster position/player
Similar relations /soccer/football team/current roster./sports/sports team roster/player
/soccer/football team/current roster./soccer/football roster position/player
/sports/sports team/roster./sports/sports team roster/player
/basketball/basketball team/historical roster./sports/sports team roster/player
/sports/sports team/roster./basketball/basketball historical roster position/player
</table>
<tableCaption confidence="0.989289">
Table 7: Relation projection vectors similarity computed on FB15k. The similarity scores are computed
</tableCaption>
<bodyText confidence="0.972900538461538">
with cosine function.
ing triplets in knowledge graphs, such as rela-
tions gender, place of place, parents and chil-
dren. These relations are difficult to infer from all
other information, but they are also useful resource
for practical applications and incomplete, i.e. the
place of birth attribute is missing for 71% of all
people included in FreeBase (Nickel, et al. 2015).
One possible way to obtain these new triplets is
to extract facts from plain texts. We will seek
methods to complete knowledge graphs with new
triplets whose entities and relations come from
plain texts.
</bodyText>
<sectionHeader confidence="0.998239" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99880075">
This work was supported by the National Basic
Research Program of China (No. 2014CB340503)
and the National Natural Science Foundation of
China (No. 61272332 and No. 61202329).
</bodyText>
<sectionHeader confidence="0.999215" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998034">
George A. Miller. 1995. WordNet: A lexical
database for english. Communications of the ACM,
38(11):39-41.
Bollacker K., Evans C., Paritosh P., Sturge T., and Tay-
lor J. 2008. Freebase: A collaboratively created
graph database for structuring human knowledge.
In Proceedings of the 2008 ACM SIGMOD Inter-
national Conference on Management of Data. pages
1247-1250.
Fabian M. Suchanek, Kasneci G., Weikum G. 2007.
YAGO: A core of semantic Knowledge Unifying
WordNet and Wikipedia. In Proceedings of the 16th
international conference on World Wide Web.
Bordes A., Usunier N., Garcia-Dur´an A. 2013. Trans-
lating Embeddings for Modeling Multi-relational
Data. In Proceedings of NIPS. pags:2787-2795.
Wang Z., Zhang J., Feng J. and Chen Z. 2014.
Knowledge graph embedding by translating on hy-
perplanes. In Proceedings of AAAI. pags:1112-
1119.
Lin Y., Zhang J., Liu Z., Sun M., Liu Y., Zhu X.
2015. Learning Entity and Relation Embeddings for
Knowledge Graph Completion. In Proceedings of
AAAI.
Bordes A., Glorot X., Weston J., and Bengio Y. 2012.
Joint learning of words and meaning representations
for open-text semantic parsing. In Proceedings of
AISTATS. pags:127-135.
Bordes A., Glorot X., Weston J., and Bengio Y.
2014. A semantic matching energy function for lear-
ing with multirelational data. Machine Learning.
94(2):pags:233-259.
Bordes A., Weston J., Collobert R., and Bengio Y.
2011. Learning structured embeddings of knowl-
edge bases. In Proceedings of AAAI. pags:301-306.
</reference>
<page confidence="0.984692">
695
</page>
<reference confidence="0.999928615384615">
Jenatton R., Nicolas L. Roux, Bordes A., and Oboz-
inaki G. 2012. A latent factor model for highly
multi-relational data. In Proceedings of NIPS.
pags:3167-3175.
Sutskever I., Salakhutdinov R. and Joshua B. Tenen-
baum. 2009. Modeling Relational Data using
Bayesian Clustered Tensor Factorization. In Pro-
ceedings of NIPS. pags:1821-1828.
Socher R., Chen D., Christopher D. Manning and An-
drew Y. Ng. 2013. Reasoning With Neural Tensor
Networks for Knowledge Base Completion. In Pro-
ceedings of NIPS. pags:926-934.
Weston J., Bordes A., Yakhnenko O. Manning and Un-
unier N. 2013. Connecting language and knowledge
bases with embedding models for relation extrac-
tion. In Proceedings of EMNLP. pags:1366-1371.
Matthew D. Zeiler. 2012. ADADELTA: AN ADAP-
TIVE LEARNING RATE METHOD. In Proceed-
ings of CVPR.
Socher R., Huval B., Christopher D Manning. Manning
and Andrew Y. Ng. 2012. Semantic Compositional-
ity through Recursive Matrix-vector Spaces. In Pro-
ceedings of EMNLP.
Nickel M., Tresp V., Kriegel H-P. 2011. A three-
way model for collective learning on multi-relational
data. In Proceedings of ICML. pages:809-816.
Nickel M., Tresp V., Kriegel H-P. 2012. Factorizing
YAGO: Scalable Machine Learning for Linked Data.
In Proceedings of WWW.
Nickel M., Tresp V. 2013a. An Analysis of Ten-
sor Models for Learning from Structured Data.
Machine Learning and Knowledge Discovery in
Databases, Springer.
Nickel M., Tresp V. 2013b. Tensor Factorization for
Multi-Relational Learning. Machine Learning and
Knowledge Discovery in Databases, Springer.
Nickel M., Murphy K., Tresp V., Gabrilovich E.
2015. A Review of Relational Machine Learning
for Knowledge Graphs. In Proceedings of IEEE.
</reference>
<page confidence="0.998779">
696
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.582561">
<title confidence="0.994701">Knowledge Graph Embedding via Dynamic Mapping Matrix</title>
<author confidence="0.944415">Guoliang Ji</author>
<author confidence="0.944415">Shizhu He</author>
<author confidence="0.944415">Liheng Xu</author>
<author confidence="0.944415">Kang Liu</author>
<author confidence="0.944415">Jun</author>
<affiliation confidence="0.7936925">National Laboratory of Pattern Recognition Institute of Automation Chinese Academy of Sciences, Beijing, 100190,</affiliation>
<abstract confidence="0.998378777777778">Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--11</pages>
<contexts>
<context position="1392" citStr="Miller 1995" startWordPosition="205" endWordPosition="206">first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-art methods. 1 Introduction Knowledge Graphs such as WordNet (Miller 1995), Freebase (Bollacker et al. 2008) and Yago (Suchanek et al. 2007) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&amp;A), etc. They usually contain huge amounts of structured data as the form of triplets (head entity, relation, tail entity)(denoted as (h, r, t)), where relation models the relationship between the two entities. As most knowledge graphs have been built either collaboratively or (partly) automatically, they often suffer from incompleteness. Knowledge graph completion is to predict relations between entities based on exi</context>
<context position="18859" citStr="Miller 1995" startWordPosition="3260" endWordPosition="3261">s of generality, we assume m ≥ n, the projected vectors can be computed as follows: h1 = Mrhh = hTp hrp + [ hT, 0T T (18) t1 = Mrtt = tTp trp + [ tT, 0T ]T (19) Therefore, TransD has less calculation than TransR/CTransR, which makes it train faster and can be applied on large-scale knowledge graphs. 4 Experiments and Results Analysis We evaluate our apporach on two tasks: triplets classification and link prediction. Then we show the experiments results and some analysis of them. 4.1 Data Sets Triplets classification and link prediction are implemented on two popular knowledge graphs: WordNet (Miller 1995) and Freebase (Bollacker et al. 2008). WordNet is a large lexical knowledge graph. Entities in WordNet are synonyms which express distinct concepts. Relations in WordNet are conceptual-semantic and lexical relations. In this paper, we use two subsets of WordNet: WN11 (Socher et al. 2013) and WN18 (Bordes et al. 2014). Freebase is a large collaborative knowledge base consists of a large number of the world facts, such as triplets (anthony asquith, location, london) and (nobuko otowa, profession, actor). We also use two subsets of Freebase: FB15k (Bordes et al. 2014) and FB13 (Socher et al. 2013</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for english. Communications of the ACM, 38(11):39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bollacker</author>
<author>C Evans</author>
<author>P Paritosh</author>
<author>T Sturge</author>
<author>J Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<contexts>
<context position="1426" citStr="Bollacker et al. 2008" startWordPosition="208" endWordPosition="211"> meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-art methods. 1 Introduction Knowledge Graphs such as WordNet (Miller 1995), Freebase (Bollacker et al. 2008) and Yago (Suchanek et al. 2007) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&amp;A), etc. They usually contain huge amounts of structured data as the form of triplets (head entity, relation, tail entity)(denoted as (h, r, t)), where relation models the relationship between the two entities. As most knowledge graphs have been built either collaboratively or (partly) automatically, they often suffer from incompleteness. Knowledge graph completion is to predict relations between entities based on existing triplets in a knowledge grap</context>
<context position="18896" citStr="Bollacker et al. 2008" startWordPosition="3264" endWordPosition="3267">m ≥ n, the projected vectors can be computed as follows: h1 = Mrhh = hTp hrp + [ hT, 0T T (18) t1 = Mrtt = tTp trp + [ tT, 0T ]T (19) Therefore, TransD has less calculation than TransR/CTransR, which makes it train faster and can be applied on large-scale knowledge graphs. 4 Experiments and Results Analysis We evaluate our apporach on two tasks: triplets classification and link prediction. Then we show the experiments results and some analysis of them. 4.1 Data Sets Triplets classification and link prediction are implemented on two popular knowledge graphs: WordNet (Miller 1995) and Freebase (Bollacker et al. 2008). WordNet is a large lexical knowledge graph. Entities in WordNet are synonyms which express distinct concepts. Relations in WordNet are conceptual-semantic and lexical relations. In this paper, we use two subsets of WordNet: WN11 (Socher et al. 2013) and WN18 (Bordes et al. 2014). Freebase is a large collaborative knowledge base consists of a large number of the world facts, such as triplets (anthony asquith, location, london) and (nobuko otowa, profession, actor). We also use two subsets of Freebase: FB15k (Bordes et al. 2014) and FB13 (Socher et al. 2013). Table 2 lists statistics of the 4 </context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Bollacker K., Evans C., Paritosh P., Sturge T., and Taylor J. 2008. Freebase: A collaboratively created graph database for structuring human knowledge.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data.</booktitle>
<pages>1247--1250</pages>
<marker></marker>
<rawString>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. pages 1247-1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>G Kasneci</author>
<author>G Weikum</author>
</authors>
<title>YAGO: A core of semantic Knowledge Unifying WordNet and Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web.</booktitle>
<contexts>
<context position="1458" citStr="Suchanek et al. 2007" startWordPosition="214" endWordPosition="217">), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-art methods. 1 Introduction Knowledge Graphs such as WordNet (Miller 1995), Freebase (Bollacker et al. 2008) and Yago (Suchanek et al. 2007) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&amp;A), etc. They usually contain huge amounts of structured data as the form of triplets (head entity, relation, tail entity)(denoted as (h, r, t)), where relation models the relationship between the two entities. As most knowledge graphs have been built either collaboratively or (partly) automatically, they often suffer from incompleteness. Knowledge graph completion is to predict relations between entities based on existing triplets in a knowledge graph. In the past decade, much work</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Kasneci G., Weikum G. 2007. YAGO: A core of semantic Knowledge Unifying WordNet and Wikipedia. In Proceedings of the 16th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bordes</author>
<author>N Usunier</author>
<author>A Garcia-Dur´an</author>
</authors>
<title>Translating Embeddings for Modeling Multi-relational Data.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<pages>2787--2795</pages>
<marker>Bordes, Usunier, Garcia-Dur´an, 2013</marker>
<rawString>Bordes A., Usunier N., Garcia-Dur´an A. 2013. Translating Embeddings for Modeling Multi-relational Data. In Proceedings of NIPS. pags:2787-2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wang</author>
<author>J Zhang</author>
<author>J Feng</author>
<author>Z Chen</author>
</authors>
<title>Knowledge graph embedding by translating on hyperplanes.</title>
<date>2014</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<pages>1112--1119</pages>
<contexts>
<context position="3125" citStr="Wang et al. 2014" startWordPosition="484" endWordPosition="487">ds, TransE (Bordes et al. 2013) is simple and effective, and also achieves state-of-the-art prediction performance. It learns low-dimensional embeddings for every entity and relation in knowledge graphs. These vector embeddings are denoted by the same letter in boldface. The basic idea is that every relation is regarded as translation in the embedding space. For a golden triplet (h, r, t), the embedding h is close to the embedding t by adding the embedding r, that is h + r ≈ t. TransE is suitable for 1-to-1 relations, but has flaws when dealing with 1-toN, N-to-1 and N-to-N relations. TransH (Wang et al. 2014) is proposed to solve these issues. TransH regards a relation as a translating operation on a relation-specific hyperplane, which is characterized by a norm vector wr and a translation vector dr. The embeddings h and t are first projected to the hyperplane of relation r to obtain vectors h1 = h − wr hwr and t1 = t − wr twr, and then h1 + dr ≈ t1. Both in TransE and TransH, the embeddings of entities and relations are in the same space. However, entities and relations are different types objects, it is insufficient to model them in the same space. TransR/CTransR (Lin et al. 2015) set a mapping </context>
<context position="7960" citStr="Wang et al. 2014" startWordPosition="1305" endWordPosition="1308">, t) that corresponds to a true fact in real world, it always get a relatively higher score, and lower for an negative triplet. Other notations will be described in the appropriate sections. 2.1 TransE, TransH and TransR/CTransR As mentioned in Introduction section, TransE (Bordes et al. 2013) regards the relation r as translation from h to t for a golden triplet (h, r, t). Hence, (h+r) is close to (t) and the score function is fr(h,t) = −kh + r − tk22 (1) TransE is only suitable for 1-to-1 relations, there remain flaws for 1-to-N, N-to-1 and N-to-N relations. To solve these problems, TransH (Wang et al. 2014) proposes an improved model named translation on a hyperplane. On hyperplanes of different relations, a given entity has different representations. Similar to TransE, TransH has the score function as follows: fr(h,t) = −kh⊥ + r − t⊥k22 (2) Entity Space —non apace 688 Model #Parameters # Operations (Time complexity) Unstructured (Bordes et al. 2012; 2014) O(Nem) O(Nt) SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt) SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM (Jenatto</context>
<context position="16643" citStr="Wang et al. 2014" startWordPosition="2874" endWordPosition="2877">ti)(i = 1, 2, ... , nt). Each triplet has a label yi to indicate the triplet is positive (yi = 1) or negative (yi = 0). Then the golden and negative triplets are denoted by A = {(hj, rj, tj) | yj = 1} and A0 = {(hj, rj, tj) |yj = 0}, respectively. Before training, one important trouble is that knowledge graphs only encode positive training triplets, they do not contain negative examples. Therefore, we obtain A from knowledge graphs and generate A0 as follows: A0 = {(hl, rk, tk) | hl =6 hk ∧ yk = 1} ∪ {(hk, rk, tl) |tl =6 tk ∧ yk = 1}. We also use two strategies “unif” and “bern” described in (Wang et al. 2014) to replace the head or tail entity. Let us use ξ and ξ0 to denote a golden triplet and a corresponding negative triplet, respectively. Then we define the following margin-based ranking loss as the objective for training: XL = X [γ + fr(ξ0) − fr(ξ)]+ (15) ξEΔ ξ0EΔ0 where [x]+ °= max (0, x), and γ is the margin separating golden triplets and negative triplets. The process of minimizing the above objective is carried out with stochastic gradient descent (SGD). In order to speed up the convergence and avoid overfitting, we initiate the entity and relation embeddings with the results of TransE and</context>
<context position="19947" citStr="Wang et al. 2014" startWordPosition="3437" endWordPosition="3440"> (nobuko otowa, profession, actor). We also use two subsets of Freebase: FB15k (Bordes et al. 2014) and FB13 (Socher et al. 2013). Table 2 lists statistics of the 4 datasets. Dataset #Rel #Ent #Train #Valid #Test WN11 11 38,696 112,581 2,609 10,544 WN18 18 40,943 141,442 5,000 5,000 FB13 13 75,043 316,232 5908 23,733 FB15k 1,345 14,951 483,142 50,000 59,071 Table 2: Datesets used in the experiments. 691 4.2 Triplets Classification Triplets classification aims to judge whether a given triplet (h, r, t) is correct or not, which is a binary classification task. Previous work (Socher et al. 2013; Wang et al. 2014; Lin et al. 2015) had explored this task. In this paper ,we use three datasets WN11, FB13 and FB15k to evaluate our approach. The test sets of WN11 and FB13 provided by (Socher et al. 2013) contain golden and negative triplets. As to FB15k, its test set only contains correct triplets, which requires us to construct negative triplets. In this parper, we construct negative triplets following the same setting used for FB13 (Socher et al. 2013). For triplets classification, we set a threshold 6r for each relation r. 6r is obtained by maximizing the classification accuracies on the valid set. For </context>
<context position="29034" citStr="Wang et al. 2014" startWordPosition="5005" endWordPosition="5008">693 Data sets WN18 FB15K Metric Mean Rank Hits@10 Mean Rank Hits@10 Raw Filt Raw Filt Raw Filt Raw Filt Unstructured (Bordes et al. 2012) 315 304 35.3 38.2 1,074 979 4.5 6.3 RESCAL (Nickle, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 828 683 28.4 44.1 SE (Bordes et al. 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8 SME (linear) (Bordes et al.2012) 545 533 65.1 74.1 274 154 30.7 40.8 SME (Bilinear) (Bordes et al. 2012) 526 509 54.7 61.3 284 158 31.3 41.3 LFM (Jenatton et al. 2012) 469 456 71.4 81.6 283 164 26.0 33.1 TransE (Bordes et al. 2013) 263 251 75.4 89.2 243 125 34.9 47.1 TransH (unif) (Wang et al. 2014) 318 303 75.4 86.7 211 84 42.5 58.5 TransH (bern) (Wang et al. 2014) 401 388 73.0 82.3 212 87 45.7 64.4 TransR (unif) (Lin et al. 2015) 232 219 78.3 91.7 226 78 43.8 65.5 TransR (bern) (Lin et al. 2015) 238 225 79.8 92.0 198 77 48.2 68.7 CTransR (unif) (Lin et al. 2015) 243 230 78.9 92.3 233 82 44.0 66.3 CTransR (bern) (Lin et al. 2015) 231 218 79.4 92.3 199 75 48.4 70.2 TransD (unif) 242 229 79.2 92.5 211 67 49.4 74.2 TransD (bern) 224 212 79.6 92.2 194 91 53.4 77.3 Table 4: Experimental results on link prediction. Tasks Prediction Head (Hits@10) Prediction Tail (Hits@10) Relation Category 1-</context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Wang Z., Zhang J., Feng J. and Chen Z. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of AAAI. pags:1112-1119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lin</author>
<author>J Zhang</author>
<author>Z Liu</author>
<author>M Sun</author>
<author>Y Liu</author>
<author>X Zhu</author>
</authors>
<title>Learning Entity and Relation Embeddings for Knowledge Graph Completion.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="3710" citStr="Lin et al. 2015" startWordPosition="596" endWordPosition="599">ions. TransH (Wang et al. 2014) is proposed to solve these issues. TransH regards a relation as a translating operation on a relation-specific hyperplane, which is characterized by a norm vector wr and a translation vector dr. The embeddings h and t are first projected to the hyperplane of relation r to obtain vectors h1 = h − wr hwr and t1 = t − wr twr, and then h1 + dr ≈ t1. Both in TransE and TransH, the embeddings of entities and relations are in the same space. However, entities and relations are different types objects, it is insufficient to model them in the same space. TransR/CTransR (Lin et al. 2015) set a mapping matrix Mr and a vector r for every relation r. In TransR, h and t are projected to the aspects that relation r focuses on through the ma687 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 687–696, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Simple illustration of TransD. Each shape represents an entity pair appearing in a triplet of relation r. Mrh and Mrt are mapping matrices of h and t, respectively. hip, tip(</context>
<context position="8922" citStr="Lin et al. 2015" startWordPosition="1489" endWordPosition="1492">012; 2014) O(Nem) O(Nt) SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt) SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM (Jenatton et al. 2012; Sutskever et al. 2009) O(Nem + Nrn2)(m = n) O((m2 + m)Nt) SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt) NTN (Socher et al. 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2 + m)s + 2mk + k)Nt) TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt) TransH (Wang et al. 2014) O(Nem + 2Nrn)(m = n) O(2mNt) TransR (Lin et al. 2015) O(Nem + Nr(m + 1)n) O(2mnNt) CTransR (Lin et al. 2015) O(Nem + Nr(m + d)n) O(2mnNt) TransD (this paper) O(2Nem + 2Nrn) O(2nNt) Table 1: Complexity (the number of parameters and the number of multiplication operations in an epoch) of several embedding models. Ne and Nr represent the number of entities and relations, respectively. Nt represents the number of triplets in a knowledge graph. m is the dimension of entity embedding space and n is the dimension of relation embedding space. d denotes the average number of clusters of a relation. k is the number of hidden nodes of a neural network and </context>
<context position="13085" citStr="Lin et al. 2015" startWordPosition="2235" endWordPosition="2238">is tanh operation. NTN is the most expressive model so far, but it has so many parameters that it is difficult to scale up to large knowledge graphs. Table 1 lists the complexity of all the above models. The complexity (especially for time) of TransD is much less than TransR/CTransR and is similar to TransE and TransH. Therefore, TransD is effective and train faster than TransR/CTransR. Beyond these embedding models, there is other related work of modeling multi-relational data, such as matrix factorization, recommendations, etc. In experiments, we refer to the results of RESCAL presented in (Lin et al. 2015) and compare with it. 3 Our Method We first define notations. Triplets are represented as (hi, ri, ti)(i = 1, 2, ... , nt), where hi denotes a head entity, ti denotes a tail entity and ri denotes a relation. Their embeddings are denoted by hi, ri, ti(i = 1, 2,... , nt). We use A to represent golden triplets set, and use A, to denote negative triplets set. Entities set and relations set are denoted by E and R, respectively. We use Imxn to denote the identity matrix of size m x n. 3.1 Multiple Types of Entities and Relations Considering the diversity of relations, CTransR segments triplets of a </context>
<context position="19965" citStr="Lin et al. 2015" startWordPosition="3441" endWordPosition="3444">ofession, actor). We also use two subsets of Freebase: FB15k (Bordes et al. 2014) and FB13 (Socher et al. 2013). Table 2 lists statistics of the 4 datasets. Dataset #Rel #Ent #Train #Valid #Test WN11 11 38,696 112,581 2,609 10,544 WN18 18 40,943 141,442 5,000 5,000 FB13 13 75,043 316,232 5908 23,733 FB15k 1,345 14,951 483,142 50,000 59,071 Table 2: Datesets used in the experiments. 691 4.2 Triplets Classification Triplets classification aims to judge whether a given triplet (h, r, t) is correct or not, which is a binary classification task. Previous work (Socher et al. 2013; Wang et al. 2014; Lin et al. 2015) had explored this task. In this paper ,we use three datasets WN11, FB13 and FB15k to evaluate our approach. The test sets of WN11 and FB13 provided by (Socher et al. 2013) contain golden and negative triplets. As to FB15k, its test set only contains correct triplets, which requires us to construct negative triplets. In this parper, we construct negative triplets following the same setting used for FB13 (Socher et al. 2013). For triplets classification, we set a threshold 6r for each relation r. 6r is obtained by maximizing the classification accuracies on the valid set. For a given triplet (h</context>
<context position="29169" citStr="Lin et al. 2015" startWordPosition="5033" endWordPosition="5036">) 315 304 35.3 38.2 1,074 979 4.5 6.3 RESCAL (Nickle, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 828 683 28.4 44.1 SE (Bordes et al. 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8 SME (linear) (Bordes et al.2012) 545 533 65.1 74.1 274 154 30.7 40.8 SME (Bilinear) (Bordes et al. 2012) 526 509 54.7 61.3 284 158 31.3 41.3 LFM (Jenatton et al. 2012) 469 456 71.4 81.6 283 164 26.0 33.1 TransE (Bordes et al. 2013) 263 251 75.4 89.2 243 125 34.9 47.1 TransH (unif) (Wang et al. 2014) 318 303 75.4 86.7 211 84 42.5 58.5 TransH (bern) (Wang et al. 2014) 401 388 73.0 82.3 212 87 45.7 64.4 TransR (unif) (Lin et al. 2015) 232 219 78.3 91.7 226 78 43.8 65.5 TransR (bern) (Lin et al. 2015) 238 225 79.8 92.0 198 77 48.2 68.7 CTransR (unif) (Lin et al. 2015) 243 230 78.9 92.3 233 82 44.0 66.3 CTransR (bern) (Lin et al. 2015) 231 218 79.4 92.3 199 75 48.4 70.2 TransD (unif) 242 229 79.2 92.5 211 67 49.4 74.2 TransD (bern) 224 212 79.6 92.2 194 91 53.4 77.3 Table 4: Experimental results on link prediction. Tasks Prediction Head (Hits@10) Prediction Tail (Hits@10) Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N Unstructured (Bordes et al. 2012) 34.5 2.5 6.1 6.6 34.3 4.2 1.9 6.6 SE (Bordes et</context>
<context position="30432" citStr="Lin et al. 2015" startWordPosition="5269" endWordPosition="5272">1.3 SME (linear) (Bordes et al.2012) 35.1 53.7 19.0 40.3 32.7 14.9 61.6 43.3 SME (Bilinear) (Bordes et al. 2012) 30.9 69.6 19.9 38.6 28.2 13.1 76.0 41.8 TransE (Bordes et al. 2013) 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0 TransH (unif) (Wang et al. 2014) 66.7 81.7 30.2 57.4 63.7 30.1 83.2 60.8 TransH (bern) (Wang et al. 2014) 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2 TransR (unif) (Lin et al. 2015) 76.9 77.9 38.1 66.9 76.2 38.4 76.2 69.1 TransR (bern) (Lin et al. 2015) 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1 CTransR (unif) (Lin et al. 2015) 78.6 77.8 36.4 68.0 77.4 37.8 78.0 70.3 CTransR (bern) (Lin et al. 2015) 81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8 TransD (unif) 80.7 85.8 47.1 75.6 80.0 54.5 80.7 77.9 TransD (bern) 86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2 Table 5: Experimental results on FB15K by mapping properities of relations (%). N-to-1 relations (predicting head) and 1-to-N relations (predicting tail), TransD improves the accuracy by 9.0% and 14.7% compared with previous state-of-the-art results, respectively. Therefore, the diversity of entities and relations in knowledge grahps is an important factor and the dynamic mapping matrix is suitable for modeling knowledge graphs. 5 Properties of Pr</context>
</contexts>
<marker>Lin, Zhang, Liu, Sun, Liu, Zhu, 2015</marker>
<rawString>Lin Y., Zhang J., Liu Z., Sun M., Liu Y., Zhu X. 2015. Learning Entity and Relation Embeddings for Knowledge Graph Completion. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bordes</author>
<author>X Glorot</author>
<author>J Weston</author>
<author>Y Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of AISTATS.</booktitle>
<pages>127--135</pages>
<contexts>
<context position="8309" citStr="Bordes et al. 2012" startWordPosition="1362" endWordPosition="1365"> a golden triplet (h, r, t). Hence, (h+r) is close to (t) and the score function is fr(h,t) = −kh + r − tk22 (1) TransE is only suitable for 1-to-1 relations, there remain flaws for 1-to-N, N-to-1 and N-to-N relations. To solve these problems, TransH (Wang et al. 2014) proposes an improved model named translation on a hyperplane. On hyperplanes of different relations, a given entity has different representations. Similar to TransE, TransH has the score function as follows: fr(h,t) = −kh⊥ + r − t⊥k22 (2) Entity Space —non apace 688 Model #Parameters # Operations (Time complexity) Unstructured (Bordes et al. 2012; 2014) O(Nem) O(Nt) SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt) SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM (Jenatton et al. 2012; Sutskever et al. 2009) O(Nem + Nrn2)(m = n) O((m2 + m)Nt) SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt) NTN (Socher et al. 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2 + m)s + 2mk + k)Nt) TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt) TransH (Wang et al. 2014) O(Nem + 2Nrn)(m = n) O(2mNt) TransR (Lin</context>
<context position="10395" citStr="Bordes et al. 2012" startWordPosition="1757" endWordPosition="1760">rent types of objects, they should not be in the same vector space. TransR/CTransR (Lin et al. 2015) is proposed based on the idea. TransR set a mapping matrix Mr for each relation r to map entity embedding into relation vector space. Its score function is: fr(h, t) = −I IMrh + r − MrtI I22. (3) where Mr E Rm×n, h, t E Rn and r E Rm. CTransR is an extension of TransR. As head-tail entity pairs present various patterns in different relations, CTransR clusters diverse head-tail entity pairs into groups and sets a relation vector for each group. 2.2 Other Models Unstructured. Unstructured model (Bordes et al. 2012; 2014) ignores relations, only models entities as embeddings. The score function is fr(h,t) = −I Ih − tI I22. (4) It’s a simple case of TransE. Obviously, Unstructured model can not distinguish different relations. Structured Embedding (SE). SE model (Bordes et al. 2011) sets two separate matrices Mrh and Mrt to project head and tail entities for each relation. Its score function is defined as follows: fr(h,t) = −I IMrhh − MrttI I1 (5) Semantic Matching Energy (SME). SME model (Bordes et al. 2012; 2014) encodes each named symbolic object (entities and relations) as a vector. Its score functio</context>
<context position="28554" citStr="Bordes et al. 2012" startWordPosition="4911" endWordPosition="4914">relations1 on FB15k. From Table 5, we can see that TransD outperforms TransE, TransH and TransR/CTransR significantly in both “unif” and “bern” settings. TransD achieves better performance than CTransR in all types of relations (1-to-1, 1-to-N, N-to-1 and N-to-N). For N-to-N relations in predicting both head and tail, our approach improves the Hits@10 by almost 7.4% than CTransR. In particular, for 1Mapping properties of relations follows the same rules in (Bordes et al. 2013) 693 Data sets WN18 FB15K Metric Mean Rank Hits@10 Mean Rank Hits@10 Raw Filt Raw Filt Raw Filt Raw Filt Unstructured (Bordes et al. 2012) 315 304 35.3 38.2 1,074 979 4.5 6.3 RESCAL (Nickle, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 828 683 28.4 44.1 SE (Bordes et al. 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8 SME (linear) (Bordes et al.2012) 545 533 65.1 74.1 274 154 30.7 40.8 SME (Bilinear) (Bordes et al. 2012) 526 509 54.7 61.3 284 158 31.3 41.3 LFM (Jenatton et al. 2012) 469 456 71.4 81.6 283 164 26.0 33.1 TransE (Bordes et al. 2013) 263 251 75.4 89.2 243 125 34.9 47.1 TransH (unif) (Wang et al. 2014) 318 303 75.4 86.7 211 84 42.5 58.5 TransH (bern) (Wang et al. 2014) 401 388 73.0 82.3 212 87 45.7 64.4 TransR (unif) (L</context>
<context position="29928" citStr="Bordes et al. 2012" startWordPosition="5172" endWordPosition="5175">230 78.9 92.3 233 82 44.0 66.3 CTransR (bern) (Lin et al. 2015) 231 218 79.4 92.3 199 75 48.4 70.2 TransD (unif) 242 229 79.2 92.5 211 67 49.4 74.2 TransD (bern) 224 212 79.6 92.2 194 91 53.4 77.3 Table 4: Experimental results on link prediction. Tasks Prediction Head (Hits@10) Prediction Tail (Hits@10) Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N Unstructured (Bordes et al. 2012) 34.5 2.5 6.1 6.6 34.3 4.2 1.9 6.6 SE (Bordes et al. 2011) 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3 SME (linear) (Bordes et al.2012) 35.1 53.7 19.0 40.3 32.7 14.9 61.6 43.3 SME (Bilinear) (Bordes et al. 2012) 30.9 69.6 19.9 38.6 28.2 13.1 76.0 41.8 TransE (Bordes et al. 2013) 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0 TransH (unif) (Wang et al. 2014) 66.7 81.7 30.2 57.4 63.7 30.1 83.2 60.8 TransH (bern) (Wang et al. 2014) 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2 TransR (unif) (Lin et al. 2015) 76.9 77.9 38.1 66.9 76.2 38.4 76.2 69.1 TransR (bern) (Lin et al. 2015) 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1 CTransR (unif) (Lin et al. 2015) 78.6 77.8 36.4 68.0 77.4 37.8 78.0 70.3 CTransR (bern) (Lin et al. 2015) 81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8 TransD (unif) 80.7 85.8 47.1 75.6 80.0 54.5 80.7 77.9 T</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Bordes A., Glorot X., Weston J., and Bengio Y. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In Proceedings of AISTATS. pags:127-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bordes</author>
<author>X Glorot</author>
<author>J Weston</author>
<author>Y Bengio</author>
</authors>
<title>A semantic matching energy function for learing with multirelational data.</title>
<date>2014</date>
<journal>Machine Learning.</journal>
<pages>94--2</pages>
<contexts>
<context position="19177" citStr="Bordes et al. 2014" startWordPosition="3308" endWordPosition="3311">nts and Results Analysis We evaluate our apporach on two tasks: triplets classification and link prediction. Then we show the experiments results and some analysis of them. 4.1 Data Sets Triplets classification and link prediction are implemented on two popular knowledge graphs: WordNet (Miller 1995) and Freebase (Bollacker et al. 2008). WordNet is a large lexical knowledge graph. Entities in WordNet are synonyms which express distinct concepts. Relations in WordNet are conceptual-semantic and lexical relations. In this paper, we use two subsets of WordNet: WN11 (Socher et al. 2013) and WN18 (Bordes et al. 2014). Freebase is a large collaborative knowledge base consists of a large number of the world facts, such as triplets (anthony asquith, location, london) and (nobuko otowa, profession, actor). We also use two subsets of Freebase: FB15k (Bordes et al. 2014) and FB13 (Socher et al. 2013). Table 2 lists statistics of the 4 datasets. Dataset #Rel #Ent #Train #Valid #Test WN11 11 38,696 112,581 2,609 10,544 WN18 18 40,943 141,442 5,000 5,000 FB13 13 75,043 316,232 5908 23,733 FB15k 1,345 14,951 483,142 50,000 59,071 Table 2: Datesets used in the experiments. 691 4.2 Triplets Classification Triplets cl</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2014</marker>
<rawString>Bordes A., Glorot X., Weston J., and Bengio Y. 2014. A semantic matching energy function for learing with multirelational data. Machine Learning. 94(2):pags:233-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bordes</author>
<author>J Weston</author>
<author>R Collobert</author>
<author>Y Bengio</author>
</authors>
<title>Learning structured embeddings of knowledge bases.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<pages>301--306</pages>
<contexts>
<context position="8353" citStr="Bordes et al. 2011" startWordPosition="1370" endWordPosition="1373">s close to (t) and the score function is fr(h,t) = −kh + r − tk22 (1) TransE is only suitable for 1-to-1 relations, there remain flaws for 1-to-N, N-to-1 and N-to-N relations. To solve these problems, TransH (Wang et al. 2014) proposes an improved model named translation on a hyperplane. On hyperplanes of different relations, a given entity has different representations. Similar to TransE, TransH has the score function as follows: fr(h,t) = −kh⊥ + r − t⊥k22 (2) Entity Space —non apace 688 Model #Parameters # Operations (Time complexity) Unstructured (Bordes et al. 2012; 2014) O(Nem) O(Nt) SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt) SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM (Jenatton et al. 2012; Sutskever et al. 2009) O(Nem + Nrn2)(m = n) O((m2 + m)Nt) SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt) NTN (Socher et al. 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2 + m)s + 2mk + k)Nt) TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt) TransH (Wang et al. 2014) O(Nem + 2Nrn)(m = n) O(2mNt) TransR (Lin et al. 2015) O(Nem + Nr(m + 1)n) O(2mnNt) C</context>
<context position="10667" citStr="Bordes et al. 2011" startWordPosition="1801" endWordPosition="1804"> IMrh + r − MrtI I22. (3) where Mr E Rm×n, h, t E Rn and r E Rm. CTransR is an extension of TransR. As head-tail entity pairs present various patterns in different relations, CTransR clusters diverse head-tail entity pairs into groups and sets a relation vector for each group. 2.2 Other Models Unstructured. Unstructured model (Bordes et al. 2012; 2014) ignores relations, only models entities as embeddings. The score function is fr(h,t) = −I Ih − tI I22. (4) It’s a simple case of TransE. Obviously, Unstructured model can not distinguish different relations. Structured Embedding (SE). SE model (Bordes et al. 2011) sets two separate matrices Mrh and Mrt to project head and tail entities for each relation. Its score function is defined as follows: fr(h,t) = −I IMrhh − MrttI I1 (5) Semantic Matching Energy (SME). SME model (Bordes et al. 2012; 2014) encodes each named symbolic object (entities and relations) as a vector. Its score function is a neural network that captures correlations between entities and relations via matrix operations. Parameters of the neural network are shared by all relations. SME defines two semantic matching energy functions for optimization, a linear form gη = Mη1eη + Mη2r + bη (</context>
<context position="28695" citStr="Bordes et al. 2011" startWordPosition="4938" endWordPosition="4941">n” settings. TransD achieves better performance than CTransR in all types of relations (1-to-1, 1-to-N, N-to-1 and N-to-N). For N-to-N relations in predicting both head and tail, our approach improves the Hits@10 by almost 7.4% than CTransR. In particular, for 1Mapping properties of relations follows the same rules in (Bordes et al. 2013) 693 Data sets WN18 FB15K Metric Mean Rank Hits@10 Mean Rank Hits@10 Raw Filt Raw Filt Raw Filt Raw Filt Unstructured (Bordes et al. 2012) 315 304 35.3 38.2 1,074 979 4.5 6.3 RESCAL (Nickle, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 828 683 28.4 44.1 SE (Bordes et al. 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8 SME (linear) (Bordes et al.2012) 545 533 65.1 74.1 274 154 30.7 40.8 SME (Bilinear) (Bordes et al. 2012) 526 509 54.7 61.3 284 158 31.3 41.3 LFM (Jenatton et al. 2012) 469 456 71.4 81.6 283 164 26.0 33.1 TransE (Bordes et al. 2013) 263 251 75.4 89.2 243 125 34.9 47.1 TransH (unif) (Wang et al. 2014) 318 303 75.4 86.7 211 84 42.5 58.5 TransH (bern) (Wang et al. 2014) 401 388 73.0 82.3 212 87 45.7 64.4 TransR (unif) (Lin et al. 2015) 232 219 78.3 91.7 226 78 43.8 65.5 TransR (bern) (Lin et al. 2015) 238 225 79.8 92.0 198 77 48.2 68.7 CTransR (unif) (Lin et </context>
</contexts>
<marker>Bordes, Weston, Collobert, Bengio, 2011</marker>
<rawString>Bordes A., Weston J., Collobert R., and Bengio Y. 2011. Learning structured embeddings of knowledge bases. In Proceedings of AAAI. pags:301-306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jenatton</author>
<author>Nicolas L Roux</author>
<author>A Bordes</author>
<author>G Obozinaki</author>
</authors>
<title>A latent factor model for highly multi-relational data.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<pages>3167--3175</pages>
<contexts>
<context position="8573" citStr="Jenatton et al. 2012" startWordPosition="1414" endWordPosition="1417">. 2014) proposes an improved model named translation on a hyperplane. On hyperplanes of different relations, a given entity has different representations. Similar to TransE, TransH has the score function as follows: fr(h,t) = −kh⊥ + r − t⊥k22 (2) Entity Space —non apace 688 Model #Parameters # Operations (Time complexity) Unstructured (Bordes et al. 2012; 2014) O(Nem) O(Nt) SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt) SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM (Jenatton et al. 2012; Sutskever et al. 2009) O(Nem + Nrn2)(m = n) O((m2 + m)Nt) SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt) NTN (Socher et al. 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2 + m)s + 2mk + k)Nt) TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt) TransH (Wang et al. 2014) O(Nem + 2Nrn)(m = n) O(2mNt) TransR (Lin et al. 2015) O(Nem + Nr(m + 1)n) O(2mnNt) CTransR (Lin et al. 2015) O(Nem + Nr(m + d)n) O(2mnNt) TransD (this paper) O(2Nem + 2Nrn) O(2nNt) Table 1: Complexity (the number of parameters and the number of multiplication operations in an epoch) of several embedding</context>
<context position="11586" citStr="Jenatton et al. 2012" startWordPosition="1967" endWordPosition="1971">s a vector. Its score function is a neural network that captures correlations between entities and relations via matrix operations. Parameters of the neural network are shared by all relations. SME defines two semantic matching energy functions for optimization, a linear form gη = Mη1eη + Mη2r + bη (6) and a bilinear form gη = (Mη1eη) ® (Mη2r) + bη (7) where q = {left, right}, eleft = h, eright = t and ® is the Hadamard product. The score function is fr(h, t) = gleftTgright (8) In (Bordes et al.2014), matrices of the bilinear form are replaced by tensors. Latent Factor Model (LFM). LFM model (Jenatton et al. 2012; Sutskever et al. 2009) encodes each entity into a vector and sets a matrix for every relation. It defines a score function fr(h, t) = hTMrt, which incorporates the interaction of the two entity vectors in a simple and effecitve way. Single Layer Model (SLM). SLM model is designed as a baseline of Neural Tensor Network (Socher et al. 2013). The model constructs a nonlinear neural network to represent the score function defined as follows. fr(h, t) = uTr f(Mr1h + Mr2t + br) (9) where Mr1, Mr2 and br are parameters indexed by relation r, f() is tanh operation. 689 Neural Tensor Network (NTN). N</context>
<context position="28901" citStr="Jenatton et al. 2012" startWordPosition="4978" endWordPosition="4981"> Hits@10 by almost 7.4% than CTransR. In particular, for 1Mapping properties of relations follows the same rules in (Bordes et al. 2013) 693 Data sets WN18 FB15K Metric Mean Rank Hits@10 Mean Rank Hits@10 Raw Filt Raw Filt Raw Filt Raw Filt Unstructured (Bordes et al. 2012) 315 304 35.3 38.2 1,074 979 4.5 6.3 RESCAL (Nickle, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 828 683 28.4 44.1 SE (Bordes et al. 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8 SME (linear) (Bordes et al.2012) 545 533 65.1 74.1 274 154 30.7 40.8 SME (Bilinear) (Bordes et al. 2012) 526 509 54.7 61.3 284 158 31.3 41.3 LFM (Jenatton et al. 2012) 469 456 71.4 81.6 283 164 26.0 33.1 TransE (Bordes et al. 2013) 263 251 75.4 89.2 243 125 34.9 47.1 TransH (unif) (Wang et al. 2014) 318 303 75.4 86.7 211 84 42.5 58.5 TransH (bern) (Wang et al. 2014) 401 388 73.0 82.3 212 87 45.7 64.4 TransR (unif) (Lin et al. 2015) 232 219 78.3 91.7 226 78 43.8 65.5 TransR (bern) (Lin et al. 2015) 238 225 79.8 92.0 198 77 48.2 68.7 CTransR (unif) (Lin et al. 2015) 243 230 78.9 92.3 233 82 44.0 66.3 CTransR (bern) (Lin et al. 2015) 231 218 79.4 92.3 199 75 48.4 70.2 TransD (unif) 242 229 79.2 92.5 211 67 49.4 74.2 TransD (bern) 224 212 79.6 92.2 194 91 53.4 </context>
</contexts>
<marker>Jenatton, Roux, Bordes, Obozinaki, 2012</marker>
<rawString>Jenatton R., Nicolas L. Roux, Bordes A., and Obozinaki G. 2012. A latent factor model for highly multi-relational data. In Proceedings of NIPS. pags:3167-3175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sutskever</author>
<author>R Salakhutdinov</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Modeling Relational Data using Bayesian Clustered Tensor Factorization.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<pages>1821--1828</pages>
<contexts>
<context position="8597" citStr="Sutskever et al. 2009" startWordPosition="1418" endWordPosition="1421">proved model named translation on a hyperplane. On hyperplanes of different relations, a given entity has different representations. Similar to TransE, TransH has the score function as follows: fr(h,t) = −kh⊥ + r − t⊥k22 (2) Entity Space —non apace 688 Model #Parameters # Operations (Time complexity) Unstructured (Bordes et al. 2012; 2014) O(Nem) O(Nt) SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt) SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM (Jenatton et al. 2012; Sutskever et al. 2009) O(Nem + Nrn2)(m = n) O((m2 + m)Nt) SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt) NTN (Socher et al. 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2 + m)s + 2mk + k)Nt) TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt) TransH (Wang et al. 2014) O(Nem + 2Nrn)(m = n) O(2mNt) TransR (Lin et al. 2015) O(Nem + Nr(m + 1)n) O(2mnNt) CTransR (Lin et al. 2015) O(Nem + Nr(m + d)n) O(2mnNt) TransD (this paper) O(2Nem + 2Nrn) O(2nNt) Table 1: Complexity (the number of parameters and the number of multiplication operations in an epoch) of several embedding models. Ne and Nr repre</context>
<context position="11610" citStr="Sutskever et al. 2009" startWordPosition="1972" endWordPosition="1975">function is a neural network that captures correlations between entities and relations via matrix operations. Parameters of the neural network are shared by all relations. SME defines two semantic matching energy functions for optimization, a linear form gη = Mη1eη + Mη2r + bη (6) and a bilinear form gη = (Mη1eη) ® (Mη2r) + bη (7) where q = {left, right}, eleft = h, eright = t and ® is the Hadamard product. The score function is fr(h, t) = gleftTgright (8) In (Bordes et al.2014), matrices of the bilinear form are replaced by tensors. Latent Factor Model (LFM). LFM model (Jenatton et al. 2012; Sutskever et al. 2009) encodes each entity into a vector and sets a matrix for every relation. It defines a score function fr(h, t) = hTMrt, which incorporates the interaction of the two entity vectors in a simple and effecitve way. Single Layer Model (SLM). SLM model is designed as a baseline of Neural Tensor Network (Socher et al. 2013). The model constructs a nonlinear neural network to represent the score function defined as follows. fr(h, t) = uTr f(Mr1h + Mr2t + br) (9) where Mr1, Mr2 and br are parameters indexed by relation r, f() is tanh operation. 689 Neural Tensor Network (NTN). NTN model (Socher et al. </context>
</contexts>
<marker>Sutskever, Salakhutdinov, Tenenbaum, 2009</marker>
<rawString>Sutskever I., Salakhutdinov R. and Joshua B. Tenenbaum. 2009. Modeling Relational Data using Bayesian Clustered Tensor Factorization. In Proceedings of NIPS. pags:1821-1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>D Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Reasoning With Neural Tensor Networks for Knowledge Base Completion.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<pages>926--934</pages>
<contexts>
<context position="8657" citStr="Socher et al. 2013" startWordPosition="1431" endWordPosition="1434">of different relations, a given entity has different representations. Similar to TransE, TransH has the score function as follows: fr(h,t) = −kh⊥ + r − t⊥k22 (2) Entity Space —non apace 688 Model #Parameters # Operations (Time complexity) Unstructured (Bordes et al. 2012; 2014) O(Nem) O(Nt) SE (Bordes et al. 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt) SME(linear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) (Bordes et al. 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM (Jenatton et al. 2012; Sutskever et al. 2009) O(Nem + Nrn2)(m = n) O((m2 + m)Nt) SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt) NTN (Socher et al. 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2 + m)s + 2mk + k)Nt) TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt) TransH (Wang et al. 2014) O(Nem + 2Nrn)(m = n) O(2mNt) TransR (Lin et al. 2015) O(Nem + Nr(m + 1)n) O(2mnNt) CTransR (Lin et al. 2015) O(Nem + Nr(m + d)n) O(2mnNt) TransD (this paper) O(2Nem + 2Nrn) O(2nNt) Table 1: Complexity (the number of parameters and the number of multiplication operations in an epoch) of several embedding models. Ne and Nr represent the number of entities and relations, respectively. Nt </context>
<context position="11928" citStr="Socher et al. 2013" startWordPosition="2032" endWordPosition="2035">r) + bη (7) where q = {left, right}, eleft = h, eright = t and ® is the Hadamard product. The score function is fr(h, t) = gleftTgright (8) In (Bordes et al.2014), matrices of the bilinear form are replaced by tensors. Latent Factor Model (LFM). LFM model (Jenatton et al. 2012; Sutskever et al. 2009) encodes each entity into a vector and sets a matrix for every relation. It defines a score function fr(h, t) = hTMrt, which incorporates the interaction of the two entity vectors in a simple and effecitve way. Single Layer Model (SLM). SLM model is designed as a baseline of Neural Tensor Network (Socher et al. 2013). The model constructs a nonlinear neural network to represent the score function defined as follows. fr(h, t) = uTr f(Mr1h + Mr2t + br) (9) where Mr1, Mr2 and br are parameters indexed by relation r, f() is tanh operation. 689 Neural Tensor Network (NTN). NTN model (Socher et al. 2013) extends SLM model by considering the second-order correlations into nonlinear neural networks. The score function is fr(h, t) = uTr f (hT�Wrt + Mr L t J + br) (10) where Wr represents a 3-way tensor, Mr denotes the weight matrix, br is the bias and f() is tanh operation. NTN is the most expressive model so far,</context>
<context position="19147" citStr="Socher et al. 2013" startWordPosition="3302" endWordPosition="3305">e knowledge graphs. 4 Experiments and Results Analysis We evaluate our apporach on two tasks: triplets classification and link prediction. Then we show the experiments results and some analysis of them. 4.1 Data Sets Triplets classification and link prediction are implemented on two popular knowledge graphs: WordNet (Miller 1995) and Freebase (Bollacker et al. 2008). WordNet is a large lexical knowledge graph. Entities in WordNet are synonyms which express distinct concepts. Relations in WordNet are conceptual-semantic and lexical relations. In this paper, we use two subsets of WordNet: WN11 (Socher et al. 2013) and WN18 (Bordes et al. 2014). Freebase is a large collaborative knowledge base consists of a large number of the world facts, such as triplets (anthony asquith, location, london) and (nobuko otowa, profession, actor). We also use two subsets of Freebase: FB15k (Bordes et al. 2014) and FB13 (Socher et al. 2013). Table 2 lists statistics of the 4 datasets. Dataset #Rel #Ent #Train #Valid #Test WN11 11 38,696 112,581 2,609 10,544 WN18 18 40,943 141,442 5,000 5,000 FB13 13 75,043 316,232 5908 23,733 FB15k 1,345 14,951 483,142 50,000 59,071 Table 2: Datesets used in the experiments. 691 4.2 Tripl</context>
<context position="20392" citStr="Socher et al. 2013" startWordPosition="3515" endWordPosition="3518">s classification aims to judge whether a given triplet (h, r, t) is correct or not, which is a binary classification task. Previous work (Socher et al. 2013; Wang et al. 2014; Lin et al. 2015) had explored this task. In this paper ,we use three datasets WN11, FB13 and FB15k to evaluate our approach. The test sets of WN11 and FB13 provided by (Socher et al. 2013) contain golden and negative triplets. As to FB15k, its test set only contains correct triplets, which requires us to construct negative triplets. In this parper, we construct negative triplets following the same setting used for FB13 (Socher et al. 2013). For triplets classification, we set a threshold 6r for each relation r. 6r is obtained by maximizing the classification accuracies on the valid set. For a given triplet (h, r, t), if its score is larger than 6r, it will be classified as positive, otherwise negative. We compare our model with several previous embedding models presented in Related Work section. As we construct negative triplets for FB15k by ourselves, we use the codes of TransE, TransH and TransR/CTransR provied by (Lin et al. 2015) to evaluate the datasets instead of reporting the results of (Wang et al.2014; Lin et al. 2015)</context>
<context position="24196" citStr="Socher et al. 2013" startWordPosition="4172" endWordPosition="4175">ndom prediction accuracy. In the view of intuition, similar to can be inferred from other information. However, the number of entity pairs linked by relation similar to is only 1672, which accounts for 1.5% in all train data, and prediction of the relation needs much information about entities. Therefore, the insufficient of train data is the main cause. On FB13, the accuracies of relations cuase of death and gender are lower than that of other relations because they are difficult to infer from other imformation, especially cuase of death. Relation gender may be inferred from a person’s name (Socher et al. 2013), but we learn a vector for each name, not for the words included in the names, which makes the 692 100 95 90 85 80 75 70 65 60 55 50 45 50 60 70 80 90 100 subordinate_instance_of synset_domain_topic member_meronym member_holonym domain_regio domain_topic has_instance similar_to has_part type_of part_of 50 60 70 80 90 100 WN11 unif bern cause_of_death profession nationality institution ethnicity religion gender 50 60 70 80 90 100 FB13 unif bern l���r����%7 af ���if� FB15K Accuracy(%) Accuracy(%) Accuracy(%) of &amp;quot;bern&amp;quot; Figure 3: Classification accuracies of different relations on the three datas</context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Socher R., Chen D., Christopher D. Manning and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In Proceedings of NIPS. pags:926-934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>A Bordes</author>
<author>Yakhnenko O Manning</author>
<author>N Ununier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<pages>1366--1371</pages>
<marker>Weston, Bordes, Manning, Ununier, 2013</marker>
<rawString>Weston J., Bordes A., Yakhnenko O. Manning and Ununier N. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of EMNLP. pags:1366-1371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<date>2012</date>
<booktitle>ADADELTA: AN ADAPTIVE LEARNING RATE METHOD. In Proceedings of CVPR.</booktitle>
<contexts>
<context position="21080" citStr="Zeiler 2012" startWordPosition="3633" endWordPosition="3634"> is obtained by maximizing the classification accuracies on the valid set. For a given triplet (h, r, t), if its score is larger than 6r, it will be classified as positive, otherwise negative. We compare our model with several previous embedding models presented in Related Work section. As we construct negative triplets for FB15k by ourselves, we use the codes of TransE, TransH and TransR/CTransR provied by (Lin et al. 2015) to evaluate the datasets instead of reporting the results of (Wang et al.2014; Lin et al. 2015) directly. In this experiment, we optimize the objective with ADADELTA SGD (Zeiler 2012). We select the margin -y among 11, 2, 5, 101, the dimension of entity vectors m and the dimension of relation vectors n among 120, 50, 80, 1001, and the mini-batch size B among 1100, 200, 1000, 48001. The best configuration obtained by valid set are:-y = 1, m, n = 100, B = 1000 and taking L2 as dissimilarity on WN11; -y = 1, m, n = 100, B = 200 and taking L2 as dissimilarity on FB13; -y = 2, m, n = 100, B = 4800 and taking L1 as dissimilarity on FB15k. For all the three datasets, We traverse to training for 1000 rounds. As described in Related Work section, TransD trains much faster than Tran</context>
<context position="26789" citStr="Zeiler 2012" startWordPosition="4624" endWordPosition="4625">hieved by a good embedding model. We call the evaluation setting ”Raw’. Noting the fact that a corrupted triplet may also exist in knowledge graphs, the corrupted triplet should be regard as a correct triplet. Hence, we should remove the corrupted triplets included in train, valid and test sets before ranking. We call this evaluation setting ”Filter”. In this paper, we will report evaluation results of the two settings . In this task, we use two datasets: WN18 and FB15k. As all the data sets are the same, we refer to their experimental results in this paper. On WN18, we also use ADADELTA SGD (Zeiler 2012) for optimization. We select the margin -y among 10.1, 0.5, 1, 21, the dimension of entity vectors m and the dimension of relation vectors n among 120, 50, 80, 1001, and the mini-batch size B among 1100, 200, 1000, 14001. The best configuration obtained by valid set are:-y = 1, m, n = 50, B = 200 and taking L2 as dissimilarity. For both the two datasets, We traverse to training for 1000 rounds. Experimental results on both WN18 and FB15k are shown in Table 4. From Table 4, we can conclude that: (1) TransD outperforms other baseline embedding models (TransE, TransH and TransR/CTransR), especial</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D. Zeiler. 2012. ADADELTA: AN ADAPTIVE LEARNING RATE METHOD. In Proceedings of CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Manning, Ng, 2012</marker>
<rawString>Socher R., Huval B., Christopher D Manning. Manning and Andrew Y. Ng. 2012. Semantic Compositionality through Recursive Matrix-vector Spaces. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nickel</author>
<author>V Tresp</author>
<author>H-P Kriegel</author>
</authors>
<title>A threeway model for collective learning on multi-relational data.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML.</booktitle>
<pages>809--816</pages>
<marker>Nickel, Tresp, Kriegel, 2011</marker>
<rawString>Nickel M., Tresp V., Kriegel H-P. 2011. A threeway model for collective learning on multi-relational data. In Proceedings of ICML. pages:809-816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nickel</author>
<author>V Tresp</author>
<author>H-P Kriegel</author>
</authors>
<title>Factorizing YAGO: Scalable Machine Learning for Linked Data.</title>
<date>2012</date>
<booktitle>In Proceedings of WWW.</booktitle>
<marker>Nickel, Tresp, Kriegel, 2012</marker>
<rawString>Nickel M., Tresp V., Kriegel H-P. 2012. Factorizing YAGO: Scalable Machine Learning for Linked Data. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nickel</author>
<author>V Tresp</author>
</authors>
<title>An Analysis of Tensor Models for Learning from Structured Data. Machine Learning and Knowledge Discovery in Databases,</title>
<date>2013</date>
<publisher>Springer.</publisher>
<marker>Nickel, Tresp, 2013</marker>
<rawString>Nickel M., Tresp V. 2013a. An Analysis of Tensor Models for Learning from Structured Data. Machine Learning and Knowledge Discovery in Databases, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nickel</author>
<author>V Tresp</author>
</authors>
<title>Tensor Factorization for Multi-Relational Learning.</title>
<date>2013</date>
<booktitle>Machine Learning and Knowledge Discovery in Databases,</booktitle>
<publisher>Springer.</publisher>
<marker>Nickel, Tresp, 2013</marker>
<rawString>Nickel M., Tresp V. 2013b. Tensor Factorization for Multi-Relational Learning. Machine Learning and Knowledge Discovery in Databases, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nickel</author>
<author>K Murphy</author>
<author>V Tresp</author>
<author>E Gabrilovich</author>
</authors>
<title>A Review of Relational Machine Learning for Knowledge Graphs.</title>
<date>2015</date>
<booktitle>In Proceedings of IEEE.</booktitle>
<marker>Nickel, Murphy, Tresp, Gabrilovich, 2015</marker>
<rawString>Nickel M., Murphy K., Tresp V., Gabrilovich E. 2015. A Review of Relational Machine Learning for Knowledge Graphs. In Proceedings of IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>